paperId,url,title,abstract,venue,year,externalIds.DOI,database,query_name,query_value
95b0e56cccc9141e371700ae6173ba5b9d46ed7c,https://www.semanticscholar.org/paper/95b0e56cccc9141e371700ae6173ba5b9d46ed7c,Fine-grained classification of social science journal articles using textual data: A comparison of supervised machine learning approaches,"We compare two supervised machine learning algorithms—Multinomial Naïve Bayes and Gradient Boosting—to classify social science articles using textual data. The high level of granularity of the classification scheme used and the possibility that multiple categories are assigned to a document make this task challenging. To collect the training data, we query three discipline specific thesauri to retrieve articles corresponding to specialties in the classification. The resulting data set consists of 113,909 records and covers 245 specialties, aggregated into 31 subdisciplines from three disciplines. Experts were consulted to validate the thesauri-based classification. The resulting multilabel data set is used to train the machine learning algorithms in different configurations. We deploy a multilabel classifier chaining model, allowing for an arbitrary number of categories to be assigned to each document. The best results are obtained with Gradient Boosting. The approach does not rely on citation data. It can be applied in settings where such information is not available. We conclude that fine-grained text-based classification of social sciences publications at a subdisciplinary level is a hard task, for humans and machines alike. A combination of human expertise and machine learning is suggested as a way forward to improve the classification of social sciences documents.",Quantitative Science Studies,2021,10.1162/qss_a_00106,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3a2f8def7038c5454f8c20a789fe3454d9a14e7b,https://www.semanticscholar.org/paper/3a2f8def7038c5454f8c20a789fe3454d9a14e7b,ML4Chem: A Machine Learning Package for Chemistry and Materials Science,"ML4Chem is an open-source machine learning library for chemistry and materials science. It
provides an extendable platform to develop and deploy machine learning models and pipelines and
is targeted to the non-expert and expert users. ML4Chem follows user-experience design and offers
the needed tools to go from data preparation to inference. Here we introduce its atomistic module
for the implementation, deployment, and reproducibility of atom-centered models. This module is
composed of six core building blocks: data, featurization, models, model optimization, inference,
and visualization. We present their functionality and ease of use with demonstrations utilizing
neural networks and kernel ridge regression algorithms.",ArXiv,2020,10.26434/chemrxiv.11952516.v1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
88edac037922f4a1e507b56f3fc85984bc63f848,https://www.semanticscholar.org/paper/88edac037922f4a1e507b56f3fc85984bc63f848,ML4Chem: A Machine Learning Package for Chemistry and Materials Science,"ML4Chem is an open-source machine learning library for chemistry and materials science. It
provides an extendable platform to develop and deploy machine learning models and pipelines and
is targeted to the non-expert and expert users. ML4Chem follows user-experience design and offers
the needed tools to go from data preparation to inference. Here we introduce its atomistic module
for the implementation, deployment, and reproducibility of atom-centered models. This module is
composed of six core building blocks: data, featurization, models, model optimization, inference,
and visualization. We present their functionality and ease of use with demonstrations utilizing
neural networks and kernel ridge regression algorithms.",,2020,10.26434/chemrxiv.11952516,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
71960b10294f741fdcb1c339a218c378eba6bc03,https://www.semanticscholar.org/paper/71960b10294f741fdcb1c339a218c378eba6bc03,Trust in AutoML: exploring information needs for establishing trust in automated machine learning systems,"We explore trust in a relatively new area of data science: Automated Machine Learning (AutoML). In AutoML, AI methods are used to generate and optimize machine learning models by automatically engineering features, selecting models, and optimizing hyperparameters. In this paper, we seek to understand what kinds of information influence data scientists' trust in the models produced by AutoML? We operationalize trust as a willingness to deploy a model produced using automated methods. We report results from three studies - qualitative interviews, a controlled experiment, and a card-sorting task - to understand the information needs of data scientists for establishing trust in AutoML systems. We find that including transparency features in an AutoML tool increased user trust and understandability in the tool; and out of all proposed features, model performance metrics and visualizations are the most important information to data scientists when establishing their trust with an AutoML tool.",IUI,2020,10.1145/3377325.3377501,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b14558c0b727af0ac9086f463b6030b9072dbe16,https://www.semanticscholar.org/paper/b14558c0b727af0ac9086f463b6030b9072dbe16,Methods for Automatic Machine-Learning Workflow Analysis,,ECML/PKDD,2021,10.1007/978-3-030-86517-7_4,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
de4322c57d80e0d85412aea6b2ff84b621ecd7ac,https://www.semanticscholar.org/paper/de4322c57d80e0d85412aea6b2ff84b621ecd7ac,Discover Discriminatory Bias in High Accuracy Models Embedded in Machine Learning Algorithms,,"Advances in Natural Computation, Fuzzy Systems and Knowledge Discovery",2020,10.1007/978-3-030-70665-4_166,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
76b80b610537ab916e81e31901c81156eadf0d61,https://www.semanticscholar.org/paper/76b80b610537ab916e81e31901c81156eadf0d61,Automated Machine Learning for Wind Farms Location,": Automated Machine Learning aims at preparing effective Machine Learning models with little or no data science expertise. Tedious tasks like preprocessing, algorithm selection and hyper-parameters optimization are then automatized: end-users just have to apply and deploy the model that best suits the real world problem. In this paper, we experiment Automated Machine Learning to leverage open data sources for predicting potential next wind farms location in Luxembourg, France, Belgium and Germany.",ICPRAM,2021,10.5220/0010232102220227,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6a29fae129632f40acf6cbed76645a054f6979a9,https://www.semanticscholar.org/paper/6a29fae129632f40acf6cbed76645a054f6979a9,Politics of Adversarial Machine Learning,"In addition to their security properties, adversarial machine-learning attacks and defenses have political dimensions. They enable or foreclose certain options for both the subjects of the machine learning systems and for those who deploy them, creating risks for civil liberties and human rights. In this paper, we draw on insights from science and technology studies, anthropology, and human rights literature, to inform how defenses against adversarial attacks can be used to suppress dissent and limit attempts to investigate machine learning systems. To make this concrete, we use real-world examples of how attacks such as perturbation, model inversion, or membership inference can be used for socially desirable ends. Although the predictions of this analysis may seem dire, there is hope. Efforts to address human rights concerns in the commercial spyware industry provide guidance for similar measures to ensure ML systems serve democratic, not authoritarian ends.",SSRN Electronic Journal,2020,10.2139/SSRN.3547322,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b3d6e532ef958296f99fb62600f7bf1275b15e3e,https://www.semanticscholar.org/paper/b3d6e532ef958296f99fb62600f7bf1275b15e3e,Object Detection in Fog Computing Using Machine Learning Algorithms,"The moment we live in today demands the convergence of the cloud computing, fog computing, machine learning, and IoT to explore new technological solutions. Fog computing is an emerging architecture intended for alleviating the network burdens at the cloud and the core network by moving resource-intensive functionalities such as computation, communication, storage, and analytics closer to the end users. Machine learning is a subfield of computer science and is a type of artificial intelligence (AI) that provides machines with the ability to learn without explicit programming. IoT has the ability to make decisions and take actions autonomously based on algorithmic sensing to acquire sensor data. These embedded capabilities will range across the entire spectrum of algorithmic approaches that is associated with machine learning. Here the authors explore how machine learning methods have been used to deploy the object detection, text detection in an image, and incorporated for better fulfillment of requirements in fog computing.",,2020,10.4018/978-1-7998-0194-8.ch006,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1d3539a8d94bd3ab78993d7cc584efc06ed0e460,https://www.semanticscholar.org/paper/1d3539a8d94bd3ab78993d7cc584efc06ed0e460,Synthetic Benchmarks for Scientific Research in Explainable Machine Learning,"As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on realworld datasets. In this work, we address this issue by releasing XAI-BENCH: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate groundtruth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and across a variety of settings. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench.",NeurIPS Datasets and Benchmarks,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b5b98051b65da6b1b3b579862b0407d48c5bef48,https://www.semanticscholar.org/paper/b5b98051b65da6b1b3b579862b0407d48c5bef48,Principles and Practice of Explainable Machine Learning,"Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.",Frontiers in Big Data,2020,10.3389/fdata.2021.688969,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b0d0f2a4eb97fb36198672ff5d4307bb48057b01,https://www.semanticscholar.org/paper/b0d0f2a4eb97fb36198672ff5d4307bb48057b01,"Code-free machine learning for object detection in surgical video: a benchmarking, feasibility, and cost study.","OBJECTIVE
While the utilization of machine learning (ML) for data analysis typically requires significant technical expertise, novel platforms can deploy ML methods without requiring the user to have any coding experience (termed AutoML). The potential for these methods to be applied to neurosurgical video and surgical data science is unknown.


METHODS
AutoML, a code-free ML (CFML) system, was used to identify surgical instruments contained within each frame of endoscopic, endonasal intraoperative video obtained from a previously validated internal carotid injury training exercise performed on a high-fidelity cadaver model. Instrument-detection performances using CFML were compared with two state-of-the-art ML models built using the Python coding language on the same intraoperative video data set.


RESULTS
The CFML system successfully ingested surgical video without the use of any code. A total of 31,443 images were used to develop this model; 27,223 images were uploaded for training, 2292 images for validation, and 1928 images for testing. The mean average precision on the test set across all instruments was 0.708. The CFML model outperformed two standard object detection networks, RetinaNet and YOLOv3, which had mean average precisions of 0.669 and 0.527, respectively, in analyzing the same data set. Significant advantages to the CFML system included ease of use, relatively low cost, displays of true/false positives and negatives in a user-friendly interface, and the ability to deploy models for further analysis with ease. Significant drawbacks of the CFML model included an inability to view the structure of the trained model, an inability to update the ML model once trained with new examples, and the inability for robust downstream analysis of model performance and error modes.


CONCLUSIONS
This first report describes the baseline performance of CFML in an object detection task using a publicly available surgical video data set as a test bed. Compared with standard, code-based object detection networks, CFML exceeded performance standards. This finding is encouraging for surgeon-scientists seeking to perform object detection tasks to answer clinical questions, perform quality improvement, and develop novel research ideas. The limited interpretability and customization of CFML models remain ongoing challenges. With the further development of code-free platforms, CFML will become increasingly important across biomedical research. Using CFML, surgeons without significant coding experience can perform exploratory ML analyses rapidly and efficiently.",Neurosurgical focus,2022,10.3171/2022.1.FOCUS21652,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f22f3d344654c2cbf28329a0359790c2be2b8f3e,https://www.semanticscholar.org/paper/f22f3d344654c2cbf28329a0359790c2be2b8f3e,"Bighead: A Framework-Agnostic, End-to-End Machine Learning Platform","With the increasing need to build systems and products powered by machine learning inside organizations, it is critical to have a platform that provides machine learning practitioners with a unified environment to easily prototype, deploy, and maintain their models at scale. However, due to the diversity of machine learning libraries, the inconsistency between environments, and various scalability requirement, there is no existing work to date that addresses all of these challenges. Here, we introduce Bighead, a framework-agnostic, end-to-end platform for machine learning. It offers a seamless user experience requiring only minimal efforts that span feature set management, prototyping, training, batch (offline) inference, real-time (online) inference, evaluation, and model lifecycle management. In contrast to existing platforms, it is designed to be highly versatile and extensible, and supports all major machine learning frameworks, rather than focusing on one particular framework. It ensures consistency across different environments and stages of the model lifecycle, as well as across data sources and transformations. It scales horizontally and elastically in response to the workload such as dataset size and throughput. Its components include a feature management framework, a model development toolkit, a lifecycle management service with UI, an offline training and inference engine, an online inference service, an interactive prototyping environment, and a Docker image customization tool. It is the first platform to offer a feature management component that is a general-purpose aggregation framework with lambda architecture and temporal joins. Bighead is deployed and widely adopted at Airbnb, and has enabled the data science and engineering teams to develop and deploy machine learning models in a timely and reliable manner. Bighead has shortened the time to deploy a new model from months to days, ensured the stability of the models in production, facilitated adoption of cutting-edge models, and enabled advanced machine learning based product features of the Airbnb platform. We present two use cases of productionizing models of computer vision and natural language processing.",2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),2019,10.1109/DSAA.2019.00070,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ad92f0a37c2ac423423cfc9f5be53085b46505c7,https://www.semanticscholar.org/paper/ad92f0a37c2ac423423cfc9f5be53085b46505c7,From Science to Practice: Improving ROP by Utilizing a Cloud-Based Machine-Learning Solution in Real-Time Drilling Operations,"
 This paper is a follow up to the URTeC (2019-343) publication where the training of a Machine Learning (ML) model to predict rate of penetration (ROP) is described. The ML model gathers recent drilling parameters and approximates drilling conditions downhole to predict ROP. In real time, the model is run through an optimization sweep by adjusting parameters which can be controlled by the driller. The optimal drilling parameters and modeled ROP are then displayed for the driller to utilize.
 The ML model was successfully deployed and tested in real time in collaboration with leading shale operators in the Permian Basin. The testing phase was split in two parts, preliminary field tests and trials of the end-product. The key learnings from preliminary field tests were used to develop an integrated driller's dashboard with optimal drilling parameters recommendations and situational awareness tools for high dysfunction and procedural compliance which was used for designed trials.
 The results of field trials are discussed where subject well ROP was improved between 19-33% when comparing against observation/control footage. The overall ROP on subject wells was also compared against offset wells with similar target formations, BHAs, and wellbore trajectories. In those comparisons against qualified offsets, ROP was improved by as little as 5% and as much as 33%.
 In addition to comparing ROP performance, results from post-run data analysis are also presented. Detailed drilling data analytics were performed to check if using the recommendations during the trial caused any detrimental effects such as divergence in directional trends or high lateral or axial vibrations.
 The results from this analysis indicate that the measured downhole axial and lateral vibrations were in the safe zone. Also, no significant deviations in rotary trends were observed.",,2021,10.2118/204043-MS,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4650205b4834825b72644bf9d35553f657e141a8,https://www.semanticscholar.org/paper/4650205b4834825b72644bf9d35553f657e141a8,Yes We Care! - Certification for Machine Learning Methods through the Care Label Framework,"Machine learning applications have become ubiquitous. Their applications from machine embedded control in production over process optimization in diverse areas (e.g., traffic, finance, sciences) to direct user interactions like advertising and recommendations. This has led to an increased effort of making machine learning trustworthy. Explainable and fair AI have already matured. They address knowledgeable users and application engineers. However, there are users that want to deploy a learned model in a similar way as their washing machine. These stakeholders do not want to spend time understanding the model. Instead, they want to rely on guaranteed properties. What are the relevant properties? How can they be expressed to stakeholders without presupposing machine learning knowledge? How can they be guaranteed for a certain implementation of a model? These questions move far beyond the current state-of-the-art and we want to address them here. We propose a unified framework that certifies learning methods via care labels. They are easy to understand and draw inspiration from well-known certificates like textile labels or property cards of electronic devices. Our framework considers both, the machine learning theory and a given implementation. We test the implementation’s compliance with theoretical properties and bounds. In this paper, we illustrate care labels by a prototype implementation of a certification suite for a selection of probabilistic graphical models. K. Morik, H. Kotthaus, L. Heppe, D. Heinrich, R. Fischer, S. Mücke, A. Pauly, M. Jakobs TU Dortmund University, Germany E-mail: {forename}.{surname}@tu-dortmund.de N. Piatkowski Fraunhofer IAIS, Germany E-mail: nico.piatkowski@iais.fraunhofer.de ar X iv :2 10 5. 10 19 7v 1 [ cs .L G ] 2 1 M ay 2 02 1 2 Katharina Morik et al.",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
423c16fb66d255928b5cbd03aae96529f9c3d702,https://www.semanticscholar.org/paper/423c16fb66d255928b5cbd03aae96529f9c3d702,Emerging Applications of Machine Learning in Food Safety.,"Food safety continues to threaten public health. Machine learning holds potential in leveraging large, emerging data sets to improve the safety of the food supply and mitigate the impact of food safety incidents. Foodborne pathogen genomes and novel data streams, including text, transactional, and trade data, have seen emerging applications enabled by a machine learning approach, such as prediction of antibiotic resistance, source attribution of pathogens, and foodborne outbreak detection and risk assessment. In this article, we provide a gentle introduction to machine learning in the context of food safety and an overview of recent developments and applications. With many of these applications still in their nascence, general and domain-specific pitfalls and challenges associated with machine learning have begun to be recognized and addressed, which are critical to prospective use and future deployment of large data sets and their associated machine learning models for food safety applications. Expected final online publication date for the Annual Review of Food Science and Technology, Volume 12 is March 2021. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",Annual review of food science and technology,2021,10.1146/annurev-food-071720-024112,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
56e63ffea11c875f7eee257798e18cd04e453b6c,https://www.semanticscholar.org/paper/56e63ffea11c875f7eee257798e18cd04e453b6c,Synthetic Benchmarks for Scientific Research in Explainable Machine Learning,"As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on realworld datasets. In this work, we address this issue by releasing XAI-BENCH: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate groundtruth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and across a variety of settings. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench.",,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ed9b2c4d4ea93d86a6e1167c9e92cfa0a243c4bc,https://www.semanticscholar.org/paper/ed9b2c4d4ea93d86a6e1167c9e92cfa0a243c4bc,Challenges and Opportunities for Unikernels in Machine Learning Inference,"Machine Learning has become a value creator for many new and old businesses. However, efficient realworld machine learning deployments are still a challenge. Traditional Machine Learning deployments suffer from efficient resource utilization and achieving predictable latency. They cannot be treated in the same manner as other application server deployments. Unikernels are a method to specialize application deployment and performance to suit the needs of the application. Traditionally, building or porting applications to unikernels have been challenging. However, recent work has been into simplifying the development of unikernels. Real-world Unikernels as of now are only for specializing applications that run on the CPU. We survey machine learning practitioners and find out that the majority of machine learning practitioners are using the CPU for machine learning deployments, thus, creating an opportunity for unikernels to optimize the performance of these applications. We compare the architecture of two unikernels: nanos and Unikraft. We benchmarked scikit-learn, a popular machine library, inside a unikernel and found that it only offered a 1% advantage over a traditional deployment. However, our testing could not include more innovative systems like Unikraft due to their immaturity and inability to run machine learning libraries. We include a dependency analysis of three popular machine learning libraries Tensorflow Lite, PyTorch and ONNX, to help pave the way for building machine learning applications as Unikraft unikernels.","2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",2021,10.1109/icrito51393.2021.9596080,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f330179465f41025d65f23ad061e9158eca6df4b,https://www.semanticscholar.org/paper/f330179465f41025d65f23ad061e9158eca6df4b,Trending in human ARTs: Jumping on the Artificial Intelligence and Machine Learning bandwagon,,Journal of Assisted Reproduction and Genetics,2021,10.1007/s10815-021-02280-4,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5d747901282d1fc8f5c1d76a5921c4e07ef8ecdf,https://www.semanticscholar.org/paper/5d747901282d1fc8f5c1d76a5921c4e07ef8ecdf,Early Prediction of Sepsis in the ICU Using Machine Learning: A Systematic Review,"Background: Sepsis is among the leading causes of death in intensive care units (ICUs) worldwide and its recognition, particularly in the early stages of the disease, remains a medical challenge. The advent of an affluence of available digital health data has created a setting in which machine learning can be used for digital biomarker discovery, with the ultimate goal to advance the early recognition of sepsis. Objective: To systematically review and evaluate studies employing machine learning for the prediction of sepsis in the ICU. Data Sources: Using Embase, Google Scholar, PubMed/Medline, Scopus, and Web of Science, we systematically searched the existing literature for machine learning-driven sepsis onset prediction for patients in the ICU. Study Eligibility Criteria: All peer-reviewed articles using machine learning for the prediction of sepsis onset in adult ICU patients were included. Studies focusing on patient populations outside the ICU were excluded. Study Appraisal and Synthesis Methods: A systematic review was performed according to the PRISMA guidelines. Moreover, a quality assessment of all eligible studies was performed. Results: Out of 974 identified articles, 22 and 21 met the criteria to be included in the systematic review and quality assessment, respectively. A multitude of machine learning algorithms were applied to refine the early prediction of sepsis. The quality of the studies ranged from “poor” (satisfying ≤ 40% of the quality criteria) to “very good” (satisfying ≥ 90% of the quality criteria). The majority of the studies (n = 19, 86.4%) employed an offline training scenario combined with a horizon evaluation, while two studies implemented an online scenario (n = 2, 9.1%). The massive inter-study heterogeneity in terms of model development, sepsis definition, prediction time windows, and outcomes precluded a meta-analysis. Last, only two studies provided publicly accessible source code and data sources fostering reproducibility. Limitations: Articles were only eligible for inclusion when employing machine learning algorithms for the prediction of sepsis onset in the ICU. This restriction led to the exclusion of studies focusing on the prediction of septic shock, sepsis-related mortality, and patient populations outside the ICU. Conclusions and Key Findings: A growing number of studies employs machine learning to optimize the early prediction of sepsis through digital biomarker discovery. This review, however, highlights several shortcomings of the current approaches, including low comparability and reproducibility. Finally, we gather recommendations how these challenges can be addressed before deploying these models in prospective analyses. Systematic Review Registration Number: CRD42020200133.",Frontiers in Medicine,2020,10.3389/fmed.2021.607952,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7d312ab14de253d96efd5139e8fc755e4962b7b5,https://www.semanticscholar.org/paper/7d312ab14de253d96efd5139e8fc755e4962b7b5,Value Cards: An Educational Toolkit for Teaching Social Impacts of Machine Learning through Deliberation,"Recently, there have been increasing calls for computer science curricula to complement existing technical training with topics related to Fairness, Accountability, Transparency and Ethics (FATE). In this paper, we present Value Cards, an educational toolkit to inform students and practitioners the social impacts of different machine learning models via deliberation. This paper presents an early use of our approach in a college-level computer science course. Through an in-class activity, we report empirical data for the initial effectiveness of our approach. Our results suggest that the use of the Value Cards toolkit can improve students' understanding of both the technical definitions and trade-offs of performance metrics and apply them in real-world contexts, help them recognize the significance of considering diverse social values in the development and deployment of algorithmic systems, and enable them to communicate, negotiate and synthesize the perspectives of diverse stakeholders. Our study also demonstrates a number of caveats we need to consider when using the different variants of the Value Cards toolkit. Finally, we discuss the challenges as well as future applications of our approach.",FAccT,2020,10.1145/3442188.3445971,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e0baaf44290991d674b6d54bd5dad535648a9877,https://www.semanticscholar.org/paper/e0baaf44290991d674b6d54bd5dad535648a9877,Credit Card Fraud Detection using Machine Learning and Data Science,"It is important that companies are able to identify fraudulent credit card transactions so that customers are not charged for items that they did not purchase. These problems can be handled with Data Science and its importance, along with Machine Learning. This project aim is to illustrate the modelling of a data set using machine learning with Credit Card. Our objective is to detect 100% of the fraudulent transactions while minimizing the incorrect fraud classifications. Credit Card Fraud Detection is a sample of classification. In this process, we have focused on analysing and pre-processing data sets as well as the deployment of multiple anomaly detection algorithms such as Local Outlier Factor and Isolation Forest algorithm on the PCA transformed Credit Card Transaction data.",International Journal for Research in Applied Science and Engineering Technology,2021,10.22214/ijraset.2021.37200,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9f4ae17db46e604337aa62ddb2c340af5a306553,https://www.semanticscholar.org/paper/9f4ae17db46e604337aa62ddb2c340af5a306553,Interpretable Automated Machine Learning in Maana(TM) Knowledge Platform,"Machine learning is becoming an essential part of developing solutions for many industrial applications, but the lack of interpretability hinders wide industry adoption to rapidly build, test, deploy and validate machine learning models, in the sense that the insight of developing machine learning solutions are not structurally encoded, justified and transferred. In this paper we describe the Maana Meta-learning Service, an interpretable and interactive automated machine learning service residing in XYZ Knowledge Platform that performs machine-guided, user assisted pipeline search and hyper-parameter tuning and generates structured knowledge about decisions for pipeline profiling and selection. The service is shipped with the Maana Knowledge Platform and is validated using benchmark dataset. Furthermore, its capability of deriving knowledge from pipeline search facilitates various inference tasks and transferring to similar data science projects.",AAMAS,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7ea0b0b290678da73a460326fbc86f353ce25077,https://www.semanticscholar.org/paper/7ea0b0b290678da73a460326fbc86f353ce25077,Earning Stripes in Medical Machine Learning,"& TODAY, WE ARE living through one of those heady situations in which scientific, technical, and commercial frontiers all simultaneously advance in a grand interrelated dance. Advances in computer technology in the last decade opened up the potential for big gains in applications of neural networks aimed at recognizing and diagnosing visual images. Many startups and established firms are making decisions about how to develop and deploy such software, and what products to develop next. While the science continues to expand into some headline popping territory, and the technologist find new prototypes for mind-altering forecasting, what has happened within markets? What has found its way into practical applications with commercial appeal? We can start with a simple observation: If real people are spending real money on an application of machine learning, then it must have created new economic value. However, it can be difficult to provide an overview of a broad moving target. Today’s column goes inside the actions of one firm’s experience as a way to provide insight into the broad trends. It will focus on the factors shaping the creation of value from applying machine learning in medical imaging. The focus will be on a firm called Zebra Medical Vision, an Israeli startup in the business of using machine learning to improve X-rays. Its experience illustrates many of the same technology trends and strategic issues faced by other machine learning startups, particularly those in healthcare markets.",IEEE Micro,2019,10.1109/MM.2019.2932278,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,"TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",ArXiv,2016,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d2d0476aa3e513313780cc3aefbc82fa5334e206,https://www.semanticscholar.org/paper/d2d0476aa3e513313780cc3aefbc82fa5334e206,Azure Machine Learning (ML) Workbench,,Machine Learning with Microsoft Technologies,2019,10.1007/978-1-4842-3658-1_14,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
824e402846f4c5cff6b4064f823e74a3f3ff1d2f,https://www.semanticscholar.org/paper/824e402846f4c5cff6b4064f823e74a3f3ff1d2f,Predictive Analytics with Microsoft Azure Machine Learning: Build and Deploy Actionable Solutions in Minutes,"Data Science and Machine Learning are in high demand, as customers are increasingly looking for ways to glean insights from all their data. More customers now realize that Business Intelligence is not enough as the volume, speed and complexity of data now defy traditional analytics tools. While Business Intelligence addresses descriptive and diagnostic analysis, Data Science unlocks new opportunities through predictive and prescriptive analysis. The purpose of this book is to provide a gentle and instructionally organized introduction to the field of data science and machine learning, with a focus on building and deploying predictive models. The book also provides a thorough overview of the Microsoft Azure Machine Learning service using task oriented descriptions and concrete end-to-end examples, sufficient to ensure the reader can immediately begin using this important new service. It describes all aspects of the service from data ingress to applying machine learning and evaluating the resulting model, to deploying the resulting model as a machine learning web service. Finally, this book attempts to have minimal dependencies, so that you can fairly easily pick and choose chapters to read. When dependencies do exist, they are listed at the start and end of the chapter. The simplicity of this new service from Microsoft will help to take Data Science and Machine Learning to a much broader audience than existing products in this space. Learn how you can quickly build and deploy sophisticated predictive models as machine learning web services with the new Azure Machine Learning service from Microsoft. What youll learn A structured introduction to Data Science and its best practices An introduction to the new Microsoft Azure Machine Learning service, explaining how to effectively build and deploy predictive models as machine learning web services Practical skills such as how to solve typical predictive analytics problems like propensity modeling, churn analysis and product recommendation. An introduction to the following skills: basic Data Science, the Data Mining process, frameworks for solving practical business problems with Machine Learning, and visualization with Power BIWho this book is for Data Scientists, Business Analysts, BI Professionals and Developers who are interested in expanding their repertoire of skill applied to machine learning and predictive analytics, as well as anyone interested in an in-depth explanation of the Microsoft Azure Machine Learning service through practical tasks and concrete applications. The reader is assumed to have basic knowledge of statistics and data analysis, but not deep experience in data science or data mining. Advanced programming skills are not required, although some experience with R programming would prove very useful.",,2014,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b6453f5f87c07d31d4fd0fa89d0f716036a3cc26,https://www.semanticscholar.org/paper/b6453f5f87c07d31d4fd0fa89d0f716036a3cc26,"Cloud computing survey on services, enhancements and challenges in the era of machine learning and data science","Cloud computing has sweeping impact on the human productivity. Today it’s used for Computing, Storage, Predictions and Intelligent Decision Making, among others. Intelligent Decision Making using Machine Learning has pushed for the Cloud Services to be even more fast, robust and accurate. Security remains one of the major concerns which affect the cloud computing growth however there exist various research challenges in cloud computing adoption such as lack of well managed service level agreement (SLA), frequent disconnections, resource scarcity, interoperability, privacy, and reliability. Tremendous amount of work still needs to be done to explore the security challenges arising due to widespread usage of cloud deployment using Containers. We also discuss Impact of Cloud Computing and Cloud Standards. Hence in this research paper, a detailed survey of cloud computing, concepts, architectural principles, key services, and implementation, design and deployment challenges of cloud computing are discussed in detail and important future research directions in the era of Machine Learning and Data Science have been identified.",International Journal of Informatics and Communication Technology (IJ-ICT),2020,10.11591/IJICT.V9I2.PP117-139,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3be7a0b80128dd2395d3230982dbd44d656d658e,https://www.semanticscholar.org/paper/3be7a0b80128dd2395d3230982dbd44d656d658e,Toward an Intelligent Edge: Wireless Communication Meets Machine Learning,"The recent revival of AI is revolutionizing almost every branch of science and technology. Given the ubiquitous smart mobile gadgets and IoT devices, it is expected that a majority of intelligent applications will be deployed at the edge of wireless networks. This trend has generated strong interest in realizing an ""intelligent edge"" to support AI-enabled applications at various edge devices. Accordingly, a new research area, called edge learning, has emerged, which crosses and revolutionizes two disciplines: wireless communication and machine learning. A major theme in edge learning is to overcome the limited computing power, as well as limited data, at each edge device. This is accomplished by leveraging the mobile edge computing platform and exploiting the massive data distributed over a large number of edge devices. In such systems, learning from distributed data and communicating between the edge server and devices are two critical and coupled aspects, and their fusion poses many new research challenges. This article advocates a new set of design guidelines for wireless communication in edge learning, collectively called learning- driven communication. Illustrative examples are provided to demonstrate the effectiveness of these design guidelines. Unique research opportunities are identified.",IEEE Communications Magazine,2018,10.1109/MCOM.001.1900103,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
952ff7836acc252b55dc65c4455a8c1209d8f64a,https://www.semanticscholar.org/paper/952ff7836acc252b55dc65c4455a8c1209d8f64a,Tools Needed for Automating Science : Formalizing the use of Active Machine Learning to Drive Experimentation,"There is a need to develop and deploy advanced technologies for fully automating the execution of science and engineering projects. These technologies could dramatically decrease the costs of research and engineering, while increasing throughput and reproducibility. Existing platforms for automating research merely execute experiments selected by humans. What is needed are generalizable technologies (open source software and community standards) capable of closed-loop hypothesis generation from available data, experiment selection, and automated execution. Many biological and chemical systems are too complex for humans to understand completely, due to their scale and their nonlinear and stochastic behaviors. Traditionally, scientists and engineers choose and perform experiments to test hypotheses or to optimize designs. As the system’s complexity increases, the number of possible experiments that could be performed to study it rises exponentially, and, since resource constraints limit the number of experiments performed, we are faced with the need to select a maximally informative set of experiments from a combinatorial space of possible experiments, trying to optimize financial or other constraints. Unfortunately, the human mind is not well suited to solving this type of optimization problem, due most often to our inability to form predictive models at the scales involved. The result is that, in practice, many humanselected experiments are “wasted” on conditions where no effect is observed or, more importantly, where the effect is predictable from other experiments, given computational assistance. This waste of resources ultimately limits what scientists and engineers can accomplish. This type of problem is the realm of Active Learning [1-6], a sub-domain of Machine Learning focused on algorithms for iteratively choosing experiments expected to optimally improve an underlying computational model (Figure 1). While active learning could provide benefits for essentially all large scale screening and experimentation, such as drug development [7-8], there are significant barriers to its routine use. Perhaps the most significant is the absence of robust, readily available software to facilitate use by any group embarking on large scale experimentation. We therefore suggest the need for the development of open source tools and open access standards to enable the routine use of active learning driven experimentation. We suggest tools are needed for four connected tasks: random access experimentation; experimental data analysis; predictive model construction; and active learning experiment selection (Figure 2). The first component is the most involved, in that it may be highly specialized for particular types of experiments. However, the first step is to have the experimenter communicate to an automated system the specifics of how to perform an experiment and what experiments are allowed (e.g., which cell lines and drugs may be chosen from). The former is simply a protocol that, for example, liquid-handling robots and automated measurement systems are used to execute and open standards currently exist. The latter is simply defining the source plates/libraries. However, most current systems can only run the protocol for entire rows, columns or plates. The key to using such systems for active learning is to allow a computer to specify a particular set of experiments to perform that does not conform to these limitations (e.g., cells 1a, 4c, 9f, 2e, etc.). We suggest the need for collaboration between software developers, instrument manufacturers and contract research organizations to Murphy, Kangas and Langmead White Paper: Tools Needed for Automating Science 2 implement such systems and create open standards for a computer to communicate a desired set of experiments to an automated system without human intervention. The second component, predictive model generation, is also specialized for a particular type of data or problem, and would typically be paired with a particular protocol or instrument type. However, much work has been done on automated analysis and modeling pipelines for various data sources , the interfaces to which can be standardized. There has been significant work on the third of these components in the context of large experimental spaces. This first is on matrix completion methods that construct a predictive model for an entire space given data for some parts of that space. For example, work has been done on constructing a predictive model of drug-target interactions in the setting of drug discovery [9-11]. However, that work has focused on the task of predicting which new drugs will interact with known targets given data on the interactions of known drugs with those targets. In most settings, this has meant providing complete data for the subset of known drugs in order to train the model (i.e., values for many complete columns of the drug-target matrix); the assumption was that one was going to do no new experiments but simply try to predict their outcomes from a large body of comprehensive data. Recent results for the setting in which the training data is non-uniformly distributed over the drug-target matrix has been done [12,13]. The fourth component is active learning engines. Active learning has been studied in many contexts and for a number of different criteria for choosing experiments. However, the vast majority of this work has been retrospective: a large, complete dataset is ‘hidden’ from the active learner and individual data points are revealed upon request. This setting enables the calculation of the accuracy of the model at any point in the active learning process because all of the data is actually available. This setting does not apply to any real-world application in which the point is to avoid collecting all of the data. Additional work is needed on approaches for estimating the accuracy of an actively-learned model so that we can know when the model is good enough to stop doing acquisition Conclusion: Automation is the future of science and engineering. It will dramatically reduce the costs of discovery and development, while increasing throughput and reproducibility. More importantly, the use of automated model building and experiment selection via active learning will overcome the limits of the human mind, when it comes to reasoning about complex systems and the data they produce Figure 1. The Active Learning Cycle. The key is to iteratively select and execute experiments based on the current predictive model. Note that this is not the traditional “systems biology” approach that focuses on constructing a predictive model using data from a very large set of experiments and then trying to “prove” the model by doing selected additional Murphy, Kangas and Langmead White Paper: Tools Needed for Automating Science 3 experiments to verify high-confidence predictions. Such approaches ignore the fact that it is impossible to prove empirical models, and that the most appropriate use of new data is to improve a model! Note also that this different from trying to predict everything in silico – the active learning approach optimally combines computational prediction and experimental data acquisition. Figure 2. Components to be developed by the STC. 1) Tools for the experimenter to communicate to an automated system the specifics of what experiments are allowed (e.g., which cell lines and compounds may be chosen from) and how to perform them. 2) Tools for processing measurements (e.g., image analysis). These are specific to each type of study. 3) Tools for converting processed data into predictive models. This uses traditional machine learning methods or system identification methods, depending on the study. 4) Active learning engines. Most past work has been retrospective: a large, complete dataset is ‘hidden’ from the active learner and individual data points are revealed upon request. The STC will demonstrate the utility in real-world, prospective settings. Figure 3. Active Learning Examples. In a retrospective study of drug effects, 57% of active compounds were discovered with only 2.5% of possible experiments [13]. In a prospective study, a 92% accurate model of complex phenotypes was obtained after only 28% of possible experiments [14]. Murphy, Kangas and Langmead White Paper: Tools Needed for Automating Science",,2017,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5ba9d06b091074bf7ea3c3dcacf6abbb48d2d4af,https://www.semanticscholar.org/paper/5ba9d06b091074bf7ea3c3dcacf6abbb48d2d4af,Self-Service Data Science in Healthcare with Automated Machine Learning,"(1) Background: This work investigates whether and how researcher-physicians can be supported in their knowledge discovery process by employing Automated Machine Learning (AutoML). (2) Methods: We take a design science research approach and select the Tree-based Pipeline Optimization Tool (TPOT) as the AutoML method based on a benchmark test and requirements from researcher-physicians. We then integrate TPOT into two artefacts: a web application and a notebook. We evaluate these artefacts with researcher-physicians to examine which approach suits researcher-physicians best. Both artefacts have a similar workflow, but different user interfaces because of a conflict in requirements. (3) Results: Artefact A, a web application, was perceived as better for uploading a dataset and comparing results. Artefact B, a Jupyter notebook, was perceived as better regarding the workflow and being in control of model construction. (4) Conclusions: Thus, a hybrid artefact would be best for researcher-physicians. However, both artefacts missed model explainability and an explanation of variable importance for their created models. Hence, deployment of AutoML technologies in healthcare remains currently limited to the exploratory data analysis phase.",,2020,10.3390/app10092992,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c7330fdaef1cba1bd54be4421901677dc379c41c,https://www.semanticscholar.org/paper/c7330fdaef1cba1bd54be4421901677dc379c41c,Barefoot Rover: a Sensor-Embedded Rover Wheel Demonstrating In-Situ Engineering and Science Extractions using Machine Learning,"In this work, we demonstrate an instrumented wheel concept which utilizes a 2D pressure grid, an electrochemical impedance spectroscopy (EIS) sensor and machine learning (ML) to extract meaningful metrics from the interaction between the wheel and surface terrain. These include continuous slip/skid estimation, balance, and sharpness for engineering applications. Estimates of surface hydration, texture, terrain patterns, and regolith physical properties such as cohesion and angle of internal friction are additionally calculated for science applications. Traditional systems rely on post-processing of visual images and vehicle telemetry to estimate these metrics. Through in-situ sensing, these metrics can be calculated in near real time and made available to onboard science and engineering autonomy applications. This work aims to provide a deployable system for future planetary exploration missions to increase science and engineering capabilities through increased knowledge of the terrain.",2020 IEEE International Conference on Robotics and Automation (ICRA),2020,10.1109/ICRA40945.2020.9197500,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
09a7eb1c08d47442e8434f6abaaf19b2c50f1063,https://www.semanticscholar.org/paper/09a7eb1c08d47442e8434f6abaaf19b2c50f1063,A Machine Learning Approach for Heart Attack Prediction,"124 Published By: Blue Eyes Intelligence Engineering and Sciences Publication © Copyright: All rights reserved. Retrieval Number: 100.1/ijeat.F30430810621 DOI:10.35940/ijeat.F3043.0810621 Journal Website: www.ijeat.org Abstract: A heart attack also known as cardiac arrest, diversify various conditions impacting the heart and became one of the chief-reason for death worldwide over the last few decades. Approximately, 31% of total deaths globally are due to CVDs. It constitutes the pinnacle of chronic processes which involve complex interactions between risk factors which can and cannot be improved. Most of the instances or cases of cardiovascular diseases can be allocated to revisable risk factors where most of the instances are considered preventable. ML became the enhancing approach for the evolution of predictive models in health care industries and was decided to test various algorithms to check what extent their prediction scores estimate or ameliorate upon the results acquired. Researchers deploy various machine learning and data mining techniques over a set of enormous data of cardiovascular patients to attain the prediction for heart attacks before their occurrence for helping healthcare industries and professionals. This research comprises various Supervised ML classifiers like, Gradient Boosting, Decision Tree, Random Forest and Logistic Regression that have been used to deploy a model for Myocardial Infarction prediction. It uses the existing datasets from the Framingham database and others from the database of the UCI Heart repository. This research intends to ideate the prediction for probabilities of occurrence of a heart attack in the patients. These classifiers have been deployed in pipeline approach of machine learning to attain the prediction using both ways i.e., without optimizations and feature transformations as well as vice-versa. The results impersonate that the Gradient Boosting classifier is achieving the highest accuracy score in such a way that prediction used by our model is of binary form in where 1 means a chance of heart attack and 0 means no chance. Some of the most influential attributes are chest pain type among which the typical angina is the most influential and asymptotic chest pain is least, cholesterol level in which the level greater than 200mg/dl are more prone, increased heart rate, thal, and age. It is concluded that premature heart attack is preventable in 80% of the total cases just by using a healthy diet along with regular exercises and not using tobacco products also the person who drinks more than 5 glasses of water daily are less likely to develop attacks.",,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6bce2e822dae71a7e85ec1d8cc86f2180451d528,https://www.semanticscholar.org/paper/6bce2e822dae71a7e85ec1d8cc86f2180451d528,"AI, Machine Learning, and International Criminal Investigations: The Lessons From Forensic Science","The evolving field of machine learning and artificial intelligence is frequently presented as a positively disruptive branch of data science whose expansion allows for improvements in the speed, efficiency, and reliability of decision-making, and whose potential is impacting across diverse zones of human activity. A particular focus for development is within the criminal justice sector, and more particularly the field of international criminal justice, where AI is presented as a means to filter evidence from digital media, to perform visual analyses of satellite data, or to conduct textual analyses of judicial reporting datasets. Nonetheless, for all of its myriad potentials, the deployment of forensic machine learning and AI may also generate seemingly insoluble challenges. The critical discourse attendant upon the expansion of automated decision-making, and its social and legal consequences, resolves around two interpenetrating issues; specifically, algorithmic bias, and algorithmic opacity, the latter phenomena being the focus of this study. It is posited that the seemingly intractable evidential challenges associated with the introduction of opaque computational machine learning algorithms, though global in nature, are neither novel nor unfamiliar. Indeed, throughout the past decade and across a multitude of jurisdictions, criminal justice systems have been required to respond to the implementation of opaque forensic algorithms, particularly in relation to complex DNA mixture analysis. Therefore, with the objective of highlighting the potential avenues of challenge which may follow from the introduction of forensic AI, this study focusses on the prior experience of litigating, and regulating, probabilistic genotyping algorithms within the forensic science and criminal justice fields. Crucially, the study proposes that machine learning opacity constitutes an enhanced form of algorithmic opacity. Therefore, the challenges to rational fact-finding generated through the use of probabilistic genotyping software may be encountered anew, and exacerbated, through the introduction of forensic AI. In anticipating these challenges, the paper explores the distinct categories of opacity, and suggests collaborative solutions which may empower contemporary legal academics – and both legal and forensic practitioners - to set more rigorous and usable standards. The paper concludes by considering the ways in which academics, forensic scientists, and legal practitioners, particularly those working in the field of international criminal justice, might re-conceptualize these opaque technologies, opening a new field of critique and analysis. Using findings from case analyses, overarching regulatory guidance, and data drawn from empirical research interviews, this article addresses the validity, transparency, and interpretability problems, leading to a comprehensive assessment of the current challenges facing the introduction of forensic AI. It builds upon work undertaken at the Nuffield Council on Bioethics Horizon Scanning Workshop: The future of science in crime and security (5th July 2019, London).",,2020,10.2139/SSRN.3727899,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2dfe0d7617d8c34d9fbdc7689b1cc6f7bb1b9f8a,https://www.semanticscholar.org/paper/2dfe0d7617d8c34d9fbdc7689b1cc6f7bb1b9f8a,COVID-19 Mortality Prediction among Patients using Epidemiological parameters: An Ensemble Machine Learning Approach,"Coronavirus infection (COVID-19) is a dangerous disease caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) that has quickly spread all around the world, becoming a global pandemic on 11th March 2020. Vaccines have been developed to prevent the spread of this disease and various researches are being conducted to find the cure too. Machine learning (ML) has shown to be useful in battling COVID-19 and various applications have been deployed to comprehend real-world events through the meticulous analysis of data. In this study, we perform a retrospective study of epidemiological parameters to predict the mortality among SARS-CoV-2 patients. The goal of this research is to find important predictive parameters that can indicate the patients who are at the highest risk of death. Supervised ensemble machine learning models were developed that included random forest, catboost, adaboost, gradient boost, extreme gradient boosting and light GBM (Gradient Boosting Machine) for the COVID-19 epidemiology dataset that was obtained from Mexico. Prior to creating the models, Pearson’s co-relation and mutual information analysis between various dependent and independent features were used to establish the strength of the association between features in the dataset. Extreme Gradient Boosting achieved the highest results with an accuracy of 96%. © Engineered Science Publisher LLC 2021",Engineered Science,2021,10.30919/es8d579,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d14ac2acf1b18e815385c631216eb4ee3a4fc842,https://www.semanticscholar.org/paper/d14ac2acf1b18e815385c631216eb4ee3a4fc842,"Artificial intelligence, machine learning and the evolution of healthcare","vol. 7, No. 3, MaRch 2018 223 First proposed by Professor John Mccarthy at Dartmouth college in the summer of 1956,1 artificial Intelligence (aI) – human intelligence exhibited by machines – has occupied the lexicon of successive generations of computer scientists, science fiction fans, and medical researchers. The aim of countless careers has been to build intelligent machines that can interpret the world as humans do, understand language, and learn from realworld examples. In the early part of this century, two events coincided that transformed the field of aI. The advent of widely available Graphic Processing Units (GPUs) meant that parallel processing was faster, cheaper, and more powerful. at the same time, the era of ‘Big Data’ – images, text, bioinformatics, medical records, and financial transactions, among others – was moving firmly into the mainstream, along with almost limitless data storage. These factors led to a dramatic resurgence in interest in aI in both academic circles and industries outside traditional computer science. once again, aI occupies the zeitgeist, and is poised to transform medicine at a basic science, clinical, healthcare management, and financial level. Terminology surrounding these technologies continues to evolve and can be a source of confusion for non-computer scientists. aI is broadly classified as: general aI, machines that replicate human thought, emotion, and reason (and remain, for now, in the realm of science fiction); and narrow aI, technologies that can perform specific tasks as well as, or better than, humans. Machine learning (Ml) is the study of computer algorithms that can learn complex relationships or patterns from empirical data and make accurate decisions.2 Rather than coding specific sets of instructions to accomplish a task, the machine is ‘trained’ using large amounts of data and algorithms that confer it the ability to learn how to perform the task. Unlike normal algorithms, it is the data that ‘tells’ the machine what the ‘good answer’ is, and learning occurs without explicit programming. Ml problems can be classified as supervised learning or unsupervised learning.3 In a supervised machine learning algorithm, such as face recognition, the machine is shown several examples of ‘face’ or ‘non-face’ and the algorithm learns to predict whether an unseen image is a face or not. In unsupervised learning, the images shown to the machine are not labelled as ‘face’ or ‘non-face’. artificial Neural Networks (aNN)4 are one group of algorithms used for machine learning. While aNNs have existed for over 60 years, they fell out of favour during the 1990s and 2000s. In the last half-decade, aNNs have had a resurgence under a new name: deep artificial networks (or ‘Deep learning’). aNNs are uniquely poised to take full advantage of the computational boost offered by GPUs, allowing them to crunch through data sets of enormous sizes. These range from computer vision tasks, such as image classification, object detection, face recognition, and optical character recognition (ocR), to natural language processing and even gameplaying problems (from mastering simple atari games to the recent alphaGo victory against human grandmasters).5 aNNs work by constructing layers upon layers of simple processing units (often referred to as ‘neurons’), interconnected via many differentially weighted connections. aNNs are ‘trained’ by using backpropagation algorithms, essentially telling the machine how to alter the internal parameters that are used to compute the representation in each layer from the representation in the previous Artificial intelligence, machine learning and the evolution of healthcare",Bone & joint research,2018,10.1302/2046-3758.73.BJR-2017-0147.R1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
81cd45d9dd13e9c0a15b076cfd97f7d0768cf238,https://www.semanticscholar.org/paper/81cd45d9dd13e9c0a15b076cfd97f7d0768cf238,Credit Card Fraud Detection using Machine Learning and Data Science,"It is vital that credit card companies are able to identify fraudulent credit card transactions so that customers are not charged for items that they did not purchase. Such problems can be tackled with Data Science and its importance, along with Machine Learning, cannot be overstated. This project intends to illustrate the modelling of a data set using machine learning with Credit Card Fraud Detection. The Credit Card Fraud Detection Problem includes modelling past credit card transactions with the data of the ones that turned out to be fraud. This model is then used to recognize whether a new transaction is fraudulent or not. Our objective here is to detect 100% of the fraudulent transactions while minimizing the incorrect fraud classifications. Credit Card Fraud Detection is a typical sample of classification. In this process, we have focused on analysing and pre-processing data sets as well as the deployment of multiple anomaly detection algorithms such as Local Outlier Factor and Isolation Forest algorithm on the PCA transformed Credit Card Transaction data. Keywords— Credit card fraud, applications of machine learning, data science, isolation forest algorithm, local outlier factor, automated fraud detection.",International Journal of Engineering Research and,2019,10.17577/ijertv8is090031,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5509ef863628664dccf49d424250d7ff3a173bf4,https://www.semanticscholar.org/paper/5509ef863628664dccf49d424250d7ff3a173bf4,Challenges to the Reproducibility of Machine Learning Models in Health Care.,"Reproducibility has been an important and intensely debated topic in science and medicine for the past few decades.1 As the scientific enterprise has grown in scope and complexity, concerns regarding how well new findings can be reproduced and validated across different scientific teams and study populations have emerged. In some instances,2 the failure to replicate numerous previous studies has added to the growing concern that science and biomedicine may be in the midst of a “reproducibility crisis.” Against this backdrop, high-capacity machine learning models are beginning to demonstrate early successes in clinical applications,3 and some have received approval from the US Food and Drug Administration. This new class of clinical prediction tools presents unique challenges and obstacles to reproducibility, which must be carefully considered to ensure that these techniques are valid and deployed safely and effectively. Reproducibility is a minimal prerequisite for the creation of new knowledge and scientific progress, but defining precisely what it means for a scientific study to be “reproducible” is complex and has been the subject of considerable effort by both individual researchers and organizations like the National Academies of Science, Engineering, and Medicine. First, it is important to distinguish between the notions of reproducibility and replication. A study is reproducible if, given access to the underlying data and analysis code, an independent group can obtain the same result observed in the original study. However, being reproducible does not imply that a study is correct, only thattheresultswereabletobeverifiedbyadifferentgroup not involved in the original study. A study is replicable if an independent group studying the same phenomenon reaches the same conclusion after performing the same set of experiments or analyses after collecting new data. The discussion around reproducibility and replication has primarily focused on traditional statistical models and the results from randomized clinical trials, but these considerations can and should apply equally to machine learning studies. Challenges to reproducibility and replication include confounding, multiple hypothesis testing, randomness inherent to the analysis procedure, incomplete documentation, and restricted access to the underlying data and code. The last concern, data access, is especially germane for medicine, as privacy barriers are important considerations for data sharing. However, by definition, replication does not require access to the original data or code because a replication exercise examines the extent to which the original phenomenon generalizes to new contexts and new populations. This Viewpoint focuses on reproducibility, even though it is important to acknowledge that replication is often the ultimate goal. Replication is especially important for studies that use observational data (which is almost always the case for machine learning studies) because these dataareoftenbiased,andmodelscouldoperationalizethis bias if not replicated. The challenges of reproducing a machinelearningmodeltrainedbyanotherresearchteamcan be difficult, perhaps even prohibitively so, even with unfettered access to raw data and code.",JAMA,2020,10.1001/jama.2019.20866,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a064706bf3ac79523b40e44c181306d1045e9d2a,https://www.semanticscholar.org/paper/a064706bf3ac79523b40e44c181306d1045e9d2a,Characterizing and Detecting Mismatch in Machine-Learning-Enabled Systems,"Increasing availability of machine learning (ML) frameworks and tools, as well as their promise to improve solutions to data-driven decision problems, has resulted in popularity of using ML techniques in software systems. However, end-to-end development of ML-enabled systems, as well as their seamless deployment and operations, remain a challenge. One reason is that development and deployment of ML-enabled systems involves three distinct workflows, perspectives, and roles, which include data science, software engineering, and operations. These three distinct perspectives, when misaligned due to incorrect assumptions, cause ML mismatches which can result in failed systems. We conducted an interview and survey study where we collected and validated common types of mismatches that occur in end-to-end development of ML-enabled systems. Our analysis shows that how each role prioritizes the importance of relevant mismatches varies, potentially contributing to these mismatched assumptions. In addition, the mismatch categories we identified can be specified as machine readable descriptors contributing to improved ML-enabled system development. In this paper, we report our findings and their implications for improving end-to-end ML-enabled system development.",2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN),2021,10.1109/WAIN52551.2021.00028,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
024b960a1d833e495f0230c3d789571dae7ec0b1,https://www.semanticscholar.org/paper/024b960a1d833e495f0230c3d789571dae7ec0b1,Interpretability of Machine Learning Solutions in Public Healthcare: The CRISP-ML Approach,"Public healthcare has a history of cautious adoption for artificial intelligence (AI) systems. The rapid growth of data collection and linking capabilities combined with the increasing diversity of the data-driven AI techniques, including machine learning (ML), has brought both ubiquitous opportunities for data analytics projects and increased demands for the regulation and accountability of the outcomes of these projects. As a result, the area of interpretability and explainability of ML is gaining significant research momentum. While there has been some progress in the development of ML methods, the methodological side has shown limited progress. This limits the practicality of using ML in the health domain: the issues with explaining the outcomes of ML algorithms to medical practitioners and policy makers in public health has been a recognized obstacle to the broader adoption of data science approaches in this domain. This study builds on the earlier work which introduced CRISP-ML, a methodology that determines the interpretability level required by stakeholders for a successful real-world solution and then helps in achieving it. CRISP-ML was built on the strengths of CRISP-DM, addressing the gaps in handling interpretability. Its application in the Public Healthcare sector follows its successful deployment in a number of recent real-world projects across several industries and fields, including credit risk, insurance, utilities, and sport. This study elaborates on the CRISP-ML methodology on the determination, measurement, and achievement of the necessary level of interpretability of ML solutions in the Public Healthcare sector. It demonstrates how CRISP-ML addressed the problems with data diversity, the unstructured nature of data, and relatively low linkage between diverse data sets in the healthcare domain. The characteristics of the case study, used in the study, are typical for healthcare data, and CRISP-ML managed to deliver on these issues, ensuring the required level of interpretability of the ML solutions discussed in the project. The approach used ensured that interpretability requirements were met, taking into account public healthcare specifics, regulatory requirements, project stakeholders, project objectives, and data characteristics. The study concludes with the three main directions for the development of the presented cross-industry standard process.",Frontiers in Big Data,2021,10.3389/fdata.2021.660206,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3a00bab02bd541ffdf40c40ac8640d855008e898,https://www.semanticscholar.org/paper/3a00bab02bd541ffdf40c40ac8640d855008e898,Applications and Techniques for Fast Machine Learning in Science,"In this community review report, we discuss applications and techniques for fast machine learning (ML) in science—the concept of integrating powerful ML methods into the real-time experimental data processing loop to accelerate scientific discovery. The material for the report builds on two workshops held by the Fast ML for Science community and covers three main areas: applications for fast ML across a number of scientific domains; techniques for training and implementing performant and resource-efficient ML algorithms; and computing architectures, platforms, and technologies for deploying these algorithms. We also present overlapping challenges across the multiple scientific domains where common solutions can be found. This community report is intended to give plenty of examples and inspiration for scientific discovery through integrated and accelerated ML solutions. This is followed by a high-level overview and organization of technical advances, including an abundance of pointers to source material, which can enable these breakthroughs.",Frontiers in Big Data,2021,10.3389/fdata.2022.787421,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7ef96d798b576c75f4665a7ad64fdedd9f803c54,https://www.semanticscholar.org/paper/7ef96d798b576c75f4665a7ad64fdedd9f803c54,GenoML: Automated Machine Learning for Genomics,"GenoML is a Python package automating machine learning workflows for genomics (genetics and multi-omics) with an open science philosophy. Genomics data require significant domain expertise to clean, pre-process, harmonize and perform quality control of the data. Furthermore, tuning, validation, and interpretation involve taking into account the biology and possibly the limitations of the underlying data collection, protocols, and technology. GenoML’s mission is to bring machine learning for genomics and clinical data to non-experts by developing an easy-to-use tool that automates the full development, evaluation, and deployment process. Emphasis is put on open science to make workflows easily accessible, replicable, and transferable within the scientific community. Source code and documentation is available at https://genoml.com.",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ee8d2bce77c65a37b0591c73972e1d52197f2a96,https://www.semanticscholar.org/paper/ee8d2bce77c65a37b0591c73972e1d52197f2a96,A Survey on Optimal Transport for Machine Learning: Theory and Applications,"Optimal Transport (OT) theory has seen an increasing amount of attention from the computer science community due to its potency and relevance in modeling and machine learning. It introduces means that serve as powerful ways to compare probability distributions with each other, as well as producing optimal mappings to minimize cost functions. Therefor, it has been deployed in computer vision, improving image retrieval, image interpolation, and semantic correspondence algorithms, as well as other fields such as domain adaptation, natural language processing, and variational inference. In this survey, we propose to convey the emerging promises of the optimal transport methods across various fields, as well as future directions of study for OT in machine learning. We will begin by looking at the history of optimal transport and introducing the founders of this field. We then give a brief glance into the algorithms related to OT. Then, we will follow up with a mathematical formulation and the prerequisites to understand OT, these include Kantorovich duality, entropic regularization, KL Divergence, and Wassertein barycenters. Since OT is a computationally expensive problem, we then introduce the entropy-regularized version of computing optimal mappings, which allowed OT problems to become applicable in a wide range of machine learning problems. In fact, the methods generated from OT theory are competitive with the current state-of-the-art methods. The last portion of this survey will analyze papers that focus on the application of OT within the context of machine learning. We first cover computer vision problems; these include GANs, semantic correspondence, and convolutional Wasserstein distances. Furthermore, we follow this up by breaking down research papers that focus on graph learning, neural architecture search, document representation, and domain adaptation. We close the paper with a small section on future research. Of the recommendations presented, three main problems are fundamental to allow OT to become widely applicable but rely strongly on its mathematical formulation and thus are hardest to answer. Since OT is a novel method, there is plenty of space for new research, and with more and more competitive methods (either on an accuracy level or computational speed level) being created, the future of applied optimal transport is bright as it has become pervasive in machine learning.",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9cd80a501ea586086f614f08667e7ed7533a1204,https://www.semanticscholar.org/paper/9cd80a501ea586086f614f08667e7ed7533a1204,From Machine Learning to Robotics: Challenges and Opportunities for Embodied Intelligence,"Machine learning has long since become a keystone technology, accelerating science and applications in a broad range of domains. Consequently, the notion of applying learning methods to a particular problem set has become an established and valuable modus operandi to advance a particular field. In this article we argue that such an approach does not straightforwardly extended to robotics — or to embodied intelligence more generally: systems which engage in a purposeful exchange of energy and information with a physical environment. In particular, the purview of embodied intelligent agents extends significantly beyond the typical considerations of main-stream machine learning approaches, which typically (i) do not consider operation under conditions significantly different from those encountered during training; (ii) do not consider the often substantial, long-lasting and potentially safety-critical nature of interactions during learning and deployment; (iii) do not require ready adaptation to novel tasks while at the same time (iv) effectively and efficiently curating and extending their models of the world through targeted and deliberate actions. In reality, therefore, these limitations result in learning-based systems which suffer from many of the same operational shortcomings as more traditional, engineering-based approaches when deployed on a robot outside a well defined, and often narrow operating envelope. Contrary to viewing embodied intelligence as another application domain for machine learning, here we argue that it is in fact a key driver for the advancement of machine learning technology. In this article our goal is to highlight challenges and opportunities that are specific to embodied intelligence and to propose research directions which may significantly advance the state-of-the-art in robot learning.",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a4f60886534256904a8d8850ecbd32d9a18a8fbc,https://www.semanticscholar.org/paper/a4f60886534256904a8d8850ecbd32d9a18a8fbc,Machine Learning Applied to the Analysis of Nonlinear Beam Dynamics Simulations for the CERN Large Hadron Collider and Its Luminosity Upgrade,"A Machine Learning approach to scientific problems has been in use in Science and Engineering for decades. High-energy physics provided a natural domain of application of Machine Learning, profiting from these powerful tools for the advanced analysis of data from particle colliders. However, Machine Learning has been applied to Accelerator Physics only recently, with several laboratories worldwide deploying intense efforts in this domain. At CERN, Machine Learning techniques have been applied to beam dynamics studies related to the Large Hadron Collider and its luminosity upgrade, in domains including beam measurements and machine performance optimization. In this paper, the recent applications of Machine Learning to the analyses of numerical simulations of nonlinear beam dynamics are presented and discussed in detail. The key concept of dynamic aperture provides a number of topics that have been selected to probe Machine Learning. Indeed, the research presented here aims to devise efficient algorithms to identify outliers and to improve the quality of the fitted models expressing the time evolution of the dynamic aperture.",Inf.,2021,10.3390/info12020053,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2ff8ef80f664e39cdb5bfd4e14a095919d904a47,https://www.semanticscholar.org/paper/2ff8ef80f664e39cdb5bfd4e14a095919d904a47,"Machine learning analysis: general features, requirements and cardiovascular applications.","Artificial intelligence represents the science which will probably change the future of medicine by solving actually challenging issues. In this special article, the general features of machine learning are discussed. First, a background explanation regarding the division of artificial intelligence, machine learning and deep learning is given and a focus on the structure of machine learning subgroups is shown. The traditional process of a machine learning analysis is described, starting from the collection of data, across features engineering, modelling and till the validation and deployment phase. Due to the several applications of machine learning performed in literature in the last decades and the lack of some guidelines, the need of a standardization for reporting machine learning analysis results emerged. Some possible standards for reporting machine learning results are identified and discussed deeply; these are related to study population (number of subjects), repeatability of the analysis, validation, results, comparison with current practice. The way to the use of machine learning in clinical practice is open and the hope is that, with emerging technology and advanced digital and computational tools, available from hospitalization and subsequently after discharge, it will also be possible, with the help of increasingly powerful hardware, to build assistance strategies useful in clinical practice.",Minerva cardiology and angiology,2021,10.23736/S2724-5683.21.05637-4,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0377dba80a49c35f8b3678e60833173f8c4ba81f,https://www.semanticscholar.org/paper/0377dba80a49c35f8b3678e60833173f8c4ba81f,Supervised machine learning based liver disease prediction approach with LASSO feature selection,"In this contemporary era, the uses of machine learning techniques are increasing rapidly in the field of medical science for detecting various diseases such as liver disease (LD). Around the globe, a large number of people die because of this deadly disease. By diagnosing the disease in a primary stage, early treatment can be helpful to cure the patient. In this research paper, a method is proposed to diagnose the LD using supervised machine learning classification algorithms, namely logistic regression, decision tree, random forest, AdaBoost, KNN, linear discriminant analysis, gradient boosting and support vector machine (SVM). We also deployed a least absolute shrinkage and selection operator (LASSO) feature selection technique on our taken dataset to suggest the most highly correlated attributes of LD. The predictions with 10 fold cross-validation (CV) made by the algorithms are tested in terms of accuracy, sensitivity, precision and f1-score values to forecast the disease. It is observed that the decision tree algorithm has the best performance score where accuracy, precision, sensitivity and f1-score values are 94.295%, 92%, 99% and 96% respectively with the inclusion of LASSO. Furthermore, a comparison with recent studies is shown to prove the significance of the proposed system. ",Bulletin of Electrical Engineering and Informatics,2021,10.11591/EEI.V10I6.3242,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
38be58a9f51ed2af7e9e75793c38529de8aa5267,https://www.semanticscholar.org/paper/38be58a9f51ed2af7e9e75793c38529de8aa5267,A Systematic Review on the Application of Machine Learning in Exploiting Mineralogical Data in Mining and Mineral Industry,"Machine learning is a subcategory of artificial intelligence, which aims to make computers capable of solving complex problems without being explicitly programmed. Availability of large datasets, development of effective algorithms, and access to the powerful computers have resulted in the unprecedented success of machine learning in recent years. This powerful tool has been employed in a plethora of science and engineering domains including mining and minerals industry. Considering the ever-increasing global demand for raw materials, complexities of the geological structure of ore deposits, and decreasing ore grade, high-quality and extensive mineralogical information is required. Comprehensive analyses of such invaluable information call for advanced and powerful techniques including machine learning. This paper presents a systematic review of the efforts that have been dedicated to the development of machine learning-based solutions for better utilizing mineralogical data in mining and mineral studies. To that end, we investigate the main reasons behind the superiority of machine learning in the relevant literature, machine learning algorithms that have been deployed, input data, concerned outputs, as well as the general trends in the subject area.",Minerals,2021,10.3390/min11080816,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b0c2e2723b7f74bc6125431f8b0bd8b48b41d400,https://www.semanticscholar.org/paper/b0c2e2723b7f74bc6125431f8b0bd8b48b41d400,Recent Advances in Computer-Aided Medical Diagnosis Using Machine Learning Algorithms With Optimization Techniques,"Artificial intelligence is a spectacular part of computer engineering that has earned a compelling diversion in the field of medical data classification due to its state-of-art algorithmic strength and learning capabilities. Machine Learning is a major sub-domain of artificial intelligence, where it has become one of the most promising fields in computer science. In recent years, there is a large spectrum of healthcare and biomedical data that has been growing intensely. Due to the huge labeled or unlabeled data, it is important to have a compact and robust machine learning solution for classification. Several optimizers have been deployed to improve the inclusive performance of machine learning models. The classification of machine learning models depends on several factors. This comprehensive review paper aims to insight into the current stage of optimized machine learning success on medical data classification. An increasing number of unstructured medical data has been utilizing in machine learning algorithms to predict intuitions. But it is difficult to inherent immense intuition from those data. So machine learning researchers have utilized state-of-art optimizers and novel feature selection techniques to overcome and emend the performance accuracy. We have highlighted some recent literature, which exhibits the robust impact of optimizers and feature selection on machine learning techniques on medical data characterization. On the other hand, a clean-cut introduction on machine learning and theoretical outlook of widely utilized optimization techniques like genetic algorithm, gray wolf optimization, and particle swarm optimization are discussed for initial understanding of the optimization techniques.",IEEE Access,2021,10.1109/access.2021.3108892,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1c5127b6d9fd6e968680ead11860fe8015b33f79,https://www.semanticscholar.org/paper/1c5127b6d9fd6e968680ead11860fe8015b33f79,FairRover: explorative model building for fair and responsible machine learning,"The potential harms and drawbacks of automated decision making has become a challenge as data science blends into our lives. In particular, fairness issues with deployed machine learning models have drawn significant attention from the research community. Despite the myriad of algorithmic fairness work in various research communities, in practice data scientists still face many roadblocks in ensuring the fairness of their machine learning models. This is primarily because there does not exist an end-to-end system that guides the users in building a fair machine learning model in a responsible way from model auditing, to model explanation, to bias mitigation. We propose a explorative model building system FairRover for responsible fair model building. FairRover guides users in (1) discovering the potential biases in the model; (2) providing explanation to the discovered biases so as to help users in understanding potential causes of the biases; and (3) mitigating the most important biases selected by the users. Because of the impossibility theorem of fairness, and the well-known trade-off between fairness and accuracy, it is generally impossible to achieve a completely fair and accurate machine learning model. Therefore, this responsible model building process is naturally performed iteratively until a satisfying trade-off is reached. Human users are involved in the loop to make various decisions guided by FairRover. We demonstrate a case study on the Adult Census dataset, which shows how FairRover guides users in iteratively building a fair income prediction model in a responsible way. We discuss the current limitations of FairRover and future work.",DEEM@SIGMOD,2021,10.1145/3462462.3468882,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d706165f1d3d176ae80f2f4380980ae20cc327f8,https://www.semanticscholar.org/paper/d706165f1d3d176ae80f2f4380980ae20cc327f8,Machine learning-guided design and development of metallic structural materials,"In recent years, the advent of machine learning (ML) in materials science has provided a new tool for accelerating the design and discovery of new materials with a superior combination of mechanical properties for structural applications. In this review, we provide a brief overview of the current status of the ML-aided design and development of metallic alloys for structural applications, including high-performance copper alloys, nickel- and cobalt-based superalloys, titanium alloys for biomedical applications and high strength steel. We also present our perspectives regarding the further acceleration of data-driven discovery, development, design and deployment of metallic structural materials and the adoption of ML-based techniques in this endeavor.",Journal of Materials Informatics,2021,10.20517/jmi.2021.08,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5c0a5844f81579f385f12597bd9371b0a0bc3caf,https://www.semanticscholar.org/paper/5c0a5844f81579f385f12597bd9371b0a0bc3caf,Active Annotation in Evaluating the Credibility of Web-Based Medical Information: Guidelines for Creating Training Data Sets for Machine Learning,"Background The spread of false medical information on the web is rapidly accelerating. Establishing the credibility of web-based medical information has become a pressing necessity. Machine learning offers a solution that, when properly deployed, can be an effective tool in fighting medical misinformation on the web. Objective The aim of this study is to present a comprehensive framework for designing and curating machine learning training data sets for web-based medical information credibility assessment. We show how to construct the annotation process. Our main objective is to support researchers from the medical and computer science communities. We offer guidelines on the preparation of data sets for machine learning models that can fight medical misinformation. Methods We begin by providing the annotation protocol for medical experts involved in medical sentence credibility evaluation. The protocol is based on a qualitative study of our experimental data. To address the problem of insufficient initial labels, we propose a preprocessing pipeline for the batch of sentences to be assessed. It consists of representation learning, clustering, and reranking. We call this process active annotation. Results We collected more than 10,000 annotations of statements related to selected medical subjects (psychiatry, cholesterol, autism, antibiotics, vaccines, steroids, birth methods, and food allergy testing) for less than US $7000 by employing 9 highly qualified annotators (certified medical professionals), and we release this data set to the general public. We developed an active annotation framework for more efficient annotation of noncredible medical statements. The application of qualitative analysis resulted in a better annotation protocol for our future efforts in data set creation. Conclusions The results of the qualitative analysis support our claims of the efficacy of the presented method.",JMIR medical informatics,2021,10.2196/26065,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d730457840d4790864a9bf775b11074a69033f95,https://www.semanticscholar.org/paper/d730457840d4790864a9bf775b11074a69033f95,Beauty in Machine Learning: Fluency and Leaps,"The extrapolative leaps from training to deployment are precisely where ML’s best methods for system development succeed, and where most—and the most consequential—failures occur. I argue that the leap is where beauty can and does play a role. Beauty also serves to highlight some important aspects of the leap that are not speciﬁc to beauty. Based on this, and drawing on research on beauty from psychology, cognitive science, and philosophy of science, I articulate some fundamental problems and suggest directions for potential solutions. Should beauty play a role in machine learning (ML) research? If so, how? In many scientiﬁc ﬁelds, particularly physics, beauty is valued. The mathematician and physicist Hermann Weyl famously said, “My work always tried to unite the true with the beautiful; but when I had to choose one or the other, I usually chose the beautiful” [1]. The physicist Steven Weinberg wrote, “The physicist’s sense of beauty is also supposed to serve a purpose—it is supposed to help the physicist select ideas that help us to explain nature” [2]. These ideas are intuitively appealing and have helped in the development of successful theories, but putting them into practice is challenging. Beauty has been used to describe different things by different scientists. There is also the risk of conﬂating it with truth and using it as part of a non-empirical “logic of justiﬁcation,” or reason to believe a theory without empirical evidence [3, 4]. For example, the physicist Sabine Hossenfelder has argued at length that an over-emphasis on beauty (particularly the elegance of string theory) was largely responsible for a stagnation in the foundations of physics, making much of it “post-empirical” [5, 6]. So beauty might be a good guide to individual scientists for developing novel theories, but a theory’s beauty should not be its primary justiﬁcation, particularly for the continued efforts of large groups of researchers. Ultimately, it must be reconciled with empiricism. Is the situation the same in ML? For",,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b1d831ae4dd4ccd6415c28052fdb24b86a7c4800,https://www.semanticscholar.org/paper/b1d831ae4dd4ccd6415c28052fdb24b86a7c4800,A Full-Stack Machine Learning Environment for Rapidly Evolving Industry Applications,"Developing, deploying, and maintaining machine learning models is a key function of many data science teams. We describe a framework built by American Family Insurance to model the risk profiles of properties. Through empirical experiments, we demonstrate that our automated, end-to-end framework provides a rapid platform for experimentation and productionalization in a business environment.",2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA),2021,10.1109/DSAA53316.2021.9564174,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
978437538d70075676f7ca0ceaab3bb685872d25,https://www.semanticscholar.org/paper/978437538d70075676f7ca0ceaab3bb685872d25,Application of Artificial Intelligence and Machine Learning to Detect Drilling Anomalies Leading to Stuck Pipe Incidents,"
 This project used predictive analytics and machine learning-based modeling to detect drilling anomalies, namely stuck pipe events. Analysis focused on historical drilling data and real-time operational data to address the limitations of physics-based modeling. This project was designed to enable drilling crews to minimize downtime and non-productive time through real-time anomaly management.
 The solution used data science techniques to overcome data consistency/quality issues and flag drilling anomalies leading to a stuck pipe event. Predictive machine learning models were deployed across seven wells in different fields. The models analyzed both historical and real-time data across various data channels to identify anomalies (difficulties that impact non-productive time). The modeling approach mimicked the behavior of drillers using surface parameters. Small deviations from normal behavior were identified based on combinations of surface parameters, and automated machine learning was used to accelerate and optimize the modeling process. The output was a risk score that flags deviations in rig surface parameters.
 During the development phase, multiple data science approaches were attempted to monitor the overall health of the drilling process. They analyzed both historical and real-time data from torque, hole depth and deviation, standpipe pressure, and various other data channels.
 The models detected drilling anomalies with a harmonic model accuracy of 80% and produced valid alerts on 96% of stuck pipe and tight hole events. The average forewarning was two hours. This allowed personnel ample time to make corrections before stuck pipe events could occur. This also enabled the drilling operator to save the company upwards of millions of dollars in drilling costs and downtime.
 This project introduced novel data aggregation and deep learning-based normal behavior modeling methods. It demonstrates the benefits of adopting predictive analytics and machine learning in drilling operations. The approach enabled operators to mitigate data issues and demonstrate real-time, high-frequency and high-accuracy predictions. As a result, the operator was able to significantly reduce non-productive time.","Day 4 Thu, November 18, 2021",2021,10.2118/207987-ms,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d946e7914cf039c05d418fed677da6a0cb94cfec,https://www.semanticscholar.org/paper/d946e7914cf039c05d418fed677da6a0cb94cfec,A Performance Characterization of Scientific Machine Learning Workflows,"Scientific workflows are one of the well-established pillars of modern large-scale computational science. More recently, scientists have started to leverage machine learning (ML) capabilities in their workflows, leading to a new category of scientific workflows, denoted as scientific ML workflows. ML is not only about training and inference, modern ML workflows also involve complex data processing steps before the training can start, which are not often accounted for in most performance studies. In this work, we consider scientific ML workflows, from data pre-processing to training, inference, and model evaluation. We aim to explore (i) how scientific ML workflows differ from more traditional scientific workflows and; (ii) how we can characterize ML workflows both in terms of execution time and data movements when executing on a target cloud platform. We select three representative workflows, ranging from image classification to natural language processing and image segmentation, which have been executed using the academic cloud platform, Chameleon. We build four realistic deployment scenarios for each workflow, which stress data movements during workflow executions. Then, we compare the performance observed when utilizing these different configurations and study how different settings impact overall workflows performance and efficiency when running on cloud infrastructures. Finally, we summarize our findings and discuss performance impacts when augmenting scientific workflows with ML techniques and how traditional workflow management systems can improve their support for such workflows.",2021 IEEE Workshop on Workflows in Support of Large-Scale Science (WORKS),2021,10.1109/WORKS54523.2021.00013,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6263c516e26af6164cd3b8e2f6abc823d2eca6be,https://www.semanticscholar.org/paper/6263c516e26af6164cd3b8e2f6abc823d2eca6be,CREDIT CARD FRAUD DETECTION USING MACHINE LEARNING,"Credit card fraud events take place frequently and then result in huge financial losses. The number of online transactions has grown in large quantities and online credit card transactions hold a huge share of these transactions. It is vital that credit card companies are able to identify fraudulent credit card transactions so that customers are not charged for items that they did not purchase. Such problems can be tackled with Data Science and its importance, along with Machine Learning, cannot be overstated. This project intends to illustrate the modelling of a data set using machine learning with Credit Card Fraud Detection. The Credit Card Fraud Detection Problem includes modelling past credit card transactions with the data of the ones that turned out to be fraud. Credit Card Fraud Detection is a typical sample of classification. In this process, we have focused on analysing and pre-processing data sets as well as the deployment of multiple anomaly detection algorithms such as Local Outlier Factor and Isolation Forest algorithm and classification algorithms such as Random Forest on the PCA transformed Credit Card Transaction data.",,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fef41f32ea74058d5e67b2cfddf26dba92010f5e,https://www.semanticscholar.org/paper/fef41f32ea74058d5e67b2cfddf26dba92010f5e,Crowdsourcing Artificial Intelligence in Africa: Findings from a Machine Learning Contest,"In this paper, we study the crowdsourcing of innovation in Africa through a data science contest on an intermediated digital platform. We ran a Machine Learning (ML) contest on the continent's largest data science contest platform, Zindi. Contestants were surveyed on their motivations to take part and their perceptions about AI in Africa. In total, 614 contestants submitted 15,832 entries, and 559 responded to the accompanying survey. From the findings, we answered several questions: who take part in these contests and why? Who is most likely to win? What are contestants' entrepreneurial aspirations in deploying AI? What are the obstacles they perceive to the greater diffusion of AI in Africa? We conclude that crowdsourcing of AI via data contest platforms offers a potential mechanism to alleviate some of the constraints in the adoption and diffusion of AI in Africa. Recommendations for further research are made.",SSRN Electronic Journal,2021,10.2139/ssrn.3892573,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fb7caddac20dca012f48c90b2e1e2383f7185051,https://www.semanticscholar.org/paper/fb7caddac20dca012f48c90b2e1e2383f7185051,Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning,"Machine learning (ML) models are now routinely deployed in domains ranging from criminal justice to healthcare. With this newfound ubiquity, ML has moved beyond academia and grown into an engineering discipline. To that end, interpretability tools have been designed to help data scientists and machine learning practitioners better understand how ML models work. However, there has been little evaluation of the extent to which these tools achieve this goal. We study data scientists' use of two existing interpretability tools, the InterpretML implementation of GAMs and the SHAP Python package. We conduct a contextual inquiry (N=11) and a survey (N=197) of data scientists to observe how they use interpretability tools to uncover common issues that arise when building and evaluating ML models. Our results indicate that data scientists over-trust and misuse interpretability tools. Furthermore, few of our participants were able to accurately describe the visualizations output by these tools. We highlight qualitative themes for data scientists' mental models of interpretability tools. We conclude with implications for researchers and tool designers, and contextualize our findings in the social science literature.",CHI,2020,10.1145/3313831.3376219,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c6bde5c3b6b5bd9cc4c9de2be3a3c4ed48fe4e86,https://www.semanticscholar.org/paper/c6bde5c3b6b5bd9cc4c9de2be3a3c4ed48fe4e86,"Tambe , Developing the Science and Applications of Security Games : Machine Learning , Uncertainty and Preference Elicitation in Game Theory for Security","Having successfully founded the research area of security games, which has led to real-world applications in scheduling the deployment of limited resources (patrols, checkpoints, inspections, etc.), we now provide fundamental advances by incorporating machine learning to enhance realworld security applications, new models of opportunistic security games, robust methods for handling uncertainty, and novel techniques for preference elicitation techniques.",,2015,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c7b08c2e69a338e8d0c8444ce081b51caa50b273,https://www.semanticscholar.org/paper/c7b08c2e69a338e8d0c8444ce081b51caa50b273,Monte Carlo Gradient Estimation in Machine Learning,"This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical developments, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.",J. Mach. Learn. Res.,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7409014be54b9e9b248dac03421068a62ba36e3d,https://www.semanticscholar.org/paper/7409014be54b9e9b248dac03421068a62ba36e3d,An Acquisition Parameter Study for Machine-Learning-Enabled Electron Backscatter Diffraction,"Abstract Methods within the domain of artificial intelligence are gaining traction for solving a range of materials science objectives, notably the use of deep neural networks for computer vision for the analysis of electron diffraction patterns. An important component of deploying these models is an understanding of the performance as experimental diffraction conditions are varied. This knowledge can inspire confidence in the classifications over a range of operating conditions and identify where performance is degraded. Elucidating the relative impact of each parameter will suggest the most important parameters to vary during the collection of future training data. Knowing which data collection efforts to prioritize is of concern given the time required to collect or simulate vast libraries of diffraction patterns for a wide variety of materials without considering varying any parameters. In this work, five parameters, frame averaging, detector tilt, sample-to-detector distance, accelerating voltage, and pattern resolution, essential to electron diffraction are individually varied during the collection of electron backscatter diffraction patterns to explore the effect on the classifications produced by a deep neural network trained from diffraction patterns captured using a fixed set of parameters. The model is shown to be resilient to nearly all the individual changes examined here.",Microscopy and Microanalysis,2021,10.1017/S1431927621000556,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
649024e1a5fa08d209e1b8a4882bcd9e5876061f,https://www.semanticscholar.org/paper/649024e1a5fa08d209e1b8a4882bcd9e5876061f,Editorial: Special Section on Services Computing Management for Artificial Intelligence and Machine Learning,"F IFTEEN years ago, few would have imagined that employees could work entirely remotely or that an entire business infrastructure could exist on the Internet. With the adoption of services computing, a service that allows companies to access processing and data storage through the Internet, these business models are becoming a reality. Services computing requires a multidisciplinary lens that integrates science and technology to bridge the gap between business services and information technology (IT) services [item 1) in the Appendix]. Services computing management involves 1) ensuring services computing strategy which is allied with how the organization manages IT and how IT is aligned with organizational strategy, 2) designing, building, sourcing, and deploying resilient computing solutions, trusted, efficient, and address quality of service (QoS) expectations, and 3) overseeing all matters related to business and IT services operations and resources both across business domains and within domains such as retail, finance, healthcare, logistics, and others [item 2) in the Appendix]. The goal of services computing is to enable IT services and computing technology to perform business services more efficiently and effectively [item 3) in the Appendix]. The pervasive nature of services computing management is exhibited in almost all industry settings [item 4) in the Appendix]. In everyday life, new business service innovations will give rise to an emergent dataand information-focused economy that will only pick up steam as both consumer and business utilization of Internet of Things are advanced. Concomitantly, we are moving toward an era of artificially intelligent (AI) (e.g., cognitive computing) services, which are deployed in multiscale, complex distributed architectures. Cognitive computing is the use of computerized models to simulate the human thought process in complex situations where the answers may be ambiguous and uncertain. Computers are increasingly capable of doing things that humans could once do exclusively. Today smart machines are becoming like humans by recognizing voices, processing natural language, learning, and interacting and learning with the physical world through their vision, smell, touch, and other senses, mobility, and motor control. In some cases, they do a much faster and better job than humans at recognizing patterns, performing rule-based analysis on a very large amount of data, and solving both structured and unstructured problems [item 5) in the Appendix].",IEEE Trans. Engineering Management,2021,10.1109/tem.2020.3024363,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e35b8ad1f54dbdd9d88a0b1dc8869afd72106d62,https://www.semanticscholar.org/paper/e35b8ad1f54dbdd9d88a0b1dc8869afd72106d62,The 1st ACM international workshop on big data and machine learning for smart buildings and cities,"The proliferation of urban sensing, IoT, and big data in buildings, cities, and urban areas provides unprecedented opportunities for a deeper understanding of occupant behavior, transportation, and energy and water usage patterns. However, utilizing the existing data sources and modeling methods in building science to model urban scale occupant behaviors can be pretty challenging. Therefore, technological progress is needed to unlock its full potential. In order to fulfill the latter task, this workshop focuses on the methodologies for big urban and building data collection, analytics, modeling, and real-world technology deployment. The workshop aims to open discussion on the current challenges of big data in smart buildings and cities.","Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",2021,10.1145/3486611.3491139,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
acfc133d91c8313e1a360c5aa07b98951c3bae80,https://www.semanticscholar.org/paper/acfc133d91c8313e1a360c5aa07b98951c3bae80,A Machine Learning Approach for Rate Prediction in Multicast File-stream Distribution Networks,"Large-volume scientific data is one of the prominent driving forces behind next generation networking. In particular, Software Defined Network (SDN) makes leveraging path-based network multicast services practically feasible. In our prior work, we have developed a cross-layer architecture for supporting reliable file-streams multicasting over SDN-enabled Layer-2 network, and implemented the architecture for a meteorology data distribution application in atmospheric science. However, it is challenging to determine an optimal rate for this application with the varying type, volume, and quality of meteorological data. In this paper, we propose a Quality of Service (QoS)-driven rate management pipeline to determine the optimal rate based on the input traffic characteristics and performance constraints. Specifically, the pipeline employs a feedtype classifier using Multi-Layer Perception (MLP) to recognize the type of meteorological data and a delay prediction regressor using stacked Long Short-Term Memory (LSTM) to predict per-file delay for the file-streams. Finally, we determine the optimal rate for the given file-streams using the trained regressor. We implement this pipeline to test the real-world file-stream data collected from a trial deployment, and the results show that our regressor outperforms all baselines by selecting the optimal rate in the presence of varying file set sizes.",2021 IEEE Global Communications Conference (GLOBECOM),2021,10.1109/GLOBECOM46510.2021.9685807,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0ea5760e94a5785f582d96991123e84a2b4be35b,https://www.semanticscholar.org/paper/0ea5760e94a5785f582d96991123e84a2b4be35b,A Novel Realistic Dataset for Intrusion Detection in IoT based on Machine Learning,"The safe deployment of Internet of Things (IoT) devices has emerged as one of the most pressing issues in computer science. The Lossless Low Power Network (RPL) routing protocol is an excellent choice for Internet of Things routing. The ubiquity, connectivity, and low processing capabilities of IoT network devices identify them. These features, along with their exponential growth in recent years (more than 60 billion devices linked to the Internet by the end of 2021), have led in a surge in IoT-based cyber-attacks. To fight against these cyber-attacks, an intrusion detection system (IDS) is required to secure the privacy, availability, and performance of the IoT network. Unfortunately, most public data sets related to IDS, such as UNSW NB15 and KDD CUP99, are incompatible with the IoT network’s unique environment. To solve these issues, we created a data collection framework that includes the recording of network traffic from its unique environment to IoT device needs. The examined dataset’s advantages are tailored to 6LoWPAN/RPL and CoAP, the most widely used protocols for IoT network deployment.","2021 International Symposium on Networks, Computers and Communications (ISNCC)",2021,10.1109/ISNCC52172.2021.9615841,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0f55d8116c458a6aa46c67b1b25c174ec4461d3c,https://www.semanticscholar.org/paper/0f55d8116c458a6aa46c67b1b25c174ec4461d3c,Data Science and Development Team Remote Communication: the use of the Machine Learning Canvas,In this article we present the use of the Machine Learning Canvas as a tool to help communication and project planning between a team of data scientists working in Brazil and a development team distributed in the US. The product being developed is an appliance to be deployed on telecommunication networks and has anomaly detection capabilities.,2019 ACM/IEEE 14th International Conference on Global Software Engineering (ICGSE),2019,10.1109/ICGSE.2019.00018,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6975dd8fab72879873e2a7a948a07f5cdbe7ac1e,https://www.semanticscholar.org/paper/6975dd8fab72879873e2a7a948a07f5cdbe7ac1e,Data Science and Digital Systems: The 3Ds of Machine Learning Systems Design,"Machine learning solutions, in particular those based on deep learning methods, form an underpinning of the current revolution in ""artificial intelligence"" that has dominated popular press headlines and is having a significant influence on the wider tech agenda. Here we give an overview of the 3Ds of ML systems design: Data, Design and Deployment. By considering the 3Ds we can move towards \emph{data first} design.",ArXiv,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7d795919b9da58b96b6dc6a7cf6f47932543cdcf,https://www.semanticscholar.org/paper/7d795919b9da58b96b6dc6a7cf6f47932543cdcf,Machine Learning Lifecycle for Earth Science Application: A Practical Insight into Production Deployment,"Enterprises are making machine learning for production as an integral part of their future roadmaps and Earth science domain is no exception. However, there is common problem in transitioning machine learning from science to production due to a major difference in constructing a model versus deploying it for people to use to make decisions. Phases of machine learning lifecycle that includes model transition to production using a successful application is discussed.",IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium,2019,10.1109/IGARSS.2019.8899031,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
647da1aa8bd39087a77c5e1e043cdddb9b78b9e5,https://www.semanticscholar.org/paper/647da1aa8bd39087a77c5e1e043cdddb9b78b9e5,Propensity-to-Pay: Machine Learning for Estimating Prediction Uncertainty,"Predicting a customer's propensity-to-pay at an early point in the revenue cycle can provide organisations many opportunities to improve the customer experience, reduce hardship and reduce the risk of impaired cash flow and occurrence of bad debt. With the advancements in data science; machine learning techniques can be used to build models to accurately predict a customer's propensity-to-pay. Creating effective machine learning models without access to large and detailed datasets presents some significant challenges. This paper presents a case-study, conducted on a dataset from an energy organisation, to explore the uncertainty around the creation of machine learning models that are able to predict residential customers entering financial hardship which then reduces their ability to pay energy bills. Incorrect predictions can result in inefficient resource allocation and vulnerable customers not being proactively identified. This study investigates machine learning models' ability to consider different contexts and estimate the uncertainty in the prediction. Seven models from four families of machine learning algorithms are investigated for their novel utilisation. A novel concept of utilising a Baysian Neural Network to the binary classification problem of propensity-to-pay energy bills is proposed and explored for deployment.",ArXiv,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b14692c1e0658dd1d814c737171ce714b1588360,https://www.semanticscholar.org/paper/b14692c1e0658dd1d814c737171ce714b1588360,An Intelligent UAV Deployment Scheme for Load Balance in Small Cell Networks Using Machine Learning,"In wireless networks, network load can be highly unbalanced due to the mobility of user equipments (UEs). Unmanned Aerial Vehicles (UAVs) supported base station with the advantage of flexible deployment, ubiquitous wireless coverage and high speed data rate, is a promising approach to handle with the foregoing problem. However, how to achieve cost-effective UAV deployment in an autonomous and dynamic manner is a significant challenge. Facing this problem, we propose a novel UAV base station intelligent deployment scheme based on machine learning and evaluate its performance on a realworld dataset. First, we conduct data preprocessing to process, clean, and transform raw data into formatted data. Missing values are filled by Conditional Mean Imputation (CMI) method and outliers are corrected by pauta criterion. Then, we use hybrid approach which contains ARIMA model and XGBoost model. Linear predictions are carried out by ARIMA model and later nonlinear model XGBoost are applied on residue of ARIMA. Resultant prediction is obtained by adding linear and nonlinear prediction, hybrid model is estimated by Root Mean Square Error (RMSE) and R2 score. Finally, according to predicted results, UAV base stations can be deployed to cater for dynamically changing demands in the hotspot areas and achieve cost-effective deployment. Simulation results show that the propose scheme is superior to other benchmark schemes in load balancing.",2019 IEEE Wireless Communications and Networking Conference (WCNC),2019,10.1109/WCNC.2019.8885648,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
19552c33b6144ba9cf02b52310cfdccdc66b14f2,https://www.semanticscholar.org/paper/19552c33b6144ba9cf02b52310cfdccdc66b14f2,Empirical observation of negligible fairness-accuracy trade-offs in machine learning for public policy,,Nat. Mach. Intell.,2020,10.1038/s42256-021-00396-x,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6956a3b5ebf744d9cfe9aac87352efe155281a38,https://www.semanticscholar.org/paper/6956a3b5ebf744d9cfe9aac87352efe155281a38,Literature on Applied Machine Learning in Metagenomic Classification: A Scoping Review,"Simple Summary Technological advancements have led to modern DNA sequencing methods, capable of generating large amounts of data describing the microorganisms that live in samples taken from the environment. Metagenomics, the field that studies the different genomes within these samples, is becoming increasingly popular, as it has many real-world applications, such as the discovery of new antibiotics, personalized medicine, forensics, and many more. From a computer science point of view, it is interesting to see how these large volumes of data can be processed efficiently to accurately identify (classify) the microorganisms from the input DNA data. This scoping review aims to give an insight into the existing state of the art computational methods for processing metagenomic data through the prism of machine learning, data science, and big data. We provide an overview of the state of the art metagenomic classification methods, as well as the challenges researchers face when tackling this complex problem. The end goal of this review is to help researchers be up to date with current trends, as well as identify opportunities for further research and improvements. Abstract Applied machine learning in bioinformatics is growing as computer science slowly invades all research spheres. With the arrival of modern next-generation DNA sequencing algorithms, metagenomics is becoming an increasingly interesting research field as it finds countless practical applications exploiting the vast amounts of generated data. This study aims to scope the scientific literature in the field of metagenomic classification in the time interval 2008–2019 and provide an evolutionary timeline of data processing and machine learning in this field. This study follows the scoping review methodology and PRISMA guidelines to identify and process the available literature. Natural Language Processing (NLP) is deployed to ensure efficient and exhaustive search of the literary corpus of three large digital libraries: IEEE, PubMed, and Springer. The search is based on keywords and properties looked up using the digital libraries’ search engines. The scoping review results reveal an increasing number of research papers related to metagenomic classification over the past decade. The research is mainly focused on metagenomic classifiers, identifying scope specific metrics for model evaluation, data set sanitization, and dimensionality reduction. Out of all of these subproblems, data preprocessing is the least researched with considerable potential for improvement.",Biology,2020,10.3390/biology9120453,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2a307e8776e63e3d583ec4fe332fa380d40eb696,https://www.semanticscholar.org/paper/2a307e8776e63e3d583ec4fe332fa380d40eb696,Conservation machine learning,,BioData Mining,2020,10.1186/s13040-020-00220-z,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a87e5da618ba8e27f447a7b93a598818565b7b96,https://www.semanticscholar.org/paper/a87e5da618ba8e27f447a7b93a598818565b7b96,Prevention of Crop Disease in plants (Groundnut) using IoT and Machine Learning Models,"1Assistant Professor, Dept. of Computer Science and Engineering, Agni College of Technology, Chennai, India 2,3,4Student, Computer Science and Engineering, Agni College of Technology, Chennai, India ---------------------------------------------------------------------***--------------------------------------------------------------------Abstract Plant disease is one of the most vital among the crops which annoys the farmers. In order to help the farmers, it is a vital role to prevent the crop disease. If not done then huge amount of loss will happen to them. To address this problem a naï ve model is designed for the monitoring of crop disease with help of the sensors. Internet of things (IoT) is a promising technology which provides efficient and relevant solutions towards the modernization of agricultural domains. Humidity and Temperature sensor is deployed to verify the humidity and the atmospheric temperature of the plant. Similarly soil moisture sensor is deployed and to get status of the soil. Sensors, webcam, GSM and Controllers are used for receiving the data from the groundnut farm. The received data is analyzed using machine learning models (XG boost) and so the prediction of crop disease is done. Thus a novel approach for preventing the crop disease (Groundnut Crop) is proposed and the prediction is intimated to farmers through SMS/E-mail.",,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7e6168e88d4339a470a2a14cb553ec846c73b47d,https://www.semanticscholar.org/paper/7e6168e88d4339a470a2a14cb553ec846c73b47d,Machine-Learning Image Recognition Enhances Rock Classification,"This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 196657, “Machine Learning for 3D Image Recognition To Determine Porosity and Lithology of Heterogeneous Carbonate Rock,” by Omar Al-Farisi, SPE, Hongtao Zhang, and Aikifa Raza, Khalifa University of Science and Technology, prepared for the 2019 SPE Reservoir Characterization and Simulation Conference and Exhibition, Abu Dhabi, 17-19 September. The paper has not been peer reviewed.
 Automated image-processing algorithms can improve the quality and speed in classifying the morphology of heterogeneous carbonate rock. Several commercial products have produced petrophysical properties from 2D images and, to a lesser extent, from 3D images. Images are mainly microcomputed tomography (μCT), optical images of thin sections, or magnetic resonance images (MRI). However, most successful work is from homogeneous and clastic rocks. In the complete paper, the authors have demonstrated a machine-learning-assisted image-recognition (MLIR) approach to determine the porosity and lithology of heterogeneous carbonate rock by analyzing 3D images from µCT and MRI.
 Introduction
 The authors’ literature review has revealed the pressing need to perform 3D image processing instead of 2D. Achieving this goal requires an interdisciplinary approach. This study deployed new analysis and verification approaches, including 3D micromodels (3DMM) with various micropore sizes and uses 3DMM as an image-processing calibration reference. Additionally, a new image-resolution enhancement for quality segmentation is developed. Porosity was determined mainly using two methods. The first is a standalone image processing, in which image-information extraction was successful. The second is MLIR. The difference between image processing and image analysis is important. If image processing enables extraction of meaningful data, then image analysis is the ability to interpret this data through numerical analysis.",,2020,10.2118/1020-0063-jpt,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ed5960b20fa6986d960e9f7d0386ca850077a8ce,https://www.semanticscholar.org/paper/ed5960b20fa6986d960e9f7d0386ca850077a8ce,Machine Learning for Speaker Recognition,"This book will help readers understand fundamental and advanced statistical models and deep learning models for robust speaker recognition and domain adaptation. This useful toolkit enables readers to apply machine learning techniques to address practical issues, such as robustness under adverse acoustic environments and domain mismatch, when deploying speaker recognition systems. Presenting state-of-the-art machine learning techniques for speaker recognition and featuring a range of probabilistic models, learning algorithms, case studies, and new trends and directions for speaker recognition based on modern machine learning and deep learning, this is the perfect resource for graduates, researchers, practitioners and engineers in electrical engineering, computer science and applied mathematics.",,2020,10.1017/9781108552332,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6afa12b4b6e62ad08d6186c4acc06842819cac06,https://www.semanticscholar.org/paper/6afa12b4b6e62ad08d6186c4acc06842819cac06,Forest-Scale Phenotyping: Productivity Characterisation Through Machine Learning,"Advances in remote sensing combined with the emergence of sophisticated methods for large-scale data analytics from the field of data science provide new methods to model complex interactions in biological systems. Using a data-driven philosophy, insights from experts are used to corroborate the results generated through analytical models instead of leading the model design. Following such an approach, this study outlines the development and implementation of a whole-of-forest phenotyping system that incorporates spatial estimates of productivity across a large plantation forest. In large-scale plantation forestry, improving the productivity and consistency of future forests is an important but challenging goal due to the multiple interactions between biotic and abiotic factors, the long breeding cycle, and the high variability of growing conditions. Forest phenotypic expression is highly affected by the interaction of environmental conditions and forest management but the understanding of this complex dynamics is incomplete. In this study, we collected an extensive set of 2.7 million observations composed of 62 variables describing climate, forest management, tree genetics, and fine-scale terrain information extracted from environmental surfaces, management records, and remotely sensed data. Using three machine learning methods, we compared models of forest productivity and evaluate the gain and Shapley values for interpreting the influence of categorical variables on the power of these methods to predict forest productivity at a landscape level. The most accurate model identified that the most important drivers of productivity were, in order of importance, genetics, environmental conditions, leaf area index, topology, and soil properties, thus describing the complex interactions of the forest. This approach demonstrates that new methods in remote sensing and data science enable powerful, landscape-level understanding of forest productivity. The phenotyping method developed here can be used to identify superior and inferior genotypes and estimate a productivity index for individual site. This approach can improve tree breeding and deployment of the right genetics to the right site in order to increase the overall productivity across planted forests.",Frontiers in Plant Science,2020,10.3389/fpls.2020.00099,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1f844958e3a0b6ea2fbf38a2750e853539daa40b,https://www.semanticscholar.org/paper/1f844958e3a0b6ea2fbf38a2750e853539daa40b,Enhancing dependability of wireless sensor network under flooding attack: a machine learning perspective,"The wireless sensor network (WSN) is gaining paramount importance due to its application in real-time monitoring of vast geographical regions. The deployment paradigm shift is taking place from mobile computing to data science. Bridging the two technologies results in the development of dependable network in which security plays a pivotal role. This work considers the flooding attack which causes the communication failure. To detect this attack, an intrusion detection system (IDS) based on the randomised and the normalised deployment of nodes is proposed. Furthermore, machine learning techniques are implemented to enhance the dependability of network under flooding attack. The data flow is a significant parameter for governing the flooding effect on the network. It is found that machine learning models play a significant role in the prediction of the data flow. The experiments on simulated dataset underline the role of machine learning model for data flow prediction on the normalised dataset.",Int. J. Ad Hoc Ubiquitous Comput.,2020,10.1504/ijahuc.2020.10027115,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9306ef1d5d2e0262300b68c3ae3549966bf2673e,https://www.semanticscholar.org/paper/9306ef1d5d2e0262300b68c3ae3549966bf2673e,ML-MEDIC: A Preliminary Study of an Interactive Visual Analysis Tool Facilitating Clinical Applications of Machine Learning for Precision Medicine,"Accessible interactive tools that integrate machine learning methods with clinical research and reduce the programming experience required are needed to move science forward. Here, we present Machine Learning for Medical Exploration and Data-Inspired Care (ML-MEDIC), a point-and-click, interactive tool with a visual interface for facilitating machine learning and statistical analyses in clinical research. We deployed ML-MEDIC in the American Heart Association (AHA) Precision Medicine Platform to provide secure internet access and facilitate collaboration. ML-MEDIC’s efficacy for facilitating the adoption of machine learning was evaluated through two case studies in collaboration with clinical domain experts. A domain expert review was also conducted to obtain an impression of the usability and potential limitations.",Applied sciences,2020,10.3390/app10093309,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3adc30a295424f13c89482b6d00c76f3bef9d864,https://www.semanticscholar.org/paper/3adc30a295424f13c89482b6d00c76f3bef9d864,Increasing Enrollment by Optimizing Scholarship Allocations Using Machine Learning and Genetic Algorithms,"Effectively estimating student enrollment and recruiting students is critical to the success of any university. However, despite having an abundance of data and researchers at the forefront of data science, traditional universities are not fully leveraging machine learning and data mining approaches to improve their enrollment management strategies. In this project, we use data at a large, public university to increase their student enrollment. We do this by first predicting the enrollment of admitted first-year, first-time students using a suite of machine learning classifiers (AUROC = 0.85). We then use the results from these machine learning experiments in conjunction with genetic algorithms to optimize scholarship disbursement. We show the effectiveness of this approach using real-world enrollment metrics. Our optimized model was expected to increase enrollment yield by 15.8% over previous disbursement strategies. After deploying the model and confirming student enrollment decisions, the university actually saw a 23.3% increase in enrollment yield. This resulted in millions of dollars in additional annual tuition revenue and a commitment by the university to employ the method in subsequent enrollment cycles. We see this as a successful case study of how educational institutions can more effectively leverage their data.",EDM,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5aeb009cdaca8ff5fcff6927470ca7cd62b75c6d,https://www.semanticscholar.org/paper/5aeb009cdaca8ff5fcff6927470ca7cd62b75c6d,Survey on Machine Learning Algorithms for SDN/NFV Automation,"As applications of network become popular and the number of the end devices increases, modern network needs to cover more intensive demands compared to traditional networks. Since networks such as 5G that aim to guarantee massive connectivity, high data rate, and ultra-low latency, have a limitation based on hardware-based architectures, it has been proposed to deploy software-based programmable SDN and NFV architectures. By deploying SDN and NFV, it has been proposed to adopt machine learning algorithms to automatically control SDN and NFV, leading to intelligent networks. In this paper, we extensively review and summarize prior works on machine learning based SDN/NFV network management and orchestration. Also, we discuss limitation of prior works and direction for future research. 논문 19-44-01-14 The Journal of Korean Institute of Communications and Information Sciences '19-01 Vol.44 No.01 https://doi.org/10.7840/kics.2019.44.1.92 92 ※ 본 연구는 과학기술정보통신부 및 정보통신기술진흥센터의 정보통신·방송 연구개발 사업(2018-0-00701)의 연구결과로 수행되었으며 2017년도 정부(과학기술정보통신부)의 재원으로 한국연구재단(No. NRF-2017R1A2B4005041)의 지원을 받아 수행된 연구임. First Author : (ORCID:0000-0003-2193-5246)Department of Electronic and Electrical Engineering, Ewha Womans University, sunwoo.cho@ewhain.net, 학생회원 ° Corresponding Author : (ORCID:0000-0002-5079-1504)Department of Electronic and Electrical Engineering, Ewha Womans University, hyunggon.park@ewha.ac.kr, 종신회원 * (ORCID:0000-0001-6278-0952)Department of Electronic and Electrical Engineering, Ewha Womans University, daeun0710@ewhain.net, 학생회원 ** Electronics and Telecommunications Research Institute, soohwan@etri.re.kr, mkshin@etri.re.kr, 정회원 논문번호:201810-343-B-RN, Received October 25, 2018; Revised November 12, 2018; Accepted December 18, 2018 논문 / SDN/NFV 자동화를 위한 머신러닝기술 연구 동향",The Journal of Korean Institute of Communications and Information Sciences,2019,10.7840/KICS.2019.44.1.92,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fb0e9edb7c08d60a64794052df51b821de6b62da,https://www.semanticscholar.org/paper/fb0e9edb7c08d60a64794052df51b821de6b62da,Artificial intelligence and machine learning for efficient minefield clearance,"Landmines, particularly anti-personnel mines, are dreadful instruments of war. Mines can remain in the ground for decades and injure or kill long after the original conflict. Clearing mines is a dangerous, time consuming and expensive task. Fortunately, mine clearing already has well established and documented processes. To further support these efforts a new research project has started at Wrexham Glyndŵr University to explore the use of machine learning to create a prediction model able to better suggest the positions of hidden landmines based on locations of those already found. Research in psychology and computer science demonstrates the difficulty for humans and machines to create true randomness in their actions. The project will investigate whether it is possible to discover hidden patterns or sequences in mine deployment that could give hints where to look for more. The advantage of the envisioned technique is a lightweight data set only comprising numerical values and their simple acquisition in the field. The proposed system will support – not replace – conventional technology. Although machine learning and A.I. can discover structures, patterns and sequences in a huge data set, that humans cannot, it remains a form of prediction. The aim is therefore not to declare the ground safe (‘cleared’) but to give suggestions where additional explosives are likely to be found and thus, it is proposed, help to direct mine clearing resources better.",,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b29ed6a4644e34151cee26286877117aa2933c9e,https://www.semanticscholar.org/paper/b29ed6a4644e34151cee26286877117aa2933c9e,CryoDiscoveryTM: A Machine Learning Platform for Automated Cryo-electron Microscopy Particle Classification,"Cryogenic electron microscopy (Cryo-EM) produces high-resolution 3D images at angstrom levels used by researchers across a broad range of fields including structural biology, life Science, materials science, nanotechnology, semiconductors, energy, environmental science, and food science. Advancements in microscopy hardware enable production of 2D and 3D micrographs with near sub-Angstrom resolution, but require exponentially increasing data processing and storage capability. Images generated by cryoEM are visually noisy, and each project can produce more than 100,000 images and take weeks to arrive at one viewable 3D structure. Many steps in the cryo-EM workflow require manual intervention and analysis that can take several weeks and result in errors due to user bias, time waiting and user fatigue. Current image processing and data analysis solutions are not well-integrated, requiring extensive manual user involvement and long wait times before assessing image quality. Here we describe our development of machine learning models for automation of single particle classification during cryo-EM image processing with repeatable accuracy levels and integrated into the cryo-EM workflow for easy deployment with a new machine learning platform, called CryoDiscovery.",Microscopy and Microanalysis,2020,10.1017/S1431927620021145,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6b0699444091c4b4efda4e0076d1ae0069db688c,https://www.semanticscholar.org/paper/6b0699444091c4b4efda4e0076d1ae0069db688c,MACHINE LEARNING BASED AUTOMATED DRIVER-BEHAVIOR PREDICTION FOR AUTOMOTIVE CONTROL SYSTEMS,"The impact of good driving and rode safety plays a major role in automobile sector. Though autonomous driving and modern driving techniques are improving worldwide, the study of driver behavior and characteristics become indispensable. The research on driving science has taken long strides since its inception. Driving behavior analysis requires more valid attributes and the evaluation process requires better prediction models .The role of Artificial intelligence and machine learning in driver-behavior prediction have given new dimension to extract valuable results. This paper deploys a novel scheme to predict the driver behavior using advanced machine learning technique.",,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
be7206f940065f81e5d905fc3aa9aa9985ac5e67,https://www.semanticscholar.org/paper/be7206f940065f81e5d905fc3aa9aa9985ac5e67,A Machine Learning Based Framework for Predicting Student’s Academic Performance,"Educational Data Mining (EDM) as a new technology has become a field of research as a result of continuous improvement in numerous approaches in statistics, exploring hidden data in educational environment. An application associated with EDM is a predictive system that can be deployed in early prediction of student academic performance. The importance is to identify poor performers and provide necessary remediation to avoid school drop outs and also encourage high performers. This paper explores certain features of a population of 103 first year students majoring in Computer Science at University of Nigeria, Nsukka. Due to the high number of predicting variables determining student’s performance, it is necessary to apply feature selection mechanism using rapid miner to filter these variables. Decision tree, a Machine Learning Algorithm (MLA) was used in training and testing. It was observed that the accuracy is dependent on the datasets on which the model is trained. Two dissimilar datasets achieve different accuracy on the same algorithm. This leads us to conclude that the greatest factor in achieving higher accuracy is the type of datasets not actually the type of classification algorithm.",,2020,10.23880/PSBJ-16000145,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e7c8fe46d53dddf8ae02c112d8933207e22fb4d5,https://www.semanticscholar.org/paper/e7c8fe46d53dddf8ae02c112d8933207e22fb4d5,Production Operations Efficiency Gains Enablers: Machine Learning or Physics Based Models?,"
 Industry reports confirm that while the unconventional wells drilling and completion operations have achieved significant efficiency gains, which translated into steep cost reductions, the production operations costs remained largely flat. The authors provide a quick review of the above and discuss technologies aimed at addressing this situation including machine learning, AI and physics-based modeling. The latter has been the cornerstone for production optimization since the early days of oil and gas exploitation, however, it has been uneconomical for systematic use in unconventional wells and is applied to only a small fraction of the large number of active wells. This is mainly due to their complex completions, which makes modeling a tedious and resource intensive task. Data science promises faster and nimbler solutions, however, while advanced techniques such as deep learning have achieved impressive advances in many areas like face and speech recognition, they remain mostly rooted in pattern recognition, with no grasp of cause and effect, which limits the type of problems they can successfully resolve. This paper will discuss the above solutions and limitations and presents a novel physics-based approach that would be economical for widespread deployment.",,2020,10.2118/201501-ms,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
818e68f19f82df182f2880f0825f0de97c0f0f29,https://www.semanticscholar.org/paper/818e68f19f82df182f2880f0825f0de97c0f0f29,Delivering a machine learning course on HPC resources,"In recent years, proficiency in data science and machine learning (ML) became one of the most requested skills for jobs in both industry and academy. Machine learning algorithms typically require large sets of data to train the models and extensive usage of computing resources, both for training and inference. Especially for deep learning algorithms, training performances can be dramatically improved by exploiting Graphical Processing Units (GPUs). The needed skill set for a data scientist is therefore extremely broad, and ranges from knowledge of ML models to distributed programming on heterogeneous resources. While most of the available training resources focus on ML algorithms and tools such as TensorFlow, we designed a course for doctoral students where model training is tightly coupled with underlying technologies that can be used to dynamically provision resources. Throughout the course, students have access to a dedicated cluster of computing nodes on local premises. A set of libraries and helper functions is provided to execute a parallelized ML task by automatically deploying a Spark driver and several Spark execution nodes as Docker containers. Task scheduling is managed by an orchestration layer (Kubernetes). This solution automates the delivery of the software stack required by a typical ML workflow and enables scalability by allowing the execution of ML tasks, including training, over commodity (i.e. CPUs) or high-performance (i.e. GPUs) resources distributed over different hosts across a network. The adaptation of the same model on OCCAM, the HPC facility at the University of Turin, is currently under development.",EPJ Web of Conferences,2020,10.1051/epjconf/202024508016,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a9b646049e621a29d74acbfc3c4cea7506780803,https://www.semanticscholar.org/paper/a9b646049e621a29d74acbfc3c4cea7506780803,Machine learning improves antibiotics use,,Nature Reviews Urology,2020,10.1038/s41585-020-00413-5,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cc06199ccc2db316894c13599324634f77fc3fcd,https://www.semanticscholar.org/paper/cc06199ccc2db316894c13599324634f77fc3fcd,Design and Deployment of E-Health System in Perspective of Developing Countries: Machine Learning Based Approach (Preprint),"
 BACKGROUND
 We are living in a world where data science and machine learning is tightening its grasp on many sectors of modern life. The medical sector is not an exception. In developing countries, healthcare is one of the domains that need immediate attention. Due to the lack of manpower and technical resources, a large number of people in these regions do not receive proper medical care. Designing an E-health system with the help of machine learning and web technologies would be a great aid in such circumstances.
 
 
 OBJECTIVE
 This proposed E-health System will assist the medical professionals in determining diseases. Moreover, the system will be also helpful for the patients to check whether they have been diagnosed correctly. Based on their diagnosis results they can get medical specialist recommendation and medicine suggestions from the system. The automation of identifying the diseases and suggestion models with the help of machine learning will be cost-efficient and time-saving compared to the traditional methods. The main objective of this E-health system is to provide health care with the help of sustainable and realistic machine learning technologies.
 
 
 METHODS
 In this research, for the disease identification part, machine learning techniques have been applied to identify three diseases which are Dengue, Diabetes, and Thyroid. Decision Tree, Gaussian Naive-Bayes, Random Forest, Logistic Regression, k-Nearest Neighbors, Multilayer Perceptron, and Support Vector Machine Classifiers have been used for all three diseases. The E-health system comprised of disease identification model, medical specialist recommendation model, and the medicine suggestion model has been deployed on the web. The medical specialist recommendation model and the medicine suggestion model results are based on the finding of the disease identification model. Any user can insert their disease-specific data to use these three features of the E-health system.
 
 
 RESULTS
 For the disease identification model, Multilayer Perceptron for Dengue, Logistic Regression for Diabetes, and Random Forest for Thyroid performed the best with accuracies of 88.3%, 82.5%, and 98.5% respectively. These classifiers also showed good precision, recall, and F1 score.
 
 
 CONCLUSIONS
 The E-health system has performed well with real-time data. By making the dataset more enriched, the disease identification model will be more robust and thorough. Moreover, usability and acceptance tests can help us in finding different real-time scenarios of the E-health system.
",,2020,10.2196/preprints.23368,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
85a421a35767e4b35c17ef3506c08bc50c436a7c,https://www.semanticscholar.org/paper/85a421a35767e4b35c17ef3506c08bc50c436a7c,Towards Predicting Flood Event Peak Discharge in Ungauged Basins by Learning Universal Hydrological Behaviors with Machine Learning,"In the hydrological sciences, the outstanding challenge of regional modeling requires to capture common and event-specific hydrologic behaviors driven by rainfall spatial variability and catchment physiography during floods. The overall objective of this study is to develop robust understanding and predictive capability of how rainfall spatial variability influences flood peak discharge relative to basin physiography. A machine learning approach is used on a high-resolution dataset of rainfall and flooding events spanning 10 years, with rainfall events and basins of widely varying characteristics selected across the continental United States. It overcomes major limitations in prior studies that were based on limited observations or hydrological model simulations. This study explores first-order dependencies in the relationships between peak discharge, rainfall variability, and basin physiography, and it sheds light on these complex interactions using a multi-dimensional statistical modeling approach. Amongst different machine learning techniques, XGBoost is used to determine the significant physiographical and rainfall characteristics that influence peak discharge through variable importance analysis. A parsimonious model with low bias and variance is created which can be deployed in the future for flash flood forecasting. The results confirm that although the spatial organization of rainfall within a basin has a major influence on basin response, basin physiography is the primary driver of peak discharge. These findings have unprecedented spatial and temporal representativeness in terms of flood characterization across basins. An improved understanding of sub-basin scale rainfall spatial variability will aid in robust flash flood characterization as well as with identifying basins which could most benefit from distributed hydrologic modeling.",Journal of Hydrometeorology,2021,10.1175/jhm-d-20-0302.1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
192b558550d2b25fbf935425bc473741cf428659,https://www.semanticscholar.org/paper/192b558550d2b25fbf935425bc473741cf428659,Informing Developmental Milestone Achievement for Children With Autism: Machine Learning Approach,"Background Care for children with autism spectrum disorder (ASD) can be challenging for families and medical care systems. This is especially true in low- and- middle-income countries such as Bangladesh. To improve family–practitioner communication and developmental monitoring of children with ASD, mCARE (Mobile-Based Care for Children with Autism Spectrum Disorder Using Remote Experience Sampling Method) was developed. Within this study, mCARE was used to track child milestone achievement and family sociodemographic assets to inform mCARE feasibility/scalability and family asset–informed practitioner recommendations. Objective The objectives of this paper are threefold. First, it documents how mCARE can be used to monitor child milestone achievement. Second, it demonstrates how advanced machine learning models can inform our understanding of milestone achievement in children with ASD. Third, it describes family/child sociodemographic factors that are associated with earlier milestone achievement in children with ASD (across 5 machine learning models). Methods Using mCARE-collected data, this study assessed milestone achievement in 300 children with ASD from Bangladesh. In this study, we used 4 supervised machine learning algorithms (decision tree, logistic regression, K-nearest neighbor [KNN], and artificial neural network [ANN]) and 1 unsupervised machine learning algorithm (K-means clustering) to build models of milestone achievement based on family/child sociodemographic details. For analyses, the sample was randomly divided in half to train the machine learning models and then their accuracy was estimated based on the other half of the sample. Each model was specified for the following milestones: Brushes teeth, Asks to use the toilet, Urinates in the toilet or potty, and Buttons large buttons. Results This study aimed to find a suitable machine learning algorithm for milestone prediction/achievement for children with ASD using family/child sociodemographic characteristics. For Brushes teeth, the 3 supervised machine learning models met or exceeded an accuracy of 95% with logistic regression, KNN, and ANN as the most robust sociodemographic predictors. For Asks to use toilet, 84.00% accuracy was achieved with the KNN and ANN models. For these models, the family sociodemographic predictors of “family expenditure” and “parents’ age” accounted for most of the model variability. The last 2 parameters, Urinates in toilet or potty and Buttons large buttons, had an accuracy of 91.00% and 76.00%, respectively, in ANN. Overall, the ANN had a higher accuracy (above ~80% on average) among the other algorithms for all the parameters. Across the models and milestones, “family expenditure,” “family size/type,” “living places,” and “parent’s age and occupation” were the most influential family/child sociodemographic factors. Conclusions mCARE was successfully deployed in a low- and middle-income country (ie, Bangladesh), providing parents and care practitioners a mechanism to share detailed information on child milestones achievement. Using advanced modeling techniques this study demonstrates how family/child sociodemographic elements can inform child milestone achievement. Specifically, families with fewer sociodemographic resources reported later milestone attainment. Developmental science theories highlight how family/systems can directly influence child development and this study provides a clear link between family resources and child developmental progress. Clinical implications for this work could include supporting the larger family system to improve child milestone achievement.",JMIR Medical Informatics,2021,10.2196/29242,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8ac8c6e363fcb301f327005880fa87fdc31de4dc,https://www.semanticscholar.org/paper/8ac8c6e363fcb301f327005880fa87fdc31de4dc,Network Activities Recognition and Analysis Based on Supervised Machine Learning Classification Methods Using J48 and Naïve Bayes Algorithm,"Network activities recognition has always been a significant component of intrusion detection. However, with the increasing network traffic flow and complexity of network behavior, it is becoming more and more difficult to identify the specific behavior quickly and accurately by user network monitoring software. It also requires the system security staff to pay close attention to the latest intrusion monitoring technology and methods. All of these greatly increase the difficulty and complexity of intrusion detection tasks. The application of machine learning methods based on supervised classification technology would help to liberate the network security staff from the heavy and boring tasks. A finetuned model would accurately recognize user behavior, which could provide persistent monitoring with a relative high accuracy and good adaptability. Finally, the results of network activities recognition by J48 and Naïve Bayes algorithms are introduced and evaluated. Keywords—network activities, user behavior, machine learning, J48 algorithm, Naïve Bayes algorithm I. FOCUSED FORENSIC PROBLEM Given three datasets that do not have user activity label, the possible activities are browser using, music playing, and troubleshooting. And the user activity labels of three datasets are mutual exclusive, which provides more ways to make classification prediction task after the confirmation of one dataset label. However, this paper would focused on providing a general and reliable method to make classification on network activities. II. SURVEY OF RELATED WORK A. Academic Researches Researchers have done many works on the application of machine learning method to network intrusion detections. For example, the usage of genetic algorithms and decision trees to create rules for intrusion detection expert system is a very nice implement of machine learning method. The rule generation is an impressive direction[1]. However, rule generation possess a bigger time complexity, which would possible lead to long response time and even memory crash that are fatal to some certain intrusion detection situations. The supervised and unsupervised machine learning methods could be both useful during classifying user activities form network traffic. The combination of K-Means and Random Forest algorithms could obtain 97.37% accuracy[2]. Theoretically, the unsupervised algorithms (like K-Means and GM) and the supervised algorithms (like SVM and random forest) could be combined to produce a better accuracy than the models only use supervised algorithms. Because the pure supervised models would process the risks of losing flow information and low quality of training dataset labeling process. B. Open Source Projects The main purpose of this paper is to explore and provide a solid, compact, easy-to-deploy, easy-to-adjust system for intrusion detection researchers and cyber security professionals. Therefore, the open source community is also an important reference and guidance for the design, development, and evaluation of this paper. One machine learning method focused on the detection of attacks to IoT networks uses random forest and feed-forward neural network to classify the sample dataset. It labels the data as normal and malicious and then classify the test data with different types. The accuracy is 88%[3], while the project still possess the common shortage like huge memory consumption. This project show a good example of how to conduct machine learning method using Python. And it also demonstrates that the most common and important problem when conducting a machine learning system is how to relatively obtain the best accuracy or performance under the restrictions of resources consumption. III. SELECTION OF EXISTING COMPUTATIONAL METHODS The purpose of this project is mainly to build a compact machine learning system used to recognize the user activities with the best flexibility of machine learning algorithms and running environment. So, it is good to choose the Weka and the Wireshark as the main tool stack for this project. As for the specific algorithm, I would choose to set decision tree type algorithm outcome as the baseline, and then make prediction based on the outcome. After that, I would choose to use different ways to finetune prediction outcome. A. Weka The full name of the Weka is Waikato environment for knowledge analysis. It is a free, non-commercial, and opensourced software for machine learning or data mining tasks, which is based on Java environment. The application with GUI and its Jar package could be downloaded from its official website. It supports various platforms including Windows, Mac, and Linux. The official instruction video and document is accessible in the official website. The main developer of Weka is from the University of Waikato of New Zealand. The Weka has a large scale of algorithms clustered by hierarchical structures including data preprocessing, classification, clustering, association rules and visualization functions, which is a very helpful tools for low memory consuming tasks or small range testing tasks. The built-in visualization toolkits and detail parameter setting interfaces make it much easier and straightforward for users to operation. The user friendly interface and auto reminders would also help beginners to master this software even more faster. In addition, users could import the official Jar package to the java programs and then they could use the exact same function and operations with all parameter settings just like those in Weka software. The Weka also support mainstream data science tools such as R, Python, and Spark for more integrated developments and researches. B. WireShark The Wireshark is not an intrusion detection system (IDS), because it will not alert or prompt any abnormal traffic behavior on the network. Consequently, the users should make careful analysis of the packets retrieved. The Wireshark does not modify the content of network packets, it only reflects the packet information in circulation. And it does not send packets to the network, in other words, it is a pure observe and record tool for internet activities. And this could let users to retrieve the raw and complete data directly from internet driver of the current computer. Then, they would have a better understanding the details and complete processes of network behavior. As for the attributes of its retrieved data, the ID number and the time attribute shares basically the same function in the whole dataset. The time scale depends on the system time of the bag grabbing computer. So, the time attribute possesses more useful information than the ID number does. The deleting of ID number could be a good preprocessing method to improve the time consumption problem when training, validating, and testing. Meanwhile, it is important to note that the time scales of the first and last messages captured are not highly accurate, which means that in the final evaluation users should take the negative influence from time attribute to final prediction result seriously. Besides, users could also use the powerful filter engine of the Wireshark to filter out useful packets and eliminate the interference of irrelevant information. This is a very good way to generate the specific application network activities, which could contribute to improve the accuracy of trained model when making predictions. C. Decision Tree baseline The decision tree is a supervised learning, which uses decision analysis method to get and compare the probability of different leaves and nodes. And then evaluate and the best model with highest feasibility. It is a graphic method using probability analysis to provide an intuitive model format, which means that it is easy to generate and understand. It uses the entropy to measure and evaluate the degree of system clusters using algorithms like ID3, C4.5 (which is also known as J48), and C5.0.It is a very common classification method, and is suitable to be the baseline of classification tasks. ID3 algorithm is based on information theory, and takes information entropy and information gain degree as the measurement standards, so as to generate the inductive classification of data. J48 algorithm is a classification decision tree algorithm, which is based on the ID3 algorithm. This algorithm inherits the advantages of ID3 algorithm, and improves ID3 algorithm in the following aspects: • Use the information gain rate to select attributes, which overcomes the disadvantage of the preference of selecting attributes with more values only using the information gain; • Carry out pruning in the process of tree construction; • Possess the capability of completing the discretization operation to continuous attributes; • Possess the capability of handling incomplete data. The J48 algorithm has the advantages of the highly readable classification rules and high accuracy. However, it also has the disadvantages of relatively higher time consumption when scanning and sorting the dataset and higher system I/O resources consumption compared with other decision tree models. The high accuracy is more important than the resources and time consumption, so the J48 decision tree model would be conducted as the baseline of this project. D. Formulas The ID3 algorithm uses the information gain (IG) as the measure parameter and criteria during optimizing decision attributes, while the J48 algorithm uses the SplitInfo and the information gain rate (IGR) as the measure parameter and criteria. The IG, SplitInfo, and IGR could be calculated by the following formulas: IG(attr) = Entropy(S) − Entropy(S|attr) (1) SplitInfo4556(S) = −7 8 |9:| ; + log> 9: ; ? @A",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c889679840050252b98154028975e81afda80d69,https://www.semanticscholar.org/paper/c889679840050252b98154028975e81afda80d69,A machine learning approach to inform developmental milestone achievement for children with autism.,"BACKGROUND
Care for children with autism spectrum disorder (ASD) can be challenging for families and medical care systems. This is especially true in Low-and-Middle-Income-countries (LMIC) like Bangladesh. To improve family-practitioner communication and developmental monitoring of children with ASD, [spell out] (mCARE) was developed. Within this study, mCARE was used to track child milestone achievement and family socio-demographic assets to inform mCARE feasibility/scalability and family-asset informed practitioner recommendations.


OBJECTIVE
The objectives of this paper are three-fold. First, document how mCARE can be used to monitor child milestone achievement. Second, demonstrate how advanced machine learning models can inform our understanding of milestone achievement in children with ASD. Third, describe family/child socio-demographic factors that are associated with earlier milestone achievement in children with ASD (across five machine learning models).


METHODS
Using mCARE collected data, this study assessed milestone achievement in 300 children with ASD from Bangladesh. In this study, we used four supervised machine learning (ML) algorithms (Decision Tree, Logistic Regression, k-Nearest Neighbors, Artificial Neural Network) and one unsupervised machine learning (K-means Clustering) to build models of milestone achievement based on family/child socio-demographic details. For analyses, the sample was randomly divided in half to train the ML models and then their accuracy was estimated based on the other half of the sample. Each model was specified for the following milestones: Brushes teeth, Asks to use the toilet, Urinates in the toilet or potty, and Buttons large buttons.


RESULTS
This study aimed to find a suitable machine learning algorithm for milestone prediction/achievement for children with ASD using family/child socio-demographic characteristics. For, Brushes teeth, the three supervised machine learning models met or exceeded an accuracy of 95% with Logistic Regression, KNN, and ANN as the most robust socio-demographic predictors. For Asks to use toilet, 84.00% accuracy was achieved with the KNN and ANN models. For these models, the family socio-demographic predictors of ""family expenditure"" and ""parents' age"" accounted for most of the model variability. The last two parameters, Urinates in toilet or potty and Buttons large buttons had an accuracy of 91.00% and 76.00%, respectively, in ANN. Overall, the ANN had a higher accuracy (Above ~80% on average) among the other algorithms for all the parameters. Across the models and milestones, ""family expenditure"", ""family size/ type"", ""living places"" and ""parent's age and occupation"" were the most influential family/child socio-demographic factors.


CONCLUSIONS
mCARE was successfully deployed in an LMIC (i.e., Bangladesh), allowing parents and care-practitioners a mechanism to share detailed information on child milestones achievement. Using advanced modeling techniques this study demonstrates how family/child socio-demographic elements can inform child milestone achievement. Specifically, families with fewer socio-demographic resources reported later milestone attainment. Developmental science theories highlight how family/systems can directly influence child development and this study provides a clear link between family resources and child developmental progress. Clinical implications for this work could include supporting the larger family system to improve child milestone achievement.


CLINICALTRIAL
We took the IRB from Marquette University Institutional Review Board on July 9, 2020, with the protocol number HR-1803022959, and titled ""MOBILE-BASED CARE FOR CHILDREN WITH AUTISM SPECTRUM DISORDER USING REMOTE EXPERIENCE SAMPLING METHOD (MCARE)"" for recruiting a total of 316 subjects, of which we recruited 300. (Details description of participants in Methods section).",JMIR medical informatics,2021,10.2196/29242,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
620776dac30bce3e65b7a3ff5725d2d376a581c8,https://www.semanticscholar.org/paper/620776dac30bce3e65b7a3ff5725d2d376a581c8,Proactive Virtual Network Function Live Migration using Machine Learning,"VM (Virtual Machine) live migration is a server virtualization technique for deploying a running VM to another server node while minimizing downtime of a service the VM provides. Currently, in cloud data centers, VM live migration is widely used to apply load balancing on CPU workload and network traffic, to reduce electricity consumption by consolidating active VMs into specific location groups of servers, and to provide uninterrupted service during the maintenance of hardware and software update on servers. It is critical to use VM live migration as a prevention or mitigation measure for possible failure when its indications are detected or predicted. In this paper, we propose two VNF live migration methods; one for predictive load balancing and the other for a proactive measure in failure. Both need machine learning models that learn periodic monitoring data of resource usage and logs from servers and VMs/VNFs. We apply the second method to a vEPC (Virtual Evolved Pakcet Core) failure scenario to provide a detailed case study. ※ 이 논문은 2021년도 정부(과학기술정보통신부)의 재원으로 정보통신기획평가원의 지원(2018-0-00749, 인공지능 기반 가상 네트워크 관리기술 개발)을 받아 수행된 연구임 w First Author, ° Corresponding Author : : Pohang University of Science and Technology Department of Computer Science Engineering, jsy0906@postech.ac.kr * Pohang University of Science and Technology Department of Computer Science Engineering, {jhyoo78, jwkhong}@postech.ac.kr 논문번호:KNOM2021-01-05, Received July 5, 2021; Revised July 29,, 2021; Accepted August 13, 2021 KNOM Review '21-01 Vol.24 No.01",,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0d8ac956bdea92958278300b66c3b787a4d57052,https://www.semanticscholar.org/paper/0d8ac956bdea92958278300b66c3b787a4d57052,Network Requirements for Distributed Machine Learning Training in the Cloud,"In this thesis, I characterize the impact of network bandwidth on distributed machine learning training. I test four popular machine learning models (ResNet, DenseNet, VGG, and BERT) on an Nvidia A-100 cluster to determine the impact of bursty and non-bursty cross traffic (such as web-search traffic and long-lived flows) on the iteration time and throughput of distributed training. By varying the cross traffic load, I measure the impact of network congestion on training iteration times. I observe that with heavy web-search cross traffic (80% of link capacity), on average training iteration time is increased by up to 4 to 8×, for ResNet and BERT models, respectively. Further, I establish that the ring-all reduce communication collective is negatively impacted by network congestion even if the congestion is only affecting part of the ring. I also develop empirical models for the behavior of machine learning training in the presence of each type of cross traffic deployed. These results provide the motivation for developing novel congestion control protocols that are tailored for distributed training environments. Thesis Supervisor: Manya Ghobadi Title: TIBCO Career Development Assistant Professor of Electrical Engineering and Computer Science",,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
71adbc6367f7d10d3af87c1147fb57dd6925b101,https://www.semanticscholar.org/paper/71adbc6367f7d10d3af87c1147fb57dd6925b101,Prediction of cellulose sheet cutting using Machine Learning,"Cellulose is the main raw material for the production of paper. Companies that produce it present in their production line the cutting of the cellulose sheet. This failure is sporadic and has a high economic impact since it paralyzes the production line for several hours, incurring unproductive hours and a large deployment of human and financial resources. In this research, the use of Data Mining is proposed to define a machine learning algorithm that allows predicting the cutting of the cellulose sheet in a production line of a cellulose plant in Chile. The results show that by applying this technique it is possible to predict the cutting of the cellulose sheet well in advance to take corrective actions to avoid cutting and thus minimize the economic impact associated with the failure. 
Keywords: Data Mining, machine learning, cellulose, productivity. 
References 
[1]B. Ranaganth y G. Viswanath, «Application of artificial neural network for optimizing cutting variables in laser cutting of 304 grade stainless steel,» International Journal of Applied Engineering and Technology, vol. 1, nº 1, pp. 106-112, 2011. 
[2]M. Durica, J. Frnda y L. Svabova, «Decision tree based model of business failure prediction for Polish companies,» Oeconomia Copernicana, vol. 10, nº 3, pp. 453-469, 2019. 
[3]G. Köksal, İ. Batmaz y M. C. Testik, «A review of data mining applications for quality improvement in manufacturing industry,» Expert systems with Applications, vol. 38, nº 10, pp. 13448-13467, 2011. 
[4]H. Poblete y R. Vargas, «Relacion entre densidad y propiedades de tableros HDF producidos por un proceso seco,» Maderas. Ciencia y tecnología, vol. 8, nº 3, pp. 169-182, 2006. 
[5]B. Kovalerchuk y E. Vityaev, «Data mining for financial applications,» Data Mining and Knowledge Discovery Handbook, pp. 1203-1224, 2005. 
[6]U. Fayyad, G. Piatetsky-Shapiro, P. Smyth y R. Uthurusamy, «Advances in knowledge discovery and data mining,» American Association for Artificial Intelligence, 1996. 
[7]A. K. Pandey y A. K. Dubey, «Neuro fuzzy modeling of laser beam cutting process,» Applied Mechanics and Materials, vol. 110, pp. 4109-4117, 2012. 
[8]M. Németh y G. Michaľčonok, «Preparation and cluster analysis of data from the industrial production process for failure prediction,» Research Papers Faculty of Materials Science and Technology Slovak University of Technology, vol. 24, nº 39, pp. 111-116, 2016. 
[9]S. Ballı, «A data mining approach to the diagnosis of failure modes for two serial fastened sandwich composite plates,» Journal of Composite Materials, vol. 51, nº 20, pp. 2853-2862, 2017. 
[10]S. Dindarloo y E. Siami-Irdemoosa, «Data mining in mining engineering: results of classification and clustering of shovels failures data,» International Journal of Mining, Reclamation and Environment, vol. 31, nº 2, pp. 105-118, 2017. 
[11]E. e Oliveira, V. Miguéis, L. Guimarães y J. L. Borges, «Power Transformer Failure Prediction: Classification in Imbalanced Time Series,» U. Porto Journal of Engineering, vol. 3, nº 2, pp. 34-48, 2017. 
[12]A. Taghizadeh y N. Demirel, «Application of Machine Learning for Dragline Failure Prediction,» E3S Web of Conferences, vol. 15, p. 03002, 2017. 
[13]W. Chang, Z. Xu, M. You, S. Zhou, Y. Xiao y Y. Cheng, «A Bayesian Failure Prediction Network Based on Text Sequence Mining and Clustering,» Entropy, vol. 20, nº 12, p. 923, 2018. 
[14]K. Halteh, K. Kumar y A. Gepp, «Financial distress prediction of Islamic banks using tree-based stochastic techniques,» Managerial Finance, vol. 44, nº 6, pp. 759-773, 2018. 
[15]C.-H. Liu, C.-J. Lin, Y.-H. Hu y Z.-H. You, «Predicting the failure of dental implants using supervised learning techniques,» Applied Sciences, vol. 8, nº 5, p. 698, 2018. 
[16]B. Mohammed, I. Awan, H. Ugail y M. Younas, «Failure prediction using machine learning in a virtualised HPC system and application,» Cluster Computing, vol. 22, nº 2, pp. 471-485, 2019. 
[17]O. Sukhbaatar, T. Usagawa y L. Choimaa, «An artificial neural network based early prediction of failure-prone students in blended learning course,» International Journal of Emerging Technologies in Learning (iJET)}, vol. 14, nº 19, pp. 77-92, 2019. 
[18]Z. Wang, W. Zhao y X. Hu, «Analysis of prediction model of failure depth of mine floor based on fuzzy neural network,» Geotechnical and Geological Engineering, vol. 37, nº 1, pp. 71-76, 2019. 
[19]V. S. Gujre y R. Anand, «Machine learning algorithms for failure prediction and yield improvement during electric resistance welded tube manufacturing,» Journal of Experimental \& Theoretical Artificial Intelligence, vol. 32, nº 4, pp. 601-622, 2020. 
[20]P. du Jardin, «Forecasting corporate failure using ensemble of self-organizing neural networks,» European Journal of Operational Research, vol. 288, nº 3, pp. 869-885, 2021. 
[21]R. Brachman y T. Anand, «The process of knowledge discovery in databases,» Advances in knowledge discovery and data mining, pp. 37-57, 1996. 
[22]W. Frawley, G. Piatetsky-Shapiro y C. Matheus, «Knowledge discovery in databases: An overview,» AI magazine, vol. 13, nº 3, p. 57, 1992. 
[23]F. H. Troncoso Espinosa y J. V. Ruiz Tapia, «Predicción de fuga de clientes en una empresa de distribución de gas natural mediante el uso de minería de datos,» Universidad Ciencia y Tecnología, vol. 24, nº 106, pp. 79-87, 2020. 
[24]F. H. Troncoso, «Prediction of Recidivism in Thefts and Burglaries Using Machine Learning,» Indian Journal of Science and Technology, vol. 13, nº 6, pp. 696-711, March 2020. 
[25]M. Kantardzic, Data mining: concepts, models, methods, and algorithms, John Wiley & Sons, 2011. 
[26]F. H. Troncoso Espinosa, P. G. Fuentes Figueroa y I. R. Belmar Arriagada, «Predicción de fraudes en el consumo de agua potable mediante el uso de Minería de Datos,» Universidad Ciencia y Tecnología, vol. 24, nº 104, pp. 58-66, 2020. 
[27]C. Romero y S. Ventura, «Data mining in education,» Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 3, nº 1, pp. 12-27, 2013. 
[28]D. Larose y C. Larose, Discovering knowledge in data: an introduction to data mining, John Wiley & Sons, 2014.",Universidad Ciencia y Tecnología,2021,10.47460/uct.v25i110.481,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
28530da341c7371f28323bf2a8ec094caf47adae,https://www.semanticscholar.org/paper/28530da341c7371f28323bf2a8ec094caf47adae,Guide on Way to approach a Machine Learning problem,"Today, algorithms are like buzz words. Everyone is going for learningdifferent kinds of algorithms – logistic regression, randomforests, decision tress, SVMs, Gradient boosting algorithms,neural networks etc.. Everyday new algorithms are beingmade. But Data Science is not just applying different algorithmsto the data. Before applying any algorithm, you must understandyour data because that will help you in improving performance ofyour algorithms later. For any problem one needs to iterate overthe same steps- data preparation, model planning, model buildingand model evaluation, for improving accuracy. If we directlyjump to model building, we end up directionless after one iteration.Following are few defined steps per me for approaching anymachine learning problem:The first step I suggest is to understandyour problem properly with a good understanding of the businessmarket. There is no scenario like: here is the data, here is the algorithmand Bam! Proper business understanding will help you inhandling the data in upcoming steps. For example, if you do nothave any idea about the banking system you will not understand ifa feature like income of customer, should be included or not. Thenext step is to collect relevant data for your problem. Other thanthe data you have internally in your company, you should also addexternal data source. For example, for sales prediction you shouldunderstand the market scenario for sales of your product. GDPmay affect your sales or may be population affects. So, collect suchkind of external data. Also remember the fact that any externaldata that you use should be available to you in the future whenyour model gets deployed. Like if you use population in your model,next year also you should be able to collect this data for gettingpredictions in the next year. I have seen many people who only usetheir internal data without realizing the importance of externaldata to their dataset. But in reality, external features have a goodimpact on our use case. Now when you have collected all therelevant data for your problem, you must divide it for training andtesting. Many data scientists follow the 70/30 rule to divide thedata into two parts: training and test set. While many follow the60/20/20 rule to divide the data into three parts: training set, testset and validation set. I prefer the second option because in thiscase you use test set for improving your model and validation setfor final verification of your model in actual scenario. with it. I wasworking on a default loan prediction problem. My accuracy was78%. I took my problem to the person who was handling financialsystems related to loans.",,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9ece83a564817ef3afe771344442ebefdafcd4f2,https://www.semanticscholar.org/paper/9ece83a564817ef3afe771344442ebefdafcd4f2,Type : New PW Title : Using Frontera GPU computing to accelerate developments in interpretable machine learning,"With this Pathways allocation we will obtain InSAR velocity fields using Sentinel-1 data for both the Indonesian volcanic arc and the Tibetan Plateau. These data will be used (1) to test the hypothesis that in Indonesia the is a temporal correlation between volcanic unrest and precipitation with more unrest occurring during the rainy season because infiltrated rainwater weakens the rock and (2) to constrain models of continental deformation. Type: New PW Title: Planet-disk interaction in three dimensions Principal Investigator: Jaehan Bae (Carnegie Institution of Washington) Co-Investigators: Field of Science: Stellar Astronomy and Astrophysics Abstract: Recent observations of circumstellar disks around young, forming stars have revealed a plethora of Recent observations of circumstellar disks around young, forming stars have revealed a plethora of structures. One of the most exciting possibilities is the interaction between the disk and planets embedded therein. In order to improve our understanding of planet-disk interaction and help better interpret observations taken at unprecedented spatial resolution and sensitivity, we propose to carry out three-dimensional numerical simulations of planet-disk interaction. Type: New PW Title: Using Frontera GPU computing to accelerate developments in interpretable machine learning Principal Investigator: David Benkeser (Emory University) Co-Investigators: Field of Science: Statistics and Probability Abstract: Machine learning for health care and public health decision making faces a significant tradeoff: accuracy Machine learning for health care and public health decision making faces a significant tradeoff: accuracy vs. interpretability. Simple rules (e.g., based on logistic regression) for predicting outcomes or providing treatment recommendations are easy-to interpret, but often suffer in terms of performance. More complex rules, based on black-box algorithms like deep learning, may predict more accurately, but are difficult to interpret. We were recently funded by a three year grant from the National Science Foundation to develop methods that bridge this gap and provide machine learning that is both accurate and interpretable, making it amenable for deployment in clinical and public health settings. GPU computing could play a vital role in achieving the grant's objectives of generating fast and scalable implementations of our proposed algorithms. Type: New PW Title: Development of multiphase fluid-structure interaction methods Principal Investigator: Amneet Pal Bhalla (San Diego State University) Co-Investigators: KAUSTUBH KHEDKAR (San Diego State University) Field of Science: Applied Mathematics Abstract: Applications involving fluid-structure interaction (FSI) are ubiquitous in natural and engineering processes Applications involving fluid-structure interaction (FSI) are ubiquitous in natural and engineering processes that can range from bacterial swimming to the interaction of waves with ships. FSI also plays a vital role in new methodological approaches for modeling energy harvesting devices such as wave energy converters (WEC) and simulating 3D printing processes. Executing these applications at large-scale with the traditional body-fitted grid approach becomes prohibitively expensive due to the re-meshing requirement of the numerical scheme to conform to the deforming or moving body in the domain. On the other hand, the immersed boundary (IB) methods discretize the computational domain using Cartesian grids, and the dynamics of the immersed structure is resolved by modifying the underlying equations of motion. Thus, the computational cost involved in resolving FSI is substantially reduced by adopting the IB approach. In our group, we study the interaction of surface gravity waves with moving structures such as wave energy converter devices and naval ships, as well as model the stereo-lithography 3D printing process using state-of-the-art immersed boundary method and adaptive mesh refinement based software infrastructure named IBAMR, which is a National Science Foundation (NSF OAC 1450327, OAC 1450374 and OAC 1931516) sponsored advanced cyberinfrastructure (CI) project. In the following sections, we give a brief review of our research work and the questions that we aim to address. Our proposed project is funded by NSF award 1931368. Type: New PW Title: Real-time High Resolution Ensemble Numerical Weather Forecasts Using SAR-FV3 for the Hydrometeorology Testbed Ramping toward Exascale Principal Investigator: Keith Brewster (University of Oklahoma) Co-Investigators: Nathan Snook (University of Oklahoma); Tim Supinie (University of Oklahoma) Field of Science: Meteorology",,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
12d94fdd1d3cd63b304a4724f4fedd3e80c3bc8f,https://www.semanticscholar.org/paper/12d94fdd1d3cd63b304a4724f4fedd3e80c3bc8f,Machine Learning and Text Analysis in the Tasks of Knowledge Graphs Refinement and Enrichment,"Working prototypes of the scalable semantic web portals, which are deployed on cloud platforms and intended for use in universities educational activity, are discussed. The first project is related to teaching in the field of nuclear physics and nuclear power engineering. The second project is related to training in computer science and programming. The possibility of using the DLLearner software in conjunction with the Apache Jena Reasoners in order to refine the ontologies that are designed on the basis of the SROIQ(D) description logic is shown. A software agent for the context-sensitive searching for new knowledge in the WWW has been developed as a toolkit for ontologies enrichment. The binary Pareto relation and Levenshtein metrics are used in order to evaluate the measure of compliance of the found content concerning a specific domain. It allows the knowledge engineer to calculate the measure of the proximity of an arbitrary network resource about classes and objects of specific knowledge graphs. The suggested software solutions are based on cloud computing using DBaaS and PaaS service models to ensure the scalability of data warehouses and network services. Examples of applying the software and technologies under discuss are given.",,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9ece83a564817ef3afe771344442ebefdafcd4f2,https://www.semanticscholar.org/paper/9ece83a564817ef3afe771344442ebefdafcd4f2,Type : New PW Title : Using Frontera GPU computing to accelerate developments in interpretable machine learning,"With this Pathways allocation we will obtain InSAR velocity fields using Sentinel-1 data for both the Indonesian volcanic arc and the Tibetan Plateau. These data will be used (1) to test the hypothesis that in Indonesia the is a temporal correlation between volcanic unrest and precipitation with more unrest occurring during the rainy season because infiltrated rainwater weakens the rock and (2) to constrain models of continental deformation. Type: New PW Title: Planet-disk interaction in three dimensions Principal Investigator: Jaehan Bae (Carnegie Institution of Washington) Co-Investigators: Field of Science: Stellar Astronomy and Astrophysics Abstract: Recent observations of circumstellar disks around young, forming stars have revealed a plethora of Recent observations of circumstellar disks around young, forming stars have revealed a plethora of structures. One of the most exciting possibilities is the interaction between the disk and planets embedded therein. In order to improve our understanding of planet-disk interaction and help better interpret observations taken at unprecedented spatial resolution and sensitivity, we propose to carry out three-dimensional numerical simulations of planet-disk interaction. Type: New PW Title: Using Frontera GPU computing to accelerate developments in interpretable machine learning Principal Investigator: David Benkeser (Emory University) Co-Investigators: Field of Science: Statistics and Probability Abstract: Machine learning for health care and public health decision making faces a significant tradeoff: accuracy Machine learning for health care and public health decision making faces a significant tradeoff: accuracy vs. interpretability. Simple rules (e.g., based on logistic regression) for predicting outcomes or providing treatment recommendations are easy-to interpret, but often suffer in terms of performance. More complex rules, based on black-box algorithms like deep learning, may predict more accurately, but are difficult to interpret. We were recently funded by a three year grant from the National Science Foundation to develop methods that bridge this gap and provide machine learning that is both accurate and interpretable, making it amenable for deployment in clinical and public health settings. GPU computing could play a vital role in achieving the grant's objectives of generating fast and scalable implementations of our proposed algorithms. Type: New PW Title: Development of multiphase fluid-structure interaction methods Principal Investigator: Amneet Pal Bhalla (San Diego State University) Co-Investigators: KAUSTUBH KHEDKAR (San Diego State University) Field of Science: Applied Mathematics Abstract: Applications involving fluid-structure interaction (FSI) are ubiquitous in natural and engineering processes Applications involving fluid-structure interaction (FSI) are ubiquitous in natural and engineering processes that can range from bacterial swimming to the interaction of waves with ships. FSI also plays a vital role in new methodological approaches for modeling energy harvesting devices such as wave energy converters (WEC) and simulating 3D printing processes. Executing these applications at large-scale with the traditional body-fitted grid approach becomes prohibitively expensive due to the re-meshing requirement of the numerical scheme to conform to the deforming or moving body in the domain. On the other hand, the immersed boundary (IB) methods discretize the computational domain using Cartesian grids, and the dynamics of the immersed structure is resolved by modifying the underlying equations of motion. Thus, the computational cost involved in resolving FSI is substantially reduced by adopting the IB approach. In our group, we study the interaction of surface gravity waves with moving structures such as wave energy converter devices and naval ships, as well as model the stereo-lithography 3D printing process using state-of-the-art immersed boundary method and adaptive mesh refinement based software infrastructure named IBAMR, which is a National Science Foundation (NSF OAC 1450327, OAC 1450374 and OAC 1931516) sponsored advanced cyberinfrastructure (CI) project. In the following sections, we give a brief review of our research work and the questions that we aim to address. Our proposed project is funded by NSF award 1931368. Type: New PW Title: Real-time High Resolution Ensemble Numerical Weather Forecasts Using SAR-FV3 for the Hydrometeorology Testbed Ramping toward Exascale Principal Investigator: Keith Brewster (University of Oklahoma) Co-Investigators: Nathan Snook (University of Oklahoma); Tim Supinie (University of Oklahoma) Field of Science: Meteorology",,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
12d94fdd1d3cd63b304a4724f4fedd3e80c3bc8f,https://www.semanticscholar.org/paper/12d94fdd1d3cd63b304a4724f4fedd3e80c3bc8f,Machine Learning and Text Analysis in the Tasks of Knowledge Graphs Refinement and Enrichment,"Working prototypes of the scalable semantic web portals, which are deployed on cloud platforms and intended for use in universities educational activity, are discussed. The first project is related to teaching in the field of nuclear physics and nuclear power engineering. The second project is related to training in computer science and programming. The possibility of using the DLLearner software in conjunction with the Apache Jena Reasoners in order to refine the ontologies that are designed on the basis of the SROIQ(D) description logic is shown. A software agent for the context-sensitive searching for new knowledge in the WWW has been developed as a toolkit for ontologies enrichment. The binary Pareto relation and Levenshtein metrics are used in order to evaluate the measure of compliance of the found content concerning a specific domain. It allows the knowledge engineer to calculate the measure of the proximity of an arbitrary network resource about classes and objects of specific knowledge graphs. The suggested software solutions are based on cloud computing using DBaaS and PaaS service models to ensure the scalability of data warehouses and network services. Examples of applying the software and technologies under discuss are given.",,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f1927f584680008bab255b6ecb10c259b5450603,https://www.semanticscholar.org/paper/f1927f584680008bab255b6ecb10c259b5450603,A machine learning approach for IoT cultural data,,Journal of Ambient Intelligence and Humanized Computing,2019,10.1007/s12652-019-01452-6,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ebab687cd1be7d25392c11f89fce6a63bef7219d,https://www.semanticscholar.org/paper/ebab687cd1be7d25392c11f89fce6a63bef7219d,Towards the Science of Security and Privacy in Machine Learning,"Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive—new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community’s understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.",ArXiv,2016,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e2a676c3c3d97980bb3b00a0a4b828697d61f062,https://www.semanticscholar.org/paper/e2a676c3c3d97980bb3b00a0a4b828697d61f062,Sibyl: Understanding and Addressing the Usability Challenges of Machine Learning In High-Stakes Decision Making,"Machine learning (ML) is being applied to a diverse and ever-growing set of domains. In many cases, domain experts - who often have no expertise in ML or data science - are asked to use ML predictions to make high-stakes decisions. Multiple ML usability challenges can appear as result, such as lack of user trust in the model, inability to reconcile human-ML disagreement, and ethical concerns about oversimplification of complex problems to a single algorithm output. In this paper, we investigate the ML usability challenges that present in the domain of child welfare screening through a series of collaborations with child welfare screeners. Following the iterative design process between the ML scientists, visualization researchers, and domain experts (child screeners), we first identified four key ML challenges and honed in on one promising explainable ML technique to address them (local factor contributions). Then we implemented and evaluated our visual analytics tool, Sibyl, to increase the interpretability and interactivity of local factor contributions. The effectiveness of our tool is demonstrated by two formal user studies with 12 non-expert participants and 13 expert participants respectively. Valuable feedback was collected, from which we composed a list of design implications as a useful guideline for researchers who aim to develop an interpretable and interactive visualization tool for ML prediction models deployed for child welfare screeners and other similar domain experts.",IEEE Transactions on Visualization and Computer Graphics,2021,10.1109/TVCG.2021.3114864,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d6eaf8bc3cf052a37a4f4618b38df5076120556d,https://www.semanticscholar.org/paper/d6eaf8bc3cf052a37a4f4618b38df5076120556d,Humans Learning from Machines: Data Science Meets Network Management,"Internet Service Providers need to deploy and maintain many wireless sites in isolated or inaccessible terrain to provide Internet connectivity to rural communities. Addressing failures at such sites can be very expensive, both in identifying the fault, and also in the repair or rectification. Data monitoring can be useful, to spot anomalies and predict a fault (and possibly pre-empt it altogether), or to locate and isolate it quickly once it causes an issue for the network. There might be hundreds of variables to be monitored in principle, but only a few of significance for detecting faults. Here, in a case study involving a Wireless Internet Service Provider (WISP) in a rural area, we first illustrate a bottom-up approach to the identification of variables likely to be of use in an automatic anomaly detector. For the purpose of this study, the detector consists of an autoencoder neural network with weights optimized by machine learning (ML). We then show how the cause of an anomaly can be derived from indirect measurements, and use the model to learn relationships between certain variables.",2021 International Conference on COMmunication Systems & NETworkS (COMSNETS),2021,10.1109/COMSNETS51098.2021.9352890,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4dd16c5b14b1fae474e152f2c2f7aa78f3e2e541,https://www.semanticscholar.org/paper/4dd16c5b14b1fae474e152f2c2f7aa78f3e2e541,Machine Learning and Data Cleaning: Which Serves the Other?,"The last few years witnessed signiicant advances in building automated or semi-automated data quality, data cleaning and data integration systems powered by machine learning (ML). In parallel, large deployment of ML systems in business, science, environment and various other areas started to realize the strong dependency on the quality of the input data to these ML models to get reliable predictions or insights. That dual relationship between ML and data cleaning has been addressed by many recent research works under terms such as łData cleaning for MLž and łML for automating data cleaning and data preparationž. In this article, we highlight this symbiotic relationship between ML and data cleaning and discuss few challenges that require collaborative eforts of multiple research communities.",Journal of Data and Information Quality,2022,10.1145/3506712,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e5e851351cfb5ef6ab4b00e30545283dfa1959e4,https://www.semanticscholar.org/paper/e5e851351cfb5ef6ab4b00e30545283dfa1959e4,Search-based fairness testing for regression-based machine learning systems,,Empir. Softw. Eng.,2022,10.1007/s10664-022-10116-7,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e31e82fc1c83564f69d3448822dc5ab17d4b8984,https://www.semanticscholar.org/paper/e31e82fc1c83564f69d3448822dc5ab17d4b8984,Modeling Analysts’ Recommendations via Bayesian Machine Learning,"Individual analysts typically publish recommendations several times per year on the handful of stocks they follow within their specialized fields. How should investors interpret this information? How can they factor in the past performance of individual analysts when assessing whether to invest long or short in a stock? This is a complicated problem to model quantitatively: There are thousands of individual analysts, each of whom follows only a small subset of the thousands of stocks available for investment. Overcoming this inherent sparsity naturally raises the question of how to learn an analyst’s forecasting ability by integrating track-record information from all the stocks the analyst follows; in other words, inferring an analyst’s ability on Stock X from track records on both Stock X and stocks other than X. The authors address this topic using a state-of-the-art computationally rapid Bayesian machine learning technique called independent Bayesian classifier combination (IBCC), which has been deployed in the physical and biological sciences. The authors argue that there are many similarities between the analyst forecasting problem and a very successful application of IBCC in astronomy, a study in which it dominates heuristic alternatives including simple or weighted averages and majority voting. The IBCC technique is ideally suited to this particularly sparse problem, enabling computationally efficient inference, dynamic tracking of analyst performance through time, and real-time online forecasting. The results suggest the IBCC technique holds promise in extracting information that can be deployed in active discretionary and quantitative investment management.",The Journal of Financial Data Science,2018,10.3905/jfds.2019.1.1.075,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
91fab3f200f3f63d15785867e4ab41535f9ecd34,https://www.semanticscholar.org/paper/91fab3f200f3f63d15785867e4ab41535f9ecd34,Predictive models for clinical decision making: Deep dives in practical machine learning,"The deployment of machine learning for tasks relevant to complementing standard of care and advancing tools for precision health has gained much attention in the clinical community, thus meriting further investigations into its broader use. In an introduction to predictive modelling using machine learning, we conducted a review of the recent literature that explains standard taxonomies, terminology and central concepts to a broad clinical readership. Articles aimed at readers with little or no prior experience of commonly used methods or typical workflows were summarised and key references are highlighted. Continual interdisciplinary developments in data science, biostatistics and epidemiology also motivated us to further discuss emerging topics in predictive and data‐driven (hypothesis‐less) analytics with machine learning. Through two methodological deep dives using examples from precision psychiatry and outcome prediction after lymphoma, we highlight how the use of, for example, natural language processing can outperform established clinical risk scores and aid dynamic prediction and adaptive care strategies. Such realistic and detailed examples allow for critical analysis of the importance of new technological advances in artificial intelligence for clinical decision‐making. New clinical decision support systems can assist in prevention and care by leveraging precision medicine.",Journal of internal medicine,2022,10.1111/joim.13483,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6c2451fecba39eb03361fca793019732a0a76a7c,https://www.semanticscholar.org/paper/6c2451fecba39eb03361fca793019732a0a76a7c,Architectural Design Decisions for Machine Learning Deployment,"Deploying machine learning models to production is challenging, partially due to the misalignment between software engineering and machine learning disciplines but also due to potential practitioner knowledge gaps. To reduce this gap and guide decision-making, we conducted a qualitative investigation into the technical challenges faced by practitioners based on studying the grey literature and applying the Straussian Grounded Theory research method. We modelled current practices in machine learning, resulting in a UML-based architectural design decision model based on current practitioner understanding of the domain and a subset of the decision space and identified seven architectural design decisions, various relations between them, twenty-six decision options and forty-four decision drivers in thirty-five sources. Our results intend to help bridge the gap between science and practice, increase understanding of how practitioners approach deployment of their solutions, and support practitioners in their decision-making.",2022 IEEE 19th International Conference on Software Architecture (ICSA),2022,10.1109/ICSA53651.2022.00017,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d68977d037eb946674ca66b047d521e3b9b964ce,https://www.semanticscholar.org/paper/d68977d037eb946674ca66b047d521e3b9b964ce,Promoting Ethical Deployment of Artificial Intelligence and Machine Learning in Healthcare,"The ethics of artificial intelligence (AI) and machine learning (ML) exemplify the conceptual struggle between applying familiar pathways of ethical analysis versus generating novel strategies. Melissa McCradden et al.’s “A Research Ethics Framework for the Clinical Translation of Healthcare Machine Learning” puts pressure on this tension while still attempting not to break it—trying to impute structure and epistemic consistency where it is currently lacking (McCradden et al. 2022). They highlight an “AI chasm” “generated by a clash between the... cultures of computer science and clinical science,” but argue that the “ethical norms of human subjects research” are still the right pathway to bridge this divide. The Open Peer Commentaries included in this issue agree with this central premise while critiquing the insufficiency of current ethics and regulatory solutions to adequately protect communities at higher risk for ML bias. The current U.S. human subjects research regulations (HSRR) were developed from traditional conceptions of research ethics (Schupmann and Moreno 2020). Research ethicists have subsequently been asked to apply existing concepts to new areas over which the regulatory structure does not reach. AI/ML is an excellent example of the strengths and limitations of this current default.",The American journal of bioethics : AJOB,2022,10.1080/15265161.2022.2059206,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a84ed973979d864ee3c71bf8b8ef4d5e3c29ffc9,https://www.semanticscholar.org/paper/a84ed973979d864ee3c71bf8b8ef4d5e3c29ffc9,Screening Native Machine Learning Pipelines with ArgusEyes,"Software systems that learn from data are being deployed in increasing numbers in industrial and institutional scenarios. Developing these machine learning (ML) applications im-poses additional challenges beyond those of traditional software systems. The behavior of such applications very much depends on their input data, and they are based on systems and libraries from a relatively young data science ecosystem, which is rapidly evolving all the time. Experience shows that it is diﬃcult to ensure that such ML applications are imple-mented correctly [Polyzotis et al. 2018, Stoyanovich et al. 2020], and as a consequence, data scientists building these applications require fundamental system support.",CIDR,2022,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3848a772ce1f276d7b4ec51b35205ff4afc6d7ed,https://www.semanticscholar.org/paper/3848a772ce1f276d7b4ec51b35205ff4afc6d7ed,A Machine Learning Centered Approach for Uncovering Excavators’ Last Known Location Using Bluetooth and Underground WSN,"Machine learning and data analytics are two of the most popular subdisciplines of modern computer science which have a variety of scopes in most of the industries ranging from hospitals to hotels, manufacturing to pharmaceuticals, mining to banking, etc. Additionally, mining and hospitals are two of the most critical industries where applications when deployed security, accuracy, and cost effectiveness are the major concerns, due to the huge involvement of man and machines. In this paper, the problem of finding out the location of man and machines has been focused on in case of an accident during the mining process. The primary scope of the research is to guarantee that the projected position is near to the real place so that the trained model’s performance can be tested. The solution has been implemented by first proposing the MLAELD (Machine Learning Architecture for Excavators’ Location Detection), in which Bluetooth Low Energy (BLE) beacons have been used for tracking the live locations of excavators preceded by collecting the data of the signal strength mapping from multiple beacons at each specific point in a closed area. Second, machine learning techniques are proposed to develop and train multioutput regression models using linear regression, K-nearest neighbor regression, decision tree regression, and random forest regression. These techniques can predict the live locations of the required persons and machines with a high level of precision from the last beacon strengths received.",Wireless Communications and Mobile Computing,2022,10.1155/2022/9160031,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7b746df9505180a678d18f3fafb9200c3342e878,https://www.semanticscholar.org/paper/7b746df9505180a678d18f3fafb9200c3342e878,An Empirical Evaluation of Flow Based Programming in the Machine Learning Deployment Context,"As use of data driven technologies spreads, software engineers are more often faced with the task of solving a business problem using data-driven methods such as machine learning (ML) algorithms. Deployment of ML within large software systems brings new challenges that are not addressed by standard engineering practices and as a result businesses observe high rate of ML deployment project failures. Data Oriented Architecture (DOA) is an emerging approach that can support data scientists and software developers when addressing such challenges. However, there is a lack of clarity about how DOA systems should be implemented in practice. This paper proposes to consider Flow-Based Programming (FBP) as a paradigm for creating DOA applications. We empirically evaluate FBP in the context of ML deployment on four applications that represent typical data science projects. We use Service Oriented Architecture (SOA) as a baseline for comparison. Evaluation is done with respect to different application domains, ML deployment stages, and code quality metrics. Results reveal that FBP is a suitable paradigm for data collection and data science tasks, and is able to simplify data collection and discovery when compared with SOA. We discuss the advantages of FBP as well as the gaps that need to be addressed to increase FBP adoption as a standard design paradigm for DOA. CCS CONCEPTS • Software and its engineering → Software design tradeoffs; • Computing methodologies → Machine learning.",2022 IEEE/ACM 1st International Conference on AI Engineering – Software Engineering for AI (CAIN),2022,10.48550/arXiv.2204.12781,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9c22663b176c8329e993a5da6f82379b7918d17f,https://www.semanticscholar.org/paper/9c22663b176c8329e993a5da6f82379b7918d17f,A Review of Intrusion Detection Systems in RPL Routing Protocol Based on Machine Learning for Internet of Things Applications,"IPv6 routing protocol for low-power and lossy networks (RPL) has been developed as a routing agent in low-power and lossy networks (LLN), where nodes’ resource constraint nature is challenging. This protocol operates at the network layer and can create routing and optimally distribute routing information between nodes. RPL is a low-power, high-throughput IPv6 routing protocol that uses distance vectors. Each sensor-to-wire network router has a collection of fixed parents and a preferred parent on the path to the Destination-oriented directed acyclic graph (DODAG) graph’s root in steady-state. Each router part of the graph sends DODAG information object (DIO) control messages and specifies its rank within the graph, indicating its position within the network relative to the root. When a node receives a DIO message, it determines its network rank, which must be higher than all its parents’ rank, and then continues sending DIO messages using the trickle timer. As a result, DODAG begins at the root and eventually extends to encompass the whole network. This paper is the first review to study intrusion detection systems in the RPL protocol based on machine learning (ML) techniques to the best of our knowledge. The complexity of the new attack models identified for RPL and the efficiency of ML in intelligent and collaborative threats detection, and the issues of deploying ML in challenging LLN environments underscore the importance of research in this area. The analysis is done using research sources of “Google Scholar,” “Crossref,” “Scopus,” and “Web of Science” resources. The evaluations are assessed for studies from 2016 to 2021. The results are illustrated with tables and figures.",Wirel. Commun. Mob. Comput.,2021,10.1155/2021/8414503,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d5d4c5a7f6b4e8882878cd17c6c3032e71a657fb,https://www.semanticscholar.org/paper/d5d4c5a7f6b4e8882878cd17c6c3032e71a657fb,Leveraging Social Media Activity and Machine Learning for HIV and Substance Abuse Risk Assessment: Development and Validation Study,"Background Social media networks provide an abundance of diverse information that can be leveraged for data-driven applications across various social and physical sciences. One opportunity to utilize such data exists in the public health domain, where data collection is often constrained by organizational funding and limited user adoption. Furthermore, the efficacy of health interventions is often based on self-reported data, which are not always reliable. Health-promotion strategies for communities facing multiple vulnerabilities, such as men who have sex with men, can benefit from an automated system that not only determines health behavior risk but also suggests appropriate intervention targets. Objective This study aims to determine the value of leveraging social media messages to identify health risk behavior for men who have sex with men. Methods The Gay Social Networking Analysis Program was created as a preliminary framework for intelligent web-based health-promotion intervention. The program consisted of a data collection system that automatically gathered social media data, health questionnaires, and clinical results for sexually transmitted diseases and drug tests across 51 participants over 3 months. Machine learning techniques were utilized to assess the relationship between social media messages and participants' offline sexual health and substance use biological outcomes. The F1 score, a weighted average of precision and recall, was used to evaluate each algorithm. Natural language processing techniques were employed to create health behavior risk scores from participant messages. Results Offline HIV, amphetamine, and methamphetamine use were correctly identified using only social media data, with machine learning models obtaining F1 scores of 82.6%, 85.9%, and 85.3%, respectively. Additionally, constructed risk scores were found to be reasonably comparable to risk scores adapted from the Center for Disease Control. Conclusions To our knowledge, our study is the first empirical evaluation of a social media–based public health intervention framework for men who have sex with men. We found that social media data were correlated with offline sexual health and substance use, verified through biological testing. The proof of concept and initial results validate that public health interventions can indeed use social media–based systems to successfully determine offline health risk behaviors. The findings demonstrate the promise of deploying a social media–based just-in-time adaptive intervention to target substance use and HIV risk behavior.",Journal of medical Internet research,2021,10.2196/22042,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
34d6b71fc28975843f019a1222e593c589f8733b,https://www.semanticscholar.org/paper/34d6b71fc28975843f019a1222e593c589f8733b,Case study: Real-world machine learning application for hardware failure detection,"When designing microprocessors, engineers must verify whether the proposed design, defined in hardware description language, does what is intended. During this verification process, engineers run simulation tests and can fix bugs if the tests have failed. Due to the complexity of the design, the baseline approach is to provide random stimuli to verify random parts of the design. However, this method is time-consuming and redundant especially when the design becomes mature and thus failure rate is low. To increase efficiency and detect failures faster, it is possible to train machine learning models by using previously run tests, and assess the likelihood of failure of new test candidates before running them. This way, instead of running random tests agnostically, engineers use the model prediction on a new set of test candidates and run a subset of them (i.e., ""filtering"" the tests) that are more likely to fail. Due to the severe imbalance (1% failure rate), I trained an ensemble of supervised (classification) and unsupervised (outlier detection) models and used the union of the prediction from both models to catch more failures. The tool has been deployed in an internal high performance computing (HPC) cluster early this year, as a complementary workflow which does not interfere with the existing workflow. After the deployment, I found performance instability in post-deployment performance and ran various experiments to address the issue, such as by identifying the effect of the randomness in the test generation process. In addition to introducing the relatively new data-driven approach in hardware design verification, this study also discusses the details of post-deployment evaluation such as retraining, and working around real-world constraints, which are sometimes not discussed in machine learning and data science research.",Proceedings of the 18th Python in Science Conference,2019,10.25080/MAJORA-7DDC1DD1-001,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
287b17f660bb39916dc6c15950ef2f834d41c4e9,https://www.semanticscholar.org/paper/287b17f660bb39916dc6c15950ef2f834d41c4e9,High precision machine learning-enabled ECG algorithm for predicting sites of idiopathic ventricular arrhythmia origin,"
 
 
 Radiofrequency catheter ablation (CA) is an efficient antiarrhythmic treatment with a class I indication for idiopathic ventricular arrhythmia (IVA). The accurate prediction of the origins of IVA can significantly increase the procedure success rate, reduce operation duration and decrease the risk of complications. The present work proposes an ECG analysis algorithm to estimate 21 possible origins of idiopathic ventricular arrhythmia at a clinical-grade level accuracy, which include left coronary cusp (LCC), right coronary cusp (RCC), aortomitral continuity (AMC), summit, LCC-RCC commissure, left His bundle, mitral valve (MV), left septal including left anterior fascicle (LAF), left posterior fascicle (LPF), left anterior papillary muscle (LAPM), left posterior papillary muscle (LPPM), anterior cusp (AC), left cusp (LC), right cusp (RC), RVOT septal, free wall, right His bundle, tricuspid valve (TV), and right anterior papillary muscle (RAPM).
 
 
 
 A total of 18,612 ECG recordings extracted from 545 patients who underwent successful CA to treat IVA were proportionally sampled into training, validation and testing cohorts. We designed four classification schemes responding to different hierarchical levels of the possible IVA origins. The first scheme will help the operators to figure out the origin from epicardium of left ventricular summit, right, and left ventricle. The second one can separate origins from left/right outflow tract and left/right non-out flow tract, respectively. The third one is able to predict 18 anatomical locations, and the fourth scheme can distinguish 21 possible sites. For every classification scheme, we compared 98 distinct machine learning models with optimized hyperparameter values obtained through extensive grid search and reported an optimal algorithm with the highest accuracy scores attained on the validation cohorts.
 
 
 
 In the first classification scheme used to predict right ventricular endocardium, left ventricular endocardium, and epicardium of left ventricular summit, the model achieved an accuracy of 99.79 (99.41–99.89) and a F1-score of 99.84 (99.6–99.96). For scheme 2, the proposed method reached an accuracy of 99.62 (99.09–99.78) and a F1-score of 99.42 (98.79–99.75). For scheme 3, the model achieved an accuracy of 97.78 (96.76–98.41), a F1-score of 97.74 (94.15–99.73), and an adjusted accuracy of 98.53 (98.33–99.15). For scheme 4 that can distinguish 21 origin sites, the proposed model attained an accuracy of 98.24 (97.36–98.71), a F1-score of 98.56 (97.88–99.12) and an adjusted accuracy of 98.75 (98.35–99.38).
 
 
 
 The proposed machine learning model can be immediately and effortlessly deployed to electrophysiology labs allowing cardiologists to predict the exact origins of arrhythmia and provide an optimum treatment plan both before and during the CA procedure. This approach will significantly reduce the CA procedure duration and the risk of complications.
 
 
 
 Type of funding sources: Foundation. Main funding source(s): 2020 Natural Science Foundation of Zhengjiang Province Confusion matrix for classification schemes
",European Heart Journal,2021,10.1093/eurheartj/ehab724.0303,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a2899d33c84761af206a21c214a72862d5ebfb5d,https://www.semanticscholar.org/paper/a2899d33c84761af206a21c214a72862d5ebfb5d,ARGUSEYES: Screening Native Machine Learning Pipelines,"Software systems that learn from data are being deployed in increasing numbers in real-world application scenarios. It is a difficult and tedious task to ensure at development time that the end-to-end ML pipelines for such applications adhere to sound experimentation practices, such as the strict isolation of train and test data. Furthermore, there is a dire need to enforce legal and ethical compliance in automated decision-making with ML. For example, in order to determine whether a model works equally well for different groups. For enforcing privacy rights (such as the ‘right to be forgotten’ [1]), we must identify which models actually consumed the user’s data for model training, in order to retrain them without this data. Moreover, model predictions can be corrupted due to undetected data distribution shift, e.g., when the train/test data was incorrectly sampled [2] or changed over time (covariate shift) or when the distribution of the target label changed (label shift) [3]. Data scientists also require support for uncovering erroneous data, e.g., to identify samples that are not helpful for the classifier and potentially dirty or mislabeled [4] or to identify subsets of data for which a model does not work well. Towards automated low-effort screening of ML pipelines. Most of the listed issues are typically addressed manually in an ad-hoc way and require a lot of expertise and extra code. In many cases, there is no system support for detecting particular issues, and typically, data has to be integrated first, as common libraries assume the input to be in a single table. Furthermore, specialised solutions are often incompatible with popular libraries in the ecosystem. This situation is in stark contrast to the software engineering world, with established best practices and infrastructure for testing and continuous integration. Provenance is all you need. We find that we can automate the detection of many common correctness issues in ML pipelines with access to (i) the materialised artifacts of a pipeline (its input relations, and its outputs, e.g., the feature matrices, labels, and predictions of a classifier) as well as (ii) their why-provenance [5] (e.g., the information which input records were used to compute a particular output). This allows us to design lightweight screening techniques with low invasiveness for natively written ML pipelines, which combine code from different libraries from the rapidly evolving data science ecosystem. Pipeline screening with ARGUSEYES. Based on these insights, we present our ARGUSEYES prototype, which operates on a natively written ML pipeline in Python, extracts intermediate results and provenance (in the form of provenance polynomials [6]) with MLINSPECT [7], and infers the semantics of their artifacts based on predefined “templates” (e.g., for a classification task). Our prototype enables the automatic detection of common issues w.r.t. best practices in ML, and the computation of metadata such as group fairness metrics, record usage by the model, or data valuation with Shapley values. Our prototype handles classification pipelines natively written in pandas/sklearn and keras, stores their artifacts and run data via mlflow [8], and can be easily hooked into continuous integration workflows. Current State & Future Work. An abstract about this work has been accepted at CIDR 2022. We provide a prototypical implementation of our proposed approach at https://github. com/schelterlabs/arguseyes. In the future, we plan to add support for additional pipeline types (e.g., clustering, recommendation, learning-to-rank), and more detection techniques for correctness violations, especially over multiple pipeline executions. A current limitation of our approach is that we rely on the pipeline being written based on many declarative constructs from pandas and scikit-learn, which might often not be the case for data science code. We intend to increase the robustness of MLINSPECT and ARGUSEYES against such scripts. This work was supported by Ahold Delhaize. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their employers.",,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0fdc9fc8ecff787abaaee53680b8d91e4bf9d3e9,https://www.semanticscholar.org/paper/0fdc9fc8ecff787abaaee53680b8d91e4bf9d3e9,One Explanation Does Not Fit All: The Promise of Interactive Explanations for Machine Learning Transparency,"The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations – a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an Kacper Sokol Department of Computer Science, University of Bristol Bristol, United Kingdom E-mail: K.Sokol@bristol.ac.uk Peter Flach Department of Computer Science, University of Bristol Bristol, United Kingdom E-mail: Peter.Flach@bristol.ac.uk impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human-machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms.",ArXiv,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
22e7fcc6173af8194a7ea234fa6f2c6ad4c7cb79,https://www.semanticscholar.org/paper/22e7fcc6173af8194a7ea234fa6f2c6ad4c7cb79,Machine learning for multi-fidelity scale bridging and dynamical simulations of materials,"Molecular dynamics (MD) is a powerful and popular tool for understanding the dynamical evolution of materials at the nano and mesoscopic scales. There are various flavors of MD ranging from the high fidelity albeit computationally expensive ab-initio MD to relatively lower fidelity but much more efficient classical MD such as atomistic and coarse-grained models. Each of these different flavors of MD have been independently used by materials scientists to bring about breakthroughs in materials discovery and design. A significant gulf exists between the various MD flavors, each having varying levels of fidelity. The accuracy of DFT or ab-initio MD is generally much higher than that of classical atomistic simulations which is higher than that of coarse-grained models. Multi-fidelity scale bridging to combine the accuracy and flexibility of ab-initio MD with efficiency classical MD has been a longstanding goal. The advent of big-data analytics has brought to the forefront powerful machine learning methods that can be deployed to achieve this goal. Here, we provide our perspective on the challenges in multi-fidelity scale bridging and trace the developments leading up to the use of machine learning algorithms and data-science towards addressing this grand challenge.",,2020,10.1088/2515-7639/ab8c2d,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
230a2113fd944924bfd50329d86679388f650b0c,https://www.semanticscholar.org/paper/230a2113fd944924bfd50329d86679388f650b0c,Machine Learning in tunnelling – Capabilities and challenges,"Digitalization will change the way of gathering geological data, methods of rock classification, application of design analyses in the field of tunnelling as well as tunnel construction and maintenance processes. In recent years, a rapid increase in the successful application of digital techniques (Building Information Modelling and Machine Learning (ML)) for a variety of challenging tasks has been observed. Driven by the increasing overall amount of data combined with the easy availability of more computing power, a sharp increase in the successful deployment of techniques of ML has been seen for different tasks. ML has been introduced in many sciences and technologies and it has finally arrived in the fields of geotechnical engineering, tunnelling and engineering geology, although still not as far developed as in other disciplines. This paper focuses on the potential of ML methods for geotechnical purposes in general and tunnelling in particular. Applications such as automatic rock mass behaviour classification using data from tunnel boring machines (TBM), updating of the geological prognosis ahead of the tunnel face, data driven interpretation of 3D displacement data or fully automatic tunnel inspection will be discussed.",Geomechanics and Tunnelling,2020,10.1002/geot.202000001,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
eb2d52c6b146e9d8a11f2b61ce8aaef2854ee04b,https://www.semanticscholar.org/paper/eb2d52c6b146e9d8a11f2b61ce8aaef2854ee04b,"The past, the present and the future of machine learning and artificial intelligence in anesthesia and Post Anesthesia Care Units (PACU).","Over the past decade, artificial intelligence (AI) has largely penetrated our daily life. Hence, our expectations regarding clinical AI are very high. However, in healthcare and especially in perioperative medicine, the impact of AI is still relatively limited. This is in contrast with an exponential increase in the academic investment and productivity in this field of data science. Implementation challenges are numerous, including technological and regulatory challenges. In addition, the clinical and economic impact of deploying clinical AI at scale is still lacking. However, if these implementation challenges are properly addressed, the potential of AI to profoundly transform our practice is real. If successfully implemented and integrated into the clinical workflow, AI-assisted perioperative medicine could become more preventative and personalized. However, AI implementation is not the final step. New challenges will follow implementation including algorithm maintenance, continuous monitoring, and improvement.",Minerva anestesiologica,2022,10.23736/S0375-9393.22.16518-1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b3623d441e4a929895497b24b29a8103ad726ddd,https://www.semanticscholar.org/paper/b3623d441e4a929895497b24b29a8103ad726ddd,Machine Learning Technologies for Bakery Management Decisions,"The paper discusses using Deep Stream technologies in tasks for predicting best locations for deployment the bakeries. Such methods provides the calculation of people going through possible bakery and use convolutional neural networks for detection people and some effective algorithms for counting. The proposed solution allows deploying successful bakeries and keeping money in real production. Furthermore a lot of applied data science models were used for data analysis in these conditions. The paper discusses in detail the regression, factorial, cluster and discriminant analysis on the example of real data on the operation of a chain of bakeries with changes to preserve trade secrets. The analysis made it possible to simplify the decision-making process for managers among many factors. Moreover, the proposed method made it possible to predict profitability when opening a new point and explore various models of its development. Comparison results are provided for 3 models. The choice was made in favor of one of them. This choice resulted in the opening of a profitable bakery with a high profit margin for the retail market. The regression, factor and cluster analysis results show good opportunities to apply the results of the analysis for making management decisions when choosing the location of bakeries.",2022 24th International Conference on Digital Signal Processing and its Applications (DSPA),2022,10.1109/dspa53304.2022.9790767,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
53dee47b72ca96e33a06b1b785ee694f4b310bf3,https://www.semanticscholar.org/paper/53dee47b72ca96e33a06b1b785ee694f4b310bf3,Interplay of Machine Learning and Software Engineering for Quality Estimations,"In this era, the agile mindset has innovated the traditional software engineering (SE) process through the integration of DevOps flow engines, scrum iterations, and automation of continuous integration (CI) and continuous deployment (CD) cycles. However, the CI/CD integration requires manual code-revisions and refactoring at large scales. Recently, machine-learning (ML) is employed in SE that allows legacy codes to be highly dynamic, less coupling in related modules, automatic code versioning, and refactoring, with less coupling among related modules. However, over time, ML models tend to become bulky, with increasing monotonic losses during model training. To address this, SE techniques like code revisions are employed over ML codes to allow low-order training losses, that enables seamless and precise workflow structures. Motivated from the aforementioned discussions, the paper presents a systematic review of the close interplay of SE and ML and possible interactions in different applications. Suitable research questions and case studies are presented for possible adoption scenarios that depict the close ML-SE interplay share with each other, with the concluding remarks. The paper forms useful insights for ML engineers, data science practitioners, and SE quality estimators towards the building of efficient and scalable software solutions.","2020 International Conference on Communications, Computing, Cybersecurity, and Informatics (CCCI)",2020,10.1109/CCCI49893.2020.9256507,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
022d778a8681702b634d17fa3c7fc0c05574b1df,https://www.semanticscholar.org/paper/022d778a8681702b634d17fa3c7fc0c05574b1df,Evaluation of Algorithm-Based Fault Tolerance for Machine Learning and Computer Vision under Neutron Radiation,"In the past decade, there has been a push for deployment of commercial-off-the-shelf (COTS) avionics due in part to cheaper costs and the desire for more performance. Traditional radiation-hardened processors are expensive and only provide limited processing power. With smaller mission budgets and the need for more computational power, low-cost and highperformance COTS solutions become more attractive for these missions. Due to the computational capacity enhancements provided by COTS technology, machine-learning and computer-vision applications are now being deployed on modern space missions. However, COTS electronics are highly susceptible to radiation environments. As a result, reliability in the underlying computations becomes a concern. Matrix multiplication is used in machine-learning and computer-vision applications as the main computation for decisions, making it a critical part of the application. Therefore, the large time and memory footprint of the matrix multiplication in machine-learning and computer-vision applications makes them even more susceptible to single-event upsets. In this paper, algorithm-based fault tolerance (ABFT) is investigated to mitigate silent data errors in machine learning and computer vision. ABFT is a methodology of data error detection and correction using information redundancy contained in separate data structures from the primary data. In matrix multiplication, ABFT consists of storing checksum data in vectors separate from the matrix to use for error detection and correction. Fault injection into a matrix-multiplication kernel was performed prior to irradiation. Irradiation was then performed on the kernel under wide-spectrum neutrons at Los Alamos Neutron Science Center to observe the mitigation effects of ABFT. Fault injections targeted towards the general-purpose registers show a $48 \times$ reduction in data errors using data-error mitigation with ABFT with a negligible change in run-time. Cross-section results from irradiation show a $5.3 \times$ improvement in reliability of using ABFT as opposed to no mitigation with a $> 99.9999$ confidence level. The results of this experiment demonstrate that ABFT is a viable solution for run-time error correction in matrix multiplication for machine-learning and computer-vision applications in future spacecraft.",2020 IEEE Aerospace Conference,2020,10.1109/AERO47225.2020.9172799,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1f0721a3f54a1f56e647a0e02014be06acc9c5a2,https://www.semanticscholar.org/paper/1f0721a3f54a1f56e647a0e02014be06acc9c5a2,Proactive machine-learning-based approaches to vaccine hesitancy for a potential SARS-Cov-2 vaccine,"Abstract Background Polls in the US and France found a concerning share of respondents (50% and 26%, respectively) stating that they are not committed to receiving or simply saying they would not accept vaccination against SARS-CoV-2[1][2]. In this context, it is worth revisiting machine-learning approaches to predicting vaccine hesitancy - such as the one developed for MMR vaccination at the individual level by Bell et al.[4] amid Europe's recent measles epidemic - as a first step of a proactive policy. Proposed Methods and Expectations In the MMR case, using 44K child-healthcare records including vaccination data, a LASSO logistic regression based on a low number of attributes of the child and his or her family and community produced risk scores, making them readily interpretable by healthcare professionals. Since children are regularly the target population for immunization efforts, recent pediatric and school-age records, in concert with other social and medical features, could provide suitable input for algorithms estimating the probability of refusal of a SARS-Cov-2 vaccine for other members of a household. This is contingent upon data on acceptance and refusal being collected and paired with these inputs in areas where the vaccine will first be deployed (if developed), which gives another argument for such timely and organized data collection. Speculating about the future performance of a new model trained on truly “out of sample” data specific to a novel problem should be avoided. Benchmarks for success in terms of measures such as precision and recall, however, have to be set in light of the gravity of the issue and other available methods. Finally, any model trained with the aim of predicting vaccine hesitancy for a SARS-Cov-2 vaccine should be coupled with tailored communication policies tested as part of the first vaccination efforts. Cornwall, Science Mag, Jun 30, 2020 Peretti-Watel et al., The Lancet, May 20, 2020 Bell et al., IEEE ICHI, 2019 Key messages Data on acceptance and refusal for the first (potential) SARS-Cov-2 vaccination campaigns should be collected and matched with health records to enable models predicting vaccine hesitancy. The output of machine learning models predicting vaccine hesitancy should be paired with tested policies respectfully communicating reliable information on vaccination.",The European Journal of Public Health,2020,10.1093/eurpub/ckaa165.035,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e358eb83a90666ef3f19a2d07be3e86795a54b7f,https://www.semanticscholar.org/paper/e358eb83a90666ef3f19a2d07be3e86795a54b7f,Predictive Analytics with Microsoft Azure Machine Learning,,Apress,2015,10.1007/978-1-4842-1200-4,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d8a8f130905bad5a745e5ff8734e2bcf901ef9b3,https://www.semanticscholar.org/paper/d8a8f130905bad5a745e5ff8734e2bcf901ef9b3,Machine Learning-based Optimal VNF Deployment Prediction,"Network Function Virtualization (NFV) environment can deal with dynamic changes in traffic status with appropriate deployment and scaling of Virtualized Network Function (VNF). However, determining and applying the optimal VNF deployment is a complicated and difficult task. In particular, it is necessary to predict the situation at a future point because it takes for the process to be applied and the deployment decision to the actual NFV environment. In this paper, we randomly generate service requests in Multiaccess Edge Computing (MEC) topology, then obtain training data for machine learning model from an Integer Linear Programming (ILP) solution. We use the simulation data to train the machine learning model which predicts the optimal VNF deployment in a predefined future point. The prediction model shows the accuracy over 90% compared to the ILP solution in a 5-minute future time point. ※ 본 연구는 과학기술정보통신부 및 정보통신기술진흥센터의 정보통신·방송 연구개발사업의 일환으로 수행하였음.(No. 2018-0-00749, 인공지능 기반 가상 네트워크 관리기술 개발) ※ 본 연구는 과학기술정보통신부 및 정보통신기획평가원의 대학 ICT 연구센터지원사업의 연구결과로 수행되었음 (IITP-2020-2017-0-01633) w First Author : Pohang University of Science and Technology Department of Computer Science Engineering, limjiyoon@postech.ac.kr °Corresponding Author : Pohang University of Science and Technology Department of Computer Science Engineering, sinjint,hosewq@postech.ac.kr *Pohang University of Science and Technology Department of Computer Science Engineering, {sinjint,hosewq,jhyoo78,jwkhong}@postech.ac.kr 논문번호:KNOM2020-01-09, Received July 17, 2020; Revised July 30, 2020; Accepted August 16, 2020 기계학습 기반 VNF 최적 배치 예측 기술연구",,2020,10.22670/KNOM.2020.23.1.34,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
12cd81c083ce4f357991525fa09f5dbad86d33b7,https://www.semanticscholar.org/paper/12cd81c083ce4f357991525fa09f5dbad86d33b7,Guest Editors' Introduction to the Special Issue on Machine Learning Architectures and Accelerators,"DEEP learning or deep neural networks (DNNs), as one of the most powerful machine learning techniques, has achieved extraordinary performance in computer vision and surveillance, speech recognition and natural language processing, healthcare and disease diagnosis, etc. Various forms of DNNs have been proposed, includingConvolutional Neural Networks, Recurrent Neural Networks, Deep Reinforcement Learning, Transformer model, etc. Deep learning exhibits an offline training phase to derive theweight parameters from an excessive training dataset, as well as an online inference phase to perform classification/prediction/perception/control tasks based on the trained model. Recently, the online learning (e.g., federated learning, transfer learning) capabilities of DNNs are also being investigated such that DNNs can adapt to new situations encountered during actual system operation and time-varying scenarios. Deep learning models are both computation and storageintensive since it is necessary to extract high-level features for optimization. This poses significant challenges during both the inference and training phases of the application. The inference is expected to be deployed onto mobile platforms, embedded and IoT devices with restricted power and form factor budget, but with real-time performance requirement. The training phase of the application is highly computation and data-intensive, and thus software/algorithm optimization aswell as hardware acceleration re critically required. Prior research has investigated software/algorithm optimization which includes DNNmodel architecture search for computation/storage reduction (e.g., depthwise-separate convolutions), model compression (weight pruning, quantization, matrix transformation, etc.), compiler-assisted optimizations, parallel computing techniques such as data parallelism and model parallelism, distributed training algorithms, federated learning, to name a few. Some of the prior research investigating hardware acceleration includes CPU/GPU-based accelerations, FPGAs, dedicated ASIC architectures, to more recently emerging in-memory computing techniques. Computer architecture research plays a key role here, both in terms of the general-purpose architectures that accommodate awide range of hardware platforms, DNN types, and applications, as well as the specialized architectures targeting the most advanced DNN structure and specialized, critical applications. This Special Issue of IEEETransactions onComputers aims to find a convergence of software and hardware/architecture. It aims at DNN algorithms, parallel computing, and compiler code generation techniques that are hardware/architecture friendly, as well as computer architectures that are universal and consistently highly performant on a wide range of DNN algorithms and applications. In this co-design and co-optimization frameworkwe canmitigate the limitation of investigating in only a single direction, shedding some light on the future of embedded, ubiquitous artificial intelligence. Following an open call for papers, we received 56 submissions from authors in 20 different countries and on a broad range of hardware/software aspects related to deep learning/artificial intelligence acceleration. After a preliminary screening, each submission has been assigned to at least three reviewers. Eventually 12 manuscripts have been selected to form this special issue of IEEE Transactions on Computers. Below, we provide a brief explanation of the contribution of the paper for the special issue. The first paper, “Crane: Mitigating accelerator under-utilization caused by sparsity irregularities in CNNs” by Y. Guan et al., proposes a method of load-balancing based on a workload stealing technique, in order to mitigate the problem of computation resource under-utilization in sparse CNN accelerators. Based on this method, the authors present an accelerator, called Crane, which addresses all kinds of sparsity irregularities in CNNs. Experimental results show that Crane improves performance by 27%–88% and reduces energy consumption by 16%–48%, respectively. The second paper, “CIMAT: A compute-in-memory architecture for on-chip training based on transpose SRAM arrays” by H. Jiang et al., designs the data flow for the backpropagation process and weight update to support the onchip training based on CIM (compute-in-memory) approach. The authors utilize themature and advanced CMOS technology at 7nm to design the CIM architecture with 7T transpose SRAMarray that supports bidirectional parallel read. The third paper, “MViD: Sparse matrix-vector multiplication in mobile DRAM for accelerating recurrent neural networks” by B. Kim et al., proposes a main-memory architecture called MViD, which performs MV-mul (matrixXuehai Qian is with Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA. E-mail: xuehai.qian@usc.edu. Yanzhi Wang is with Electrical and Computer Engineering, Northeastern University, Boston, MA. E-mail: yanz.wang@northeastern.edu. Avinash Karanth is with School of Electrical Engineering and Computer Science, Ohio University, Athens, OH. E-mail: karanth@ohio.edu.",IEEE Trans. Computers,2020,10.1109/tc.2020.2997574,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8d985c7cfde1b760e685c4b09604593f2ca714be,https://www.semanticscholar.org/paper/8d985c7cfde1b760e685c4b09604593f2ca714be,Machine Learning Approach for Agriculture IoT using SVM&ANN,"The rapid growth of Internet of Things (IoT) devices in cities, homes, buildings, industries, health care, automotive and also in agricultural farms have paved the way for deployment of wide range of sensors in them. In return IoT turns out to be the major contributor of new data in any of these fields. A data driven farm management techniques will in turn help in increasing the agricultural yield by planning the input cost, reducing loss and efficient use of resources. IoT on top of increasing the volume of data it also give rise to big data with varied characteristics based on time and locality. To increase the agricultural yield by smart farm management astute analysis and processing of the data generated becomes imperative. With high performance computing at machine learning has created new opportunities for data intensive science. Machine learning will help the farm management system to achieve its goal by exploiting the data that is continuously made available with the help of Agricultural IoT (AIoT) platform and helps the farmer with insights, decisive action and support.",,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c64aed1549ac94a5110ca9fb9f0da784355c5a4b,https://www.semanticscholar.org/paper/c64aed1549ac94a5110ca9fb9f0da784355c5a4b,Democratisation of Usable Machine Learning in Computer Vision,"Many industries are now investing heavily in data science and automation to replace manual tasks and/or to help with decision making, especially in the realm of leveraging computer vision to automate many monitoring, inspection, and surveillance tasks. This has resulted in the emergence of the 'data scientist' who is conversant in statistical thinking, machine learning (ML), computer vision, and computer programming. However, as ML becomes more accessible to the general public and more aspects of ML become automated, applications leveraging computer vision are increasingly being created by non-experts with less opportunity for regulatory oversight. This points to the overall need for more educated responsibility for these lay-users of usable ML tools in order to mitigate potentially unethical ramifications. In this paper, we undertake a SWOT analysis to study the strengths, weaknesses, opportunities, and threats of building usable ML tools for mass adoption for important areas leveraging ML such as computer vision. The paper proposes a set of data science literacy criteria for educating and supporting lay-users in the responsible development and deployment of ML applications.",CVPR 2019,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
41b54a00afa4518e00bfe5d4afe89d9291f9019c,https://www.semanticscholar.org/paper/41b54a00afa4518e00bfe5d4afe89d9291f9019c,Exploiting Scalable Machine-Learning Distributed Frameworks to Forecast Power Consumption of Buildings,"The pervasive and increasing deployment of smart meters allows collecting a huge amount of fine-grained energy data in different urban scenarios. The analysis of such data is challenging and opening up a variety of interesting and new research issues across energy and computer science research areas. The key role of computer scientists is providing energy researchers and practitioners with cutting-edge and scalable analytics engines to effectively support their daily research activities, hence fostering and leveraging data-driven approaches. This paper presents SPEC, a scalable and distributed engine to predict building-specific power consumption. SPEC addresses the full analytic stack and exploits a data stream approach over sliding time windows to train a prediction model tailored to each building. The model allows us to predict the upcoming power consumption at a time instant in the near future. SPEC integrates different machine learning approaches, specifically ridge regression, artificial neural networks, and random forest regression, to predict fine-grained values of power consumption, and a classification model, the random forest classifier, to forecast a coarse consumption level. SPEC exploits state-of-the-art distributed computing frameworks to address the big data challenges in harvesting energy data: the current implementation runs on Apache Spark, the most widespread high-performance data-processing platform, and can natively scale to huge datasets. As a case study, SPEC has been tested on real data of an heating distribution network and power consumption data collected in a major Italian city. Experimental results demonstrate the effectiveness of SPEC to forecast both fine-grained values and coarse levels of power consumption of buildings.",Energies,2019,10.3390/EN12152933,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
58be9d73a63ca59380a767a964e829e819c9bd5c,https://www.semanticscholar.org/paper/58be9d73a63ca59380a767a964e829e819c9bd5c,Application of Machine Learning Techniques As a Means of Mooring Integrity Monitoring,"
 There is growing importance in the offshore floating production sector to develop reliable and robust means of continuously monitoring the integrity of mooring systems for FPSOs and FPUs, particularly in light of the upcoming introduction of API-RP-2MIM. Here, the limitations of the current range of monitoring techniques are discussed, including well established technologies such as load cells, sonar, or visual inspection, within the context of the growing mainstream acceptance of data science and machine learning. Due to the large fleet of floating production platforms currently in service, there is a need for a readily deployable solution that can be retrofitted to existing platforms to passively monitor the performance of floating assets on their moorings, for which machine learning based systems have particular advantages.
 An earlier investigation conducted in 2016 on a shallow water, single point moored FPSO employed host facility data from in-service field measurements before and after a single mooring line failure event. This paper presents how the same machine learning techniques were applied to a deep water, semi taut, spread moored system where there was no host facility data available, therefore requiring a calibrated hydrodynamic numerical model to be used as the basis for the training data set.
 The machine learning techniques applied to both real and synthetically generated data were successful in replicating the response of the original system, even with the latter subjected to different variations of artificial noise. Furthermore, utilizing a probability-based approach, it was demonstrated that replicating the response of the underlying system was a powerful technique for predicting changes in the mooring system.","Volume 3: Structures, Safety, and Reliability",2019,10.1115/omae2019-96411,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5e2d0f0f96eb25a5da9200e0514829efaa0fb566,https://www.semanticscholar.org/paper/5e2d0f0f96eb25a5da9200e0514829efaa0fb566,Blueprinting the Workflow of Medical Diagnosis through the Lens of Machine Learning Perspective,"The association of machine learning into medical data and healthcare communities embraces substantial improvement in both health care and machine learning itself. Many companies are racing to integrate machine learning into medical diagnosis process that boosts the automatic medical decision, reducing the inferior effects of data overload and increasing the accurate prediction and time effectiveness. It is one of today's most rapidly growing technical fields, lying at the intersection between health care and computer science in general. Thus, there is an urgent need to optimize medical processes, guidelines and workflows to increase the workload capacity while reducing costs and improving efficiencies. Moreover, no medical doctor or experts can manually keep pace today due to increasingly large and complex datasets. In this paper, the authors aim at addressing the mentioned issue by proposing a workflow of medical diagnosis through the lens of the machine learning perspective. An intensive comparison has been conducted applying 5 well-known machine learning algorithms on 8 real-world categorized datasets. A mobile application has been also deployed to enhance the incorporation from hospital experts.",2019 International Conference on Advanced Computing and Applications (ACOMP),2019,10.1109/ACOMP.2019.00011,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8198db8fbf5beffef9f3862f07d5f494da8123f5,https://www.semanticscholar.org/paper/8198db8fbf5beffef9f3862f07d5f494da8123f5,Machine learning bias game,"Machine learning is the science that helps computers uncover data patterns and relationships. It is a powerful tool that studies how computers simulate or implement human learning behaviors to acquire new knowledge or skills and reorganize existing knowledge structures to continuously improve their performance. But as these systems become more complex and powerful, researchers have found that the widespread use of artificial intelligence systems can cause some erroneous decisions. However, it is not the use of these AI algorithms themselves that is an issue, but rather that human biases are incorporated into the resulting models, and into the systems that use these models. This paper focuses on the concept of how human bias effect on machine learning, analyzes the main reasons for its formation through two practical cases, and the impact of this bias on machine learning algorithms and the impact on practical engineering applications. Then we try to design a game in which we use regional crime rate prediction and the mathematical model of police deployment to combine the biased concepts we studied to show the impact of bias in the game and discuss how to eliminate this bias. Through the above work, we could get a better understanding of the concept of bias, and reflect on the incompleteness and defects of machine learning algorithms, thereby further improving the robustness, efficiency and reliability of machine learning algorithms.",,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5200a4605bdec47a45fc223d1f7aa5969dfdcc05,https://www.semanticscholar.org/paper/5200a4605bdec47a45fc223d1f7aa5969dfdcc05,Machine Learning with R - Rattle Package,"The objective of this book is to address the needs of students in using R-Rattle software for data mining and data modelling techniques. The pacing and content of this book is set in such a way so that it gives user friendly approach to students and other learners. R is a statistical computing environment. It is free (open source) software for statistical computation and graphics and a computer language designed for typical statistical and graphical applications. R is gaining a wider acceptance because it includes the ability to save and run commands stored in script files, and an integrated editor in the R Graphical User Interface (R-GUI). It is available for most platforms including unix/linux, PC, and Macintosh platforms. Rattle package is hosted on bitbucket where anyone can contribute to the package. It deploys more than 100 packages to build robust models in data science. It is the only GUI in R for machine learning and data mining. It presents statistical and visual summaries of data, transforms the data, builds the supervised and unsupervised model.",,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5dc8073e67a71cf2b5360d5c286d6e668b34cd80,https://www.semanticscholar.org/paper/5dc8073e67a71cf2b5360d5c286d6e668b34cd80,Using Machine Learning and Genetic Algorithms to Optimize Scholarship Allocation for Student Yield,"Effectively estimating student enrollment and recruiting students is critical to the success of any university. However, despite having an abundance of data and researchers at the forefront of data science, universities are not fully leveraging machine learning and data mining approaches to improve their enrollment management strategies. In this project, we use data at a large, public university to increase their student enrollment. We do this by first predicting the enrollment of admitted first-year, first-time students using a suite of machine learning classifiers (AUROC = 0.85). We then use the results from these machine learning experiments in conjunction with genetic algorithms to optimize scholarship disbursement. We show the effectiveness of this approach using actual enrollment metrics. Our optimized model was expected to increase enrollment yield by 15.8% over previous disbursement strategies. After deploying the model and confirming student enrollment decisions, the university actually saw a 23.3% increase in enrollment yield. This resulted in millions of dollars in additional annual tuition revenue and a commitment by the university to employ the method in subsequent enrollment cycles. We see this as a successful case study of how educational institutions can more effectively leverage their data.",,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1e5e0623e764d00285d7797cdd4159d8ea750f9c,https://www.semanticscholar.org/paper/1e5e0623e764d00285d7797cdd4159d8ea750f9c,Deployment on Machine Learning Algorithms on Cloud,Cloud-based systems can provide access to a large pool of data and computational resources to consumers and developers. Everything can be done on a cloud from running applications to store data off-site. This project harnessed the collaborative nature and mobility of cloud to provide something to everyone. The idea of this project is to use cloud services to provide machine learning services to consumers where one can access trained and tested models or train and test a model of their own. Our aim is to provide these facilities to every data scientist and developer as a fully organized and supervised service. This platform enables users to access and use other deployed machine learning and data science models/tools to meet their needs and expertise. The main focus is on the improvement of delivering machine learning-based services over cloud technology.,,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9ff8b9bc8afcddab1123101152c2d7adfd19ae24,https://www.semanticscholar.org/paper/9ff8b9bc8afcddab1123101152c2d7adfd19ae24,Using Machine Learning to Design Precision Digital Engagement,"MEMOTEXT has developed the following 6step Digital Health Engagement Methodology for Data Mining (DHEM-DM) based on the systematic process and experience of designing, developing, and deploying personalized digital health interventions across chronic disease and patient-specific domains. This structure approach draws on aspects from the Cross Industry Standard Process Data Mining (CRISPDM), the Analytics Solutions Unified Method Data Mining (ASUM-DM) and the Team Data Science Process (TDSP) methodologies but is specifically tailored for a data-driven approach in designing digital health interventions to help patients meet their health goals and produce sustained behaviour change.",,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
302a251058006493ef50cc966c78cdc88c80a2c7,https://www.semanticscholar.org/paper/302a251058006493ef50cc966c78cdc88c80a2c7,Predictive Analytics with Microsoft Azure Machine Learning,,Apress,2015,10.1007/978-1-4842-0445-0,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3b6319d92e77b2b157103d357b1d398bbddd4617,https://www.semanticscholar.org/paper/3b6319d92e77b2b157103d357b1d398bbddd4617,Leveraging Social Media Activity and Machine Learning for HIV and Substance Abuse Risk Assessment: Development and Validation Study (Preprint),"
 BACKGROUND
 Social media networks provide an abundance of diverse information that can be leveraged for data-driven applications across various social and physical sciences. One opportunity to utilize such data exists in the public health domain, where data collection is often constrained by organizational funding and limited user adoption. Furthermore, the efficacy of health interventions is often based on self-reported data, which are not always reliable. Health-promotion strategies for communities facing multiple vulnerabilities, such as men who have sex with men, can benefit from an automated system that not only determines health behavior risk but also suggests appropriate intervention targets.
 
 
 OBJECTIVE
 This study aims to determine the value of leveraging social media messages to identify health risk behavior for men who have sex with men.
 
 
 METHODS
 The Gay Social Networking Analysis Program was created as a preliminary framework for intelligent web-based health-promotion intervention. The program consisted of a data collection system that automatically gathered social media data, health questionnaires, and clinical results for sexually transmitted diseases and drug tests across 51 participants over 3 months. Machine learning techniques were utilized to assess the relationship between social media messages and participants' offline sexual health and substance use biological outcomes. The F1 score, a weighted average of precision and recall, was used to evaluate each algorithm. Natural language processing techniques were employed to create health behavior risk scores from participant messages.
 
 
 RESULTS
 Offline HIV, amphetamine, and methamphetamine use were correctly identified using only social media data, with machine learning models obtaining F1 scores of 82.6%, 85.9%, and 85.3%, respectively. Additionally, constructed risk scores were found to be reasonably comparable to risk scores adapted from the Center for Disease Control.
 
 
 CONCLUSIONS
 To our knowledge, our study is the first empirical evaluation of a social media–based public health intervention framework for men who have sex with men. We found that social media data were correlated with offline sexual health and substance use, verified through biological testing. The proof of concept and initial results validate that public health interventions can indeed use social media–based systems to successfully determine offline health risk behaviors. The findings demonstrate the promise of deploying a social media–based just-in-time adaptive intervention to target substance use and HIV risk behavior.
",,2020,10.2196/preprints.22042,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
311d1c7aebea006e7860845e2df2cb5bddd6c1ec,https://www.semanticscholar.org/paper/311d1c7aebea006e7860845e2df2cb5bddd6c1ec,Should attention be all we need? The epistemic and ethical implications of unification in machine learning,"“Attention is all you need” has become a fundamental precept in machine learning research. Originally designed for machine translation, transformers and the attention mechanisms that underpin them now find success across many problem domains. With the apparent domain-agnostic success of transformers, many researchers are excited that similar model architectures can be successfully deployed across diverse applications in vision, language and beyond. We consider the benefits and risks of these waves of unification on both epistemic and ethical fronts. On the epistemic side, we argue that many of the arguments in favor of unification in the natural sciences fail to transfer over to the machine learning case, or transfer over only under assumptions that might not hold. Unification also introduces epistemic risks related to portability, path dependency, methodological diversity, and increased black-boxing. On the ethical side, we discuss risks emerging from epistemic concerns, further marginalizing underrepresented perspectives, the centralization of power, and having fewer models across more domains of application.",FAccT,2022,10.1145/3531146.3533206,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e4a9a09cd5f016590252d717b07e846e9995967d,https://www.semanticscholar.org/paper/e4a9a09cd5f016590252d717b07e846e9995967d,A Method for Improving Prediction of Human Heart Disease Using Machine Learning Algorithms,"A great diversity comes in the field of medical sciences because of computing capabilities and improvements in techniques, especially in the identification of human heart diseases. Nowadays, it is one of the world’s most dangerous human heart diseases and has very serious effects the human life. Accurate and timely identification of human heart disease can be very helpful in preventing heart failure in its early stages and will improve the patient’s survival. Manual approaches for the identification of heart disease are biased and prone to interexaminer variability. In this regard, machine learning algorithms are efficient and reliable sources to detect and categorize persons suffering from heart disease and those who are healthy. According to the recommended study, we identified and predicted human heart disease using a variety of machine learning algorithms and used the heart disease dataset to evaluate its performance using different metrics for evaluation, such as sensitivity, specificity, F-measure, and classification accuracy. For this purpose, we used nine classifiers of machine learning to the final dataset before and after the hyperparameter tuning of the machine learning classifiers, such as AB, LR, ET, MNB, CART, SVM, LDA, RF, and XGB. Furthermore, we check their accuracy on the standard heart disease dataset by performing certain preprocessing, standardization of dataset, and hyperparameter tuning. Additionally, to train and validate the machine learning algorithms, we deployed the standard K-fold cross-validation technique. Finally, the experimental result indicated that the accuracy of the prediction classifiers with hyperparameter tuning improved and achieved notable results with data standardization and the hyperparameter tuning of the machine learning classifiers.",Mobile Information Systems,2022,10.1155/2022/1410169,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0407bbeb69cccb430e02ee759103618d378a44a2,https://www.semanticscholar.org/paper/0407bbeb69cccb430e02ee759103618d378a44a2,Explainability in Machine Learning: a Pedagogical Perspective,"Given the importance of integrating of explainability into machine learning, at present, there are a lack of pedagogical resources exploring this. Specifically, we have found a need for resources in explaining how one can teach the advantages of explainability in machine learning. Often pedagogical approaches in the field of machine learning focus on getting students prepared to apply various models in the real world setting, but much less attention is given to teaching students the various techniques one could employ to explain a model’s decision-making process. Furthermore, explainability can benefit from a narrative structure that aids one in understanding which techniques are governed by which questions about the data. We provide a pedagogical perspective on how to structure the learning process to better impart knowledge to students and researchers in machine learning, when and how to implement various explainability techniques as well as how to interpret the results. We discuss a system of teaching explainability in machine learning, by exploring the advantages and disadvantages of various opaque and transparent machine learning models, as well as when to utilize specific explainability techniques and the various frameworks used to structure the tools for explainability. Among discussing concrete assignments, we will also discuss ways to structure potential assignments to best help students learn to use explainability as a tool alongside any given machine learning application. Data science professionals completing the course will have a birds-eye view of a rapidly developing area and will be confident to deploy machine learning more widely. A preliminary analysis on the effectiveness of a recently delivered course following the structure presented here is included as evidence supporting our pedagogical approach.",ArXiv,2022,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b73b79ff98bbdd0034a29be5a661ad216905953b,https://www.semanticscholar.org/paper/b73b79ff98bbdd0034a29be5a661ad216905953b,Learning with a purpose: the balancing acts of machine learning and individuals in the digital society,"The Internet has transformed some of the most basic processes in our society, such as trade, payment, and communication. We now have more access to products, services, and opinions than we ever had, but at the same time our behavior is tracked more closely than ever. For example, large online retailers offer hundreds of thousands of products and can readily observe how each consumer interacts with any of them in great detail. They can also rapidly deploy individual-level, in-vivo, randomized online experiments at population scale to test concepts, insights, and communication approaches which can lead to better services and products. However, there are often billions of possibilities, such as product-consumer combinations for product recommendations. The scale and complexity of these experiments create amazing challenges. Thus, firms face balancing acts. For example, they need to constantly choose between profiting from what they already know about consumers (such as the genres of movies already watched) and learning more about the same consumers (such as by recommending a movie of an untested genre). Consumers also face their own balancing acts. In the digital society, we inevitably leave digital footprints, but we have some discretion in terms of how much information we want to keep private. Typically, consumers who are more open to sharing their preferences are also exposed to higher risks, but at the same time they can get better access to the products and services they need. In this talk, I’ll explain how advances in machine learning and reinforcement learning can alleviate these challenging balancing acts. First, I’ll give you some background information and then I’ll briefly describe how these methods are helping firms and consumers, using examples from my own work. Next, I’ll indicate some exciting areas for future research. I’ll conclude by illustrating the implications for marketing science and prescriptive analytics more generally.",,2018,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f10cc958252a7c18444ee2634f84b48623c5725a,https://www.semanticscholar.org/paper/f10cc958252a7c18444ee2634f84b48623c5725a,"Machine learning 2018 and Big Data 2018- Digital transformation and the convergence of new emerging digital technologies - Samir El Masri - Digitalization. Cloud, UAE","Digital transformation is a journey that stems from strong beliefs in the digital economy by senior management supported by a digital transformation strategy. The strategy is much more difficult to deploy than develop and it may only be achieved when the transformation is led by CEOs reinforced by mature capabilities. Unfortunately, most digital transformation initiatives have failed in the past and many more will fail in the future. Advanced change is the method of using advanced innovations to make modern — or adjust existing — trade forms, culture, and client encounters to meet changing business and showcase prerequisites. Additionally, advanced changes have reshaped how companies approach client benefits. Making call centers and in-store benefit work areas run more proficiently with advanced innovation is of course awesome. But genuine change comes after you see at all accessible advances and consider how adjusting your trade to them can donate clients distant better;a much better;a higher;a stronger;an improved"">a higher involvement. Social media wasn’t designed to require the put of call centers, but it’s ended up an extra channel (and opportunity) to offer way better client benefit. Adjusting your benefit offerings to grasp social media is another great case of a computerized change.The ancient show was to hold up for clients to come to discover you, whether in individual or by calling an 800 number. But the rise of social media has changed benefit much like it’s changed promoting, promoting, and indeed deals and client benefit. Dynamic companies grasp social media as a chance to expand their benefits offerings by assembly clients on their stages of choice. 
 
Machine learning includes computers finding how they can perform errands without being expressly modified to do so. For straightforward errands relegated to computers, it is conceivable to program calculations telling the machine how to execute all steps required to fathom the issue at hand; on the computer's portion, no learning is required. For more progressed assignments, it can be challenging for a human to physically make the required calculations. In hone, it can turn out to be more successful to assist the machine develop its claim calculation, instead of have human software engineers indicate each required step. The discipline of machine learning employs different approaches to assist computers learn to achieve errands where no completely palatable calculation is accessible. In cases where tremendous numbers of potential answers exist, one approach is to name a few of the right answers as substantial. This will at that point be utilized as preparing information for the computer to progress the algorithm(s) it employments to decide rectify answers. 
This reimagining of commerce within the advanced age is computerized transformation.A key component of computerized change is understanding the potential of your innovation. Once more, that doesn’t cruel inquiring “How much speedier can we do things the same way?” It implies inquiring “What is our innovation truly able of, and how can we adjust our trade and forms to create the foremost of our technology investments?” Before Netflix, individuals chose motion pictures to lease by attending to stores and combing through racks of tapes and circles in look of something that looked great. Presently, libraries of computerized substance are served up on individual gadgets, total with proposals and audits based on client inclinations.It rises above conventional parts like deals, showcasing, and client benefit. Instep, computerized change starts and closes with how you think almost, and lock in with, clients. As we move from paper to spreadsheets to keen applications for overseeing our trade, we have the chance to reimagine how we do commerce — how we lock in our clients — with computerized innovation on our side. For little businesses fair getting begun, there’s no ought to set up your trade forms and change them afterward. You'll be able to future-proof your association from the word go. Building a 21st-century commerce on stickies and manually written records fair isn’t feasible. Considering, arranging, and building carefully sets you up to be spry, adaptable, and prepared to develop. Not so long prior, businesses kept records on paper. Whether transcribed in records or written into reports, commerce information was analog. On the off chance that you needed to assemble or share data, you managed with physical reports — papers and folios, xeroxes, and faxes. Then computers went standard, and most businesses begun changing over all of those ink-on-paper records to advanced computer records. Typically called digitisation: the method of changing over data from analog to computerized.The method of using digitized data to form set up ways of working less difficult and more proficient is called digitalisation. Note the word established in that definition: Digitalisation isn’t approximately changing how you are doing trade, or making unused sorts of businesses. It’s around keeping on keeping on, but quicker and way better presently that your information is right away open and not caught in a record cabinet someplace in a dusty archive. Think of client benefit, whether in retail, field ops, or a call center. Digitalization changed benefit until the end of time by making client records effortlessly and quickly retrievable through the computer. The essential technique of client benefit didn’t alter, but the method of handling an inquiry, looking up the important information, and advertising a determination got to be much more productive when looking paper records was supplanted by entering many keystrokes on a computer screen or versatile gadget. Digital change is changing the way trade gets done and, in a few cases, making completely unused classes of businesses. With the computerized change, companies are taking a step back and returning to everything they do, from inside frameworks to clients intuitive both online and in individual. They’re inquiring huge questions like “Can we alter our forms in a way that will empower way better decision-making, game-changing efficiencies, or distant better;a much better; a higher;a stronger;an improved"">an improved client encounter with more personalization?” Now we’re immovably dug in within the computerized age, and businesses of all sorts are making intelligent, viable, and troublesome ways of leveraging innovation. Netflix could be an incredible case. It begun out as a mail arrange benefit and disturbed the brick-and-mortar video rental commerce. At that point, computerized advancements made wide-scale gushing video conceivable. Nowadays, Netflix takes on conventional broadcast and cable tv systems and generation studios all at once by advertising a developing library of on-demand 
 
These failures have been mainly due to organizations undertaking digital change instead of digital transformation in addition to the lack of capabilities and non-readiness of the company to manage this transformation. New digital emerging technologies remain the backbone and the enabler of any digital transformation activities. The digitization of operations, workforce, marketing, and new digital business models will be realized by the convergence of all new emerging digital technologies through new products/services, price, customer experience, and platform values. In this talk, data science, machine learning, analytics, big data, IOT and their interrelationships will be demonstrated. Examples of how digital initiatives could help the industry by improving efficiency, avoiding trips, reducing unplanned downtime and transforming from time-based to condition-based maintenance will also be illustrated.",,2018,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e0a7e04cc5b64433b05c61f1dbdbb6fd75f1a5b9,https://www.semanticscholar.org/paper/e0a7e04cc5b64433b05c61f1dbdbb6fd75f1a5b9,THE MILKY WAY PROJECT: LEVERAGING CITIZEN SCIENCE AND MACHINE LEARNING TO DETECT INTERSTELLAR BUBBLES,"We present Brut, an algorithm to identify bubbles in infrared images of the Galactic midplane. Brut is based on the Random Forest algorithm, and uses bubbles identified by >35,000 citizen scientists from the Milky Way Project to discover the identifying characteristics of bubbles in images from the Spitzer Space Telescope. We demonstrate that Brut's ability to identify bubbles is comparable to expert astronomers. We use Brut to re-assess the bubbles in the Milky Way Project catalog, and find that 10%–30% of the objects in this catalog are non-bubble interlopers. Relative to these interlopers, high-reliability bubbles are more confined to the mid-plane, and display a stronger excess of young stellar objects along and within bubble rims. Furthermore, Brut is able to discover bubbles missed by previous searches—particularly bubbles near bright sources which have low contrast relative to their surroundings. Brut demonstrates the synergies that exist between citizen scientists, professional scientists, and machine learning techniques. In cases where “untrained” citizens can identify patterns that machines cannot detect without training, machine learning algorithms like Brut can use the output of citizen science projects as input training sets, offering tremendous opportunities to speed the pace of scientific discovery. A hybrid model of machine learning combined with crowdsourced training data from citizen scientists can not only classify large quantities of data, but also address the weakness of each approach if deployed alone.",,2014,10.1088/0067-0049/214/1/3,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
649ea610ba329540018d365838729eda8e6074b9,https://www.semanticscholar.org/paper/649ea610ba329540018d365838729eda8e6074b9,Signature Methods in Machine Learning,"Signature-based techniques give mathematical insight into the interactions between complex streams of evolving data. These insights can be quite naturally translated into numerical approaches to understanding streamed data, and perhaps because of their mathematical precision, have proved useful in analysing streamed data in situations where the data is irregular, and not stationary, and the dimension of the data and the sample sizes are both moderate. Understanding streamed multi-modal data is exponential: a word in n letters from an alphabet of size d can be any one of d messages. Signatures remove the exponential amount of noise that arises from sampling irregularity, but an exponential amount of information still remain. This survey aims to stay in the domain where that exponential scaling can be managed directly. Scalability issues are an important challenge in many problems but would require another survey article and further ideas. This survey describes a range of contexts where the data sets are small and the existence of small sets of context free and principled features can be used effectively. The mathematical nature of the tools can make their use intimidating to non-mathematicians. The examples presented in this article are intended to bridge this communication gap and provide tractable working examples drawn from the machine learning context. Notebooks are available online for several of these examples. This survey builds on the earlier paper of Ilya Chevryev and Andrey Kormilitzin which had broadly similar aims at an earlier point in the development of this machinery. This article illustrates how the theoretical insights offered by signatures are simply realised in the analysis of application data in a way that is largely agnostic to the data type. Larger and more complex problems would expect to address scalability issues and draw on a wider range of data science techniques. The article starts with a brief discussion of background material related to machine learning and signatures. This discussion fixes notation and terminology whilst simplifying the dependencies, but these background sections are not a substitute for the extensive literature they draw from. Hopefully, by working some of the examples the reader will find access to useful and simple to deploy tools; tools that are moderately effective in analysing longitudinal data that is complex and irregular in contexts where massive machine learning is not a possibility.",ArXiv,2022,10.48550/arXiv.2206.14674,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8052beced62611aa60da73c44281ae24d139b83b,https://www.semanticscholar.org/paper/8052beced62611aa60da73c44281ae24d139b83b,High-Resolution Urban Air Quality Mapping for Multiple Pollutants Based on Dense Monitoring Data and Machine Learning,"Spatially explicit urban air quality information is important for urban fine-management and public life. However, existing air quality measurement methods still have some limitations on spatial coverage and system stability. A micro station is an emerging monitoring system with multiple sensors, which can be deployed to provide dense air quality monitoring data. Here, we proposed a method for urban air quality mapping at high-resolution for multiple pollutants. By using the dense air quality monitoring data from 448 micro stations in Lanzhou city, we developed a decision tree model to infer the distribution of citywide air quality at a 500 m × 500 m × 1 h resolution, with a coefficient of determination (R2) value of 0.740 for PM2.5, 0.754 for CO and 0.716 for SO2. Meanwhile, we also show that the deployment density of the monitoring stations can have a significant impact on the air quality inference results. Our method is able to show both short-term and long-term distribution of multiple important pollutants in the city, which demonstrates the potential and feasibility of dense monitoring data combined with advanced data science methods to support urban atmospheric environment fine-management, policy making, and public health studies.",International journal of environmental research and public health,2022,10.3390/ijerph19138005,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
72ba72810351dd201d0559867a31c0a8808e2739,https://www.semanticscholar.org/paper/72ba72810351dd201d0559867a31c0a8808e2739,Machine learning blocks,"This work presents MLBlocks, a machine learning system that lets data scientists explore the space of modeling techniques in a very easy and efficient manner. We show how the system is very general in the sense that virtually any problem and dataset can be casted to use MLBlocks, and how it supports the exploration of Discriminative Modeling, Generative Modeling and the use of synthetic features to boost performance. MLBlocks is highly parameterizable, and some of its powerful features include the ease of formulating lead and lag experiments for time series data, its simple interface for automation, and its extensibility to additional modeling techniques. We show how we used MLBlocks to quickly get results for two very different realworld data science problems. In the first, we used time series data from Massive Open Online Courses to cast many lead and lag formulations of predicting student dropout. In the second, we used MLBlocks’ Discriminative Modeling functionality to find the best-performing model for predicting the destination of a car given its past trajectories. This later functionality is self-optimizing and will find the best model by exploring a space of 11 classification algorithms with a combination of Multi-Armed Bandit strategies and Gaussian Process optimizations, all in a distributed fashion in the cloud. Thesis Supervisor: Kalyan Veeramachaneni Title: Research Scientist",,2015,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4b868acf097dcc9357c6c633e03f37f8d12fea79,https://www.semanticscholar.org/paper/4b868acf097dcc9357c6c633e03f37f8d12fea79,Building a Rig State Classifier Using Supervised Machine Learning to Support Invisible Lost Time Analysis,"
 This paper covers the development of a key component of an internal system to report invisible lost time (ILT) metrics across drilling operations. Specifically this paper covers the development of a generalizable rig state engine based on the application of supervised machine learning. The same steps used in the creation of the production rig state engine are appled here to a smaller data set to demonstrate both the tractability of the problem and the methods used to create the rig state engine in the production system.
 The project objective was to provide efficiency and engineering metrics in a central repository covering operated regions. The system is designed to require minimal user configuration and management and provides both historic and near real time analysis to deliver a rich resource for offset comparison and benchmarking.
 Identifying rig-state is at the heart of every performance and engineering analysis system. This can be thought of as a machine learning classification problem. A large supervised learning set was constructed and used to train classification models which were compared for accuracy. A key success metric was the ability to generalise the selected model across different operations. Output from the rig-state classifier was then used to derive KPI data which was presented through a web based front end. A pilot system was then developed using agile principles allowing for rapid user engagement. Testing demonstrated that the system can support all real time operations within the company simultaneously and rapidly process historic well data for offset benchmarking. The cloud-based architecture allows rapid deployment of the system to new groups significantly reducing deployment costs. The system provides a foundation for onward data science and more advanced functionality.
 Minimal configuration, cloud storage and processing, combining contextual data with real-time rig data, near-real-time and historic analysis capabilities, rapid deployment, low cost, high accuracy and consistent metrics are all key and proven value drivers for the system. The output data is aso a valuable resource for additional machine learning and data science projects.","Day 1 Tue, March 05, 2019",2019,10.2118/194136-MS,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7df9989b86d7766292c8d9e9ab4d07ccba0f0428,https://www.semanticscholar.org/paper/7df9989b86d7766292c8d9e9ab4d07ccba0f0428,Machine Learning Image Analysis for Asset Inspection,"
 Total E&P UK Ltd and Merkle have undertaken a proof of concept project to investigate machine learning image analysis applied to image and video data from onshore and offshore sites, in preparation for the deployment of an autonomous asset inspection ground robot in 2019. The aim is to better understand the feasibility of these methods, and to demonstrate the benefits of robotic inspection with regard to improving safety and efficiency, enhancing data capture and reducing operational costs.
 An object detection model was developed based on high-performance open source algorithms. Transfer learning was applied using a custom-built image library and the result is a model able to detect a range of different types of items in the industrial environment including mobile equipment, process equipment, infrastructure and personnel (with and without PPE). The object detection model is used to feed into object classification anomaly detection models to look at the state of selected pieces of identified equipment, such as whether a valve is open or closed, which can be placed in the context of the expected state of the process equipment by relating it to the digital twin for the asset. An additional object detection model was developed to operate as a gas leak detection system for infrared cameras.
 The object detection model achieved good results and model performance was driven by the number and quality of images used for the training. An anomaly detection model designed to detect whether ball valves were open or closed delivered good results, with high accuracy and balanced false positive and false negative detection rates. The overall performance of the infrared gas detection model was restricted by the limited volume and variability of the training data, although the false positive detection rate was very low. A significant part of the machine learning was devoted to the development of a consistent labelled image library for oil and gas equipment, infrastructure and gas leaks. Image transformations were tested but boosting the number of images using transforms gave variable results. Additional training and testing data is needed to ensure that the models are as robust as possible, especially for the gas leak detection model. Once the models are productionised and in use, additional data can be used to periodically retrain the models for improved performance.
 In addition to the machine learning algorithms, a fundamental aspect of the project is the development of the overall technical architecture, supporting the data science. This includes enabling the transfer of data from inspection robots or other connected camera devices into a data store in a cloud computing environment and returning the results to dashboard systems with different levels of detail depending on user requirements.","Day 1 Tue, September 03, 2019",2019,10.2118/195773-MS,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cc0a888e047dcb2ff86072eeca9ae3e039927e26,https://www.semanticscholar.org/paper/cc0a888e047dcb2ff86072eeca9ae3e039927e26,The Stakes of Uncertainty: Developing and Integrating Machine Learning in Clinical Care,"The wide-spread deployment of machine learning tools within healthcare is on the horizon. However, the hype around “AI” tends to divert attention toward the spectacular, and away from the more mundane and ground- level aspects of new technologies that shape technological adoption and integration. This paper examines the development of a machine learning-driven sepsis risk detection tool in a hospital Emergency Department in order to interrogate the contingent and deeply contextual ways in which AI technologies are likely be adopted in healthcare. In particular, the paper bring into focus the epistemological implications of introducing a machine learning-driven tool into a clinical setting by analyzing shifting categories of trust, evidence, and authority. The paper further explores the conditions of certainty in the disciplinary contexts of data science and ethnography, and offers a potential reframing of the work of doing data science and machine learning as “computational ethnography” in order to surface potential pathways for developing effective, human-centered AI.",Ethnographic Praxis in Industry Conference Proceedings,2018,10.1111/1559-8918.2018.01213,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b00cf74044badd0e24882d6eb6748b16c525ef63,https://www.semanticscholar.org/paper/b00cf74044badd0e24882d6eb6748b16c525ef63,Machine Learning and Knowledge Discovery in Databases,,Lecture Notes in Computer Science,2018,10.1007/978-3-030-10925-7,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
29524f145db94cab2336da99f157e869d805dead,https://www.semanticscholar.org/paper/29524f145db94cab2336da99f157e869d805dead,SoK: Security and Privacy in Machine Learning,"Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive—new systems and models are being deployed in every domain imaginable, leading to widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date.We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. In particular, it is apparent that constructing a theoretical understanding of the sensitivity of modern ML algorithms to the data they analyze, à la PAC theory, will foster a science of security and privacy in ML.",2018 IEEE European Symposium on Security and Privacy (EuroS&P),2018,10.1109/EuroSP.2018.00035,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
07c8006023df923f341ed8a2e2dfbb5e2a287eab,https://www.semanticscholar.org/paper/07c8006023df923f341ed8a2e2dfbb5e2a287eab,Machine Learning Molecular Dynamics Simulations for Enhanced Student Learning,"Molecular dynamics (MD) simulations accelerated by the use of high-performance computing (HPC) have enabled the understanding of microscopic mechanisms underlying the material and biological phenomena such as protein folding, ion transport across cell membranes, and nanoparticle self-assembly. In the context of using MD simulations in education, rapid simulation-driven responses to students in classroom settings are desirable in elucidating concepts and demonstrating the application potential of HPC. We explore the idea of integrating machine learning (ML) with HPC-accelerated MD simulations to enhance their usability for education applications. The idea is illustrated using parallelized MD simulations deployed as a web application on nanoHUB and designed to extract the distribution of ions in nanoconfinement. We find that an artificial neural network-based regression model successfully learns nearly all the interesting features associated with the output ionic density profiles and rapidly generates predictions that are in excellent agreement with the results from explicit MD simulations. The inference time associated with the ML surrogate is over a factor of 10,000 smaller than the corresponding parallel MD simulation time. The dynamic, real-time and anytime engagement with the ML-integrated simulation framework can enable enhanced student learning of concepts associated with basic and advanced topics in computational science and engineering.",,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
26937db6917952a9c13bfe152eddd7defc08e695,https://www.semanticscholar.org/paper/26937db6917952a9c13bfe152eddd7defc08e695,Guest Editorial: Special Issue on Machine Learning Implementations,,J. Signal Process. Syst.,2019,10.1007/s11265-018-1432-1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e801baabcd62000d69644b3a45075ee57d23bc91,https://www.semanticscholar.org/paper/e801baabcd62000d69644b3a45075ee57d23bc91,Cleaning Up Philly’s Streets: A Cloud-Based Machine Learning Tool to Identify Illegal Trash Dumping,"Philadelphia is currently struggling with short dumping, where citizens and businesses illegally dump waste and trash on the street, in vacant lots, etc. Short dumping costs the municipality around $8,124,375/year. Currently, police officers must manually monitor the over 450 CCTV cameras to identify acts of dumping. We are developing machine learning software which can be run on video extracted from city CCTV feeds to identify the timing and location of acts of illegal dumping as they happen. Our software will help the Streets department to monitor these dumping events much more systematically and will significantly speed up the manual work currently done by police officers. Our cloud-based solution enables us to train deep learning models on images of trash bags and then run them on video from street cameras to identify illegal dumps, saving the officers the time of going through the entire feeds. Motivation and Value Proposition Like many major cities, Philadelphia has to deal with lots of litter. One particular problem is short dumping, where citizens and businesses illegally dump waste and trash on the street, in vacant lots, etc. It costs the municipality $619 to clean up one ton of short dumps. In Dec. 2016, Philadelphias Mayor Kenney signed Order No. 13-16 – the Zero Waste Litter Cabinet – aiming to make Philadelphia waste-free by 2035. In late 2017, the city passed a bill to hike fines for short dumping, signalling their resolve to alleviate the issue. After meeting with a number of officials in the Streets and Sanitation departments over the past several months, we identified that most of the short dumping is due to commercial disposal of garbage and other products, particularly tires. From this helpful perspective, we are initially focusing on addressing the dumping problem as it relates to the specific items which are most frequently illegally disposed: black garbage bags and tires. From our discussions with the Deputy Commissioner in charge of sanitation, we realized that the majority of problematic dumping is caused by individuals who regularly and systematically dispose of trash on the street. For example, a mechanic normally has to pay ∗Advisor (Department of Political Science) †Advisor (Department of Computer Science) a high price to legally dispose of tires, instead he can illegally dump the tires or pay someone to do so for him. The current solutions of fining $100 or raising awareness against dumping are not enough since it is still financially lucrative for them to short dump. The only way to challenge this behavior is to sanction the people who dump regularly, and the current resources deployed are simply not enough to consistently identify these offenders. We are developing machine learning software which, can be run on video extracted from city CCTV feeds, to identify the timing and location of acts of illegal dumping as they happen. Our software would enable the Streets department to monitor these dumping events much more systematically and therefore disincentivize the problematic behavior altogether in the long term. While interacting with the city to build our machine learning system, we uncovered some more fundamental problems with the short dumping reporting process. There was a lack of coordination between the people who watched the videos and identified short dumps and the members in the street department who had to ultimately record, respond, and clear up the dumps. The issue came down to both parties having different interfaces to do their work. The people watching the videos used internal tools to scan the videos and sent identified instances manually to the streets department, who ultimately were the ones who logged the information. Critical data was also lost/erroneous due to this process. For example, if the wrong timestamp or camera location was sent, the wrong decisions would be made and the wrong analysis would be performed. Thus, we realized that the city not only needed a more automatic solution to help flag short dumps, they also needed tools to close communication gaps between various stakeholders in the department. It was critical to build infrastructure not only on the machine learning side, but also on the software side so that city officials could all be on the same page about how to build standardized data systems to feed into the models in the future. We realized that a labelling interface could help not only acquire more data for the models, but also could be used to collaborate and share information about short dumps more easily across the department.",,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2f948365e1d59a7d59f0e0372efd94e19b9eee76,https://www.semanticscholar.org/paper/2f948365e1d59a7d59f0e0372efd94e19b9eee76,Machine-Learning Space Applications on SmallSat Platforms with TensorFlow,"Due to their attractive benefits, which include affordability, comparatively low development costs, shorter development cycles, and availability of launch opportunities, SmallSats have secured a growing commercial and educational interest for space development. However, despite these advantages, SmallSats, and especially CubeSats, suffer from high failure rates and (with few exceptions to date) have had low impact in providing entirely novel, market-redefining capabilities. To enable these more complex science and defense opportunities in the future, smallspacecraft computing capabilities must be flexible, robust, and intelligent. To provide more intelligent computing, we propose employing machine intelligence on space development platforms, which can contribute to more efficient communications, improve spacecraft reliability, and assist in coordination and management of single or multiple spacecraft autonomously. Using TensorFlow, a popular, open-source, machine-learning framework developed by Google, modern SmallSat computers can run TensorFlow graphs (principal component of TensorFlow applications) with both TensorFlow and TensorFlow Lite. The research showcased in this paper provides a flight-demonstration example, using terrestrial-scene image products collected in flight by our STP-H5/CSP system, currently deployed on the International Space Station, of various Convolutional Neural Networks (CNNs) to identify and characterize newly captured images. This paper compares CNN architectures including MobileNetV1, MobileNetV2, InceptionResNetV2, and NASNet Mobile.",,2018,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d4905e211209fada859ed8a4fe72dce6f877bb70,https://www.semanticscholar.org/paper/d4905e211209fada859ed8a4fe72dce6f877bb70,Interpretable Mammographic Image Classification using Cased-Based Reasoning and Deep Learning,"When we deploy machine learning models in highstakes medical settings, we must ensure these models make accurate predictions that are consistent with known medical science. Inherently interpretable networks address this need by explaining the rationale behind each decision while maintaining equal or higher accuracy compared to blackbox models. In this work, we present a novel interpretable neural network algorithm that uses casebased reasoning for mammography. Designed to aid a radiologist in their decisions, our network presents both a prediction of malignancy and an explanation of that prediction using known medical features. In order to yield helpful explanations, the network is designed to mimic the reasoning processes of a radiologist: our network first detects the clinically relevant semantic features of each image by comparing each new image with a learned set of prototypical image parts from the training images, then uses those clinical features to predict malignancy. Compared to other methods, our model detects clinical features (mass margins) with equal or higher accuracy, provides a more detailed explanation of its prediction, and is better able to differentiate the classification-relevant parts of the image.",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
455de02cf9fcde461b347f04d28b188e62c443a3,https://www.semanticscholar.org/paper/455de02cf9fcde461b347f04d28b188e62c443a3,Machine Learning Techniques and Challenges in Wireless Sensor Networks,"Deployment of wireless sensor networks usually found in science and designing applications, in military applications for intrusion detection and in civil applications. WSNs designing may be application specific and poses numerous concerns and restrictions. Energy aware routing, great concerning region of WSNs for analyser. As sensor nodes have limited power, information transmission, packet latency and data redundancy become essential concerns of routing algorithm for prolong lifetime of network. This review discusses various WSNs challenges with the system based on machine learning. And different solutions presented by the different authors. At last the future work emphasis on mobile sink location and dynamic clustering for improved energy efficient routing.",2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT),2018,10.1109/ICICCT.2018.8473187,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
637da8d72a9bd7a312ab0da0b08c701f1c6e7d20,https://www.semanticscholar.org/paper/637da8d72a9bd7a312ab0da0b08c701f1c6e7d20,Willump: A Statistically-Aware End-to-end Optimizer for Machine Learning Inference,"Systems for ML inference are widely deployed today, but they typically optimize ML inference workloads using techniques designed for conventional data serving workloads and miss critical opportunities to leverage the statistical nature of ML. In this paper, we present WILLUMP, an optimizer for ML inference that introduces two statistically-motivated optimizations targeting ML applications whose performance bottleneck is feature computation. First, WILLUMP automatically cascades feature computation for classification queries: WILLUMP classifies most data inputs using only high-value, low-cost features selected through empirical observations of ML model performance, improving query performance by up to 5×without statistically significant accuracy loss. Second, WILLUMP accurately approximates ML top-K queries, discarding low-scoring inputs with an automatically constructed approximate model and then ranking the remainder with a more powerful model, improving query performance by up to 10× with minimal accuracy loss. WILLUMP automatically tunes these optimizations’ parameters to maximize query performance while meeting an accuracy target. Moreover, WILLUMP complements these statistical optimizations with compiler optimizations to automatically generate fast inference code for ML applications. We show that WILLUMP improves the end-to-end performance of real-world ML inference pipelines curated from major data science competitions by up to 16×without statistically significant loss of accuracy.",,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b0bf64ccbd651e8c7bc141d8aabaecff562e93a1,https://www.semanticscholar.org/paper/b0bf64ccbd651e8c7bc141d8aabaecff562e93a1,"AI, Machine Learning & Deep Learning Risk Management & Controls: Beyond Deep Learning and Generative Adversarial Networks: Model Risk Management in AI, Machine Learning & Deep Learning","The current paper proposes how model risk management in operationalizing machine learning for algorithm deployment can be applied in national C4I and Cyber projects such as Project Maven. It builds upon recent leadership of global Management and Leadership industry executives for AI and Machine Learning Executive Education for MIT Sloan School of Management and the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and invited presentations at Princeton University. After building understanding about why model risk management is most crucial to robust AI, Machine Learning, Deep Learning, and, Neural Networks deployment, it introduces a Knowledge Management Framework for Model Risk Management to advance beyond ‘AI Automation’ to ‘AI Augmentation.’",,2018,10.2139/SSRN.3193693,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6f298d6dc360d42b2ec40d317aebec348e4f034e,https://www.semanticscholar.org/paper/6f298d6dc360d42b2ec40d317aebec348e4f034e,Reproducing Machine Learning Research on Binder,"The adoption of open-source tools to write machine learning pipelines, often in Python [66], has 9 provided researchers with access to an author’s experiments or allowed them to replicate a study 10 by reimplementing an algorithm. Open-source libraries popular in machine learning experiments 11 include Jupyter Notebooks [27], NumPy [65], CodaLab [30], TensorFlow [2], PyTorch [39]. To 12 share these experiments, researchers use platforms such as OpenML [67], Papers with Code [61], 13 Code Ocean, Inc. [12], RunMyCode [57], Colaboratory [21], and GitHub, Inc. [20]. These platforms 14 have all developed rich communities of researchers dedicated to open science, though many of the 15 deployments are closed-source or run by a single company or project. 16",,2018,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4733b9c9d3dea6d8f15914e9ad6c60203b0f7d7c,https://www.semanticscholar.org/paper/4733b9c9d3dea6d8f15914e9ad6c60203b0f7d7c,Trans-AI: How to Build True AI or Real Machine Intelligence and Learning,"We are at the edge of colossal changes. This is a critical moment of historical choice and opportunity. It could be the best 5 years ahead of us that we have ever had in human history or one of the worst, because we have all the power, technology and knowledge to create the most fundamental general-purpose technology (GPT), which could completely upend the whole human history. The most im-portant GPTs were fire, the wheel, language, writing, the printing press, the steam engine, electric power, information and telecommunications technology, all to be topped by real artificial intelligence technology. Our study refers to Why and How the Real Machine Intelligence or True AI or Real Su-perintelligence (RSI) could be designed and developed, deployed and distributed in the next 5 years. The whole idea of RSI took about three decades in three phases. The first conceptual model of Trans-AI was published in 1989. It covered all possible physical phenomena, effects and processes. The more extended model of Real AI was developed in 1999. A complete theory of superintelligence, with its reality model, global knowledge base, NL programing language, and master algorithm, was presented in 2008. The RSI project has been finally completed in 2020, with some key findings and discoveries being published on the EU AI Alliance/Futurium site in 20+ articles. The RSI features a unifying World Metamodel (Global Ontology), with a General Intelligence Framework (Master Algo-rithm), Standard Data Type Hierarchy, NL Programming Language, to effectively interact with the world by intelligent processing of its data, from the web data to the real-world data. The basic results with technical specifications, classifications, formulas, algorithms, designs and patterns, were kept as a trade secret and documented as the Corporate Confidential Report: How to Engineer Man-Machine Superintelligence 2025. As a member of EU AI Alliance, the author has proposed the Man-Machine RSI Platform as a key part of Transnational EU-Russia Project. To shape a smart and sustainable fu-ture, the world should invest into the RSI Science and Technology, for the Trans-AI paradigm is the way to an inclusive, instrumented, interconnected and intelligent world.",Ontology of Designing,2021,10.18287/2223-9537-2021-11-4-402-421,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
bd2cd62809ce68bcc0c115581c7784fb844d5a24,https://www.semanticscholar.org/paper/bd2cd62809ce68bcc0c115581c7784fb844d5a24,Embracing semantic ambiguity to enhance interpretability of complex unstructured machine learning problems.,"Ambiguity is frequently seen as an impediment to the recovery of a unique interpretation and meaning in texts. Machine learning algorithms have recently been deployed as an effective way to automate the process of semantic disambiguation. However, a growing body of literature has raised questions about problems of reduced accuracy and interpretability that come with disambiguation defined in terms of optimization at large data scales. We propose a hybrid methodology that can take advantage of the increased expediency of optimization (i.e. how a machine learning algorithm efficiently identifies semantic patterns within a corpus), while rethinking ambiguity as a semantically meaningful and interpretively useful feature of linguistic corpora. Our method examines why ambiguity occurs within a natural language corpus and the consequences of ambiguous meanings. Combining the “how” and the “why” dimensions of ambiguity requires a team science approach of researchers trained to study language from different disciplines, and we demonstrate how transdisciplinary digital scholarship centers located in academic libraries can create a space to foster precisely such collaborations purposefully created to analyze ambiguous datasets in more nuanced ways.",,2018,10.1002/pra2.2018.14505501144,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9c3a4dd3c6a4dbfba884a57bc4fe6e6edf9f358c,https://www.semanticscholar.org/paper/9c3a4dd3c6a4dbfba884a57bc4fe6e6edf9f358c,Improving the Quality of Computational Science Software by Using Metamorphic Relations to Test Machine Learning Applications,"Many applications in the field of scientific computing such as computational biology, computational linguistics, and others depend on Machine Learning algorithms to provide important core functionality to support solutions in the particular problem domains. However, it is difficult to test such applications because often there is no “test oracle” to indicate what the correct output should be for arbitrary input. To help address the quality of scientific computing software, in this paper we present a technique for testing the implementations of machine learning classification algorithms on which such scientific computing software depends. Our technique is based on an approach called “metamorphic testing”, which has been shown to be effective in such cases. In addition to presenting our technique, we describe a case study we performed on a realworld machine learning application framework, and discuss how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also discuss how our findings can be of use to other areas of computational science and engineering.",,2009,10.7916/D84X5GNS,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8ae513231015cdf4a563134970b367f9e7444f0e,https://www.semanticscholar.org/paper/8ae513231015cdf4a563134970b367f9e7444f0e,A novel machine-learning approach to measuring scientific knowledge flows using citation context analysis,,Scientometrics,2018,10.1007/s11192-018-2767-x,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c5d0eb504a14b5425d4ea012d13d71ad6beb3592,https://www.semanticscholar.org/paper/c5d0eb504a14b5425d4ea012d13d71ad6beb3592,Toward Audio Beehive Monitoring: Deep Learning vs. Standard Machine Learning in Classifying Beehive Audio Samples,"Electronic beehive monitoring extracts critical information on colony behavior and phenology without invasive beehive inspections and transportation costs. As an integral component of electronic beehive monitoring, audio beehive monitoring has the potential to automate the identification of various stressors for honeybee colonies from beehive audio samples. In this investigation, we designed several convolutional neural networks and compared their performance with four standard machine learning methods (logistic regression, k-nearest neighbors, support vector machines, and random forests) in classifying audio samples from microphones deployed above landing pads of Langstroth beehives. On a dataset of 10,260 audio samples where the training and testing samples were separated from the validation samples by beehive and location, a shallower raw audio convolutional neural network with a custom layer outperformed three deeper raw audio convolutional neural networks without custom layers and performed on par with the four machine learning methods trained to classify feature vectors extracted from raw audio samples. On a more challenging dataset of 12,914 audio samples where the training and testing samples were separated from the validation samples by beehive, location, time, and bee race, all raw audio convolutional neural networks performed better than the four machine learning methods and a convolutional neural network trained to classify spectrogram images of audio samples. A trained raw audio convolutional neural network was successfully tested in situ on a low voltage Raspberry Pi computer, which indicates that convolutional neural networks can be added to a repertoire of in situ audio classification algorithms for electronic beehive monitoring. The main trade-off between deep learning and standard machine learning is between feature engineering and training time: while the convolutional neural networks required no feature engineering and generalized better on the second, more challenging dataset, they took considerably more time to train than the machine learning methods. To ensure the replicability of our findings and to provide performance benchmarks for interested research and citizen science communities, we have made public our source code and our curated datasets.",Applied Sciences,2018,10.3390/APP8091573,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
93ac2c9a23416a533bd49c1bbe3862799f45cee5,https://www.semanticscholar.org/paper/93ac2c9a23416a533bd49c1bbe3862799f45cee5,Practical Machine Learning for Cloud Intrusion Detection: Challenges and the Way Forward,"Operationalizing machine learning based security detections is extremely challenging, especially in a continuously evolving cloud environment. Conventional anomaly detection does not produce satisfactory results for analysts that are investigating security incidents in the cloud. Model evaluation alone presents its own set of problems due to a lack of benchmark datasets. When deploying these detections, we must deal with model compliance, localization, and data silo issues, among many others. We pose the problem of ""attack disruption"" as a way forward in the security data science space. In this paper, we describe the framework, challenges, and open questions surrounding the successful operationalization of machine learning based security detections in a cloud environment and provide some insights on how we have addressed them.",AISec@CCS,2017,10.1145/3128572.3140445,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9bd5667f9cabda4bd23d588811c25e500c9bed27,https://www.semanticscholar.org/paper/9bd5667f9cabda4bd23d588811c25e500c9bed27,An end-to-end process model for supervised machine learning classification: from problem to deployment in information systems,"Extracting meaningful knowledge from (big) data represents a key success factor in many industries today. Supervised machine learning (SML) has emerged as a popular technique to learn patterns in complex data sets and to identify hidden correlations. When this insight is turned into action, business value is created. However, common data mining processes are generally not tailored to SML. In addition, they fall short of providing an end-to-end view that not only supports building a ”one off” model, but also covers its operational deployment within an information system. In this research-in-progress work we apply a Design Science Research (DSR) approach to develop a SML process model artifact that comprises model initiation, error estimation and deployment. In a first cycle, we evaluate the artifact in an illustrative scenario to demonstrate suitability. The results encourage us to further refine the approach and to prepare evaluations in concrete use cases. Thus, we move towards contributing a general process model that supports the systematic design of machine learning solutions to turn insights into continuous action.",,2017,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cf7518ac6dee13c534a9dbb53a648e9e7beb809f,https://www.semanticscholar.org/paper/cf7518ac6dee13c534a9dbb53a648e9e7beb809f,Machine Learning in Java,"Design, build, and deploy your own machine learning applications by leveraging key Java machine learning librariesAbout This BookDevelop a sound strategy to solve predictive modelling problems using the most popular machine learning Java librariesExplore a broad variety of data processing, machine learning, and natural language processing through diagrams, source code, and real-world applicationsPacked with practical advice and tips to help you get to grips with applied machine learningWho This Book Is ForIf you want to learn how to use Java's machine learning libraries to gain insight from your data, this book is for you. It will get you up and running quickly and provide you with the skills you need to successfully create, customize, and deploy machine learning applications in real life. You should be familiar with Java programming and data mining concepts to make the most of this book, but no prior experience with data mining packages is necessary.What You Will LearnUnderstand the basic steps of applied machine learning and how to differentiate among various machine learning approachesDiscover key Java machine learning libraries, what each library brings to the table, and what kind of problems each are able to solveLearn how to implement classification, regression, and clusteringDevelop a sustainable strategy for customer retention by predicting likely churn candidatesBuild a scalable recommendation engine with Apache MahoutApply machine learning to fraud, anomaly, and outlier detectionExperiment with deep learning concepts, algorithms, and the toolbox for deep learningWrite your own activity recognition model for eHealth applications using mobile sensorsIn DetailAs the amount of data continues to grow at an almost incomprehensible rate, being able to understand and process data is becoming a key differentiator for competitive organizations. Machine learning applications are everywhere, from self-driving cars, spam detection, document search, and trading strategies, to speech recognition. This makes machine learning well-suited to the present-day era of Big Data and Data Science. The main challenge is how to transform data into actionable knowledge.Machine Learning in Java will provide you with the techniques and tools you need to quickly gain insight from complex data. You will start by learning how to apply machine learning methods to a variety of common tasks including classification, prediction, forecasting, market basket analysis, and clustering.Moving on, you will discover how to detect anomalies and fraud, and ways to perform activity recognition, image recognition, and text analysis. By the end of the book, you will explore related web resources and technologies that will help you take your learning to the next level.By applying the most effective machine learning methods to real-world problems, you will gain hands-on experience that will transform the way you think about data.Style and approachThis is a practical tutorial that uses hands-on examples to step through some real-world applications of machine learning. Without shying away from the technical details, you will explore machine learning with Java libraries using clear and practical examples. You will explore how to prepare data for analysis, choose a machine learning method, and measure the success of the process.",,2016,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5994ea254d8e60e9b240cf0c3824b3e5f40df85a,https://www.semanticscholar.org/paper/5994ea254d8e60e9b240cf0c3824b3e5f40df85a,Machine Learning for Indoor Localization Using Mobile Phone-Based Sensors,"In this paper we investigate the problem of localizing a mobile device based on readings from its embedded sensors utilizing machine learning methodologies. We consider a realworld environment, collect a large dataset of 3110 datapoints, and examine the performance of a substantial number of machine learning algorithms in localizing a mobile device. We have found algorithms that give a mean error as accurate as 0.76 meters, outperforming other indoor localization systems reported in the literature. We also propose a hybrid instance-based approach that results in a speed increase by a factor of ten with no loss of accuracy in a live deployment over standard instance-based methods, allowing for fast and accurate localization. Further, we determine how smaller datasets collected with less density affect accuracy of localization, important for use in real-world environments. Finally, we demonstrate that these approaches are appropriate for real-world deployment by evaluating their performance in an online, in-motion experiment.",ArXiv,2015,10.1109/CCNC.2016.7444919,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fbbee5da12c0e0f15813254b9ffca18e339d3546,https://www.semanticscholar.org/paper/fbbee5da12c0e0f15813254b9ffca18e339d3546,10 - Machine learning for future intelligent air quality networks,"During the last few years, machine learning emerged as a very effective tool for data analysis and sematic value extraction from the large amount of data generated from deployed chemical multisensors devices. Many works have now highlighted the potential impact on multisensor device calibration, drift counteraction, data assimilation, optimal deployment of these classes of algorithms. Unlike 5 years ago, the huge amount of available data make possible to confirm this potential on realworld long-term deployments. This work analyze the literature produced by EuNetAir partners extracting the lessons cooperatively learnt about their impact and propose a novel architecture for future intelligent air quality networks based on the machine learning emerging paradigm.",,2016,10.5162/6EuNetAir2016/10,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
eec1fa8d13036c9f6674765ffab0c6b546e31be4,https://www.semanticscholar.org/paper/eec1fa8d13036c9f6674765ffab0c6b546e31be4,Enhancing Efficient Study Plan for Student with Machine Learning Techniques,"This research aims to enhance the achievement of the students on their study plan. The problem of the students in the university is that some students cannot design the efficient study plan, and this can cause the failure of studying. Machine Learning techniques are very powerful technique, and they can be adopted to solve this problem. Therefore, we developed our techniques and analyzed data from 300 samples by obtaining their grades of students from subjects in the curriculum of Computer Science, Faculty of Science and Technology, Sakon Nakhon Rajabhat University. In this research, we deployed CGPA prediction models and Kmeans models on 3 rd -year and 4 th -year students. The results of the experiment show high performance of these models. 37 students as representative samples were classified for their clusters and were predicted for CGPA. After sample classification, samples can inspect all vectors in their clusters as feasible study plans for next semesters. Samples can select a study plan and predict to achieve their desired CGPA. The result shows that the samples have significant improvement in CGPA by applying self-adaptive learning according to selected study plan.",,2017,10.5815/IJMECS.2017.03.01,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cc85a418cf6f50d255e9fc86081560952bfe878d,https://www.semanticscholar.org/paper/cc85a418cf6f50d255e9fc86081560952bfe878d,A Comprehensive Study of Big Data Machine Learning Approaches and Challenges,"Big data is spreading its span in almost every walk of science and engineering. Both public and private sector enterprises have been collecting and deploying enormous amount of domain-specific information to gain insights about areas like security, marketing, forecasting, fraud-detection, strategic planning etc.. This big data potential is unquestionably noteworthy; but to explore it fully and sensibly it requires new ideas and original learning techniques to address challenges associated with it. With the universe being getting more knowledge-based and computerized, an enormous range of applications shows interest in machine learning (ML) techniques. Machine learning is one of the most sought after field to handle big data challenge. With this paper we endow with a literature analysis related to the up-to-the-minute progress in researches on big data processing deploying Machine Learning as an analytical tool. We will review machine learning techniques with a focus on the promising learning methods like transfer learning, active learning, deep learning, representation learning, distributed, kernel-based learning and parallel learning. Also we will be reviewing the challenges in big data machine learning.",2017 International Conference on Next Generation Computing and Information Systems (ICNGCIS),2017,10.1109/ICNGCIS.2017.14,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
89400251cabab54acc24a5385cbf2928c8ace956,https://www.semanticscholar.org/paper/89400251cabab54acc24a5385cbf2928c8ace956,A Survey on Optical Handwriting Recognition System using Machine Learning Algorithms,"The recent advancement and development in technical fields and logistics has tackled the challenges of science and have delivered a good impact ratio by avoiding issues faced by fully or partially challenged and impaired people. The purpose of this paper is to provide easy facilities and improved services to a wide range of customers from school going children to teachers, professors, home tutors, visually impaired, and literary scholars. The title says it all, the concepts and theoretical background in the paper explains how the handwriting scripted in the image can be converted into textual information using several machine learning algorithms which follow the principles of supervised and unsupervised learning respectively. With the help of microcontrollers, feature extraction material, trainers and image processing concepts the proposed idea can be implemented feasibly. Machine learning algorithms such as Support Vector Machine, Random forest and neural nets algorithms have been compared and evaluated on basis of precision, recall, accuracy and other performance parameters. On comparing the mentioning algorithms it has been observed that ‘SVM’ algorithm outclasses the comparison test and gives out the best result in terms of accuracy, latency and robustness. On observing the overall effects of algorithms and conversions between handwritten text into textual information (maybe pdf’s). It has been concluded that the work can be further implemented on platforms such as e-learning, in which the application can be deployed on internet for android and IoS users with the help of mobile computing principles. Students in the classroom can just click a picture of what’s written on the board by the professor and the application will convert the text written on the image into a PDF and save it in the internal memory of your gadget.",,2017,10.5120/IJCA2017915539,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b3530b8eda702a53023281a1c744c2d1f6c1b5b4,https://www.semanticscholar.org/paper/b3530b8eda702a53023281a1c744c2d1f6c1b5b4,Towards a Unified Graph Model for Supporting Data Management and Usable Machine Learning,"Data management and machine learning are two important tasks in data science. However, they have been independently studied so far. We argue that they should be complementary to each other. On the one hand, machine learning requires data management techniques to extract, integrate, clean the data, to support scalable and usable machine learning, making it user-friendly and easily deployable. On the other hand, data management relies on machine learning techniques to curate data and improve its quality. This requires database systems to treat machine learning algorithms as their basic operators, or at the very least, optimizable stored procedures. It poses new challenges as machine learning tasks tend be iterative and recursive in nature, and some models have to be tweaked and retrained. This calls for a reexamination of database design to make it machine learning friendly. In this position paper, we present a preliminary design of a graph model for supporting both data management and usable machine learning. To make machine learning usable, we provide a declarative query language, that extends SQL to support data management and machine learning operators, and provide visualization tools. To optimize data management procedures, we devise graph optimization techniques to support a finer-grained optimization than traditional tree-based optimization model. We also present a workflow to support machine learning (ML) as a service to facilitate model reuse and implementation, making it more usable and discuss emerging research challenges in unifying data management and machine learning.",IEEE Data Eng. Bull.,2017,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4a101f8de4f6691c62a06c501ccdec32e066d7f6,https://www.semanticscholar.org/paper/4a101f8de4f6691c62a06c501ccdec32e066d7f6,Machine learning application in the life time of materials,"Materials design and development typically takes several decades from the initial discovery to commercialization with the traditional trial and error development approach. With the accumulation of data from both experimental and computational results, data based machine learning becomes an emerging field in materials discovery, design and property prediction. This manuscript reviews the history of materials science as a disciplinary the most common machine learning method used in materials science, and specifically how they are used in materials discovery, design, synthesis and even failure detection and analysis after materials are deployed in real application. Finally, the limitations of machine learning for application in materials science and challenges in this emerging field is discussed.",ArXiv,2017,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b872b12fd0200641765b90203dddc7491430c0ac,https://www.semanticscholar.org/paper/b872b12fd0200641765b90203dddc7491430c0ac,A scalable and efficient solution to R deployment in industry with application to machine learning,"R is a popular programming language for data science and advanced analytics. Although its numerous advantages, mainly the large number of plug-and-play libraries it offers, R may not be as massively adopted in companies as it could, probably because it lacks the tooling traditionally needed in companies (client-server split, scalability, etc.). In this paper, we describe a straight-forward multi-layer approach to R deployment in industrial environments. To this end, we rely on Rserve framework, a client-server emulator for R, to which we add a load balancing layer to optimize CPU usage. Through an application to machine learning, we demonstrate how the proposed architecture significantly reduces response times, which opens new perspectives of application in emergent domains, like mobile apps and cloud services.","2017 8th International Conference on Information, Intelligence, Systems & Applications (IISA)",2017,10.1109/IISA.2017.8316398,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
763809224a3f09158bc04601ae429d71d4988b2f,https://www.semanticscholar.org/paper/763809224a3f09158bc04601ae429d71d4988b2f,Valuing energy storage in electricity grids : a machine learning approach,"Meeting climate change mitigation targets likely requires the integration of large amounts of renewable energy generation, as well as energy storage systems, into the electric grid. However, the deployment of energy storage systems will remain limited until they become economically attractive, with or without government policy. One of the most profitable and widely studied energy storage system ventures is realtime temporal arbitrage, where the decision to charge or discharge the energy storage device is made according to some charging policy or decision rules, ideally charging when electricity prices are low and discharging when prices are high. In this thesis, state-of-the-art Machine Learning methods in the field of electricity price forecasting were used to accurately predict electricity prices. An improvement on existing recurrent neural network methods was introduced, using contextual knowledge of nodal prices and information such as geolocational spatial correlation data. It was then demonstrated that these prices can be used to inform a charging policy for an energy storage device which will maximize its associated arbitrage revenue. The most profitable policy requires perfect foresight of electricity prices, and hence the true valuation of the energy storage device given imperfect forecasts is bounded from above by a valuation using perfect foresight. The effect of improvements in electricity price forecasting accuracy on the valuation of energy storage systems is then explored using simulations, which places an implicit value on the improvement of electricity price forecasting methods. The impact of these improvements on the introduction of energy storage systems into the grid is then evaluated. Thesis Supervisor: Christopher R. Knittel Title: Professor, Applied Economics, MIT Sloan School of Management Thesis Reader: Caroline Uhler Title: Assistant Professor, Department of Electrical Engineering and Computer Science Assistant Professor, Institute for Data, Systems, and Society",,2018,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4132e988046539efd8620d426e413cc80789a017,https://www.semanticscholar.org/paper/4132e988046539efd8620d426e413cc80789a017,The Third Army Research Office ( ARO ) Workshop on Adversarial Machine Learning Talk Abstracts and Bios Data,"The proliferation of machine learning (ML) and artificial intelligence (AI) systems for military and security applications creates substantial challenges for designing and deploying such mechanisms that would learn, adapt, reason and act with Dinky, Dirty, Dynamic, Deceptive, Distributed (D5) data. While Dinky and Dirty challenges have been extensively explored in ML theory, the Dynamic challenge has been a persistent problem in ML applications (when the statistical distribution of training data differs from that of test data). The most recent Deceptive challenge is a malicious distribution shift between training and test data that amplifies the effects of the Dynamic challenge to the complete breakdown of the ML algorithms. Using the MNIST dataset as a simple calibration example, we explore the following two questions: (1) What geometric and statistical characteristics of data distribution can be exploited by an adversary with a given magnitude of the attack? (2) What counter-measures can be used to protect the constructed decision rule (at the cost of somewhat decreased performance) against malicious distribution shift within a given magnitude of the attack? While not offering a complete solution to the problem, we collect and interpret obtained observations in a way that provides practical guidance for making more adversary-resistant choices in the design of ML algorithms. Bio: Rauf Izmailov is a Senior Research Scientist at Perspecta Labs and an established researcher in mathematical and computer models for networking and control systems, machine learning, optimization, and statistical data analysis. He has more than 20 years of industry experience (including AT&T Bell Labs and NEC Labs America) in research and technical leadership of R&D teams. With Dr. Vapnik, he co-invented the new machine learning paradigm, Learning Using Privileged Information (LUPI). He was the PI on the DARPA PPAML (“Probabilistic Programming for Advanced Machine Learning”) program and is currently the PI on the DARPA D3M (“Data-Driven Discovery of Models”) program and the analytics task leader on the DARPA LADS (“Leveraging the Analog Domain for Security”) program. He is also a co-PI on the AFOSR program “Science of Information, Computation, Learning and Fusion”. Adversarial Unsupervised Learning Abstract: Nowadays more and more data are gathered for detecting and preventing cyber attacks. In cyber security applications, data analytics techniques have to deal with active adversaries that try to deceive the data analytics models and avoid being detected. The existence of such adversarial behavior motivates the development of robust and resilient adversarial learning techniques for various tasks. Most of the existing work focused on adversarial classification techniques, which assumed the existence of a large amount of labeled data instances. However, in practice, labeling the data instances often requires costly and time-consuming human expertise and becomes a significant bottleneck. Nowadays more and more data are gathered for detecting and preventing cyber attacks. In cyber security applications, data analytics techniques have to deal with active adversaries that try to deceive the data analytics models and avoid being detected. The existence of such adversarial behavior motivates the development of robust and resilient adversarial learning techniques for various tasks. Most of the existing work focused on adversarial classification techniques, which assumed the existence of a large amount of labeled data instances. However, in practice, labeling the data instances often requires costly and time-consuming human expertise and becomes a significant bottleneck. Meanwhile, a large number of unlabeled instances can also be used to understand the adversaries' behavior. To address the above mentioned challenges, we develop a novel grid based adversarial clustering algorithm. Our adversarial clustering algorithm is able to identify the normal and abnormal regions, and to draw defensive walls around the centers of the normal objects utilizing game theoretic ideas. Our algorithm also identifies the overlapping areas within large mixed clusters, and outliers which may be potential anomalies. Bio: Bowei Xi received her Ph.D. in statistics from the Department of Statistics at the University of Michigan, Ann Arbor in 2004. She is an associate professor in the Department of Statistics at Purdue University. She was a visiting faculty in the Department of Statistics at Stanford University in summer 2007, and a visiting faculty at Statistical and Applied Mathematical Sciences Institute (SAMSI) from September 2012 to May 2013. Her research focuses on multidisciplinary work involving big datasets with complex structure from very different application areas including cyber security, Internet traffic, metabolomics, machine learning, and data mining. She has a US patent on an automatic system configuration tool and has filed another patent application for identification of blood-based metabolite biomarkers of pancreatic cancer. Limitations of the Lipschitz Constant as a Defense Against Adversarial Examples Abstract: Several recent papers have discussed utilizing Lipschitz constants to limit the susceptibility of neural networks to adversarial examples. We analyze recently proposed methods for computing the Lipschitz constant. We show that the Lipschitz constant may indeed enable adversarially robust neural networks. However, the methods currently employed for computing it suffer from theoretical and practical limitations. We argue that addressing this shortcoming is a promising direction for future research into certified adversarial defenses. Several recent papers have discussed utilizing Lipschitz constants to limit the susceptibility of neural networks to adversarial examples. We analyze recently proposed methods for computing the Lipschitz constant. We show that the Lipschitz constant may indeed enable adversarially robust neural networks. However, the methods currently employed for computing it suffer from theoretical and practical limitations. We argue that addressing this shortcoming is a promising direction for future research into certified adversarial defenses. Bio: Todd Huster is a research scientist at Perspecta Labs. He has extensive experience solving challenging problems in the fields of machine learning, remote sensing, evaluation methodologies, and symbolic reasoning. He holds an M.S. in Computer Science from Wright State University. Certified Defenses Against Adversarial Examples Abstract: While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this talk, I will present some methods based on convex relaxations (with a focus on semidefinite programming) that output a certificate that for a given network and test input, no attack can force the error to exceed a certain value. I will then discuss how these certification procedures can be incorporated into neural network training to obtain provably While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this talk, I will present some methods based on convex relaxations (with a focus on semidefinite programming) that output a certificate that for a given network and test input, no attack can force the error to exceed a certain value. I will then discuss how these certification procedures can be incorporated into neural network training to obtain provably robust networks. Finally, I will present some empirical results on the performance of attacks and different certificates on networks trained using different objectives. This is joint work with Jacob Steinhardt and Percy Liang. Bio: Aditi Raghunathan is a third year PhD student at Stanford University working with Percy Liang. She is a recipient of the Google PhD Fellowship in Machine Learning and the Open Philanthrophy Project AI Fellowship. She is primarily interested in making machine learning systems provably robust to adversarial perturbations. She is also interested in ensuring fairness in the outcomes of ML systems. She spent the summer of 2018 at Google Brain working with Ian Goodfellow and Alex Kurakin. Previously, she was an undergraduate at IIT Madras. Is Robust ML Really Robust? Abstract: Machine learning (ML) techniques are increasingly common in security applications, such as malware and intrusion detection. However, ML models are often susceptible to evasion attacks, in which an adversary makes changes to the input (such as malware) in order to avoid being detected. A conventional approach to evaluate ML robustness to such attacks, as well as to design robust ML, is by considering simplified feature-space models of attacks, where the attacker changes ML features directly to effect evasion, while minimizing or constraining the magnitude of this change. We investigate the effectiveness of this approach to designing robust ML in the face of attacks that can be realized in actual malware (realizable attacks). We demonstrate that in the context of structure-based PDF malware detection, such techniques appear to have limited effectiveness. On the other hand, they are quite effective with contentbased detectors. In either case, we show that augmenting the feature space models with conserved features (those that cannot be unilaterally modified without compromising malicious functionality",,2018,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
31f3dbc28dd7158e95d76bb0757030295c9f769d,https://www.semanticscholar.org/paper/31f3dbc28dd7158e95d76bb0757030295c9f769d,SoK: Privacy-Preserving Computation Techniques for Deep Learning,"Abstract Deep Learning (DL) is a powerful solution for complex problems in many disciplines such as finance, medical research, or social sciences. Due to the high computational cost of DL algorithms, data scientists often rely upon Machine Learning as a Service (MLaaS) to outsource the computation onto third-party servers. However, outsourcing the computation raises privacy concerns when dealing with sensitive information, e.g., health or financial records. Also, privacy regulations like the European GDPR limit the collection, distribution, and use of such sensitive data. Recent advances in privacy-preserving computation techniques (i.e., Homomorphic Encryption and Secure Multiparty Computation) have enabled DL training and inference over protected data. However, these techniques are still immature and difficult to deploy in practical scenarios. In this work, we review the evolution of the adaptation of privacy-preserving computation techniques onto DL, to understand the gap between research proposals and practical applications. We highlight the relative advantages and disadvantages, considering aspects such as efficiency shortcomings, reproducibility issues due to the lack of standard tools and programming interfaces, or lack of integration with DL frameworks commonly used by the data science community.",Proc. Priv. Enhancing Technol.,2021,10.2478/popets-2021-0064,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
09405e2a3bb4d874f4780f7464ba68bde8d2da4a,https://www.semanticscholar.org/paper/09405e2a3bb4d874f4780f7464ba68bde8d2da4a,Conceptual Thinking in Statistics and Data Science Education: Interactive Formative Assessment with Meaning Equivalence Reusable Learning Objects (MERLO),"Computer age statistics, machine learning, data science and in general, data analytics, are having an ubiquitous impact on industry, business and services. Deploying a data transformation strategy requires a workforce which is up to the job in terms of knowledge, experience and capabilities. The application of analytics needs to address organizational needs, invoke proper methods, build on adequate infrastructures and ensure availability of the right skills to the right people. such upskilling requires a focus on conceptual understanding affecting both the pedagogical approach and the complementary learning assessment tools, This paper is about the application of advanced educational concepts to the teaching and evaluation of statistical and data science related concepts. Two educational elements will be included in the discussion: i) the use of simulations to facilitate problem based experiential learning and ii) an emphasis on information quality, as the overall objective of statistics and data science activity. 
 
We begin with an introduction to conceptual thinking and meaning equivalence and the application of Meaning Equivalence Reusable Learning Objects (MERLO) in the classroom. We then describe how MERLO and concept science can be applied in the domain of Statistics and Data Science. Section 3 is about the use of simulations in statistical education. In section 4 we discuss practical aspects of an education program focused on conceptual understanding. Section 5 is on the conceptual mapping of information quality as a MERLO infrastructure. The paper concludes with a discussion.",SSRN Electronic Journal,2021,10.2139/ssrn.3862006,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f74f2f6e966b43836f5468ffe35ebcd866f75a3d,https://www.semanticscholar.org/paper/f74f2f6e966b43836f5468ffe35ebcd866f75a3d,The science of deep learning,"Scientists today have completely different ideas of what machines can learn to do than we had only 10 y ago.

In image processing, speech and video processing, machine vision, natural language processing, and classic two-player games, in particular, the state-of-the-art has been rapidly pushed forward over the last decade, as a series of machine-learning performance records were achieved for publicly organized challenge problems. In many of these challenges, the records now meet or exceed human performance level.

A contest in 2010 proved that the Go-playing computer software of the day could not beat a strong human Go player. Today, in 2020, no one believes that human Go players—including human world champion Lee Sedol—can beat AlphaGo, a system constructed over the last decade. These new performance records, and the way they were achieved, obliterate the expectations of 10 y ago. At that time, human-level performance seemed a long way off and, for many, it seemed that no technologies then available would be able to deliver such performance.

Systems like AlphaGo benefited in this last decade from a completely unanticipated simultaneous expansion on several fronts. On the one hand, we saw the unprecedented availability of on-demand scalable computing power in the form of cloud computing, and on the other hand, a massive industrial investment in assembling human engineering teams from a globalized talent pool by some of the largest global technology players. These resources were steadily deployed over that decade to allow rapid expansions in challenge problem performance.

The 2010s produced a true technology explosion, a one-time–only transition: The sudden public availability of massive image and text data. Billions of people posted trillions of images and documents on social media, as the phrase “Big Data” entered media awareness. Image processing and natural language processing were forever changed by this new data resource … 

[↵][1]1To whom correspondence may be addressed. Email: donoho{at}stanford.edu.

 [1]: #xref-corresp-1-1",Proceedings of the National Academy of Sciences,2020,10.1073/pnas.2020596117,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b390e8262dd6b744e26145fbf8502f5fdd069277,https://www.semanticscholar.org/paper/b390e8262dd6b744e26145fbf8502f5fdd069277,I'm Going to Learn What?!?: Teaching Artificial Intelligence to Freshmen in an Introductory Computer Science Course,"As artificial intelligence (AI) becomes more widely utilized, there is a need for non-computer scientists to understand 1) how the technology works, and 2) how it can impact their lives. Currently, however, computer science educators have been reluctant to teach AI to non-majors out of concern that the topic is too advanced. To fill this gap, we propose an AI and machine learning (ML) curriculum that is specifically designed for first-year students. In this paper, we describe our curriculum and show how it covers four key content areas: core concepts, implementation details, limitations, and ethical considerations. We then share our experiences teaching our new curriculum to 174 randomly-selected Freshman students. Our results show that non-computer scientists can comprehend AI/ML concepts without being overwhelmed by the subject material. Specifically, we show that students can design, code, and deploy their own intelligent agents to solve problems, and that they understand the importance and value of learning about AI in a general-education course.",SIGCSE,2021,10.1145/3408877.3432530,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ebf5ddd2e6c95e6fb3de3fdee07f3a2ec630a32a,https://www.semanticscholar.org/paper/ebf5ddd2e6c95e6fb3de3fdee07f3a2ec630a32a,Supervised machine learning for automated coding of websites: An exploratory pilot study of government hyperlink networks [Conference abstract],"In order to analyse the structure of hyperlink networks, researchers need to understand the type and nature of webpages or websites (i.e. nodes) that comprise such networks. Information such as generic top-level domain (e.g. com, org, gov) only provides ‘coarse-grained’ data about what these nodes are. However, social scientists often require more detailed information about the websites under analysis. Usually this involves manually labelling, or ‘coding’, nodes into categories, using techniques similar to textual or documentary analysis. However, the size and nature of hyperlink networks often makes this task quite time-consuming and costly. In this exploratory pilot study we investigate the use of supervised machine learning to automatically code websites in government hyperlink networks into discrete policy domains (e.g. health, education, environment). This involves extracting or ‘scraping’ text from the HTML of the sample websites in order to provide data for the algorithm to ‘learn’ from. Specifically, we deploy Support Vector Machines (SVMs) on a sample of websites already correctly categorised into policy domains by a human coder. The sample is then divided into a ‘training sample’ and a ‘test sample’; SVMs learn from websites in the training sample and are tasked with correctly predicting policy domains for websites in the test sample. Preliminary results indicate a surprising level of accuracy, with some policy domains correctly classified more than 90% of the time. This suggests that supervised machine learning may offer a powerful and useful tool for social science research involving large-scale analysis of websites, particularly where the categories of websites are discrete and fairly well-defined (e.g. policy domains in government hyperlink networks). Future work will investigate the validity and robustness of this methodology using a larger sample of data and other machine learning algorithms and techniques.",,2014,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
362edc0f31ca39ad58d473a3872715a04fe516ed,https://www.semanticscholar.org/paper/362edc0f31ca39ad58d473a3872715a04fe516ed,Fair Policy Learning,"Ensuring machine learning algorithms deployed in the real world do not result in unexpected unfairness or social implications is becoming increasingly important. However, there exists a clear gap in literature for a measure of fairness that can detect discrimination against multiple sensitive attributes while also handling continuous or discrete outcomes. In this thesis, we propose a fairness measure, Fair-COCCO, based on the conditional cross-covariance operator on reproducing kernel Hilbert Spaces. This novel method generalise to the majority of existing fairness notions and naturally extends to settings with continuous outcomes and multidimensional sensitive attributes. Additionally, we demonstrate how the proposed measure can be readily implemented in stochastic gradient optimisation for fair policy learning in supervised learning settings. Empirical evaluations of Fair-COCCO on synthetic and realworld experiments reveal favourable comparisons to state-of-the-art techniques in balancing predictive power and fairness. We also see much potential in applying machine learning to analyse fairness in observed behaviour, especially in complex and high-dimensional real-world environments. To that end, we propose the first known definition of fairness for sequences of decisions and showcase how Fair-COCCO can be applied to quantify fairness in these problems. Building off these definitions, we turn to learning fair policies in real-world conditions, where learning is constrained to be performed offline. We propose Fair-PoLe, a novel inverse reinforcement learning that operates completely offline and is computationally efficient and functionally expressive when compared to existing methods. We illustrate the potential for Fair-PoLe to learn policies that balance imitation of expert policies with fair outcomes on the challenging problem of sepsis treatment.",,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e129340f016e11f2b328c6fc4d2ad9f6658b8cc1,https://www.semanticscholar.org/paper/e129340f016e11f2b328c6fc4d2ad9f6658b8cc1,Exploring Potential Flaws and Dangers Involving Machine Learning Technology,"This paper seeks to explore the ways in which machine learning and AI may influence the world in the future and the potential for the technology to be misused or exploited. In 1959 Arthur Samuel defined machine learning as “the field of study that gives computers the ability to learn without being explicitly programmed” (Munoz). This paper will also seek to find out if there is merit to the current worry that robots will take over some jobs based in cognitive abilities. In the past, a human was required to perform these jobs, but with the rise of more complex automation a person may not be necessary. Many of the sources cited throughout this paper focus on the innovation of machine learning and AI and how dangerous the over automation of the world could be. Machine learning and the resulting AI’s have their place in the world and more than likely they will do nothing but push the world towards a more fruitful future. Looking at potential risks of letting lines of code make important decisions is crucial given the consequences that negligence can have. There is a need to explore these topics because losing the human element in decision making can have some big implications if the AI is not programmed correctly. Machine learning has one of the greatest opportunities to impact the world. The need for caution however cannot be understated because of the potential dangers it may pose to jobs, security, and the overall stability of an ever changing world. 2 Missouri S&T’s Peer to Peer, Vol. 1, Iss. 2 [2017], Art. 4 https://scholarsmine.mst.edu/peer2peer/vol1/iss2/4 DANGERS OF MACHINE LEARNING TECHNOLOGY Skoff 3 Exploring Potential Flaws and Dangers Involving Machine Learning Technology Humans are always looking to evolve and automate tasks. Programming has come a long way since the early programming languages of FORTRAN and the like. Programming is now a complex task which creates complex solutions to problems plaguing all aspects of humanity. One of the complex solutions is artificial intelligence or AI. Machine learning and AI have created the potential for complete automation at home and in the workplace. There are of course problems with removing a human element from complex tasks. The potential effect on the workplace cannot be understated. Complete automation may even lead to more pressing issues. While the possibility of rogue AI seems straight from a science fiction film, the dangers of full automation are extensive. This danger could come from someone intentionally creating malicious AI or from a simple and innocent error in algorithm construction. In the future, there may need to be certain restrictions and sanctions targeting algorithms that could be used to create powerful AI’s that could impact more than just the workplace. As the world nears complete automation in some sectors, security becomes paramount in ensuring safe execution of tasks. Machine learning can be a great tool for shaping the future, but its potential perils cannot be understated. Workplace Impact AI taking over the workplace removes the human element from decision making and introduces the potential for malicious attacks upon critical systems. Carl Frey and Michael Osborne explored the fact that jobs that usually require high cognitive ability are being replaced by an automated solution. They say, “Text and data mining has improved the quality of legal research as constant access to market information has improved the efficiency of managerial decisionmaking” (Frey & Osborne, 2017). This means that in the near future, tasks believed to require a human may become automated. Frey and Osborne specifically mentions such tasks as legal 3 Skoff: Dangers of Machine Learning Technology Published by Scholars' Mine, 2017 DANGERS OF MACHINE LEARNING TECHNOLOGY Skoff 4 writing and truck driving may be taken over by computerization (Frey & Osborne 2017). Darrell West from The Center of Technology Innovation at Brookings says, “Telemarketers, title examiners, hand sewers, mathematical technicians, insurance underwriters, watch repairers, cargo agents, tax preparers, photographic process workers, new accounts clerks, library technicians, and data-entry specialists have a 99 percent chance of having their jobs computerized” (West, 2015). This does not necessarily mean that more complicated jobs such as those in the medical and legal fields can be computerized. In fact, West says that these jobs have a less than one percent chance of being replaced (West, 2015). If phased out by robots then the workforce potentially gains efficiency and accuracy but loses the human element. Another concerning factor is the potential breach of algorithms that dictate AI for critical systems. In these situations, a real person would be unaffected by such malicious attacks on critical systems. These types of attacks may become more probable as time goes on. Researchers from Stanford and Georgetown dissected the fact that making viruses has never been easier. They state, “To complicate matters, writing malicious programs has become easier: There are virus kits freely available on the Internet. Individuals who write viruses have become more sophisticated, often using mechanisms to change or obfuscate their code to produce so-called polymorphic viruses” (Kolter & Maloof, 2006). Surely, the security on critical systems which house essential AI would be strong. This however, has never stopped determined hackers from trying to crack through every firewall and security protocol. The computerization of certain jobs is coming and being prepared for such a future would be beneficial for the whole world. Security Concerns The potential dangers and pitfalls of machine learning are vast, and include potential attacks on algorithms themselves that are deliberate and destructive. In a paper from scholars at the 4 Missouri S&T’s Peer to Peer, Vol. 1, Iss. 2 [2017], Art. 4 https://scholarsmine.mst.edu/peer2peer/vol1/iss2/4 DANGERS OF MACHINE LEARNING TECHNOLOGY Skoff 5 University of California, Berkeley, researchers explore the potential dangers of machine learning and its uses in modern technology. They say, “Use of machine learning opens the possibility of an adversary who maliciously ‘mis-trains’ a learning system in an IDS” (Barreno, Nelson, Sears, Joseph & Tygar, 2006). An Intrusion Detection System (IDS) will monitor network traffic and identify potential threats. The authors identify some real dangers of unchecked machine learning algorithms, and this shows why the technical community cannot sit idly by and let potentially dangerous technology run rampant. This fear of a potentially dangerous AI may seem farfetched and even impossible given the current level of technological advancement. This however, may be more feasible than once expected. Two researchers from the University of Louisville explored a way to intentionally create a malevolent AI. They say, “Just like computer viruses and other malware is intentionally produced today, in the future we will see premeditated production of hazardous and unfriendly intelligent systems” (Pistono & Yampolskiy, 2016). This leads to a world where the average person can manufacture these unsanctioned malevolent AI’s that eventually compromise vital systems and databases. Even if someone were to make a secure machine learning algorithm that cannot be exploited, there could easily be someone deliberately making an algorithm that nullifies the good actions of ethical programmers. The ethical issues associated with compromising machine learning algorithms and making malevolent AI’s from scratch are monstrous. Both sets of authors seem convinced that the possibility of malevolent AIs are worth some level of concern. Security should always hold a top priority, but the potential creation of an AI which has no inherent conscience could be very destructive. Being aware that there could be ways to specifically target algorithms dictating AI’s becomes essential in preparing certain systems for the future. 5 Skoff: Dangers of Machine Learning Technology Published by Scholars' Mine, 2017 DANGERS OF MACHINE LEARNING TECHNOLOGY Skoff 6 Malicious AI Malicious AI has been a topic of science fiction since the inception of the concept of an autonomous artificial intelligence. Artificial intelligence could eventually completely overtake some areas of labor as the need of a human becomes negligible. The potential for an AI to become malevolent is not an immediate problem, but with machines taking over there is a real potential for this sort of thing to happen. A researcher from Louisville University says, “Just because developers might succeed in creating a safe AI, it doesn’t mean that it will not become unsafe at some later point. In other words, a perfectly friendly AI could be switched to the ‘dark side’ during the post-deployment stage” (Yampolskiy, 2015). The potential for malevolence is a concern to a professional environment with large amounts of sensitive data. A single AI which was deemed safe may suddenly “go rogue”. This prompts the question if these AI can be regulated and what implications such regulations would bring. Researchers from Stanford explored the possibility of certain regulations on algorithms. They state, “On the one hand, overlooking ethical issues may prompt negative impact and social rejection,’ but ‘on the other hand, overemphasizing the protection of individual rights in the wrong contexts may lead to regulations that are too rigid, and this in turn can cripple the chances to harness the social value of data science” (Floridi & Taddeo, 2016). This shows that there may need to be certain restrictions, but individual rights are jeopardized by the implementation of these regulations. One individual however can throw things into chaos. Stephen DeAngelis, CEO of Enterra Solutions, says, “It only takes one ‘evil genius’ to undo the best laid plans that more ethical scientists put into place” (DeAngelis, 2014). There will always be that ",,2017,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
daa235fd8059ac47d8288ae6e1d2ecd81c525ce9,https://www.semanticscholar.org/paper/daa235fd8059ac47d8288ae6e1d2ecd81c525ce9,Identifying Opioid and Illicit Drug Use from Adverse Event Reported Outcomes Using Machine Learning,"Earlier identification of opioid and illicit drug use may be used as a powerful tool for better guiding treatment strategies as well as appropriate triage of suspected illicit drug overdose patients. In this study, a machine learning model was used to distinguish patient drug use based solely on reported physiological events. For training and testing sets, data were derived from AEOLUS, a database of curated adverse drug reports based on the US Food and Drug Administration (FDA) Adverse Event Reporting System. Google’s TensorFlow library was used to build, train, and test the linear regression model. The positive results of this study suggest that machine learning approaches can be used to identify drugs based on reported outcomes. Introduction Toxicology screening with patient urine and blood samples has become a standard of care for patients suspected of illicit or opioid drug use [1]. Absent essential information about whether illicit or opioid drugs are involved in the manifestation of symptoms can lead to challenges in developing strategies for patient treatment. Improvements in rapidly identifying illicit drugs and opioids based on presentation of symptoms have the potential to change practice patterns, especially in acute care environments. The steady increase in drug overdose deaths since the 2000s[2] therefore calls for improved drug screening methods in the clinical setting. In emergency situations, rapid detection of exact prescription or illicit drug use can be crucial for determining proper care delivery. The availability of drug adverse event reporting provides an opportunity to build predictive models for detection of drug use. This study uses data from the US Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS), a database containing quarterly reports on adverse event and medication error reports submitted to FDA [3]. In the FAERS dataset, combinations of drugs are reported with combinations of resulting adverse outcomes [3], providing information linking drugs with certain physiological effects. Combined with current machine learning techniques, this information can be used to create algorithms correlating certain physiological factors with known drug outcomes. These correlations can then be used to construct predictive algorithms that determine drug use. Machine learning involves the use of data to create predictive models that can learn and improve without the aid of explicit programming. Two steps are involved in the creation of a machine learning algorithm: (1) training and (2) testing. Training involves performing statistics iteratively on a set of data until the predictions made by the model reach a certain level of accuracy. Afterwards, testing is done to further improve the model and determine the final accuracy. Machine learning techniques have been used previously in clinical settings to improve viral testing [4], graft failure prediction [5], and clinical decision making for breast cancer drug therapies[6]. Given the breadth of clinical data available, machine learning provides techniques to standardize and improve clinical care. In this study, machine learning techniques were used to explore the potential to identify opioid and illicit drug intake based on electronically captured FAERS data. Opioids and illicit drugs were tested specifically given the rise in illicit drug and opioid overdose cases nationwide[7], making drug detection increasingly important in the clinical setting. Utilizing machine learning tools provided for by TensorFlow[8], Google’s machine learning library, the goal of this study was to explore the potential to develop a predictive modeling system that does not require full patient history and makes classifications based on a patient’s current physiological state. Specific focus of this study was on four commonly prescribed opioids: (1) oxycodone; (2) hydrocodone; (3) fentanyl; and (4) morphine and three commonly abused drugs: (1) cocaine; (2) heroine; and (3) methamphetamine. The promising findings suggest that the machine learning approach employed in this study can indeed be used to rapidly identify individuals who may be at high risk of illicit or opioid drug use. Materials and Methods FAERS The FDA Adverse Event Reporting System (FAERS) is a database containing information on adverse event and medication error reports submitted to the FDA [3]. Event reports are submitted quarterly by health professionals and consumers and evaluated by clinical reviewers in the Center for Drug Evaluation and Research. Information is presented as reports stating all drugs taken by patients followed by all outcomes presented. No causal links are recorded between product and outcome. Using FAERS for mining drug-effect associations has been an active area of research and multiple data mining algorithms have been created for this purpose [9-11]. To date, there has been no reported use of predictive machine learning models for determining possible drugs based upon given events. AEOLUS From the community efforts of the Observational Health Data Science and Informatics (OHDSI) initiative, the FAERS and LAERS (Legacy Adverse Event Reporting System, which contains adverse event reports before 2012) datasets were reprocessed, cleaned, and standardized to form the Adverse Event Open Learning through Universal Standard (AEOLUS) database[15]. Single missing value imputation was first performed followed by case de-duplication. Every case was then given a primaryid or isr number, which indicate FAERS or LAERS cases respectively. Linked to each case are reported outcomes, given in OHDSI outcome concepts, and associated drugs, standardized to RxNorm Concept Unique Identifiers. Data are organized in AEOLUS into seven different MySQL tables, two of which were used in this study: one listing case ids with reported outcome concept ids and another listing case ids with standard drug concept ids. There were a total of 4245 distinct drug IDs and 17,710 distinct outcome IDs. y = Wx + b drug = W ∗ event + b Figure 1: Overview of linear regression model employed by TensorFlow. en represents event n and dm represents drug m. Wm,n represents the weight multiplied to event n for drug m and bm represents the bias for drug m. The bottom half of the figure shows the resulting matrix multiplication. Machine Learning Model A linear regression model, similar to that employed with the Mixed National Institute of Standards and Technology (MNIST), was used for training and testing. MNIST is a large database of handwritten digits that have been subjected to various machine learning methods to identify systems or approaches that achieve near-human performance. The model follows a y=Wx+b equation (Figure 1), with drugs as the dependent variable and physiological events as the independent variable. The linear model also accounted for biases (b) and contained a matrix for weights (W). The drugs and outcomes were given Boolean 0 or 1 values depending on if the drug or outcome was present in the tested case (Figure 2). For example, following Figure 1, e1 would be a Boolean for the presence of event A and would be 0 or 1 depending on if event A was present in the test case. d1 would be a Boolean for the presence of drug B and would be 0 or 1 depending on if drug B was present in the test case. W1,1 is the weight multiplied to e1 for the calculation of the probability d1 was present (Figure 1). In this study, this was implemented using TensorFlow, which is Google’s open-source software for deep neural networks, and provides a platform for accurate, large-scale machine learning research [8]. In TensorFlow, the data are vectorized into tensors and used to construct a data-flow graph. The graph is altered as more training data are deployed, adjusting the weights of the neural network with each iteration. In this study, TensorFlow was used to follow this machine learning model, iteratively adjusting weights and biases using softmax regression and loss functions. Conditions Tested This study focused on evaluating the ability to develop a prediction model using TensorFlow for effectiveness in classifying based on drug class, identifying the presence of specific drugs, and distinguishing between individual drugs. For drug class, opioids were chosen given the rising opioid epidemic [7] and four commonly prescribed opioids were specifically analyzed: (1) oxycodone; (2) hydrocodone; (3) fentanyl; and (4) morphine. For individual drug identification, three of the most common illicit drugs reported in AEOLUS were used: (1) cocaine, (2) methamphetamine, and (3) heroin. Evaluation of the model thus came from three tested conditions: (1) classification of opioid versus non-opioid; (2) prediction of illicit drug presence versus absence; (3) identification between different illicit drugs. Developing Outcome and Drug Arrays Arrays of Outcomes and Drugs were generated based on cases reported in AEOLUS. For identifying opioids, the total set of outcomes considered was narrowed down to the 35 outcomes with the highest numbers of cases. In other words, each drug-associated outcome was required to occur in a certain number of cases for each drug, and only the 35 outcomes with the highest number of cases were selected for the final outcome array (Table 1). For illicit drugs, the 20 outcomes with the highest number of cases for each illicit drug were used given the smaller number of total outcomes for illicit drugs (Table 2). Given some of the outcomes coded in the AEOLUS dataset were non-physiological, another round of testing was performed removing all non-physiological outcomes and all cases containing only non-physiological outcomes for the illicit drugs. The twenty highest physiological outcomes with the highest number of cases were used for the outcome arrays. The differences in outcome are shown in Table 3. A total of 5000 cases were used as training and test cases for each condition tested except for the between drug",,2017,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
248dde04122b20516b2e44f10e82dcc0214d42ee,https://www.semanticscholar.org/paper/248dde04122b20516b2e44f10e82dcc0214d42ee,Machine learning through exploration for perception-driven robotics = Machinelles Lernen in der Perzeptions-basierte Robotik,"The ability of robots to perform tasks in human environments has 
largely been limited to rather simple and specific tasks, such as lawn mowing 
and vacuum cleaning. As such, current robots are far away from the robot butlers, assistants, 
and housekeepers that are depicted in science fiction movies. Part of this gap can be 
explained by the fact that human environments are hugely varied, complex and unstructured. 
For example, the homes that a domestic robot might end up in are hugely varied. Since 
every home has a different layout with different objects and furniture, it is impossible for 
a human designer to anticipate all challenges a robot might 
face, and equip the robot a priori with all the necessary perceptual and manipulation skills. 
 
 
Instead, robots could be programmed in a way that allows them to adapt to any 
environment that they are in. In that case, the robot designer would not 
need to precisely anticipate such environments. The ability to adapt can be provided by 
robot learning techniques, which can be applied to learn skills for perception and manipulation. 
Many of the current 
robot learning techniques, 
however, rely on human supervisors to provide annotations or demonstrations, and to fine-tuning the methods parameters and heuristics. As such, 
it can require a significant amount of human time investment to 
make a robot perform a task in a novel environment, even if statistical learning techniques are used. 
 
In this thesis, I focus on another way of obtaining the data a robot needs to 
learn about the environment and how to successfully 
perform skills in it. By exploring the environment using its own sensors and actuators, rather than 
passively waiting for annotations or demonstrations, a 
robot can obtain this data by itself. I investigate multiple approaches that allow a robot 
to explore its environment autonomously, while trying to minimize the design effort 
required to deploy such algorithms in different situations. 
 
First, I consider an unsupervised robot with minimal prior knowledge 
about its environment. It can only learn through observed 
sensory feedback obtained though interactive exploration of its 
environment. In a bottom-up, probabilistic approach, the robot tries to segment 
the objects in its environment through clustering with minimal prior knowledge. This clustering is 
based on static visual scene features and observed movement. Information theoretic principles are used to autonomously select actions that maximize 
the expected information gain, and thus learning speed. Our evaluations 
on a real robot system equipped with an on-board camera show that the proposed 
method handles noisy inputs better than previous methods, and that 
action selection according to the information gain criterion does increase the learning speed. 
 
Often, however, the goal of a robot is not just to learn the structure of the environment, but to learn 
how to perform a task encoded by a reward signal. In addition to the weak feedback provided by reward signals, the robot has access to rich sensory data, that, even for 
simple tasks, is often non-linear and high-dimensional. Sensory data can be 
leveraged to learn a system model, but in high-dimensional sensory spaces this 
step often requires manually designing features. I propose a robot 
reinforcement learning algorithm with learned non-parametric models, value 
functions, and policies that can deal with high-dimensional state representations. 
As such, the proposed algorithm is well-suited to deal with high-dimensional signals 
such as camera images. To avoid that the robot converges prematurely to a sub-optimal solution, 
the information loss of policy updates is limited. This constraint makes sure the robot keeps exploring the effects 
of its behavior on the environment. The experiments show that the proposed non-parametric 
relative entropy policy search algorithm performs better than prior methods that either do not employ bounded updates, 
or that try to cover the state-space with general-purpose radial basis functions. Furthermore, 
the method is validated on a 
real-robot setup with high-dimensional camera image inputs. 
 
One problem with typical exploration strategies is that the behavior is perturbed independently 
in each time step, for example through selecting a random action or random policy parameters. 
As such, the resulting exploration behavior might be incoherent. Incoherence causes 
inefficient random walk behavior, makes the system less robust, and causes wear and tear on the robot. 
A typical solution is to perturb the policy parameters directly, and use the same perturbation for an entire episode. However, this 
strategy 
tends to increase the number of episodes needed, since only a single perturbation can be evaluated per episode. I introduce a 
strategy that can make a more balanced trade-off between the advantages of these two approaches. 
The experiments show that intermediate trade-offs, rather than independent or episode-based exploration, 
is beneficial across different tasks and learning algorithms. 
 
This thesis thus addresses how robots can learn autonomously by exploring the world through 
unsupervised learning and reinforcement learning. Throughout the thesis, new approaches 
and algorithms are introduced: a probabilistic interactive segmentation approach, the non-parametric 
relative entropy policy search algorithm, and a framework for generalized exploration. 
To allow the learning algorithms to be applied in different and unknown environments, 
the design effort and supervision required from human designers or users is minimized. 
These approaches and algorithms contribute 
towards the capability of robots to autonomously learn useful skills in human environments in a practical manner.",,2016,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a44c9535f3ed8acc4977c07c843471f7be5d2ebc,https://www.semanticscholar.org/paper/a44c9535f3ed8acc4977c07c843471f7be5d2ebc,Visual Computing and Machine Learning Techniques for Digital Forensics,"It is impressive how fast science has improved day by day in so many different fields. In special, technology advances are shocking so many people bringing to their reality facts that previously were beyond their imagination. Inspired by methods earlier presented in scientific fiction shows, the computer science community has created a new research area named Digital Forensics, which aims at developing and deploying methods for fighting against digital crimes such as digital image forgery.This work presents some of the main concepts associated with Digital Forensics and, complementarily, presents some recent and powerful techniques relying on Computer Graphics, Image Processing, Computer Vision and Machine Learning concepts for detecting forgeries in photographs. Some topics addressed in this work include: source attribution, spoofing detection, pornography detection, multimedia phylogeny, and forgery detection. Finally, this work highlights the challenges and open problems in Digital Image Forensics to provide the readers with the myriad opportunities available for research.",RITA,2015,10.22456/2175-2745.49492,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
31d64f800fb1e91de2b3fe52726b9cf879ee40a0,https://www.semanticscholar.org/paper/31d64f800fb1e91de2b3fe52726b9cf879ee40a0,Materials Representation and Transfer Learning for Multi-Property Prediction,"The adoption of machine learning in materials science has rapidly transformed materials property prediction. Hurdles
limiting full capitalization of recent advancements in machine learning include the limited development of methods
to learn the underlying interactions of multiple elements, as well as the relationships among multiple properties,
to facilitate property prediction in new composition spaces. To address these issues, we introduce the Hierarchical
Correlation Learning for Multi-property Prediction (H-CLMP) framework that seamlessly integrates (i) prediction
using only a material’s composition, (ii) learning and exploitation of correlations among target properties in multitarget regression, and (iii) leveraging training data from tangential domains via generative transfer learning. The model
is demonstrated for prediction of spectral optical absorption of complex metal oxides spanning 69 3-cation metal oxide
composition spaces. H-CLMP accurately predicts non-linear composition-property relationships in composition spaces
for which no training data is available, which broadens the purview of machine learning to the discovery of materials
with exceptional properties. This achievement results from the principled integration of latent embedding learning,
property correlation learning, generative transfer learning, and attention models. The best performance is obtained using
H-CLMP with Transfer learning (H-CLMP(T)) wherein a generative adversarial network is trained on computational
density of states data and deployed in the target domain to augment prediction of optical absorption from composition.
H-CLMP(T) aggregates multiple knowledge sources with a framework that is well-suited for multi-target regression
across the physical sciences.",Applied Physics Reviews,2021,10.26434/chemrxiv.14612307,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
dff829abffea125d72b9a2f2e0c1a0119b72438f,https://www.semanticscholar.org/paper/dff829abffea125d72b9a2f2e0c1a0119b72438f,"Reinforcement Learning in Medical Image Analysis: Concepts, Applications, Challenges, and Future Directions","Motivation : Medical image analysis involves tasks to assist physicians in qualitative and quantitative analysis of lesions or anatomical structures, significantly improving the accuracy and reliability of diagnosis and prognosis. Traditionally, these tasks are finished by physicians or medical physicists and lead to two major problems: (i) low efficiency; (ii) biased by personal experience. In the past decade, many machine learning methods have been applied to accelerate and automate the image analysis process. Compared to the enormous deployments of supervised and unsupervised learning models, attempts to use reinforcement learning in medical image analysis are scarce. This review article could serve as the stepping-stone for related research. Significance : From our observation, though reinforcement learning has gradually gained momentum in recent years, many researchers in the medical analysis field find it hard to understand and deploy in clinics. One cause is lacking well-organized review articles targeting readers lacking professional computer science backgrounds. Rather than providing a comprehensive list of all reinforcement learning models in medical image analysis, this paper may help the readers to learn how to formulate and solve their medical image analysis research as reinforcement learning problems. Approach & Results : We selected published articles from Google Scholar and PubMed. Considering the scarcity of related articles, we also included some outstanding newest preprints. The papers are carefully reviewed and categorized according to the type of image analysis task. We first review the basic concepts and popular models of reinforcement learning. Then we explore the applications of reinforcement learning models in landmark detection. Finally, we conclude the article by discussing the reviewed reinforcement learning approaches’ limitations and possible improvements.",ArXiv,2022,10.48550/arXiv.2206.14302,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
67b0ed6255c47c691cf38e0cfd13b82a0828ffdd,https://www.semanticscholar.org/paper/67b0ed6255c47c691cf38e0cfd13b82a0828ffdd,Bank Loan Approval Prediction Using Data Science Technique (ML),"Abstract: Banks are making major part of profits through loans. Loan approval is a very important process for banking organizations. It is very difficult to predict the possibility of payment of loan by the customers because there is an increasing rate of loan defaults and the banking authorities are finding it more difficult to correctly access loan requests and tackle the risks of people defaulting on loans. In the recent years, many researchers have worked on prediction of loan approval systems. Machine learning technique is very useful in predicting outcomes for large amount of data. In this paper, four algorithms are used such as Random Forest algorithm, Decision Tree algorithm, Naive Bayes algorithm, Logistic Regression algorithm to predict the loan approval of customers. All the four algorithms are going to be used on the same dataset and going to find the algorithm with maximum accuracy to deploy the model. Henceforth, we develop bank loan prediction system using machine learning techniques, so that the system automatically selects the eligible candidates to approve the loan. Keywords: Loan approval, Loan Default, Random Forest algorithm, Decision Tree algorithm, Naive Bayes algorithm, Logistic Regression algorithm, Loan prediction, Machine learning.",International Journal for Research in Applied Science and Engineering Technology,2022,10.22214/ijraset.2022.43665,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
297e2ed22af4b9de6264d08d060145881da83b4e,https://www.semanticscholar.org/paper/297e2ed22af4b9de6264d08d060145881da83b4e,Materialization and Reuse Optimizations for Production Data Science Pipelines,"Many companies and businesses train and deploy machine learning (ML) pipelines to answer prediction queries. In many applications, new training data continuously becomes available. A typical approach to ensure that ML models are up-to-date is to retrain the ML pipelines following a schedule, e.g., every day on the last seven days of data. Several use cases, such as A/B testing and ensemble learning, require many pipelines to be deployed in parallel. Existing solutions train each pipeline separately, which generates redundant data processing. Our goal is to eliminate redundant data processing in such scenarios using materialization and reuse optimizations. Our solution comprises of two main parts. First, we propose a materialization algorithm that given a storage budget, materializes the subset of the artifacts to minimize the run time of the subsequent executions. Second, we design a reuse algorithm to generate an execution plan by combining the pipelines into a directed acyclic graph (DAG) and reusing the materialized artifacts when appropriate. Our experiments show that our solution can reduce the training time by up to an order of magnitude for different deployment scenarios.",SIGMOD Conference,2022,10.1145/3514221.3526186,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4bf35038402a861d367401ff800bb1f472129cbf,https://www.semanticscholar.org/paper/4bf35038402a861d367401ff800bb1f472129cbf,Machine Learning for Microbial Phenotype Prediction,,BestMasters,2016,10.1007/978-3-658-14319-0,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
aef772dc765f63e9ee02ac1f501641535b5b0ed7,https://www.semanticscholar.org/paper/aef772dc765f63e9ee02ac1f501641535b5b0ed7,Applications of Machine Learning in Cyber Security,"Machine learning techniques have been applied in many areas of science due to their unique properties like adaptability, scalability, and potential to rapidly adjust to new and unknown challenges. Cyber security is a fastgrowing field demanding a great deal of attention because of remarkable progresses in social networks, cloud and web technologies, online banking, mobile environment, smart grid, etc. Diverse machine learning methods have been successfully deployed to address such wide-ranging problems in computer security. This paper discusses and highlights different applications of machine learning in cyber security. This study covers phishing detection, network intrusion detection, testing security properties of protocols, authentication with keystroke dynamics, cryptography, human interaction proofs, spam detection in social network, smart meter energy consumption profiling, and issues in security of machine learning techniques itself. keywords: Security, machine learning, survey.",,2014,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0839e7ae961b404ffbbe876af5f2e54a271b4235,https://www.semanticscholar.org/paper/0839e7ae961b404ffbbe876af5f2e54a271b4235,Institutional Knowledge at Singapore Management University Institutional Knowledge at Singapore Management University SoK: Towards the Science of security and privacy in machine SoK: Towards the Science of security and privacy in machine learning learning,"—Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive—new systems and models are being deployed in every domain imaginable, leading to rapid and widespread de- ployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community’s understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent ﬁndings on ML security and privacy, focusing on attacks identiﬁed on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identiﬁed and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cfcac91c2b7805bc1a3b551aaf3128e734811ef3,https://www.semanticscholar.org/paper/cfcac91c2b7805bc1a3b551aaf3128e734811ef3,Deploying deep learning in OpenFOAM with TensorFlow,"We outline the development of a data science module within OpenFOAM which allows for the in-situ deployment of trained deep learning architectures for general-purpose predictive tasks. This module is constructed with the TensorFlow C API and is integrated into OpenFOAM as an application that may be linked at run time. Notably, our formulation precludes any restrictions related to the type of neural network architecture (i.e., convolutional, fully-connected, etc.). This allows for potential studies of complicated neural architectures for practical CFD problems. In addition, the proposed module outlines a path towards an open-source, unified and transparent framework for computational fluid dynamics and machine learning.",AIAA Scitech 2021 Forum,2020.0,10.2514/6.2021-1485,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e42058e040efa1614be9357448f15f077c5b68cf,https://www.semanticscholar.org/paper/e42058e040efa1614be9357448f15f077c5b68cf,Rapid Deployment for Machine Learning in Educational Cloud,"In the cloud era, the acquisition of new cloud skills is a constant requirement of IT specialists. Educational organizations such as universities have a need to provide educational cloud curriculums for their students. In our current research, we are constructing a private cloud based on super-saturation, which is defined as the allocation of a much greater amount of logical resources than physical resources. Super-saturated clouds therefore realize up to 10 times more running instances than conventional clouds. While the performance of super-saturated clouds decreases somewhat compared with conventional clouds, their costs also greatly decrease. Moreover, in the post-cloud era, i.e., the big data era, data scientists will be increasingly required to process big data in the cloud. Mahout and Hadoop are two popular tools used in the fields of data science and machine learning. However, a certain level of skill is required to build such machine learning systems, and because it takes a long time to build such systems, the curriculums available to learners are limited. In this paper, we propose a method of rapid deployment for machine learning systems in the educational cloud. We show that our proposed method can reduce the required preparation time.",2013 16th International Conference on Network-Based Information Systems,2013.0,10.1109/NBiS.2013.59,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cc72ccf348d985f3b656e6e3a52d3a1476aabda3,https://www.semanticscholar.org/paper/cc72ccf348d985f3b656e6e3a52d3a1476aabda3,Identifying animal species in camera trap images using deep learning and citizen science,"Ecologists often study wildlife populations by deploying camera traps. Large datasets are generated using this approach which can be difficult for research teams to manually evaluate. Researchers increasingly enlist volunteers from the general public as citizen scientists to help classify images. The growing number of camera trap studies, however, makes it ever more challenging to find enough volunteers to process all projects in a timely manner. Advances in machine learning, especially deep learning, allow for accurate automatic image classification. By training models using existing datasets of images classified by citizen scientists and subsequent application of such models on new studies, human effort may be reduced substantially. The goals of this study were to (a) assess the accuracy of deep learning in classifying camera trap data, (b) investigate how to process datasets with only a few classified images that are generally difficult to model, and (c) apply a trained model on a live online citizen science project. Convolutional neural networks (CNNs) were used to differentiate among images of different animal species, images of humans or vehicles, and empty images (no animals, vehicles, or humans). We used four different camera trap datasets featuring a wide variety of species, different habitats, and a varying number of images. All datasets were labelled by citizen scientists on Zooniverse. Accuracies for identifying empty images across projects ranged between 91.2% and 98.0%, whereas accuracies for identifying specific species were between 88.7% and 92.7%. Transferring information from CNNs trained on large datasets (“transfer‐learning”) was increasingly beneficial as the size of the training dataset decreased and raised accuracy by up to 10.3%. Removing low‐confidence predictions increased model accuracies to the level of citizen scientists. By combining a trained model with classifications from citizen scientists, human effort was reduced by 43% while maintaining overall accuracy for a live experiment running on Zooniverse. Ecology researchers can significantly reduce image classification time and manual effort by combining citizen scientists and CNNs, enabling faster processing of data from large camera trap studies.",Methods in Ecology and Evolution,2018.0,10.1111/2041-210X.13099,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
41608da59ddb101cfa7c64c072509865f187458c,https://www.semanticscholar.org/paper/41608da59ddb101cfa7c64c072509865f187458c,CDS&E: Collaborative Research: Machine Learning for Automated Discovery and Control in Turbulent Plasma,"Executions of large-scale simulations that generate enormous amounts of data are now ubiquitous across the sciences. Low-level differential equations are used to drive the simulations and scientists hope to see and understand the larger scale phenomena that arise from them. Traditionally, simulations would be visualized by scientists or summarized by a few simple statistics. As these data sets increase in size, however, scientists are increasingly unable to easily answer even basic questions: Did anything interesting or unusual happen in the simulation? When and where did it happen? What are the most common phenomena comprising the simulation? What are the spatial and temporal distributions of those phenomena? Do they interact with each other? How can we find interesting relationships among different parts of the data? What quantities are conserved in these the experiments? Recent advances in machine learning (ML) open a new avenue to help scientists make new discoveries in complex datasets. In this work, we propose new ML methods to help domain scientists understand complicated properties of turbulent plasma. In particular we will propose data processing methods for i) turbulent simulated plasma (including atmospheric turbulence), ii) solar wind data, and iii) fusion data collected from a tokamak reactor. The unique role of our proposed work is to rigorously and systematically study and develop effective methodologies for data processing in turbulent plasma, and deploy these innovative methods and tools to help the daily practice of computational physicists and enable them to make scientific discoveries faster, easier, and cheaper. Understanding the nature of turbulence, its development and evolution (including transition, damping, decay and amplification mechanisms) is one of the key questions of fundamental physics. Turbulence, independently of medium, is always associated with chaotic changes of variables (although not all chaotic processes are turbulent). Indeed, the solution of Navier-Stoke equation equation that governs the simplest form of hydrodynamic turbulent motions remains elusive, despite being declared as one of the Clay Millennium Problems∗. The extreme sensitivity of the solution on the initial and boundary conditions makes turbulent motions chaotic, both in time and space. Hence, the turbulent processes have to be treated statistically, rather than deterministically. Turbulence occurs in different media ranging from cosmic plasma (at extremely large scales order of Mpc) till blood flow in human heart. There are several common features that characterize turbulent processes including: i) random fluctuations of physical variables such as velocity, pressure, density, etc; ii) the rotationality of motions unavoidable presence of kinematic vorticity; iii) diffusivity a tendency to mixing media (e.g. homogenization) due to eddy motions; iv) dissipation (e.g. damping) transfer of turbulent energy through viscous process to internal thermal energy. Turbulence produces variety of eddies (in size), while most of the energy is contained in the large-scale eddies: the size of these structures sets the integral scale of turbulence. In standard forward cascade of turbulence the energy of chaotic fluctuations is transferred from large to small scales producing smaller and smaller structures (eddies) down to the viscous scales where dissipative effects become dominant (scale called the Kolmogorov scale) and energy of turbulent fluctuations dissipates into heat. The statistical theory of turbulence proposed by Kolmogorov [1,2] is based on the concept of the energy cascade (originally discussed by Richardson [3], and self-similarity hypothesis. The former assumption has been revisited (see Ref. [4] for a review) leading to reconsideration of turbulence statistical description [5], and the nature of turbulence still remains one of unsolved problems of physics. Some open questions include: i) turbulence non scale-invariance and related issues with rescaling; ii) turbulence non-locality and understanding of the local versus non-local contributions to the magnetohydrodynamic (MHD) turbulence; iii) inverse transfer of energy and difference between",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b787d8d30a7c60c1caefdee25077e74bcdafe459,https://www.semanticscholar.org/paper/b787d8d30a7c60c1caefdee25077e74bcdafe459,"Deep Ensemble Learning Approaches in Healthcare to Enhance the Prediction and Diagnosing Performance: The Workflows, Deployments, and Surveys on the Statistical, Image-Based, and Sequential Datasets","With the development of information and technology, especially with the boom in big data, healthcare support systems are becoming much better. Patient data can be collected, retrieved, and stored in real time. These data are valuable and meaningful for monitoring, diagnosing, and further applications in data analysis and decision-making. Essentially, the data can be divided into three types, namely, statistical, image-based, and sequential data. Each type has a different method of retrieval, processing, and deployment. Additionally, the application of machine learning (ML) and deep learning (DL) in healthcare support systems is growing more rapidly than ever. Numerous high-performance architectures are proposed to optimize decision-making. As reliability and stability are the most important factors in the healthcare support system, enhancing the predicted performance and maintaining the stability of the model are always the top priority. The main idea of our study comes from ensemble techniques. Numerous studies and data science competitions show that by combining several weak models into one, ensemble models can attain outstanding performance and reliability. We propose three deep ensemble learning (DEL) approaches, each with stable and reliable performance, that are workable on the above-mentioned data types. These are deep-stacked generalization ensemble learning, gradient deep learning boosting, and deep aggregation learning. The experiment results show that our proposed approaches achieve more vigorous and reliable performance than traditional ML and DL techniques on statistical, image-based, and sequential benchmark datasets. In particular, on the Heart Disease UCI dataset, representing the statistical type, the gradient deep learning boosting approach dominates the others with accuracy, recall, F1-score, Matthews correlation coefficient, and area under the curve values of 0.87, 0.81, 0.83, 0.73, and 0.91, respectively. On the X-ray dataset, representing the image-based type, the deep aggregation learning approach shows the highest performance with values of 0.91, 0.97, 0.93, 0.80, and 0.94, respectively. On the Depresjon dataset, representing the sequence type, the deep-stacked generalization ensemble learning approach outperforms the others with values of 0.91, 0.84, 0.86, 0.8, and 0.94, respectively. Overall, we conclude that applying DL models using our proposed approaches is a promising method for the healthcare support system to enhance prediction and diagnosis performance. Furthermore, our study reveals that these approaches are flexible and easy to apply to achieve optimal performance.",International journal of environmental research and public health,2021.0,10.3390/ijerph182010811,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
852811010772b12f18cbbf95c4813e8b0070d6a7,https://www.semanticscholar.org/paper/852811010772b12f18cbbf95c4813e8b0070d6a7,A Survey Of Supervised Machine Learning In Wireless Sensor Network: A Power Management Perspective,"Wireless sensor network is a collection of a number of homogeneous and/or heterogeneous nodes in a particular area to monitor specific changes in environmental or physical occurrence. A power management in the wireless sensor network is considered as important criteria for any kind of wireless sensor network application. Multi-functional nodes are the basic unit of WSN for the current era of sensing the environmental changes, communication and computation aptitudes [Villalba et al 2009]. Wireless sensor network can be deployed in different fields of science, engineering etc. For example VANET is one of the examples of adhoc network implementation of WSN, which monitor and update traffic management system. WSN can also be used in monitoring the movement and behavior of animal migration in different seasons. WSN nodes communicate over small distance using the 
wireless medium with each other and with the data center to record the changes in surroundings like in agricultural environments, military based monitoring and industrial process observation [Singh et al 2011, Akyildiz et al 2002]. After the installation of wireless sensor nodes it is almost impossible to replace the power source in a device. The sensor node uses different routes to send data to sink node 
and to base station. The route is considered as best, where the wireless sensor network nodes use less power to transmit the data. This mechanism increases the life of the node and of the overall network. In the recent era, researchers used different routing algorithms to decrease the power consumption in order to increase the wireless sensor network life. LEACH used the randomize technique to select the 
cluster head according to the energy level of the node. Each node near the cluster head join the local cluster [Wendi et al 2000]. PEGASIS is greedy chain formation for data transfer. Each node sends and receives data from its nearest neighbor node. In this approach one node become the leader node which is responsible to collect data from all nodes and send the data to base station. In PEGASIS, unlike 
LEACH there are no clusters and multiple cluster heads, all the nodes had global information about the network nodes and sends data to only one leader node [Lindsey et al 2002]. Machine learning algorithms are considered as an efficient way for decision making in computational environments. 
Machine learning algorithms are iteration based algorithms, as the new knowledge is based on the previous predicted /calculated knowledge which helps to decrease errors in order to increase efficiency. This survey paper is focused on the discussion of best optimal path routing algorithms in wireless sensor networks by using supervised machine learning approaches.",,2013.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d9ef2e8f75ef77219f34e34974044670571ab52e,https://www.semanticscholar.org/paper/d9ef2e8f75ef77219f34e34974044670571ab52e,Few-Shot Learning on 3D Surface Defect Detection with PM Networks,"Machine learning has grown rapidly in recent years. The research and application of machine learning technology can be widely seen in many fields of science. As a result, various machine learning-based detection methods have been deployed in manufacturing lines. Compared with the traditional methods, machine learning methods improve the defect detection performance better. But there are still shortcomings such as the need for a large amount of data to train the classification model. We have developed and validated an Automatic Optical Inspection (AOI) system to detect surface defects based on 3D point cloud data. Our system can be divided into 4 main fundamental parts: Data Mapping, Defect Segmentation, Data Augmentation, and Few-shot Learning. This system can effectively reduce the workload of data collection. The PM Networks we proposed in the Few-shot learning stage are evaluated on Omniglot. We also provide our dataset of the Tile surface to evaluate the system performance. Our algorithm obtains significant improvements over the mainstream models on these datasets.",AIAM,2021.0,10.1145/3495018.3495037,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
68b03ad0ed6c1c9ac968fa4c5ae66c3020dc783e,https://www.semanticscholar.org/paper/68b03ad0ed6c1c9ac968fa4c5ae66c3020dc783e,Multiscale simulations of complex systems by learning their effective dynamics,,Nature Machine Intelligence,2020.0,10.1038/s42256-022-00464-w,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3757ac88d181bad8fffc7a33af34a4af9cbc965f,https://www.semanticscholar.org/paper/3757ac88d181bad8fffc7a33af34a4af9cbc965f,Exploiting Network Science for Feature Extraction and Representation Learning,"Networks are ubiquitous for many real-world problems such as modeling information diffusion over social networks, transportation systems, understanding protein-proteininteractions, human mobility, computational sustainability, among many others. Recently, due to the ongoing Big Data revolution, the fields of machine learning and Artificial Intelligence (AI) have also become extremely important, with AI mostly being dominated by representation learning techniques such as deep learning. However, research at the intersection of network science, machine learning and AI has been mostly unexplored. Specifically, most of the prior research focuses on how machine learning techniques can be used to solve “network” problems such as predicting information diffusion on social networks or classifying blogger interests in a blog network, etc. On the contrary, in this thesis, we answer the following key question: How canwe exploit network science to improve machine learning and representation learning models when addressing general problems? To answer the above question, we address several problems at the intersection of network science, machine learning, and AI. Specifically, we address four fundamental research challenges: (i) Network Science for Traditional Machine Learning, (ii) Representation Learning for Small-Sample Datasets, (iii) Network Science-BasedDeep Learning Model Compression, and (iv) Network Science for Neural Architecture Space Exploration. In other words, we show that many problems are governed by latent network dynamics which must be incorporated into the machine learning or representation learning models.To this end, we first demonstrate how network science can be used for traditional machine learning problems such as spatiotemporal timeseries prediction and application-specific feature extraction. More precisely, we propose a new framework called Network-of-Dynamic Bayesian Networks (NDBN) to address a complex probabilistic learning problem over networks with known but rapidly changing structure. We also propose a new domain-specific network inference approach when the network structure is unknown and only the high-dimensional data is available. We further introducea new network science-based, application-specific feature extraction method called K-Hop Learning. As concrete case studies, we show that both NDBN framework and K-Hop Learning significantly outperform traditional machine learning techniques for computational sustainability problems such as short-term solar energy and river flow prediction, respectively. We then discuss how network science can be used to address general representationlearning problems with high-dimensional and small-sample datasets. Here, we propose a new network community-based dimensionality reduction framework calledFeatureNet. Our approach is based on a new correlations-based network construction technique that explicitly discovers hidden communities in high-dimensional raw data.We show the effectiveness of FeatureNet on many diverse small-sample problems as deep learning typically overfits for such problems. We demonstrate that our techniqueachieves significantly higher accuracy than ten state-of-the-art dimensionality reduction methods (up to 40% improvement) for the small-sample problems. Since a simple correlations-based network alone cannot capture meaningful features for problems like image classification, we focus on deep learning models like Convolutional Neural Networks (CNN). Indeed, in the era of Internet-of-Things (IoT),computational costs of deep networks have become a critical challenge for deploying such models on resource-constrained edge devices. Towards this, model compressionhas emerged as an important area of research. However, when a computationally expensive CNN (or even a compressed model) cannot fit within the memory-budgetof a single IoT-device, it must be distributed across multiple devices which leads to significant inter-device communication. To alleviate the above problem, we propose a new model compression framework called the Network-of-Neural Networks (NoNN) which first exploits network science to partition a large “teacher” model’s knowledge into disjoint groups and then trains individual “student” models for each group. This results in a set of student moduleswhich satisfy the strict resource-constraints of individual IoT-devices. Extensive experiments on five well-known image classification tasks show that NoNN achieves similar accuracy as the teacher model and significantly outperforms the prior art. We also deploy our proposed framework on real hardware such as Raspberry Pi’s and Odroids to demonstrate that NoNN results in up to 12? reduction in latency, and up to 14? reduction in energy per device with negligible loss of accuracy. Finally, since deep networks are essentially a network of (artificial) neurons, networkscience is a perfect candidate to study their architectural characteristics. Hence, we model deep networks from a network science perspective to identify which architecture level characteristics enable models with different number of parameters and layers to achieve comparable accuracy. To this end, we propose new metrics called NN-Massand NN-Density to study the architecture design space of deep networks. We further theoretically demonstrate that (i) For a given depth and width, CNN architectures withhigher NN-Mass achieve lower generalization error, and (ii) Irrespective of number of parameters and layers (but same width), models with similar NN-Mass yield similar test accuracy. We then present extensive empirical evidence towards the above two theoretical insights by conducting experiments on real image classification tasks suchas CIFAR-10 and CIFAR-100. Lastly, we exploit the latter insight to directly design efficient architectures which achieve comparable accuracy to large models (? 97%on CIFAR-10 dataset) with up to 3? reduction in total parameters. This ultimately reveals how model sizes can be reduced directly from the architecture perspective.In summary, in this thesis, we address several problems at the intersection of network science, machine learning, and representation learning. Our research comprehensivelydemonstrates that network science can not only play a significant role but also lead to excellent results in both machine learning and representation learning.",,2019.0,10.1184/R1/9907484.V1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2efecdfa2c6d7459b2b941d714e24bd3e64e6bc5,https://www.semanticscholar.org/paper/2efecdfa2c6d7459b2b941d714e24bd3e64e6bc5,Automated Experimentation Powers Data Science in Chemistry.,"ConspectusData science has revolutionized chemical research and continues to break down barriers with new interdisciplinary studies. The introduction of computational models and machine learning (ML) algorithms in combination with automation and traditional experimental techniques has enabled scientific advancement across nearly every discipline of chemistry, from materials discovery, to process optimization, to synthesis planning. However, predictive tools powered by data science are only as good as their data sets and, currently, many of the data sets used to train models suffer from several limitations, including being sparse, limited in scope and requiring human curation. Likewise, computational data faces limitations in terms of accurate modeling of nonideal systems and can suffer from low translation fidelity from simulation to real conditions. The lack of diverse data and the need to be able to test it experimentally reduces both the accuracy and scope of the predictive models derived from data science. This Account contextualizes the need for more complex and diverse experimental data and highlights how the seamless integration of robotics, machine learning, and data-rich monitoring techniques can be used to access it with minimal human labor.We propose three broad categories of data in chemistry: data on fundamental properties, data on reaction outcomes, and data on reaction mechanics. We highlight flexible, automated platforms that can be deployed to acquire and leverage these data. The first platform combines solid- and liquid-dosing modules with computer vision to automate solubility screening, thereby gathering fundamental data that are necessary for almost every experimental design. Using computer vision offers the additional benefit of creating a visual record, which can be referenced and used to further interrogate and gain insight on the data collected. The second platform iteratively tests reaction variables proposed by a ML algorithm in a closed-loop fashion. Experimental data related to reaction outcomes are fed back into the algorithm to drive the discovery and optimization of new materials and chemical processes. The third platform uses automated process analytical technology to gather real-time data related to reaction kinetics. This system allows the researcher to directly interrogate the reaction mechanisms in granular detail to determine exactly how and why a reaction proceeds, thereby enabling reaction optimization and deployment.",Accounts of chemical research,2021.0,10.1021/acs.accounts.0c00736,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
114aa720872462b0ca1b97bfdec0ebd56c36fd0a,https://www.semanticscholar.org/paper/114aa720872462b0ca1b97bfdec0ebd56c36fd0a,Towards Understanding and Mitigating Social Biases in Language Models,"Warning: this paper contains model outputs that may be offensive or upsetting. As machine learning methods are deployed in realworld settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for highfidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",ICML,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6d0adac188152fbaa45a88ba4da788926ed8144a,https://www.semanticscholar.org/paper/6d0adac188152fbaa45a88ba4da788926ed8144a,Reinforcement Learning in Practice: Opportunities and Challenges,"This article is a gentle discussion about the ﬁeld of reinforcement learning in practice, about opportunities and challenges, touching a broad range of topics, with perspectives and without technical details. The article is based on both historical and recent research papers, surveys, tutorials, talks, blogs, books, (panel) discussions, and workshops/conferences. Various groups of readers, like researchers, engineers, students, managers, investors, ofﬁcers, and people wanting to know more about the ﬁeld, may ﬁnd the article interesting. In this article, we ﬁrst give a brief introduction to reinforcement learning (RL), and its relationship with deep learning, machine learning and AI. Then we discuss opportunities of RL, in particular, products and services, games, bandits, recommender systems, robotics, transportation, ﬁnance and economics, healthcare, education, combinatorial optimization, computer systems, and science and engineering. Then we discuss challenges, in particular, 1) foundation, 2) representation, 3) reward, 4) exploration, 5) model, simulation, planning, and benchmarks, 6) off-policy/ofﬂine learning, 7) learning to learn a.k.a. meta-learning, 8) explainability and interpretability, 9) constraints, 10) software development and deployment, 11) business perspectives, and 12) more challenges. We conclude with a discussion, attempting to answer: “Why has RL not been widely adopted in practice yet?” and “When is RL helpful?”. for discussing transfer domain randomization, knowledge distillation, imitation learning, meta-learning, robust RL.",,2022.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e545b981c54b55e01bf52e0c82d8eb589841302d,https://www.semanticscholar.org/paper/e545b981c54b55e01bf52e0c82d8eb589841302d,Systematic trading : calibration advances through machine learning,"Systematic trading in finance uses computer models to define trade goals, risk controls and rules that can execute trade orders in a methodical way. This thesis investigates how performance in systematic trading can be crucially enhanced by both i) persistently reducing the bid-offer spread quoted by the trader through optimized and realistically backtested strategies and ii) improving the out-of-sample robustness of the strategy selected through the injection of theory into the typically data-driven calibration processes. While doing so it brings to the foreground sound scientific reasons that, for the first time to my knowledge, technically underpin popular academic observations about the recent nature of the financial markets. The thesis conducts consecutive experiments across strategies within the three important building blocks of systematic trading: a) execution, b) quoting and c) risk-reward allowing me to progressively generate more complex and accurate backtested scenarios as recently demanded in the literature (Cahan et al. (2010)). The three experiments conducted are: 1. Execution: an execution model based on support vector machines. The first experiment is deployed to improve the realism of the other two. It analyses a popular model of execution: the volume weighted average price (VWAP). The VWAP algorithm targets to split the size of an order along the trading session according to the expected intraday volume's profile since the activity in the markets typically resembles convex seasonality – with more activity around the open and the closing auctions than along the rest of the day. In doing so, the main challenge is to provide the model with a reasonable expected profile. After proving in my data sample that two simple static approaches to the profile overcome the PCA-ARMA from Bialkowski et al. (2008) (a popular two-fold model composed by a dynamic component around an unsupervised learning structure) a further combination of both through an index based on supervised learning is proposed. The Sample Sensitivity Index hence successfully allows estimating the expected volume's profile more accurately by selecting those ranges of time where the model shall be less sensitive to past data through the identification of patterns via support vector machines. Only once the intraday execution risk has been defined can the quoting policy of a mid-frequency (in general, up to a week) hedging strategy be accurately analysed. 2. Quoting: a quoting model built upon particle swarm optimization. The second experiment analyses for the first time to my knowledge how to achieve the disruptive 50% bid-offer spread discount observed in Menkveld (2013) without increasing the risk profile of a trading agent. The experiment depends crucially on a series of variables of which market impact and slippage are typically the most difficult to estimate. By adapting the market impact model in Almgren et al. (2005) to the VWAP developed in the previous experiment and by estimating its slippage through its errors' distribution a framework within which the bid-offer spread can be assessed is generated. First, a full-replication spread, (that set out following the strict definition of a product in order to hedge it completely) is calculated and fixed as a benchmark. Then, by allowing benefiting from a lower market impact at the cost of assuming deviation risk (tracking error and tail risk) a non-full-replication spread is calibrated through particle swarm optimization (PSO) as in Diez et al. (2012) and compared with the benchmark. Finally, it is shown that the latter can reach a discount of a 50% with respect to the benchmark if a certain number of trades is granted. This typically occurs on the most liquid securities. This result not only underpins Menkveld's observations but also points out that there is room for further reductions. When seeking additional performance, once the quoting policy has been defined, a further layer with a calibrated risk-reward policy shall be deployed. 3. Risk-Reward: a calibration model defined within a Q-learning framework. The third experiment analyses how the calibration process of a risk-reward policy can be enhanced to achieve a more robust out-of-sample performance – a cornerstone in quantitative trading. It successfully gives a response to the literature that recently focusses on the detrimental role of overfitting (Bailey et al. (2013a)). The experiment was motivated by the assumption that the techniques underpinned by financial theory shall show a better behaviour (a lower deviation between in-sample and out-of-sample performance) than the classical data-driven only processes. As such, both approaches are compared within a framework of active trading upon a novel indicator. The indicator, called the Expectations' Shift, is rooted on the expectations of the markets' evolution embedded in the dynamics of the prices. The crucial challenge of the experiment is the injection of theory within the calibration process. This is achieved through the usage of reinforcement learning (RL). RL is an area of ML inspired by behaviourist psychology concerned with how software agents take decisions in an specific environment incentivised by a policy of rewards. By analysing the Q-learning matrix that collects the set of state/actions learnt by the agent within the environment, defined by each combination of parameters considered within the calibration universe, the rationale that an autonomous agent would have learnt in terms of risk management can be generated. Finally, by then selecting the combination of parameters whose attached rationale is closest to that of the portfolio manager a data-driven solution that converges to the theory-driven solution can be found and this is shown to successfully outperform out-of-sample the classical approaches followed in Finance. The thesis contributes to science by addressing what techniques could underpin recent academic findings about the nature of the trading industry for which a scientific explanation was not yet given: • A novel agent-based approach that allows for a robust out-of-sampkle performance by crucially providing the trader with a way to inject financial insights into the generally data-driven only calibration processes. It this way benefits from surpassing the generic model limitations present in the literature (Bailey et al. (2013b), Schorfheid and Wolpin (2012), Van Belle and Kerr (2012) or Weiss and Kulikowski (1991)) by finding a point where theory-driven patterns (the trader's priors tend to enhance out-of-sample robustness) merge with data-driven ones (those that allow to exploit latent information). • The provision of a technique that, to the best of my knowledge, explains for the first time how to reduce the bid-offer spread quoted by a traditional trader without modifying her risk appetite. A reduction not previously addressed in the literature in spite of the fact that the increasing regulation against the assumption of risk by market makers (e.g. Dodd–Frank Wall Street Reform and Consumer Protection Act) does yet coincide with the aggressive discounts observed by Menkveld (2013). As a result, this thesis could further contribute to science by serving as a framework to conduct future analyses in the context of systematic trading. • The completion of a mid-frequency trading experiment with high frequency execution information. It is shown how the latter can have a significant effect on the former not only through the erosion of its performance but, more subtly, by changing its entire strategic design (both, optimal composition and parameterization). This tends to be highly disregarded by the financial literature. More importantly, the methodologies disclosed herein have been crucial to underpin the setup of a new unit in the industry, BBVA's Global Strategies & Data Science. This disruptive, global and cross-asset team gives an enhanced role to science by successfully becoming the main responsible for the risk management of the Bank's strategies both in electronic trading and electronic commerce. Other contributions include: the provision of a novel risk measure (flowVaR); the proposal of a novel trading indicator (Expectations’ Shift); and the definition of a novel index that allows to improve the estimation of the intraday volume’s profile (Sample Sensitivity Index).",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8c8868d75f5fc7a055fdbc8610ab20b0a4304829,https://www.semanticscholar.org/paper/8c8868d75f5fc7a055fdbc8610ab20b0a4304829,Deep Reinforcement Learning: Opportunities and Challenges,"This article is a gentle discussion about the field of reinforcement learning for real life, about opportunities and challenges, with perspectives and without technical details, touching a broad range of topics. The article is based on both historical and recent research papers, surveys, tutorials, talks, blogs, and books. Various groups of readers, like researchers, engineers, students, managers, investors, officers, and people wanting to know more about the field, may find the article interesting. In this article, we first give a brief introduction to reinforcement learning (RL), and its relationship with deep learning, machine learning and AI. Then we discuss opportunities of RL, in particular, applications in products and services, games, recommender systems, robotics, transportation, economics and finance, healthcare, education, combinatorial optimization, computer systems, and science and engineering. The we discuss challenges, in particular, 1) foundation, 2) representation, 3) reward, 4) model, simulation, planning, and benchmarks, 5) learning to learn a.k.a. meta-learning, 6) off-policy/offline learning, 7) software development and deployment, 8) business perspectives, and 9) more challenges. We conclude with a discussion, attempting to answer: “Why has RL not been widely adopted in practice yet?” and “When is RL helpful?”.",ArXiv,2022.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
efb58f552acb998d38b2bf65c37147a5c85eed28,https://www.semanticscholar.org/paper/efb58f552acb998d38b2bf65c37147a5c85eed28,A bibliometric analysis on deep learning during 2007–2019,,Int. J. Mach. Learn. Cybern.,2020.0,10.1007/s13042-020-01152-0,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0966363ea46368f297999b026c827f0f5ea7fcc9,https://www.semanticscholar.org/paper/0966363ea46368f297999b026c827f0f5ea7fcc9,Efficient Machine-Type Communication Using Multi-Metric Context-Awareness for Cars Used as Mobile Sensors in Upcoming 5G Networks,"Upcoming 5G-based communication networks will be confronted with huge increases in the amount of transmitted sensor data related to massive deployments of static and mobile Internet of Things (IoT) systems. Cars acting as mobile sensors will become important data sources for cloud-based applications like predictive maintenance and dynamic traffic forecast. Due to the limitation of available communication resources, it is expected that the grows in Machine-Type Communication (MTC) will cause severe interference with Human-to-human (H2H) communication. Consequently, more efficient transmission methods are highly required. In this paper, we present a probabilistic scheme for efficient transmission of vehicular sensor data which leverages favorable channel conditions and avoids transmissions when they are expected to be highly resource-consuming. Multiple variants of the proposed scheme are evaluated in comprehensive realworld experiments. Through machine learning based combination of multiple context metrics, the proposed scheme is able to achieve up to 164% higher average data rate values for sensor applications with soft deadline requirements compared to regular periodic transmission.",2018 IEEE 87th Vehicular Technology Conference (VTC Spring),2018.0,10.1109/VTCSpring.2018.8417753,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
39cebfa5273b7206cea8459f9860fe2354b2f52a,https://www.semanticscholar.org/paper/39cebfa5273b7206cea8459f9860fe2354b2f52a,A Cross-Layer Review of Deep Learning Frameworks to Ease Their Optimization and Reuse,"Machine learning and especially Deep Learning (DL) approaches are at the heart of many domains, from computer vision and speech processing to predicting trajectories in autonomous driving and data science. Those approaches mainly build upon Neural Networks (NNs), which are compute-intensive in nature. A plethora of frameworks, libraries and platforms have been deployed for the implementation of those NNs, but end users often lack guidance on what frameworks, platforms and libraries to use to obtain the best implementation for their particular needs. This paper analyzes the DL ecosystem providing a structured view of some of the main frameworks, platforms and libraries for DL implementation. We show how those DL applications build ultimately on some form of linear algebra operations such as matrix multiplication, vector addition, dot product and the like. This analysis allows understanding how optimizations of specific linear algebra functions for specific platforms can be effectively leveraged to maximize specific targets (e.g. performance or power-efficiency) at application level reusing components across frameworks and domains.",2020 IEEE 23rd International Symposium on Real-Time Distributed Computing (ISORC),2020.0,10.1109/ISORC49007.2020.00030,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
001095256e8fc3211a3f4016f528e06ffde8732c,https://www.semanticscholar.org/paper/001095256e8fc3211a3f4016f528e06ffde8732c,Learning the Effective Dynamics of Complex Multiscale Systems,"Simulations of complex multiscale systems are essential for science and technology ranging from weather forecasting to aircraft design. The predictive capabilities of simulations hinges on their capacity to capture the governing system dynamics. Large scale simulations, resolving all spatiotemporal scales, provide invaluable insight at a high computational cost. In turn, simulations using reduced order models are affordable but their veracity hinges often on linearisation and/or heuristics. Here we present a novel systematic framework to extract and forecast accurately the effective dynamics (LED) of complex systems with multiple spatio-temporal scales. The framework fuses advanced machine learning algorithms with equation-free approaches. It deploys autoencoders to obtain a mapping between fine and coarse grained representations of the system and learns to forecast the latent space dynamics using recurrent neural networks. We compare the LED framework with existing approaches on a number of benchmark problems and demonstrate reduction in computational efforts by several orders of magnitude without sacrificing the accuracy of the system.",ArXiv,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
dc88d2bbcebd810d7c80ba281739908005b12235,https://www.semanticscholar.org/paper/dc88d2bbcebd810d7c80ba281739908005b12235,Neurocompositional computing in human and machine intelligence: A tutorial,"The past decade has produced a revolution in Artificial Intelligence (AI), after a half-century of AI repeatedly failing to meet expectations. What explains the dramatic change from 20th-century to 21st-century AI, and how can remaining limitations of current AI be overcome? Until now, the widely accepted narrative has attributed the recent progress in AI to technical engineering advances that have yielded massive increases in the quantity of computational resources and training data available to support statistical learning in deep artificial neural networks. Although these quantitative engineering innovations are important, here we show that the latest advances in AI are not solely due to quantitative increases in computing power but also qualitative changes in how that computing power is deployed. These qualitative changes have brought about a new type of computing that we call neurocompositional computing . In neurocompositional computing, neural networks exploit two scientific principles that contemporary theory in cognitive science maintains are simultaneously necessary to enable human-level cognition. The Compositionality Principle asserts that encodings of complex information are structures that are systematically composed from simpler structured encodings. The Continuity Principle states that the encoding and processing of information is formalized with real numbers that vary continuously. These principles have seemed irreconcilable until the recent mathematical discovery that compositionality can be realized not only through the traditional discrete methods of symbolic computing, well developed in 20th-century AI, but also through novel forms of continuous neural computing—neurocompositional computing. The unprecedented progress of 21st-century AI has resulted from the use of limited—first-generation—forms of neurocompositional computing. We show that the new techniques now being deployed in second-generation neurocompositional computing create AI systems that are not only more robust and accurate than current systems, but also more comprehensible—making it possible to diagnose errors in, and exert human control over, artificial neural networks through interpretation of their internal states and direct intervention upon those states. Note: This tutorial is intended for those new to this topic, and does not assume familiarity with cognitive science, AI, or deep learning. Appendices provide more advanced material. Each figure, and the associated box explaining it, provides an exposition, illustration, or further details of a main point of the paper; in order to make these figures relatively self-contained, it has sometimes been necessary to repeat some material from the text. For a brief introduction and additional development of some of this material see [212]. . abstract mental processes”",,2022.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
09322829c7cab5fe4f2d3853c631501b87e63276,https://www.semanticscholar.org/paper/09322829c7cab5fe4f2d3853c631501b87e63276,Enriching Undergraduate Mathematics Curriculum with Computer Science Courses,"Traditional mathematics curriculum faces several issues nowadays. The gap between course materials and students’ real-life mathematical experiences, the scattering of knowledge in different courses, and the lack of mathematics applications to other subjects all hinder the learning of students. The emerg-ing trends in data science, machine learning, and artificial intelligence also impel higher education to enrich and refine mathematics education. In order to better incubate students for future, the experience of enriching undergrad-uate mathematics curriculum with computer science courses is introduced in this study. The curriculum is designed and implemented for students who major in applied mathematics to better stimulate the learning, participation, exercise, and innovation. It provides students with comprehensive theoretical and practical knowledge for the challenges and industrial requirements now-adays. Evaluations, major findings, and lessons learned from three refined courses are discussed for more insight into the following deployment and re-finement of the curriculum.",Int. J. Eng. Pedagog.,2021.0,10.3991/ijep.v11i5.21701,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
47c12ddda40b9bc03f50e78d388ee1e463ffa689,https://www.semanticscholar.org/paper/47c12ddda40b9bc03f50e78d388ee1e463ffa689,Deep Neural Network to Predict Diabetes: A Data Science Approach,"Diabetes has become a famous and lethal disease among the low and medium-income countries. People could not overcome this deadly abnormal condition due to the current lifestyle, food habit and the genetic transmittance. Medical practitioners provide advice to prevent the diabetic condition and medications to control as this disease does not have a permanent cure. However, the detection of the disease is being a tidy process and deployment of machine learning predictive models to conduct smart diagnosis/detection is vital in the healthcare domain nowadays. Though several machine learning models were built in this regard, deploying a Deep Neural Network seems less focused. Therefore, a Deep Neural Network model was built with the support of complete preprocessing, class balancing, normalization, feature selection process and hyper-parameter tuning using the cross-validated searching technique. The model achieved 88% of accuracy and 0.88 ROC score and standing out as a promising predictive model in diagnosing/detecting diabetes.",,2021.0,10.35940/ijrte.e5255.039621,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
207634d7e8a2dd145547d4f5a8825b8377190c3c,https://www.semanticscholar.org/paper/207634d7e8a2dd145547d4f5a8825b8377190c3c,Open Science Discovery of Oral Non-Covalent SARS-CoV-2 Main Protease Inhibitors,"The COVID-19 pandemic is a stark reminder that a barren global antiviral pipeline has grave humanitarian consequences. Future pandemics could be prevented by accessible, easily deployable broad-spectrum oral antivirals and open knowledge bases that derisk and accelerate novel antiviral discovery and development. Here, we report the results of the COVID Moonshot, a fully open-science structure-enabled drug discovery campaign targeting the SARS-CoV-2 main protease. We discovered a novel chemical scaffold that is differentiated to current clinical candidates in terms of toxicity and pharmacokinetics liabilities, and developed it into orally-bioavailable inhibitors with clinical potential. Our approach leverages crowdsourcing, high throughput structural biology, machine learning, and exascale molecular simulations. In the process, we generated a detailed map of the structural plasticity of the main protease, extensive structure-activity relationships for multiple chemotypes, and a wealth of biochemical activity data. In a first for a structure-based drug discovery campaign, all compound designs (>18,000 designs), crystallographic data (>500 ligand-bound X-ray structures), assay data (>10,000 measurements), and synthesized molecules (>2,400 compounds) for this campaign were shared rapidly and openly, creating a rich open and IP-free knowledgebase for future anti-coronavirus drug discovery.",,2021.0,10.33774/chemrxiv-2021-585ks-v2,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
69839d5b7efe1fda4b60229748d31b5841601b1b,https://www.semanticscholar.org/paper/69839d5b7efe1fda4b60229748d31b5841601b1b,Data Science in Translational Vision Science and Technology,"Data science involves the use of a variety of quantitative methods (e.g. mathematics, statistics, computer science) to extract useful information from structured and unstructured data.1 Typically, data scientists undertake exploratory data analysis by deploying machine learning principles and algorithms to identify patterns in raw data with the purpose of understanding processes and predicting outcomes. These analytic approaches include predictive causal analytics, prescriptive analytics, andmachine learning for pattern discovery and outcome prediction, and they require a large volume and variety of data (i.e. structured as well as unstructured data).",Translational vision science & technology,2021.0,10.1167/tvst.10.8.20,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7f002c2059f0714b5423f084913e54a50fe1ac58,https://www.semanticscholar.org/paper/7f002c2059f0714b5423f084913e54a50fe1ac58,Deep learning for electroencephalogram (EEG) classification tasks: a review,"Objective. Electroencephalography (EEG) analysis has been an important tool in neuroscience with applications in neuroscience, neural engineering (e.g. Brain–computer interfaces, BCI’s), and even commercial applications. Many of the analytical tools used in EEG studies have used machine learning to uncover relevant information for neural classification and neuroimaging. Recently, the availability of large EEG data sets and advances in machine learning have both led to the deployment of deep learning architectures, especially in the analysis of EEG signals and in understanding the information it may contain for brain functionality. The robust automatic classification of these signals is an important step towards making the use of EEG more practical in many applications and less reliant on trained professionals. Towards this goal, a systematic review of the literature on deep learning applications to EEG classification was performed to address the following critical questions: (1) Which EEG classification tasks have been explored with deep learning? (2) What input formulations have been used for training the deep networks? (3) Are there specific deep learning network structures suitable for specific types of tasks? Approach. A systematic literature review of EEG classification using deep learning was performed on Web of Science and PubMed databases, resulting in 90 identified studies. Those studies were analyzed based on type of task, EEG preprocessing methods, input type, and deep learning architecture. Main results. For EEG classification tasks, convolutional neural networks, recurrent neural networks, deep belief networks outperform stacked auto-encoders and multi-layer perceptron neural networks in classification accuracy. The tasks that used deep learning fell into five general groups: emotion recognition, motor imagery, mental workload, seizure detection, event related potential detection, and sleep scoring. For each type of task, we describe the specific input formulation, major characteristics, and end classifier recommendations found through this review. Significance. This review summarizes the current practices and performance outcomes in the use of deep learning for EEG classification. Practical suggestions on the selection of many hyperparameters are provided in the hope that they will promote or guide the deployment of deep learning to EEG datasets in future research.",Journal of neural engineering,2019.0,10.1088/1741-2552/ab0ab5,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
20e2fa00d226a34d87d88869e5eff23d117d7d89,https://www.semanticscholar.org/paper/20e2fa00d226a34d87d88869e5eff23d117d7d89,OVERCOMING PRACTICAL ENTERPRISE CHALLENGES FOR DEEP LEARNING,"We hear, see or experience AI and Machine Learning in our everyday life these days. Autonomous cars from Google, face or voice recognition to unlock your phone, or that amusing app that tells you if your friend is happy or sad when you click his or her picture. AI has been on the forefront of the technology revolution for the last half a decade. The recent developments in research and technology in this field has given the necessary spark for enterprises to start applying AI and advanced machine-learning solutions. However, practical business problems that leading organizations are trying to solve with AI are a lot more complex than classifying an image as a cat or a dog, and with that complexity comes its own set of problems. Implementing a Deep Learning technology (de facto core of AI techniques) has many technical and functional requirements that are hard to meet when operating in the real world with businesses. In this paper, we explore the variety of challenges that a data science team can face while implementing an AI solution at the enterprise level and reveal some of the best practices to efficiently deliver and adopt AI solutions in an organization. A Mindtree Whitepaper 2 Executive Summary WHAT DEEP LEARNING SOLVE? Business problems that require to generate insights by making predictions or mine patterns from unstructured and unconventional data such as images, text and voice can be solved with Deep Learning. Traditional Machine Learning solutions generally fall behind in terms of accuracy and performance when they deal with such cases. Business use cases ranging from brand sentiment, manufacturing efficiency and process optimization can all benefit from Deep Learning solutions. WHAT ARE THE PERTINENT ROADBLOCKS FOR A DEPLOYING DEEP LEARNING SOLUTION? Enterprises operate under many constraints when working with any technology. The key success factors for any Deep Learning solution is availability of large labeled data sets, knowledge of state-of-the-art techniques in building Deep Learning models and understanding innovative frameworks for deployment and validation of the solution. Meeting these requirements are often hurdled by:  Lack of good quality and large volumes of training data relevant to the business use case  Failure of building robust models with high accuracy due to lack of data  Poor deployment architecture and higher costs per insights since Deep Learning demands higher computational resources  Black box characteristics of Deep Learning models lead to difficulties in validating outputs through traditional business feedback A Mindtree Whitepaper 3 REAL WORLD AI USE CASES At its core, Deep Learning is an advanced version of Machine Learning that can deal with a variety of unstructured data without the need for explicit pattern mining or feature engineering and selection. This makes it significantly more useful in today’s age, where businesses often find themselves dealing with data like images and text, from which manually analyzing and mining information is difficult, time-consuming and sometimes next to impossible. A few leading examples that we see where Deep Learning is enabling business solutions and creating value are:  Image and text analysis for gauging consumer behavior towards brand and marketing campaigns. Say, a consumer product company wants to understand the sentiments of its customers with respect to their brand and customer service. It collects valuable data in the form of tweets, social media posts and blogs, which contain the direct voice of customer. The images of the brand elements (product, banners etc.) the customers are uploading are signals of what people are talking about with respect to your brand. One can correlate the sentiment in the social media posts and correlate that to existing social media marketing campaigns and spends. It helps the company track what kind of brand images are going viral, or if there a spurge of ‘unhealthy’ brand images like crushed product containers, negative comments and so on. By leveraging Deep Learning for images and text, the organization can carefully monitor and score such customer activities. The value generated goes even further by helping maintaining a strong brand sentiment and optimizing marketing expenses. WHAT IS THE MANTRA FOR SUCCESSFUL PROOF OF CONCEPTS FOR DEEP LEARNING SOLUTIONS? Tackling each challenge with the best practices can help ease the adoption and validation of Deep Learning solutions. An integral part of success is to have the know-how of state-of-the-art Deep Learning modeling techniques and Big Data architectures. Systematically tackling the identified challenges involve:  Clearly defining the business problem and breaking it into simpler mathematical problems that can be solved through Deep Learning models  Understand data limitations and build a model training strategy accordingly  Employ the advanced techniques of Deep Learning modeling such as data augmentation and transfer the learning to improve and optimize accuracy of models  Deploy models with cloud-enabled solutions to reduce fixed costs and operationalize the solution at reduced costs per insights  Validate the model with the business considering the continuous learning and evolution of the model through a feedback mechanism and considering all relevant business KPIs such as cost of incorrect predictions, benefits/upside and opportunity costs",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
40c9c5a98db41276c2fb60e0a3ab83061e4324eb,https://www.semanticscholar.org/paper/40c9c5a98db41276c2fb60e0a3ab83061e4324eb,A workflow for segmenting soil and plant X-ray CT images with deep learning in Googles Colaboratory,"X-ray micro-computed tomography (X-ray μCT) has enabled the characterization of the properties and processes that take place in plants and soils at the micron scale. Despite the widespread use of this advanced technique, major limitations in both hardware and software limit the speed and accuracy of image processing and data analysis. Recent advances in machine learning, specifically the application of convolutional neural networks to image analysis, have enabled rapid and accurate segmentation of image data. Yet, challenges remain in applying convolutional neural networks to the analysis of environmentally and agriculturally relevant images. Specifically, there is a disconnect between the computer scientists and engineers, who build these AI/ML tools, and the potential end users in agricultural research, who may be unsure of how to apply these tools in their work. Additionally, the computing resources required for training and applying deep learning models are unique, more common to computer gaming systems or graphics design work, than to traditional computational systems. To navigate these challenges, we developed a modular workflow for applying convolutional neural networks to X-ray μCT images, using low-cost resources in Google’s Colaboratory web application. Here we present the results of the workflow, illustrating how parameters can be optimized to achieve best results using example scans from walnut leaves, almond flower buds, and a soil aggregate. We expect that this framework will accelerate the adoption and use of emerging deep learning techniques within the plant and soil sciences. Introduction: Researchers have long been interested in analyzing the in-situ physical, chemical, and biological properties and processes that take place in plants and soils. To accomplish this, researchers have widely adopted the use of X-ray micro-computed tomography (X-ray μCT) for 3D analysis of flower buds, seeds, leaves, stems, roots, and soils (Anderson et al., 1990; Brodersen et al., 2010; Crestana et al., 1986, 1985; Cuneo et al., 2020; Duncan et al., 2022; Hapca et al., 2011; Helliwell et al., 2013; Mooney et al., 2012; Petrovic et al., 1982; Théroux-Rancourt et al., 2020; Xiao et al., 2021). In plants, researchers have used X-ray μCT to visualize the internal structures of leaves, allowing for the quantification of CO2 diffusion through the leaf based on path length tortuosity from the stomata to the mesophyll (Mathers et al., 2018; Théroux-Rancourt et al., 2021). Other applications of X-ray μCT in plants include the visualization of embolism formation and repair in plant xylem tissue, allowing for the development of new models to better understand drought stress recovery, along with non-destructive quantification of carbohydrates in plant stems (Brodersen et al., 2010; Earles et al., 2018; Torres-Ruiz et al., 2015). In soils, Xray μCT was used to visualize soil porosity, soil aggregate distribution, and plant root growth (Ahmed et al., 2016; Gerth et al., 2021; Helliwell et al., 2013; Keyes et al., 2022.; Mairhofer et al., 2013; Mooney et al., 2012; Tracy et al., 2010; Yudina and Kuzyakov, 2019). Despite the wide use of advanced imaging techniques like X-ray μCT and imaging more generally in agricultural research, major limitations in both hardware and software hinder the speed and accuracy of image processing and data analysis. Historically X-ray μCT data collection was extremely time consuming, and resource intensive as individual scans can exceed 50 Gb in size. Data acquisition rates were limited by the ability of X-ray detectors to transfer data to computers, limited hard drive storage capacity once the data was transferred, and intensive hardware requirements that limited the size of files that could be analyzed at any given time. Many of these limits have been removed as detector hardware has improved, hard drive storage transfer speed and space has increased, and computing hardware has advanced. Now a major limiting step to the wide spread use of X-ray μCT in the agricultural sciences is data analysis; while data can be acquired in hours to seconds, the laborious task of hand segmenting images can lead to analysis times of weeks to years for large data sets (Théroux-Rancourt et al., 2020). Recent advances in machine learning, specifically the application of convolutional neural networks to image analysis, have enabled rapid and accurate segmentation of image data (Chen et al., 2018; Long et al., 2015; Ronneberger et al., 2015). Such applications have met with great success in medical imaging analysis, outperforming radiologists for early cancer diagnosis in Xray μCT images (Lotter et al., 2021). However, challenges remain to applying convolutional neural networks to the analysis of agriculturally relevant X-ray μCT images. Specifically, training accurate models for image segmentation requires the production of hand annotated training datasets, which is time consuming and requires specialized expertise to properly annotate training image data. Further, the computing resources required for training and applying deep learning models are unique, more common to computer gaming systems or graphics design work, rather than traditional computational systems. To navigate these challenges, we developed a modular workflow for image annotation and segmentation using open-source tools. Specifically, image annotation is done in ImageJ and the semantic segmentation of X-ray μCT image data is accomplished using Google’s Colab to run PyTorch implementations of a Fully Convolutional Network (FCN) with a ResNet101.(Chen et al., 2017; He et al., 2015; Long et al., 2015; Paszke et al., 2019; Ronneberger et al., 2015) The FCN architecture, while older, allows for model development on variable size images due to the exclusion of fully connected layers in the FCN architecture.(Long et al., 2015; Ronneberger et al., 2015) The workflow is applicable to any X-ray μCT dataset with the flexibility to work on virtually any image data set, as long as annotated images for the associated image data set are available for model training. Additionally, by developing and deploying the code in Google’s Colaboratory, users have access to free or low-cost GPU resources that might otherwise be cost prohibitive to access. If users have access to better hardware than is available through Colaboratory, the notebooks can be run locally to utilize advanced hardware. Materials and Methods: In the following section we will describe the parameters under which our CT data was collected, how the CT image data was annotated for model training, and the parameters used to train the various models. The actual workflow and corresponding training video can be found here: https://github.com/daripp/XCT_FCN. CT data Acquisition: Leaf sections (3 mm x 7 mm) from English walnut (Juglans regia, and a soil aggregate collected at UC Davis Russel Ranch were scanned at 23 keV using the 10x objective lens with a pixel resolution of 650 nanometers on the X-ray μCT beamline (8.3.2) at the Advanced Light Source (ALS) in Lawrence Berkeley National Laboratory (LBNL), Berkeley, CA USA). Additionally, a almond flower bud (Prunis dulcis) was scanned using a 4x lens with a pixel resolution of 1.72 μm on the same beamline. Raw tomographic image data was reconstructed using TomoPy. Reconstructions were converted to 8-bit tif or png format using ImageJ or the PIL package in Python before further processing (Fig. 1).(Gürsoy et al., 2014) Image Annotation: Leaf images were annotated in ImageJ following Théroux-Rancourt et al. (2020) (Fig.1).(Théroux-Rancourt et al., 2020) Flower bud and soil aggregate images were annotated using Intel’s Computer Vision Annotation Tool (CVAT) and ImageJ (Fig. 1).(Schindelin et al., 2012) Both CVAT and ImageJ are free to use and open source. To annotate the flower bud and soil aggregate, images were imported into CVAT. The exterior border of the bud (i.e. bud scales) and flower were annotated in CVAT and exported as masks. Similarly, the exterior of the soil aggregate and particulate organic matter identified by eye were annotated in CVAT and exported as masks. To annotate air spaces in both the bud and soil aggregate, images were imported into ImageJ. A gaussian blur was applied to the image to decrease noise and then the air space was segmented using thresholding. After applying the threshold, the selected air space region was converted to a binary image with white representing the air space and black representing everything else. This binary image was overlaid upon the original image and the air space within the flower bud and aggregate was selected using the “free hand” tool. Air space outside of the region of interest for both image sets was eliminated. The quality of the air space annotation was then visually inspected for accuracy against the underlying original image; incomplete annotations were corrected using the brush or pencil tool to paint missing air space white and incorrectly identified air space black. Once the annotation was satisfactorily corrected, the binary image of the air space was saved. Finally, the annotations of the bud and flower or aggregate and organic matter were opened in ImageJ and the associated air space mask was overlaid on top of them forming a three-layer mask suitable for training the fully convolutional network. Training General Juglans Leaf Segmentation Model: Images and associated annotations from 6 walnut leaf scans were uploaded to Google Drive (Fig 1). Using Google’s Colaboratory resources, a PyTorch implementation of a Fully Convolutional Network (FCN) with a ResNet-101 backbone was used to train 10 models using 5 image/annotation pairs from 1, 2, 3, 4, and 5 leaves; (5, 10, 15, 20, and 25 images/annotation pairs respectively) (Fig. 1).(He et al., 2015; Paszke et al., 2019) Models were trained using a binary cross-entropy loss function and an Adam optimizer for stochastic optimization with the learning rat",ArXiv,2022.0,10.48550/arXiv.2203.09674,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
91472aeef0381f4d601cf1cab585c2fbb8d1046c,https://www.semanticscholar.org/paper/91472aeef0381f4d601cf1cab585c2fbb8d1046c,Robust Brain Age Estimation based on sMRI via Nonlinear Age-Adaptive Ensemble Learning.,"Precise prediction on brain age is urgently needed by many biomedical areas including mental rehabilitation prognosis as well as various medicine or treatment trials. People began to realize that contrasting physical (real) age and predicted brain age can help to highlight brain issues and evaluate if patients' brains are healthy or not. Such age prediction is often challenging for single model-based prediction, while the conditions of brains vary drastically over age. In this work, we present an age-adaptive ensemble model that is based on the combination of four different machine learning algorithms, including a support vector machine (SVR), a convolutional neural network (CNN) model, and the popular GoogLeNet and ResNet deep networks. The ensemble model proposed here is nonlinearly adaptive, where age is taken as a key factor in the nonlinear combination of various single-algorithm-based independent models. In our age-adaptive ensemble method, the weights of each model are learned automatically as nonlinear functions over age instead of fixed values, while brain age estimation is based on such an age-adaptive integration of various single models. The quality of the model is quantified by the mean absolute errors (MAE) and spearman correlation between the predicted age and the actual age, with the least MAE and the highest Spearman correlation representing the highest accuracy in age prediction. By testing on the Predictive Analysis Challenge 2019 (PAC 2019) dataset, our novel ensemble model has achieved a MAE down to 3.19, which is a significantly increased accuracy in this brain age competition. If deployed in the real world, our novel ensemble model having an improved accuracy could potentially help doctors to identify the risk of brain diseases more accurately and quickly, thus helping pharmaceutical companies develop drugs or treatments precisely, and potential offer a new powerful tool for researchers in the field of brain science.",IEEE transactions on neural systems and rehabilitation engineering : a publication of the IEEE Engineering in Medicine and Biology Society,2022.0,10.1109/TNSRE.2022.3190467,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
95beb6c2a030929486c91d101f05caf8622c1527,https://www.semanticscholar.org/paper/95beb6c2a030929486c91d101f05caf8622c1527,Deploying the Big Data Science Center at the Shanghai Synchrotron Radiation Facility: the first superfacility platform in China,"With recent technological advances, large-scale experimental facilities generate huge datasets, into the petabyte range, every year, thereby creating the Big Data deluge effect. Data management, including the collection, management, and curation of these large datasets, is a significantly intensive precursor step in relation to the data analysis that underpins scientific investigations. The rise of artificial intelligence (AI), machine learning (ML), and robotic automation has changed the landscape for experimental facilities, producing a paradigm shift in how different datasets are leveraged for improved intelligence, operation, and data analysis. Therefore, such facilities, known as superfacilities, which fully enable user science while addressing the challenges of the Big Data deluge, are critical for the scientific community. In this work, we discuss the process of setting up the Big Data Science Center within the Shanghai Synchrotron Radiation Facility (SSRF), China’s first superfacility. We provide details of our initiatives for enabling user science at SSRF, with particular consideration given to recent developments in AI, ML, and robotic automation.",Mach. Learn. Sci. Technol.,2021.0,10.1088/2632-2153/abe193,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a034a02ec75437b30e2438efa05ded5f174afc5f,https://www.semanticscholar.org/paper/a034a02ec75437b30e2438efa05ded5f174afc5f,Decision Support Systems in Temporomandibular Joint Osteoarthritis: A review of Data Science and Artificial Intelligence Applications.,"With the exponential growth of computational systems and increased patient data acquisition, dental research faces new challenges to manage a large quantity of information. For this reason, data science approaches are needed for the integrative diagnosis of multifactorial diseases, such as Temporomandibular joint (TMJ) Osteoarthritis (OA). The Data science spectrum includes data capture/acquisition, data processing with optimized web-based storage and management, data analytics involving in-depth statistical analysis, machine learning (ML) approaches, and data communication. Artificial intelligence (AI) plays a crucial role in this process. It consists of developing computational systems that can perform human intelligence tasks, such as disease diagnosis, using many features to help in the decision-making support. Patient's clinical parameters, imaging exams, and molecular data are used as the input in cross-validation tasks, and human annotation/diagnosis is also used as the gold standard to train computational learning models and automatic disease classifiers. This paper aims to review and describe AI and ML techniques to diagnose TMJ OA and data science approaches for imaging processing. We used a web-based system for multi-center data communication, algorithms integration, statistics deployment, and process the computational machine learning models. We successfully show AI and data-science applications using patients' data to improve the TMJ OA diagnosis decision-making towards personalized medicine.",Seminars in orthodontics,2021.0,10.1053/J.SODO.2021.05.004,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
391a1c4e9a3661d5eae9491e345b7226a9f1677f,https://www.semanticscholar.org/paper/391a1c4e9a3661d5eae9491e345b7226a9f1677f,Predicting Me: The Route to Digital Immortality?,,The Mind-Technology Problem,2021.0,10.1007/978-3-030-72644-7_9,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ff0e0b9c44d749beee3edb75ff7af0f0ee432b1a,https://www.semanticscholar.org/paper/ff0e0b9c44d749beee3edb75ff7af0f0ee432b1a,Current Trends and Future Directions of Large Scale Image and Video Annotation: Observations From Four Years of BIIGLE 2.0,"Marine imaging has evolved from small, narrowly focussed applications to large-scale applications covering areas of several hundred square kilometers or time series covering observation periods of several months. The analysis and interpretation of the accumulating large volume of digital images or videos will continue to challenge the marine science community to keep this process efficient and effective. It is safe to say that any strategy will rely on some software platform supporting manual image and video annotation, either for a direct manual annotation-based analysis or for collecting training data to deploy a machine learning–based approach for (semi-)automatic annotation. This paper describes how computer-assisted manual full-frame image and video annotation is currently performed in marine science and how it can evolve to keep up with the increasing demand for image and video annotation and the growing volume of imaging data. As an example, observations are presented how the image and video annotation tool BIIGLE 2.0 has been used by an international community of more than one thousand users in the last 4 years. In addition, new features and tools are presented to show how BIIGLE 2.0 has evolved over the same time period: video annotation, support for large images in the gigapixel range, machine learning assisted image annotation, improved mobility and affordability, application instance federation and enhanced label tree collaboration. The observations indicate that, despite novel concepts and tools introduced by BIIGLE 2.0, full-frame image and video annotation is still mostly done in the same way as two decades ago, where single users annotated subsets of image collections or single video frames with limited computational support. We encourage researchers to review their protocols for education and annotation, making use of newer technologies and tools to improve the efficiency and effectivity of image and video annotation in marine science.",Frontiers in Marine Science,2021.0,10.3389/fmars.2021.760036,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cec495c9338929c20f6991ae80a97d6300d1c242,https://www.semanticscholar.org/paper/cec495c9338929c20f6991ae80a97d6300d1c242,ProtoTransformer: A Meta-Learning Approach to Providing Student Feedback,"High-quality computer science education is limited by the difficulty of providing instructor feedback to students at scale. While this feedback could in principle be automated, supervised approaches to predicting the correct feedback are bottlenecked by the intractability of annotating large quantities of student code. In this paper, we instead frame the problem of providing feedback as few-shot classification, where a meta-learner adapts to give feedback to student code on a new programming question from just a few examples annotated by instructors. Because data for meta-training is limited, we propose a number of amendments to the typical few-shot learning framework, including task augmentation to create synthetic tasks, and additional side information to build stronger priors about each task. These additions are combined with a transformer architecture to embed discrete sequences (e.g. code) to a prototypical representation of a feedback class label. On a suite of few-shot natural language processing tasks, we match or outperform state-of-the-art performance. Then, on a collection of student solutions to exam questions from an introductory university course, we show that our approach reaches an average precision of 88% on unseen questions, surpassing the 82% precision of teaching assistants. Our approach was successfully deployed to deliver feedback to 16,000 student exam-solutions in a programming course offered by a tier 1 university. This is, to the best of our knowledge, the first successful deployment of a machine learning based feedback to open-ended student code.",ArXiv,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9cd93137427957a0ce5b071965d78b946e6f5088,https://www.semanticscholar.org/paper/9cd93137427957a0ce5b071965d78b946e6f5088,"Call for Papers: Science teaching, learning, and assessment with 21st century, cutting-edge digital ecologies","The science education community has embraced the deployment of contemporaneous technological tools and platforms in the service of improving science teaching, learning, and assessment. Technology use in science education has ranged—among many other things—from computer-assisted instruction in the 1970s, to using microcomputer-based laboratories and first-generation simulations and micro-worlds in the 1980s and throughout the 1990s. The 1990s also witnessed the deployment of interactive videodiscs, multimedia, hypermedia, and other digital resources as cognitive tools in science classrooms (Songer, 2007), which was followed by efforts to harness the power of the Internet to, for instance, share data in support of multisite student-driven inquiry projects, among many other applications (Abd-El-Khalick, 2001). The last two decades featured the expanded and integrated use of learning-specific software applications, interactive visualizations, modeling tools, and immersive e-learning environments in science teaching and learning (Krajcik & Mun, 2014). The last decade or so has witnessed rapid and groundbreaking advancements in technologies and digital platforms, as well as their application to teaching and learning. These include advances that made high-powered computing and powerful applications in serious gaming, and virtual and augmented reality more accessible to mainstream users. Similar advances have been evident in big data curation, data mining and data analytics, natural language processing, as well as next-generation machine learning and the application of artificial intelligence (AI) to the real-time and adaptive assessment of learning. The coordination through powerful computing and AI of tangible, immersive, intelligent, and multiuser technologies and digital media systems (coordinating, for instance, learner interactions with interactive wall displays, intelligent interfaces, multitouch tables, motion sensors, etc.) now allow the creation of digital ecologies that provide learners with highly engaging and authentic interactive science learning experiences. Simultaneously, these technologies and interfaces enable the collection of massive data about the choices, behaviors, and cognition of an individual learner or groups of learners (keyboard strokes, mouse clicks, eye tracking, body movement, etc.) that allow for real-time feedback both to learners and their teachers, as well as the delivery of adaptive and personalized learning experiences (ILSDI, 2014). These digital technologies and ecologies have the potential to transform science teaching and learning, as well as deliver on the promise of more personalized science education experiences in service of promoting scientific literacy for all (NGSS Lead States, 2013) including, but not limited to, historically underrepresented populations in science, culturally, ethnically, and linguistically diverse students, as well as learners in underprivileged and underserved communities in the United States and around the globe. The aims of this Special Issue of JRST is to provide a platform for reporting on empirical research that examines the use and impact of 21st century cutting-edge technologies, technological platforms, technological activity, and digital ecologies on science teaching, learning, and assessment. We also Received: 20 December 2018 Accepted: 20 December 2018",Journal of Research in Science Teaching,2019.0,10.1002/TEA.21529,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8dd6676d6e90821c155b403187fd62b33f1f9bf8,https://www.semanticscholar.org/paper/8dd6676d6e90821c155b403187fd62b33f1f9bf8,Data Science Data Governance,"An increasing number of consequential decisions are made automatically by software that employs machine learning, data analytics, and artificial intelligence to discover decision rules using data. The shift to data driven systems exacerbates gaps between traditional governance and oversight processes and the realities of software-driven decision-making. And with more and more software-mediated systems turning to machine learning, data analytics, and artificial intelligence to discover decision rules using data instead of having humans code those rules by hand, this gap can exist even for the software engineers, data scientists, and system operators who design, build, deploy, and manage the machines that mediate our modern lives. Whether algorithms are approving credit applications, selecting travelers for security screening, driving a car, granting and denying visas, or determining the risk profile of an accused or convicted criminal, there is a broad societal interest in ensuring the good governance of these technologies and building accountable algorithms.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0257c2427bded20dc7a25ed432d69195c4aa1b21,https://www.semanticscholar.org/paper/0257c2427bded20dc7a25ed432d69195c4aa1b21,Hacking Covid-19 with Technology,"The development, implementation and advancement of technology solutions aimed at combating the COVID-19 outbreak are rapidly taking shape in India. Governments, Venture Capitalists, Academic Institutions, Incubators, Start-ups, and businesses large and small are all doing their part to deploy new innovative solutions as quickly as possible. Various databases were searched to look for different advancements in technology during the current coronavirus pandemic. It is seen that on one end nonpharmacological measure (social distancing, self-isolation, clean hands, and face masks) are time-tested and low-tech ways to help mitigate the viral spread. On the other end, Science and technology sector constituting of data science, machine learning, rapid diagnostic tests, mobile-first telehealth and computational simulation systems for drug development, artificial intelligence, virtual collaboration, and data tracking are complex ways of using the technology that have strengthened our pandemic response.",International journal of preventive medicine,2021.0,10.4103/ijpvm.IJPVM_439_20,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
adf13be7ea729496cff080742ab00357550ed583,https://www.semanticscholar.org/paper/adf13be7ea729496cff080742ab00357550ed583,"Citizen science, computing, and conservation: How can ""Crowd AI"" change the way we tackle large-scale ecological challenges?","Camera traps - remote cameras that capture images of passing wildlife - have become a ubiquitous tool in ecology and conservation. Systematic camera trap surveys generate ‘Big Data’ across broad spatial and temporal scales, providing valuable information on environmental and anthropogenic factors affecting vulnerable wildlife populations. However, the sheer number of images amassed can quickly outpace researchers’ ability to manually extract data from these images (e.g., species identities, counts, and behaviors) in timeframes useful for making scientifically-guided conservation and management decisions. Here, we present ‘Snapshot Safari’ as a case study for merging citizen science and machine learning to rapidly generate highly accurate ecological Big Data from camera trap surveys. Snapshot Safari is a collaborative cross-continental research and conservation effort with 1500+ cameras deployed at over 40 eastern and southern Africa protected areas, generating millions of images per year. As one of the first and largest-scale camera trapping initiatives, Snapshot Safari spearheaded innovative developments in citizen science and machine learning. We highlight the advances made and discuss the issues that arose using each of these methods to annotate camera trap data. We end by describing how we combined human and machine classification methods (‘Crowd AI’) to create an efficient integrated data pipeline. Ultimately, by using a feedback loop in which humans validate machine learning predictions and machine learning algorithms are iteratively retrained on new human classifications, we can capitalize on the strengths of both methods of classification while mitigating the weaknesses. Using Crowd AI to quickly and accurately ‘unlock’ ecological Big Data for use in science and conservation is revolutionizing the way we take on critical environmental issues in the Anthropocene era.",Hum. Comput.,2021.0,10.15346/hc.v8i2.123,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
62811d1b98f890569951f8efb67256a86b80455c,https://www.semanticscholar.org/paper/62811d1b98f890569951f8efb67256a86b80455c,"Crowdsourcing, computing, and conservation: how citizen science and artificial intelligence can improve the use of camera trap data to tackle large-scale ecological challenges","Camera traps remote cameras that capture images of passing wildlife have become a ubiquitous tool in ecology and conservation. Systematic camera trap surveys generate ‘Big Data’ across broad spatial and temporal scales, providing valuable information on environmental and anthropogenic factors affecting vulnerable wildlife populations. However, the sheer number of images amassed can quickly outpace researchers’ ability to manually extract data from these images (e.g., species identities, counts, and behaviors) in timeframes useful for making scientifically-guided conservation and management decisions. Here, we present ‘Snapshot Safari’ as a case study for merging citizen science and machine learning to rapidly generate highly accurate ecological data from camera trap surveys. Snapshot Safari is a collaborative crosscontinental research and conservation effort with 1500+ cameras deployed at over 40 protected areas in eastern and southern Africa, generating millions of images per year. As one of the first and largest-scale camera trapping initiatives, Snapshot Safari spearheaded innovative developments in citizen science and machine learning. We highlight the advances made and discuss the challenges that arose using each of these methods to annotate camera trap data. We end by describing how we combined human and machine classification methods (‘Crowd AI’) to create an efficient integrated data pipeline. Ultimately, by using a feedback loop in which humans validate machine learning predictions and machine learning algorithms are iteratively retrained on new human classifications, we can capitalize on the strengths of both classification methods while M.S. Palmer, S.E. Huebner, M. Willi, L. Fortson and C. Packer / Human Computation (2021) 8:2 55 mitigating their weaknesses. Using Crowd AI to quickly and accurately ‘unlock’ ecological Big Data is revolutionizing the way we take on critical environmental issues in the Anthropocene era. 1. CAMERA TRAP ‘BIG DATA’ IN CONSERVATION OPPORTUNITIES AND CHALLENGES The Earth is currently undergoing extreme human-driven biodiversity loss, with devastating impacts on the functioning of ecological systems (Barnosky et al., 2011). In the current extinction crisis, collecting and analyzing Big Data is critical for identifying global biodiversity trends and diagnosing drivers of wildlife decline (Jetz et al., 2012; Stephenson et al., 2017; Kays et al., 2019). Technological advances are transforming our ability to rapidly gather massive quantities of high-resolution ecological data at unprecedented spatial and temporal scales (Rich et al., 2017; Steenweg et al., 2017). Remote (e.g., satellite) and in situ (e.g., camera traps, biologgers) sensors allow monitoring of entire wildlife communities across large areas and over long time periods (O’Connell et al., 2010; Wearn & Glover-Kapfer, 2017). While this information is vital for management and conservation, the rate of accumulation quickly outpaces researchers’ abilities to extract the necessary data from collected information on timescales necessary to effectively understand and protect wildlife populations (Ahumada et al., 2019). Camera traps have emerged as a popular tool in ecological research, enabling systematic collection of spatial and temporal information on wildlife community dynamics (O’Connell et al., 2010). These remote cameras are automatically triggered by passing animals, unobtrusively collecting data on the abundance, distribution, and behavior of mediumand large-bodied vertebrates. Each camera trap trigger generates a photographic record that includes the date, time, and location of the animal observation. Recent technological innovations have improved camera trap capabilities while lowering costs, leading to an exponential increase in the number of camera trap studies (Burton et al., 2015). While camera trap use has grown, a major obstacle to harnessing their full potential is the substantial time and effort involved for researchers to manually annotate each image (Ahumada et al. 2019). Even an average-sized survey (~78 camera traps; Steenweg et al. 2017) produces tens to hundreds of thousands of images over a 12-month period, which can take a research team months of dedicated work to classify (Ahumada et al., 2019; Glover-Kapfer et al., 2019). This image annotation bottleneck severely limits the usefulness of these data to address rapidly-changing local and global challenges. Here, we present a case study of how citizen science and machine learning (ML), independently and, ultimately, combined created new opportunities for efficiently processing Big Data generated by one of the world's largest camera trapping initiatives. ‘Snapshot Safari’ (www.snapshotsafari.org) is a collaboration between dozens of standardized wildlife monitoring surveys across eastern and southern Africa. This network produces vast quantities of image data 56 M.S. Palmer, S.E. Huebner, M. Willi, L. Fortson and C. Packer / Human Computation (2021) 8:2 at a continent-wide scale, analysis of which drives scientific discovery and conservation policy. As one of the first large-scale camera trap projects, Snapshot Safari has been at the forefront of developments in citizen science (Swanson et al., 2015, 2016a,b; Hines et al., 2015; Kosmala et al., 2016), ML (Villa et al., 2017; Norouzzadeh et al., 2018; Tabak et al., 2019; Willi et al., 2019), and now, the powerful integration of human and machine classifications into a single pipeline, what we term ‘Crowd AI’. We outline our successes using citizen science and ML to process camera trap images within the quick timeframe necessary to address critical conservation issues and describe the barriers we encountered employing each of these methods independently. We end by detailing the assimilation of both annotation techniques in our current pipeline and highlighting current challenges and new opportunities presented by using Crowd AI to process ecological data from camera trap images. 2. SNAPSHOT SAFARI PROJECT DESCRIPTION AND AIMS Substantive conservation science increasingly relies on trans-national collaborations to understand the drivers for global wildlife decline (O’Brien et al., 2010; Hampton et al., 2013) and standardized sampling over multiple sites allows direct comparison of the ecological and anthropogenic factors shaping animal communities at regional, continental, or even global scales (Rich et al., 2017; Steenweg et al., 2017). Snapshot Safari began as a single camera trap survey in Serengeti National Park, Tanzania, in 2010 (‘Snapshot Serengeti’; Swanson et al., 2015, 2016a) and has grown into a collaborative effort managing surveys run in conjunction with 30 partner organizations across six African countries. Camera trap monitoring within each protected area provides continuous, fine-scale data on mammal and bird species >1 kg (O’Connell et al., 2010; Wearn & Glover-Kapfer, 2017). Data generated by Snapshot Safari are being used by government agencies, conservation organizations, protected area managers, and academic institutions to understand patterns of wildlife movement, behavior, and interactions and to examine how these trends change across environmental gradients and in the face of anthropogenic perturbations (e.g., Anderson et al., 2016; Swanson et al., 2016b; Palmer et al., 2017, 2019; Allen et al., 2018; Palmer & Packer, 2018; Muzena et al., 2019). In the Snapshot Safari network, over 40 permanently-deployed camera trap surveys (totaling ~1500 individual cameras as of 2020) generate in excess of three million images every year. At each site, cameras are laid out in systematic 5-km2 grids and positioned to maximize the likelihood of observing mediumand large-sized vertebrates (see Swanson et al., 2015 for details on survey design). Each camera is programmed to take a ‘capture event’ – a series of three quickfire images when motion and heat sensors are triggered during the day and a single image at night. These individual surveys operate continuously, monitoring expanses of up to 1400-km2, and are intended to run for a decade or longer. In the following sections, we work through the successes and challenges encountered while developing the Snapshot Safari platform. M.S. Palmer, S.E. Huebner, M. Willi, L. Fortson and C. Packer / Human Computation (2021) 8:2 57 3. FIRST HURDLE: OVERCOMING DATA PROCESSING BOTTLENECKS THROUGH CITIZEN SCIENCE ‘Citizen science’, the colloquial term for the involvement of non-professionals in scientific research, has played a key role in environmental research for centuries (Lepczyk et al., 2009). Technological advances over the past few decades have proliferated the number and diversity of projects citizen scientists can participate in, either as data contributors, processors, or analyzers (Shirk et al., 2012; Kosmala et al., 2016; Watson & Floridi, 2018). In particular, the advent of online citizen science now allows millions of participants to advance scientific research by classifying (predominantly) image, video, or audio information, enabling ecologists to gather and utilize data on a scale that would be infeasible without crowdsourcing (Bonney et al., 2014, 2016; McKinley et al., 2015; Jennett et al., 2016). The world’s largest online citizen science platform, Zooniverse (www.zooniverse.org), was created in 2007 with the goal of engaging volunteers in data classification tasks that would otherwise exceed the researchers’ own capacity (Fortson et al., 2012; Prather et al., 2013; Watson & Floridi, 2018; Trouille et al. 2019). Data object annotation is decomposed into simple tasks and crowdsourced to volunteers (Rosser & Wiggins, 2018). While the Zooniverse platform was originally developed for astronomy projects, through the launch of the Zooniverse Project Builder in 2015 (www.zooniverse.org/lab) it has come to host a broad range of projects in biology, earth sciences, art, history, and social sciences (Simpson et al., 2014, Spiers et al., 2019). T",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a27fbb6c005b1dee24adf58f79fa715d0a1af89f,https://www.semanticscholar.org/paper/a27fbb6c005b1dee24adf58f79fa715d0a1af89f,"Theory-Guided Data Science, A Petrophysical Case Study from the Diyab Formation","
 A practical example of a theory-guided data science case study is presented to evaluate the potential of the Diyab formation, an Upper Jurassic interval, source rock of some of the largest reservoirs in the Arabian Peninsula.
 A workflow base on a three-step approach combining the physics of logging tool response and a probabilistic machine-learning algorithm was undertaken to evaluate four wells of the prospect. At first, a core-calibrated multi-mineral model was established on a concept well for which an extensive suite of logs and core measurements had been acquired. To transfer the knowledge gained from the latter physics-driven interpretation onto the other data-scarce wells, the relationship between the output rock and fluid volumes and their input log responses was then learned by means of a Gaussian Process Regression (GPR). Finally, once trained on the key well, the latter probabilistic algorithm was deployed on the three remaining wells to predict reservoir properties, quantify resource potential and estimate volumetric-related uncertainties. The physics-informed machine-learning approach introduced in this work was found to provide results which matches with the majority of the available core data, while discrepancies could generally be explained by the occurrence of laminations which thickness are under the resolution of nuclear logs.
 Overall, the GPR approach seems to enable an efficient transfer of knowledge from data-rich key wells to other data-scarce wells. As opposed to a more conventional formation evaluation process which is carried out more independently from the key well, the present approach ensures that the final petrophysical interpretation reflects and benefits from the insights and the physics-driven coherency achieved at key well location.","Day 3 Tue, November 30, 2021",2021.0,10.2118/204532-ms,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2839a17cf9e0913488bc549374ccfae57fd7b93a,https://www.semanticscholar.org/paper/2839a17cf9e0913488bc549374ccfae57fd7b93a,"Proceedings of the 9th International Symposium on Symbolic Computation in Software Science, SCSS 2021, Hagenberg, Austria, September 8-10, 2021","This volume contains papers presented at the Ninth International Symposium on Symbolic Computation in Software Science, SCSS 2021. Symbolic Computation is the science of computing with symbolic objects (terms, formulae, programs, representations of algebraic objects, etc.). Powerful algorithms have been developed during the past decades for the major subareas of symbolic computation: computer algebra and computational logic. These algorithms and methods are successfully applied in various fields, including software science, which covers a broad range of topics about software construction and analysis. Meanwhile, artificial intelligence methods and machine learning algorithms are widely used nowadays in various domains and, in particular, combined with symbolic computation. Several approaches mix artificial intelligence and symbolic methods and tools deployed over large corpora to create what is known as cognitive systems. Cognitive computing focuses on building systems that interact with humans naturally by reasoning, aiming at learning at scale. The purpose of SCSS is to promote research on theoretical and practical aspects of symbolic computation in software science, combined with modern artificial intelligence techniques. These proceedings contain the keynote paper by Bruno Buchberger and ten contributed papers. Besides, the conference program included three invited talks, nine short and work-in-progress papers, and a special session on computer algebra and computational logic. Due to the COVID-19 pandemic, the symposium was held completely online. It was organized by the Research Institute for Symbolic Computation (RISC) of the Johannes Kepler University Linz on September 8--10, 2021.",SCSS,2021.0,10.4204/EPTCS.342,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1ad436269bdfa7f335af910bad22c18f7e65c229,https://www.semanticscholar.org/paper/1ad436269bdfa7f335af910bad22c18f7e65c229,Interfacing participation in citizen science projects with conversational agents,"This paper assesses the use of conversational agents (chatbots) as an interface to enhance communication with participants in citizen science projects. After developing a study of the engagement and motivations to interact with chatbots, we explored our results. We based our analysis on the current needs exposed in citizen science literature to assess the opportunities. We found that chatbots are great communication platforms that can help to engage participants as an all-in-one interface. Chatbots can benefit projects in reducing the need for developing an exclusive app while it can be deployed on several platforms. Finally, we establish design suggestions to help citizen science practitioners to incorporate such platforms to new projects. We encourage the development of more advanced interfaces through the incorporation of Machine Learning to several processes.",Hum. Comput.,2021.0,10.15346/hc.v8i2.114,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b2c7e4d8a9a909a6ba3b699e66cfb97f4ea63ac0,https://www.semanticscholar.org/paper/b2c7e4d8a9a909a6ba3b699e66cfb97f4ea63ac0,SOLIS - The MLOps journey from data acquisition to actionable insights,"Machine Learning operations is unarguably a very important and also one of the hottest topics in Artificial Intelligence lately. Being able to define very clear hypotheses for real-life problems that can be addressed by machine learning models, collecting and curating large amounts of data for model training and validation followed by model architecture search and optimization, then finally presenting the results fits very well the scenario of Data Science experiments. However, this approach does not supply the needed procedures and pipelines to deploy machine learning capabilities in real production-grade systems. Automating live configuration mechanisms, on the fly adapting to live or offline data capture and consumption procedures, serving multiple models in parallel either on edge or cloud architectures, addressing specific limitations of GPU memory or compute power, post-processing inference or prediction results, and serving those either as APIs or with IoT based communication stacks, all these in the same end-to-end pipeline are the real challenges that we try to address in this paper. This paper presents a unified deployment pipeline and a freedom-to-operate approach that supports all the above requirements using the known cross-platform tensor frameworks and script language engines.",ArXiv,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
dbc5cd52e9772bac226c246646ce543347ac744d,https://www.semanticscholar.org/paper/dbc5cd52e9772bac226c246646ce543347ac744d,Vector-valued Gaussian Processes on Riemannian Manifolds via Gauge Equivariant Projected Kernels,"Gaussian processes are machine learning models capable of learning unknown functions in a way that represents uncertainty, thereby facilitating construction of optimal decision-making systems. Motivated by a desire to deploy Gaussian processes in novel areas of science, a rapidly-growing line of research has focused on constructively extending these models to handle non-Euclidean domains, including Riemannian manifolds, such as spheres and tori. We propose techniques that generalize this class to model vector fields on Riemannian manifolds, which are important in a number of application areas in the physical sciences. To do so, we present a general recipe for constructing gauge equivariant kernels, which induce Gaussian vector fields, i.e. vector-valued Gaussian processes coherent with geometry, from scalar-valued Riemannian kernels. We extend standard Gaussian process training methods, such as variational inference, to this setting. This enables vector-valued Gaussian processes on Riemannian manifolds to be trained using standard methods and makes them accessible to machine learning practitioners.",ArXiv,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
32acdd6bf696ca1fe5e916e8c5bf554cc6d07ecf,https://www.semanticscholar.org/paper/32acdd6bf696ca1fe5e916e8c5bf554cc6d07ecf,Automatic Grading Tool for Jupyter Notebooks in Artificial Intelligence Courses,"Jupyter notebooks provide an interactive programming environment that allows writing code, text, equations, and multimedia resources. They are widely used as a teaching support tool in computer science and engineering courses. However, manual grading programming assignments in Jupyter notebooks is a challenging task, thus using an automatic grader becomes a must. This paper presents UNCode notebook auto-grader, that offers summative and formative feedback instantaneously. It provides instructors with an easy-to-use grader generator within the platform, without having to deploy a new server. Additionally, we report the experience of employing this tool in two artificial intelligence courses: Introduction to Intelligent Systems and Machine Learning. Several programming activities were carried out using the proposed tool. Analysis of students’ interactions with the tool and the students’ perceptions are presented. Results showed that the tool was widely used to evaluate their tasks, as a large number of submissions were performed. Students expressed positive opinions mostly, giving feedback about the auto-grader, highlighting the usefulness of the immediate feedback and the grading code, among other aspects that helped them to solve the activities. Results remarked on the importance of providing clear grading code and formative feedback to help the students to identify errors and correct them.",Sustainability,2021.0,10.3390/su132112050,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7370210dbaff2962e49b884cf41a34d1a5fabf34,https://www.semanticscholar.org/paper/7370210dbaff2962e49b884cf41a34d1a5fabf34,Portable in vivo measurement of apple sugar content based on mobile phone,"With the development of science and technology, people's living standards continue to improve. Sugar content has been widely studied as an important index to measure fruit quality, but at present, most sugar content detection needs expensive equipment or accessories, which is difficult to enter daily life. In this paper, we propose a convenient scheme for detecting apple sugar degree based on multispectral and machine learning. With the mobile phone screen as the main light source, the front camera captures Apple pictures, and changes the color of the mobile phone screen to change the wavelength of the light source. By photographing different surfaces of apples under different wavelengths of visible light, obtain pictures of the same apple with different spectra, sort out the data, make the data set and train the machine learning network model, deploy the trained network into the app, and then predict the sugar value of apples, so as to achieve the purpose of rapid and nondestructive detection of apple sugar.","2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",2021.0,10.1109/CISP-BMEI53629.2021.9624327,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
afb968c4b78bf2a8f34a9956eb86cde51cd05d5b,https://www.semanticscholar.org/paper/afb968c4b78bf2a8f34a9956eb86cde51cd05d5b,Vector-valued Gaussian Processes on Riemannian Manifolds via Gauge Independent Projected Kernels,"Gaussian processes are machine learning models capable of learning unknown functions in a way that represents uncertainty, thereby facilitating construction of optimal decision-making systems. Motivated by a desire to deploy Gaussian processes in novel areas of science, a rapidly-growing line of research has focused on constructively extending these models to handle non-Euclidean domains, including Riemannian manifolds, such as spheres and tori. We propose techniques that generalize this class to model vector fields on Riemannian manifolds, which are important in a number of application areas in the physical sciences. To do so, we present a general recipe for constructing gauge independent kernels, which induce Gaussian vector fields, i.e. vector-valued Gaussian processes coherent with geometry, from scalar-valued Riemannian kernels. We extend standard Gaussian process training methods, such as variational inference, to this setting. This enables vector-valued Gaussian processes on Riemannian manifolds to be trained using standard methods and makes them accessible to machine learning practitioners.",NeurIPS,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
55ad5e818cfed72317576027fb33a9609210d592,https://www.semanticscholar.org/paper/55ad5e818cfed72317576027fb33a9609210d592,Training and Evaluating a Jupyter Notebook Data Science Assistant,"We study the feasibility of a Data Science assistant powered by a sequence-to-sequence transformer by training a new model JuPyT5 on all publicly available Jupyter Notebook GitHub repositories and developing a new metric: Data Science Problems (DSP). DSP is a collection of 1119 problems curated from 306 pedagogical notebooks with 92 dataset dependencies, natural language and Markdown problem descriptions, and assert-based unit tests. These notebooks were designed to test university students’ mastery of various Python implementations of Math and Data Science, and we now leverage them to study the ability of JuPyT5 to understand and pass the tests. We analyze the content of DSP, validate its quality, and we find that given 100 sampling attempts JuPyT5 is able to solve 77.5% of the DSP problems. We further present various ablation and statistical analyses and compare DSP to other recent natural language to code benchmarks. One focus of machine learning research is to build intelligent assistants which can fill-in or predict information based on a context provided by a user. These agents can be as simple as next-word or phrase prediction like in GMail Smart Compose (Chen et al. 2019) or as complex as conversational agents based on language models like GPT-3 (Brown et al. 2020). Much work has been done to use these agents to solve natural language tasks, and more recently, to solve software engineering tasks. For example Svyatkovskiy et al. (2020) offers line-completion to a user and the Codex model (Chen et al. 2021) offers complete methods and classes. While these works evaluate general-purpose coding, there is an opportunity to focus on an agent which offers suggestions in a pedagogical environment: can we develop an agent which can offer students suggestions to solve problems in data science via Jupyter notebooks? Large language models (Radford et al. 2018) and transformers (Lewis et al. 2019) have unlocked consistent improvements (Kaplan et al. 2020; Brown et al. 2020) in natural language processing and more recently in code synthesis from natural language and examples (Chen et al. 2021; Austin et al. 2021; Clement et al. 2020), code completion (Svyatkovskiy et al. 2020, 2019; Raychev, Vechev, and Yahav 2014; Bruch, Monperrus, and Mezini 2009), code search (Husain et al. 2019; Feng et al. 2020), bug fixing (Drain et al. 2021) and Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. detection (Zhai et al. 2020), unit test generation (Tufano et al. 2020), and many other applications. Extending the evaluation of these transformers beyond traditional NLP metrics like BLEU/ROUGE scores, Chen et al. (2021) and Austin et al. (2021) introduced HumanEval and Mostly Basic Programming Problems (MBPP), respectively, which are sets of natural language descriptions of programs along with unit tests and ground truth Python implementations. By compiling and executing generated hypotheses for these Python programs, these works established generally that larger models solve more problems and that drawing more samples (giving the models more attempts) can solve more problems. Further, Austin et al. (2021) showed that human-in-the-loop feedback with model hypotheses could help the model overcome incorrect solutions. Both works observed generally that models struggled to compose descriptions of multiple chained operations. Figure 1: An example (top) from Data Science Problems which loads a file and suggests a data-cleaning modification of the Pandas dataframe, where a solution is to be inserted between the prompt cell and assert-laden grading cell. Given the prompt and context, our model JuPyT5 correctly interprets and implements this code (bottom). Inspired by these evaluations of natural language modeling 1 ar X iv :2 20 1. 12 90 1v 1 [ cs .L G ] 3 0 Ja n 20 22 of source code, this paper introduces an executable Jupyter notebook metric which tests a models ability to solve data science and college-level Computer Science problems. Jupyter notebooks are hybrid code and documentation environments, organized by cells, containing rich Markdown cells, presentation cells, code cells, and output cells. Jupyter notebooks are used widely in both education and business, promoting easily shareable self-documented code in one user experience. As such, Jupyter notebooks are an important environment to test the efficacy of code generation and program language understanding. This paper offers the following contributions: 1. We introduce a new evaluation called Data Science Problems (DSP)1, a curated set of pedagogical Python notebooks and data contexts containing rich Markdown descriptions of problems, solutions, and unit tests, which uses the teaching tool nbgrader2 to automatically evaluate model hypotheses. DSP problem descriptions also contains natural language with unit tests, featuring LTEX and math, data-dependencies, and implicit dependencies between the problems in a single notebook. 2. We introduce a new ’code-infilling’ pre-training objective, similar to span-mask pre-training and the method-featurefilling objective of PyMT5 (Clement et al. 2020), wherein each cell in each notebook is considered a target in one example, and the source is the neighboring cells with a control code indicating which cell type to produce and where it should be inserted. The resulting model trained by this objective is called JuPyT5 – Jupyter Python Textto-text Transfer Transformer. 3. While we do not follow the trend of exploring model size, we focus on evaluating a model size with more modest deployment cost by training and evaluating one model size of 350M parameters (300M non-embedding parameters). We evaluate this model size trained on the cell-infilling objective for our new DSP metric, showing it can solve 78% of the DSP tasks given 100 sampled attempts. We find similarly that model performance improves with larger number of samples, and more context cells improves the model performance. Surprisingly, training the model with the ability to look ahead a single cell doubles the performance on DSP compared to a 3-cell look-back baseline. Showing the model unit tests also improves performance, and interestingly, the model learns to adapt the solutions to previous problems on a subsequent problem. 4. We also evaluate JuPyT5 on HumanEval and MBPP. JuPyT5 can beat a much larger 68B parameter model at MBPP with a modest 300M parameters, but was also explicitly trained only on code domain sources. While large models are very impressive general-learners from diverse data sources, it is still much more economical to focus a model training on a task domain. JuPyT5 was outperformed by a similar-sized Codex model on HumanEval, but we were able to partially close the gap by adapting HumanEval docstrings to look more like Markdown. We conclude naturally that smaller models are more formatting-sensitive. github.com/microsoft/DataScienceProblems https://nbgrader.readthedocs.io/en/stable/ 5. We evaluate our model trained on all naturally occurring types of Jupyter notebooks on GitHub, and also on a model trained on a subset of the notebooks containing large amounts of Markdown cells, showing improvement on DSP by balancing the training data to contain similar amounts of code and Markdown. Data Science Problems are inspired by several existing code execution evaluations in the literature. The first is the APPS dataset (Hendrycks et al. 2021), a collection of 10,000 problems from code competitions. We did not evaluate on this evaluation as we were interested in more task-grounded domains. The second is HumanEval, introduced with the codefine-tuned GPT-3 model Codex (Chen et al. 2021), which features Python signatures containing doctest unit tests and natural language docstring descriptions of problems to be solve. The third is Mostly Basic Programming Problems (MBPP), which are natural language descriptions of problems along with assert-based unit tests. HumanEval and MBPP are more similar to our new DSP metric, but differ a lot in the task domain. DSP is task-grounded as 35% of its problems explicitly depend on a data context. Further, DSP is an educational domain, so can evaluate the ability of a model to potentially assist in student coaching or continued data science education in business environments. DSP is also larger than HumanEval and MBPP, with over 1000 problem-test pairs, and can test the ability of a model to understand sequences of tasks in context with one another. Figure 2: An example (top) from Data Science Problems describing an implementation of a log-Bernoulli loss function using LTEX. Our model JuPyT5 correctly interprets the LTEX definition of the loss as using torch and even names the function and arguments as described in the prompt.",ArXiv,2022.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a4b965e662899d240c82a1302384a64e9db55526,https://www.semanticscholar.org/paper/a4b965e662899d240c82a1302384a64e9db55526,Data Science Driven Methods for Sustainable and Failure Tolerant Edge Systems,"Nowadays we experience a paradigm shift in our society, where every item around us is becoming a computer facilitating life-changing applications like self-driving cars, tele-medicine, precision agriculture or virtual reality. On one hand, for the execution of such resource demanding applications we need powerful IT facilities. On the other hand, the requirements often include latencies below 100 ms or even below 10 ms -- what is called ''tactile internet''. To facilitate low latency computation has to be placed in the vicinity of the end users by utilizing the concept of Edge Computing. In this talk we explain the challenges of Edge systems in combination with tactile internet. We discuss the recent problems of geographically distributed machine learning applications and novel approaches to balance competing priorities like the energy efficiency and the staleness of the machine learning models. Available failure resilience mechanisms designed for Cloud computing or generic distributed systems cannot be applied to Edge systems due to timeliness, hyper heterogeneity and resource scarcity. Therefore, we discuss a novel machine learning based mechanism that evaluates the failure resilience of a service deployed redundantly on the edge infrastructure. Our approach learns the spatiotemporal dependencies between edge server failures and combines them with the topological information to incorporate link failures by utilizing the concept of the Dynamic Bayesian Networks (DBNs). Eventually, we infer the probability that a certain set of servers fails or disconnects concurrently during service runtime.",ICPE,2022.0,10.1145/3489525.3511694,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
632231776b7ce403dbd5b043d8f621eda366aae8,https://www.semanticscholar.org/paper/632231776b7ce403dbd5b043d8f621eda366aae8,Knowledge Engineering Paradigms for Smart Education and Learning Systems,"Recently, artificial intelligence (AI) receive increasing attention within the field of developing smart digital education. Researchers have been used the computational intelligence (CI) and machine learning techniques methodologies to develop a smart tutoring systems (STSs). On the other side, the convergence of AI, data science and Internet of Things (IoT) is enabling the creation of a new generation of web-based smart systems for all educational and learning tasks. This paper discusses the CI and knowledge engineering paradigms for developing the smart educational and learning systems. In this study the two popular CI paradigms; case-based reasoning and ontological engineering are discussed and analyzed namely. The main objective of this study is to determine and exploration the benefits and advantages of such intelligent paradigms to increase the effectiveness and enhancing the efficiency of the smart tutoring systems. Moreover, the paper addresses the challenges faced by the application developers and knowledge engineers in developing and deploying such systems.","2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",2019.0,10.23919/MIPRO.2019.8756685,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
bf2fc9a43cba9ac7e7ce4d872f3a08fd220cf65f,https://www.semanticscholar.org/paper/bf2fc9a43cba9ac7e7ce4d872f3a08fd220cf65f,Scientific AI in materials science: a path to a sustainable and scalable paradigm,"Recently there has been an ever-increasing trend in the use of machine learning (ML) and artificial intelligence (AI) methods by the materials science, condensed matter physics, and chemistry communities. This perspective article identifies key scientific, technical, and social opportunities that the materials community must prioritize to consistently develop and leverage Scientific AI (SciAI) to provide a credible path towards the advancement of current materials-limited technologies. Here we highlight the intersections of these opportunities with a series of proposed paths forward. The opportunities are roughly sorted from scientific/technical (e.g. development of robust, physically meaningful multiscale material representations) to social (e.g. promoting an AI-ready workforce). The proposed paths forward range from developing new infrastructure and capabilities to deploying them in industry and academia. We provide a brief introduction to AI in materials science and engineering, followed by detailed discussions of each of the opportunities and paths forward.",Mach. Learn. Sci. Technol.,2020.0,10.1088/2632-2153/ab9a20,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c80d6fd9f3da5bd5b3d80b7d9e69b68abbca0781,https://www.semanticscholar.org/paper/c80d6fd9f3da5bd5b3d80b7d9e69b68abbca0781,Engine of Automatic Machine Learning,"As of 2018, many open-source libraries exist out there that solve specific problems in machine learning very well, at least when operated by programmers with a good understanding of the mathematics that underlie them. Despite all the resources being poured into artificial intelligence through these libraries, few tools exist that solve the simple data science needs of users lacking a strong technical background. In developing this project, we set out to capitalize on these high-quality open-source libraries to solve simple machine learning tasks with very little configuration. In this project, we set out to build a program that could, with a high degree of autonomy, execute tasks that data scientists are commonly in charge of, such as training and deploying machine learning models to predict labels in datasets, finding anomalies in spreadsheets etc. Equipped with such a tool, programmers would be able to create applications that automate part of the typical data science workflow and help teams with little technical background profit from recent advances in artificial intelligence. With this use case in mind, we set out to build our solution as a service​: a program that runs on a remote server and executes jobs it receives from a messaging queue. In order to solve machine learning problems autonomously, our goal was to capitalize on existing high-quality data science libraries using whichever library was most suited for the task at hand. In the end, we succeeded in developing a service that is capable of executing classification and regression tasks given any tabular data, and used it to solve competitions on the website Kaggle.",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2516652a00f5620ff27a1a7c99a99a7926466ca2,https://www.semanticscholar.org/paper/2516652a00f5620ff27a1a7c99a99a7926466ca2,MLaaS : A Framework for Exposing Machine Learning as a Service on Cloud Platforms,"​ ​​Machine​ ​Learning​ ​has​ ​been​ ​seeping​ ​into​ ​every​ ​sphere​ ​of​ ​our​ ​lives​ ​from​ ​autonomous driving,​ ​movie​ ​recommendations​ ​and​ ​online​ ​shopping​ ​to​ ​targeted​ ​advertising​ ​campaigns, detecting​ ​anomalous​ ​masses​ ​in​ ​the​ ​brain​ ​and​ ​stock​ ​market​ ​analysis.​ ​With​ ​the​ ​impact​ ​machine learning​ ​has​ ​had​ ​on​ ​our​ ​lives,​ ​there​ ​have​ ​been​ ​advocates​ ​for​ ​democratizing​ ​the​ ​science​ ​behind​ ​it to​ ​make​ ​it​ ​more​ ​accessible​ ​to​ ​people​ ​who​ ​might​ ​need​ ​it.​ ​In​ ​our​ ​project,​ ​we​ ​aim​ ​to​ ​make​ ​a contribution​ ​to​ ​this​ ​drive​ ​by​ ​exposing​ ​the​ ​power​ ​of​ ​machine​ ​learning​ ​as​ ​a​ ​service​ ​on​ ​a​ ​cloud based​ ​distributed​ ​platform.​ ​We​ ​have​ ​striven​ ​to​ ​abstract​ ​the​ ​intricacies​ ​of​ ​training​ ​and​ ​predicting on​ ​data​ ​as​ ​much​ ​as​ ​possible​ ​to​ ​allow​ ​the​ ​end​ ​user​ ​to​ ​focus​ ​on​ ​the​ ​application​ ​of​ ​the​ ​​machine learning​​ ​rather​ ​than​ ​the​ ​science​ ​behind​ ​it.​ ​We​ ​have​ ​designed​ ​a​ ​distributed​ ​framework​ ​capable​ ​of both​ ​automatic​ ​scale-up​ ​and​ ​scale-out​ ​to​ ​ensure​ ​that​ ​the​ ​user​ ​gets​ ​the​ ​performance​ ​they​ ​seek​ ​from the​ ​service​ ​without​ ​any​ ​hiccups. 1.​​ ​​ ​​ ​​ ​​ ​​Introduction: Machine Learning[1] has boomed in the last few years from being used by just a handful of computer engineers exploring whether computers could learn to play games and mimic the human brain, to a broad discipline that has produced fundamental statistical and computational theories of learning trends and patterns in any and all forms of data. In recent years, machine learning has been incorporated into almost all applications, from driving cars to monitoring your heart-rate in order to detect anomalies. But even now, ​machine learning is still inaccessible outside a niche community pushing forward it development. However, that trend is changing and in recent years, we have seen the community working towards making machine learning accessible​ ​to​ ​all​ ​[2]. Our project aims to give anybody access to the learning capabilities of machine learning without the baggage of having to understand the inner workings of its various algorithms. Additionally, the power of machine learning is amplified by the benefits of cloud computing. Users can offload the computational and management heavy-lifting to the cloud wherein the performance guarantee is ensured by the inherent automatic scale-out and scale-up design principles of the same. Our framework is primarily targeted towards IoT developers in need of real-time training and prediction on raw data, which can be accomplished through our API endpoints. However, we have designed our framework to also allow for a more hands-on and guided setting up of a machine​ ​learning​ ​pipeline​ ​through​ ​an​ ​intuitive​ ​user​ ​interface. 2.​​ ​​ ​​ ​​ ​​ ​​Related​ ​Work: Machine Learning as a Service: Baldominos et al.[3] also proposed a platform built on top of Hadoop. Its implementation was capable of handling up to 30 requests at one time while maintaining a response time of less than one second. Our implementation, on the other hand, is not built on top of any existing distributed computing framework. OpenCPU is another open-source platform, launched in 2014, that creates a Web API for R, a popular statistical analysis software environment. However, because it is practically a middleware for accessing R functions, it does not take into account many non-functional requirements like scalability and performance. In the industry, Google, Microsoft, and Amazon have been releasing their own proprietary platforms. Google released its Prediction API2 [4] in 2014 but it largely allows for prediction capabilities on Google’s own pretrained machine learning model. Also in 2014, Microsoft launched Azure Machine Learning .and inn 2015, Amazon released AWS Machine Learning. Their popularity proves that the demand exists but unfortunately, the designs and implementation specifications​ ​of​ ​these​ ​products​ ​are​ ​not​ ​publicly​ ​available. PredictionIO, OpenCPU, and Baldominos’ platforms are built on top of a specific analytical tools and suffer from its restrictions. This means less flexibility for adding new machine learning algorithms, for data storage, and for deployment. Although Hadoop and R are open-source projects, it is not a trivial challenge to adapt them to a new approach. The same happens with the industry players and their proprietary solutions when external developers cannot have access to the​ ​code​ ​to​ ​add​ ​new​ ​algorithms. Machine Learning Through An Interface: We have striven to abstract the complexities involved with setting up a machine learning pipeline as much as possible. This allows users to focus on the results rather than the intermediate steps of reaching their goals. The closest to our work is Keras [5] which offers an easy to use programming interface over more involved backend libraries like Tensorflow [6] or Theano [7]. Our implementation on the other hand is library agnostic and in our attempt to provide fast and reliable results, we have chosen whichever library​ ​or​ ​tool​ ​is​ ​best​ ​suited​ ​to​ ​the​ ​task​ ​at​ ​hand. Moreover, different models can perform better or worse, depending on the used algorithms, parameters and data set. There is no such a thing as the best learning algorithm. For any algorithm, there are data sets that perform very accurately and others that perform very poorly. For the same data set, different algorithms can perform differently because of their own nature. MLaaS helps the user to run multiple algorithms and compare their performances, so the most suitable​ ​algorithm​ ​can​ ​be​ ​chosen. 3.​​ ​​ ​​ ​​Implementation​ ​Details: Our approach for this project is to build an open source cloud framework powered by a highly custom server-client architecture that handles user interaction with management of machine learning​ ​tools. The architecture of our proposed model is described above. The architecture is separated into two primary zones the Client and the Cloud. The MLaaS framework ties components in both these zones together and utilizes the developed mechanisms to alleviate a seamless interaction between the user and cloud framework. The user interacts with a client application which allows several interactions to access the functionalities and features of the MLaaS cloud framework. The user can create a machine learning model using the front-end client application which simultaneously initializes a shared placeholder that has all the information related to the model to be trained including, but not restricted to, the model name, type, parameters and the dataset to be trained. The different​ ​components​ ​and​ ​its​ ​role​ ​in​ ​the​ ​architecture​ ​is​ ​described​ ​below: 1. Client: a. The client application allows the user to manage multiple machine learning models and offers interfaces for these models to be exposed through the cloud as a personalized​ ​RESTful​ ​service. b. The client application exposes an interface to the user for the management of machine learning model. It is responsible for assisting the user with the following operations: 1. Creation​ ​of​ ​a​ ​Machine​ ​Learning​ ​Model 2. Reading​ ​Properties​ ​of​ ​an​ ​existing​ ​Machine​ ​Learning​ ​Model 3. Updating​ ​the​ ​Properties​ ​of​ ​an​ ​existing​ ​Machine​ ​Learning​ ​Model 4. Deleting​ ​an​ ​existing​ ​Machine​ ​Learning​ ​Model 5. Triggering the training of an existing Machine Learning Model on the Distributed​ ​Framework. 6. Displaying the URL for the user to access the Machine Learning model for real-time​ ​use. c. The​ ​following​ ​screenshots​ ​highlight​ ​the​ ​main​ ​features​ ​of​ ​the​ ​Client​ ​Application",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
617412eb294e5dbbcbc481237eaef8e2226d441f,https://www.semanticscholar.org/paper/617412eb294e5dbbcbc481237eaef8e2226d441f,E1 Reconceiving Machine Learning E2 Aims and Background,"Beware of the man of one method or one instrument, either experimental or theoretical. He tends to become method oriented rather than problem oriented. The method-oriented man is shackled: the problem-oriented man is at least reaching freely toward what is most important. 52 Context Machine Learning is a sub-discipline of Information and Communication Technology (ICT) that develops the technologies for machines to recognise and learn patterns in data. It is distinct from, although related to, statistics. It can be differentiated by its focus on creating technology rather than the human-centred analysis of data. It is the science and engineering behind Data Mining. Machine learning is pervasive: it plays a key role in all stages of the scientific process and across diverse fields including bioinformatics, engineering and finance. It is widely accepted that ICT plays an enabling role across almost all technological disciplines. Analogously, Machine Learning plays an enabling role across most parts of ICT, from embedded to enterprise systems, and consequently is a crucial enabler of the Digital Economy 16. Vast quantities of data are now routinely collected and stored because it is affordable to do so. Machine learning makes sense of this data flood. The Problem The massive reduction in the cost of collecting, storing, transporting and processing data has meant an increasing need for tools to make sense of it. Unfortunately, the deployment of modern machine learning tools is more akin to a craft than an engineering discipline: the inference problems to be solved are often under-specified or ill-posed and the available tools are often ad hoc — lacking generality, transparency, usability and interoperability. Our premise is that the root cause of these difficulties is a lack of a clear conceptual basis for machine learning as an information engineering discipline. Research in machine learning is currently organised by technique (e.g. The last of these organisational approaches tend to be monistic — proposing that all problems be framed according to their principles. The first two approaches make no attempt to be comprehensive. There are two key symptoms arising from this lack of conceptual foundations: A lack of direction leading to no clear research agenda and a plethora of incremental advances that do not help solve real problems 31 leading to "" errors of the third kind (giving the right answer to the wrong question) "" 32. Furthermore there is a lack of usability of the tools created. There is …",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
36260dbbf0f9d6c86f833cec70fc452079ba05f2,https://www.semanticscholar.org/paper/36260dbbf0f9d6c86f833cec70fc452079ba05f2,Virtual Gene Concept and a Corresponding Pragmatic Research Program in Genetical Data Science,"Mendel proposed an experimentally verifiable paradigm of particle-based heredity that has been influential for over 150 years. The historical arguments have been reflected in the near past as Mendel’s concept has been diversified by new types of omics data. As an effect of the accumulation of omics data, a virtual gene concept forms, giving rise to genetical data science. The concept integrates genetical, functional, and molecular features of the Mendelian paradigm. I argue that the virtual gene concept should be deployed pragmatically. Indeed, the concept has already inspired a practical research program related to systems genetics. The program includes questions about functionality of structural and categorical gene variants, about regulation of gene expression, and about roles of epigenetic modifications. The methodology of the program includes bioinformatics, machine learning, and deep learning. Education, funding, careers, standards, benchmarks, and tools to monitor research progress should be provided to support the research program.",Entropy,2021.0,10.3390/e24010017,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2152b40170ed648682aedd2fdc5bb92f58c9ae1a,https://www.semanticscholar.org/paper/2152b40170ed648682aedd2fdc5bb92f58c9ae1a,211 Using team science to support outbreak management in a large urban region during the COVID-19 pandemic,"OBJECTIVES/GOALS: To describe how the UCLA Clinical and Translational Science Institute (CTSI) assembled and deployed a science team in support of a local jurisdictions effort to manage and control COVID-19 outbreaks in one of the nations largest metropolitan regions, Los Angeles County (LAC). METHODS/STUDY POPULATION: During the COVID-19 pandemic (2020-21), building an efficient data infrastructure to support outbreak management became a priority for the local health department. In response, the UCLA CTSI assembled a science team with expertise across the translational continuum: epidemiology, laboratory and microbiology, machine learning, health policy, medicine and clinical care, and community engagement. The team partnered with a new LAC Data Science Team to foster a collaborative learning environment for scientists and public health personnel, employing improvement and implementation science to help mitigate COVID-19 outbreaks in sectors including healthcare, skilled nursing facilities, and K-12 education. The goal was a public health workforce that is prepared to problem-solve complex, evolving outbreaks. RESULTS/ANTICIPATED RESULTS: The science team created a learning environment with data modeling and visualization, problem-based learning, and active knowledge and skills acquisition. First, control charts and time series methods were used to visualize COVID-19 data and find signals for action. Second, a series of 16 Grand Rounds offered interactive sessions on problem-solving of outbreak challenges in different sectors. Third, a biweekly Public Health Digest provided fieldworkers with the latest scientific studies on COVID-19. All three elements guided and empowered the workforce to implement timelier, efficient outbreak mitigation strategies in the field. The partnered team also identified barriers to adoption of selected new data and management techniques, revealing areas for further skill-building and data-driven leadership. DISCUSSION/SIGNIFICANCE: The UCLA CTSI science team offered a backbone science infrastructure for helping public health and other sector agencies manage COVID-19 outbreaks and mitigation. It showed promise in bringing and translating science into public health practice. It revealed future priorities for CTSI innovation and scientific support of public agencies.",Journal of Clinical and Translational Science,2022.0,10.1017/cts.2022.113,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6273802518d19437b9de790b61c3a3abf7d916ec,https://www.semanticscholar.org/paper/6273802518d19437b9de790b61c3a3abf7d916ec,Deep Learning Techniques for Geospatial Data Analysis,,ArXiv,2020.0,10.1007/978-3-030-49724-8_3,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f05da1f06d8afce9f1086c37c1b80bda2bb47dec,https://www.semanticscholar.org/paper/f05da1f06d8afce9f1086c37c1b80bda2bb47dec,Sensing Drunken Drivers Using Data Science,Global Effoets for reducing the accidents are growing faster but the accidents happened is growing are the fastest one. An analysis clealy shows around 30 percent of the accidents happening around the road is mainly because of the drunk and drive. Through survelliance camera and alcohol measuring meters we can monitor the drunk and drive incidents around the city. But cars are speedy at highways where deploying security person for checking drunken drivers is unimaginobly difficult. To solve this in the prescribed work we deploy the sensors in every tool booth to monitor the incoming vehicles and stop the drunken drivers to further drive on the highway. This device is built from existing data acquired from the drunken drivers which is analyzed with machine learning techniques and mathematical data analysis models.,2019 5th International Conference on Advanced Computing & Communication Systems (ICACCS),2019.0,10.1109/ICACCS.2019.8728553,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a78a795f6b2916e93e7c5df147a3aea76a1752cb,https://www.semanticscholar.org/paper/a78a795f6b2916e93e7c5df147a3aea76a1752cb,"Jellyfish encounters: science, technology and security in the Anthropocene ocean","ABSTRACT The article furthers the debate on security in the Anthropocene by scrutinizing one crucial aspect of it – that is: the relation between knowledge, technology and security. For this, it takes the emerging debate in International Relations and Critical Security Studies to a space that it has so far neglected: the global ocean. Focusing on countermeasures against rising jellyfish blooms – from early-warning systems to autonomous killer robots – the article studies how digital technologies are increasingly being deployed to cope with anthropogenic environmental risks. Conceptually, the article develops the notion of the Anthropocene ocean. This concept is used to show how complex and entangled phenomena – such as jellyfish blooms – challenge existing regimes of security as well as underlying forms of knowledge production. The analysis shows how attempts to control and manage risks in the ocean space are replaced by experimental forms of governance that are enabled by digital technologies including big data, machine learning and sensors of different kinds. The final part of the article argues that such forms of technological experimentation and related knowledge practices are underwritten by a logic of geopolitics and war.",,2020.0,10.1080/21624887.2020.1815478,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
68bee44cc58b1853c7ddcb41aa3c6d29f363637a,https://www.semanticscholar.org/paper/68bee44cc58b1853c7ddcb41aa3c6d29f363637a,Vamsa: Automated Provenance Tracking in Data Science Scripts,"There has recently been a lot of ongoing research in the areas of fairness, bias and explainability of machine learning (ML) models due to the self-evident or regulatory requirements of various ML applications. We make the following observation: All of these approaches require a robust understanding of the relationship between ML models and the data used to train them. In this work, we introduce the ML provenance tracking problem: the fundamental idea is to automatically track which columns in a dataset have been used to derive the features/labels of an ML model. We discuss the challenges in capturing such information in the context of Python, the most common language used by data scientists. We then present Vamsa, a modular system that extracts provenance from Python scripts without requiring any changes to the users' code. Using 26K real data science scripts, we verify the effectiveness of Vamsa in terms of coverage, and performance. We also evaluate Vamsa's accuracy on a smaller subset of manually labeled data. Our analysis shows that Vamsa's precision and recall range from 90.4% to 99.1% and its latency is in the order of milliseconds for average size scripts. Drawing from our experience in deploying ML models in production, we also present an example in which Vamsa helps automatically identify models that are affected by data corruption issues.",KDD,2020.0,10.1145/3394486.3403205,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
43cb9b50b1ca3e8fd7928ed7ed405f01302f5dcf,https://www.semanticscholar.org/paper/43cb9b50b1ca3e8fd7928ed7ed405f01302f5dcf,Cognitive ecology and social learning inspired machine learning: with particular reference to the evolving of resilient Airborne Networks (AN),"This paper tries to bring together three seemingly only remotely related fields: animal cognition in biology, machine learning in computer science, and the planning and deployment of resilient Airborne Networks. The underlying motivation is that the latest advances in animal behavior ecology such as social learning, innovation, and cognitive ecology may offer some meaningful insights for computational intelligence such as machine learning. Motivated by this expectation, I first review some of the latest advances in the field of animal cognition, with focusing on social learning, teaching, innovation and cognitive ecology. The justification for this focus is not only because they are interesting and are among the most actively studied topics in behavior biology, but also because the existing machine learning research, which from time to time takes cues from cognitive science, seems to only incorporate the traditional learning theory such as associative learning and reinforcement learning. After a briefly review of the major advances in animal learning and cognitive ecology, I look into the possibility to incorporate the principles and mechanisms from social learning and cognitive ecology into a typical machine learning architecture. By examining the Gadanho's (2001, 2003) ALEC (Asynchronous Learning by Emotion and Cognition) architecture, I propose to add a high layer to the ALEC architecture, and the resulting CEML (Cognitive Ecology and social learning inspired Machine Learning) offers a framework that uses a population of agents and can readily consider social learning, teaching, and innovation as well as the influences of environment in a comprehensive manner. Finally, I consider the problem of planning and deployment of Airborne Networks (AN) and suggests that the new CEML should be ideal for tackling the AN problem.",2009 IEEE Aerospace conference,2009.0,10.1109/AERO.2009.4839429,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ee9e36c0abe8fd1c48aa73d1f83bcbb22fe3670b,https://www.semanticscholar.org/paper/ee9e36c0abe8fd1c48aa73d1f83bcbb22fe3670b,Hyper-Tune: Towards Efficient Hyper-parameter Tuning at Scale,"The ever-growing demand and complexity of machine learning are putting pressure on hyper-parameter tuning systems: while the evaluation cost of models continues to increase, the scalability of state-of-the-arts starts to become a crucial bottleneck. In this paper, inspired by our experience when deploying hyper-parameter tuning in a real-world application in production and the limitations of existing systems, we propose Hyper-Tune, an efficient and robust distributed hyper-parameter tuning framework. Compared with existing systems, Hyper-Tune highlights multiple system optimizations, including (1) automatic resource allocation, (2) asynchronous scheduling, and (3) multi-fidelity optimizer. We conduct extensive evaluations on benchmark datasets and a large-scale realworld dataset in production. Empirically, with the aid of these optimizations, Hyper-Tune outperforms competitive hyper-parameter tuning systems on a wide range of scenarios, including XGBoost, CNN, RNN, and some architectural hyper-parameters for neural networks. Compared with the state-of-the-art BOHB and A-BOHB, Hyper-Tune achieves up to 11.2× and 5.1× speedups, respectively. PVLDB Reference Format: Yang Li, Yu Shen, Huaijun Jiang, Wentao Zhang, Jixiang Li, Ji Liu, Ce Zhang, and Bin Cui. Hyper-Tune: Towards Efficient Hyper-parameter Tuning at Scale. PVLDB, 14(1): XXX-XXX, 2020. doi:XX.XX/XXX.XX PVLDB Availability Tag: The source code of this research paper has been made publicly available at https://github.com/PKU-DAIR/HyperTune.",Proc. VLDB Endow.,2022.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d0d4f6c60f463afeb2484ddc6cc44a08a40cfddc,https://www.semanticscholar.org/paper/d0d4f6c60f463afeb2484ddc6cc44a08a40cfddc,Vamsa: Tracking Provenance in Data Science Scripts,"Machine learning (ML) which was initially adopted for search ranking and recommendation systems has firmly moved into the realm of core enterprise operations like sales optimization and preventative healthcare. For such ML applications, often deployed in regulated environments, the standards for user privacy, security, and data governance are substantially higher. This imposes the need for tracking provenance end-to-end, from the data sources used for training ML models to the predictions of the deployed models. 
In this work, we take a first step towards this direction by introducing the ML provenance tracking problem in the context of data science scripts. The fundamental idea is to automatically identify the relationships between data and ML models and in particular, to track which columns in a dataset have been used to derive the features of a ML model. We discuss the challenges in capturing such provenance information in the context of Python, the most common language used by data scientists. We then, present Vamsa, a modular system that extracts provenance from Python scripts without requiring any changes to the user's code. Using up to 450K real-world data science scripts from Kaggle and publicly available Python notebooks, we verify the effectiveness of Vamsa in terms of coverage, and performance. We also evaluate Vamsa's accuracy on a smaller subset of manually labeled data. Our analysis shows that Vamsa's precision and recall range from 87.5% to 98.3% and its latency is typically in the order of milliseconds for scripts of average size.",ArXiv,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f02f6e666aef6b0675cc4a189f9962b716c17487,https://www.semanticscholar.org/paper/f02f6e666aef6b0675cc4a189f9962b716c17487,FARE: Enabling Fine-grained Attack Categorization under Low-quality Labeled Data,"Supervised machine learning classifiers have been widely used for attack detection, but their training requires abundant high-quality labels. Unfortunately, high-quality labels are difficult to obtain in practice due to the high cost of data labeling and the constant evolution of attackers. Without such labels, it is challenging to train and deploy targeted countermeasures. In this paper, we propose FARE, a clustering method to enable fine-grained attack categorization under low-quality labels. We focus on two common issues in data labels: 1) missing labels for certain attack classes or families; and 2) only having coarsegrained labels available for different attack types. The core idea of FARE is to take full advantage of the limited labels while using the underlying data distribution to consolidate the lowquality labels. We design an ensemble model to fuse the results of multiple unsupervised learning algorithms with the given labels to mitigate the negative impact of missing classes and coarsegrained labels. We then train an input transformation network to map the input data into a low-dimensional latent space for fine-grained clustering. Using two security datasets (Android malware and network intrusion traces), we show that FARE significantly outperforms the state-of-the-art (semi-)supervised learning methods in clustering quality/correctness. Further, we perform an initial deployment of FARE by working with a large e-commerce service to detect fraudulent accounts. With realworld A/B tests and manual investigation, we demonstrate the effectiveness of FARE to catch previously-unseen frauds.",NDSS,2021.0,10.14722/NDSS.2021.24403,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
dba3478cb46874a97e301deb0f20f7692c1f1ae9,https://www.semanticscholar.org/paper/dba3478cb46874a97e301deb0f20f7692c1f1ae9,PenDer: Incorporating Shape Constraints via Penalized Derivatives,"When deploying machine learning models in the real-world, system designers may wish that models exhibit certain shape behavior, i.e., model outputs follow a particular shape with respect to input features. Trends such as monotonicity, convexity, diminishing or accelerating returns are some of the desired shapes. Presence of these shapes makes the model more interpretable for the system designers, and adequately fair for the customers. We notice that many such common shapes are related to derivatives, and propose a new approach, PenDer (Penalizing Derivatives), which incorporates these shape constraints by penalizing the derivatives. We further present an Augmented Lagrangian Method (ALM) to learn the joint unconstrained objective function. Experiments on three realworld datasets illustrate that even though both PenDer and state-of-the-art Lattice models achieve similar conformance to shape, PenDer captures better sensitivity of prediction with respect to intended features. We also demonstrate that PenDer achieves better test performance than Lattice while enforcing more desirable shape behavior.",AAAI,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
93b4a6d068806c454851ed0270957161f8c9649f,https://www.semanticscholar.org/paper/93b4a6d068806c454851ed0270957161f8c9649f,Workshop on Machine Learning for Autonomous Vehicles 2017,"3: Efficient deep neural networks for perception in autonomous driving (Jose M. Alvarez, TRI) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:00 AM Abstract Convolutional neural networks have achieved impressive success in many tasks in computer vision such as image classification, object detection / recognition or semantic segmentation. While these networks have proven effective in all these applications, they come at a high memory and computational cost, thus not feasible for applications where power and computational resources are limited. In addition, the process to train the network reduces productivity as it not only requires large computer servers but also takes a significant amount of time (several weeks) with the additional cost of engineering the architecture. In this talk, I first introduce our efficient architecture based on filter-compositions and then, a novel approach to jointly learn the architecture and explicitly account for compression during the training process. Our results show that we can learn much more compact models and significantly reduce training and inference time.Convolutional neural networks have achieved impressive success in many tasks in computer vision such as image classification, object detection / recognition or semantic segmentation. While these networks have proven effective in all these applications, they come at a high memory and computational cost, thus not feasible for applications where power and computational resources are limited. In addition, the process to train the network reduces productivity as it not only requires large computer servers but also takes a significant amount of time (several weeks) with the additional cost of engineering the architecture. In this talk, I first introduce our efficient architecture based on filter-compositions and then, a novel approach to jointly learn the architecture and explicitly account for compression during the training process. Our results show that we can learn much more compact models and significantly reduce training and inference time. Bio: Dr. Jose Alvarez is a senior research scientist at Toyota Research Institute. His main research interests are in developing robust and efficient deep learning algorithms for perception with focus on autonomous vehicles. Previously, he was a researcher at Data61 / CSIRO (formerly NICTA), a Postdoctoral researcher at the Courant Institute of Mathematical Science, New York University, and visiting scholar at University of Amsterdam and Group Research Electronics at Volkswagen. Dr. Alvarez graduated in 2012 and he was awarded the best Ph.D. Thesis award. Dr. Alvarez serves as associate editor for IEEE Trans. on Intelligent Transportation Systems. Abstract 4: Visual 3D Scene Understanding and Prediction for ADAS (Manmohan Chandraker, NEC Labs) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:30 AM4: Visual 3D Scene Understanding and Prediction for ADAS (Manmohan Chandraker, NEC Labs) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:30 AM Abstract: Modern advanced driver assistance systems (ADAS) rely on a range of sensors including radar, ultrasound, LIDAR and cameras. Active sensors have found applications in detecting traffic participants (TPs) such as cars or pedestrians and scene elements (SEs) such as roads. However, camera-based systems have the potential to achieve or augment these capabilities at a much lower cost, while allowing new ones such as determination of TP and SE semantics as well as their interactions in complex traffic scenes. Modern advanced driver assistance systems (ADAS) rely on a range of sensors including radar, ultrasound, LIDAR and cameras. Active sensors have found applications in detecting traffic participants (TPs) such as cars or pedestrians and scene elements (SEs) such as roads. However, camera-based systems have the potential to achieve or augment these capabilities at a much lower cost, while allowing new ones such as determination of TP and SE semantics as well as their interactions in complex traffic scenes. In this talk, we present several technical advances for vision-based ADAS. A common theme is to overcome the challenges posed by lack of large-scale annotations in deep learning frameworks. We introduce approaches to correspondence estimation that are trained on purely synthetic data but adapt well to real data at test-time. We introduce object detectors that are light enough for ADAS, trained with knowledge distillation to retain accuracies of deeper architectures. Our semantic segmentation methods are trained on weak supervision that requires only a tenth of conventional annotation time. We propose methods for 3D reconstruction that use deep supervision to recover fine TP part locations while relying on purely synthetic 3D CAD models. We develop deep ICML 2017 Workshop book Generated Tue Nov 21, 2017 Page 4 of 30 learning frameworks for multi-target tracking, as well as occlusion-reasoning in TP localization and SE layout estimation. Finally, we present a framework for TP behavior prediction in complex traffic scenes that accounts for TP-TP and TP-SE interactions. Our approach allows prediction of diverse multimodal outcomes and aims to account for long-term strategic behaviors in complex scenes. Bio: Manmohan Chandraker is an assistant professor at the CSE department of the University of California, San Diego and leads the computer vision research effort at NEC Labs America in Cupertino. He received a B.Tech. in Electrical Engineering at the Indian Institute of Technology, Bombay and a PhD in Computer Science at the University of California, San Diego. His personal research interests are 3D scene understanding and reconstruction, with applications to autonomous driving and human-computer interfaces. His works have received the Marr Prize Honorable Mention for Best Paper at ICCV 2007, the 2009 CSE Dissertation Award for Best Thesis at UCSD, a PAMI special issue on best papers of CVPR 2011 and the Best Paper Award at CVPR 2014. Abstract 6: 2 x 15 Contributed Talks on Datasets and Occupancy Maps in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 10:30 AM6: 2 x 15 Contributed Talks on Datasets and Occupancy Maps in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 10:30 AM Jonathan Binas, Daniel Neil, Shih-Chii Liu, Tobi Delbruck, DDD17: End-To-End DAVIS Driving Dataset Ransalu Senanayake and Fabio Ramos, Bayesian Hilbert Maps for Continuous Occupancy Mapping in Dynamic Environments Abstract 7: Beyond Hand Labeling: Simulation and Self-Supervision for Self-Driving Cars (Matt Johnson, University of Michigan) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:00 AM7: Beyond Hand Labeling: Simulation and Self-Supervision for Self-Driving Cars (Matt Johnson, University of Michigan) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:00 AM Self-driving cars now deliver vast amounts of sensor data from large unstructured environments. In attempting to process and interpret this data there are many unique challenges in bridging the gap between prerecorded data sets and the field. This talk will present recent work addressing the application of deep learning techniques to robotic perception. We focus on solutions to several pervasive problems when attempting to deploy such techniques on fielded robotic systems. The themes of the talk revolve around alternatives to gathering and curating data sets for training. Are there ways of avoiding the labor-intensive human labeling required for supervised learning? These questions give rise to several lines of research based around self-supervision, adversarial learning, and simulation. We will show how these approaches applied to self-driving car problems have great potential to change the way we train, test, and validate machine learning-based systems. Bio: Matthew Johnson-Roberson is Assistant Professor of Engineering in the Department of Naval Architecture & Marine Engineering and the Department of Electrical Engineering and Computer Science at the University of Michigan. He received a PhD from the University of Sydney in 2010. He has held prior postdoctoral appointments with the Centre for Autonomous Systems CAS at KTH Royal Institute of Technology in Stockholm and the Australian Centre for Field Robotics at the University of Sydney. He is a recipient of the NSF CAREER award (2015). He has worked in robotic perception since the first DARPA grand challenge and his group focuses on enabling robots to better see and understand their environment. Abstract 8: Learning Affordance for Autonomous Driving (JianXiong Xiao, AutoX) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:30 AM8: Learning Affordance for Autonomous Driving (JianXiong Xiao, AutoX) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:30 AM Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception based approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. We evaluate our approach in a virtual racing game as well as real world driving and show that our model can work well to drive a car in a very diverse set of virtual and realistic environments. Jianxiong Xiao (",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c295601603325b21aa2497cd503ab49cb71d40e9,https://www.semanticscholar.org/paper/c295601603325b21aa2497cd503ab49cb71d40e9,E 1 Analysing Iterative Machine Learning Algorithms with In formation Geometric Methods,"Research Associate (level A, step 6) The research associate is essential for adequate progress on the project. We expect the RA to be involved in both aspects of the project (theoretical and algorithmic), depending to an extent on the background of the person we manage to recruit. We have asked for level A step 6 which is the lowest level of appointment the ANU makes for someone with a PhD. The salary for that position would be topped up with a market loading by RSISE because of the huge starting salaries offered in the machine learning field ($100,000US/yr for PhD graduates is not uncommon). We have built in standard annual increments. Programmer (ANU06) Although the majority of the work in this project will be theoretical the eventual goals are practical ones — the development of improved learning algorithms. In order to make honest assessments of the algorithms, and more significantly to attempt their deployment on a range of practical problems, we will need the use of a part-time programmer. We are asking for a programmer for 5 days a month at ANU06 level. This is at a salary that is typically commanded by a good programmer of the sort we seek (we are looking to final year computer science students who would be interested in this sort of part-time work). PhD Scholarships Machine learning is an attractive area for PhD students and we expect to be able to make considerable use of at least two students on this project. Their presence is necessary in order to make satisfactory progress on the range of topics to be investigated. We have used the standard ARC figure for a cost. Note that RSISE will provide $8000 top-up per year in order to better attract high quality candidates. Equipment We have sought funds to provide a standard desktop computing environment for the RA and the programmer(s) (a total of two machines will suffice). We used a figure of $2000 each based on our recent experience in sourcing suitable machines for other projects. (There is little point in getting a quote now because of the rapidity of change; $2000 is a modest figure.) The university provides such facilties for the CIs, but it does not factor provision of such things to staff appointed on specific research grants. We have also sought $2000/yr for maintenance of existing computing equipment (our Beowulf cluster) (a consequence of using …",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7ccf43efaefe83bbd1886f1713a2eed407fe9c37,https://www.semanticscholar.org/paper/7ccf43efaefe83bbd1886f1713a2eed407fe9c37,A machine learning approach to inform developmental milestone achievement for children with autism (Preprint),"
 BACKGROUND
 Care for children with autism spectrum disorder (ASD) can be challenging for families and medical care systems. This is especially true in Low-and-Middle-Income-countries (LMIC) like Bangladesh. To improve family-practitioner communication and developmental monitoring of children with ASD, [spell out] (mCARE) was developed. Within this study, mCARE was used to track child milestone achievement and family socio-demographic assets to inform mCARE feasibility/scalability and family-asset informed practitioner recommendations.
 
 
 OBJECTIVE
 The objectives of this paper are three-fold. First, document how mCARE can be used to monitor child milestone achievement. Second, demonstrate how advanced machine learning models can inform our understanding of milestone achievement in children with ASD. Third, describe family/child socio-demographic factors that are associated with earlier milestone achievement in children with ASD (across five machine learning models).
 
 
 METHODS
 Using mCARE collected data, this study assessed milestone achievement in 300 children with ASD from Bangladesh. In this study, we used four supervised machine learning (ML) algorithms (Decision Tree, Logistic Regression, k-Nearest Neighbors, Artificial Neural Network) and one unsupervised machine learning (K-means Clustering) to build models of milestone achievement based on family/child socio-demographic details. For analyses, the sample was randomly divided in half to train the ML models and then their accuracy was estimated based on the other half of the sample. Each model was specified for the following milestones: Brushes teeth, Asks to use the toilet, Urinates in the toilet or potty, and Buttons large buttons.
 
 
 RESULTS
 This study aimed to find a suitable machine learning algorithm for milestone prediction/achievement for children with ASD using family/child socio-demographic characteristics. For, Brushes teeth, the three supervised machine learning models met or exceeded an accuracy of 95% with Logistic Regression, KNN, and ANN as the most robust socio-demographic predictors. For Asks to use toilet, 84.00% accuracy was achieved with the KNN and ANN models. For these models, the family socio-demographic predictors of “family expenditure” and “parents’ age” accounted for most of the model variability. The last two parameters, Urinates in toilet or potty and Buttons large buttons had an accuracy of 91.00% and 76.00%, respectively, in ANN. Overall, the ANN had a higher accuracy (Above ~80% on average) among the other algorithms for all the parameters. Across the models and milestones, “family expenditure”, “family size/ type”, “living places” and “parent’s age and occupation” were the most influential family/child socio-demographic factors.
 
 
 CONCLUSIONS
 mCARE was successfully deployed in an LMIC (i.e., Bangladesh), allowing parents and care-practitioners a mechanism to share detailed information on child milestones achievement. Using advanced modeling techniques this study demonstrates how family/child socio-demographic elements can inform child milestone achievement. Specifically, families with fewer socio-demographic resources reported later milestone attainment. Developmental science theories highlight how family/systems can directly influence child development and this study provides a clear link between family resources and child developmental progress. Clinical implications for this work could include supporting the larger family system to improve child milestone achievement.
 
 
 CLINICALTRIAL
 We took the IRB from Marquette University Institutional Review Board on July 9, 2020, with the protocol number HR-1803022959, and titled “MOBILE-BASED CARE FOR CHILDREN WITH AUTISM SPECTRUM DISORDER USING REMOTE EXPERIENCE SAMPLING METHOD (MCARE)” for recruiting a total of 316 subjects, of which we recruited 300. (Details description of participants in Methods section)
",,,10.2196/preprints.29242,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
77a7187b9650260f840dce0d23eba8073a784e2e,https://www.semanticscholar.org/paper/77a7187b9650260f840dce0d23eba8073a784e2e,Compounded Mediation: A Data Archaeology of the Newspaper Navigator Dataset,"The increasing roles of machine learning and artificial intelligence in the construction of cultural heritage and humanities datasets necessitate critical examination of the myriad biases introduced by machines, algorithms, and the humans who build and deploy them. From image classification to optical character recognition, the effects of decisions ostensibly made by machines compound through the digitization pipeline and redouble in each step, mediating our interactions with digitally-rendered artifacts through the search and discovery process. As a result, scholars within the digital humanities community have begun advocating for the proper contextualization of cultural heritage datasets within the socio-technical systems in which they are created and utilized. One such approach to this contextualization is the data archaeology, a form of humanistic excavation of a dataset that Paul Fyfe defines as “recover[ing] and reconstitut[ing] media objects within their changing ecologies” [Fyfe 2016]. Within critical data studies, this excavation of a dataset including its construction and mediation via machine learning has proven to be a capacious approach. However, the data archaeology has yet to be adopted as standard practice among cultural heritage practitioners who produce such datasets with machine learning. In this article, I present a data archaeology of the Library of Congress’s Newspaper Navigator dataset, which I created as part of the Library of Congress’s Innovator in Residence program [Lee et al. 2020]. The dataset consists of visual content extracted from 16 million historic newspaper pages in the Chronicling America database using machine learning techniques. In this case study, I examine the manifold ways in which a Chronicling America newspaper page is transmuted and decontextualized during its journey from a physical artifact to a series of probabilistic photographs, illustrations, maps, comics, cartoons, headlines, and advertisements in the Newspaper Navigator dataset [Fyfe 2016]. Accordingly, I draw from fields of scholarship including media archaeology, critical data studies, science and technology studies, and the autoethnography throughout. To excavate the Newspaper Navigator dataset, I consider the digitization journeys of four different pages in Black newspapers included in Chronicling America, all of which reproduce the same photograph of W.E.B. Du Bois in an article announcing the launch of The Crisis, the official magazine of the NAACP. In tracing the newspaper pages’ journeys, I unpack how each step in the Chronicling America and Newspaper Navigator pipelines, such as the imaging process and the construction of training data, not only imprints bias on the resulting Newspaper Navigator dataset but also propagates the bias through the pipeline via the machine learning algorithms employed. Along the way, I investigate the limitations of the Newspaper Navigator dataset and machine learning techniques more generally as they relate to cultural heritage, with a particular focus on marginalization and erasure via algorithmic bias, which implicitly rewrites the archive itself. In presenting this case study, I argue for the value of the data archaeology as a mechanism for contextualizing and critically examining cultural heritage datasets within the communities that create, release, and utilize them. I offer this autoethnographic investigation of the Newspaper Navigator dataset in the hope that it will be considered not only by users of this dataset in particular but also by digital humanities practitioners and end users of cultural heritage datasets writ large.",Digit. Humanit. Q.,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5b1fe016f58ad195bc15d753448243006737c2c0,https://www.semanticscholar.org/paper/5b1fe016f58ad195bc15d753448243006737c2c0,Codification Challenges for Data Science in Construction,"AbstractNew forms of data science, including machine learning and data analytics, are enabled by machine-readable information but are not widely deployed in construction. A qualitative study of inf...",,2020.0,10.1061/(asce)co.1943-7862.0001846,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d3ac65b10af091759863b8e2c488036bf52a2ce6,https://www.semanticscholar.org/paper/d3ac65b10af091759863b8e2c488036bf52a2ce6,DAG Card is the new Model Card,"With the progressive commoditization of modeling capabilities, data-centric AI recognizes that what happens before and after training becomes crucial for realworld deployments. Following the intuition behind Model Cards, we propose DAG Cards as a form of documentation encompassing the tenets of a data-centric point of view. We argue that Machine Learning pipelines (rather than models) are the most appropriate level of documentation for many practical use cases, and we share with the community an open implementation to generate cards from code.",ArXiv,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a91cb4a9995e526f8d01029696138b1db0e6d8f7,https://www.semanticscholar.org/paper/a91cb4a9995e526f8d01029696138b1db0e6d8f7,ml-experiment: A Python framework for reproducible data science,"Nowadays, data science projects are usually developed in an unstructured way, which makes it difficult to reproduce. It is also hard to move from an experimental environment to production. Operational workflows such as containerization, continuous deployment, and cloud orchestration allow data science researchers to move a pipeline from a local environment to the cloud. Being aware of the difficulties of setting those workflows up, this paper presents a framework to ease experiment tracking and operationalizing machine learning by combining existent and well-supported technologies. These technologies include Docker, Mlflow, Ray, among others. The framework provides an opinionated workflow to design and execute experiments either on a local environment or the cloud. ml-experiment includes: an automatic tracking system for the most famous machine learning libraries: Tensorflow, Keras, Fastai, Xgboost and Lightgdm, first-class support for distributed training and hyperparameter optimization, and a Command Line Interface (CLI) for packaging and running projects inside containers.",Journal of Physics: Conference Series,2020.0,10.1088/1742-6596/1603/1/012025,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0a9d51f38afba2a9c22f39cb8b07f3ec7fe75a81,https://www.semanticscholar.org/paper/0a9d51f38afba2a9c22f39cb8b07f3ec7fe75a81,Early Β-Amyloid Accumulation in the Brain Is Associated With Blood T and B Cell Alterations,"Fast and minimally invasive approaches for early, preclinical diagnosis of neurodegenerative Alzheimer’s disease (AD) are highly anticipated. Evidence of adaptive immune cells responding to cerebral β-amyloidosis, one of the pathological hallmarks of AD, has raised the question of whether immune markers could be used as proxies for β-amyloid accumulation in the brain. Here, we deploy multidimensional mass cytometry combined with unbiased machine learning techniques to immunophenotype peripheral blood mononuclear cells from study participants in cross-sectional and longitudinal cohorts. We show that increases in antigen-experienced T and B cell subpopulations in blood are associated with early accumulation of β-amyloid in still cognitively healthy human brains. Our results suggest that preclinical AD pathology stages are associated with systemic alterations of the adaptive immune system. These β-amyloid-induced immunophenotype changes may be exploited in the future to identify and develop novel diagnostic tools for early AD detection and prediction of clinical outcomes. 
 
Funding Information: This work was supported by grants from the Synapsis Foundation – Alzheimer Research Switzerland ARS (No. 2019-PI06 to R.M.N., C.G., and A.G.), the Swiss National Science Foundation (SNF 33CM30-124111, SNF 320030-125387/1 to C.H.), the Maxi Foundation (to C.H.) and the Velux Foundation (to L.K.). 
 
Declaration of Interests: T.K. is currently an employee of Biogen, Switzerland; L.K. and V.Tosevski are employees of Roche, Switzerland; C.H. and R.M.N. are members of the board of directors and shareholders of Neurimmune AG, Switzerland; M.T.F. is co-founder and CSO of the Women’s Brain Project. 
 
Ethics Approval Statement: All participants gave written informed consent. The studies were conducted in concordance with the guidelines of the local ethics committee (Kantonale Ethikkommission Zurich) and the Declaration of Helsinki (World Medical Association, 2013). The current analyses were conducted with permission of the ethics committee for further use of data and samples.",SSRN Electronic Journal,2021.0,10.2139/ssrn.3869111,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
706e81609b6cf4223d90ebd964c2c3709c3255b4,https://www.semanticscholar.org/paper/706e81609b6cf4223d90ebd964c2c3709c3255b4,"Aaron Roth- Foundations for Private, Fair, and Robust Data Science","Much of modern machine learning and statistics is based on the following paradigm: the algorithm designer specifies an objective function, and then optimizes it over some class of models. This is a powerful methodology, but while it generally results in a tool that is exceedingly good as measured by the designers narrow objective function, when the optimization is performed over a rich class of models, the result can have unintended and unanticipated side effects. This is especially problematic when — as is increasingly the norm — the models are trained on people’s sensitive data, or deployed to make important decisions about peoples lives. In these cases, the “side effects” can manifest themselves as gross violations of the social norms that we would expect of human beings occupying the same parts of the decision making pipelines that we are ceding to algorithms: norms like privacy and fairness. This training paradigm also assumes that the environment that the algorithm will operate on is static, and so can have unanticipated consequences when the resulting algorithms and models are deployed in dynamic environments, in which people change their behavior in response to the incentives engendered by the algorithm. And machine learning pipelines make it easy and tempting to re-use the same datasets over and over again, which can lead to spurious conclusions. The main thrust of my research is to discover principled ways to avoid this kind of misbehavior. This includes embedding constraints of “privacy”, “fairness”, and other norms directly into the design of algorithms, using game theoretic reasoning to make predictions about the effects of algorithmic interventions in dynamic environments, and developing algorithmic principles that lead to rigorous statistical guarantees when data can be dynamically re-used. This broad research program involves at least three distinct exercises:",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
27a92f069aa172d734690cc8517bbe84f96c6b08,https://www.semanticscholar.org/paper/27a92f069aa172d734690cc8517bbe84f96c6b08,Performance Evaluation of Distributed Deep Learning Frameworks in Cloud Environment,"2016 has become the year of the Artificial Intelligence explosion. AI technologies are getting more and more matured that most world well-known tech giants are making large investment to increase the capabilities in AI. Machine learning is the science of getting computers to act without being explicitly programmed, and deep learning is a subset of machine learning that uses deep neural network to train a machine to learn features directly from data. Deep learning realizes many machine learning applications which expand the field of AI. At the present time, deep learning frameworks have been widely deployed on servers for deep learning applications in both academia and industry. In training deep neural networks, there are many standard processes or algorithms, but the performance of different frameworks might be different. In this paper we evaluate the running performance of two state-of-the-art distributed deep learning frameworks that are running training calculation in parallel over multi GPU and multi nodes in our cloud environment. We evaluate the training performance of the frameworks with ResNet-50 convolutional neural network, and we analyze what factors that result in the performance among both distributed frameworks as well. Through the experimental analysis, we identify the overheads which could be further optimized. The main contribution is that the evaluation results provide further optimization directions in both performance tuning and algorithmic design. Keywords—Artificial Intelligence, machine learning, deep learning, convolutional neural networks",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
977d48f384bf68c9da6a74fc498720e7efe708d6,https://www.semanticscholar.org/paper/977d48f384bf68c9da6a74fc498720e7efe708d6,Machine Learning in Secondary Education ?,"Given the alarming drop in applications to Computer Science (CS) studies across Europe, any efforts at presenting CS as an interesting, challenging, and useful discipline are welcome and necessary. In this note I present my view that it is possible to introduce motivated secondary-school students to the basic goals, techniques, and applications of Machine Learning. Furthermore, I claim that Machine Learning is almost unique among the subfields of CS with this property, and therefore the Machine Learning community has a special duty to help attracting students to CS. I then present some very preliminary thoughts on a project aiming at developing and deploying appropriate teaching materials, and argue that the PASCAL2 NOE is the ideal agent to set up and coordinate such an initiative in Europe. An even more preliminary version of this note was presented at the 2008 Teaching Machine Learning Workshop in Saint-Étienne, France.",,2008.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6ebe22b41dfdf94f578d212db73e50facdacc546,https://www.semanticscholar.org/paper/6ebe22b41dfdf94f578d212db73e50facdacc546,Serverless Supercomputing: High Performance Function as a Service for Science,"Growing data volumes and velocities are driving exciting new methods across the sciences in which data analytics and machine learning are increasingly intertwined with research. These new methods require new approaches for scientific computing in which computation is mobile, so that, for example, it can occur near data, be triggered by events (e.g., arrival of new data), or be offloaded to specialized accelerators. They also require new design approaches in which monolithic applications can be decomposed into smaller components, that may in turn be executed separately and on the most efficient resources. To address these needs we propose funcX---a high-performance function-as-a-service (FaaS) platform that enables intuitive, flexible, efficient, scalable, and performant remote function execution on existing infrastructure including clouds, clusters, and supercomputers. It allows users to register and then execute Python functions without regard for the physical resource location, scheduler architecture, or virtualization technology on which the function is executed---an approach we refer to as ""serverless supercomputing."" We motivate the need for funcX in science, describe our prototype implementation, and demonstrate, via experiments on two supercomputers, that funcX can process millions of functions across more than 65000 concurrent workers. We also outline five scientific scenarios in which funcX has been deployed and highlight the benefits of funcX in these scenarios.",ArXiv,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
29d12a495ae810de9e3bc6e53af31934a164b821,https://www.semanticscholar.org/paper/29d12a495ae810de9e3bc6e53af31934a164b821,Accelerating Scientific Applications With SambaNova Reconfigurable Dataflow Architecture,"Artificial intelligence (AI)-driven science is an integral component in several science domains such as materials, biology, high energy physics, and smart energy. Science workflows can span one or more computational, observational, and experimental systems. The AI for Science report put forth by a wide community of stakeholders from national laboratories, academia, and industry collectively stress the need for a tighter integration of AI infrastructure ecosystem with experimental and leadership computing facilities. The AI component of science applications, which generally deploy deep learning (DL) models, are unique and exhibit different characteristics from traditional industrial workloads. They implement complex models and typically incorporate hundreds of millions of model parameters. Data from simulations are usually sparse, multimodal, multidimensional, and exhibit temporal and spatial correlations.Moreover, AI-driven science applications benefit from flexible coupling of simulations with DL training or inference. Such complexity of the AI for science workloads with increasingly large DL models is typically limited by traditional computing architectures. The adoption of novel AI architectures and systems aimed to accelerate machine learning models is critical to reduce the time-to-discovery for science. The Argonne Leadership Computing Facility (ALCF), a US Department of Energy Office of Science user facility, provides supercomputing resources to power scientific breakthroughs. Applications with significant DL components are increasingly being run on existing supercomputers at the facility. Scientists at ALCF are exploring novel AI-hardware systems, such as SambaNova, in an attempt to address the challenges in scaling the performance of AI models.",Computing in Science & Engineering,2021.0,10.1109/MCSE.2021.3057203,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0f5dc98f4ad051b7e49aa31cfc8fae5c2b532a1e,https://www.semanticscholar.org/paper/0f5dc98f4ad051b7e49aa31cfc8fae5c2b532a1e,Using system context information to complement weakly labeled data,"Real-world datasets collected with sensor networks often contain incomplete and uncertain labels as well as artefacts arising from the system environment. Complete and reliable labeling is often infeasible for large-scale and long-term sensor network deployments due to the labor and time overhead, limited availability of experts and missing ground truth. In addition, if the machine learning method used for analysis is sensitive to certain features of a deployment, labeling and learning needs to be repeated for every new deployment. To address these challenges, we propose to make use of system context information formalized in an information graph and embed it in the learning process via contrastive learning. Based on realworld data we show that this approach leads to an increased accuracy in case of weakly labeled data and leads to an increased robustness and transferability of the classifier to new sensor locations.",ArXiv,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
34c59ec266e2f1798918d765c2a2519a2b8d03d2,https://www.semanticscholar.org/paper/34c59ec266e2f1798918d765c2a2519a2b8d03d2,ReLink: Complete-Link Industrial Record Linkage Over Hybrid Feature Spaces,"Record Linkage (ReL) is the task of identifying records from a pair of databases referring to the same realworld entity. This has many applications in organisations of all sizes where related data often exist in silos leading to inefficiency in data engineering and analytics applications as well as ineffectiveness of business applications (e.g., unable to personalise marketing campaigns).State-of-the-art (SOTA) machine learning and deep learning based ReL techniques use adaptive similarity measures and learn their relative contributions based on labeled data. However, we report here that they do not work with similar efficacy on industrial data owing to its fundamental differences such as magnitude of schema heterogeneity, need for leveraging structure of the data, lack of training data etc. Through our proposed system ‘ReLink’, we carefully mitigate these challenges and demonstrate that it not only significantly outperforms SOTA baselines on industrial datasets but also on majority of research benchmarks. ReLink introduces the notion of complete-linkage over attributes as well as uses hybrid feature spaces on lexical and semantic similarity measures using pre-trained models such as BERT. Going beyond empirical demonstration, we provide insights and prescriptive guidance on choice of ReL techniques in industrial settings from our observations and lessons learnt from the experience of transferring and deploying for real use-cases in a large financial services organization.",2021 IEEE 37th International Conference on Data Engineering (ICDE),2021.0,10.1109/ICDE51399.2021.00293,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3352eb2b731571ca9ba85e671a214b2e2467d8d9,https://www.semanticscholar.org/paper/3352eb2b731571ca9ba85e671a214b2e2467d8d9,Online and Scalable Model Selection with Multi-Armed Bandits,"Many online applications running on live traffic are powered by machine learning models, for which training, validation, and hyperparameter tuning are conducted on historical data. However, it is common for models demonstrating strong performance in offline analysis to yield poorer performance when deployed online. This problem is a consequence of the difficulty of training on historical data in non-stationary environments. Moreover, the machine learning metrics used for model selection may not sufficiently correlate with real-world business metrics used to determine the success of the applications being tested. These problems are particularly prominent in the Real-Time Bidding (RTB) domain, in which ML models power bidding strategies, and a change in models will likely affect performance of the advertising campaigns. In this work, we present Automatic Model Selector (AMS), a system for scalable online selection of RTB bidding strategies based on realworld performance metrics. AMS employs Multi-Armed Bandits (MAB) to near-simultaneously run and evaluate multiple models against live traffic, allocating the most traffic to the bestperforming models while decreasing traffic to those with poorer online performance, thereby minimizing the impact of inferior models on overall campaign performance. The reliance on offline data is avoided, instead making model selections on a case-by-case basis according to actionable business goals. AMS allows new models to be safely introduced into live campaigns as soon as they are developed, minimizing the risk to overall performance. In livetraffic tests on multiple ad campaigns, the AMS system proved highly effective at improving ad campaign performance.",ArXiv,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
aafdd1a684235fbce45add109f4271110bfadde0,https://www.semanticscholar.org/paper/aafdd1a684235fbce45add109f4271110bfadde0,"The National COVID Cohort Collaborative (N3C): Rationale, design, infrastructure, and deployment","Abstract Objective Coronavirus disease 2019 (COVID-19) poses societal challenges that require expeditious data and knowledge sharing. Though organizational clinical data are abundant, these are largely inaccessible to outside researchers. Statistical, machine learning, and causal analyses are most successful with large-scale data beyond what is available in any given organization. Here, we introduce the National COVID Cohort Collaborative (N3C), an open science community focused on analyzing patient-level data from many centers. Materials and Methods The Clinical and Translational Science Award Program and scientific community created N3C to overcome technical, regulatory, policy, and governance barriers to sharing and harmonizing individual-level clinical data. We developed solutions to extract, aggregate, and harmonize data across organizations and data models, and created a secure data enclave to enable efficient, transparent, and reproducible collaborative analytics. Results Organized in inclusive workstreams, we created legal agreements and governance for organizations and researchers; data extraction scripts to identify and ingest positive, negative, and possible COVID-19 cases; a data quality assurance and harmonization pipeline to create a single harmonized dataset; population of the secure data enclave with data, machine learning, and statistical analytics tools; dissemination mechanisms; and a synthetic data pilot to democratize data access. Conclusions The N3C has demonstrated that a multisite collaborative learning health network can overcome barriers to rapidly build a scalable infrastructure incorporating multiorganizational clinical data for COVID-19 analytics. We expect this effort to save lives by enabling rapid collaboration among clinicians, researchers, and data scientists to identify treatments and specialized care and thereby reduce the immediate and long-term impacts of COVID-19.",J. Am. Medical Informatics Assoc.,2020.0,10.1093/jamia/ocaa196,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6a1ef8c870ccb24eed8c11c761f88ed99c118094,https://www.semanticscholar.org/paper/6a1ef8c870ccb24eed8c11c761f88ed99c118094,Oracle AutoML,"Machine learning (ML) is at the forefront of the rising popularity of data-driven software applications. The resulting rapid proliferation of ML technology, explosive data growth, and shortage of data science expertise have caused the industry to face increasingly challenging demands to keep up with fast-paced develop-and-deploy model lifecycles. Recent academic and industrial research efforts have started to address this problem through automated machine learning (AutoML) pipelines and have focused on model performance as the first-order design objective. We present Oracle AutoML, a novel iteration-free AutoML pipeline designed to not only provide accurate models, but also in a shorter runtime. We are able to achieve these objectives by eliminating the need to continuously iterate over various pipeline configurations. In our feed-forward approach, each pipeline stage makes decisions based on metalearned proxy models that can predict candidate pipeline configuration performances before building the full final model. Our approach, which builds and tunes only the best candidate pipeline, achieves better scores at a fraction of the time compared to state-of-the-art open source AutoML tools, such as H2O and Auto-sklearn. This makes Oracle AutoML a prime candidate for addressing current industry challenges.",Proc. VLDB Endow.,2020.0,10.14778/3415478.3415542,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f51bf5c421a39296272dda52245eac22ab671806,https://www.semanticscholar.org/paper/f51bf5c421a39296272dda52245eac22ab671806,A Model Workflow for GeoDeepDive: Locating Pliocene and Pleistocene Ice-Rafted Debris,"Machine learning technology promises a more efficient and scalable approach to locating and aggregating data and information from the burgeoning scientific literature. Realizing this promise requires provision of applications, data resources, and the documentation of analytic workflows. GeoDeepDive provides a digital library comprising over 13 million peer-reviewed documents and the computing infrastructure upon which to build and deploy search and text-extraction capabilities using regular expressions and natural language processing. Here we present a model GeoDeepDive workflow and accompanying R package to show how GeoDeepDive can be employed to extract spatiotemporal information about site-level records in the geoscientific literature. We apply these capabilities to a proof-of-concept subset of papers in a case study to generate a preliminary distribution of ice-rafted debris (IRD) records in both space and time. We use regular expressions and natural language-processing utilities to extract and plot reliable latitude-longitude pairs from publications containing IRD, and also extract age estimates from those publications. This workflow and R package provides researchers from the geosciences and allied disciplines a general set of tools for querying spatiotemporal information from GeoDeepDive for their own science questions.",,2021.0,10.31223/x54312,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e4adba27eed8693966b15aaad16936688752b04a,https://www.semanticscholar.org/paper/e4adba27eed8693966b15aaad16936688752b04a,FathomNet: A global image database for enabling artificial intelligence in the ocean,"The ocean is experiencing unprecedented rapid change, and visually monitoring marine biota at the spatiotemporal scales needed for responsible stewardship is a formidable task. As baselines are sought by the research community, the volume and rate of this required data collection rapidly outpaces our abilities to process and analyze them. Recent advances in machine learning enables fast, sophisticated analysis of visual data, but have had limited success in the ocean due to lack of data standardization, insufficient formatting, and demand for large, labeled datasets. To address this need, we built FathomNet, an open-source image database that standardizes and aggregates expertly curated labeled data. FathomNet has been seeded with existing iconic and non-iconic imagery of marine animals, underwater equipment, debris, and other concepts, and allows for future contributions from distributed data sources. We demonstrate how FathomNet data can be used to train and deploy models on other institutional video to reduce annotation effort, and enable automated tracking of underwater concepts when integrated with robotic vehicles. As FathomNet continues to grow and incorporate more labeled data from the community, we can accelerate the processing of visual data to achieve a healthy and sustainable global ocean. A vast, changing ocean impacts a myriad of life The ocean is experiencing unprecedented rapid change, and visually monitoring marine biota at the spatiotemporal scales needed for responsible stewardship is a formidable task. Monitoring a space as vast as the ocean1 that is filled with life that we have yet to describe2, using traditional, resource-intensive (e.g., time, person-hours, cost) sampling methodologies are limited in their ability to scale in spatiotemporal resolution and engage diverse communities3. However, with the advent of modern robotics4, low-cost observation platforms, and distributed sensing5, we are beginning to see a paradigm shift in ocean exploration and discovery. This shift is evidenced in oceanographic monitoring via satellite remote sensing of near-surface ocean conditions and the global ARGO float array, where distributed platforms and open data structures are propelling the chemical and remote sensing communities to new scales of observation6, 7. Due to a variety of constraints, large-scale sampling of biological communities or processes below the surface waters of the ocean has largely lagged behind. There are three common modalities for observing biology and biological processes in the ocean – acoustics, “-omics"", and imaging – each with their strengths and weaknesses. Acoustics allow for observations of populationand group-scale dynamics, however individual-scale observations, especially determination of animals down to lower taxonomic groups like species, are challenging tasks8. The promising field of eDNA allows for identification of biological communities based on their shed DNA in the water column. While eDNA studies provide broad-scale views of biological communities with only a few discrete samples, determining the spatial source of the DNA, relating the measurements to population sizes, and the presence of confounding non-marine biological markers in samples are active areas of research that still need to be addressed9. Ultimately, -omics and acoustics approaches rely on visual observations for verification. Imaging, a non-extractive method for ocean observation, enables the identification of animals to the species level, elucidates community structure and spatial relationships in a variety of habitats, and reveals fine-scale behavior of animal groups10. However, processing visual data, particularly data with complex scenes and organisms that require expert classifications, is a resource-intensive process that cannot be scaled without significant investment, capacity building, and advances in automation11, 12. Non-invasively capturing life in the ocean with imaging Imaging is an increasingly common modality for sampling biological communities in a variety of environments due to the ease by which the technology can be deployed, and the number of remotely controlled and autonomous platforms that can be used13. Imaging has also been used for real-time, underwater vehicle navigation and control while performing difficult tasks in complex environments14. Moreover, imaging is an effective engagement tool to share marine life information and the issues facing the ocean with broader communities15, 16. In short, visual data is an invaluable tool for better understanding the ocean and conveying that information broadly. Given all the applications of marine imaging, a number of annotation tools have been developed to manage and analyze visual data. These efforts have resulted in many capable software solutions that can either be deployed locally on a computer, during field expeditions, or broadly on the worldwide web17. However, with the limited availability of experts and the prohibitive costs to annotate and store footage, novel methods for automated annotation of marine visual data are desperately needed. This need is motivating the development and deployment of artificial intelligence and data science tools for ocean ecology. Automating analysis of visual data using artificial intelligence Artificial intelligence (AI) is a broad term that encompasses many different approaches18, some of which have already been used to study marine systems. Statistical learning methods like random forests have been used in the plankton imaging community, achieving automated classification of microscale plants and animals with accuracies greater than 90%11. Unsupervised learning can be deployed with minimal data and a priori knowledge of marine environments, however these algorithms have limited application for automating the detection and classification of objects in marine imagery with sufficient granularity and detail to be used for annotation19. Deep learning algorithms trained on visual data where all objects have been identified (e.g., Figure 1) have improved performance of automated annotation and classification tasks to finer taxonomic levels20, 21, however this approach requires publicly available, large-scale labeled image datasets for training22–24. Image repositories for terrestrial applications have been available to the computer vision (CV) community for many years. ImageNet was the first labeled set based on a hierarchy, or the number of classes (or “things"") in WordNet, with the long-term goal to collect 500 to 1k full-resolution images for 80k concepts, or ⇠50M images22. In order to reach this scale, ImageNet used images scraped from Flickr, resulting in a collection of largely iconic images (e.g., centered objects in relatively uncluttered environments). Like ImageNet, Microsoft’s COCO23 used workers with Amazon’s Mechanical Turk to generate labels (5k labeled instances) for images with 90 classes, resulting in 2.5M labeled instances in more than 328k images. More recently, iNat2017 is a biologically focused dataset built from 675k images of 5k species of animals that have been collected and verified by users of iNaturalist25. Unlike ImageNet and iNat2017, COCO was specifically built to include non-iconic views of “things"" that provides imagery with contextual relationships between objects, which is especially relevant to marine environments.",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2893f7e450862ff7545feb36ed56a6cd28b6d62e,https://www.semanticscholar.org/paper/2893f7e450862ff7545feb36ed56a6cd28b6d62e,Melanoma detection with a deep learning model,"Background: Skin cancer is one of the most common forms of cancer in the world and melanoma is the deadliest type of skin cancer. Both melanoma and melanocytic nevi begin in melanocytes (cells that produce melanin). However, melanocytic nevi are benign whereas melanoma is malignant. This work proposes a deep learning model for classification of these two lesions. Methods: In this analytic study, the database of HAM10000 (human against machine with 10000 training images) dermoscopy images, 1000 melanocytic nevi and 1000 melanoma images were employed, where in each category 900 images were selected randomly and were designated as the training set. The remaining 100 images in each category were considered as the test set. A deep learning convolutional neural network (CNN) was deployed with AlexNet (Krizhevsky et al., 2012) as a pretrained model. The network was trained with 1800 dermoscope images and subsequently was validated with 200 test images. The proposed method removes the need for cumbersome tasks of lesion segmentation and feature extraction. Instead, the CNN can automatically learn and extract useful features from the raw images. Therefore, no image preprocessing is required. Study was conducted at Shahid Beheshti University of Medical Sciences, Tehran, Iran from January to February, 2020. Results: The proposed model achieved an area under the receiver operating characteristic (ROC) curve of 0.98. Using a confidence score threshold of 0.5, a classification accuracy of 93%, sensitivity of 94%, and specificity of 92% was attained. The user can adjust the threshold to change the model performance according to preference. For example, if sensitivity is the main concern; i.e. false negative is to be avoided, then the threshold must be reduced to improve sensitivity at the cost of specificity. The ROC curve shows that to achieve sensitivity of 100%, specificity is decreased to 83%. Conclusion: The results show the strength of convolutional neural networks in melanoma detection in dermoscopy images. The proposed method can be deployed to help dermatologists in identifying melanoma. It can also be implemented for self diagnosis of photographs taken from skin lesions. This may facilitate early detection of melanoma, and hence substantially reduce the mortality chance of this dangerous malignancy.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2c26af6464c842f8462b1d6d8a588aa5f0ef04e1,https://www.semanticscholar.org/paper/2c26af6464c842f8462b1d6d8a588aa5f0ef04e1,Judging machines: philosophical aspects of deep learning,,Synthese,2019.0,10.1007/s11229-019-02167-z,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
71668b03a243dfbcf255c96445b78e5adb7db3e3,https://www.semanticscholar.org/paper/71668b03a243dfbcf255c96445b78e5adb7db3e3,Biomind ArrayGenius and GeneGenius: Web Services Offering Microarray and SNP Data Analysis via Novel Machine Learning Methods,"Analysis of postgenomic biological data (such as microarray and SNP data) is a subtle art and science, and the statistical methods most commonly utilized sometimes prove inadequate. Machine learning techniques can provide superior understanding in many cases, but are rarely used due to their relative complexity and obscurity. A challenge, then, is to make machine learning approaches to data analysis available to the average biologist in a user-friendly way. This challenge is addressed by the Biomind ArrayGenius product, an easy-to-use Web-based system providing microarray analysis based on genetic prognunming, kernel methods, and incorporation of knowledge from biological ontologies; and GeneGenius, its sister product for SNP data. This paper focuses on the obstacles faced and lessons learned in the course of creating, deploying, maintaining and selling ArrayGenius and GeneGenius - many of which are generic to any effort involving the creation of complex AI-based products addressing complex domain problems.",AAAI,2007.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
31e7d4af4798a1311ad6e1aff2b91b3585e07ca6,https://www.semanticscholar.org/paper/31e7d4af4798a1311ad6e1aff2b91b3585e07ca6,"Confluence of Artificial Intelligence and High Performance Computing for Accelerated, Scalable and Reproducible Gravitational Wave Detection","Finding new ways to use artificial intelligence (AI) to accelerate the analysis of gravitational wave data, and ensuring the developed models are easily reusable promises to unlock new opportunities in multi-messenger astrophysics (MMA), and to enable wider use, rigorous validation, and sharing of developed models by the community. In this work, we demonstrate how connecting recently deployed DOE and NSF-sponsored cyberinfrastructure allows for new ways to publish models, and to subsequently deploy these models into applications using computing platforms ranging from laptops to high performance computing clusters. We develop a workflow that connects the Data and Learning Hub for Science (DLHub), a repository for publishing machine learning models, with the Hardware Accelerated Learning (HAL) deep learning computing cluster, using funcX as a universal distributed computing service. We then use this workflow to search for binary black hole gravitational wave signals in open source advanced LIGO data. We find that using this workflow, an ensemble of four openly available deep learning models can be run on HAL and process the entire month of August 2017 of advanced LIGO data in just seven minutes, identifying all four binary black hole mergers previously identified in this dataset, and reporting no misclassifications. This approach, which combines advances in AI, distributed computing, and scientific data infrastructure opens new pathways to conduct reproducible, accelerated, data-driven gravitational wave detection.",ArXiv,2020.0,10.21203/RS.3.RS-138409/V1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7d6e9e6e39c70b947296cb4c17aacf4452fc5b29,https://www.semanticscholar.org/paper/7d6e9e6e39c70b947296cb4c17aacf4452fc5b29,Exploring the Use of Synthetic Gradients for Distributed Deep Learning across Cloud and Edge Resources,"With the explosive growth of data, largely contributed by the rapidly and widely deployed smart devices on the edge, we need to rethink the training paradigm for learning on such realworld data. The conventional cloud-only approach can hardly keep up with the computational demand from these deep learning tasks; and the traditional back propagation based training method also makes it difficult to scale out the training. Fortunately, the continuous advancement in System on Chip (SoC) hardware is transforming edge devices into capable computing platforms, and can potentially be exploited to address these challenges. These observations have motivated this paper’s study on the use of synthetic gradients for distributed training cross cloud and edge devices. We employ synthetic gradients into various neural network models to comprehensively evaluate its feasibility in terms of accuracy and convergence speed. We distribute the training of the various layers of a model using synthetic gradients, and evaluate its effectiveness on the edge by using resource-limited containers to emulate edge devices. The evaluation result shows that the synthetic gradient approach can achieve comparable accuracy compared to the conventional back propagation, for an eight-layer model with both fully-connected and convolutional layers. For a more complex model (VGG16), the training suffers from some accuracy degradation (up to 15%). But it achieves 11% improvement in training speed when the layers of a model are decoupled and trained on separate resource-limited containers, compared to the training of the whole model using the conventional method on the physical machine.",HotEdge,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0e0d72367be96583522cb10194b806d167293b80,https://www.semanticscholar.org/paper/0e0d72367be96583522cb10194b806d167293b80,Teaching AI through machine learning projects,"An introductory Artificial Intelligence (AI) course provides students with basic knowledge of the theory and practice of AI as a discipline concerned with the methodology and technology for solving problems that are difficult to solve by other means. It is generally recognized that an introductory Artificial Intelligence course is challenging to teach. This is, in part, due to the diverse and seemingly disconnected core AI topics that are typically covered. Recently, work has been done to address the diversity of topics covered in the course and to create a theme-based approach. Russell and Norvig present an agent-centered approach [9]. Others have been working to integrate Robotics into the AI course [1, 2, 3].We present work on a project funded by the National Science Foundation with a goal of unifying the artificial intelligence course around the theme of machine learning. This involves the development and testing of an adaptable framework for the presentation of core AI topics that emphasizes the relationship between AI and computer science. Machine learning is inherently connected with the AI core topics and provides methodology and technology to enhance real-world applications within many of these topics. Machine learning also provides a bridge between AI technology and modern software engineering. In his article, Mitchell discusses the increasingly important role that machine learning plays in the software world and identifies three important areas: data mining, difficult-to-program applications, and customized software applications [6].We have developed a suite of adaptable, hands-on laboratory projects that can be closely integrated into the introductory AI course. Each project involves the design and implementation of a learning system which will enhance a particular commonly-deployed application. The goal is to enhance the student learning experience in the introductory artificial intelligence course by (1) introducing machine learning elements into the AI course, (2) implementing a set of unifying machine learning laboratory projects to tie together the core AI topics, and (3) developing, applying, and testing an adaptable framework for the presentation of core AI topics which emphasizes the important relationship between AI and computer science in general, and software development in particular. Details on this project as well as samples of course materials developed are published in [4, 5, 7, 8] and are available at the project website at http://uhaweb.hartford.edu/compsci/ccli.We present an overview of our work along with a detailed presentation of one of these projects and how it meets our goals.The project involves the development of a learning system for web document classification. Students investigate the process of classifying hypertext documents, called tagging, and apply machine learning techniques and data mining tools for automatic tagging. Our experiences using the projects are also presented.",ITICSE '06,2006.0,10.1145/1140124.1140230,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
dc17ccb18ae7b8e0d4800a274b02d13e3497a63c,https://www.semanticscholar.org/paper/dc17ccb18ae7b8e0d4800a274b02d13e3497a63c,METATECH: METeorological Data Analysis for Thermal Energy CHaracterization by Means of Self-Learning Transparent Models,"In the last few years, a large number of smart meters have been deployed in buildings to continuously monitor fine-grained energy consumption. Meteorological data deeply impact energy consumption, and an in-depth analysis of collected and correlated data can uncover interesting and actionable insights to improve the overall energy balance of our communities and to enhance people’s awareness of energy wasting. To effectively extract meaningful and interpretable insights from large collections of energy measurements and multi-dimensional meteorological data, innovative data science methodologies should be devised. Research frontiers are addressing self-learning approaches, which allow non-experts to exploit machine learning techniques more easily, and algorithmic transparency of models, hence providing actionable, explicit, declarative knowledge representation. This paper presents METeorological Data Analysis for Thermal Energy CHaracterization (METATECH), a data mining engine based on both exploratory and unsupervised data analytics algorithms, devised to build transparent models correlating weather conditions and energy consumption in buildings. METATECH exploits a joint approach coupling cluster analysis and generalized association rules to allow a deeper yet human-readable understanding of how meteorological data impact heating consumption. First, a partitional clustering algorithm is applied to weather conditions. Then, resulting clusters are characterized by means of generalized association rules, which provide a self-learning explainable model of the most interesting correlations between energy consumption and weather conditions at different granularity levels. The experimental evaluation performed on real datasets demonstrates the effectiveness of the proposed approach in automatically extracting interesting knowledge from data, and provide it transparently to domain experts.",,2018.0,10.3390/EN11061336,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e48cb1800abe66268b8f91b240b73367a8fe9214,https://www.semanticscholar.org/paper/e48cb1800abe66268b8f91b240b73367a8fe9214,Enhancing undergraduate AI courses through machine learning projects,"It is generally recognized that an undergraduate introductory artificial intelligence course is challenging to teach. This is, in part, due to the diverse and seemingly disconnected core topics that are typically covered. The paper presents work funded by the National Science Foundation to address this problem and to enhance the student learning experience in the course. Our work involves the development of an adaptable framework for the presentation of core AI topics through a unifying theme of machine learning. A suite of hands-on semester-long projects are developed, each involving the design and implementation of a learning system that enhances a commonly-deployed application. The projects use machine learning as a unifying theme to tie together the core AI topics. In this paper, we will first provide an overview of our model and the projects being developed and will then present in some detail our experiences with one of the projects, Web User Profiling, which we have used in our AI class",Proceedings Frontiers in Education 35th Annual Conference,2005.0,10.1109/FIE.2005.1611941,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d59d71744ea7c49975c9a0fd8d0b1801c67d6d2d,https://www.semanticscholar.org/paper/d59d71744ea7c49975c9a0fd8d0b1801c67d6d2d,SEARCHING THE EDGES OF THE PROTEIN UNIVERSE USING DATA SCIENCE,"Data science uses the latest techniques in statistics and machine learning to extract insights from data. With the increasing amount of protein data, a number of novel research approaches have become feasible.Micropeptides are an emerging field in the protein universe. They are small proteins with 200 bp and does not encode a protein. Contrary to their “noncoding” definition, an increasing number of lncRNAs have been found to be translated into functional micropeptides. Therefore, whether most lncRNAs are translated is an open question of great significance. To address this question, by harnessing the availability of large-scale human variation data, we have explored the relationships between lncRNAs, micropeptides, and canonical regular proteins (> 100 aa) from the perspective of genetic variation, which has long been used to study natural selection to infer functional relevance. Through rigorous statistical analyses, we find that lncRNAs share a similar genetic variation profile with proteins regarding single nucleotide polymorphism (SNP) density, SNP spectrum, enrichment of rare SNPs, etc., suggesting lncRNAs are under similar negative selection strength with proteins. Our study revealed similarities between micropeptides, lncRNAs, and canonical proteins and is the first attempt to explore the relationships between the three groups from a genetic variation perspective.Deep learning has been tremendously successful in 2D image recognition. Protein binding ligand prediction is fundamental topic in protein research as most proteins bind ligands to function. Proteins are 3D structures and can be considered as 3D images. Prediction of binding ligands of proteins can then be converted to a 3D image classification problem. In addition, a large number of protein structure data are available now. We therefore utilized deep learning to predict protein binding ligands by designing a 3D convolutional neural network from scratch and by building a large 3D image dataset of protein structures. The trained model achieved an average F1 score of over 0.8 across 151 classes on the holdout test set. Compared to existing methods, our model performed better. In summary, we showed the feasibility of deploying deep learning in protein structure research.In conclusion, by exploring various edges of the protein universe from the perspective of data science, we showed that the increasing amount of data and the advancement of data science methods made it possible to address a wide variety of pressing biological questions. We showed that for a successful data science study, the three components – goal, data, method – all of them are indispensable. We provided three successful data science studies: the careful data cleaning and selection of machine learning algorithm lead to the development of MiPepid that fits the urgent need of a micropeptide prediction method; identifying the question and exploring it from a different angle lead to the key insight that lncRNAs resemble micropeptides; applying deep learning to protein structure data lead to a new approach to the long-standing question of protein-ligand binding. The three studies serve as excellent examples in solving a wide range of data science problems with a variety of issues.",,2020.0,10.25394/PGS.12209987.V1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
eb288f80300e07d2da618f408fe82ecdd05bdc1f,https://www.semanticscholar.org/paper/eb288f80300e07d2da618f408fe82ecdd05bdc1f,SEAFLOOR MAPPING FROM MULTISPECTRAL MULTIBEAM ACOUSTIC DATA AT THE EUROPEAN OPEN SCIENCE CLOUD,"Abstract. Recent technological advances in the underwater sensing instrumentation provide currently active multibeam echosounders that can acquire backscatter observations from multiple spectral frequencies. In this paper, the main objective was to design, develop and validate an efficient and robust multispectral, multibeam data processing framework including advanced machine learning tools for seabed classification. In order to do so, we have integrated different machine learning tools like support vector machines and random forests towards the classification of seabed classes. We have performed extensive experiments with different splitting ratios, regarding training and testing sets, in order to assess possible overfitting. The entire pipeline has been implemented in a scalable containerized manner in order to be deployed in cloud infrastructures and more specifically at the European Open Science Cloud. Experimental results, the performed qualitative and quantitative evaluation along with the comparison with the state of the art indicated the quite promising potential of our approach.",,2020.0,10.5194/isprs-archives-xliii-b2-2020-985-2020,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
745a0bf6606c592124f7cfd9a8a8ce2ce9b22453,https://www.semanticscholar.org/paper/745a0bf6606c592124f7cfd9a8a8ce2ce9b22453,Unifying an Introduction to Artificial Intelligence Course through Machine Learning Laboratory Experiences,"This paper presents work on a collaborative project funded by the National Science Foundation that incorporates machine learning as a unifying theme to teach fundamental concepts typically covered in the introductory Artificial Intelligence courses. The project involves the development of an adaptable framework for the presentation of core AI topics. This is accomplished through the development, implementation, and testing of a suite of adaptable, hands-on laboratory projects that can be closely integrated into the AI course. Through the design and implementation of learning systems that enhance commonly-deployed applications, our model acknowledges that intelligent systems are best taught through their application to challenging problems. The goals of the project are to (1) enhance the student learning experience in the AI course, (2) increase student interest and motivation to learn AI by providing a framework for the presentation of the major AI topics that emphasizes the strong connection between AI and computer science and engineering, and (3) highlight the bridge that machine learning provides between AI technology and modern software engineering. In this paper we will present our approach, an overview of the project, and the hands-on laboratory modules. Our preliminary experiences incorporating these modules into our introductory AI course will also be presented.",,2005.0,10.18260/1-2--14721,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
559d0ebab46656f64c6eb5e347de891be2170b1d,https://www.semanticscholar.org/paper/559d0ebab46656f64c6eb5e347de891be2170b1d,Machine Learning Techniques for Short-Term Rain Forecasting System in the Northeastern Part of Thailand,"This paper presents the methodology from machine learning approaches for short-term rain forecasting system. Decision Tree, Artificial Neural Network (ANN), and Support Vector Machine (SVM) were applied to develop classification and prediction models for rainfall forecasts. The goals of this presentation are to demonstrate (1) how feature selection can be used to identify the relationships between rainfall occurrences and other weather conditions and (2) what models can be developed and deployed for predicting the accurate rainfall estimates to support the decisions to launch the cloud seeding operations in the northeastern part of Thailand. Datasets collected during 2004-2006 from the Chalermprakiat Royal Rain Making Research Center at Hua Hin, Prachuap Khiri khan, the Chalermprakiat Royal Rain Making Research Center at Pimai, Nakhon Ratchasima and Thai Meteorological Department (TMD). A total of 179 records with 57 features was merged and matched by unique date. There are three main parts in this work. Firstly, a decision tree induction algorithm (C4.5) was used to classify the rain status into either rain or no-rain. The overall accuracy of classification tree achieves 94.41% with the five-fold cross validation. The C4.5 algorithm was also used to classify the rain amount into three classes as no-rain (0-0.1 mm.), few-rain (0.110 mm.), and moderate-rain (>10 mm.) and the overall accuracy of classification tree achieves 62.57%. Secondly, an ANN was applied to predict the rainfall amount and the root mean square error (RMSE) were used to measure the training and testing errors of the ANN. It is found that the ANN yields a lower RMSE at 0.171 for daily rainfall estimates, when compared to next-day and next-2-day estimation. Thirdly, the ANN and SVM techniques were also used to classify the rain amount into three classes as no-rain, few-rain, and moderate-rain as above. The results achieved in 68.15% and 69.10% of overall accuracy of same-day prediction for the ANN and SVM models, respectively. The obtained results illustrated the comparison of the predictive power of different methods for rainfall estimation. Keywords—Machine learning, decision tree, artificial neural network, support vector machine, root mean square error. L. Ingsrisawang and P. Aungsuratana are with Department of Statistics, Kasetsart University, Faculty of Science, Kasetsart University, Bangkok, Thailand (e-mail: fscilli@ku.ac.th). Supawadee Ingsriswang is with National Center for Genetic Engineering and Biotechnology, Bangkok, Thailand. P. Aungsuratana and W. Khantiyanan are with Bureau of the Royal Rainmaking and Agriculture Aviation, Bangkok, Thailand.",,2008.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3e2081b6f541add86094fd9a29f20426ac58ac68,https://www.semanticscholar.org/paper/3e2081b6f541add86094fd9a29f20426ac58ac68,Understanding I/O behavior of Scientific Deep Learning Applications in HPC systems,"In the past decade, deep learning (DL) has been applied to a wide range of applications [1], [2], [3] to achieve unprecedented results. These include image recognition [4], natural language processing [5], and even autonomous driving [6], as well as physical science domains such as cosmology [7], materials science [8], [9], and biology [10], [11]. DL methods iteratively adjust the weights within the network to minimize a loss function. At each training step, the application reads a mini-batch of data, computes the gradient of the loss function and then updates the weights of the network using stochastic gradient descent. Many new AI hardware (e.g., GPU, TPU, Cerebras, etc.) have been designed and deployed to accelerate the computation during the training. However, as the size and complexity of the datasets grow rapidly, DL training becomes increasingly read intensive. I/O is a potential bottleneck in the DL applications [12]. On the other hand, more and more scientific DL studies are performed in high performance supercomputers through a distributed training framework to reduce the training time-tosolution [13]. Therefore, characterizing the I/O behavior of DL workloads in high-performance computing (HPC) environment is crucial for us to address any potential bottlenecks in I/O and to provide useful guidance in performing efficient parallel I/O. In this study, we aim to understand the I/O behavior in scientific DL applications. As a starting point, we explore a collection of scientific deep learning workloads which are currently running at Argonne Leadership Computing Facility (ALCF). These workloads are selected from various projects, such as Argonne Data Science Program (ADSP), Aurora Early Science Program (ESP), and Exascale Computing Projects (ECP). The science domains represented by the workloads include neutrino physics [7], cosmology [14], materials science [8], computational physics [15], and biology [16], [10]. Many of the workloads are in active development targeting the upcoming future exascale supercomputers. One of the long term goals for this study is to identify any existing I/O bottlenecks in these workloads on current production machines and suggest I/O optimizations for current applications and as we develop these for future systems. We profile the I/O behavior of eight DL applications on Theta, our current production leadership supercomputer at ALCF. To realize this, we utilize the profilers provided by the DL framework such as TensorFlow profiler as well as lowlevel I/O profiler such as Darshan, to study the I/O behavior of applications on supercomputers. These profilers are accompanied with their analysis tools. However, to get a holistic view of the application, we developed a Python library, VaniDL, for integrating and post-processing the information obtained from the profiling tools and generating high level I/O summary of the application. The main contributions of this work are: 1) proposing a systematic framework for I/O profiling for DL workloads and developing an analyzer tool, VaniDL, which provides insights on the I/O behavior of DL applications. 2) preliminary exploration of the I/O behavior of eight scientific DL applications on a leadership supercomputer.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4d8e7c7f09ed151ee1e9fd37a4638e528432526e,https://www.semanticscholar.org/paper/4d8e7c7f09ed151ee1e9fd37a4638e528432526e,Adaptive and Blended Learning _ the Panacea for the Challenges of E-Learning,"In spite of increased use of information and communication technologies in many areas, E-learning is not widely used in Higher Education as it is still considered as an inferior way of education and also due to the fact it carries its own barriers. In this article the advantages as well as the disadvantages of Elearning are listed. Then the solutions such as Blended Learning and Adaptive Learning embedded with Gamification, Group Dynamics and Graphic Organizers and Multimedia packages to solve the impediments are dealt with, in detail. The population of the study included the teaching staff and undergraduate students of An Nabhanya College of Science and Arts. About 60 Questionnaires are distributed amongst them. Questionnaires are designed consisting three parts. The first part deals with the number of courses where e-learning is implemented and the second part lists the benefits availed by the teachers and students by E-Learning. The third part enlists the problems faced by the students and the instructors during E-Learning. The study indicates that there is a poor milieu as far as educational environment is concerned due to the boredom of e-learning, there is also the lack of proper resources, PCs, Wi-Fi’s and study materials and also, procrastination, lack of motivation, lack of serious Assignments. Moreover, there is also a dearth of well trained and experienced Instructors and technical staff in the campus. Thereby this paper insists that concentration is to be bestowed on Blended Learning and adaptive E-learning as tools to overcome the disadvantages. Also this article suggests that the teaching staff should be provided with many professional Development Sessions and Workshops in ELearning, Blended -Learning Adaptive E-Learning. It also suggests that there can be availability of a technical staff for each programme in the colleges. Key Terms: E-Learning Challenges, Blended Learning , Adaptive Learning, Professional Development Sessions, Provision of Technical Assistants. 1. Background of the study Both globalization and the innovations in information and communication technology brought in a seachange in learning. One of the such born e-children, e-learning, the new changing face of learning, is learning eased by electronic technology with the objective of better the learners’ knowledge, skills and productive capabilities. It comprises of an extensive array of digitalization approaches, components and delivery methods in education through internet. Distance education, online instruction, e-learning, online training, asynchronous/synchronous learning, distant education, and web-based education programs are some of the most popular terms recently being used in instructional and technological contexts (Moore, Dickson-Deane & Galyen, 2011)20. E-learning is utilized by all walks of people. This paper presents a brief overview the concept of e-learning, the history of e-learning, the benefits of e-learning, unfortunately there are many a drawback to eLearning which makes the education community ponder on the question of ‘To be E or Not to be E’. Fortunately, adaptive e-learning and blended Learning can help to overcome the disadvantages of e-learning. The current research study will certainly play an important role in providing valuable insights about the attitudes of students and the staff at An Nabhanya College of Science and Arts Girls’ College towards elearning activities. The research study will therefore suggest the deployment of instructors and staff who are professionally well trained in Blended Learning and adaptive learning. © 2020 JETIR May 2020, Volume 7, Issue 5 www.jetir.org (ISSN-2349-5162) JETIR2005241 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 708 2. E-learning 2.1 What is E-learning Millions and millions of millennials have already knocked at the Gateways of E-Learning, as it has become the ‘summum bonum’ of educational setting. It is internet-enabled or web-based (LaRose et al, 1998)16 learning. There are live instructions where specialized instructors can remain in their own places and deliver lessons to students in different locations. Video content delivery can be viewed and reviewed whenever students need. The e-learning concept means all the computer based educational tools or systems will allow the learners to receive their lessons anytime and anywhere. The transition from the usage of just the CD-ROM to the delivery of e-learning lessons [2] that bridges the geographical gap and conducts online virtual classes and that leads the students roam in a new landscape and lets them actively participate [1] is beyond one’s imagination. 2.2 The History of E-learning The term “e-learning” came into existence in 1999, though words used to describe e-learning such as “online learning” and “virtual learning” were in existence earlier in the 19th century [1]. Isaac Pitman taught his pupils shorthand via correspondence in the 1840s paving the road to e-learning. Then, in 1954, BF Skinner invented the “teaching machine” a big achievement to help the schools to administer programmed instruction to their students. In 1960, the first computer-based training program was introduced to the world. This computer-based training program (CBT) was also termed as Programmed Logic for Automated Teaching Operations (PLATO). It was originally designed for students attending the University of Illinois.[1] In the second half of the 20th century, e-learning tools and delivery methods have become very much in vogue. Then, the advent of Virtual learning began and thrived. 2.3 E Learning in KSA The e-learning milieu in KSA is a shining like a star embedded sky as every University tries to board the e-learning bandwagon. The Custodian of the Two Holy Mosques King Abdullah launching the first phase of the university and higher education city projects on an iPad in a ceremony in the first week of May,2012 speaks volumes of the nation’s interest and efforts in promoting e learning. The ministry of Higher education in the kingdom has already set up a National centre that caters for both E learning and distance learning. In 2007, KSA established a new university, the Knowledge International University (KIU), dedicated to the use of e-learning resources. The MOHE has set up a repository for e-learning material to help universities adopt e-learning and e-books for engineering, medical, computer science and humanities courses, these are either planned or available and academics are able to receive training. [Al-Kahtani SA. 2001]3 2.4 Benefits of E-Learning It is an All in One Learning by linking the various resources in several varying formats at the same time. It promotes active and independent learning. Liberty is at one’s Door as no dependence on anyone for anything and at one’s own pace. E-Learning can be done on laptops, tablets and phone. It eradicates boundaries of place and time. (Holmes and Gardner, 2006)13. The Good News is the teachers no longer worry about the class room management. E Learning is available at minimum cost. 2.5 Barriers to E-learning: All-time favourite is still face-to face learning. Lack of self-discipline is the biggest threat to e-learning. Though gadget-friendly people are gratified with e-learning immensely, there are many with GadgetPhobia. E-Learning is not feasible to all courses for instance, science courses. There will be cheating and plagiarism. There is also a dearth of resources like technologies and infrastructure for the communication, security, personal relations and motivation. © 2020 JETIR May 2020, Volume 7, Issue 5 www.jetir.org (ISSN-2349-5162) JETIR2005241 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 709 According to Judahil et al (2007)15, it demands a variety of skills in Information and Communication Technology (ICT). The most noticeable condemnation of e-Learning is poor ICT infrastructure. Most importantly, the void of vital personal interactions between learners and instructors, and also among colearners (Young, 1997;Burdman, 1998)36 is the biggest criticism on eLearning. 3. Advanced E Learning Methods 3.1 Blended Learning Blended Learning and Adaptive Learning embedded with Gamification, Group Dynamics and Graphic Organizers and Multimedia packages to solve the impediments of e learning. Blended learning, a mix of direct face-to-face interactions and technology-mediated interactions between students, teachers and learning resources. The adoption of blended education is increasingly seen in higher education institutions, and researchers expect that blended learning will become the new ""traditional model"" (Ross & Gage, 2006)28 or the ""new norm"" in the introduction of higher education courses (Norberg, 2011)24 According to Stacey & Gerbic [2007]30 learning experience and performance improve when traditional course delivery is paired with online learning. A study by Marriot, Marriot, and Selwyn [2004]18 showed learners expressing their preference for face-to-face due to its facilitation of social interaction and communication skills acquired from classroom environment. Blackboard network platform Developed by Blackboard Company (the United States), a curriculumoriented digital teaching platform can display teaching contents and implement the comprehensive strategy of teaching activities conducive to represent the teaching contents efficiently and improve the use of teaching resources. [14 &36] An Example for Blended Learning Structure In class: Students select an historical event to research, such as., Indo –China War, Kargil War, Indian Independence, World War I, World War II. Students brainstorm what they know about each of the events. Outside of class: Students access one of the interactive sites and complete a graphic organizer. In class: Students collaborate to create a timeline of Indian history (on poster board or chart paper posted around the classroom). 3.2 Addressing Potential Impediments to Lear",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9aeea1db3d9931832ee9404a016920c6425f57cc,https://www.semanticscholar.org/paper/9aeea1db3d9931832ee9404a016920c6425f57cc,Fully Homomorphic based Privacy-Preserving Distributed Expectation Maximization on Cloud,"Expectation maximization (EM) is a clustering-based machine learning algorithm that is widely used in many areas of science (e.g., bioinformatics and computer vision) to find maximum likelihood and maximum a posteriori estimates for models with latent variables. To deploy such an algorithm in cloud environments, security and privacy issues need be considered to avoid data breaches or abuses by external malicious parties or even by cloud service providers. However, the processing performance of the EM algorithm poses a challenge in terms of building a secure environment. This article describes an innovative and practical privacy-preserving EM algorithm for cloud systems that addresses this challenge, and estimates the EM parameters in an accurate and secure manner. Fully homomorphic encryption (FHE) is used to ensure the privacy of both the EM algorithm computations and the users’ sensitive data in the cloud. A distributed-based approach is also proposed to overcome the overheads of FHE computations and ensure a fast convergence of the EM algorithm. The conducted experiments demonstrate a significant improvement in the convergence time of the distributed EM algorithm, while achieving a high level of accuracy and reducing the associated computational FHE overheads.",IEEE Transactions on Parallel and Distributed Systems,2020.0,10.1109/TPDS.2020.2999407,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
daf5b88fda869080b52d288bdfe00bf64c442f37,https://www.semanticscholar.org/paper/daf5b88fda869080b52d288bdfe00bf64c442f37,Deep‐Learning Super‐Resolution MRI: Getting Something From Nothing,"The well-worn narrative of artificial intelligence (AI) as a competitive replacement to diagnostic radiologists is by now a tired trope, with opinions rapidly shifting away from hype towards concrete applications. A revolution that began with deep convolutional neural networks has paved the way for a much more nuanced discussion of the impact of AI on radiology workflow: its possibilities and its limitations. While this debate continues, repeating the computer-aided detection narrative ignores the much wider scope of what is possible with machine learning. In the March 2020 issue of JMRI, Chaudhari et al employ one of the most exciting recent developments in deep learning, Super-Resolution, and apply it to the reconstruction of knee MRIs performed to diagnose early changes of osteoarthritis in their article entitled ""Utility of Deep Learning Super-Resolution in the Context of Osteoarthritis MRI Biomarkers."" The technical challenge has long been balancing the need for high-resolution images against the longer acquisition times of 3D sequences. Using a multi-reviewer dataset from the Osteoarthritis Initiative, the authors processed 124 3D high-resolution double-echo steady-state (DESS) acquisitions, at varying levels of disease severity. Unlike many other supervised machine-learning studies, which ordinarily require the curation of manual training labels, the original DESS acquisition, acquired at 0.7 mm thickness, was degraded using an antialiasing filter to a 3× thicker slice at 2.1 mm, in effect providing matched pairs of highand lowquality images. It is these pairs of images that are then used to train the Super-Resolution network, DeepResolve. From this, the evaluation task becomes the question: Starting from the low-quality image, is it possible to reconstruct a high-quality image? It was performed by comparing the image quality and automated cartilage segmentation performance on Super-Resolution (SR) vs. classical Tricubic Interpolation (TCI) images. In multireader image quality evaluation, the readers consistently felt that the SR DeepResolve images had higher cartilage sharpness and contrast scores than TCI. Moreover, performing automated cartilage segmentation using a 3D U-Net segmentation network upon DeepResolve SR images showed no statistically significant differences with the original high-resolution images in Dice coefficient (DC), volumetric overlap error (VOE), and absolute volume differences, vs. statistically significant reductions in DC and VOE when segmenting on TCI images. This article applies several unique methods that are worthy of highlighting. The first is the efficient usage of simulated training labels. By having the higher-quality image be the desired output, and a degraded lower-quality image act as an input to the SR network, preexisting raw imaging data are efficiently leveraged, minimizing the bottleneck of manually generating training labels. Where this article takes the methodology further is in the detailed manner it evaluates the SR network towards the specific clinical application of osteoarthritis evaluation. SR images consistently outperformed TCI, with respect to many segmentation metrics and in the clinician detection and evaluation of osteophyte grading. It is this painstaking process of evaluating a new technique, across all subjective, objective, and clinical performance metrics, relative to existing practices, that is the true value of this article. Image SR, the idea of transforming low-resolution into high-resolution images and accurately recovering detail from sparse information, is perhaps a bit of a holy grail of image reconstruction, sounding like something straight out of science fiction, but it is in fact a field with a long history, albeit a more nascent history in Deep Learning. This technique is being applied in many other contexts, from improving the image quality of heart coronary MRAs, to adaptive MRI radiotherapy, to improving the appearance of multiple sclerosis white matter lesions on FLAIR images. However, unlike more generic methods like classical interpolation, the improved performance of many of these deep-learning SR methods may be bound to a specific examination, protocol, or clinical application. Because of the reduced model transparency of such deep-learning methods, continuous realworld evaluation is required, backed by prospective trials evaluating reconstruction from original low-resolution images at the scanner, to establish a safe and reliable evidence base for usage. Nonetheless, while it may be hyperbolic to say that SR gives us the ability to reconstruct something from nothing, the idea that adapting interpolation algorithms to a specific exam or body part could increase performance, offers exciting See related article in our March issue: Chaudhari AS, Stevens KJ, Wood JP, et al. Utility of deep learning super-resolution in the context of osteoarthritis MRI biomarkers. J Magn Reson Imaging 2020;51:768-779.",Journal of magnetic resonance imaging : JMRI,2020.0,10.1002/jmri.26939,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ef3edc323da0ff4ff9183fd6f56531e62bc1cdd8,https://www.semanticscholar.org/paper/ef3edc323da0ff4ff9183fd6f56531e62bc1cdd8,Cognitive learning and the multimodal memory game: Toward human-level machine learning,"Machine learning has made great progress during the last decades and is being deployed in a wide range of applications. However, current machine learning techniques are far from sufficient for achieving human-level intelligence. Here we identify the properties of learners required for human-level intelligence and suggest a new direction of machine learning research, i.e. the cognitive learning approach, that takes into account the recent findings in brain and cognitive sciences. In particular, we suggest two fundamental principles to achieve human-level machine learning: continuity (forming a lifelong memory continuously) and glocality (organizing a plastic structure of localized micromodules connected globally). We then propose a multimodal memory game as a research platform to study cognitive learning architectures and algorithms, where the machine learner and two human players question and answer about the scenes and dialogues after watching the movies. Concrete experimental results are presented to illustrate the usefulness of the game and the cognitive learning framework for studying human-level learning and intelligence.",2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),2008,10.1109/IJCNN.2008.4634261,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7058de5bf7126b310509b2f147c5c2940fcb689b,https://www.semanticscholar.org/paper/7058de5bf7126b310509b2f147c5c2940fcb689b,Challenges for the Application of Machine Learning,"By most accounts, the applied branch of machine learning has been a clear success. Induction techniques have aided the development of elded systems in science and industry, on a range of tasks, including me-searchers in the area can feel genuinely proud that their algorithms have proven so robust and developers deserve major credit for identifying promising applications and seeing them through to completion. The basic development story should by now be quite The developer works with a domain expert or user to understand some problem, and then reformulates the problem into one that can be addressed by well-established methods for supervised learning. He then determines a set of likely features to describe the training cases, and devises an approach to collecting and preparing those data. Once these are available, he runs some induction method over the data. The developer (and possibly the expert) then evaluate the resulting knowledge base along dimensions of interest, such as accuracy, understandability, and consistency with existing domain knowledge. If the result seems acceptable, they attempt to deploy the learned knowledge in the eld. 1 Applied work in machine learning diiers from academic learning research in its acknowledgement of this development process. Most research papers on learning continue to emphasize reenements in the induction technique and ignore the steps that must occur 1 Of course, this process is not linear but iterative. Problems at any step can lead the developer to backtrack to an earlier stage and revisit decisions made there. before and after its invocation. In contrast, applied efforts recognize the importance of problem formulation, representation engineering, data collection and preparation , inspection of the learned knowledge, and the elding process itself. Within the applications community , there is an emerging consensus that these steps play a role at least as important as the induction stage itself. Indeed, there is even a common belief that, once they are handled, the particular induction method one uses has little eeect on the outcome. 2 Automating the Overall Process Clearly, the applied induction community could continue along these lines and be quite successful. The discipline could use its established approach to develop more elded applications and train a cadre of problem and representation engineers to become expert at the overall process. Over time, this new generation of developers would come to replace the knowledge engineers currently charged with creating knowledge-based systems. But this is a limited vision, and …",,1997,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7f2d2d14f0c07e4b3d9d636d904afa7087673b62,https://www.semanticscholar.org/paper/7f2d2d14f0c07e4b3d9d636d904afa7087673b62,A Roadmap for a Rigorous Science of Interpretability,"From autonomous cars and adaptive email-filters to predictive policing systems, machine learning (ML) systems are increasingly ubiquitous; they outperform humans on specific tasks [Mnih et al., 2013, Silver et al., 2016, Hamill, 2017] and often guide processes of human understanding and decisions [Carton et al., 2016, Doshi-Velez et al., 2014]. The deployment of ML systems in complex applications has led to a surge of interest in systems optimized not only for expected task performance but also other important criteria such as safety [Otte, 2013, Amodei et al., 2016, Varshney and Alemzadeh, 2016], nondiscrimination [Bostrom and Yudkowsky, 2014, Ruggieri et al., 2010, Hardt et al., 2016], avoiding technical debt [Sculley et al., 2015], or providing the right to explanation [Goodman and Flaxman, 2016]. For ML systems to be used safely, satisfying these auxiliary criteria is critical. However, unlike measures of performance such as accuracy, these criteria often cannot be completely quantified. For example, we might not be able to enumerate all unit tests required for the safe operation of a semi-autonomous car or all confounds that might cause a credit scoring system to be discriminatory. In such cases, a popular fallback is the criterion of interpretability : if the system can explain its reasoning, we then can verify whether that reasoning is sound with respect to these auxiliary criteria. Unfortunately, there is little consensus on what interpretability in machine learning is and how to evaluate it for benchmarking. Current interpretability evaluation typically falls into two categories. The first evaluates interpretability in the context of an application: if the system is useful in either a practical application or a simplified version of it, then it must be somehow interpretable (e.g. Ribeiro et al. [2016], Lei et al. [2016], Kim et al. [2015a], Doshi-Velez et al. [2015], Kim et al. [2015b]). The second evaluates interpretability via a quantifiable proxy: a researcher might first claim that some model class—e.g. sparse linear models, rule lists, gradient boosted trees—are interpretable and then present algorithms to optimize within that class (e.g. Bucilu et al. [2006], Wang et al. [2017], Wang and Rudin [2015], Lou et al. [2012]). To large extent, both evaluation approaches rely on some notion of “you’ll know it when you see it.” Should we be concerned about a lack of rigor? Yes and no: the notions of interpretability above appear reasonable because they are reasonable: they meet the first test of having facevalidity on the correct test set of subjects: human beings. However, this basic notion leaves many kinds of questions unanswerable: Are all models in all defined-to-be-interpretable model classes equally interpretable? Quantifiable proxies such as sparsity may seem to allow for comparison, but how does one think about comparing a model sparse in features to a model sparse in prototypes? Moreover, do all applications have the same interpretability needs? If we are to move this field forward—to compare methods and understand when methods may generalize—we need to formalize these notions and make them evidence-based. The objective of this review is to chart a path toward the definition and rigorous evaluation of interpretability. The need is urgent: recent European Union regulation will require algorithms",ArXiv,2017,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
875f69328d0ac683d5c37c306f2af025b5193814,https://www.semanticscholar.org/paper/875f69328d0ac683d5c37c306f2af025b5193814,Theory-based learning in humans and machines,"Humans are remarkable in their ability to rapidly learn complex tasks from little experience. Recent successes in Al have produced algorithms that can perform complex tasks well in environments whose simple dynamics are known in advance, as well as models that can learn to perform expertly in unknown environments after a great amount of experience. Despite this, no current AI models are able to learn sufficiently rich and general representations so as to support rapid, human-level learning on new, complex, tasks. This thesis examines some of the epistemic practices, representations, and algorithms that we believe underlie humans' ability to quickly learn about their world and to deploy that understanding to achieve their aims. In particular, the thesis examines humans' ability to effectively query their environment for information that helps distinguish between competing hypotheses (Chapter 2); children's ability to use higher-level amodal features of data to match causes and effects (Chapter 3); and adult human rapid-learning abilities in artificial video-game environments (Chapter 4). The thesis culminates by presenting and testing a model, inspired by human inductive biases and epistemic practices, that learns to perform complex video-game tasks at human levels with human-level amounts of experience (Chapter 5). The model is an instantiation of a more general approach, Theory-Based Reinforcement Learning, which we believe can underlie the development of human-level agents that may eventually learn and act adaptively in the real world. Thesis Supervisor: Joshua B. Tenenbaum Title: Professor of Computational Cognitive Science",,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
be88daa1e8e95877797468246f8ac16c07670a6d,https://www.semanticscholar.org/paper/be88daa1e8e95877797468246f8ac16c07670a6d,A Novel Radar Signal Recognition Method Based On Deep Learning Doc Download,"Micro-Doppler Characteristics of Radar Targets is a monograph on radar target’s micro-Doppler effect theory and micro-Doppler feature extraction techniques. The micro-Doppler effect is presented from two aspects, including micro-Doppler effect analysis and micro-Doppler feature extraction, with micro-Doppler effects induced by different micro-motional targets in different radar systems analyzed and several methods of micro-Doppler feature extraction and three-dimensional micro-motion feature reconstruction presented. The main contents of this book include micro-Doppler effect in narrowband radar, micro-Doppler effect in wideband radar, micro-Doppler effect in bistatic radar, microDoppler feature analysis and extraction, and three-dimensional micro-motion feature reconstruction, etc. This book can be used as a reference for scientific and technical personnel engaged in radar signal processing and automatic target recognition, etc. It is especially suitable for beginners who are interested in research on micro-Doppler effect in radar. Presents new views on micro-Doppler effects, analyzing and discussing micro-Doppler effect in wideband radar rather than focusing on narrowband Provides several new methods for micro-Doppler feature extraction which are very helpful and practical for readers Includes practical cases that align with main MATLAB codes in each chapter, with detailed program annotations Wavelet analysis and its applications have been one of the fastest-growing research areas in the past several years. Wavelet theory has been employed in numerous fields and applications, such as signal and image processing, communication systems, biomedical imaging, radar, and air acoustics. Active media technology is concerned with the development of autonomous computational or physical entities capable of perceiving, reasoning, adapting, learning, cooperating, and delegating in a dynamic environment.This book captures the essence of the state of the art in wavelet analysis and its applications and active media technology. At the Congress, invited talks were delivered by distinguished researchers, namely Prof John Daugman of Cambridge University, UK; Prof Bruno Torresani of INRIA, France; Prof Victor Wickerhauser of Washington University, USA, Prof Ning Zhong of the Maebashi Institute of Technology, Japan; Prof John Yen of Pennsylvania State University, USA; and Prof Sankar K Pal of the Indian Statistical Institute, India. This volume is an initiative undertaken by the IEEE Computational Intelligence Society’s Task Force on Security, Surveillance and Defense to consolidate and disseminate the role of CI techniques in the design, development and deployment of security and defense solutions. Applications range from the detection of buried explosive hazards in a battlefield to the control of unmanned underwater vehicles, the delivery of superior video analytics for protecting critical infrastructures or the development of stronger intrusion detection systems and the design of military surveillance networks. Defense scientists, industry experts, academicians and practitioners alike will all benefit from the wide spectrum of successful applications compiled in this volume. Senior undergraduate or graduate students may also discover uncharted territory for their own research endeavors. By studying applications in radar, telecommunications and digital image restoration, this monograph discusses signal processing techniques based on bispectral methods. Improved robustness against different forms of noise as well as preservation of phase information render this method a valuable alternative to common power-spectrum analysis used in radar object recognition, digital wireless communications, and jitter removal in images. The first book to present a systematic and coherent picture of MIMO radars Due to its potential to improve target detection and discrimination capability, Multiple-Input and Multiple-Output (MIMO) radar has generated significant attention and widespread interest in academia, industry, government labs, and funding agencies. This important new work fills the need for a comprehensive treatment of this emerging field. Edited and authored by leading researchers in the field of MIMO radar research, this book introduces recent developments in the area of MIMO radar to stimulate new concepts, theories, and applications of the topic, and to foster further cross-fertilization of ideas with MIMO communications. Topical coverage includes: Adaptive MIMO radar Beampattern analysis and optimization for MIMO radar MIMO radar for target detection, parameter estimation, tracking,association, and recognition MIMO radar prototypes and measurements Space-time codes for MIMO radar Statistical MIMO radar Waveform design for MIMO radar Written in an easy-to-follow tutorial style, MIMO Radar Signal Processing serves as an excellent course book for graduate students and a valuable reference for researchers in academia and industry. This 1179-page book assembles the complete contributions to the International Conference on Intelligent Computing, ICIC 2006: one volume of Lecture Notes in Computer Science (LNCS); one of Lecture Notes in Artificial Intelligence (LNAI); one of Lecture Notes in Bioinformatics (LNBI); and two volumes of Lecture Notes in Control and Information Sciences (LNCIS). Include are 149 revised full papers, and a Special Session on Computing for Searching Strategies to Control Dynamic Processes. Novel deep learning approaches are achieving state-of-the-art accuracy in the area of radar target recognition, enabling applications beyond the scope of human-level performance. This book provides an introduction to the unique aspects of machine learning for radar signal processing that any scientist or engineer seeking to apply these technologies ought to be aware of. Collects the revised and updated versions of lectures presented at an advanced course on [title] held at the Accademia dei Lincei, Rome, 1988, as well as some additional chapters. The 13 chapters address basic concepts on detection, estimation, and optimum filtering; models of clutter; CFAR techniques in clutter; pulse compression and equivalent technologies; pulse doppler radar; MTI, MTD, and adaptive clutter cancellation; rejection of active interference; architecture and implementation of radar signal processors; identification of radar targets; phased arrays; bistatic radars; space-based radar; and evolution and future trends of radar. Primarily for radar engineers and researchers, as well as advanced students. Distributed by INSPEC. Annotation copyright by Book News, Inc., Portland, OR This definitive volume covers state-of-the-art over-the-horizon radar systems, with emphasis on the practical application of advanced signal processing techniques. In recent years rough set theory has attracted the attention of many researchers and practitioners all over the world, who have contributed essentially to its development and applications. Weareobservingagrowingresearchinterestinthefoundationsofroughsets, including the various logical, mathematical and philosophical aspects of rough sets. Some relationships have already been established between rough sets and other approaches, and also with a wide range of hybrid systems. As a result, rough sets are linked with decision system modeling and analysis of complex systems, fuzzy sets, neural networks, evolutionary computing, data mining and knowledge discovery, pattern recognition, machine learning, and approximate reasoning. In particular, rough sets are used in probabilistic reasoning, granular computing (including information granule calculi based on rough mereology), intelligent control, intelligent agent modeling, identi?cation of autonomous stems, and process speci?cation. Methods based on rough set theory alone or in combination with other proacheshavebeendiscoveredwith awide rangeofapplicationsinsuchareasas: acoustics, bioinformatics, business and ?nance, chemistry, computer engineering (e.g., data compression, digital image processing, digital signal processing, pallel and distributed computer systems, sensor fusion, fractal engineering), desion analysis and systems, economics, electrical engineering (e.g., control, signal analysis, power systems), environmental studies, informatics, medicine, molelar biology, musicology, neurology, robotics, social science, software engineering, spatial visualization, Web engineering, and Web mining. These proceedings present technical papers selected from the 2012 International Conference on Intelligent Systems and Knowledge Engineering (ISKE 2012), held on December 15-17 in Beijing. The aim of this conference is to bring together experts from different fields of expertise to discuss the state-of-the-art in Intelligent Systems and Knowledge Engineering, and to present new findings and perspectives on future developments. The proceedings introduce current scientific and technical advances in the fields of artificial intelligence, machine",,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
37ff5fe11f4bd2f2c91d1f0864d18beec92f700d,https://www.semanticscholar.org/paper/37ff5fe11f4bd2f2c91d1f0864d18beec92f700d,Impact of Quantum Computing in India and it’s Applications,"Quantum Computing has brought us a new line of scientific era, predicted entirely inconceivable possibilities and influenced several areas of modern technologies. Quantum technology is going to be the next step to make computers both faster and smarter. It is laying the foundation for complex codes, computers that can work with numbers at an incredible rate, and super speed database searches. All these functions will be important to deploy Machine learning and Artificial Intelligence at a greater scale. Quantum technology could spur the development of new breakthroughs in science ,for example medications to save lives, machine learning methods to diagnose illness sooner, cryptography ,financial modeling, cyber security , weather forecasting and climate change. It is expected that many commercial applications would emerge from their theoretical concepts which are developing in this line. India’s quantum mission, is to be administered by Department of Science and Technology (DST) the recent investment of 8000 crore in India on quantum computing will ensure that India can make significant contributions in this part. One of the aim being developing a 50-qubit computer within next few years. This will lead to spearhead scientific breakthroughs and boost quantum technology which will eventually lead in the growth of India’s economic.",,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e1535acc1a53034283a555d25e037e8fb27b3381,https://www.semanticscholar.org/paper/e1535acc1a53034283a555d25e037e8fb27b3381,Systematic map of the literature on carbon lock-in induced by long-lived capital,"Long-lived capital-stocks (LLCS) such as infrastructure and buildings have significant and long-lasting implications for greenhouse gas emissions. They contribute to carbon lock-in and may hinder a rapid decarbonization of energy systems. Here we provide a systematic map of the literature on carbon lock-in induced by LLCS. Based on a structured search of the Web of Science and Scopus, we identified 226 publications from 38 095 search results using a supervised machine learning approach. We show biases toward power generation and toward developed countries. We also identify 11 indicators used to quantify carbon lock-in. Quantifications of committed emissions (cumulative emissions that would occur over the remaining operational lifetime of an asset) or stranded assets (premature retirement/retrofitting or under-utilization of assets along a given pathway) are the most commonly used metrics, whereas institutional indicators are scarcely represented. The synthesis of quantifications shows that (i) global committed emissions have slightly increased over time, (ii) coal power plants are a major source of committed emissions and are exposed to risk of becoming stranded, (iii) delayed mitigation action increases stranded assets and (iv) sectoral distribution and amount of stranded assets differ between countries. A thematic analysis of policy implications highlights the need to assure stability and legitimacy of climate policies and to enable coordination between stakeholders. Carbon pricing is one of the most cited policy instrument, but the literature emphasizes that it should not be the only instrument used and should instead be complemented with other policy instruments, such as technical regulations and financial support for low carbon capital deployment. Further research is warranted on urban-scale, in developing countries and outside the electricity generation sector, notably on buildings, where stranded assets could be high.",Environmental Research Letters,2020,10.1088/1748-9326/aba660,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f6dab84c2c00ab92d8ee9d9359d7e530512114f9,https://www.semanticscholar.org/paper/f6dab84c2c00ab92d8ee9d9359d7e530512114f9,"Finance Big Data: Management, Analysis, and Applications","Big Data is an emerging paradigm in almost all industries. Finance big data (FBD) is becoming one of the most promising areas of management and governance in the financial sector. It is significantly changing business models in financial companies. Many researchers argue that Big Data is fueling the transformation of finance and business at-large in the ways that we cannot as yet assess. A new research area is evolving to study quantitative models and econometric approaches for financial studies that can bridge the gap between empirical finance research and data science. In this fascinating area, experts and scientists can propose novel finance business models by using the Big Data methods, present sophisticated methods for risk control with machine learning tools, provide visualization tools for financial markets analysis, create new finance sentiment indexes by mining public feelings from the massive textual data from social networks, and deploy the information-based tools in other creative ways. Due to the 4V characteristics of Big Data—volume (large data scale), velocity (real-time data streaming), variety (different data formats), and veracity (data uncertainty)—a long list of challenges for FBD management, analytics, and applications exists. These challenges include (1) to organize and manage FBD in effective and efficient ways; (2) to find novel business models from FBD analytics; (3) to handle traditional finance issues like high-frequency trading, sentiments, credit risk, financial analysis, risk management and regulation, and others, in creative Big Data–driven ways; (4) to integrate the variety of heterogeneous data from different sources; and (5) to ensure the security and safety of finance systems and to protect the individual privacy in view of the availability of Big Data. To meet these challenges, we need fundamental research on both data analytics technology and finance business. This special issue, “Finance Big Data: Management, Analysis, and Applications,” of International Journal of Electronic Commerce, is motivated by the need to meet the challenges of the fast development of finance big data. The papers brought together in this special issue highlight research efforts focused on the development of methods, tools, and techniques for the handling of various aspects of FBD from academia and industries. Viktor Manahov and Hanxiong Zhang, in “Forecasting Financial Markets Using High-Frequency Trading Data: Examination with Strongly Typed Genetic Programming,” develop an artificial futures market populated with high-frequency (HF) traders and institutional traders using Strongly Typed Genetic Programming trading algorithm. The authors simulate real-life futures trading at the millisecond time frame by applying Strongly Typed",Int. J. Electron. Commer.,2019,10.1080/10864415.2018.1512270,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8,https://www.semanticscholar.org/paper/28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8,Milvus: A Purpose-Built Vector Data Management System,"Recently, there has been a pressing need to manage high-dimensional vector data in data science and AI applications. This trend is fueled by the proliferation of unstructured data and machine learning (ML), where ML models usually transform unstructured data into feature vectors for data analytics, e.g., product recommendation. Existing systems and algorithms for managing vector data have two limitations: (1) They incur serious performance issue when handling large-scale and dynamic vector data; and (2) They provide limited functionalities that cannot meet the requirements of versatile applications. This paper presents Milvus, a purpose-built data management system to efficiently manage large-scale vector data. Milvus supports easy-to-use application interfaces (including SDKs and RESTful APIs); optimizes for the heterogeneous computing platform with modern CPUs and GPUs; enables advanced query processing beyond simple vector similarity search; handles dynamic data for fast updates while ensuring efficient query processing; and distributes data across multiple nodes to achieve scalability and availability. We first describe the design and implementation of Milvus. Then we demonstrate the real-world use cases supported by Milvus. In particular, we build a series of 10 applications (e.g., image/video search, chemical structure analysis, COVID-19 dataset search, personalized recommendation, biological multi-factor authentication, intelligent question answering) on top of Milvus. Finally, we experimentally evaluate Milvus with a wide range of systems including two open source systems (Vearch and Microsoft SPTAG) and three commercial systems. Experiments show that Milvus is up to two orders of magnitude faster than the competitors while providing more functionalities. Now Milvus is deployed by hundreds of organizations worldwide and it is also recognized as an incubation-stage project of the LF AI & Data Foundation. Milvus is open-sourced at https://github.com/milvus-io/milvus.",SIGMOD Conference,2021,10.1145/3448016.3457550,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
32bc2551883fc0e8063340b64fb275912a1dbd6b,https://www.semanticscholar.org/paper/32bc2551883fc0e8063340b64fb275912a1dbd6b,Pyfhel: PYthon For Homomorphic Encryption Libraries,"Fully Homomorphic Encryption (FHE) allows private computation over encrypted data, disclosing neither the inputs, intermediate values nor results. Thanks to recent advances, FHE has become feasible for a wide range of applications, resulting in an explosion of interest in the topic and ground-breaking real-world deployments. Given the increasing presence of FHE beyond the core academic community, there is increasing demand for easier access to FHE for wider audiences. Efficient implementations of FHE schemes are mostly written in high-performance languages like C++, posing a high entry barrier to novice users. We need to bring FHE to the (higher-level) languages and ecosystems non-experts are already familiar with, such as Python, the de-facto standard language of data science and machine learning. We achieve this through wrapping existing FHE implementations in Python, providing one-click installation and convenience in addition to a significantly higher-level API. In contrast to other similar works, Pyfhel goes beyond merely exposing the underlying API, adding a carefully designed abstraction layer that feels at home in Python. In this paper, we present Pyfhel, introduce its design and usage, and highlight how its unique support for accessing low-level features through a high-level API makes it an ideal teaching tool for lectures on FHE.",WAHC@CCS,2021,10.1145/3474366.3486923,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d62515b028f382e02f8b898215d2ccb1dddaed3e,https://www.semanticscholar.org/paper/d62515b028f382e02f8b898215d2ccb1dddaed3e,Polymer Informatics: Opportunities and Challenges.,"We are entering an era where large volumes of scientific data, coupled with algorithmic and computational advances, can reduce both the time and cost of developing new materials. This emerging field known as materials informatics has gained acceptance for a number of classes of materials, including metals and oxides. In the particular case of polymer science, however, there are important challenges that must be addressed before one can start to deploy advanced machine learning approaches for designing new materials. These challenges are primarily related to the manner in which polymeric systems and their properties are reported. In this viewpoint, we discuss the opportunities and challenges for making materials informatics as applied to polymers, or equivalently polymer informatics, a reality.",ACS macro letters,2017,10.1021/acsmacrolett.7b00228,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5767fcfdda0517fd9531d46a608e804e7e467c67,https://www.semanticscholar.org/paper/5767fcfdda0517fd9531d46a608e804e7e467c67,Cybersecurity in the Era of Data Science: Examining New Adversarial Models,"The ever-increasing volume, variety, and velocity of threats dictates a big data problem in cybersecurity and necessitates deployment of AI and machine-learning (ML) algorithms. The limitations and vulnerabilities of AI/ML systems, combined with complexity of data, introduce a new adversarial model, which is defined and discussed in this article.",IEEE Security & Privacy,2019,10.1109/MSEC.2019.2907097,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
050a20a1244c952768235f3be4613845c2f3d2ca,https://www.semanticscholar.org/paper/050a20a1244c952768235f3be4613845c2f3d2ca,The 2nd Learning from Limited Labeled Data (LLD) Workshop: Representation Learning for Weak Supervision and Beyond,"s (11): Abstract 2: A New Perspective on Adversarial Perturbations in Debugging Machine Learning Models, Madry 08:00 AM2: A New Perspective on Adversarial Perturbations in Debugging Machine Learning Models, Madry 08:00 AM The widespread susceptibility of the current ML models to adversarial perturbations is an intensely studied but still mystifying phenomenon. A popular view is that these perturbations are aberrations that arise due to statistical fluctuations in the training data and/or high-dimensional nature of our inputs. But is this really the case? In this talk, I will present a new perspective on the phenomenon of adversarial perturbations. This perspective ties this phenomenon to the existence of ""non-robust"" features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. Such patterns turn out to be prevalent in our real-world datasets and also shed light on previously observed phenomena in adversarial robustness, including transferability of adversarial examples and properties of robust models. Finally, this perspective suggests that we may need to recalibrate our expectations in terms of how models should make their decisions, and how we should interpret them. ICLR 2019 Workshop book Generated Mon Jul 13, 2020 Page 6 of 15 Abstract 3: Similarity of Neural Network Representations Revisited in Debugging Machine Learning Models, Kornblith 08:30 AM Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We introduce a similarity index that measures the relationship between representational similarity matrices. We show that this similarity index is equivalent to centered kernel alignment (CKA) and analyze its relationship to canonical correlation analysis. Unlike other methods, CKA can reliably identify correspondences between representations of layers in networks trained from different initializations. Moreover, CKA can reveal network pathology that is not evident from test accuracy alone. Abstract 6: Verifiable Reinforcement Learning via Policy Extraction in Debugging Machine Learning Models, Bastani 09:10 AM6: Verifiable Reinforcement Learning via Policy Extraction in Debugging Machine Learning Models, Bastani 09:10 AM While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose VIPER, an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). We use VIPER to learn a decision tree policy for a toy game based on Pong that provably never loses. Abstract 7: Debugging Machine Learning via Model Assertions in Debugging Machine Learning Models, Kang 09:40 AM7: Debugging Machine Learning via Model Assertions in Debugging Machine Learning Models, Kang 09:40 AM Machine learning models are being deployed in mission-critical settings, such as self-driving cars. However, these models can fail in complex ways, so it is imperative that application developers find ways to debug these models. We propose adapting software assertions, or boolean statements about the state of a program that must be true, to the task of debugging ML models. With model assertions, ML developers can specify constraints on model outputs, e.g., cars should not disappear and reappear in successive frames of a video. We propose several ways to use model assertions in ML debugging, including use in runtime monitoring, in performing corrective actions, and in collecting “hard examples” to further train models with human labeling or weak supervision. We show that, for a video analytics task, simple assertions can effectively find errors and correction rules can effectively correct model output (up to 100% and 90% respectively). We additionally collect and label parts of video where assertions fire (as a form of active learning) and show that this procedure can improve model performance by up to 2×. Abstract 10: Discovering Natural Bugs Using Adversarial Data Perturbations in Debugging Machine Learning Models, Singh 10:10 AM10: Discovering Natural Bugs Using Adversarial Data Perturbations in Debugging Machine Learning Models, Singh 10:10 AM Determining when a machine learning model is “good enough” is challenging since held-out accuracy metrics significantly overestimate real-world performance. In this talk, I will describe automated techniques to detect bugs that can occur naturally when a model is deployed. I will start by approaches to identify “semantically equivalent” adversaries that should not change the meaning of the input, but lead to a change in the model’s predictions. Then I will present our work on evaluating the consistency behavior of the model by exploring performance on new instances that are “implied” by the model’s predictions. I will also describe a method to understand and debug models by adversarially modifying the training data to change the model’s predictions. The talk will include applications of these ideas on a number of NLP tasks, such as reading comprehension, visual QA, and knowledge graph completion. Abstract 11: ""Debugging"" Discriminatory ML Systems in Debugging Machine Learning Models, Raji 10:40 AM11: ""Debugging"" Discriminatory ML Systems in Debugging Machine Learning Models, Raji 10:40 AM If a machine learning (ML) model is illegally discriminatory towards vulnerable and underrepresented populations, can we really say it works? Of course not! That illegal behaviour negates the functionality of the ML model, just as much as overfitting or other typically acknowledged ML ""bugs"". This talk explores the redefinition of what it means for a model to ""work"" well enough to deploy and dives into the analogy of software engineering debugging practice to explain current strategies for diagnosing, reporting, addressing and preventing the further development of discriminatory ML models. Abstract 12: NeuralVerification.jl: Algorithms for Verifying Deep Neural Networks in Debugging Machine Learning Models, Arnon, Lazarus 11:00 AM12: NeuralVerification.jl: Algorithms for Verifying Deep Neural Networks in Debugging Machine Learning Models, Arnon, Lazarus 11:00 AM Deep neural networks (DNNs) are widely used for nonlinear function approximation with applications ranging from computer vision to control. Although DNNs involve the composition of simple arithmetic operations, it can be very challenging to verify whether a particular network satisfies certain input-output properties. This work introduces NeuralVerification.jl, a software package that implements methods that have emerged recently for soundly verifying such properties. These methods borrow insights from reachability analysis, optimization, and search. We present the formal problem definition and briefly discuss the fundamental differences between the implemented algorithms. In addition, we provide a pedagogical example of how to use the library. Abstract 15: Safe and Reliable Machine Learning: Preventing and Identifying Failures in Debugging Machine Learning Models, Saria 01:30 PM15: Safe and Reliable Machine Learning: Preventing and Identifying Failures in Debugging Machine Learning Models, Saria 01:30 PM Machine Learning driven decision-making systems are being increasingly used to decide bank loans, make hiring decisions,perform clinical decision-making, and more. As we march towards a future in which these systems underpin most of society’s decision-making infrastructure, it is critical for us to understand the principles that will help us engineer for reliability. Drawing from reliability engineering, we will briefly outline three principles to group and guide technical solutions for addressing and ensuring reliability in machine learning systems: 1) Failure Prevention, 2) Failure Identification, and 3) Maintenance. In particular, we will discuss a framework (https://arxiv.org/abs/1904.07204) for preventing failures due to differences between the training and deployment environments that proactively addresses the problem of dataset shift. We will contrast this with typical reactive solutions which require deployment environment data and discuss relations with similar problems such as robustness to adversarial examples. Abstract 16: Better Code for Less Debugging with AutoGraph in Debugging Machine Learning Models, Moldovan 02:00 PM16: Better Code for Less Debugging with AutoGraph in Debugging Machine Learning Models, Moldovan 02:00 PM The fast-paced nature of machine learning research and development, with many ideas advancing rapidly from research to production, puts it at increased risk of programming errors, which can be particularly insidious when combined with machine learning. In this talk we discuss defensive design as a way to reduce the chance for such errors to occur in the first place, and present AutoGraph, a tool which facilitates defensive design by allowing more legible code that is still efficient and portable. ICLR 2019 Workshop book Generated Mon Jul 13, 2020 Page 7 of 15 Abstract 18: The Scientific Method in the Science of Machine Learning in Debugging Machine Learning Models, Paganini 03:20 PM In the quest to align deep learning with the sciences to address calls for rigor, safety, and interpretability in machine learning systems, this contribution identifies key missing pieces: the stages of hypothesis formulation and testing, as well as statistical and systematic uncertainty estimation – core tenets of the scientific method. This position paper discusses the ways in which contemporary science is conducted in other domains and identifies potentially useful practices. We present a case study from physics and describe how this field has promoted rigor ",ICLR 2019,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4709f78223cd9825829f6cf577a5222bd215f6ae,https://www.semanticscholar.org/paper/4709f78223cd9825829f6cf577a5222bd215f6ae,Intelligent electromagnetic metasurface camera: system design and experimental results,"Abstract Electromagnetic (EM) sensing is uniquely positioned among nondestructive examination options, which enables us to see clearly targets, even when they visually invisible, and thus has found many valuable applications in science, engineering and military. However, it is suffering from increasingly critical challenges from energy consumption, cost, efficiency, portability, etc., with the rapidly growing demands for the high-quality sensing with three-dimensional high-frame-rate schemes. To address these difficulties, we propose the concept of intelligent EM metasurface camera by the synergetic exploitation of inexpensive programmable metasurfaces with modern machine learning techniques, and establish a Bayesian inference framework for it. Such EM camera introduces the intelligence over the entire sensing chain of data acquisition and processing, and exhibits good performance in terms of the image quality and efficiency, even when it is deployed in highly noisy environment. Selected experimental results in real-world settings are provided to demonstrate that the developed EM metasurface camera enables us to see clearly human behaviors behind a 60 cm-thickness reinforced concrete wall with the frame rate in order of tens of Hz. We expect that the presented strategy could have considerable impacts on sensing and beyond, and open up a promising route toward smart community and beyond.",Nanophotonics,2022,10.1515/nanoph-2021-0665,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
32b84733c0f9898bab54c69a2de5534d2ed1b120,https://www.semanticscholar.org/paper/32b84733c0f9898bab54c69a2de5534d2ed1b120,Intelligent Techniques for Data Science,,Springer International Publishing,2016,10.1007/978-3-319-29206-9,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f41a32130b4d9db7e57fbca691184b894ec36ccb,https://www.semanticscholar.org/paper/f41a32130b4d9db7e57fbca691184b894ec36ccb,TensorDiffEq: Scalable Multi-GPU Forward and Inverse Solvers for Physics Informed Neural Networks,"Physics-Informed Neural Networks promise to revolutionize science and engineering practice, by introducing domain-aware deep machine learning models into scientific computation. Several software suites have emerged to make the implementation and usage of these architectures available to the research and industry communities. Here we introduce TensorDiffEq, built on Tensorflow 2.x, which presents an intuitive Keras-like interface for problem domain definition, model definition, and solution of forward and inverse problems using physics-aware deep learning methods. TensorDiffEq takes full advantage of Tensorflow 2.x infrastructure for deployment on multiple GPUs, allowing the implementation of large high-dimensional and complex models. Simultaneously, TensorDiffEq supports the Keras API for custom neural network architecture definitions. In the case of smaller or simpler models, the package allows for rapid deployment on smaller-scale CPU platforms with negligible changes to the implementation scripts. We demonstrate the basic usage and capabilities of TensorDiffEq in solving forward, inverse, and data assimilation problems of varying sizes and levels of complexity. The source code is available at https://github.com/tensordiffeq.",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a40bf4700d4fa10a206144fa4469e5efec68fa3a,https://www.semanticscholar.org/paper/a40bf4700d4fa10a206144fa4469e5efec68fa3a,Knowledge graph construction and application in geosciences: A review,"Knowledge graph (KG) is a topic of great interests to geoscientists as it can be deployed throughout the data life cycle in data-intensive geoscience studies. Nevertheless, comparing with the large amounts of publications on machine learning applications in geosciences, summaries and reviews of geoscience KGs are still limited. The aim of this paper is to present a comprehensive review of KG construction and implementation in geosciences. It consists of four major parts: 1) concepts relevant to KG and approaches for KG construction, 2) KG application in data collection, curation, and service, 3) KG application in data analysis, and 4) challenges and trends of geoscience KG creation and application in the near future. For each of the first three parts, a list of concepts, exemplar studies, and best practices are summarized. Those summaries are synthesized together in the challenge and trend analyses. As artificial intelligence and data science are thriving in geosciences, we hope this review of geoscience KGs can be of value to practitioners in data-intensive geoscience studies.",Comput. Geosci.,2021,10.31223/x5z898,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0fade43292b3a1e2d6059c58838a4de89db929bb,https://www.semanticscholar.org/paper/0fade43292b3a1e2d6059c58838a4de89db929bb,Data Analytics and Modeling in IoT-Fog Environment for Resource Constrained IoT-Applications - A Review,"

The emergence of the concepts like Big Data, Data Science, Machine Learning (ML), and the Internet of Things (IoT) has added the potential of research in today's world. The continuous use of IoT devices, sensors, etc. that collect data continuously puts tremendous pressure on the existing IoT network.



This resource-constrained IoT environment is flooded with data acquired from millions of IoT nodes deployed at the device level. The limited resources of the IoT Network have driven the researchers towards data Management. This paper focuses on data classification at the device level, edge/fog level, and cloud level using machine learning techniques.



The data coming from different devices is vast and is of variety. Therefore, it becomes essential to choose the right approach for classification and analysis. It will help optimize the data at the device edge/fog level to better the network's performance in the future.



This paper presents data classification, machine learning approaches, and a proposed mathematical model for the IoT environment.
",Recent Advances in Computer Science and Communications,2021,10.2174/2666255814666210715161630,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7f9f193ceda838b4d54c3871e0ca8ea9048d90a0,https://www.semanticscholar.org/paper/7f9f193ceda838b4d54c3871e0ca8ea9048d90a0,Expert Group on Antarctic Biodiversity Informatics: Coordinating the state-of-the-art internationally for biodiversity informatics in Antarctica,"Biodiversity informatics have emerged as a key asset in wildlife and ecological conservation around the world. This is especially true in Antarctica, where climate change continues to threaten marine and terrestrial species. It is well documented that the polar regions experience the most drastic rate of climate change compared to the rest of the world (IPCC 2021). Research approaches within the scope of polar biodiversity informatics consist of computational architectures and systems, analysis and modelling methods, and human-computer interfaces, ranging from more traditional statistical techniques to more recent machine learning and artificial intelligence-based imaging techniques. Ongoing discussions include making datasets findable, accessible, interoperable and reusable (FAIR) (Wilkinson et al. 2016). The deployment of biodiversity informatics systems and coordination of standards around their utilization in the Antarctic are important areas of consideration. 
 To bring together scientists and practitioners working at the nexus of informatics and Antarctic biodiversity, the Expert Group on Antarctic Biodiversity Informatics (EG-ABI) was formed under the Scientific Committee on Antarctic Research (SCAR). EG-ABI was created during the SCAR Life Sciences Standing Scientific Group meeting at the SCAR Open Science Conference in Portland Oregon, in July 2012, to advance work at this intersection by coordinating and participating in a range of projects across the SCAR biodiversity science portfolio. SCAR, meanwhile, is a thematic organisation of the International Science Council (ISC), which is the primary entity tasked with coordinating high-quality scientific research on all aspects of Antarctic sciences and humanities, including the Southern Ocean and the interplay between Antarctica and the other six continents. The expert group is led by an international steering committee of roughly ten members, who take an active role in leading related initiatives. Currently, researchers from Australia, Belgium, the United Kingdom, Chile, Germany, France, and the United States are represented on the committee. The current steering committee is comprised of a diverse range of scientists, including early-career researchers and scientists that have primary focuses in both the computational and ecological aspects of Antarctic biodiversity informatics.
 Current projects that are being coordinated or co-coordinated by EG-ABI include the SCAR/rOpenSci initiative, which is a collaboration with the rOpenSci community to improve resources for users of the R software package in Antarctic and Southern Ocean science. Additionally, EG-ABI has contributed to the POLA3R project (Polar Omics Linkages Antarctic Arctic and Alpine Regions), which is an information system dedicated to aid in the access and discovery of molecular microbial diversity data generated by Antarctic scientists. Furthermore, EG-ABI has trained and helped collate additional species trait information such as feeding and diet information, development, mobility and their importance to society, documented through Vulnerable Marine Ecosystem (VME) indicator taxa, in The Register of Antarctic Species (http://ras.biodiversity.aq/), and the comprehensive inventory of Antarctic and Southern Ocean organisms, which is also a component of the World Register of Marine Species (https://marinespecies.org/). The efforts highlighted are only some of the projects that the expert groups have contributed to.
 In our presentation, we discuss the previous accomplishments of the EG-ABI from the perspective of a currently serving steering committee member and outline its state in the status quo including collaborations and coordinated activities. We also highlight opportunities for engagement and the benefits for various stakeholders in terms of interacting with EG-ABI on multiple levels, within the SCAR ecosystem and elsewhere. Developing consistent and practical standards for data use in Antarctic ecology, in addition to fostering interdisciplinary and cross-sectoral collaborations for the successful deployment of conservation mechanisms, are key to a sustainable and biodiverse Antarctica, and EG-ABI is one of the premier organizations working towards these aims.",Biodiversity Information Science and Standards,2021,10.3897/biss.5.74332,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0dac918303b162fadc6c834c9cd43c5c9094755e,https://www.semanticscholar.org/paper/0dac918303b162fadc6c834c9cd43c5c9094755e,"Living up to the Hype of Hyperspectral Aquatic Remote Sensing: Science, Resources and Outlook","Intensifying pressure on global aquatic resources and services due to population growth and climate change is inspiring new surveying technologies to provide science-based information in support of management and policy strategies. One area of rapid development is hyperspectral remote sensing: imaging across the full spectrum of visible and infrared light. Hyperspectral imagery contains more environmentally meaningful information than panchromatic or multispectral imagery and is poised to provide new applications relevant to society, including assessments of aquatic biodiversity, habitats, water quality, and natural and anthropogenic hazards. To aid in these advances, we provide resources relevant to hyperspectral remote sensing in terms of providing the latest reviews, databases, and software available for practitioners in the field. We highlight recent advances in sensor design, modes of deployment, and image analysis techniques that are becoming more widely available to environmental researchers and resource managers alike. Systems recently deployed on space- and airborne platforms are presented, as well as future missions and advances in unoccupied aerial systems (UAS) and autonomous in-water survey methods. These systems will greatly enhance the ability to collect interdisciplinary observations on-demand and in previously inaccessible environments. Looking forward, advances in sensor miniaturization are discussed alongside the incorporation of citizen science, moving toward open and FAIR (findable, accessible, interoperable, and reusable) data. Advances in machine learning and cloud computing allow for exploitation of the full electromagnetic spectrum, and better bridging across the larger scientific community that also includes biogeochemical modelers and climate scientists. These advances will place sophisticated remote sensing capabilities into the hands of individual users and provide on-demand imagery tailored to research and management requirements, as well as provide critical input to marine and climate forecasting systems. The next decade of hyperspectral aquatic remote sensing is on the cusp of revolutionizing the way we assess and monitor aquatic environments and detect changes relevant to global communities.",Frontiers in Environmental Science,2021,10.3389/fenvs.2021.649528,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
01866ca6dad2fe6a396d568e7e84bad516be007d,https://www.semanticscholar.org/paper/01866ca6dad2fe6a396d568e7e84bad516be007d,CateCom: a practical data-centric approach to categorization of computational models,"The advent of data-driven science in the 21st century brought about the need for well-organized structured data and associated infrastructure able to facilitate the applications of artificial intelligence and machine learning. We present an effort aimed at organizing the diverse landscape of physics-based and data-driven computational models in order to facilitate the storage of associated information as structured data. We apply object-oriented design concepts and outline the foundations of an open-source collaborative framework that is (1) capable of uniquely describing the approaches in structured data, (2) flexible enough to cover the majority of widely used models, and (3) utilizes collective intelligence through community contributions. We present example database schemas and corresponding data structures and explain how these are deployed in software at the time of this writing.",J. Chem. Inf. Model.,2021,10.1021/acs.jcim.2c00112,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
af26348693321847450d78f862f5ddd8d137af3e,https://www.semanticscholar.org/paper/af26348693321847450d78f862f5ddd8d137af3e,Special issue on Artificial Intelligence in Engineering Education,"Artificial intelligence (AI) can be defined as the intelligence exhibited by machines and computers in accomplishing desired tasks in a similar way to how normal human beings think and act. Hence AI is also termed machine intelligence. For a computational system to be artificially intelligent, the system should possess the ability to understand the surrounding environment, make proper assumptions, and based on the circumstances make judicious decisions that maximize the possibilities of accomplishing goals most of the time. These AI‐enabled devices are also called Intelligent Agents. These intelligent agents use some mapping functions also termed cognitive functions, which take these environmental parameters and contextual information as inputs along with the goal to be accomplished and manipulate the right means to accomplish the targeted goal. AI can also be considered inter‐ disciplinary as it involves several other disciplines such as Machine Learning, Computer Vision, Cognitive Science, Neural Networks, Data Mining, Natural Language Processing (NLP), robotics, and mathematics. All these disciplines are related, and thereby intelligent agents are trained to understand and adapt to the surrounding environment according to the context. The use of AI spans across several applications such as Human–Computer Interaction (HCI) based smart agent development, devising smart surveillance solutions using computer vision, creating robust and stable decision making systems that can understand, evaluate, manipulate, analyze, and predict several novel patterns by processing large volumes of application data, development of multilingual systems that uses NLP to understand the language features used across the context and aid decision making and so on. Also, since its inception as an academic discipline in the 1950s, AI has grown leaps and bounds as a discipline, and its applications have stretched across several domains such as Retail and Business solutions, Manufacturing and Logistics, Automobiles, Business Analytics and Market predictions, Healthcare, Security Systems, and Education. One of the key emerging areas where extensive efforts are spent towards developing smart applications and agents is the educational domain. Gone are the days where the educational system was completely driven by humans, and the growth of AI‐enabled intelligent agents has set the tone by replacing most human work with that of smart agents. Educational systems use AI‐based agents to study the behavior of students and suggest suitable courses for them. Smart agents are nowadays deployed in classrooms for complete classroom monitoring that includes tracking attendance, monitoring classroom activities, student and staff behavior monitoring, and so on. Similarly, smart agents are deployed to scan through the contents available online and suggest suitable content to students according to the course and also according to the different levels of understanding of student fraternity. Also, computer vision‐based smart agents are deployed to study the state of mind of students when they undergo different courses and provide insightful information about their likeness towards a subject or course. This agent‐based information serves as useful information in deciding the teaching methodology and also framing of course contents. Also, smart systems play a vital role in analyzing student results and providing insightful information about student performance. Thus, it is imperative that AI has become an indispensable force to reckon with in the future forward across the educational domain. However, the major drawback in these artificially intelligent systems is that they are not always accurate with decision making and at times predict otherwise. Also, training the AI‐based agent to understand the contextual paradigm and surrounding environment is a challenge. This special issue on “Artificial Intelligence In Education” is focused on drawing original studies related to the development and refinement of smart agents that can be applied across the educational domain.",Comput. Appl. Eng. Educ.,2021,10.1002/cae.22398,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
445cf9e3b58333c56e3dfd1339cea79fc2cf04a7,https://www.semanticscholar.org/paper/445cf9e3b58333c56e3dfd1339cea79fc2cf04a7,Pilot-Edge: Distributed Resource Management Along the Edge-to-Cloud Continuum,"Many science and industry IoT applications necessitate data processing across the edge-to-cloud continuum to meet performance, security, cost, and privacy requirements. However, diverse abstractions and infrastructures for managing resources and tasks across the edge-to-cloud scenario are required. We propose Pilot-Edge as a common abstraction for resource management across the edge-to-cloud continuum. Pilot-Edge is based on the pilot abstraction, which decouples resource and workload management, and provides a Function-as-a-Service (FaaS) interface for application-level tasks. The abstraction allows applications to encapsulate common functions in high-level tasks that can then be configured and deployed across the continuum. We characterize Pilot-Edge on geographically distributed infrastructures using machine learning workloads (e. g., k-means and auto-encoders). Our experiments demonstrate how Pilot-Edge manages distributed resources and allows applications to evaluate task placement based on multiple factors (e. g., model complexities, throughput, and latency).",2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),2021,10.1109/IPDPSW52791.2021.00130,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
84dcbcd5061069118bc6a1e02c6bffe04f58e98c,https://www.semanticscholar.org/paper/84dcbcd5061069118bc6a1e02c6bffe04f58e98c,CS-Annotate: A Tool for Using NMR Chemical Shifts to Annotate RNA Structure,"Here, we introduce CS-Annotate, a tool that uses assigned NMR chemical shifts to annotate structural features in RNA. At its core, CS-Annotate is a deployment of a multitask deep learning model that simultaneously classifies the solvent exposure, base-stacking and -pairing status, and conformation of individual RNA residues from their chemical shift fingerprint. Here, we briefly describe how we trained and tested the classifier and demonstrate its application to a model RNA system. CS-Annotate can be accessed via the SMALTR (Structure-based MAchine Learning Tools for RNA) Science Gateway (smaltr.org).",J. Chem. Inf. Model.,2021,10.1021/acs.jcim.1c00006,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6ea59f72f3239e524b7fb5d09eaf2fc9638cb018,https://www.semanticscholar.org/paper/6ea59f72f3239e524b7fb5d09eaf2fc9638cb018,Ai Technology Achieving General Purpose Ai That Can,"Artificial IntelligenceArtificial Intelligence in Cyber Security: Impact and ImplicationsTechnological Progress, Artificial Intelligence, and Inclusive GrowthEnergy Research AbstractsCan Artificial Intelligence ImproveHandbook of Pharmaceutical Granulation TechnologyArtificial Intelligence for BusinessArtificial Intelligence in EducationThe Democratization of Artificial IntelligenceAdvances in Artificial Intelligence, Software and Systems EngineeringFinancing Our FutureProceedings of the Future Technologies Conference (FTC) 2021, Volume 1Artificial IntelligenceNew Technologies in Dermatological Science and PracticeArtificial IntelligenceGame Theory and Machine Learning for Cyber SecurityRegulatory Aspects of Artificial Intelligence on BlockchainConcise Encyclopedia of Software EngineeringEuropean Artificial Intelligence (AI) Leadership, the Path for an Integrated VisionAI In The Age Of Cyber-DisorderReadings in Artificial Intelligence and DatabasesHuman decisionsConstitution 3.0Artificial Intelligence and IoT-Based Technologies for Sustainable Farming and Smart AgricultureArtificial Intelligence and Deep Learning for Decision MakersThe Regional Economics of Technological TransformationsArchitects of IntelligenceArtificial IntelligenceArtificial Intelligence and Integrated Intelligent Information SystemsECIAIR 2019 European Conference on the Impact of Artificial Intelligence and Robotics Intelligence UnboundHow to Achieve Inclusive GrowthArtificial Intelligence for Business OptimizationArtificial Intelligence in SocietyThe Myth of Artificial IntelligenceThe Digital Innovation RaceAI-First HealthcareProject Management Best Practices: Achieving Global ExcellenceConnected WorldBiotechnology: Concepts, Methodologies, Tools, and Applications The interaction of database and AI technologies is crucial to such applications as data mining, active databases, and knowledge-based expert systems. This volume collects the primary readings on the interactions, actual and potential, between these two fields. The editors have chosen articles to balance significant early research and the best and most comprehensive articles from the 1980s. An in-depth introduction discusses basic research motivations, giving a survey of the history, concepts, and terminology of the interaction. Major themes, approaches and results, open issues and future directions are all discussed, including the results of a major survey conducted by the editors of current work in industry and research labs. Thirteen sections follow, each with a short introduction. Topics examined include semantic data models with emphasis on conceptual modeling techniques for databases and information systems and the integration of data model concepts in high-level data languages, definition and maintenance of integrity constraints in databases and knowledge bases, natural language front ends, object-oriented database management systems, implementation issues such as concurrency control and error recovery, and representation of time and knowledge incompleteness from the viewpoints of databases, logic programming, and AI.The artificial intelligence (AI) landscape has evolved significantly from 1950 when Alan Turing first posed the question of whether machines can think. Today, AI is transforming societies and economies. It promises to generate productivity gains, improve well-being and help address global challenges, such as climate change, resource scarcity and health crises.As technology continues to saturate modern society, agriculture has started to adopt digital computing and data-driven innovations. This emergence of “smart” farming has led to various advancements in the field, including autonomous equipment and the collection of climate, livestock, and plant data. As connectivity and data management continue to revolutionize the farming industry, empirical research is a necessity for understanding these technological developments. Artificial Intelligence and IoT-Based Technologies for Sustainable Farming and Smart Agriculture provides emerging research exploring the theoretical and practical aspects of critical technological solutions within the farming industry. Featuring coverage on a broad range of topics such as crop monitoring, precision livestock farming, and agronomic data processing, this book is ideally designed for farmers, agriculturalists, product managers, farm holders, manufacturers, equipment suppliers, industrialists, governmental professionals, researchers, academicians, and students seeking current research on technological applications within agriculture and farming.This book constitutes the refereed proceedings of the Second International Conference, SLAAI-ICAI 2018, held in Moratuwa, Sri Lanka, in December 2018. The 32 revised full papers presented were carefully reviewed and selected from numerous submissions. The papers are organized in the following topical sections: ?intelligence systems; neural networks; game theory; ontology engineering; natural language processing; agent based system; signal and image processing.After a long time of neglect, Artificial Intelligence is once again at the center of most of our political, economic, and socio-cultural debates. Recent advances in the field of Artifical Neural Networks have led to a renaissance of dystopian and utopian speculations on an AI-rendered future. Algorithmic technologies are deployed for identifying potential terrorists through vast surveillance networks, for producing sentencing guidelines and recidivism risk profiles in criminal justice systems, for demographic and psychographic targeting of bodies for advertising or propaganda, and more generally for automating the analysis of language, text, and images. Against this background, the aim of this book is to discuss the heterogenous conditions, implications, and effects of modern AI and Internet technologies in terms of their political dimension: What does it mean to critically investigate efforts of net politics in the age of machine learning algorithms?Intelligence Unbound explores the prospects, promises, and potential dangers of machine intelligence and uploaded minds in a collection of stateof-the-art essays from internationally recognized philosophers, AI researchers, science fiction authors, and theorists. Compelling and intellectually sophisticated exploration of the latest thinking on Artificial Intelligence and machine minds Features contributions from an international cast of philosophers, Artificial Intelligence researchers, science fiction authors, and more Offers current, diverse perspectives on machine intelligence and uploaded minds, emerging topics of tremendous interest Illuminates the nature and ethics of tomorrow’s machine minds—and of the convergence of humans and machines—to consider the pros and cons of a variety of intriguing possibilities Considers classic philosophical puzzles as well as the latest topics debated by scholars Covers a wide range of viewpoints and arguments regarding the prospects of uploading and machine intelligence, including proponents and skeptics, pros and consCompanies that don't use AI to their advantage will soon be left behind. Artificial intelligence and machine learning will drive a massive reshaping of the economy and society. What should you and your company be doing right now to ensure that your business is poised for success? These articles by AI experts and consultants will help you understand today's essential thinking on what AI is capable of now, how to adopt it in your organization, and how the technology is likely to evolve in the near future. Artificial Intelligence: The Insights You Need from Harvard Business Review will help you spearhead important conversations, get going on the right AI initiatives for your company, and capitalize on the opportunity of the machine intelligence revolution. Catch up on current topics and deepen your",,2022,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1b96c504d5b2a622a60275afb38fef5dcfcc3ae2,https://www.semanticscholar.org/paper/1b96c504d5b2a622a60275afb38fef5dcfcc3ae2,PolyFrame,"In the last few years, the field of data science has been growing rapidly as various businesses have adopted statistical and machine learning techniques to empower their decision-making and applications. Scaling data analyses to large volumes of data requires the utilization of distributed frameworks. This can lead to serious technical challenges for data analysts and reduce their productivity. AFrame, a data analytics library, is implemented as a layer on top of Apache AsterixDB, addressing these issues by providing the data scientists' familiar interface, Pandas Dataframe, and transparently scaling out the evaluation of analytical operations through a Big Data management system. While AFrame is able to leverage data management facilities (e.g., indexes and query optimization) and allows users to interact with a large volume of data, the initial version only generated SQL++ queries and only operated against AsterixDB. In this work, we describe a new design that retargets AFrame's incremental query formation to other query-based database systems, making it more flexible for deployment against other data management systems with composable query languages.",Proceedings of the VLDB Endowment,2021,10.14778/3476249.3476281,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
46c99babfcb5b1af94fbaec1a2017b2d60603e87,https://www.semanticscholar.org/paper/46c99babfcb5b1af94fbaec1a2017b2d60603e87,"Audio, Image, Video, and Weather Datasets for Continuous Electronic Beehive Monitoring","In 2014, we designed and implemented BeePi, a multi-sensor electronic beehive monitoring system. Since then we have been using BeePi monitors deployed at different apiaries in northern Utah to design audio, image, and video processing algorithms to analyze forager traffic in the vicinity of Langstroth beehives. Since our first publication on BeePi in 2016, we have received multiple requests from researchers and practitioners for the datasets we have used in our research. The main objective of this article is to provide a comprehensive point of reference to the datasets that we have so far curated for our research. We hope that our datasets will provide stable performance benchmarks for continuous electronic beehive monitoring, help interested parties verify our findings and correct errors, and advance the state of the art in continuous electronic beehive monitoring and related areas of AI, machine learning, and data science.",Applied Sciences,2021,10.3390/APP11104632,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f1bc43932beb14a00cd47feac4e40951601dd7a9,https://www.semanticscholar.org/paper/f1bc43932beb14a00cd47feac4e40951601dd7a9,Key challenges for delivering clinical impact with artificial intelligence,,BMC Medicine,2019,10.1186/s12916-019-1426-2,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
293ee59b4214a9b01a7fb1925c08be03642cc44f,https://www.semanticscholar.org/paper/293ee59b4214a9b01a7fb1925c08be03642cc44f,Privacy and Security in Cognitive Cities: A Systematic Review,"The emerging paradigm of the cognitive city, which augments smart cities with learning and behavioral change capabilities, is gaining increasing attention as a promising solution to the challenges of future mega-cities. Cognitive cities are built upon artificial learning and behavioral analysis techniques founded on the exploitation of human-machine collective intelligence. Hence, cognitive cities rely on the sharing of citizens’ daily-life data, which might be considered sensitive personal data. In this context, privacy and security of the shared information become critical issues that have to be addressed to guarantee the proper deployment of cognitive cities and the fundamental rights of people. This article provides a thorough literature review using the recommendations for systematic reviews proposed by Vom Brocke et al. and the PRISMA statement. We analyze peer-reviewed publications indexed in ACM Digital Library, IEEE Xplore, Scopus, and Web of Science until July 2020. We identify the main challenges on privacy and information security within cognitive cities, and the proposals described in the literature to address them. We conclude that many challenges remain open and we suggest several research lines that will require further examination in the years to come.",,2021,10.3390/APP11104471,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3877de5d08efa2c7cf2d5c4405d35143427733ff,https://www.semanticscholar.org/paper/3877de5d08efa2c7cf2d5c4405d35143427733ff,Images of the arXiv: Reconfiguring large scientific image datasets,"In an ongoing research project on the ascendancy of statistical visual forms, we have been concerned with the transformations wrought by such images and their organisation as datasets in ‘re-drawing’ knowledge about empirical phenomena. Historians and science studies researchers have long established the generative rather than simply illustrative role of images and figures within scientific practice. More recently, the deployment and generation of images by scientific research and its communication via publication has been impacted by the tools, techniques, and practices of working with large (image) datasets. Against this background, we built a dataset of 10 million-plus images drawn from all preprint articles deposited in the open access repository arXiv from 1991 (its inception) until the end of 2018. In this article, we suggest ways – including algorithms drawn from machine learning that facilitate visually ’slicing’ through the image data and metadata – for exploring large datasets of statistical scientific images. By treating all forms of visual material found in scientific publications – whether diagrams, photographs, or instrument data – as bare images, we developed methods for tracking their movements across a range of scientific research. We suggest that such methods allow us different entry points into large scientific image datasets and that they initiate a new set of questions about how scientific representation might be operating at more-than-human scale.",,2021,10.22148/001C.21374,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
628d396a08030d8b289d4279ea3586f15ff5ede9,https://www.semanticscholar.org/paper/628d396a08030d8b289d4279ea3586f15ff5ede9,A Research Ecosystem for Secure Computing,"Computing devices are vital to all areas of modern life and permeate every aspect of our society. The ubiquity of computing and our reliance on it has been accelerated and amplified by the COVID-19 pandemic. From education to work environments to healthcare to defense to entertainment - it is hard to imagine a segment of modern life that is not touched by computing. The security of computers, systems, and applications has been an active area of research in computer science for decades. However, with the confluence of both the scale of interconnected systems and increased adoption of artificial intelligence, there are many research challenges the community must face so that our society can continue to benefit and risks are minimized, not multiplied. Those challenges range from security and trust of the information ecosystem to adversarial artificial intelligence and machine learning. Along with basic research challenges, more often than not, securing a system happens after the design or even deployment, meaning the security community is routinely playing catch-up and attempting to patch vulnerabilities that could be exploited any minute. While security measures such as encryption and authentication have been widely adopted, questions of security tend to be secondary to application capability. There needs to be a sea-change in the way we approach this critically important aspect of the problem: new incentives and education are at the core of this change. Now is the time to refocus research community efforts on developing interconnected technologies with security ""baked in by design"" and creating an ecosystem that ensures adoption of promising research developments. To realize this vision, two additional elements of the ecosystem are necessary - proper incentive structures for adoption and an educated citizenry that is well versed in vulnerabilities and risks.",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2acc5c6e8c3f384efb23f791d7792a1dbd014f8f,https://www.semanticscholar.org/paper/2acc5c6e8c3f384efb23f791d7792a1dbd014f8f,PolyFrame: A Retargetable Query-based Approach to Scaling Dataframes,"In the last few years, the field of data science has been growing rapidly as various businesses have adopted statistical and machine learning techniques to empower their decision making and applications. Scaling data analysis, possibly including the application of custom machine learning models, to large volumes of data requires the utilization of distributed frameworks. This can lead to serious technical challenges for data analysts and reduce their productivity. AFrame, a Python data analytics library, is implemented as a layer on top of Apache AsterixDB, addressing these issues by incorporating the data scientists' development environment and transparently scaling out the evaluation of analytical operations through a Big Data management system. While AFrame is able to leverage data management facilities (e.g., indexes and query optimization) and allows users to interact with a very large volume of data, the initial version only generated SQL++ queries and only operated against Apache AsterixDB. In this work, we describe a new design that retargets AFrame's incremental query formation to other query-based database systems as well, making it more flexible for deployment against other data management systems with composable query languages.",Proc. VLDB Endow.,2020,10.14778/3476249.3476281,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0ab144e7e8c6f629385baf30d386e8e25f573be0,https://www.semanticscholar.org/paper/0ab144e7e8c6f629385baf30d386e8e25f573be0,Exploring Robustness of Neural Networks through Graph Measures,"Motivated by graph theory, artificial neural networks (ANNs) are traditionally structured as layers of neurons (nodes), which learn useful information by the passage of data through interconnections (edges). In the machine learning realm, graph structures (i.e., neurons and connections) of ANNs have recently been explored using various graph-theoretic measures linked to their predictive performance. On the other hand, in network science (NetSci), certain graph measures including entropy and curvature are known to provide insight into the robustness and fragility of real-world networks. In this work, we use these graph measures to explore the robustness of various ANNs to adversarial attacks. To this end, we (1) explore the design space of inter-layer and intra-layers connectivity regimes of ANNs in the graph domain and record their predictive performance after training under different types of adversarial attacks, (2) use graph representations for both inter-layer and intra-layers connectivity regimes to calculate various graph-theoretic measures, including curvature and entropy, and (3) analyze the relationship between these graph measures and the adversarial performance of ANNs. We show that curvature and entropy, while operating in the graph domain, can quantify the robustness of ANNs without having to train these ANNs. Our results suggest that the realworld networks, including brain networks, financial networks, and social networks may provide important clues to the neural architecture search for robust ANNs. We propose a search strategy that efficiently finds robust ANNs amongst a set of well-performing ANNs without having a need to train all of these ANNs.",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0283ae2a6125b4a797c8113b348565b4d407c0cc,https://www.semanticscholar.org/paper/0283ae2a6125b4a797c8113b348565b4d407c0cc,Post Quantum Cryptography: Readiness Challenges and the Approaching Storm,"While advances in quantum computing promise new opportunities for scientific advancement (e.g., material science and machine learning), many people are not aware that they also threaten the widely deployed cryptographic algorithms that are the foundation of today’s digital security and privacy. From mobile communications to online banking to personal data privacy, literally billions of Internet users rely on cryptography every day to ensure that private communications and data stay private. Indeed, the emergence and growth of the public Internet and electronic commerce was arguably enabled by the invention of public-key cryptography. The key advantage offered by public-key cryptography is that it allows two parties who have never communicated previously to nevertheless establish a secure, private, communication channel over a non-private network (e.g., the Internet). Public-key cryptography is also the technology that enables digital signatures which are widely used to protect software and application updates, online contracts, and electronic identity documents like Personal Identity Verification (PIV) credentials and e-passports.",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
05f11a6357ebc89a1aa711e0a8ecd0d525cdf331,https://www.semanticscholar.org/paper/05f11a6357ebc89a1aa711e0a8ecd0d525cdf331,The Random Feature Model for Input-Output Maps between Banach Spaces,"Well known to the machine learning community, the random feature model, originally introduced by Rahimi and Recht in 2008, is a parametric approximation to kernel interpolation or regression methods. It is typically used to approximate functions mapping a finite-dimensional input space to the real line. In this paper, we instead propose a methodology for use of the random feature model as a data-driven surrogate for operators that map an input Banach space to an output Banach space. Although the methodology is quite general, we consider operators defined by partial differential equations (PDEs); here, the inputs and outputs are themselves functions, with the input parameters being functions required to specify the problem, such as initial data or coefficients, and the outputs being solutions of the problem. Upon discretization, the model inherits several desirable attributes from this infinite-dimensional, function space viewpoint, including mesh-invariant approximation error with respect to the true PDE solution map and the capability to be trained at one mesh resolution and then deployed at different mesh resolutions. We view the random feature model as a non-intrusive data-driven emulator, provide a mathematical framework for its interpretation, and demonstrate its ability to efficiently and accurately approximate the nonlinear parameter-to-solution maps of two prototypical PDEs arising in physical science and engineering applications: viscous Burgers' equation and a variable coefficient elliptic equation.",SIAM J. Sci. Comput.,2020,10.1137/20M133957X,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8955106ec2550949498b58465165253e51fdc939,https://www.semanticscholar.org/paper/8955106ec2550949498b58465165253e51fdc939,Analytical Science for Autonomy Evaluation,"Current directions in autonomous systems focus on collecting large amounts of data to verify, validate, test, and evaluate system operations. For multidomain and uncertain scenarios, data sampling may not be adequate to fully explore and represent the entire trade space for verification and validation (V&V). However, leveraging methods from test and evaluation (T&E), a hierarchy of analytics can be developed so as to narrow the trade space, while the opportunity cost of the remaining space is a risk-mitigated deployment strategy. Issues in V&V/T&E employ statistics, but could benefit from theoretical analytics, such as the ability to augment data for testing using simulated models or define tests to minimize operational risk. The use of modeling is not new; however, as analytics of artificial intelligence and machine learning (AI/ML) are designed to exploit data; then these methods are independent of the data developed from the first-principles physics models. The paper highlights the need for methods of analytical science for autonomy evaluation and presents three examples in structural, situation, and cyber awareness.",2019 IEEE National Aerospace and Electronics Conference (NAECON),2019,10.1109/NAECON46414.2019.9057992,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4cc396f9875a5551304c4726809702b49f12dc52,https://www.semanticscholar.org/paper/4cc396f9875a5551304c4726809702b49f12dc52,Cost-Sensitive Learning: Preface,"This volume contains the Proceedings of the International Workshop on Cost-Sensitive Learning COST’2018. This workshop was co-organized by the Laboratory of Artificial Intelligence and Decision Support (INESC TEC), the Department of Computer Science in Faculty of Sciences of Porto University (Portugal), the Faculty of Computer Science of Dalhousie University and the Department of Computer and Information Science in Fordham University. The workshop was co-located with the SIAM: International Conference on Data Mining (SDM) 2018 and was held on the 5th of May 2018 in the San Diego Marriott Mission Valley, San Diego, California, USA. Research on data mining and machine learning tasks is usually developed under assumptions of uniform preferences, where cases are equally important, and issues such as data acquisition costs are not considered. However, many real-world data mining applications involve complex settings where such assumptions do not hold. Frequently, predictive analytics involve settings where the consideration of costs is unavoidable. Such costs can appear at all stages of the data mining process, e.g. data acquisition, modelling or model deployment. The main goal of this workshop is to address these tasks involving the consideration of costs and/or benefits that may arise from different sources. The most frequently studied setting involves binary classification tasks with costs considered at the evaluation level. In this case, different penalizations and/or benefits are assigned to different errors and/or accurate predictions, and a cost matrix is used to express this domain-dependent information used in assessing the performance of the models. However, other predictive tasks may also be cost dependent, such as regression and time series or data streams forecasting tasks. Moreover, there are other issues which, although relevant, are still unsolved or need improved solutions, such as performance evaluation, different cost",COST@SDM,2018,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
07db97e5d50099adb50f91ec81fca22fe52c27f4,https://www.semanticscholar.org/paper/07db97e5d50099adb50f91ec81fca22fe52c27f4,Variation Aware Training of Hybrid Precision Neural Networks with 28nm HKMG FeFET Based Synaptic Core,"The plethora of data generated by edge devices and IoT devices has made machine learning the default choice of everyone for solving many complex tasks. Deep neural networks (DNNs) while deployed to perform several tasks such as speech and image recognition have shown superior performance compared to trivial machine learning algorithms. The applications like intelligent healthcare monitoring systems, smartwatches, or automatic cars require real-time processing of the data or image, which is done by machine learning algorithms with higher efficiency than humans. There are two possible methods for artificial intelligence1. non-Von-Neumann hardware-based implementation of neural network. 2. Traditional computer science base approach for neural networks or traditional Von-Neumann architecture-based implementation of neural networks. The standard Von-Neumann performance of neural networks, where the memory and the computation parts are segregated, severely suffers from latency with the rising number of edge devices. However, the plethora of usage of edge devices in our daily life foists stringent restrictions on latency, device area, and power consumption for the hardware. Therefore, we need to take the route beyond CMOS-based mixed-signal implementation of neural networks. However, the training of emerging non-volatile memory (eNVM) based DNNs is rigorous, which necessitates the search for a novel computing framework for this application. A crossbar array with eNVM devices calculates the huge, weighted vector-matrix computation after storing the weight of the synapse as conductance states. However, the main challenge is the weight update process in a reliable manner in presence of device variations. This work proposes a hybrid-precision neural network training framework with an eNVM based computational memory unit executing the weighted sum operation and another SRAM unit, which stores the error in weight update during backpropagation and the required number of pulses to update the weights in the hardware. The hybrid training algorithm for MLP based neural network with 28 nm ferroelectric FET (FeFET) as synaptic devices achieves inference accuracy up to 95% in presence of device and cycle variations. The architecture is primarily evaluated using behavioral or macromodel of FeFET devices with experimentally calibrated device variations and we have achieved accuracies compared to floating-point implementations.",ArXiv,2022,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1fa48b19812bea21c56e652749ef4fb879114c04,https://www.semanticscholar.org/paper/1fa48b19812bea21c56e652749ef4fb879114c04,Communication-efficient distributed eigenspace estimation with arbitrary node failures,"We develop an eigenspace estimation algorithm for distributed environments with arbitrary node failures, where a subset of computing nodes can return structurally valid but otherwise arbitrarily chosen responses. Notably, this setting encompasses several important scenarios that arise in distributed computing and data-collection environments such as silent/soft errors, outliers or corrupted data at certain nodes, and adversarial responses. Our estimator builds upon and matches the performance of a recently proposed non-robust estimator up to an additive O(σ √ α) error, where σ is the variance of the existing estimator and α is the fraction of corrupted nodes. 1 Problem overview and background Modern machine learning has seen the proliferation of heterogeneous distributed environments for training and deploying data science pipelines. As communication between machines is often the most time-consuming operation in distributed systems, the design of communication-efficient algorithms is of paramount importance for scaling to massive datasets [36]. However, the move to distributed environments also adds several additional layers of complexity in the design of algorithms. For example, in the distributed setting we would like our algorithms to be robust and providing meaningful answers even in settings where some nodes contain outlier data [4], silently fail during the computation [27, 31], or are compromised and returning malicious results designed to corrupt the central solution. This work focuses on distributed eigenspace estimation in the context of robustness to node-level corruptions. Formally, we assume a computing environment with nodes numbered i = 1, . . . ,m, where every node i observes a local version Ai of an unknown symmetric matrix A ∈ Rd×d; the goal is to approximate the subspace spanned by the r d principal eigenvectors of A. Distributed PCA is a standard example in this framework: every machine draws n i.i.d. samples from an unknown distribution P with covariance matrix A and forms a local empirical covariance matrix Ai. Recently proposed communication-efficient algorithms have every node i transmit Vi, the d × r matrix of principal eigenvectors of Ai, to a central server, which then aggregates all the local solutions via a carefully-crafted aggregation procedure [8, 17]. Preprint. Under review. ar X iv :2 20 6. 00 12 7v 1 [ st at .M L ] 3 1 M ay 2 02 2 We devise and analyze an algorithm that is robust to a wide range of corruptions that can occur to a subset of the computational nodes. In particular, we assume that some fraction α of the computational nodes can respond with completely arbitrary, but structurally valid, responses (i.e., they return arbitrary matrices Vi with orthonormal columns). This model encompasses three common forms of node-level corruption that cannot be easily detected by the central machine in isolation: Silent/soft errors: While computational errors may be rare on single machines, as distributed workloads span large numbers of nodes the probability that some of them fail becomes significant. Though catastrophic failures may be detectable, allowing the central server to simply ignore the output of specific nodes, the more nefarious issue is that of so-called silent (or soft) errors [15, 18, 27]. More specifically, a silent error is one where a node returns an erroneous but structurally valid response to the central machine query. Because the response is structurally valid and the central machine may not have access to the per-node data it is not possible to “validate” the response of each node and, instead, the central estimator must be adapted to be robust to such errors. Outliers or corrupted data: In certain settings the data collection may be distributed in addition to the computation. If some of the nodes are drawing samples from an invalid or corrupted data source they may introduce gross outliers to the set of responses {Vi | i ∈ [m]}. Similarly, in the distributed PCA example, while most machines draw a sufficient number of samples, a minority of them may have only a small amount of data available such that the principal eigenspaces of the local empirical covariance matrices are too far from the ground truth, and thus violate standard modelling assumptions in distributed learning. Again, robustness to such outlier responses must be a feature of the estimator since they cannot be detected by individual nodes (as they do not have information about the global problem). Adversarial responses: In some settings, a subset of nodes may be compromised by an adversary who wishes to influence the central solution by crafting and returning malicious Vi. In fact, the adversarial nodes may be collaborating when constructing their responses. Since the central node does not get to see all the data it cannot validate responses or directly detect adversaries. Therefore, the estimator itself must be adapted to be robust to collections of responses designed to push the solution in specific directions. The main contribution of our paper is a communication-efficient algorithm that is robust to node corruptions (as outlined above) for the distributed eigenspace estimation problem. We note in passing that our corruption model is similar to so-called Byzantine failures [25] in distributed systems. 1.1 Related work Distributed eigenspace estimation. The problem of distributed eigenspace estimation has been well-studied in the absence of malicious noise. One of the challenges in the distributed setting is aggregating local solutions in the presence of symmetry: for example, if v is an eigenvector ofA, both ±v are valid solutions to our problem. Various works deal with such symmetries in different ways; in the algorithms of [5, 17], the central node averages the spectral projectors of the local eigenspaces, and performs an eigendecomposition of the resulting average to approximate the principal eigenspace. This approach is similar to the algorithms of [3, 9, 26], although the latter works focus on distributed low-rank approximations and do not address the issue of approximating the principal eigenspace directly. Another standard approach is for the central server to aggregate local solutions after an alignment step designed to remove the orthogonal ambiguity [8, 16, 20] (see also [6] for the nondistributed setting). Indeed, our work builds on the two-stage algorithm presented and analyzed in [8] for the non-robust setting. Finally, we briefly mention a recent line of work [10, 20] that adapts the shift-and-invert preconditioning framework [19] to the distributed setting; however, the latter approach leads to algorithms that require multiple rounds of communication. Robust PCA. The literature contains a number of different formulations for robust principal component analysis. The seminal work of Candés et al. [7] formulated robust PCA as the task of separating an observed matrix Y ∈ Rd×d into a low-rank and a sparse component – a slightly different problem from that considered in this paper. Xu et al. [34] considered the problem of approximating a low-dimensional distribution from a set of n i.i.d. samples, a constant fraction of which have been individually corrupted by gross outliers. Follow-up works in the robust statistics literature",ArXiv,2022,10.48550/arXiv.2206.00127,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9051bc60df14544b020689ed8fba192e68dcfaa8,https://www.semanticscholar.org/paper/9051bc60df14544b020689ed8fba192e68dcfaa8,CISE: Community Engagement of CEB Cloud Ecosystem in Box,"—The explosion of digital and observational data is having a profound effect on the nature of scientific inquiry, requiring new approaches to manipulating and analyzing large and complex data and increasing the need for collaborative solid research teams to address these challenges. These data, along with the availability of computational resources and recent advances in artificial intelligence, machine learning software tools, and methods, can enable unprecedented science and innovation. Unfortunately, these software tools and techniques are not uniformly accessible to all communities, mainly scientists and engineers at Minority Serving Institutions (MSI). Cloud computing resources are natural channels to enhance these institutions' research productivity. However, utilizing cloud computing resources for research effectively requires a significant investment in time and effort, awkward manipulation of data sets, and deployment of cloud-based applications workflows that support analysis and visualization tools.",International Journal of Advanced Computer Science and Applications,2022,10.14569/ijacsa.2022.0130401,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
249b56f2252853aff3f84e6d9b561f2b04e808dc,https://www.semanticscholar.org/paper/249b56f2252853aff3f84e6d9b561f2b04e808dc,Improving Students' Academic Performance with AI and Semantic Technologies,"Artificial intelligence and semantic technologies are evolving and have been applied in various research areas, including the education domain. Higher Education institutions strive to improve students' academic performance. Early intervention to at-risk students and a reasonable curriculum is vital for students' success. Prior research opted for deploying traditional machine learning models to predict students' performance. In terms of curriculum semantic analysis, after conducting a comprehensive systematic review regarding the use of semantic technologies in the Computer Science curriculum, a major finding of the study is that technologies used to measure similarity have limitations in terms of accuracy and ambiguity in the representation of concepts, courses, etc. To fill these gaps, in this study, three implementations were developed, that is, to predict students' performance using marks from the previous semester, to model a course representation in a semantic way and compute the similarity, and to identify the prerequisite between two similar courses. Regarding performance prediction, we used the combination of Genetic Algorithm and Long-Short Term Memory (LSTM) on a dataset from a Brazilian university containing 248730 records. As for similarity measurement, we deployed BERT to encode the sentences and used cosine similarity to obtain the distance between courses. With respect to prerequisite identification, TextRazor was applied to extract concepts from course description, followed by employing SemRefD to measure the degree of prerequisite between two concepts. The outcomes of this study can be summarized as: (i) a breakthrough result improves Manrique's work by 2.5% in terms of accuracy in dropout prediction; (ii) uncover the similarity between courses based on course description; (iii) identify the prerequisite over three compulsory courses of School of Computing at ANU.",ArXiv,2022,10.48550/arXiv.2206.03213,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cdc671224b2bff45b48d726d69b04406021c8ba4,https://www.semanticscholar.org/paper/cdc671224b2bff45b48d726d69b04406021c8ba4,Tearing Apart NOTEARS: Controlling the Graph Prediction via Variance Manipulation,"Simulations are ubiquitous in machine learning. Especially in graph learning, simulations of Directed Acyclic Graphs (DAG) are being deployed for evaluating new algorithms. In the literature, it was recently argued that continuous-optimization approaches to structure discovery such as NOTEARS might be exploiting the sortability of the variable’s variances in the available data due to their use of least square losses. Specifically, since structure discovery is a key problem in science and beyond, we want to be invariant to the scale being used for measuring our data (e.g. meter versus centimeter should not affect the causal direction inferred by the algorithm). In this work, we further strengthen this initial, negative empirical suggestion by both proving key results in the multivariate case and corroborating with further empirical evidence. In particular, we show that we can control the resulting graph with our targeted variance attacks, even in the case where we can only partially manipulate the variances of the data.",ArXiv,2022,10.48550/arXiv.2206.07195,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a61f9078a489e29766ee90fa03286b5bc20cef4a,https://www.semanticscholar.org/paper/a61f9078a489e29766ee90fa03286b5bc20cef4a,Research on Strategy Discovery and Optimisation of an Intelligent Option Trading System,"'Blue Ocean' is a term that refers to an unexploited or uncontested market space. AI, machine learning and Big Data has presented a range of technologies for exploring Blue Ocean market spaces. For example, in the world of financial technology (fintech) introducing new investment strategies
 could have an impact on investment opportunities. Professor Chien-Feng Huang, Department of Computer Science and Information Engineering, National University of Kaohsiung, Taiwan, conducts novel research to discover and optimise an intelligent option trading system. In one line of research,
 he is working to solve the problem of combining different strategies to generate generalised models to tackle various financial circumstances while investing. A feasible way to do this, according to the studies of Huang and his team, is to utilise AI-based optimisation methodologies. Through
 their studies, the researchers are seeking to advance the research and applications of fintech and discover more efficient and effective investment models to create blue ocean strategies within finance. Huang is also working on autonomous self-evolving forecasting models for price movement
 in high frequency trading (HFT). He and his team have developed novel AI-based models for forecasting price movement in HFT and found that their proposed methods can increase the prediction accuracy for HFT price movement and advance the current state of HFT research. Huang has had encouraging
 results so far with the stock selection models, pairs-trading models and options-trading models developed and deployed in the market proving viable.",Impact,2022,10.21820/23987073.2022.3.26,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b1740be13a0d3c9c05873dab4f07e5688eb39a98,https://www.semanticscholar.org/paper/b1740be13a0d3c9c05873dab4f07e5688eb39a98,The Role of the Arts and Innovation in Revitalizing Downtown,"Jim has over 20 years’ experience in the development and global deployment of advanced technology products with an emphasis on great customer experiences. Career highlights include holding executive roles in strategic marketing, global product management/development, M&A evaluation/integration, and customer experience oversight for global communication and technology leaders, including Level 3 Communications (now Lumen), Global Crossing Ltd., Silicon Graphics/Cray Research and Soleo Communications. Jim is excited to highlight Immersitech, a local technology start-up focused on machine learning based audio software tools designed to improve the overall quality and engagement levels for providers in the business communications, distance learning and social entertainment markets. the topics of Journal of Manufacturing and Materials Processing. His research is at the intersection of mechanical/industrial engineering and computer science, driven by engineering design, geometric modeling, computer vision, artificial intelligence, and human-computer interaction. His research interests include augmented reality-based human-robot interaction, virtual reality-based manufacturing training, machine learning for understanding manufacturing knowledge and skills, and gamification for training. for more This poster is based on the author's research over the last two years, with a particular emphasis on the application of augmented reality and virtual reality to novel users and K-12 education. The author's various research innovations and attempts to develop systematic STEM curricula cover systematic STEM curricula that incorporate emerging techniques such as augmented/virtual reality-assisted education, occupational training that incorporates cognition awareness, and AI-assisted data science in elementary and secondary education. Three specific research were conducted: Immersive Virtual Reality Training with Error Management for CNC Milling Set-up, System Design of A Human-centered Augmented Reality Robot Programming Interface with Cognition Awareness, and Data Analysis in Machine Learning for K-12 Students. To solve the problems of complex robot programming tasks, we propose an Augmented Reality (AR) based human-robot interface for planning a collision-free path in a complex environment. Current robot programming methods usually require a high level of experience in robot programming, the time-consuming 3D modeling of the working environment for collision detection, and a tedious and inefficient re-planning to adapt to the environment or task changes. In order to address these problems, an end-to-end AR human-robot interface is proposed, which provides a new affordance to users by enabling them to plan the path in the AR environment. A set of user-interactive tools allow users to define and edit waypoints as the high-level guidance and the direct inputs for the toolpath planning package, Kinematics and Dynamics Library (KDL). With the fast sensing of the workspace and accurate rendering, an in-situ simulation module is utilized for collision check and verification by the users’ perception. Users will repeat the process of 1) waypoints definition and editing, and 2) the collision checking and path feasibility verification until a satisfactory path is obtained. Preliminary testing is conducted in a use case with complex obstacles to verify the effectiveness and the efficiency of the proposed interface. Background: Anxiety and distress have shown to exacerbate the experience of pain (Ploghaus et al., 2001) and increase sedatives and opiates dosages required before, during, and after surgical procedures (Ina & Zeev, 1999). Virtual reality (VR) is a relatively new intervention that has been used to promote relaxation and manage perioperative stress by using the principle of distraction (Eijlers et al., 2019; Ganry et al., 2018). Primary objective: To investigate if VR reduces perioperative anxiety levels in children and adults across the departments of Oral and Maxillofacial Surgery and Pediatric Procedural Care Ambulatory Centers at the University of Rochester Medical Center/ Strong Memorial Hospital in Rochester, NY. Method: We will perform a single-center feasibility and acceptability pilot study examining the impact of VR on perioperative distress in pediatric and adult patients by collecting data from 60 children and 30 adults in the span of six months. We will measure pre-operative anxiety using a verbal rating of anxiety for adults during three different time points. For children, we will use the mYPAS Form as a reliable observational measure with four behavior domains. Furthermore, we plan to assess patients’ perception of the VR intervention at the end of the procedure using the Acceptability Intervention Measure (AIM). At the end of the study, we will use a Feasibility of Intervention Measure (FIM) for medical staff at each department to assess their perception of the VR intervention and its ability to be successfully implemented in the hospital workflow. This project develops a noninvasive ultrasound-based imaging modality, which will be applied to assess the viscoelastic properties of hepatic fibrosis. It will be used to monitor in situ changes in biomechanical properties associated with chronic liver injury. Ultrasound being a preferred choice in clinical assessments has been established as an effective modality for monitoring fibrosis progression in patients. Biological tissues being predominantly viscoelastic exhibit frequency dependent shearwave speed and attenuation. Most elastography methodologies estimate the group speed of shear waves but refrain from estimation or measurement of the frequency dependence of shear wave speed or attenuation. Previous studies in our lab have shown, that shear wave propagation in a medium is itself a function of the Acoustic Radiation Force (ARF) push beam geometry and duration. We propose to develop a robust viscoelastic estimator that accounts for variation We develop a pulsed terahertz spectroscopy-based imaging technique to study paraffin-embedded murine pancreatic ductal adenocarcinoma (PDAC) tissues. We employ a novel maximum-likelihood estimation (MLE)-based parameter extraction method to map terahertz markers namely refractive index and absorption coefficient which can reflect the tissue characteristics enabling unbiased and reproducible THz measurements. We report a well-resolved differences between the tumor and healthy pancreas along with an enhanced absorbance in tumor tissue compared to its healthy counterpart. Additionally, we probe untreated and stereotactic body radiotherapy (SBRT) treated PDAC tissue to measure degree of cytotoxic responsivity to such therapies. The research goal of this collaborative work is to explore and demonstrate the use of sub-MHz ultrasound frequencies in combination with super-resolution image reconstruction methods for transcranial brain imaging with sub-millimeter resolution capability. Low-frequency ultrasound allows higher penetration with a possibility of traversing skull layers with reduced losses. Before tissue imaging is commenced, the proposed method first determines an optimal excitation frequency by measuring echoes from skull layers in response to a range of frequencies (e.g., 300 kHz to 800 kHz). During the subsequent imaging procedure, the received echoes from brain tissue are then fit to the imaging model in a least-square sense penalized by a mixed L1 and L2-norm to estimate reflectance coefficients from the brain tissue and recover resolution losses due to the use of frequencies lower than those traditionally used in standard medical ultrasound. The proposed method can increase the imaging resolution up to an order of magnitude as compared to traditional B-mode ultrasound. Preliminary results show that Guitar tablature transcription is an important but understudied problem within the field of music information retrieval. Traditional signal processing approaches offer only limited performance on the task, and there is little acoustic data with transcription labels for training machine-learning models. However, guitar transcription labels alone are more widely available in the form of tablature, which is commonly shared among guitarists online. In this work, a collection of symbolic tablature is leveraged to estimate the pairwise likelihood of notes on the guitar. The output layer of a baseline tablature transcription model is reformulated, such that an inhibition loss can be incorporated to discourage the co-activation of unlikely note pairs. This naturally enforces playability constraints for guitar, and yields tablature which is more consistent with the symbolic data used to estimate pairwise likelihoods. With this methodology, we show that symbolic tablature can be used to shape the distribution of a tablature transcription model’s predictions, even when little acoustic data is available. Automatic Speaker Verification (ASV) systems aim to verify a speaker’s claimed identity through voice. However, voice can be easily forged with replay, text-to-speech (TTS), and voice conversion (VC) techniques, which may compromise ASV systems. Voice anti-spoofing is developed to improve the reliability of speaker verification systems against such spoofing attacks. One main issue of voice anti-spoofing systems is its generalization ability to unseen synthetic attacks, i.e., synthesis methods that are not seen during training of the anti-spoofing models. We propose one-class learning, where the model compacts the distribution of learned representations of bona fide speech while pushing away spoofing attacks to improve the results. Another issue is the robustness to variations of acoustic and telecommunication channels. To alleviate this issue, AI algorithms have been found to learn biases from data. Therefore, it is urgent and vital to identify biases in AI algorithms. However, the previous bias identification pipeline overly relies o",,2022,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6949e603b3c9cbbb0f36029516c6dab8a9026a8e,https://www.semanticscholar.org/paper/6949e603b3c9cbbb0f36029516c6dab8a9026a8e,Flex Sensor Dataset: Towards Enhancing the Performance of Sign Language detection System,"Whether it's God's creation or science, the earth has evolved and life came into existence. Not all have the same character instead exhibit similar. As of reports from the National Institute on Deafness and Communication related disorders 18.5 million(approx.) have speech related disorders. Many are born with speech inability and many get speech-related disorders and are unable to speak. As communication is key for an effective society and environment, a person with speech inability looks different from a normal person. A communication gap exists between a normal person and a person with speech inability. As technological revolutions have taken the world to a new level, still a better and efficient process to make a speech disorder person a normal person is not available. There are many processes and products available but the efficient and reliable ones are not available. The use of Machine Learning and Artificial Intelligence in many fields has high success rates. In this project, a wearable device is to be made with the help of flex sensors and an accelerometer sensor which is to be used by a speech disorder person to communicate with others. There are many sign language systems available, considering the American Sign Language (ASL) system for each alphabet and each numerical dataset is to be created. A comparative study is to be made by applying different machine learning algorithms for the created dataset. The ML algorithm with better performance is identified and to be deployed in microcontroller in the future.",2022 International Conference on Computer Communication and Informatics (ICCCI),2022,10.1109/ICCCI54379.2022.9740857,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
325657649992e8be37fe11dcf277b5ebb8f90afd,https://www.semanticscholar.org/paper/325657649992e8be37fe11dcf277b5ebb8f90afd,A Hybrid SVC-CNN based Classification Model for Handwritten Mathematical Expressions(Numbers and Operators),"Machine learning and Computer Vision are computer science domains that have been working closely for a long time. Given the ubiquity of handwritten text in human transactions, we are endeavoring to acquire the answer to the quest ""Can Computer Vision and Machine learning together be deployed effectively for decisive classification of handwritten mathematical numbers and operators?"". The easier it is to communicate via handwritten texts and documents, the more challenging the task of digitizing and prediction, especially for the two-dimensional complex math statements and operators. This paper presents a hybrid model that involves machine learning and deep learning-based decision algorithms for classifying and predicting mathematical numbers and operators. The dataset considered for the experimentation has been downloaded from the Kaggle dataset store consisting of more than 12K images. The primary tasks involved include data collection, data preprocessing, and building and deploying the model. Mainly our model focuses on the extraction of contour features and performing classification using the LinearSVC model, and the prediction of numbers has been accomplished using CNN. The proposed classification and prediction model achieves an accuracy of 89.76% for predicting the math operators and 91.48% for predicting the numbers.",2022 International Conference on Decision Aid Sciences and Applications (DASA),2022,10.1109/DASA54658.2022.9765141,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
bab1ae565c3c180cb4bacf3ea7154ae101bc01e4,https://www.semanticscholar.org/paper/bab1ae565c3c180cb4bacf3ea7154ae101bc01e4,Building a generalisable ML pipeline at ING,"Advances in data science have caused an increase in the use of Artificial Intelligence (AI), specifically Machine Learning (ML), throughout various fields. Not only in research but in the industry as well, has ML been receiving increasing amounts of interest. Many companies rely on ML models to increase the efficiency of existing processes or offer new services and products. The industry, however, is facing several additional challenges compared to the academic context. One of those challenges is applying the Development Operations (DevOps) model to an ML application, also referred to as MLOps. This thesis sets out to find the specific challenges that practitioners encounter while operationalising ML models. To do so, we perform a single-case case study on an ML pipeline built by the Trade & Communication Surveillance team at the ING bank. This case study consists of conducting a set of interviews and performing amanual code inspection of the pipeline. The team faces challenges ranging from having insufficient time for operationalising each ML project individually to operating in the highlyregulated fintech context. Their pipeline is able to deploy a single ML model but it does not generalise well to other projects. We present the first version of an application that mitigates these challenges. The application is able to deploy ML models to the development environment at ING and can be operated by data scientists to reduce the effort of operationalising an ML model.",,2022,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
52230789688d713cb3b83da470228d44c2b7b438,https://www.semanticscholar.org/paper/52230789688d713cb3b83da470228d44c2b7b438,Network Report: A Structured Description for Network Datasets,"The rapid development of network science and technologies depends on shareable datasets. Currently, there is no standard practice for reporting and sharing network datasets. Some network dataset providers only share links, while others provide some contexts or basic statistics. As a result, critical information may be unintentionally dropped, and network dataset consumers may misunderstand or overlook critical aspects. Inappropriately using a network dataset can lead to severe consequences (e.g., discrimination) especially when machine learning models on networks are deployed in high-stake domains. Challenges arise as networks are often used across different domains (e.g., network science, physics, etc) and have complex structures. To facilitate the communication between network dataset providers and consumers, we propose network report. A network report is a structured description that summarizes and contextualizes a network dataset. Network report extends the idea of dataset reports (e.g., Datasheets for Datasets) from prior work with network-specific descriptions of the non-i.i.d. nature, demographic information, network characteristics, etc. We hope network reports encourage transparency and accountability in network research and development across different fields.",ArXiv,2022,10.48550/arXiv.2206.03635,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1f2e024d9fc65a663ebf2203cdc1042371883bcc,https://www.semanticscholar.org/paper/1f2e024d9fc65a663ebf2203cdc1042371883bcc,Composing Complex and Hybrid AI Solutions,"Progress in several areas of computer science has been enabled by comfortable and efficient means of experimentation, clear interfaces, and interchangable components, for example using OpenCV for computer vision or ROS for robotics. We describe an extension of the Acumos system towards enabling the above features for general AI applications. Originally, Acumos was created for telecommunication purposes, mainly for creating linear pipelines of machine learning components. Our extensions include support for more generic components with gRPC/Protobuf interfaces, automatic orchestration of graphically assembled solutions including control loops, sub-component topologies, and event-based communication, and provisions for assembling solutions which contain user interfaces and shared storage areas. We provide examples of deployable solutions and their interfaces. The framework is deployed at http://aiexp.ai4europe.eu/ and its source code is managed as an open source Eclipse project.",ArXiv,2022,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ca1d9e0de97f45c5c23e850d2ff6801f7ea5d334,https://www.semanticscholar.org/paper/ca1d9e0de97f45c5c23e850d2ff6801f7ea5d334,Reply to Barbier et al.: NLP is not a magic bullet,"Responding to Chu and Evans (1) in PNAS, Barbier et al. (2) advocate augmenting the human ability to “ingest and analyze” the publications in a field through natural language processing (NLP) categorization of papers (3) and for “open science” to facilitate such NLP use. NLP-based categorization (perhaps augmented with nonlinguistic artificial intelligence/machine learning [AI/ML] algorithms informed by, for example, citation networks and temporal patterns) can streamline navigating scientific literature. The adoption of efficient—reducing effort by an order of magnitude or better—technologies for literature scanning can create some cognitive slack, permitting scholars added time to engage with novel ideas. We argue that fundamental progress slows when scholars are cognitively overwhelmed by the enormous number of papers published each year in large fields (1). AI/ML-based technologies can reduce this cognitive load on scholars by decreasing the time spent delineating topics, categorizing and summarizing papers, and tagging papers for close reading. These technologies must be deployed thoughtfully, however, lest they exacerbate the ossification identified in ref. 1. Categorization in AI/ML applications often depends on prior categorization by humans. Any existing human behaviors—engaging with the literature in routine ways and repeatedly citing the established canon, for example—are likely to be accentuated. AI/ML may direct even more attention to the already well attended. Categorization may not be enough. Barbier et al. (2) suggest that AI/ML be used to recognize “outliers.” But will busy scholars attend to these outliers, or discard them? In very large fields, there are likely to be numerous outliers—including many with little scientific merit. How will scholars know which of these outliers to engage with? Scholars in large fields may be overwhelmed by choice, even when selecting between the subset of papers categorized as outliers. Until AI/ML becomes capable of mining meaning and significance from each paper—moving beyond categorization to scientific understanding and even appreciation—the problem of limited scholarly attention grappling with a rapidly increasing supply of ideas remains. I agree with Barbier et al. (2) that NLP can increase scholarly efficiency (and that open science is necessary to fully realize these benefits of NLP). Efficiency alone, however, does not address the problems reported in ref. 1. The social structure of science must change: Current incentives in academia often push scholars toward exploitation rather than exploration (4); many scientists are likely to take advantage of the efficiency provided by AI/ML to increase their production of canon-based papers rather than investing more time in novel ideas.",Proceedings of the National Academy of Sciences of the United States of America,2022,10.1073/pnas.2201386119,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6f960452b851a7f068e59a61c3cb19b10b239d84,https://www.semanticscholar.org/paper/6f960452b851a7f068e59a61c3cb19b10b239d84,Introspection with Data: Recommendation of Academic Majors Based on Personality Traits,"The choice of academic major and academic institution has a large effect on a person’s career. About 40% of students either transfer to a different major or different college or drop out of college within six years. Various social science research has shown that personality traits play a significant role in academic preference. Still, there has not been a comprehensive, data-driven approach to translate this into academic choice. In light of this gap in understanding, we surveyed over 500 people between 18 and 25 years old to capture personality traits and preference of college major and used that information to train a machine learning model to predict college major preference. This research validates the viability of using personality traits as indicators for educational preference. We demonstrate that using a decision tree model, accurate classification can be done, with over 90% accuracy. Furthermore, we explored the two methods of dimension reduction - one using Principal Component Analysis (PCA) and another relying on Social Science research on the Big-Five personality Traits (also known as OCEAN indices) to simplify the problem further. With these techniques, the dimension was reduced by half without decreasing the accuracy of our classifier. We compared other popular machine learning methods and demonstrated that a decision tree is best for such an application. With this research, a readily deployable recommendation system was created that can help students find their most enjoyable academic path and aid guidance counselor and parents with their recommendations.","2022 Intermountain Engineering, Technology and Computing (IETC)",2022,10.1109/ietc54973.2022.9796766,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a5448c9e4aa06a609ee668954b4c5e2c0c39af42,https://www.semanticscholar.org/paper/a5448c9e4aa06a609ee668954b4c5e2c0c39af42,Digital Twin: From Concept to Practice,"Recent technological developments and advances in Artificial Intelligence (AI) have enabled sophisticated capabilities to be a part of Digital Twin (DT), virtually making it possible to introduce automation into all aspects of work processes. Given these possibilities that DT can offer, practitioners are facing increasingly difficult decisions regarding what capabilities to select while deploying a DT in practice. The lack of research in this field has not helped either. It has resulted in the rebranding and reuse of emerging technological capabilities like prediction, simulation, AI, and Machine Learning (ML) as necessary constituents of DT. Inappropriate selection of capabilities in a DT can result in missed opportunities, strategic misalignments, inflated expectations, and risk of it being rejected as just hype by the practitioners. To alleviate this challenge, this paper proposes the digitalization framework, designed and developed by following a Design Science Research (DSR) methodology over a period of 18 months. The framework can help practitioners select an appropriate level of sophistication in a DT by weighing",Journal of Management in Engineering,2022,10.1061/(asce)me.1943-5479.0001034,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5f6584ceaabc7811238981870e2fde195760c883,https://www.semanticscholar.org/paper/5f6584ceaabc7811238981870e2fde195760c883,Real-time detection of uncalibrated sensors using Neural Networks,,Neural Comput. Appl.,2021,10.1007/s00521-021-06865-z,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
388c1c91d886a27f26354ea8b58cb2afa13b7934,https://www.semanticscholar.org/paper/388c1c91d886a27f26354ea8b58cb2afa13b7934,A new challenge for data analytics: transposons,,BioData mining,2022,10.1186/s13040-022-00294-x,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a34b4c5a963103d09fe887cd5b5a6f9572dbc701,https://www.semanticscholar.org/paper/a34b4c5a963103d09fe887cd5b5a6f9572dbc701,Deep Learning for Multi-Messenger Astrophysics: A Gateway for Discovery in the Big Data Era,"This report provides an overview of recent work that harnesses the Big Data Revolution and Large Scale Computing to address grand computational challenges in Multi-Messenger Astrophysics, with a particular emphasis on real-time discovery campaigns. Acknowledging the transdisciplinary nature of Multi-Messenger Astrophysics, this document has been prepared by members of the physics, astronomy, computer science, data science, software and cyberinfrastructure communities who attended the NSF-, DOE- and NVIDIA-funded ""Deep Learning for Multi-Messenger Astrophysics: Real-time Discovery at Scale"" workshop, hosted at the National Center for Supercomputing Applications, October 17-19, 2018. Highlights of this report include unanimous agreement that it is critical to accelerate the development and deployment of novel, signal-processing algorithms that use the synergy between artificial intelligence (AI) and high performance computing to maximize the potential for scientific discovery with Multi-Messenger Astrophysics. We discuss key aspects to realize this endeavor, namely (i) the design and exploitation of scalable and computationally efficient AI algorithms for Multi-Messenger Astrophysics; (ii) cyberinfrastructure requirements to numerically simulate astrophysical sources, and to process and interpret Multi-Messenger Astrophysics data; (iii) management of gravitational wave detections and triggers to enable electromagnetic and astro-particle follow-ups; (iv) a vision to harness future developments of machine and deep learning and cyberinfrastructure resources to cope with the scale of discovery in the Big Data Era; (v) and the need to build a community that brings domain experts together with data scientists on equal footing to maximize and accelerate discovery in the nascent field of Multi-Messenger Astrophysics.",ArXiv,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2524e8ea6f100cfe50c794c9d599fecce443ff82,https://www.semanticscholar.org/paper/2524e8ea6f100cfe50c794c9d599fecce443ff82,P 01 – Deep learning based object detection and classification,"With the rapid advancement of deep learning and high performance computing technologies, data scientists can now use GPUs, GPU clusters or purpose-built hardware systems for deep learning to develop and deploy machine learning based applications such as massive image classification and video analytics. The aim of this project is to develop a deep learning based image classifier using the latest high performance platform for visual computing. The work will include assessing the existing deep learning software packages, deploy a selected package using the platform, and implement the image classification application using the deep learning package. The practical outcome from this work will be increased capabilities for automated fishery monitoring to support sustainable fisheries. The project has close links to computer science, mathematics and engineering, and can help bridge the gap between your courses and developing a real scientific application. The suitable candidate will have opportunities to have access to world class facilities and work alongside CSIRO senior scientists while you are enjoying generous personal development and learning opportunities. Skills required: • Programming language C or C++ preferred. • Image processing/image analysis or computer vision course or experience is preferred, but not compulsory. • Quick learner. • Good communication skills. Developmental outcomes for student: • Software that can be used by others. • Exposure to high-impact research, scientific expertise in multiple disciplines, and scientific infrastructure; • Improved image processing and computer vision skills; • Improved skills in scientific communication (e.g. writing scientific reports and oral presentations). Projects – Trustworthy Systems • Formal methods o Theorem proving o Protocols o Expressiveness o Blockchain • Systems o Kernels Middleware o Security • Programming Languages Formal Methods / Theorem Proving Project P02 Automating Formal Proofs Gerwin Klein, Daniel Matichuck Abstract: Isabelle is an interactive theorem prover which combines automated and human reasoning through the use of proof methods (or tactics). These methods allow users to make highlevel decisions on how to progress in a proof without worrying about the formal details. Powerful proof methods are required for large proof undertakings, such as the L4.verified project at Data61. Eisbach is an in-development prototype of a high-level language for writing proof methods in Isabelle, which is historically done in Standard ML. It is influenced by Coq’s Ltac, but distinguishes itself by leveraging Isabelle’s existing automation and backtracking infrastructure. The aim of this project is to investigate useful applications for Eisbach, with several possible domains to explore: An existing suite of Data61-developed separation logic tactics could be reimplemented and extended, or a similar investigation can be done against the verificationcondition generator used in the L4.verified proof; Or a novel proof method can be developed solve, for example, word arithmetic proofs that appear frequently in the verification of C programs. Novelty: This will be one of the first larger-scale uses of the new Eisbach proof language and the resulting feedback will contribute to its further development. Outcome: Automated proof tactics for previously manual proofs in one or more of the specified application domains, ideally reducing proof size and time for those applications. Reference Material Links: Trustworthy Systems Research Group (TS): http://trustworthy.systems seL4: http://ertos.nicta.com.au/research/sel4/ Isabelle: http://mirror.cse.unsw.edu.au/pub/isabelle/ P03 Formal Verification of multi-threaded embedded application software June Andronick, Corey Lewis Abstract: eChronos [0] is a small embedded OS for micro-controllers. It is commercially used in medical devices and is embedded in high-assurance autonomous flying vehicles (quadcopters) demonstrated in HACMS, a large DARPA-funded project, in collaboration with industry and university partners from the US. In Data61's eChronos verification project, we aim at proving strong guarantees about eChronos correct behavior, by means of formal (mathematical) proofs, machine-checked in the Isabelle/HOL theorem prover. The long-term goal is to provide developpers of embedded applications using eChronos with a formal and verified specification of the OS API functions used for synchronisation (semaphores, mutexes, etc). The challenge lies in providing the right abstraction level for the formal API and provide a usable framework for application code verification. This project will investigate eChronos-based application verification, via small case-studies, which could be derived or inspired from application code from the HACMS project, namely the SMACCMPilot open-source autopilot software from Galois [3]. [0] /projects/TS/echronos/ [1] ""Controlled owicki-gries concurrency: reasoning about the preemptible eChronos embedded operating system"". MARS'15. /publications/nictaabstracts/Andronick_LM_15.abstract.pml [2] ""Proof of OS scheduling behavior in the presence of interrupt-induced concurrency"". To appear in ITP'16. [3]http://smaccmpilot.org/ Novelty: Formal verification of real-world embedded application on a verified embedded OS API. Outcome:The expected outcome of the project is the experimental verification of a case-study application running on eChronos, exhibiting the required formalised API from eChronos. P04 Improving automation in concurrent software verification June Andronick, Corey Lewis Abstract: eChronos [0] is a small embedded OS for micro-controllers. It is commercially used in medical devices and is embedded in high-assurance autonomous flying vehicles (quadcopters) demonstrated in HACMS, a large DARPA-funded project, in collaboration with industry and university partners from the US. In Data61's eChronos verification project, we aim at proving strong guarantees about eChronos correct behavior, by means of formal (mathematical) proofs, machine-checked in the Isabelle/HOL theorem prover. The challenge lies in the concurrency due to eChronos running with interrupts enabled, including during scheduling operations, to ensure low latency. We have successfully proved, at a model level, the correctness of eChronos scheduling behavior in presence of interrupt-induced concurrency [1,2]. This project would look at increasing the automation and scalability of the framework. Opportunities for such improvements include (but are not limited to) increased reuse of already proven facts, investigation of more modular (but less fine-grain) approaches, more use of Isabelle automation, increased compositionality of the proof process, etc. [0] /projects/TS/echronos/ [1] ""Controlled owicki-gries concurrency: reasoning about the preemptible eChronos embedded operating system"". MARS'15. /publications/nictaabstracts/Andronick_LM_15.abstract.pml [2] ""Proof of OS scheduling behavior in the presence of interrupt-induced concurrency"". To appear in ITP'16. Novelty: Your work will contribute to the general feasibility and scalability of practical concurrent software verification. Outcome: Your work will directly impact the efficiency of the framework and proofs developed for the verification of eChronos. P05 Implement and Verify a CakeML Compiler Optimisation",,2016,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1ecbe3069ac95e54ec1db0474cde919c67cf976d,https://www.semanticscholar.org/paper/1ecbe3069ac95e54ec1db0474cde919c67cf976d,Data Pricing and Data Asset Governance in the AI Era,"Data is one of the most critical resources in the AI Era. While substantial research has been dedicated to training machine learning models using various types of data, much less efforts have been invested in the exploration of assessing and governing data assets in end-to-end processes of machine learning and data science, that is, the pipeline where data is collected and processed, and then machine learning models are produced, requested, deployed, shared and evolved. To provide a state-of-the-art overall picture of this important and novel area and advocate the related research and development, we present a tutorial addressing two essential problems. First, in the pipeline of machine learning, how can data and machine learning models be priced properly so that contributions from various parties can be assessed and recognized in a fair manner? Second, in the collaboration among many parties in building, distributing and sharing machine learning models, how can data as assets be managed? Accordingly, the first part of our proposal surveys data and model pricing in the pipeline of machine learning, while the second part discusses data asset governance for collaborative artificial intelligence. Each part is self-contained. At the same time, the two parts echo each other and connect a series of interesting and important problems into a dynamic big picture.",KDD,2021,10.1145/3447548.3470818,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7291885136fdde6e665ae5bf641d151d8965db7c,https://www.semanticscholar.org/paper/7291885136fdde6e665ae5bf641d151d8965db7c,PyGuard: Finding and Understanding Vulnerabilities in Python Virtual Machines,"Python has become one of the most popular pro-gramming languages in the era of data science and machine learning, and is also widely deployed in safety-critical fields like medical treatment, autonomous driving systems, etc. However, as the official and most widely used Python virtual machine, CPython, is implemented using C language, existing research has shown that the native code in CPython is highly vulnerable, thus defeats Python's guarantee of safety and security. This paper presents the design and implementation of PyGuard, a novel software prototype to find and understand real-world security vulnerabilities in the CPython virtual machines. With PyGuard, we carried out an empirical study of 10 different versions of CPython virtual machines (from version 3.0 to the latest 3.9). By scanning a total of 3,358,391 lines native code, we have identified 598 new vulnerabilities. Based on our study, we describe a taxonomy to classify vulnerabilities in CPython virtual machines. Our taxonomy provides a guidance to construct automated and accurate bug-finding tools. We also suggest systematic remedies that can mediate the threats posed by these vulnerabilities.",2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE),2021,10.1109/ISSRE52982.2021.00055,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8646172898aa762d014c27801b2beea4de6e8991,https://www.semanticscholar.org/paper/8646172898aa762d014c27801b2beea4de6e8991,CollectiveTeach: A System To Generate And Sequence Web-Annotated Lesson Plans,"Despite an abundance of educational resources on the Web, there exists a gap between teachers and the efficient utilization of these resources. A fundamental component of teaching is the preparation of a lesson plan—an organized sequence of educational content—and for the most part, the task of generating lesson plans today is manual and laborious. To address this gap, we present CollectiveTeach, a platform that enables educators to generate lesson plans. CollectiveTeach has two main facets: (i) an information retrieval engine that gathers relevant documents pertaining to a topic, and (ii) a framework to sequence the retrieved documents into coherent lesson plans. We present a novel architecture that leverages information retrieval algorithms, data mining techniques, and user feedback to generate automated lesson plans. We built and deployed CollectiveTeach for 3 popular undergraduate Computer Science subjects: Algorithms, Operating Systems, and Machine Learning, on a corpus of ∼ 100,000 web pages. Further, we evaluated the platform in 3 phases: (1) computing the precision of the documents retrieved, (2) a user study with 10 participants who assessed lesson plans returned by CollectiveTeach based on appropriateness, quality, and coverage and (3) benchmarking our sequencing approach against the Beam-Search approach. Our results show that CollectiveTeach achieves high precision in retrieving content relevant to a user’s query, users are satisfied with the appropriateness, coverage, and reliability of the generated lesson plans and that our sequencing approach is effective. These results indicate that CollectiveTeach is a promising platform that could enrich the lesson plan generation process and encourage collaboration amongst the community of educators and learners.",COMPASS,2021,10.1145/3460112.3471938,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c18828e346ff4ce5673a39cef86d64c8e9ee42b1,https://www.semanticscholar.org/paper/c18828e346ff4ce5673a39cef86d64c8e9ee42b1,An Optimized ANN Model For Predicting The Efficiency Of Perovskite Solar Cell Using MATLAB,"The amalgamation of material science genome and algorithmic development has elevated the evolution of material science. Traditional methods of material discovery, development and deployment takes a long time frame. Therefore, machine learning models which primarily learns from past data helps in catering to the inherit limitations of conventional methods used in material science. Hence we demonstrate the potential of deep learning via Artificial Neural Network (ANN) which utilizes radical features to predict the efficiency of perovskite solar cell. Dataset was collected varies technical papers. The trained model then predicts the efficiency on unseen perovskite data. This paper also finds insights of challenges faced with ANN and how it could be improvised in the near future.",,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2eeea74e5e66cbd51a5d611cdd4fd4db2a66ea31,https://www.semanticscholar.org/paper/2eeea74e5e66cbd51a5d611cdd4fd4db2a66ea31,"Unlocking the Potential of Electrical Submersible Pumps: the Successful Testing and Deployment of a Real-Time Artificially Intelligent System, for Failure Prediction, Run Life Extension, and Production Optimization","
 This paper is a summary of the collaborative work between ADNOC (Abu Dhabi National Oil Company) and nybl, a deep tech development company, and the results of applying nybl's proprietary ""Science-Based Artificial Intelligence"" to ADNOC Electrical Submersible Pump (ESP) wells in real-time applications. The paper demonstrates the potential benefits of the real-life application of Artificial Intelligence (AI) / Machine Learning (ML) in conjunction with traditional Petroleum Engineering concepts and algorithms to predict imminent and future failures, extend and monitor run life, and maximize the production of ESPs. This paper will highlight ADNOC's innovative approach to pilot new technology through successful deployment on 27 wells, spread onshore and offshore, in real-time, with prescriptive actions.","Day 1 Mon, October 18, 2021",2021,10.2118/208647-ms,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e3a492d26049abe9b6615259cc8817d7b8209a98,https://www.semanticscholar.org/paper/e3a492d26049abe9b6615259cc8817d7b8209a98,Tunnel disturbance events monitoring and recognition with distributed acoustic sensing (DAS),"Accurately identifying disturbance events along tunnels is essential for their safe operation, which constitutes an important part of tunnel health monitoring and abnormity warning. In recent years, distributed acoustic sensing (DAS), a state-of-the-art fiber-optic sensing technology, has developed rapidly in the field of earth science and engineering. Based on the principle of phase-sensitive optical time-domain reflectometry, DAS allows detecting acoustic/vibration signals along a common fiber-optic cable up to tens of kilometers. This brings new opportunities for monitoring of long perimeters such as underground tunnels. In this paper, we propose a DAS-based method for the recognition of disturbance events along tunnels. The EMD denoising algorithm is employed to optimize vibration signals to better extract the time–frequency domain features of monitored events. Different events are then recognized via a machine learning-based multi-class classification approach. The Random Forest algorithm is applied to analyze the DAS data acquired with fiber-optic cables deployed along the tunnel lining, successfully recognizing a variety of vibration events during the construction of the tunnel including unexpected disasters such as rockfalls, with a recognition accuracy of 92.31%. This DAS-based disturbance identification method may provide a new opportunity for unmanned, real-time monitoring of tunnel abnormal events.",IOP Conference Series: Earth and Environmental Science,2021,10.1088/1755-1315/861/4/042034,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
bb362711f0c39489d48800f654f8454324aa6c92,https://www.semanticscholar.org/paper/bb362711f0c39489d48800f654f8454324aa6c92,Information-theoretic Evolution of Model Agnostic Global Explanations,"Explaining the behavior of black box machine learning models through human interpretable rules is an important research area. Recent work has focused on explaining model behavior locally i.e. for specific predictions as well as globally across the fields of vision, natural language, reinforcement learning and data science. We present a novel model-agnostic approach that derives rules to globally explain the behavior of classification models trained on numerical and/or categorical data. Our approach builds on top of existing local model explanation methods to extract conditions important for explaining model behavior for specific instances followed by an evolutionary algorithm that optimizes an information theory based fitness function to construct rules that explain global model behavior. We show how our approach outperforms existing approaches on a variety of datasets. Further, we introduce a parameter to evaluate the quality of interpretation under the scenario of distributional shift. This parameter evaluates how well the interpretation can predict model behavior for previously unseen data distributions. We show how existing approaches for interpreting models globally lack distributional robustness. Finally, we show how the quality of the interpretation can be improved under the scenario of distributional shift by adding out of distribution samples to the dataset used to learn the interpretation and thereby, increase robustness. All of the datasets used in our paper are open and publicly available. Our approach has been deployed in a leading digital marketing suite of products.",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a2535be4f49c2a500633a113c784d56ecaa0ee08,https://www.semanticscholar.org/paper/a2535be4f49c2a500633a113c784d56ecaa0ee08,A Study on Crime Prediction to Reduce Crime Rate Based on Artificial Intelligence,"This paper was conducted to prevent and respond to crimes by predicting crimes based on artificial intelligence. While the quality of life is improving with the recent development of science and technology, various problems such as poverty, unemployment, and crime occur. Among them, in the case of crime problems, the importance of crime prediction increases as they become more intelligent, advanced, and diversified. For all crimes, it is more critical to predict and prevent crimes in advance than to deal with them well after they occur. Therefore, in this paper, we predicted crime types and crime tools using the Multiclass Logistic Regression algorithm and Multiclass Neural Network algorithm of machine learning. Multiclass Logistic Regression algorithm showed higher accuracy, precision, and recall for analysis and prediction than Multiclass Neural Network algorithm. Through these analysis results, it is expected to contribute to a more pleasant and safe life by implementing a crime prediction system that predicts and prevents various crimes. Through further research, this researcher plans to create a model that predicts the probability of a criminal committing a crime again according to the type of offense and deploy it to a web service.",,2021,10.24225/KJAI.2021.9.1.15,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2828a43f9978fa46fba55d34f4e327996ff0344c,https://www.semanticscholar.org/paper/2828a43f9978fa46fba55d34f4e327996ff0344c,A Data Augmented Bayesian Network for Node Failure Prediction in Optical Networks,"Failures in optical network backbone can cause significant interruption in internet data traffic. Hence, it is very important to reduce such network outages. Prediction of such failures would be a step forward to avoid such disruption of internet services for users as well as operators. Several research proposals are available in the literature which are applications of data science and machine learning techniques. Most of the techniques rely on significant amount of real time data collection. Network devices are assumed to be equipped to collect data and these are then analysed by different algorithms to predict failures. Every network element which is already deployed in the field may not have these data gathering or analysis techniques designed into them initially. However, such mechanisms become necessary later when they are already deployed in the field. This paper proposes a Bayesian network based failure prediction of network nodes, g., routers etc., using very basic information from the log files of the devices and applying power law based data augmentation to complement for scarce real time information. Numerical results show that network node failure prediction can be performed with high accuracy using the proposed mechanism.",2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),2021,10.1109/ICAIIC51459.2021.9415186,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
35b3fae77b36c29a14ea1ecc5af24a27cd68395b,https://www.semanticscholar.org/paper/35b3fae77b36c29a14ea1ecc5af24a27cd68395b,FMS (Federated Model as a service) for healthcare: an automated secure-framework for personalized recommendation system,"The Healthcare sector has been emerging on the platform ofdata science. And data scientists are often using machine learningtechniques based on historical data to create models, makepredictions or recommendations. This paper aims to providebackground and information for the community on the benefitsand variants of Federated Learning (F.L.) with other technologiesfor medical applications and highlight key considerationsand challenges of F.L. implementation in the digital health background.With this FMaaS, we envisage a future for digital federatedhealth. We hope to empower and raise awareness aboutthe environment and fog computing to provide a more secureand better-analyzing environment. The AutoML framework isused to generate and optimize machine learning models usingautomatic engineering tools, model selection, and hyperparameteroptimization on fog nodes. Thus, making the systemmore reliable and secure for each individual by preserving privacyat their end devices. And this will lead to a personalizedrecommendation system for each individual associated withthis framework by deploying the Model to their devices foron-device inferences through the concept of differential privateModel averaging. With this framework, users don’t haveto compromise with privacy, and all their sensitive data will besecure on their end devices.",CARDIOMETRY,2021,10.18137/cardiometry.2021.20.7078,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4736a9104a2ff041413f157d4265583b225371c4,https://www.semanticscholar.org/paper/4736a9104a2ff041413f157d4265583b225371c4,PuBliCiTy: Python Bioimage Computing Toolkit,"The Python Bioimage Computing Toolkit (PuBliCiTy) is an evolving set of functions, scripts, and classes, written primarily in Python, to facilitate the analysis of biological images, of two or more dimensions, from electron or light microscopes. While the early development was guided by the goal of replacing an existing internal code-base with Python code, the effort later came to include novel tools, specially in the areas of machine learning infrastructure and model development. The toolkit is built on top of the so-called python data science stack, which includes numpy, scipy, scikit-image, scikit-learn, and pandas. It also contains some deep learning models, written in TensorFlow and PyTorch, and a web-app for image annotation, which uses Flask as the web framework. The main features of the toolkit are: (1) simplifying the interface of some routinely used functions from underlying libraries; (2) providing helpful tools for the analysis of large images; (3) providing a web interface for image annotation, which can be used remotely and on tablets with pencils; (4) providing machine learning model implementations that are easy to read, train, and deploy – written in a way that minimizes complexity for users without a computer science or software development background. The source code is released under an MIT-like license at github.com/hms-idac/PuBliCiTy. Details, tutorials, and up-to-date documentation can be found at the project’s page as well. Project page github.com/hms-idac/PuBliCiTy",bioRxiv,2021,10.1101/2021.03.01.432926,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c43a82ad7e8b7d74fb3bf7a89b50bf772cb78b91,https://www.semanticscholar.org/paper/c43a82ad7e8b7d74fb3bf7a89b50bf772cb78b91,STAMP 4 NLP - An Agile Framework for Rapid Quality-Driven NLP Applications Development,,QUATIC,2021,10.1007/978-3-030-85347-1_12,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8242318bd6fa18304cc73a9c7f602a03468aef1c,https://www.semanticscholar.org/paper/8242318bd6fa18304cc73a9c7f602a03468aef1c,Cloud-based ML Technologies for Visual Inspection: A Case Study in Manufacturing,"In recent years, cloud-based Machine Learning services have received much attention for promising fast and cost-effective deployment. At the same time, manufacturing companies are beginning to evaluate and implement these new technologies in their production processes. This paper adopts the design science research approach to demonstrate the use of cloud-based Machine Learning services to implement a visual inspection system in the manufacturing industry. As a result, our developed IT artifact can correctly classify all of the given parts in a dataset consisting of 363 images, outperforming the current manual inspection. Thereby, it addresses the various challenges faced by the industry when introducing cloud-based Machine Learning technologies, evaluating return on investment (ROI), and how this can facilitate further digital transformation in production.",HICSS,2021,10.24251/HICSS.2021.124,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
860e7a623ffc561070800d73f8890deaecd72191,https://www.semanticscholar.org/paper/860e7a623ffc561070800d73f8890deaecd72191,Verification and Repair of Neural Networks,"Neural Networks (NNs) are popular machine learning models which have found successful application in many different domains across computer science. However it is hard to provide any formal guarantee on the behaviour of neural networks and therefore their reliability is still in doubt, especially concerning their deployment in safety and securitycritical applications. Verification emerged as a promising solution to address some of these problems. In the following I will present some of my recent efforts in verifying NNs.",AAAI,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
574cd007a95f7fc8f88db63e46b16ca2896d503c,https://www.semanticscholar.org/paper/574cd007a95f7fc8f88db63e46b16ca2896d503c,Artificial Intelligence in News Media: Current Perceptions and Future Outlook,"In recent years, news media have been hugely disrupted by the potential of technological-driven approaches in the creation, production, and distribution of news products and services. Artificial intelligence (AI) has emerged from the realm of science fiction and has become a very real tool that can aid society in addressing many issues, including the challenges faced by the news industry. The ubiquity of computing has become apparent and has shown the different approaches that can be achieved using AI. We analyzed the news industry AI adoption based on the seven subfields emanated from AI: (i) machine learning; (ii) computer vision (CV); (iii) speech recognition; (iv) natural language processing (NLP); (v) planning, scheduling, and optimization; (vi) expert systems; and (vii) robotics. Our findings suggest that three subfields are being more developed in the news media: machine learning, planning, scheduling &amp; optimization, and computer vision. Other areas are still not fully deployed in the journalistic field. Most of the AI news projects rely on funds from tech companies, such as Google. This limits the potential of AI in the news industry to a small number of players. We conclude by providing examples of how these subfields are being developed in journalism and present an agenda for future research.",Journalism and Media,2021,10.20944/preprints202110.0020.v1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b9dcad3bbd1754f5ec88b14b3233acdb251140e2,https://www.semanticscholar.org/paper/b9dcad3bbd1754f5ec88b14b3233acdb251140e2,Is your Statement Purposeless? Predicting Computer Science Graduation Admission Acceptance based on Statement Of Purpose,"We present a quantitative, data-driven machine learning approach to mitigate the problem of unpredictability of Computer Science Graduate School Admissions. In this paper, we discuss the possibility of a system which may help prospective applicants evaluate their Statement of Purpose (SOP) based on our system output. We, then, identify feature sets which can be used to train a predictive model. We train a model over fifty manually verified SOPs for which it uses an SVM classifier and achieves the highest accuracy of 92% with 10-fold cross-validation. We also perform experiments to establish that Word Embedding based features and Document Similarity-based features outperform other identified feature combinations. We plan to deploy our application as a web service and release it as a FOSS service.",ICON,2017,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2d4ad12cce9281d9c95cbd7db21079e10339a15d,https://www.semanticscholar.org/paper/2d4ad12cce9281d9c95cbd7db21079e10339a15d,Fast Automatic Artifact Annotator for EEG Signals Using Deep Learning,"Electroencephalogram (EEG) is a widely used non-invasive brain signal acquisition technique that measures voltage fluctuations from neuron activities of the brain. EEGs are typically used to diagnose and monitor disorders such as epilepsy, sleep disorders, and brain death and also to help the advancement of various fields of science such as cognitive science, and psychology. EEG signals usually suffer from a variety of artifacts caused by eye movements, chewing, muscle movements, and electrode pops, which disrupts the diagnosis and hinders precise representation of brain activities. This paper proposes a deep learning based model to detect the presence of the artifacts and to classify the kind of the artifact to help clinicians resolve problems regarding artifacts immediately during the signal collection process. The model is optimized to map the 1-second segments of raw EEG signals to detect 4 different kinds of artifacts and the real signal. The model achieves a 5-class classification accuracy of 67.59%, and a true positive rate of 80% with a 25.82% false alarm for binary artifact classification with time-lapse. The model is lightweight and could potentially be deployed in portable machines.",2019 IEEE Signal Processing in Medicine and Biology Symposium (SPMB),2019,10.1109/SPMB47826.2019.9037834,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
847a915a50b2feed58992750cbd01bd78bf8376e,https://www.semanticscholar.org/paper/847a915a50b2feed58992750cbd01bd78bf8376e,Corporate Bankruptcy Prediction: An Approach Towards Better Corporate World,"
 The area of corporate bankruptcy prediction attains high economic importance, as it affects many stakeholders. The prediction of corporate bankruptcy has been extensively studied in economics, accounting and decision sciences over the past two decades. The corporate bankruptcy prediction has been a matter of talk among academic literature and professional researchers throughout the world. Different traditional approaches were suggested based on hypothesis testing and statistical modeling. Therefore, the primary purpose of the research is to come up with a model that can estimate the probability of corporate bankruptcy by evaluating its occurrence of failure using different machine learning models. As the dataset was not well prepared and contains missing values, various data mining and data pre-processing techniques were utilized for data preparation. Within this research, the task of resolving the issues induced by the imbalance between the two classes is approached by applying different data balancing techniques. We address the problem of imbalanced data with the random undersampling and Synthetic Minority Over Sampling Technique (SMOTE). We used five machine learning models (support vector machine, J48 decision tree, Logistic model tree, random forest and decision forest) to predict corporate bankruptcy earlier to the occurrence. We use data from 2009 to 2013 on Poland manufacturing corporates and selected the 64 financial indicators to be broken down. The main finding of the study is a significant improvement in predictive accuracy using machine learning techniques. We also include other economic indicators ratios, along with Altman’s Z-score variables related to profitability, liquidity, leverage and solvency (short/long term) to propose an efficient model. Machine learning models give better results while balancing the data through SMOTE as compared to random undersampling. The machine learning technique related to decision forest led to 99% accuracy, whereas support vector machine (SVM), J48 decision tree, Logistic Model Tree (LMT) and Random Forest (RF) led to 92%, 92.3%, 93.8% and 98.7% accuracy, respectively, with all predictive financial indicators. We find that the decision forest outperforms the other techniques and previous techniques discussed in the literature. The proposed method is also deployed on the web to assist regulators, investors, creditors and scholars to predict corporate bankruptcy.",Comput. J.,2020,10.1093/comjnl/bxaa056,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
bd2110478dbd8ce2b7a295e32bc40a44cf152511,https://www.semanticscholar.org/paper/bd2110478dbd8ce2b7a295e32bc40a44cf152511,"Learning mixed membership models with a separable latent structure: Theory, provably efficient algorithms, and applications","In a wide spectrum of problems in science and engineering that includes hyperspectral imaging, gene expression analysis, and machine learning tasks such as topic modeling, the observed data is high-dimensional and can be modeled as arising from a dataspecific probabilistic mixture of a small collection of latent factors. Being able to successfully learn the latent factors from the observed data is important for efficient data representation, inference, and prediction. Popular approaches such as variational Bayesian and MCMC methods exhibit good empirical performance on some realworld datasets, but make heavy use of approximations and heuristics for dealing with the highly non-convex and computationally intractable optimization objectives that",,2015,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
320e3a66dbc51a1331f65b5ac42b937a39d5e051,https://www.semanticscholar.org/paper/320e3a66dbc51a1331f65b5ac42b937a39d5e051,Physical Activity Recognition With Statistical-Deep Fusion Model Using Multiple Sensory Data for Smart Health,"Nowadays, enhancing the living standard with smart healthcare via the Internet of Things is one of the most critical goals of smart cities, in which artificial intelligence plays as the core technology. Many smart services, deployed according to wearable sensor-based physical activity recognition, have been able to early detect unhealthy daily behaviors and further medical risks. Numerous approaches have studied shallow handcrafted features coupled with traditional machine learning (ML) techniques, which find it difficult to model real-world activities. In this work, by revealing deep features from deep convolutional neural networks (DCNNs) in fusion with conventional handcrafted features, we learn an intermediate fusion framework of human activity recognition (HAR). According to transforming the raw signal value to pixel intensity value, segmentation data acquired from a multisensor system are encoded to an activity image for deep model learning. Formulated by several novel residual triple convolutional blocks, the proposed DCNN allows extracting multiscale spatiotemporal signal-level and sensor-level correlations simultaneously from the activity image. In the fusion model, the hybrid feature merged from the handcrafted and deep features is learned by a multiclass support vector machine (SVM) classifier. Based on several experiments of performance evaluation, our fusion approach for activity recognition has achieved the accuracy over 96.0% on three public benchmark data sets, including Daily and Sport Activities, Daily Life Activities, and RealWorld. Furthermore, the method outperforms several state-of-the-art HAR approaches and demonstrates the superiority of the proposed intermediate fusion model in multisensor systems.",IEEE Internet of Things Journal,2021,10.1109/JIOT.2020.3013272,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e773b317adc7d48e0903dc356960e299418f1d8b,https://www.semanticscholar.org/paper/e773b317adc7d48e0903dc356960e299418f1d8b,Labtainers Cyber Exercises: Building and Deploying Fully Provisioned Cyber Labs that Run on a Laptop,"Labtainers are fully provisioned Linux-based computer science lab exercises with an initial emphasis on cybersecurity. Consistent lab execution environments and automated provisioning are provided by Docker containers. With over 50 lab exercises including multi-component networks that all run on a modestly performing laptop computer., Labtainers supports exploratory learning for both local and remote learners. They offer automated assessment of student lab activity and progress as well as individualized lab exercises to discourage sharing solutions. Free and open at: https://nps.edu/web/c3o/labtainers, Labtainers is distributed as a single virtual machine for either VirtualBox or VMWare. On an exercise-specific basis, the framework leverages Docker containers to instantiate one or more networked computers within that single VM. This hands-on workshop covers the basics of creating Labtainer-based labs, whether for security, networking, operating systems, or other computer science classes. The workshop also introduces how this lab framework helps remove three barriers to CS lab exercises: 1) administrative setup and resulting divergent behavior between student environments; 2) sharing of solutions amongst students; 3) assessing student work. Participants should have a computer running either VMWare or VirtualBox, with the Labtainers VM appliance installed.",SIGCSE,2021,10.1145/3408877.3432490,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f8b46a31d5f6c05ede392808bce0763e3b4dea59,https://www.semanticscholar.org/paper/f8b46a31d5f6c05ede392808bce0763e3b4dea59,"Data-Driven Construction Safety Information Sharing System Based on Linked Data, Ontologies, and Knowledge Graph Technologies","Accident, injury, and fatality rates remain disproportionately high in the construction industry. Information from past mishaps provides an opportunity to acquire insights, gather lessons learned, and systematically improve safety outcomes. Advances in data science and industry 4.0 present new unprecedented opportunities for the industry to leverage, share, and reuse safety information more efficiently. However, potential benefits of information sharing are missed due to accident data being inconsistently formatted, non-machine-readable, and inaccessible. Hence, learning opportunities and insights cannot be captured and disseminated to proactively prevent accidents. To address these issues, a novel information sharing system is proposed utilizing linked data, ontologies, and knowledge graph technologies. An ontological approach is developed to semantically model safety information and formalize knowledge pertaining to accident cases. A multi-algorithmic approach is developed for automatically processing and converting accident case data to a resource description framework (RDF), and the SPARQL protocol is deployed to enable query functionalities. Trials and test scenarios utilizing a dataset of 200 real accident cases confirm the effectiveness and efficiency of the system in improving information access, retrieval, and reusability. The proposed development facilitates a new “open” information sharing paradigm with major implications for industry 4.0 and data-driven applications in construction safety management.",International journal of environmental research and public health,2022,10.3390/ijerph19020794,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4163bdd019beaec6481042813eb4193681e5bf6b,https://www.semanticscholar.org/paper/4163bdd019beaec6481042813eb4193681e5bf6b,Enabling Seamless Execution of Computational and Data Science Workflows on HPC and Cloud with the Popper Container-native Automation Engine,"The problem of reproducibility and replication in scientific research is quite prevalent to date. Researchers working in fields of computational science often find it difficult to reproduce experiments from artifacts like code, data, diagrams, and results which are left behind by the previous researchers. The code developed on one machine often fails to run on other machines due to differences in hardware architecture, OS, software dependencies, among others. This is accompanied by the difficulty in understanding how artifacts are organized, as well as in using them in the correct order. Software containers (also known as Linux containers) can be used to address some of these problems, and thus researchers and developers have built scientific workflow engines that execute the steps of a workflow in separate containers. Existing container-native workflow engines assume the availability of infrastructure deployed in the cloud or HPC centers. In this paper, we present Popper, a container-native workflow engine that does not assume the presence of a Kubernetes cluster or any service other than a container engine such as Docker or Podman. We introduce the design and architecture of Popper and describe how it abstracts away the complexity of multiple container engines and resource managers, enabling users to focus only on writing workflow logic. With Popper, researchers can build and validate workflows easily in almost any environment of their choice including local machines, Slurm based HPC clusters, CI services, or Kubernetes based cloud computing environments. To exemplify the suitability of this workflow engine, we present a case study where we take an example from machine learning and seamlessly execute it in multiple environments by implementing a Popper workflow for it.",2020 2nd International Workshop on Containers and New Orchestration Paradigms for Isolated Environments in HPC (CANOPIE-HPC),2020,10.1109/CANOPIEHPC51917.2020.00007,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
40d7d778b9aba2794172744f928e91faa67bd603,https://www.semanticscholar.org/paper/40d7d778b9aba2794172744f928e91faa67bd603,The Impact of Bike-Sharing Ridership on Air Quality: A Scalable Data Science Framework,"This research explores the relationship between daily air quality indicator (AQI) values and the daily intensity of bike-share ridership in New York City. The authors designed and deployed a distributed data science framework on which to process and run Elastic Net, Random Forest Regression, and Gradient Boosted Regression Trees. Nine gigabytes of CitiBike ridership data, along with one gigabyte of air quality indicator (AQI) data were employed. All machine learning algorithms identified bike-share ridership intensity as either the most important or the second most important feature in predicting future daily AQIs. The authors also empirically demonstrated that although a distributed platform was necessary to ingest and pre-process the raw 10 gigabytes of data, the actual execution time of all three machine learning algorithms on cleaned, joined, and aggregated data was far faster on a local, commodity computer than on its distributed counterpart.","2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",2019,10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00341,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c2826677db3540c82a734ddb99955766f8c19e3e,https://www.semanticscholar.org/paper/c2826677db3540c82a734ddb99955766f8c19e3e,Data Science at The New York Times,"The Data Science group at The New York Times develops and deploys machine learning solutions to newsroom and business problems. Re-framing real-world questions as machine learning tasks requires not only adapting and extending models and algorithms to new or special cases but also sufficient breadth to know the right method for the right challenge. I'll first outline how unsupervised, supervised, and reinforcement learning methods are increasingly used in human applications for description, prediction, and prescription, respectively. I'll then focus on the 'prescriptive' cases, showing how methods from the reinforcement learning and causal inference literatures can be of direct impact in engineering, business, and decision-making more generally. Bio: Chris Wiggins is an associate professor of applied mathematics at Columbia University and the Chief Data Scientist at The New York Times. At Columbia he is a founding member of the executive committee of the Data Science Institute, and of the Department of Systems Biology, and is affiliated faculty in Statistics. He is a co-founder and co-organizer of hackNY (http://hackNY.org), a nonprofit which since 2010 has organized once a semester student hackathons and the hackNY Fellows Program, a structured summer internship at NYC startups. Prior to joining the faculty at Columbia he was a Courant Instructor at NYU (1998-2001) and earned his PhD at Princeton University (1993-1998) in theoretical physics. He is a Fellow of the American Physical Society and is a recipient of Columbia's Avanessians Diversity Award. For more information see: http://midas.umich.edu/events MIDAS gratefully acknowledges Wacker Chemie AG for supporting the MIDAS Seminar Series.",News Group,2019,10.1287/lytx.2021.02.26n,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5d66b5b8175c61ac8cbb84ed13a9e1e8ae331a83,https://www.semanticscholar.org/paper/5d66b5b8175c61ac8cbb84ed13a9e1e8ae331a83,Interactive Supercomputing With Jupyter,"Rich user interfaces like Jupyter have the potential to make interacting
with a supercomputer easier and more productive, consequently attracting
new kinds of users and helping to expand the application of
supercomputing to new science domains. For the scientist-user, the ideal
rich user interface delivers a familiar, responsive, introspective,
modular, and customizable platform upon which to build, run, capture,
document, re-run, and share analysis workflows. From the provider or
system administrator perspective, such a platform would also be easy to
configure, deploy securely, update, customize, and support. Jupyter
checks most if not all of these boxes. But from the perspective of
leadership computing organizations that provide supercomputing power to
users, such a platform should also make the unique features of a
supercomputer center more accessible to users and more composable with
high performance computing (HPC)
workflows. Project Jupyter’s core
design philosophy of extensibility, abstraction, and agnostic
deployment, has allowed HPC centers like NERSC to bring in advanced
supercomputing capabilities that can extend the interactive notebook
environment. This has enabled a rich scientific discovery platform,
particularly for experimental facility data analysis and machine
learning problems.",Comput. Sci. Eng.,2021,10.1109/MCSE.2021.3059037,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
829ae84db1f604eef173a0eadfeef5925c0cb487,https://www.semanticscholar.org/paper/829ae84db1f604eef173a0eadfeef5925c0cb487,How to Train Your Robot: Project-Based AI and Ethics Education for Middle School Classrooms,"We developed the How to Train Your Robot curriculum to empower middle school students to become conscientious users and creators of Artificial Intelligence (AI). As AI becomes more embedded in our daily lives, all members of society should have the opportunity to become AI literate. Today, most deployed work in K-12 AI education takes place at strong STEM schools or during extracurricular clubs. But, to promote equity in the field of AI, we must also design curricula for classroom use at schools with limited resources. How to Train Your Robot leverages a low-cost ($40) robot, a block-based programming platform, novice-friendly model creation tools, and hands-on activities to introduce students to machine learning. During the summer of 2020, we trained in-service teachers, primarily from Title 1 public schools, to deliver a five-day, online version of the curriculum to their students. In this work, we describe how students' self-directed final projects demonstrate their understanding of technical and ethical AI concepts. Students successfully selected project ideas, taking the strengths and weaknesses of machine learning into account, and implemented an array of projects about everything from entertainment to science. We saw that students had the most difficulty designing mechanisms to respond to user feedback after deployment. We hope this work inspires future AI curricula that can be used in middle school classrooms.",SIGCSE,2021,10.1145/3408877.3439690,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1b575f5f2f0605c0aab2f254ecb2bbcfac0143af,https://www.semanticscholar.org/paper/1b575f5f2f0605c0aab2f254ecb2bbcfac0143af,Deep Learning on Supercomputers,"The Deep Learning (DL) on Supercomputers workshop provides a forum for practitioners working on any and all aspects of DL for science and engineering in the High Performance Computing (HPC) context to present their latest research results and development, deployment, and application experiences. The general theme of this workshop series is the intersection of DL and HPC; the theme of this particular workshop is the applications of DL methods in science and engineering: novel uses of DL methods, e.g., convolutional neural networks (CNN), recurrent neural networks (RNN), generative adversarial networks (GAN), and reinforcement learning (RL), in the natural sciences, social sciences, and engineering, to enhance innovative applications of DL in traditional numerical computation. Its scope encompasses application development in scientific scenarios using HPC platforms; DL methods applied to numerical simulation; fundamental algorithms, enhanced procedures, and software development methods to enable scalable training and inference; hardware changes with impact on future supercomputer design; and machine deployment, performance evaluation, and reproducibility practices for DL applications with an emphasis on scientific usage. This workshop will be centered around published papers. Submissions will be peer-reviewed, and accepted papers will be published as part of the Joint Workshop Proceedings by Springer. Topics include but are not limited to: • DL as a novel approach of scientific computing – Emerging scientific applications driven by DL methods – Novel interactions between DL and traditional numerical simulation – Effectiveness and limitations of DL methods in scientific research – Algorithms and procedures to enhance reproducibility of scientific DL applications • DL for Science workflows – Data management through the life cycle of scientific DL applications – DL performance evaluation and analysis on deployed systems • Scalable DL methods to address the challenges of demanding scientific applications – General algorithms and procedures for efficient and scalable DL training – General algorithms and systems for large scale model serving for scientific use cases – New software, and enhancements to existing software, for scalable DL – DL communication optimization at scale – I/O optimization for DL at scale – DL performance modeling and tuning of DL on supercomputers – DL benchmarks on supercomputers • Novel hardware designs for more efficient DL – Processors, accelerators, memory hierarchy, interconnect changes with impact on DL in the HPC context As part of the reproducibility initiative, the workshop will require authors to provide information such as the algorithms, software releases, datasets, and hardware configurations used. For performance evaluation studies, we will encourage authors to use well-known benchmarks or applications with open accessible datasets: for example, MLPerf (https://github.com/mlperf/training) and ResNet-50 with the ImageNet-1K dataset (http://www.imagenet.org/archive/stanford/fall11 whole.tar).",,2019,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4ddaa61f40cbf2c05b2aa03a5ec283bbd417353d,https://www.semanticscholar.org/paper/4ddaa61f40cbf2c05b2aa03a5ec283bbd417353d,Unadjusted Langevin algorithm for non-convex weakly smooth potentials,"Abstract: Discretization of continuous-time diffusion processes is a widely recognized method for sampling. However, the canonical Euler Maruyama discretization of the Langevin diffusion process, referred as Unadjusted Langevin Algorithm (ULA), studied mostly in the context of smooth (gradient Lipschitz) and strongly log-concave densities, is a considerable hindrance for its deployment in many sciences, including statistics and machine learning. In this paper, we establish several theoretical contributions to the literature on such sampling methods for non-convex distributions. Particularly, we introduce a new mixture weakly smooth condition, under which we prove that ULA will converge with additional log-Sobolev inequality. We also show that ULA for smoothing potential will converge in L2-Wasserstein distance. Moreover, using convexification of nonconvex domain [24] in combination with regularization, we establish the convergence in Kullback-Leibler (KL) divergence with the number of iterations to reach ǫ-neighborhood of a target distribution in only polynomial dependence on the dimension. We relax the conditions of [31] and prove convergence guarantees under isoperimetry, and non-strongly convex at infinity.",,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
da6676b473e936d300a890fb14de89a5d9426b5a,https://www.semanticscholar.org/paper/da6676b473e936d300a890fb14de89a5d9426b5a,FathomNet: A global underwater image training set for enabling artificial intelligence in the ocean,"Ocean-going platforms are integrating high-resolution camera feeds for observation and navigation, producing a deluge of visual data. The volume and rate of this data collection can rapidly outpace researchers’ abilities to process and analyze them. Recent advances in machine learning enable fast, sophisticated analysis of visual data, but have had limited success in the ocean due to lack of data set standardization, insufficient formatting, and aggregation of existing, expertly curated imagery for use by data scientists. To address this need, we have built FathomNet, a public platform that makes use of existing, expertly curated data. Initial efforts have leveraged MBARI’s Video Annotation and Reference System and annotated deep sea video database, which has more than 7M annotations, 1M frame grabs, and 5k terms in the knowledgebase, with additional contributions by National Geographic Society (NGS) and NOAA’s Office of Ocean Exploration and Research. FathomNet has over 160k localizations of 1.4k midwater and benthic classes, and contains more than 70k iconic and non-iconic views of marine animals, underwater equipment, debris, etc. We demonstrate how machine learning models trained on FathomNet data can be applied across different institutional video data, and enable automated acquisition and tracking of midwater animals using a remotely operated vehicle. As FathomNet continues to develop and incorporate more image data from other oceanographic community members, this effort will enable scientists, explorers, policymakers, storytellers, and the public to understand and care for our ocean. The Ocean is Earth’s final frontier Exploration and discovery in the ocean are crucial for sustainable management of this resource, and is vital for fisheries, mineral extraction, pharmaceutical development, and recreation. With the role that the ocean plays in modulating the Earth’s climate, and the role that its inhabitants play in carbon sequestration, accessing and understanding the life that resides within the ocean is critical. Exploring a space as vast as the ocean1, filled with life that we have yet to describe2 and numerous chemical and physical processes that we are only beginning to understand, using traditional, resource-intensive (e.g., time, person-hours, cost) sampling methodologies are limited in their ability to scale in spatiotemporal resolution and engage diverse communities3. However, with the advent of modern robotics4, low-cost observation platforms, and distributed sensing,5, 6 we are beginning to see a paradigm shift in ocean exploration and discovery. This shift is evidenced in satellite remote sensing of near-surface ocean conditions and in the global ARGO float array, where distributed platforms and open data structures are propelling the chemical and remote sensing communities to new scales of observation7–9. Due to a variety of constraints, large-scale sampling of biological communities or processes below the surface waters of the ocean has largely lagged behind. Three common modalities for observing biology and biological processes in the ocean include acoustics, -omics, and imaging, and each sensing modality has their strengths and weaknesses. Acoustics allow for observations of populationand group-scale dynamics, however individual-scale observations, especially determination of animals down to lower taxonomic groups like species, are challenging tasks. Omics, particularly the promising field of eDNA, allows for identification of biological communities based on their shed DNA in the water column. While eDNA studies provide broad-scale views of biological communities with only a few discrete samples, knowing the spatial source of those DNA, how measurements relate to population sizes, and presence of confounding non-marine biological markers in samples are active areas of research that still need to be addressed. On the other hand, imaging enables the identification of animals to the species level, elucidates community structure and spatial relationships in a variety of habitats, and reveals fine-scale behavior of animal groups. However, processing visual data, particularly data with complex scenes and organisms that require expert classifications, is a resource-intensive process that cannot be scaled without significant advances in automation. Observing life in the ocean using imaging Despite the resource-intensive nature of analysing visual data, underwater imaging continues to be widely used in the marine science and technology communities. Common imaging techniques involve the use of high-definition video cameras, microscopy, macro-scale, stereo, volumetric, and multi-spectral camera systems with illumination provided by single-band or wide-band lights, LEDs, and lasers10, 11. Underwater surveys and counts of animals can be accomplished in a variety of environments due to the ease by which imaging technology can be deployed, and the number of remotely controlled and autonomous platforms that can be used12. Underwater imaging is also being used in real-time for underwater vehicle navigation and control while performing difficult tasks in complex environments. Imaging has also been used as an effective engagement tool to share marine life and the issues facing the ocean with broader communities. These efforts have resulted in quantifiable changes in public opinion, and in building more ocean-aware communities13. The value of imaging to convey understanding of ocean life is without doubt. Given all these applications of underwater imaging, a number of annotation tools have been developed to deal with the challenges associated with managing and analyzing visual data. These efforts have resulted in many capable software solutions that can either be deployed on your computer locally, in the field, or on the worldwide web. However, given the limited availability of experts, prohibitive costs to annotate footage, and the expansion of imaging as an engagement tool and sensing modality, novel methods for automated annotation of underwater visual data are desperately needed. This need is motivating deployment of artificial intelligence and data science tools in the ocean realm. Automating analysis of visual data using artificial intelligence Artificial intelligence (AI) is a broad term that encompasses advanced statistical methods, machine learning, supervised and unsupervised learning, and deep learning or convolutional neural networks (CNNs)14, 15. Statistical methods that include random forests have been successfully used in the plankton imaging community, achieving automated classification of microscale plants and animals with accuracies greater than 90%16, 17. Unsupervised learning has been used to identify regions or moments of interest in underwater video footage during vehicle deployments and post-processing annotation tasks. Although unsupervised learning can be deployed with minimal data and a priori knowledge of underwater scenes, these algorithms have limited application to automating the detection and classification of objects in underwater imagery with sufficient granularity and detail to be used for annotation. Supervised learning, or CNNs that are trained on visual data where all objects have been identified (Figure 1), has improved performance of automated annotation and classification tasks to the genus and species level. Given the potential for CNNs to automate the annotation and classification of animals to lower taxonomic levels, the underwater imaging community has called out the need for publicly available, large-scale image training sets. Currently, underwater image training",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b2fd99c0e8052e0b389fce6c4e15690b75016af2,https://www.semanticscholar.org/paper/b2fd99c0e8052e0b389fce6c4e15690b75016af2,Applications of artificial neural networks in health care organizational decision-making: A scoping review,"Health care organizations are leveraging machine-learning techniques, such as artificial neural networks (ANN), to improve delivery of care at a reduced cost. Applications of ANN to diagnosis are well-known; however, ANN are increasingly used to inform health care management decisions. We provide a seminal review of the applications of ANN to health care organizational decision-making. We screened 3,397 articles from six databases with coverage of Health Administration, Computer Science and Business Administration. We extracted study characteristics, aim, methodology and context (including level of analysis) from 80 articles meeting inclusion criteria. Articles were published from 1997–2018 and originated from 24 countries, with a plurality of papers (26 articles) published by authors from the United States. Types of ANN used included ANN (36 articles), feed-forward networks (25 articles), or hybrid models (23 articles); reported accuracy varied from 50% to 100%. The majority of ANN informed decision-making at the micro level (61 articles), between patients and health care providers. Fewer ANN were deployed for intra-organizational (meso- level, 29 articles) and system, policy or inter-organizational (macro- level, 10 articles) decision-making. Our review identifies key characteristics and drivers for market uptake of ANN for health care organizational decision-making to guide further adoption of this technique.",PloS one,2019,10.1371/journal.pone.0212356,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
602e5c62e76a0a187950e58b5a98152537c65a9a,https://www.semanticscholar.org/paper/602e5c62e76a0a187950e58b5a98152537c65a9a,Crossing the chasm from model performance to clinical impact: the need to improve implementation and evaluation of AI,,npj Digital Medicine,2022,10.1038/s41746-022-00572-2,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
24fe1dc1af0e6b041d432cd32f3f5a6eab789969,https://www.semanticscholar.org/paper/24fe1dc1af0e6b041d432cd32f3f5a6eab789969,PERO2 Machine Teaching based on a Normalized Ontological Knowledge Base,"In order to extend the deployment of Machines Teaching in educational institutions, by facilitating their appropriation by human learners (teachers and students) and to explore the motivation of interactive Machines Teaching to solve problems by making a human as an indispensable pillar of the teaching process. This research framework is part of PERO2 project (intelligent system for learning of reasoning and problem-solving dedicated to the physical science domain specifically the teaching subdomain “Electricity”) which is about the integration of a semantic layer within the architecture of PERO2 this by means of representing the system’s knowledge base via a normalized domain ontology and then integrating the exploitable ontological knowledge base instead of a relational database. To design this ontological knowledge base, we proposed in the current research work a hybrid construction method taking into account two main phases: (*) Conception phase of our domain ontology called “OntoPhyScEx”. (**) Normalization also called Semantic validation phase of this domain ontology. As for the integration and exploitation of this domain ontology, it‘s been discussed in another paper. Keywords—Domain Ontology, Normalization, Machines Teaching, Knowledge Base, Learning of Reasoning, ProblemSolving.",,2017,10.26438/IJSRCSE/V5I5.6374,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
25c0847681cbb83b403d4798470d3792c6b854d0,https://www.semanticscholar.org/paper/25c0847681cbb83b403d4798470d3792c6b854d0,Learning from Five-year Resource-Utilization Data of Titan System,"Titan was the flagship supercomputer at the Oak Ridge Leadership Computing Facility (OLCF). It was deployed in late 2012, became the fastest supercomputer in the world and was retired on August 2, 2019. With Titan’s mission complete, this paper provides a first-order examination of the usage of its critical resources (CPU, Memory, GPU, and I/O) over a five-year production period (2015–2019). In particular, we show quantitatively that the majority of CPU time was spent on the large-scale jobs, which is consistent with the policy of driving ground-breaking science through leadership computing. We also corroborate the general observation of the low CPU-memory usage with 95% jobs utilizing only 15% or less available memory. Additionally, we correlate the increase of total job submissions and the decrease of GPU-enabled jobs during 2016 with the GPU reliability issue which impacted the large-scale runs. We further show the surprising read/write ratio over the five-year period, which contradicts the general mindset of the large-scale simulation machines being “write-heavy”. This understanding will have potential impact on how we design our next-generation large-scale storage systems. We believe that our analyses and findings are going to be of great interest to the high-performance computing (HPC) community at large.",2019 IEEE International Conference on Cluster Computing (CLUSTER),2019,10.1109/CLUSTER.2019.8891001,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d083058260c54c9fc3371585a75fc8d7514f9dc2,https://www.semanticscholar.org/paper/d083058260c54c9fc3371585a75fc8d7514f9dc2,Digitizing DILI: Who can? RUCAM? RECAM?,"The age of machine learning (ML) and artificial intelligence (AI) is poised to dramatically alter the practice of medicine in many disciplines, including the field of liver disease. How the radiologist diagnoses hepatic tumors and how the pathologist interprets liver histology are among the areas in which the digital revolution is being incorporated in gastroenterology and hepatology. However, as others have opined, several challenges in the technology involved in deploying these various ML/AI applications remain to be overcome.[1,2] Attempts to computerize the diagnosis of druginduced liver injury (DILI) have been on the minds of hepatologists for years but have thus far remained elusive. In this issue of Hepatology, Hayashi and colleagues,[3] all of whom have longstanding expertise in the causality assessment of DILI, present their evidencebased, computerized modification of the Roussel Uclaf Causality Assessment Methodology (RUCAM), which they have renamed Revised Electronic Causality Assessment Method (RECAM), for diagnosing DILI. Their attempt to bring causality assessment into the digital age is a welcome addition to our diagnostic armamentarium. But although their offering has several advantages over the current version of RUCAM, as the authors themselves recognize, as with other attempts to enter the computerized age of medicine, it remains a work in progress. Given the hundreds of drugs, herbal products, and chemicals that can cause liver injury and mimic every possible form of acute, chronic, benign, and malignant liver disease, the ability to make a firm diagnosis of DILI has remained clinically challenging in the absence of a definitive diagnostic biomarker. Many health care professionals, especially nonhepatologists, rely heavily on consultation with an individual with expertise in the field of drug hepatotoxicity. But despite more than three decades of research searching for a validated and specific biomarker, DILI remains a diagnosis of exclusion.[4] Efforts to devise semiobjective algorithmic approaches to analyze the important elements needed to suspect possible DILI— i.e., a compatible time to onset (latency), time to recovery after the drug has been stopped (positive dechallenge), and the ability to adequately exclude alternative causes— have given rise to a number of causality assessment methodologies over the last several decades. The algorithm that has emerged as the most specific and widely used to diagnose DILI was the result of a consensus meeting of a small group of invited hepatologists with expertise in DILI held in Paris, France, hosted by Drs. Benichou and Danan, and sponsored by Roussel Uclaf Pharmaceuticals under the auspices of the Council for International Organizations of Medical Science in the late 1980s and early 1990s. Their efforts to objectify the diagnostic elements of DILI resulted in what has come to be known as the Roussel Uclaf Causality Assessment Method or RUCAM.[5] Although a RUCAM assessment involves a relatively straightforward approach, each of its seven criteria requires a certain degree of expertise in interpreting the data. Indeed, its interrater reliability has been found to be low.[6] Importantly, although the initial RUCAM was validated by a positive rechallenge response,[7] readministering a drug that may have produced an initial severe hepatotoxic reaction is generally no longer recommended or performed because the response to rechallenge could result in an even more serious reaction— including acute liver failure.[4] Given the limitations and difficulties in scoring based on its relative subjectivity,[6,8] RUCAM was sparsely used in the United States as compared with Europe and other countries during its first two decades. It did not go unnoticed that none of the invited experts at the original RUCAM conferences allowed themselves to be coauthors of the seminal manuscripts, including Drs. Hyman Zimmerman and Willis Maddrey, who felt that the original RUCAM was “not yet ready for prime time.”[9] Although many of the individual components that comprise RUCAM remain the fundamental criteria for suspecting and diagnosing DILI, attributing causality using a RUCAM score alone is still considered an imperfect methodology.[8,10,11] Most notably, although members of the US DILI Network perform their causality assessment using many of the RUCAM criteria, their final assessment relies heavily on an expert opinion process that allows these investigators to overcome many of the shortcomings inherent in RUCAM.[6] In 2015, Danan and Teschke offered a revised and updated version of RUCAM that attempted to address a number of areas of uncertainly (such as whether alcohol, pregnancy, or age >55 years should be used to increase the RUCAM score).[12] However, despite RUCAM being increasingly Received: 3 January 2022 | Accepted: 3 January 2022",Hepatology,2022,10.1002/hep.32312,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9c1e79d8633399ac07dca5a87d12f2785c244336,https://www.semanticscholar.org/paper/9c1e79d8633399ac07dca5a87d12f2785c244336,ADAPTIVE PID-LIKE CONTROL USING BROAD LEARNING SYSTEM FOR NONLINEAR DYNAMIC SYSTEMS,"This paper presents a new learning control structure using broad learning system (BLS) for adaptive PID-like control of unknown digital nonlinear dynamic systems with time delays. The proposed control method, abbreviated as BLS-APIDLC, is novel in combining BLS and model predictive control to develop a new PID-like control law for high-performance setpoint tracking control and disturbance rejection. Comparative simulations on two renowned nonlinear digital time-delay systems are well used to show the effectiveness and superiority of the proposed method by comparing to four existing methods. INTRODUCTION In the past decades, conventional proportional-integralderivative (PID) controllers have gained widespread use in numerous control applications due to its simplicity of design and efficiency in the industrial applications (Astrom and Hagglund, 1995; Silva et al., 2005; O’Dwyer, 2009; Vilanova and Visioli, 2012). Although the PID controller has only three parameters to be tuned, it is surprisingly difficult to find the right tuning for them without systematic procedures. As such, the tuning of the PID gains is always a challenge in the state of the art of PID controller design. In other words, the main problem with a PID controller is the fact that the parameters of the PID controller must be adjusted properly to meet desired performance. This problem becomes more important when considering issues that include stability and control performance. Recently, the area of adaptive PID control and its related control approaches have still been developed by researchers (Oliveira and Lemos, 2000; Pan et al., 2007; Fahmy et al., 2014; Yang et al., 2015). Adaptive PID controller designs with controller parameters updated online by the neural network models were also presented. Pan et al. (2007) developed a two-layer supervised control method for tuning PID controller parameters based on model parameters estimated by the lazy learning technique. Fahmy et al. (2014) proposed an adaptive PID controller using the recursive least square algorithm which updates the PID gains automatically online to force the actual system to behave like a desired reference model. Oliveira and Lemos (2000) proposed a comparison of some fuzzy-modelbased adaptive-predictive control strategies. Yang et al. (2015) presented an adaptive predictive control strategy based on Laguerre functions in the chopper cascade control system, and examined by experiments. More recently, machine learning algorithms have made significant progress, especially deep learning technologies that have been made in wide areas (Tsai et al., 2014; Rosa and Yu, 2016; Ghazia et al., 2017; Andò et al., 2018; Y. Kang et al., 2019). By successively adjusting the weights between neurons over many input-output pairs, the function computed by the network is refined over time so that it provides more accurate predictions. The lately presented broad learning system (BLS) is an emerging way for efficient and effective modeling of complex systems (Chen and Liu, 2018; Jin and Chen, 2018; Chen et al., 2019). Chen and Liu (2018) developed a very fast and efficient BLS based on the random vector functional-link neural networks (RVFLNN) (Pao et al., 1994) to offer an alternative way for deep learning and structure. The designed model can be expanded in wide fashion when new feature nodes and enhancement nodes are needed. Moreover, the corresponding incremental learning algorithm is also designed. The BLS offers an alternative to deep learning because it has a fast and broad expansion without the need for retraining through incremental learning. The input signals are passed to the mapped feature layer and then Paper submitted 01/12/20; revised 03/25/20; accepted 05/03/20. Author for correspondence: Ching-Chih Tsai (e-mail: cctsai@nchu.edu.tw). 1 Department of Electrical Engineering, National Chung Hsing University, Taichung, Taiwan. 2 Department of Electronic Engineering, Hsiuping University of Science and Technology, Taichung, Taiwan. 358 Journal of Marine Science and Technology, Vol. 28, No. 5 (2020) passed to the enhancement layer via a nonlinear transformation. Although NNs possess good function approximation capabilities for nonlinear dynamic systems, the training process is time-consuming. On the other hand, the BLS system has been shown to preserve good function approximation capabilities and has been illustrated the feasibility and benefits of BLS-based control techniques for identification and control of nonlinear dynamic systems (Chen and Liu, 2018; Jin and Chen, 2018; Vong et al., 2020; Xu et al., 2018; and Feng and Chen, 2018a; Chen et al., 2019). Conventional PID controllers have been regarded as the simplest and the most deployed controller in industry. To extend the robustness and adaptability of the conventional PID controller, by integrating the simplicity and effectiveness of the conventional PID controller and the learning and automatic adjustment capabilities of the intelligent control strategy based on the PID-like controller for the nonlinear dynamic system have been proposed (Wang et al., 1997; Tsai et al., 2005; Cong and Liang, 2009; Fu and Chai, 2011). For example, Wang et al. (1997) presented an adaptive PID-like controller using a Modified Neural Network (MNN) for learning the characteristics of a dynamic system. Tsai et al. (2005) proposed an adaptive PID-Like fuzzy-neural controller and applied to the nonlinear model reference control system. Fu and Chai (2011) presented a robust self-tuning PID-like controller by combining a pole assignment selftuning PID controller with a filter. Cong and Liang (2009) proposed a PID-like neural network nonlinear adaptive controller for motion control systems by using a mix locally recurrent neural network. The gradient descent method is used for online adjustment and the initial PID parameters are needed which can operate the closed-loop stably. Kumar et al. (2014) proposed a hybrid neural network-based PID like adaptive controller for precise position control of a permanent-magnet synchronous motor (PMSM) servo drive. So far, many adaptive PID control for industrial applications have been proposed (Tung, 2012; Tsai et al., 2017; Tsai et al., 2019). Feng and Chen (2018b) presented a PID-like control method using BLS; however, this kind of PID-like control method was limited to a class of nonlinear dynamic systems without time-delays. Inspired by the above-mentioned surveys, the objective of this paper is to propose a BLS-based adaptive predictive PIDlike control, called BLS-APIDLC, of a class of unknown nonlinear discrete-time time-delay systems not only for disturbance rejection but also for precise tracking and guaranteed stability. The presented contents of the paper are written in two principal contributions. One is the theoretical derivation and proof of a more general adaptive PID-like control approach using BLS for unknown nonlinear time-delay dynamic systems by comparing to the result (Feng and Chen, 2018b), and the other is comparative investigation of the proposed BLS-APIDLC in comparison with four existing adaptive PID control methods. Fig. 1. Topological structure of the used BLS model. The rest of this paper is organized as follows. The basic ideas of the BLS and BLS identifier design are described in Sections II and III, respectively. Section IV is devoted to proposing a BLS-APIDLC control law, investigating its closed-loop stability and iterative control algorithm. Section V uses computer simulations to explore the effectiveness and superiority of the proposed BLS-APIDLC method for two illustrious nonlinear time-delay systems. Section VI is finished with the conclusions and future work of the paper. II. BROAD LEARNING SYSTEM (BLS)",,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
eb563d06871b29b36751df0725dd159acc602617,https://www.semanticscholar.org/paper/eb563d06871b29b36751df0725dd159acc602617,Learning by playing – using computer games to enhance ultrasound education,"In ultrasound education, there is often a lack of hands-on experience due to the limited availability of experienced tutors, patients, volunteers, and ultrasound machines. Simulators can substitute training with real humans to some extent, but they are not widely used, mainly because of their costs. Playing a computer game might be a way to add to the training opportunities in ultrasound education. An example for such a game is the SonoGame that has been developed as part of a medical imaging research project with the help of computer science students from the Flensburg University of Applied Sciences in Germany. This simulation-based game requires only a standard computer and off-the-shelf gaming hardware, thus making it widely deployable.",Ultraschall in der Medizin - European Journal of Ultrasound,2019,10.1055/a-0833-9381,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f03d4592a23a98cbaf5f9b12922fcbece9feb90d,https://www.semanticscholar.org/paper/f03d4592a23a98cbaf5f9b12922fcbece9feb90d,Computer vision model for detecting block falls at the martian north polar region.,"<p>Dynamic changes of Martian north polar scarps present a valuable insight into the planet's natural climate cycles (Byrne, 2009; Head et al., 2003)<sup>1,2</sup>. Annual avalanches and block falls are amongst the most noticeable surface processes that can be directly linked with the extent of the latter dynamics (Fanara et al, 2020)<sup>3</sup>. New remote sensing approaches based on machine learning allow us to make precise records of the aforementioned mass wasting activity by automatically extracting and analyzing bulk information obtained from satellite imagery.&#160; Previous studies have concluded that a Support Vector Machine (SVM) classifier trained using Histograms of Oriented Gradients (HOG) can be used to efficiently detect block falls, even against backgrounds with increased complexity (Fanara et al., 2020)<sup>4</sup>. We hypothesise that this pretrained model can now be utilized to generate an extended dataset of labelled image data, sufficient in size to opt for a deep learning approach. On top of improving the detection model we also attempt to address the image co-registration protocol. Prior research has suggested this to be a substantial bottleneck, which reduces the amounts of suitable images. We plan to overcome these limitations either by extending our model to include multi-sensor data, or by deploying improved methods designed for exclusively optical data (e.g.&#160; COSI-CORR software (Ayoub, Leprince and Avouac, 2017)<sup>5</sup>).&#160; The resulting algorithm should be a robust solution capable of improving on the already established baselines of 75.1% and 8.5% for TPR and FDR respectively (Fanara et al., 2020)4. The NPLD is our primary area of interest due to it&#8217;s high levels of activity and good satellite image density, yet we also plan to apply our pipeline to different surface changes and Martian regions as well as on other celestial objects.</p><p>&#160;</p><p>1. Head, J.W., Mustard, J.F., Kreslavsky, M.A., Milliken, R.E., Marchant, D.R., 2003. Recent ice ages on Mars. Nature 426, 797&#8211;802</p><p>2. Byrne, S., 2009. The polar deposits of Mars. Annu. Rev. Earth Planet. Sci. 37, 535&#8211;560.</p><p>3. Fanara, K. Gwinner, E. Hauber, J. Oberst, Present-day erosion rate of north polar scarps on Mars due to active mass wasting; Icarus,Volume 342, 2020; 113434, ISSN 0019-1035.</p><p>4. Fanara, K. Gwinner, E. Hauber, J. Oberst, Automated detection of block falls in the north polar region of Mars; Planetary and Space Science, Volume 180, 2020; 104733, ISSN 0032-0633.</p><p>5. Ayoub, F.; Leprince, S.; Avouac, J.-P. User&#8217;s Guide to COSI-CORR Co-registration of Optically Sensed Images and Correlation; California Institute of Technology: Pasadena, CA, USA, 2009; pp. 1&#8211;49.</p>",,2021,10.5194/EGUSPHERE-EGU21-15776,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f2ff46f13f6707be0295e1d6f6d00df9f3d4ce86,https://www.semanticscholar.org/paper/f2ff46f13f6707be0295e1d6f6d00df9f3d4ce86,Ethics and Creativity in Computer Vision,"This paper offers a retrospective of what we learnt from organizing the workshop Ethical Considerations in Creative applications of Computer Vision at CVPR 2021 conference and, prior to that, a series of workshops on Computer Vision for Fashion, Art and Design at ECCV 2018, ICCV 2019, and CVPR 2020. We hope this reflection will bring artists and machine learning researchers into conversation around the ethical and social dimensions of creative applications of computer vision. 1 CVFAD: Computer Vision for Fashion, Art and Design In 2018, we organized, in conjuction with ECCV, the first workshop on Computer Vision for Fashion, Art and Design. The workshop concentrated on generating, analyzing and processing visual content and invited the computer vision community to use generative model as a tool. As part of this workshop, we organized the Fashion-Gen [4] challenge for language to visual fashion design. The workshop also included a Computer Vision Art Gallery, organized by Luba Elliot and Xavier Snelgrove, to reflect the growing community of digital artists. Overall, the workshop brought together researchers in computer vision, artists and professionals from creative domains to discuss open problems in the areas of computer vision for fashion and creative visual content generation. Our ICCV 2019 and CVPR 2020 workshops broadened the scope of focus to include economical and industrial applications of creative computer vision tools. We hosted two fashion oriented challenges, FashionIQ [8] on multimodal fashion image retrieval [8] by Wu et al and DeepFashion2 [1] on a variety of fashion and clothing tasks such as fashion landmark detection by Ge et al. In ICCV 2019, Adriana Kovashka [7] brought up discussions on biases in creative advertisement content creation, and in political campaigns. We also had discussions during the panel discussions on potential harms that can be created by lack of representation in fashion industry and datasets. 2 EC3V: Ethical Considerations in Creative Applications of Computer Vision In 2021, we organized, in conjunction with CVPR, the first workshop on Ethical Considerations in Creative applications of Computer Vision (E3CV) . This workshop built upon 3 years of prior workshop organizing experience — 4 creativity-oriented challenges, as well as a successful Creativity in AI workshop series at NeurIPS and multiple generative art symposiums at the intersection of machine learning research and fashion industry — and oriented our focus around the under-explored ethical dimensions of creative computer vision work. With this workshop, we brought together a team of computer vision researchers, artists, and sociotechnical researchers to address growing number of questions on broader impact of this research. At a high level, the workshop focused on (a) the recognition of creative computer vision technologies as a new form of art , (b) influence of these technologies on society and representations [2] and (c) the areas that requires greater attention and discussion, and can create potential harms [3, 5]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. We also hosted the Computer Vision Art Gallery this year to bring more artists as one of the major stakeholders to the discussion. The computer vision art gallery showcased the work of 60+ artists addressing a range of topics and leveraging range of computer vision methods. For example, Jake Elwes’s work ‘Zizi and Me’ 11 showcased a double act between drag queen Me and a deep fake clone of Me. In doing so, the work aims to both demystify AI and queer the process of AI development. Nouf Aljowaysir’s project, ‘Salaf’, leveraged generative techniques to make visible the patterns of inclusion and exclusion operative in AI systems and express her personal frustrations with the Western colonial gaze so frequently embedded within the systems in use today. Many fruitful conversations came out of the workshop including the oral presentation 14 by Prabhu and Isiain [3] critiquing our implementation of art gallery and submission process. Two main anchors that covered major conversations during our breakout sessions were around cultural appropriation, and ownership as well as issues integrated in training data. Cultural appropriation vs inspiration Algorithmic techniques offer new routes for adopting and transferring aesthetic styles in ways that can be beautiful and creative and even shed light on cultures and art that individuals might not otherwise engage with. However, these same tools risk enabling new forms of cultural appropriation as they can make it even easier to extract from marginalized cultures without any accompanying investment in that culture, understanding of the significance of the artefact or aesthetics, or meaningful engagement with or say from the community. Cultural appropriation [2], distributed art and eventually generative art, all raise once again the question of intellectual property. If a traditional African pattern is re-contextualised in western fashion, is it fair to share the profit? If an artwork is the result of millions of people interacting with a website, should everyone get a fraction of the credit? The question becomes more complex when considering techniques based on deep learning, where a model is trained by exposure to thousands of images. These models can later be deployed by artists as aid to the creative process. How much credit goes to the ideator of the algorithm in this case? And how much to the creators of the content that was used to create the content? While ethical considerations for what concerns the work of a specific artist are starting to be discussed in depth, the aspect of broader cultural appropriation is still relatively unexplored. Fundamental challenges arise when trying to define ownership and copyrights in the context of Traditional Cultural Expression, where the intellectual contribution can’t be attributed to a single individual, but results from, and often defines, the cultural evolution of specific groups of people. Generative art, training data: a source of inspiration or memorization? The ever-increasing ability of AI models to generate very realistic images introduces new challenges. AI-generated content can be carefully tailored to specific generations, creating (supposedly) new content that resembles the training data. Considerations related to the training data are: biases inherently present in the data, memorization of training data, and insufficient transparency around some dataset creation processes [5]. When using models trained on specific datasets to generate art, any bias present in the training data is unavoidably learned by the model and revealed in its generations [6]. Many contemporary artists have begun to engage with ML researchers to find these biases. Some have done this for the visual aesthetics that the techniques begin to allow, others engage with them more critically in order to understand and reveal the algorithmic processes that are beginning to have great social and political power. These groups often notice the social dimension of the algorithms which can be overlooked by the computer science community.",ArXiv,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
13f3a25d4b81f06f99d1352ad0a0b8ab87228279,https://www.semanticscholar.org/paper/13f3a25d4b81f06f99d1352ad0a0b8ab87228279,Classification of Optical Transients at the MeerLICHT Telescope using Deep Learning,"Astronomers require efficient automated detection and classification pipelines when conducting large-scale surveys of the optical sky. Such pipelines are fundamentally important as they permit rapid follow-up and analysis of those detections most likely to be of scientific value. We present a deep learning framework based on a convolutional neural network model known as MeerCRAB. It is designed to filter out the so called “bogus” detections from true astrophysical sources in the transient detection pipeline of the MeerLICHT telescope. Optical candidates are described using a variety of 2D images and numerical features extracted from those images. The relationship between the input images and the target classes is unclear, since the ground truth is poorly defined and often the subject of debate. This makes it difficult to determine which source of information should be used to train a classification algorithm. To proceed we deployed variants of MeerCRAB that employed different network architectures trained upon different combinations of input images and different training set choices based on volunteer’s classification labels. We found the deepest network worked best with an accuracy of 99.2% and Matthews correlation coefficient (MCC) value of 0.984. The best model is integrated in the MeerLICHT transient vetting pipeline, hence providing a contextual classification of detected transients that allows researchers to select the most promising candidates for their research goals. ∗https://zafiirah13.github.io/zafiirah-hosenie/ Third Workshop on Machine Learning and the Physical Sciences (NeurIPS 2020), Vancouver, Canada. (a) MeerVETTING web-interface. Atleast 5 Atleast 6 Atleast 7 Atleast 8 Atleast 9 Atleast 10 Number of Vetters 0 50",,2020,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7c4effd8bef2887a26f658bb56d5c1b69d9af674,https://www.semanticscholar.org/paper/7c4effd8bef2887a26f658bb56d5c1b69d9af674,Classification and Prediction of Erythemato-Squamous Diseases Through Tensor-Based Learning,,"Proceedings of the National Academy of Sciences, India Section A: Physical Sciences",2018,10.1007/S40010-018-0563-X,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cb5ef085fd6a1e3137772a4fb876d000ada297df,https://www.semanticscholar.org/paper/cb5ef085fd6a1e3137772a4fb876d000ada297df,Introduction to the Special Issue on Data Science for Next Generation Big Data,"The first age of Big Data started roughly ten years ago. It has had an enormous impact in many fields of science. It underlies the rapid development of data-driven applications and gives rise to many innovative data processing systems. Ten years on, Big Data is entering a new generation. In particular, data is being used at a much larger, global scale. Furthermore, there is a trend of multiple data owners coming together to perform collaborative data analytics, and many datadriven business decisions are made based on statistical analytics from multi-source, multimodal, and worldwide data. The new generation of Big Data opens the door for innovative data-driven applications that are not possible even in the early age of Big Data. However, the new scale, both in terms of the data and the number of participants, brings significant challenges ranging from secure data sharing to federated data analytics. At the same time, emerging technologies such as 5G, AI, and blockchains demand high-performance, scalable, and secure data management. It is therefore crucial to have new theories, algorithms, and systems, for future applications that make various trade-offs between security, performance, and data quality, in this new age of Big Data. This special issue aims to publish work on a variety of data science technologies including statistical theory, data management, data mining, and machine learning, which realize the potentials of next-generation Big Data. This special issue received five high-quality submissions, and three of them were accepted. The topics of the accepted articles are briefly introduced below. The article titled “Hierarchical Satellite System Graph for Approximate Nearest Neighbor Search on Big Data” presents a hierarchical method to build the Monotonic Search Networks to solve the approximate nearest neighbor search problem. The proposed index and search algorithms can be deployed in a distributed manner, effectively decreasing the search steps and reducing the computational cost over large-scale data. The article titled “Quantized Tensor Neural Network” introduces a quantized tensor neural network to improve the learning ability of the tensor network. The proposed method effectively integrates the high-order convolution operations into the non-linear tensor network to learn local features of high-dimensional data. A high-order error backpropagation algorithm is further developed to optimize the parameters in tensor networks. The proposed method can learn hidden features efficiently with fewer parameters on image classification tasks. The article titled “Differentially Private Deep Learning with Iterative Gradient Descent Optimization” studies the problem of privacy preservation with deep learning. It proposes a novel perturbed iterative gradient descent optimization algorithm which satisfies the differential privacy and achieves better model accuracy. A modified moments accountant method is introduced to get the tighter bound of privacy loss compared with the existing privacy accounting methods.",Trans. Data Sci.,2021,10.1145/3507467,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
70932f90e76f76027f9af08f357f1d1c1e106439,https://www.semanticscholar.org/paper/70932f90e76f76027f9af08f357f1d1c1e106439,A DATA-INTENSIVE APPROACH TO EXPLOIT NEW GNSS SCIENCE OPPORTUNITIES,"With the current GNSS infrastructure development plans, over 120 GNSS satellites (including European Galileo satellites)will provide, already this decade, continuous data, in several frequencies, without interruption and on a permanent basis.This global and permanent GNSS infrastructure constitutes a major opportunity for GNSS science applications. In themeantime, recent advances in technology have contributed ""de-facto"" to the deployment of a large GNSS receiver arraybased on Internet of Things (IoT), affordable smart devices easy to find in everybody’s pockets. These devices – evolvingfast at each new generation – feature an increasing number of capabilities and sensors able to collect a variety ofmeasurements, improving GNSS performance. Among these capabilities, Galileo dual band smartphones receivers andAndroid’s support for raw GNSS data recording represent major steps forward for Positioning, Navigation and Timing (PNT)data processing improvements. Information gathering from these devices, commonly referred as crowdsourcing, opensthe door to new data-intensive analysis techniques in many science domains. At this point, collaboration between variousresearch groups is essential to harness the potential hidden behind the large volumes of data generated by thiscyberinfrastructure. Cloud Computing technologies extend traditional computational boundaries, enabling execution ofprocessing components close to the data. This paradigm shift offers seamless execution of interactive algorithms andanalytics, skipping lengthy downloads and setups. The resulting scenario, defined by a GNSS Big Data repository with colocatedprocessing capabilities, sets an excellent basis for the application of Artificial Intelligence / Machine Learning (ML)technologies in the context of GNSS. This unique opportunity for science has been recognized by the European SpaceAgency (ESA) with the creation of the Navigation Scientific Office, which leverages on GNSS infrastructure to deliverinnovative solutions across multiple scientific domains.",Proceedings - 3rd Congress in Geomatics Engineering - CIGeo,2021,10.4995/cigeo2021.2021.12740,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1f2f6216100b3abf664a1f1385ad957e3f46efd7,https://www.semanticscholar.org/paper/1f2f6216100b3abf664a1f1385ad957e3f46efd7,Top 10 Read Article in Computer Science & Information Technology: June 2021,"Clouds provide a powerful computing platform that enables individuals and organizations to perform variety levels of tasks such as: use of online storage space, adoption of business applications, development of customized computer software, and creation of a “realistic” network environment. In previous years, the number of people using cloud services has dramatically increased and lots of data has been stored in cloud computing environments. In the meantime, data breaches to cloud services are also increasing every year due to hackers who are always trying to exploit the security vulnerabilities of the architecture of cloud. In this paper, three cloud service models were compared; cloud security risks and threats were investigated based on the nature of the cloud service models. Real world cloud attacks were included to demonstrate the techniques that hackers used against cloud computing systems. In addition,countermeasures to cloud security breaches are presented. ABSTRACT Big Data triggered furthered an influx of research and prospective on concepts and processes pertaining previously to the Data Warehouse field. Some conclude that Data Warehouse as such will disappear; others present Big Data as the natural Data Warehouse evolution (perhaps without identifying a clear division between the two); and finally, some others pose a future of convergence, partially exploring the possible integration of both. In this paper, we revise the underlying technological features of Big Data and Data Warehouse, highlighting their differences and areas of convergence. Even when some differences exist, both technologies could (and should) be integrated because they both aim at the same purpose: data exploration and decision making support. We explore some convergence strategies, based on the common elements in both technologies. We present a revision of the state-of-the-art in integration proposals from the point of view of the purpose, methodology, architecture and underlying technology, highlighting the common elements that support both technologies that may serve as a starting point for full integration and we propose a proposal of integration between the two technologies. ABSTRACT By applying RapidMiner workflows has been processed a dataset originated from different data files, and containing information about the sales over three years of a large chain of retail stores. Subsequently, has been constructed a Deep Learning model performing a predictive algorithm suitable for sales forecasting. This model is based on artificial neural network –ANN- algorithm able to learn the model starting from sales historical data and by pre-processing the data. The best built model uses a multilayer neural network together with an “optimized operator” able to find automatically the best parameter setting of the implemented algorithm. In order to prove the best performing predictive model, other machine learning algorithms have been tested. The performance comparison has been performed between Support Vector Machine –SVM-, k-Nearest Neighbor k-NN-,Gradient Boosted Trees, Decision Trees, and Deep Learning algorithms. The comparison of the degree of correlation between real and predicted values, the average absolute error and the relative average error proved that ANN exhibited the best performance. The Gradient Boosted Trees approach represents an alternative approach having the second best performance. The case of study has been developed within the framework of an industry project oriented on the integration of high performance data mining models able to predict sales using–ERP- and customer relationship management –CRM- tools. ABSTRACT Wireless network implementation is a viable option for building network infrastructure in rural communities. Rural people lack network infrastructures for information services and socio-economic development. The aim of this study was to develop a wireless network infrastructure architecture for network services to rural dwellers. A user-centered approach was applied in the study and a wireless network infrastructure was designed and deployed to cover five rural locations. Data was collected and analyzed to assess the performance of the network facilities. The results shows that the system had been performing adequately without any downtime with an average of 200 users per month and the quality of service has remained high. The transmit/receive rate of 300Mbps was thrice as fast as the normal Ethernet transmit/receive specification with an average throughput of 1 Mbps. The multiple output/multiple input(MIMO) point-to-multipoint network design increased the network throughput and the quality of serviceexperienced by the users ABSTRACT Although initially designed for co-located teams, agile methodologies promise mitigation to the challenges present in distributed software development with their demand for frequent communication. We examinethe application of agile practices in software engineering teams with low geographical distribution in Austria and Germany. To gather insights on challenges and benefits faced by distributed teams we conductinterviews with eleven representatives and analyse the interview transcripts using the inductive category formation method. As a result, we identify four major challenges, such as technical obstructions or theimpediments different language abilities have on communication, and four benefits, regardingcollaboration and information radiation, that agile methods yield in distributed teams. Based on ouranalysis of challenges and benefits, we deduct seven recommendations to improve collaboration, overcomedistance and avoid pitfalls. Key recommendations for teams with low geographical distance include thatteams should get together at certain points to build relationships and trust and share information face-to- face ABSTRACT The world is moving forward at a fast pace, and the credit goes to ever growing technology. One such concept is IOT (Internet of things) with which automation is no longer a virtual reality. IOT connects various non-living objects through the internet and enables them to share information with their community network to automate processes for humans and makes their lives easier. The paper presents the future challenges of IoT , such as the technical (connectivity , compatibility and longevity , standards , intelligent analysis and actions , security), business ( investment , modest revenue model etc. ), societal (changing demands , new devices, expense, customer confidence etc. ) and legal challenges ( laws, regulations, procedures, policies etc. ). A section also discusses the various myths that might hamper the progress of IOT, security of data being the most critical factor of all. An optimistic approach to people in adopting the unfolding changes brought by IOT will also help in its growth ABSTRACT Mobile payment allows consumers to make more flexible payments through convenient mobile devices. While mobile payment is easy and time save, the operation and security of mobile payment must ensure that the payment is fast, convenient, reliable and safety in order to increase the users’ satisfaction. Therefore, this study based on technology acceptance model to explore the impact of external variables through perceived usefulness and perceived ease of use on users’ satisfaction. The data analysis methods used in this study are descriptive statistical analysis, reliability and validity analysis, Pearson correlation analysis and regression analysis to verify the hypotheses. The results show that all hypotheses are supported. However, mobile payment is still subject to many restrictions on development and there are limited related researches. The results of this study provided insight into the factors that affect the users’ satisfaction for mobile payment. Related services development of mobile payment and future research suggestions are also offered. ABSTRACT Big Data is used in decision making process to gain useful insights hidden in the data for business and engineering. At the same time it presents challenges in processing, cloud computing has helped in advancement of big data by providing computational, networking and storage capacity. This paper presents the review, opportunities and challenges of transforming big data using cloud computing resources. ABSTRACT In this paper, we propose a new traffic flow model of the Long Term Evaluation (LTE) network for the Evolved Universal Terrestrial Radio Access Network (E-UTRAN). Here only one Evolve Node B (eNB)nearest to the Mobility Management Entity (MME) and Serving Gateway (S-GW) will use the S1 link tobridge the E-UTRAN and Evolved Packet Core (EPC). All the eNBs of a tracking area will be connected toeach other by the X2 link. Determination of capacity of a links of such a network is a challenging job sinceeach node offers its own traffic and at the same time conveys traffic of other nodes. In this paper, we applymaximum flow algorithm including superposition theorem to solve the traffic flow of radio network. Usingthe total flow per subcarrier, a new traffic model is also developed in the paper. The relation among the traffic parameters: ‘blocking probability’, ‘offered traffic’, ‘instantaneous capacity’, ‘average holdingtime’, and ‘number of users’ are shown graphically under both QPSK and 16 -QAM. The concept of thenetwork will be helpful to improve the SINR of the received signal ofeNBslocated long distance relative to MME/S-GW. ABSTRACT The huge amount of healthcare data, coupled with the need for data analysis tools has made data mining interesting research areas. Data mining tools and techniques help to discover and understand hidden patterns in a dataset which may not be possible by mainly visualization of the data. Selecting appropriate clustering method and optimal number of clusters in healthcare data can be confusing and difficult most times. Presently, a large number of clustering algorithm",,2021,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fb828661ae456cca88bec6f84db68088ee19da74,https://www.semanticscholar.org/paper/fb828661ae456cca88bec6f84db68088ee19da74,"PhytoOracle: Scalable, modular phenomic data processing pipelines","Previous crop yield improvements have been largely due to the implementation of new management strategies, mechanization, and application of emerging technologies. While these approaches have led to stable, linear improvements, increases in crop yields are currently plateauing. The use and improvement of rapid, automated, and accurate phenomic selection methods leveraging high-resolution data collected throughout a growing season could help identify stress-adaptive traits to meet the growing global food demand. As the capacity of phenomics to generate larger and higher dimensional data sets improves, there is an urgent need to develop and implement robust and scalable data processing pipelines for rapid turnaround of processed results. Current phenomics processing pipelines lack modularity and the ability to exploit the distributed computational infrastructure required for machine learning (ML)-based workloads. To address these challenges, we developed PhytoOracle (PO), a suite of modular, scalable pipelines that aim to improve data processing efficiency for plant science research. PO integrates open-source frameworks for distributed task management on local, cloud, or high-performance computing (HPC) systems. Each pipeline component is available as a standalone container which can be independently deployed or linked into a pipeline. Additionally, researchers can swap between available containers or integrate new ones suited to their specific research. PO extracts phenotype trait values such as volume, height, canopy temperature, and maximum quantum efficiency (Fv/Fm) of photosystem II from data captured in field settings, enabling the study of phenotypic variation for elucidation of the genetic components of quantitative traits.",,2021.0,10.1002/essoar.10508789.1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8bbb471a737c1b7c0f3c93653d7b2c8474558e43,https://www.semanticscholar.org/paper/8bbb471a737c1b7c0f3c93653d7b2c8474558e43,Easing and promoting the application of ML and AI in earth system sciences - introducing the KI:STE platform,"<p>Earth system modeling is virtually impossible without dedicated data analysis. Typically, data are big and due to the complexity of the system, adequate tools for the analysis lie in the domain of machine learning or artificial intelligence. However, earth system specialists have other expertise than developing and deploying state-of-the art programming code which is needed to efficiently use modern software frameworks and computing resources. In addition, Cloud and HPC infrastructure are frequently needed to run analyses with data beyond Tera- or even Petascale volume, and corresponding requirements on available RAM, GPU and CPU sizes.&#160;</p><p>Inside the KI:STE project (www.kiste-project.de), we extend the concepts of an existing project, the Mantik-platform (www.mantik.ai), such that handling of data and algorithms is facilitated for earth system analyses while abstracting technical challenges such as scheduling and monitoring of training jobs and platform specific configurations away from the user.</p><p>The principles for design are collaboration and reproducibility of algorithms from the first data load to the deployment of a model to a cluster infrastructure. In addition to the executive part where code is developed and deployed, the KI:STE project develops a learning platform where dedicated topics in relation to earth system science are systematically and pedagogically presented.</p><p>In this presentation, we show the architecture and interfaces of the KI:STE platform together with a simple example.</p>",,2021.0,10.5194/egusphere-egu21-9632,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7902570bbee92baecd98e1cc4e94d00fb7e66dcb,https://www.semanticscholar.org/paper/7902570bbee92baecd98e1cc4e94d00fb7e66dcb,How to Train Your Robot,"We developed the How to Train Your Robot curriculum to empower middle school students to become conscientious users and creators of Artificial Intelligence (AI). As AI becomes more embedded in our daily lives, all members of society should have the opportunity to become AI literate. Today, most deployed work in K-12 AI education takes place at strong STEM schools or during extracurricular clubs. But, to promote equity in the field of AI, we must also design curricula for classroom use at schools with limited resources. How to Train Your Robot leverages a low-cost ($40) robot, a block-based programming platform, novice-friendly model creation tools, and hands-on activities to introduce students to machine learning. During the summer of 2020, we trained in-service teachers, primarily from Title 1 public schools, to deliver a five-day, online version of the curriculum to their students. In this work, we describe how students’ self-directed final projects demonstrate their understanding of technical and ethical AI concepts. Students successfully selected project ideas, taking the strengths and weaknesses of machine learning into account, and implemented an array of projects about everything from entertainment to science. We saw that students had the most difficulty designing mechanisms to respond to user feedback after deployment. We hope this work inspires future AI curricula that can be used in middle school classrooms.",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2d6e28921dbd171bd26281e139e0ba0718ac18df,https://www.semanticscholar.org/paper/2d6e28921dbd171bd26281e139e0ba0718ac18df,"FOG COMPUTING TECHNOLOGIES FOR PATIENT SENSOR NETWORKS – TRENDS, ISSUES AND FUTURE DIRECTIONS","Advances in sensors and internet of things promise broad opportunities in many areas and one of them is health care. There are many solutions to manage health care data based on cloud computing. However, high response latency, large volumes of data transferred and security are the main issues of such approach. Fog computing provides immediate response and ways to process large amounts of data using real time analytics which includes machine learning and AI. Fog computing has not yet fully matured and there are still many challenges when managing health care data. It was chosen to investigate the most relevant e­health fog computing topics by analyzing review articles to explain the fog computing model and present the current trends – fog computing e­health technology application environments, deployment cases, infrastructure technologies, data processing challenges, problems and future directions. 38 scientific review articles published in the last 5 years were selected for analysis, filtering the most significant works with Web of Science article search tool.",Mokslas - Lietuvos ateitis,2021.0,10.3846/mla.2021.15174,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
144db666376e8dbe2b17578bfc29603653769495,https://www.semanticscholar.org/paper/144db666376e8dbe2b17578bfc29603653769495,Towards Developing Better Object Detectors for Real-World Use,"Deep visual models are fast surpassing human-level performance for various vision tasks, including object detection, increasing their use in day-to-day life applications. It is often the case that standard models that perform well when evaluated on the validation dataset—usually collected from the same source as the training dataset, often perform poorly on data different from that of the training data. Recent works also prove that adversarial examples can easily fool deep learning models and are primarily opaque. To address the issue of making object detectors more compatible for real-world use, we propose some steps to make them more reliable and robust for deployment. Proposed methods include the explanation method and data augmentation techniques. Data augmentation improves the performance and outcomes of machine learning models by generalizing them and explanation methods for getting new insights into black-box detectors. Such understanding can also help improve resistance to a wide range of adversarial attacks. ACM Reference Format: Akshay Gupta and Biplav Srivastava. 2022. Towards Developing Better Object Detectors for Real-World Use. In 5th Joint International Conference on Data Science & Management of Data (9th ACM IKDD CODS and 27th COMAD) (CODS-COMAD 2022), January 8–10, 2022, Bangalore, India. ACM, New York, NY, USA, 2 pages. https://doi.org/10.1145/3493700.3493741",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cb0eb42a66a7c6ee5062253b9de4fb3a83baf17f,https://www.semanticscholar.org/paper/cb0eb42a66a7c6ee5062253b9de4fb3a83baf17f,Rhode Island COBRE Center for Central Nervous System Function: Progress and Perspectives.,"The Center of Biomedical Research Excellence (COBRE) Center for Central Nervous System Function (CCNSF) was funded in 2013 by the National Institute for General Medical Sciences to establish a collaborative environment for basic and applied research in higher nervous system function with humans and experimental animal model systems. Since its inception, the COBRE CCNSF has funded junior faculty investigators as Project and Pilot Project Leaders and one established investigator on projects investigating fundamental properties of nervous system function using a range of tools spanning molecular genetics, neurophysiology, invasive and non-invasive brain stimulation, behavior and neuroimaging. The Administrative Core facilitates all Center activities with a focus on career development, grant proposal submission, and deployment of technology developed by our research cores. The Design and Analysis Core aims to provide principled study design expertise, statistical modeling, machine learning, inference, and computation. The Behavior and Neuroimaging Core provides project-specific collaboration and support to COBRE scientists to promote the acquisition of high quality behavioral, physiological, neuroimaging and neurostimulation data, to ensure the integrity of the data collection infrastructure and to help implement robust data processing and visualization pipelines. While the cores principally serve Center scientists, our Center and the core resources have availability to all Rhode Island researchers.",Rhode Island medical journal,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c91785adc8c4295ab82054d910e7b7b4434117fe,https://www.semanticscholar.org/paper/c91785adc8c4295ab82054d910e7b7b4434117fe,QuasarNet: A new research platform for the data-driven investigation of black holes,"We present QuasarNet a novel research platform for the data-driven investigation of super-massive black hole (SMBH) populations. While SMBH data sets — observations and simulations — have grown rapidly in both complexity and abundance in the past two decades, our computational environments and analysis tools have not matured commensurately to exhaust opportunities for discovery. Motivated to explore the underlying nature of the BH host galaxy and parent dark matter halo connection - in this pilot version of QuasarNet we assemble and co-locate a large, high-redshift, luminous quasar population at z ≥ 3 alongside simulated data spanning the same cosmic epochs. We demonstrate the ease of use of QuasarNet to extract properties of observed quasars and their correlations. Leveraging machine learning algorithms (ML) and applying Normalizing Flows to expand simulation volumes for analysis, we show that halo properties extracted from smaller dark-matter only simulation boxes successfully replicate halo populations in larger boxes. Next, training the Random Forest ML algorithm on the Illustris-TNG300 simulation that includes baryonic physics, we populate the larger LEGACY Expanse dark matter-only box with quasars, and show that quasar occupation statistics are accurately replicated. Our ﬁrst science results comparing co-located observational and ML trained simulated data from QuasarNet at z ∼ 3, reveal that while the recovered Black Hole Mass Functions (BHMFs) and clustering are in good agreement, the accretion rates of simulated SMBHs with M • < 10 8 − 10 9 . 5 M (cid:12) are too low. ML clearly demonstrates that simulated SMBHs fail to accrete, shine and grow at high enough rates to match observed quasars by z ∼ 3. Simulations are observationally calibrated to reproduce the BHMF at z = 0, yet they diverge from the observational data even by modest redshifts of z ∼ 3. We conclude that sub-grid models of mass accretion and BH feedback implemented in Illustris-TNG300 do not reproduce the observed mass growth. QuasarNet demonstrates the power of ML, in analyzing and exploring large datasets, and also oﬀering a unique opportunity to interrogate the theoretical assumptions that underpin accretion and feedback models. Filling in current gaps at lower luminosities for observed quasars is critically needed for a more complete understanding of BH assembly. We deploy ML again to derive a new neighborhood statistic and devise an optimal survey strategy for bringing the lower luminosity quasar population into view. QuasarNet and all related materials are publicly available at the Google Kaggle platform. the statistics from the larger volume LEGACY (1 Gpc) 3 Expanse box. Finally, showcasing the combination of simulated and observational data at just one epoch z ∼ 3, we show how ML tools permit examining and interrogating assumptions that are integral to the modeling of the growth of SMBHs. In future work, we plan to develop additional ML tools in the next stage of our project that will allow us to further mimic larger simulated volumes that correspond to observational survey volumes and predict the putative locations for fainter as yet undetected quasars. Here we present the ﬁrst key step in building up and exploring the BH-galaxy-halo connection that underpins the formation and evolution of all structures including the non-baryonic (dark matter), baryonic (galaxies) and BHs.",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
744f8cc6614a0f1742ac7846f95dca5f4ea57eff,https://www.semanticscholar.org/paper/744f8cc6614a0f1742ac7846f95dca5f4ea57eff,NTD: Non-Transferability Enabled Backdoor Detection,"A backdoored deep learning (DL) model behaves normally upon clean inputs but misbehaves upon trigger inputs as the backdoor attacker desires, posing severe consequences to DL model deployments, particularly in security-sensitive applications such as face recognition and autonomous driving. To mitigate such newly revealed adversarial attacks, great efforts have been made. Nonetheless, state-of-the-art defenses are either limited to specific backdoor attacks (i.e., source-agnostic attacks) or non-user-friendly in that machine learning (ML) expertise and/or expensive computing resources are required. This work observes that all existing backdoor attacks have an inadvertent and inevitable intrinsic weakness, termed as non-transferability, that is, a trigger input hijacks a backdoored model but cannot be effective to an another model that has not been implanted with the same backdoor. With this key observation, we propose non-transferability enabled backdoor detection (NTD) to identify trigger inputs for a model-under-test (MUT) during run-time. Specifically, NTD allows a potentially backdoored MUT to predict a class for an input. In the meantime, NTD leverages a feature extractor (FE) to extract feature vectors for the input and a group of samples randomly picked from its predicted class, and then compares similarity between the input and the samples in the FE’s latent space. If the similarity is low, the input is an adversarial trigger input; otherwise, it is benign. The FE is a free pre-trained model privately reserved from open platforms (e.g., ModelZoo) by a user and thus NTD does not require any ML expertise or costly computations from the user. As the FE and MUT are from different sources—the former can indeed be provided by a reputable party, the attacker is very unlikely to insert the same backdoor into both of them. BeCorresponding author: Yansong Gao, yansong.gao@njust.edu.cn Equal contribution: Yinshan Li and Hua Ma Yinshan Li, Yansong Gao, and Anmin Fu are with NanJing University of Science and Technology, China. Hua Ma, Said F. Al-Sarawi, and Derek Abbott are with the University of Adelaide. Australia. Zhi Zhang, and Alsharif Abuadbba are with Data61, CSIRO, Australia. Yifeng Zheng is with Harbin Institute of Technology, Shenzhen, China. cause of non-transferability, a trigger effect that does work on the MUT cannot be transferred to the FE, making NTD effective against different types of backdoor attacks. We evaluate NTD on three popular customized tasks i.e., face recognition, traffic sign recognition and general animal classification, results of which affirm that NDT has high effectiveness (low false acceptance rate) and usability (low false rejection rate) with low detection latency.",ArXiv,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
36fa3a4d2138c9ec1830ae6f88f4b4ee03db9517,https://www.semanticscholar.org/paper/36fa3a4d2138c9ec1830ae6f88f4b4ee03db9517,Measures and Best Practices for Responsible AI,"The use of machine learning (ML) based systems has become ubiquitous including their usage in critical applications like medicine and assistive technologies. Therefore, it is important to determine the trustworthiness of these ML models and tasks. A key component in this determination is the development of task specific datasets, metrics, and best practices which are able to measure the various aspects of responsible model development and deployment including robustness, interpretability and fairness. Further, datasets are also key when training for a given task, be it coreference resolution in language modeling or facial recognition in computer vision. Imbalances and inadequate representation in datasets can have repercussions of an undesirable nature. Some common examples include how coreference resolution systems in NLU are often not all gender inclusive, discrepancies in the measurement of how robust and trustworthy machine predictions are in domains where the selective labels problem is prevalent, and discriminatory determination of pain or care levels of people belonging to different demographics in health science applications. Development of task specific datasets which do better in this regard is also extremely vital. In this workshop, we invite contributions towards different (i) datasets which help enhance task performance and inclusivity, (ii) measures and metrics which help in determining the trustworthiness of a model/dataset, (iii) assessment or remediation tools for fairer, more transparent, robust, and reliable models, and (iv) case studies describing responsible development and deployment of AI systems across fields such as healthcare, financial services, insurance, etc. The datasets, measures, mitigation techniques, and best practices could focus on different areas including (but not restricted to) the following: Fairness and Bias Robustness Reliability and Safety Interpretability Explainability Ethical AI Causal Inference Counterfactual Example Analysis They could also be focussed on the applications in diverse fields such as industry, finance, healthcare and beyond. Text based datasets can be in languages other than English as well.",KDD,2021.0,10.1145/3447548.3469458,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e14938a72c5c462c64ef2be24e31a937944a4069,https://www.semanticscholar.org/paper/e14938a72c5c462c64ef2be24e31a937944a4069,Preface,"CONference on Smart and Intelligent Learning for Information Optimization (CONSILIO)-2021 was organized by Sreyas Institute of Engineering & Technology, Hyderabad, Telangana, India in collaboration with Consilio Intelligence Research Lab, Noida, UP, India at SIET, Hyderabad, India during July 09-10, 2021. Consilio Intelligence Research Lab (CIRL) is a platform well known for IOT, Machine Learning, Artificial Intelligence and Cloud Computing Services. We add value to our clients by continually staying in touch with them, listening to them and in the process improving our offerings. Consilio Research Labs is a one stop organization that provides IOT, Machine Learning, AI and Cloud Computing based Services in Pan India. It provides services with full potential of pattern recognition, learning theory of computation, self-optimizing and natureinspired algorithms to the fullest advantage of its consumers, providing tailor-made (IOT, Machine Learning, Artificial Intelligence and Cloud Computing) services and solutions. CIRL also committed to develop the science of autonomy toward a future with Machine and AI systems integrated into everyday life, supporting people with cognitive and daily tasks. The world’s toughest problems extend to the far reaches of the globe. That’s why we’ve initiated in investing towards the world-class research facilities in collaboration where more than 1,000 researchers cross-pollinate ideas that lead to major breakthroughs. CIRL focuses on research that impacts science, and society at large. Our work is inter-disciplinary, and we blend theory with practice, and computer science with social sciences. We work not only on great ideas, but also implement, deploy, experiment, contribute to and learn from the world around us. We collaborate freely with academia, NGOs and startups, and also play an active role in mentoring the young researchers and technologists. SREYAS Institute, located in the heart of the Hyderabad city, is the result of deliberation & planning of every aspect to create a world class technical education institution. The campus is scientifically planned and artistically designed. The students have access to the latest software &computing facilities for learning &research to groom them into future citizens. List of Editors, Committee are available in this pdf.",Journal of Physics: Conference Series,2021.0,10.1088/1742-6596/1998/1/011001,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2b16b1ac0efe0a420306813d68b505d81b6bcb1f,https://www.semanticscholar.org/paper/2b16b1ac0efe0a420306813d68b505d81b6bcb1f,ENNGene: an Easy Neural Network model building tool for Genomics,"Background The recent big data revolution in Genomics, coupled with the emergence of Deep Learning as a set of powerful machine learning methods, has shifted the standard practices of machine learning for Genomics. Even though Deep Learning methods such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) are becoming widespread in Genomics, developing and training such models is outside the ability of most researchers in the field. Results Here we present ENNGene—Easy Neural Network model building tool for Genomics. This tool simplifies training of custom CNN or hybrid CNN-RNN models on genomic data via an easy-to-use Graphical User Interface. ENNGene allows multiple input branches, including sequence, evolutionary conservation, and secondary structure, and performs all the necessary preprocessing steps, allowing simple input such as genomic coordinates. The network architecture is selected and fully customized by the user, from the number and types of the layers to each layer's precise set-up. ENNGene then deals with all steps of training and evaluation of the model, exporting valuable metrics such as multi-class ROC and precision-recall curve plots or TensorBoard log files. To facilitate interpretation of the predicted results, we deploy Integrated Gradients, providing the user with a graphical representation of an attribution level of each input position. To showcase the usage of ENNGene, we train multiple models on the RBP24 dataset, quickly reaching the state of the art while improving the performance on more than half of the proteins by including the evolutionary conservation score and tuning the network per protein. Conclusions As the role of DL in big data analysis in the near future is indisputable, it is important to make it available for a broader range of researchers. We believe that an easy-to-use tool such as ENNGene can allow Genomics researchers without a background in Computational Sciences to harness the power of DL to gain better insights into and extract important information from the large amounts of data available in the field.",bioRxiv,2021.0,10.1101/2021.11.26.424041,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8a33868bddae1db6fb41d6d2ea4385cc7577793b,https://www.semanticscholar.org/paper/8a33868bddae1db6fb41d6d2ea4385cc7577793b,"IoT Network Security: Threats, Risks, and a Data-Driven Defense Framework","The recent surge in Internet of Things (IoT) deployment has increased the pace of integration and extended the reach of the Internet from computers, tablets and phones to a myriad of devices in our physical world. Driven by the IoT, with each passing day, the Internet becomes more integrated with everyday life. While IoT devices provide endless new capabilities and make life more convenient, they also vastly increase the opportunity for nefarious individuals, criminal organizations and even state actors to spy on, and interfere with, unsuspecting users of IoT systems. As this looming crisis continues to grow, calls for data science approaches to address these problems have increased, and current research shows that predictive models trained with machine learning algorithms hold great potential to mitigate some of these issues. In this paper, we first carry out an analytics approach to review security risks associated with IoT systems, and then propose a machine learning-based solution to characterize and detect IoT attacks. We use a real-world IoT system with secured gate access as a platform, and introduce the IoT system in detail, including features to capture security threats/attacks to the system. By using data collected from a nine month period as our testbed, we evaluate the efficacy of predictive models trained by means of machine learning, and propose design principles and a loose framework for implementing secure IoT systems.",IoT,2020.0,10.3390/iot1020016,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4861af53198fe2d7cd84aceba16d5d5d33f58910,https://www.semanticscholar.org/paper/4861af53198fe2d7cd84aceba16d5d5d33f58910,Technology Literacy in Undergraduate Medical Education: Review and Survey of the US Medical School Innovation and Technology Programs,"Background Modern innovations, like machine learning, genomics, and digital health, are being integrated into medical practice at a rapid pace. Physicians in training receive little exposure to the implications, drawbacks, and methodologies of upcoming technologies prior to their deployment. As a result, there is an increasing need for the incorporation of innovation and technology (I&T) training, starting in medical school. Objective We aimed to identify and describe curricular and extracurricular opportunities for innovation in medical technology in US undergraduate medical education to highlight challenges and develop insights for future directions of program development. Methods A review of publicly available I&T program information on the official websites of US allopathic medical schools was conducted in June 2020. Programs were categorized by structure and implementation. The geographic distribution of these categories across US regions was analyzed. A survey was administered to school-affiliated student organizations with a focus on I&T and publicly available contact information. The data collected included the founding year, thematic focus, target audience, activities offered, and participant turnout rate. Results A total of 103 I&T opportunities at 69 distinct Liaison Committee on Medical Education–accredited medical schools were identified and characterized into the following six categories: (1) integrative 4-year curricula, (2) facilitated doctor of medicine/master of science dual degree programs in a related field, (3) interdisciplinary collaborations, (4) areas of concentration, (5) preclinical electives, and (6) student-run clubs. The presence of interdisciplinary collaboration is significantly associated with the presence of student-led initiatives (P=.001). “Starting and running a business in healthcare” and “medical devices” were the most popular thematic focuses of student-led I&T groups, representing 87% (13/15) and 80% (12/15) of respondents, respectively. “Career pathways exploration for students” was the only type of activity that was significantly associated with a high event turnout rate of >26 students per event (P=.03). Conclusions Existing school-led and student-driven opportunities in medical I&T indicate growing national interest and reflect challenges in implementation. The greater visibility of opportunities, collaboration among schools, and development of a centralized network can be considered to better prepare students for the changing landscape of medical practice.",JMIR medical education,2022.0,10.2196/32183,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0bac32cdbb26ebf8c9ce7e331a209770ce6fd520,https://www.semanticscholar.org/paper/0bac32cdbb26ebf8c9ce7e331a209770ce6fd520,Accelerating the discovery of antifungal peptides using deep temporal convolutional networks,"The application of machine intelligence in biological sciences has led to the development of several automated tools, thus enabling rapid drug discovery. Adding to this development is the ongoing COVID-19 pandemic, due to which researchers working in the field of artificial intelligence have acquired an active interest in finding machine learning-guided solutions for diseases like mucormycosis, which has emerged as an important post-COVID-19 fungal complication, especially in immunocompromised patients. On these lines, we have proposed a temporal convolutional network-based binary classification approach to discover new antifungal molecules in the proteome of plants and animals to accelerate the development of antifungal medications. Although these biomolecules, known as antifungal peptides (AFPs), are part of an organism's intrinsic host defense mechanism, their identification and discovery by traditional biochemical procedures is arduous. Also, the absence of a large dataset on AFPs is also a considerable impediment in building a robust automated classifier. To this end, we have employed the transfer learning technique to pre-train our model on antibacterial peptides. Subsequently, we have built a classifier that predicts AFPs with accuracy and precision of 94%. Our classifier outperforms several state-of-the-art models by a considerable margin. The results of its performance were proven as statistically significant using the Kruskal-Wallis H test, followed by a post hoc analysis performed using the Tukey honestly significant difference (HSD) test. Furthermore, we identified potent AFPs in representative animal (Histatin) and plant (Snakin) proteins using our model. We also built and deployed a web app that is freely available at https://tcn-afppred.anvil.app/ for the identification of AFPs in protein sequences.",Briefings Bioinform.,2022.0,10.1093/bib/bbac008,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
37050e626a2de6d4980eb4d52d8db40eb1318aff,https://www.semanticscholar.org/paper/37050e626a2de6d4980eb4d52d8db40eb1318aff,Adversarially Reprogramming Pretrained Neural Networks for Data-limited and Cost-efficient Malware Detection∗,"To mitigate evolving malware attacks, machine learning models have been successfully deployed to detect malware. However, these models are often challenged by data scarcity, design efforts and constrained resources. Inspired by the adversarial vulnerability of machine learning, in this paper, we design a novel model Adv4Mal to adversarially reprogram an ImageNet classification neural network for malware detection in both white-box and black-box settings. As such, a small or moderate amount of data are sufficient to train a promising malware detection model, the varying software features can be uniformly processed without extra efforts, and the majority of computation can be wisely shared and reused to save the resources. This, to the best of our knowledge, has not yet been explored. Specifically, Adv4Mal proceeds by embedding software features into a host image to construct new data, and learning a universal perturbation to be added to all inputs in an imperceptible manner, such that the outputs of the pretrained model can be accordingly mapped to the final detection decisions for all software. We evaluate Adv4Mal on three software datasets. The experimental results demonstrate that Adv4Mal can successfully exploit ImageNet model’s learning capability and limited data to achieve high performance in malware detection, and also yield significant advantages of model flexibility to different features, and cost efficiency in computing resources. 1 Background and Motivation Software over computers and smartphones plays a vital role in our daily lives [5]. Unfortunately, its ubiquity and benefits immensely attract attackers to disseminate malware onto unsuspecting users to deliberately fulfill their intents, such as unwanted advertising, and user tracing. According to a recent report, the total number of malware has surpassed 1.1 billion [3]. Given such a huge amount of malware, machine learning models have been successfully deployed ∗To appear in SIAM International Conference on Data Mining (SDM22). †Department of Computer Science and Engineering, Wright State University, Dayton, OH 45435, USA. ‡College of Information Sciences and Technology, Pennsylvania State University, University Park, PA 16802, USA.",,2022.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d8ffef877579dffb1d3de19741e58774f65cf8df,https://www.semanticscholar.org/paper/d8ffef877579dffb1d3de19741e58774f65cf8df,"The Pransky interview: Harry Kloor, PhD, PhD – CEO and Co-Founder, Beyond Imagination Inc.; scientist; entrepreneur; inventor; filmmaker","
Purpose
The following article is a “Q&A interview” conducted by Joanne Pransky of Industrial Robot Journal as a method to impart the combined technological, business and personal experience of a prominent, robotic industry PhD-turned successful innovator and entrepreneur regarding turning his lifelong dream into an invention and commercialized product. This paper aims to discuss these issues.


Design/methodology/approach
Harry Kloor is a successful serial entrepreneur, scientist, technologist, educator, policy advisor, author and Hollywood filmmaker. He is the CEO and co-founder of Beyond Imagination, a company that has developed a suite of exponential technology solutions that deploys artificial intelligence (AI), AR, robotics, machine learning and human–computer interaction technology to enhance and revolutionize the world’s workforce. The company early in 2021 completed BEOMNI 1.0, the world’s first fully functional humanoid robotic system with an AI evolving brain, enabling remote work at a high level of fidelity to be done from around the globe. Kloor describes how he transformed his childhood dream into his brainchild and tangible reality.


Findings
Kloor was born a groundbreaker who did not take no for an answer. He was born partially crippled with his legs facing backwards. The doctors said that he would spend his life in braces and would never be able to run. His parents told him not to let those ideas limit him and by the age of seven he ran for the first time and went on to become a martial arts master. Kloor’s childhood dream was to create ways to leave his body and inhabit a robotic body so that he could physically be free from his limited mobility. Kloor built his first computer at the age of seven and invented his first product at the age of eight. Kloor's inspiration to study science came largely from science fiction and his 20,000-plus collection of comic books. Knowing the nature of exponential growth, he spent the next 40 years building the expertise, relationships, networks and experience in all areas of exponential technology. Kloor obtained a BA from Southern Oregon State College, an MEd from Southern Oregon University and two simultaneous PhDs, one in chemistry and one in physics, from Purdue University. Kloor co-founded the company Universal Consultants, where he served as chief science consultant, providing guidance to clients in the development of new technological products, patents and policy positions. Kloor was the founder of Stem CC Inc. – a stem cell company that was sold in 2018 to Celularity, one of the world’s most cutting edge clinical-stage cell therapeutics company. Kloor is also the founder and president of Jupiter 9 Productions and is a credited film writer, director and producer. Since his graduation from Purdue University, he has written for Star Trek: Voyager and was the story editor for Gene Roddenberry’s Earth: Final Conflict, a series he co-created/developed. Kloor helped create Taiwan’s animation industry, bringing Quantum Quest: A Cassini Space Odyssey, the first big animation film that starred major Hollywood stars, to Taiwan. Kloor also sits on the board of Brain Mapping and Therapeutics Society and serves as their Chief Scientific Advisor and Educational Outreach Coordinator.


Originality/value
Kloor is a “creative consultant and universal problem solver, with an emphasis in technology and education.” Kloor has worked with Dr Peter Diamandis since the first class of the International Space University in 1988. Kloor was one of the five founding team members of XPRIZE serving as its CSO until 2005 and was one of the founders of the Rocket Racing League. He was on the founding team of Singularity University and taught at Singularity’s first summer program. In 2016 he created the $10m Avatar XPRIZE, and in 2018 he co-created the Carbon Extraction XPRIZE which obtained the largest incentive prize in history, a $100m, funded by Elon Musk and the Musk Foundation. Kloor is the only person in world history to earn two PhDs simultaneously in two distinct academic disciplines. In recognition of this achievement, he was named ABC World News’ Person of the Week in August 1994. Kloor has received numerous awards, including The Golden Axon Award from the Society for Brain Mapping & Therapeutics. He has recently created the Kloor Cycle, a four-stage experiential autonomous learning process within Beomni’s “AI Brain,” adapted from Kolb’s Learning Cycles.
",Industrial Robot: the international journal of robotics research and application,2022.0,10.1108/ir-06-2022-0148,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
64dfb21fc22523f2fbb1128cc9bd32a904df5e92,https://www.semanticscholar.org/paper/64dfb21fc22523f2fbb1128cc9bd32a904df5e92,Accelerating the discovery of materials for clean energy in the era of smart automation,,Nature Reviews Materials,2018.0,10.1038/s41578-018-0005-z,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
33707b8d128de923bd8941471eb92de5a7f7409c,https://www.semanticscholar.org/paper/33707b8d128de923bd8941471eb92de5a7f7409c,A Study on Cloud Storage Security Method Using Data Classification,"Cloud computing is a new model for providing diverse services of software and hardware. This paradigm refers to a model for enabling on-demand network access to a shared pool of configurable computing resources, that can be rapidly provisioned and released with minimal service provider interaction .It helps the organizations and individuals deploy IT resources at a reduced total cost. However, the new approaches introduced by the clouds, related to computation outsourcing, distributed resources and multi-tenancy concept, increase the security and privacy concerns and challenges. It allows users to store their data remotely and then access to them at any time from any place .Cloud storage services are used to store data in ways that are considered cost saving and easy to use. In cloud storage, data are stored on remote servers that are not physically known by the consumer. Thus, users fear from uploading their private and confidential files to cloud storage due to security concerns. The usual solution to secure data is data encryption, which makes cloud users more satisfied when using cloud storage to store their data. Motivated by the above facts; we have proposed a solution to undertake the problem of cloud storage security. In cloud storage, there are public data that do not need any security measures, and there are sensitive data that need applying security mechanisms to keep them safe. In that context, data classification appears as the solution to this problem. The classification of data into classes, with different security requirements for each class is the best way to avoid under security and over security situation. The existing cloud storage systems use the same Journal of University of Shanghai for Science and Technology ISSN: 1007-6735 Volume 23, Issue 9, September – 2021 Page-1105 key size to encrypt all data without taking into consideration its confidentiality level. Treating the low and high confidential data with the same way and at the same security level will add unnecessary overhead and increase the processing time. In our proposal, we have combined the K-NN (K Nearest Neighbors) machine learning method and the goal programming decision-making method, to provide an efficient method for data classification. This method allows data classification according to the data owner security needs. Then, we introduce the user data to the suitable security mechanisms for each class. The use of our solution in cloud storage systems makes the data security process more flexible, besides; it increases the cloud storage system performance and decreases the needed resources, which are used to store the data.",Journal of University of Shanghai for Science and Technology,2021.0,10.51201/jusst/21/09657,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
91e8691d0ba45b5f29ba0fb6d5ec2bffdfa1248c,https://www.semanticscholar.org/paper/91e8691d0ba45b5f29ba0fb6d5ec2bffdfa1248c,See Deeper: Identifying Crystal Structure from X-ray Diffraction Patterns,"X-ray diffraction is a commonly used experimental science to detect the atomic and molecular structure of crystalline material. The process is called X-ray crystallography (XRC). Traditionally, it is done by human experts with some conjecture about what structure the crystalline material is likely to be. However, the study of crystal structure using X-ray diffraction patterns is applicable in many domains, such as chemistry, physics, biology, etc. It is tedious to have manual crystallography of X-ray diffraction patterns to determine a crystal structure with a massive amount of dataset. With the advent of high computational resources, deep learning techniques have taken classification to its peak. Convolution Neural Network (CNN) maps an input image into a high dimensional space and produce a low-cost function for image classification. In this paper, we deploy a variation of the Convolution Neural Network to predict crystal structure from X-ray diffraction patterns. We compare our approach with a wide range of conventional as well as modern Machine Learning based classification techniques for the structure prediction of a crystal. We report a cross-validation accuracy of 95.6% and Micro F1-score of 0.949 with our model for the proposed task which is significantly better than the other reported baseline methods.",2020 International Conference on Cyberworlds (CW),2020.0,10.1109/CW49994.2020.00015,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
562c08c6cd90bca62a5e8729a259fe6692c30ee4,https://www.semanticscholar.org/paper/562c08c6cd90bca62a5e8729a259fe6692c30ee4,Deep Learning researches in Turkey: An academic approach,"Deep learning (DL) is deployed in Deep Neural Networks (DNNs), Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), Deep Stacked Networks (DSNs), Deep Belief Networks (DBNs), and Deep Boltzmann Machines (DBMs). DL continues to take powerful steps to become a part of almost any software in the future. DL is one of the most popular research areas of machine learning. What makes it so popular is the exciting applications of recent times. Nowadays, improvement in machine learning provide ability to determine what an object in a picture does. The aim of this study is to reveal academic studies in the field of Deep Learning in Turkey and to evaluate their numerical results. Dataset search for studies in the field of DL were done in both the YÖKSIS database in Turkey and the internationally operated Web of Science and Scopus services. Finally, some evaluations were made to emphasize the importance of DL for the country.",2017 XIIIth International Conference on Perspective Technologies and Methods in MEMS Design (MEMSTECH),2017.0,10.1109/MEMSTECH.2017.7937546,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8d99f0b45b49076ce5422c70a81ed47c6e0c4dab,https://www.semanticscholar.org/paper/8d99f0b45b49076ce5422c70a81ed47c6e0c4dab,Making Sense of Sleep,"Traditionally, sleep monitoring has been performed in hospital or clinic environments, requiring complex and expensive equipment set-up and expert scoring. Wearable devices increasingly provide a viable alternative for sleep monitoring and are able to collect movement and heart rate (HR) data. In this work, we present a set of algorithms for sleep-wake and sleep-stage classification based upon actigraphy and cardiac sensing amongst 1,743 participants. We devise movement and cardiac features that could be extracted from research-grade wearable sensors and derive models and evaluate their performance in the largest open-access dataset for human sleep science. Our results demonstrated that neural network models outperform traditional machine learning methods and heuristic models for both sleep-wake and sleep-stage classification. Convolutional neural networks (CNNs) and long-short term memory (LSTM) networks were the best performers for sleep-wake and sleep-stage classification, respectively. Using SHAP (SHapley Additive exPlanation) with Random Forest we identified that frequency features from cardiac sensors are critical to sleep-stage classification. Finally, we introduced an ensemble-based approach to sleep-stage classification, which outperformed all other baselines, achieving an accuracy of 78.2% and F1 score of 69.8% on the classification task for three sleep stages. Together, this work represents the first systematic multimodal evaluation of sleep-wake and sleep-stage classification in a large, diverse population. Alongside the presentation of an accurate sleep-stage classification approach, the results highlight multimodal wearable sensing approaches as scalable methods for accurate sleep-classification, providing guidance on optimal algorithm deployment for automated sleep assessment. The code used in this study can be found online at: https://github.com/bzhai/multimodal_sleep_stage_benchmark.git",Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.,2020.0,10.1145/3397325,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3ac4c4bda5791b076eca9a633f7226dab1cbf9b6,https://www.semanticscholar.org/paper/3ac4c4bda5791b076eca9a633f7226dab1cbf9b6,Editorial for the special issue on Machining Science,"Machining has been at the centre of manufacturing technologies since the start of the industrial revolution, and so it is fitting that this topic receives special attention in a journal that can trace its origins back to 1847. The first publication of the Proceedings of the Institution of Mechanical Engineers describes a healthy debate concerning the machining of gear teeth, alongside an obituary to George Stephenson himself – the ‘father of railways’ and the founding president of the Institution of Mechanical Engineers. The next 60 years saw a steady growth in scientific and technical dissemination of knowledge concerning machining, which is epitomised by Taylor’s famous monograph of 1907. Over 100 years later, we have of course seen huge leaps in our understanding of all manufacturing processes. But the strive for increased productivity and quality is now also matched by a need for resource efficiency in light of societal challenges such as climate change and pollution. The contributions included in this special issue seek to demonstrate how machining science research is playing a role in addressing this challenge: a rethinking of manufacturing is underway as a consequence of machining learning, ubiquitous data, and networked computing and machining science is a key part of this shifting manufacturing landscape. The topics that are covered are intentionally diverse: they illustrate a vibrant and creative scientific approach across the spectrum of material removal processes, and show emergent approaches that can harness flexible manufacturing processes, as well as data-driven and intelligent automation. The special issue begins with a focus on novel techniques for monitoring the performance of machine tools and their cutting operations. Here, there have been great developments in machine learning techniques that can be brought to bear on production processes. To pick just two examples from the manuscripts: McLeay et al. develop fault detection techniques based upon unsupervised learning methods, and Moore et al. also demonstrate how machine learning concepts can be applied to machine health monitoring. The deployment of these novel monitoring techniques necessitates effective measurement capabilities, and on novel manufacturing problems this can itself be a challenge. Alhadeff et al. explore wear measurements in micro milling, whilst Duboust et al. characterise surface roughness in machining of composites. The machining of new materials, and workpieces produced using novel additive manufacturing techniques, also presents challenges. Several manuscripts within this special issue address these problems, focussing for example on Inconel (Curtis et al.), Titanium (Khan et al.) and metal-matrix composites (Saberi et al.). Finally, the development of state-of-the art modelling techniques can help to improve the performance of machining processes, focussing for example on dynamic effects (Urena et al.) and Robotics (Rooker et al.). At The University of Sheffield, we have been fortunate to be able to explore these avenues of research within the remit of an EPSRC Centre for Doctoral Training in Machining Science (Grant Reference EP/ L016257/1). This special issue was borne from discussions with the journal’s editorial board, in particular Professors Maropoulos and Long, as a consequence of the doctoral training centre. Consequently, much of the work included in this special issue has been inspired by the work within the doctoral training centre. The guest editors, who are co-directors of the centre, are grateful for the support of the journal’s editorial office who have ensured an independent peer review process for these manuscripts. We also express our thanks to Dr Francesca Breeden for her assistance in coordinating the special issue.","Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture",2021.0,10.1177/09544054211021034,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a0350117b017deff660a0cb4ec2dfeaaf64c62b1,https://www.semanticscholar.org/paper/a0350117b017deff660a0cb4ec2dfeaaf64c62b1,Computational Propaganda,"Computational propaganda is an emergent form of political manipulation that occurs over the Internet. The term describes the assemblage of social media platforms, autonomous agents, algorithms, and big data tasked with the manipulation of public opinion. Our research shows that this new mode of interrupting and influencing communication is on the rise around the globe. Advances in computing technology, especially around social automation, machine learning, and artificial intelligence mean that computational propaganda is becoming more sophisticated and harder to track at an alarming rate. This introduction explores the foundations of computational propaganda. It describes the key role that automated manipulation of algorithms plays in recent efforts to control political communication worldwide. We discuss the social data science of political communication and build upon the argument that algorithms and other computational tools now play an important political role in areas like news consumption, issue awareness, and cultural understanding. We unpack the key findings of the nine country case studies that follow—exploring the role of computational propaganda during events from local and national elections in Brazil to the ongoing security crisis between Ukraine and Russia. Our methodology in this work has been purposefully mixed, we make use of quantitative analysis of data from several social media platforms and qualitative work that includes interviews with the people who design and deploy political bots and disinformation campaigns. Finally, we highlight original evidence about how this manipulation and amplification of disinformation is produced, managed, and circulated by political operatives and governments and describe paths for both democratic intervention and future research in this space.",Oxford Scholarship Online,2018.0,10.1093/oso/9780190931407.001.0001,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3855da29f4f6a2abba13ebc7ab4da1c611a37ad4,https://www.semanticscholar.org/paper/3855da29f4f6a2abba13ebc7ab4da1c611a37ad4,Security And Privacy Issues In A Knowledge Management System Epub Download,"Summarizes the current state and upcoming trends within the area of fog computing Written by some of the leading experts in the field, Fog Computing: Theory and Practice focuses on the technological aspects of employing fog computing in various application domains, such as smart healthcare, industrial process control and improvement, smart cities, and virtual learning environments. In addition, the Machine-toMachine (M2M) communication methods for fog computing environments are covered in depth. Presented in two parts—Fog Computing Systems and Architectures, and Fog Computing Techniques and Application—this book covers such important topics as energy efficiency and Quality of Service (QoS) issues, reliability and fault tolerance, load balancing, and scheduling in fog computing systems. It also devotes special attention to emerging trends and the industry needs associated with utilizing the mobile edge computing, Internet of Things (IoT), resource and pricing estimation, and virtualization in the fog environments. Includes chapters on deep learning, mobile edge computing, smart grid, and intelligent transportation systems beyond the theoretical and foundational concepts Explores real-time traffic surveillance from video streams and interoperability of fog computing architectures Presents the latest research on data quality in the IoT, privacy, security, and trust issues in fog computing Fog Computing: Theory and Practice provides a platform for researchers, practitioners, and graduate students from computer science, computer engineering, and various other disciplines to gain a deep understanding of fog computing. How the enabling technologies in 5G as an integral or as a part can seamlessly fuel the IoT revolution is still very challenging. This book presents the state-of-the-art solutions to the theoretical and practical challenges stemming from the integration of 5G enabling technologies into IoTs in support of a smart 5G-enabled IoT paradigm, in terms of network design, operation, management, optimization, privacy and security, and applications. In particular, the technical focus covers a comprehensive understanding of 5G-enabled IoT architectures, converged access networks, privacy and security, and emerging applications of 5G-eabled IoT. This book constitutes the refereed proceedings of the International ECML/PKDD Workshop on Privacy and Security Issues in Data Mining and Machine Learning, PSDML 2010, held in Barcelona, Spain, in September 2010. The 11 revised full papers presented were carefully reviewed and selected from 21 submissions. The papers range from data privacy to security applications, focusing on detecting malicious behavior in computer systems. This timely book provides broad coverage of security and privacy issues in the macro and micro perspective. In macroperspective, the system and algorithm fundamentals of next-generation wireless networks are discussed. In micro-perspective, this book focuses on the key secure and privacy techniques in different emerging networks from the interconnection view of human and cyber-physical world. This book includes 7 chapters from prominent international researchers working in this subject area. This book serves as a useful reference for researchers, graduate students, and practitioners seeking solutions to wireless security and privacy related issues Recent advances in wireless communication technologies have enabled the large-scale deployment of next-generation wireless networks, and many other wireless applications are emerging. The next generation of mobile networks continues to transform the way people communicate and access information. As a matter of fact, next-generation emerging networks are exploiting their numerous applications in both military and civil fields. For most applications, it is important to guarantee high security of the deployed network in order to defend against attacks from adversaries, as well as the privacy intrusion. The key target in the development of next-generation wireless networks is to promote the integration of the",,2022.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c3aefee6421e888ba5aadb9af1a979ca30e37054,https://www.semanticscholar.org/paper/c3aefee6421e888ba5aadb9af1a979ca30e37054,New Paradigms and Optimality Guarantees in Statistical Learning and Estimation,"Machine learning (ML) has become one of the most powerful classes of tools for artificial intelligence, personalized web services and data science problems across fields. Within the field of machine learning itself, there had been quite a number of paradigm shifts caused by the explosion of data size, computing power, modeling tools, and the new ways people collect, share, and make use of data sets. Data privacy, for instance, was much less of a problem before the availability of personal information online that could be used to identify users in anonymized data sets. Images, videos, as well as observations generated over a social networks, often have highly localized structures, that cannot be captured by standard nonparametric models. Moreover, the “common task framework” that is adopted by many subdisciplines of AI has made it possible for many people to collaboratively and repeated work on the same data set, leading to implicit overfitting on public benchmarks. In addition, data collected in many internet services, e.g., web search and targeted ads, are not iid, but rather feedbacks specific to the deployed algorithm. This thesis presents technical contributions under a number of new mathematical frameworks that are designed to partially address these new paradigms. Firstly, we consider the problem of statistical learning with privacy constraints. Under Vapnik’s general learning setting and the formalism of differential privacy (DP), we establish simple conditions that characterizes the private learnability, which reveals a mixture of positive and negative insight. We then identify generic methods that reuses existing randomness to effectively solve private learning in practice; and discuss weaker notions of privacy that allows for more favorable privacy-utility tradeoff. Secondly, we attempt to generalize trend filtering, a locally-adaptive nonparametric regression technique that is minimax in 1D, to the multivariate setting and to graphs. Thirdly, we investigate two problems in sequential interactive learning: a) off-policy evaluation in contextual bandits, that aims to use data collected from one algorithm to evaluate the performance of a different algorithm; b) the problem of adaptive data analysis, that uses randomization to prevent adversarial data analysts from a form of “p-hacking” through multiple steps of sequential data access. In many of the theoretical studies, we will provide not only performance guarantees of algorithms but also certain notions of optimality. Whenever applicable, careful empirical studies on synthetic and real data are also included. November 8, 2016 DRAFT",,2017.0,10.1184/r1/6720836.v1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e9c1d78b15cb52638865b63b07fe96e388a317dd,https://www.semanticscholar.org/paper/e9c1d78b15cb52638865b63b07fe96e388a317dd,Using Learning Analytics to Understand Scientific Modeling in the Classroom,"Scientific models represent ideas, processes, and phenomena by describing important components, characteristics, and interactions. Models are constructed across a variety of scientific disciplines, such as the food web in biology, the water cycle in Earth science, or the structure of the solar system in astronomy. Models are central for scientists to understand phenomena, construct explanations, and communicate theories. Constructing and using models to explain scientific phenomena is also an essential practice in contemporary science classrooms. Our research explores new techniques for understanding scientific modeling and engagement with modeling practices. We work with students in secondary biology classrooms as they use a web-based software tool - EcoSurvey - to characterize organisms and their interrelationships found in their local ecosystem. We use learning analytics and machine learning techniques to answer the following questions: 1) How can we automatically measure the extent to which students’ scientific models support complete explanations of phenomena? 2) How does the design of student modeling tools influence the complexity and completeness of students’ models? 3) How do clickstreams reflect and differentiate student engagement with modeling practices? We analyzed EcoSurvey usage data collected from two different deployments with over 1000 secondary students across a large urban school district. We observe large variations in the completeness and complexity of student models, and large variations in their iterative refinement processes. These differences reveal that certain key model features are highly predictive of other aspects of the model. We also observe large differences in student modeling practices across different classrooms and teachers. We can predict a student’s teacher based on the observed modeling practices with a high degree of accuracy without significant tuning of the predictive model. These results highlight the value of this approach for extending our understanding of student engagement with scientific modeling, an important contemporary science practice, as well as the potential value of analytics for identifying critical differences in classroom implementation.",Front. ICT,2017.0,10.3389/fict.2017.00024,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a86fed94c9d97e052d0ff84b2403b10200280c6b,https://www.semanticscholar.org/paper/a86fed94c9d97e052d0ff84b2403b10200280c6b,Large Scale Distributed Data Science from scratch using Apache Spark 2.0,"Apache Spark is an open-source cluster computing framework. It has emerged as the next generation big data processing engine, overtaking Hadoop MapReduce which helped ignite the big data revolution. Spark maintains MapReduce's linear scalability and fault tolerance, but extends it in a few important ways: it is much faster (100 times faster for certain applications), much easier to program in due to its rich APIs in Python, Java, Scala, SQL and R (MapReduce has 2 core calls) , and its core data abstraction, the distributed data frame. In addition, it goes far beyond batch applications to support a variety of compute-intensive tasks, including interactive queries, streaming, machine learning, and graph processing. With massive amounts of computational power, deep learning has been shown to produce state-of-the-art results on various tasks in different fields like computer vision, automatic speech recognition, natural language processing and online advertising targeting. Thanks to the open-source frameworks, e.g. Torch, Theano, Caffe, MxNet, Keras and TensorFlow, we can build deep learning model in a much easier way. Among all these framework, TensorFlow is probably the most popular open source deep learning library. TensorFlow 1.0 was released recently, which provide a more stable, flexible and powerful computation tool for numerical computation using data flow graphs. Keras is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation. This tutorial will provide an accessible introduction to large-scale distributed machine learning and data mining, and to Spark and its potential to revolutionize academic and commercial data science practices. It is divided into three parts: the first part will cover fundamental Spark concepts, including Spark Core, functional programming ala map-reduce, data frames, the Spark Shell, Spark Streaming, Spark SQL, MLlib, and more; the second part will focus on hands-on algorithmic design and development with Spark (developing algorithms from scratch such as decision tree learning, association rule mining (aPriori), graph processing algorithms such as pagerank/shortest path, gradient descent algorithms such as support vectors machines and matrix factorization. Industrial applications and deployments of Spark will also be presented.; the third part will introduce deep learning concepts, how to implement a deep learning model through TensorFlow, Keras and run the model on Spark. Example code will be made available in python (pySpark) notebooks.",WWW,2017.0,10.1145/3041021.3051108,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d1bfdb42538ce550410b040c5dd1bd079bd89e66,https://www.semanticscholar.org/paper/d1bfdb42538ce550410b040c5dd1bd079bd89e66,Hridaya Kalp: A Prototype for Second Generation Chronic Heart Disease Detection and Classification,,,2020.0,10.1007/978-981-15-5148-2_29,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
99c65a00daab51a651ccdc3a7993f0976e26521f,https://www.semanticscholar.org/paper/99c65a00daab51a651ccdc3a7993f0976e26521f,Learn Python by Building Data Science Applications,"Understand the constructs of the Python programming language and use them to build data science projects
Key Features
Learn the basics of developing applications with Python and deploy your first data application

Take your first steps in Python programming by understanding and using data structures, variables, and loops

Delve into Jupyter, NumPy, Pandas, SciPy, and sklearn to explore the data science ecosystem in Python
Book Description
Python is the most widely used programming language for building data science applications. Complete with step-by-step instructions, this book contains easy-to-follow tutorials to help you learn Python and develop real-world data science projects. The “secret sauce” of the book is its curated list of topics and solutions, put together using a range of real-world projects, covering initial data collection, data analysis, and production.

This Python book starts by taking you through the basics of programming, right from variables and data types to classes and functions. You'll learn how to write idiomatic code and test and debug it, and discover how you can create packages or use the range of built-in ones. You'll also be introduced to the extensive ecosystem of Python data science packages, including NumPy, Pandas, scikit-learn, Altair, and Datashader. Furthermore, you'll be able to perform data analysis, train models, and interpret and communicate the results. Finally, you'll get to grips with structuring and scheduling scripts using Luigi and sharing your machine learning models with the world as a microservice.

By the end of the book, you'll have learned not only how to implement Python in data science projects, but also how to maintain and design them to meet high programming standards.
What you will learn
Code in Python using Jupyter and VS Code

Explore the basics of coding – loops, variables, functions, and classes

Deploy continuous integration with Git, Bash, and DVC

Get to grips with Pandas, NumPy, and scikit-learn

Perform data visualization with Matplotlib, Altair, and Datashader

Create a package out of your code using poetry and test it with PyTest

Make your machine learning model accessible to anyone with the web API
Who this book is for
If you want to learn Python or data science in a fun and engaging way, this book is for you. You'll also find this book useful if you're a high school student, researcher, analyst, or anyone with little or no coding experience with an interest in the subject and courage to learn, fail, and learn from failing. A basic understanding of how computers work will be useful.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a7a54b568b608fcf636ee1010c0b53debcdc9c12,https://www.semanticscholar.org/paper/a7a54b568b608fcf636ee1010c0b53debcdc9c12,Editorial: Special issue on robotics and manufacturing systems,"With the increasing utilization of artificial intelligence technologies in manufacturing systems it is increasingly difficult to differentiate the robots from the machine tools. Innovative robot mechanisms with intelligent perception and learning abilities are now a key concern in advanced robotics and manufacturing systems. This special collection aims to disseminate the latest advances in fundamental and applied research work in the area of robotics and manufacturing systems to the international community. The selected papers include theoretical and experimental work on parallel mechanisms with variable workspace; the kinematics and dynamics of a robot; an over-constrained spatial deployable mechanism; a hybrid continuum robot; dimension synthesis of gear train; evaluation on the collaborative tasks between human and robot; analysis and an experiment on a multi-legged robot; and the design of a hydraulic drive in-pipe robot based on a flexible support structure. All papers in this special issue provide original ideas, algorithms and methods, with clear indication of the advances made in problem formulation, methodology and application with respect to existing results. These achievements reflect the latest theory and technology promotion in robotics and manufacturing systems. We hope these papers are helpful to enhance the interest and development in manufacturing technology. We would like to thank the authors and reviewers for their contribution and professional work, and thank Prof. John Chew, the Chief Editor, and the Publishing Editor, Martin McDonald, and the production team of the journal without whose constant help and professionalism our work would have been impossible. Finally, we would like to thank the support of the National Science Foundation of China (NSFC) under grant number 51575291.",,2021.0,10.1177/0954406220969094,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
541bf9f5719f4ebc66b8c688daceb97159d33a98,https://www.semanticscholar.org/paper/541bf9f5719f4ebc66b8c688daceb97159d33a98,Matching Algorithms for Blood Donation,"Managing perishable inventory, such as blood stock awaiting use by patients in need, has been a topic of research for decades. This has been investigated across several disciplines: medical and social scientists have investigated who donates blood, how frequently, and why; management science researchers have long studied the blood supply chain from a logistical perspective. Yet global demand for blood still far exceeds supply, and unmet need is greatest in low- and middle-income countries. Both academics and policy experts suggest that large-scale coordination is necessary to alleviate demand for donor blood. Using the recently-deployed Facebook Blood Donation tool, we conduct the first large-scale algorithmic matching of blood donors with donation opportunities. In both simulations and real experiments we match potential donors with opportunities, guided by a machine learning model trained on prior observations of donor behavior. While measuring actual donation rates remains a challenge, we measure donor action (i.e., calling a blood bank or making an appointment) as a proxy for actual donation. Simulations suggest that even a simple matching strategy can increase donor action rate by 10-15%; a pilot experiment with real donors finds a slightly smaller increase of roughly 5%. While overall action rates remain low, even this modest increase among donors in a global network corresponds to many thousands of more potential donors taking action toward donation. Further, observing donor action on a social network can shed light onto donor behavior and response to incentives. Our initial findings align with several observations made in the medical and social science literature regarding donor behavior.",EC,2020.0,10.1145/3391403.3399458,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d1f13de7884fd479c88d853eadb076cb3ba3018e,https://www.semanticscholar.org/paper/d1f13de7884fd479c88d853eadb076cb3ba3018e,Interactive Classification of Whole-Slide Imaging Data for Cancer Researchers,"An interactive machine learning tool for analyzing digital pathology images enables cancer researchers to apply this tool to measure histologic patterns for clinical and basic science studies. Whole-slide histology images contain information that is valuable for clinical and basic science investigations of cancer but extracting quantitative measurements from these images is challenging for researchers who are not image analysis specialists. In this article, we describe HistomicsML2, a software tool for learn-by-example training of machine learning classifiers for histologic patterns in whole-slide images. This tool improves training efficiency and classifier performance by guiding users to the most informative training examples for labeling and can be used to develop classifiers for prospective application or as a rapid annotation tool that is adaptable to different cancer types. HistomicsML2 runs as a containerized server application that provides web-based user interfaces for classifier training, validation, exporting inference results, and collaborative review, and that can be deployed on GPU servers or cloud platforms. We demonstrate the utility of this tool by using it to classify tumor-infiltrating lymphocytes in breast carcinoma and cutaneous melanoma. Significance: An interactive machine learning tool for analyzing digital pathology images enables cancer researchers to apply this tool to measure histologic patterns for clinical and basic science studies.",Cancer Research,2020.0,10.1158/0008-5472.CAN-20-0668,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4cd9c9a6dcd8831c6db4915a43126c60b0c7a987,https://www.semanticscholar.org/paper/4cd9c9a6dcd8831c6db4915a43126c60b0c7a987,Data science for software engineering,"Target audience: Software practitioners and researchers wanting to understand the state of the art in using data science for software engineering (SE). Content: In the age of big data, data science (the knowledge of deriving meaningful outcomes from data) is an essential skill that should be equipped by software engineers. It can be used to predict useful information on new projects based on completed projects. This tutorial offers core insights about the state-of-the-art in this important field. What participants will learn: Before data science: this tutorial discusses the tasks needed to deploy machine-learning algorithms to organizations (Part 1: Organization Issues). During data science: from discretization to clustering to dichotomization and statistical analysis. And the rest: When local data is scarce, we show how to adapt data from other organizations to local problems. When privacy concerns block access, we show how to privatize data while still being able to mine it. When working with data of dubious quality, we show how to prune spurious information. When data or models seem too complex, we show how to simplify data mining results. When data is too scarce to support intricate models, we show methods for generating predictions. When the world changes, and old models need to be updated, we show how to handle those updates. When the effect is too complex for one model, we show how to reason across ensembles of models. Pre-requisites: This tutorial makes minimal use of maths of advanced algorithms and would be understandable by developers and technical managers.",2013 35th International Conference on Software Engineering (ICSE),2013.0,10.1109/ICSE.2013.6606752,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9435d7d2d915345f2c8f236df50f25177cd323d3,https://www.semanticscholar.org/paper/9435d7d2d915345f2c8f236df50f25177cd323d3,Containerized Architecture for Edge Computing in Smart Home : A consistent architecture for model deployment,"Network bandwidth and high latency are the main bottlenecks of cloud computing. To combat such scenarios, new paradigm Edge Computing is used. Edge computing shifts the computation of resources from centralized cloud closer to the devices which generates data. Edge Computing reduces the response time, latency and improves the battery life while maintaining data safety and privacy. The distributed architecture of edge computing makes resource management an important aspect of edge computing. In such a resource constrained environment edge devices should be capable of processing all types of request coming from IoT devices. With the advancement in Data science and Machine learning models in different domains, many intelligent services have emerged to provide better user experience. These deep learning models have frequent updates to adapt to new hardware and software requirements. These models also have some installation dependencies and requirement of cross platform compatibility for training and prediction. In home edge environment, these issues are more important due to resource constraint in terms of memory and computing power. Also, due to the availability of extensive list of deep learning models for different service, it is difficult to maintain an environment supporting such models across various devices in the smart home to support all services. To solve this issue in smart home environment, this paper proposes architecture, using containerization techniques to deploy and manage the deep learning models. This paper also explains about the steps to convert the existing model into containers. Minimal space requirement on the edge device, data privacy, low latency along with device independence for deep learning models are prime benefits of the architecture proposed. To test the performance of the architecture, deep learning model was containerized and compared with the actual model deployed in the same edge environment. The experimental results demonstrated the performance is almost similar of the containerized architecture in terms of model execution time and CPU load vs execution time. But it comes with ease of model deployment and cross platform model execution.",2020 International Conference on Computer Communication and Informatics (ICCCI),2020.0,10.1109/ICCCI48352.2020.9104073,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9bcdc6640e28b9d8175363e910065be2d452c080,https://www.semanticscholar.org/paper/9bcdc6640e28b9d8175363e910065be2d452c080,Bayesian reverse-engineering considered as a research strategy for cognitive science,,Synthese,2016.0,10.1007/s11229-016-1180-3,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
594b4d0a1ba5dc1d0e9e6ee2b381137251eeb8fa,https://www.semanticscholar.org/paper/594b4d0a1ba5dc1d0e9e6ee2b381137251eeb8fa,Accelerating GAN training using highly parallel hardware on public cloud,"With the increasing number of Machine and Deep Learning applications in High Energy Physics, easy access to dedicated infrastructure represents a requirement for fast and efficient R&D. This work explores different types of cloud services to train a Generative Adversarial Network (GAN) in a parallel environment, using Tensorflow data parallel strategy. More specifically, we parallelize the training process on multiple GPUs and Google Tensor Processing Units (TPU) and we compare two algorithms: the TensorFlow built-in logic and a custom loop, optimised to have higher control of the elements assigned to each GPU worker or TPU core. The quality of the generated data is compared to Monte Carlo simulation. Linear speed-up of the training process is obtained, while retaining most of the performance in terms of physics results. Additionally, we benchmark the aforementioned approaches, at scale, over multiple GPU nodes, deploying the training process on different public cloud providers, seeking for overall efficiency and cost-effectiveness. The combination of data science, cloud deployment options and associated economics allows to burst out heterogeneously, exploring the full potential of cloud-based services.",EPJ Web of Conferences,2021.0,10.1051/epjconf/202125102073,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2792f3a8058c7a019b2a18fb3b7e43c828086b81,https://www.semanticscholar.org/paper/2792f3a8058c7a019b2a18fb3b7e43c828086b81,Deep Learning in Natural Language Processing,,Springer Singapore,2018.0,10.1007/978-981-10-5209-5,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6d36ca7bf91c5d9881c970367b901729abc2ed42,https://www.semanticscholar.org/paper/6d36ca7bf91c5d9881c970367b901729abc2ed42,Tree-based regressor ensemble for viral infectious diseases spread prediction,"Tree based regression models provide statistical bases for prediction of continuous response variable scores They are non-linear models founded on simplicity and efficiency when deployed on multi variable data domains Their fast prediction speed, ability to identify strong variables in prediction and reliance on statistical means to deal with missing values in datasets during prediction make these models common in modern machine learning Some of these models such as CART, RETIS and M5 have been utilized in the past yielding reliable prediction outcomes which are yet to be achieved through the use of single classifier models due the growing dataset complexities as a result of recent trends in data science including big data and internet of things Combination of several classifiers through ensemble approach can boost feature selection and enhance classifier prediction capabilities This research paper demonstrates ensemble of Decision Tree (DT) and Logistic Regression (LR) models to develop a tree-based regressor model christened Simultaneous Tree-based Regressor Interactive Model (STRIM), with improved interaction effect especially on continuous response variable predictions The model involves particle swarm optimization (PSO) for parameter tuning in an effort to ensure a balanced and reliable prediction achievement in the spread of infectious diseases, incorporating time series modeling The model aimed at providing a solution to the prediction of infectious diseases spread using publicly available Covid-19 global data for evaluation through prediction of Covid-19 spread patterns Existing models used in the domain are largely black-box and therefore the need for a glass-box model capable of disclosing the impact of prediction features to the final prediction results STRIM proved to be a robust interpretable classifier model compared to single classifiers considered for the ensemble providing 0 99 accuracy levels of prediction Copyright © 2020 for this paper by its authors Use permitted under Creative Commons License Attribution 4 0 International (CC BY 4 0)",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2167695b6ed29751ff3bfefbc0d2dd2746d3a90e,https://www.semanticscholar.org/paper/2167695b6ed29751ff3bfefbc0d2dd2746d3a90e,String Similarity Measures for Myanmar Language (Burmese),"Measuring string similarity is useful for a broad range of applications. It plays an important role in machine learning, information retrieval, natural language processing, error encoding, and bioinformatics. Measuring string similarity is a fundamental operation of data science, important for data cleaning and integration. Realworld applications such as spell checking, duplicate finding, searching similar words, and retrieving tasks use string similarity. In this study, string similarity metrics have been calculated for Burmese (Myanmar language). The encoding table for Burmese has been built based on the pronunciation similarity of characters and vowel combination positions with a consonant. According to the table, strings and words are encoded. Similarity distance is measured between the dataset and query words. Previous string similarity approaches are not suitable for fuzzy string matching of tonal-based Burmese. Therefore, three mapping approaches are proposed in this study.",NSURL,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e095c272b84ef4cd8d6ceb9b594c7798d3de8d34,https://www.semanticscholar.org/paper/e095c272b84ef4cd8d6ceb9b594c7798d3de8d34,"Governing AI: Understanding the Limits, Possibility, and Risks of AI in an Era of Intelligent Tools and Systems","In debates about artificial intelligence (AI), imaginations often run wild. Policy-makers, opinion leaders, and the public tend to believe that AI is already an immensely powerful universal technology, limitless in its possibilities. However, while machine learning (ML), the principal computer science tool underlying today’s AI breakthroughs, is indeed powerful, ML is fundamentally a form of context-dependent statistical inference and as such has its limits. Specifically, because ML relies on correlations between inputs and outputs or emergent clustering in training data, today’s AI systems can only be applied in well-specified problem domains, still lacking the context-sensitivity of a typical toddler or house-pet. Consequently, instead of constructing policies to govern artificial general intelligence (AGI), decision-makers should focus on the distinctive and powerful problems posed by narrow AI, including misconceived benefits and the distribution of benefits, autonomous weapons, and bias in algorithms. AI governance, at least for now, is less about managing super-intelligent systems than about managing those who would create and deploy them and supporting the application of AI to narrow, well-defined problem domains. 
 
Specific implications of our discussion are as follows: 
 
• AI applications are part of a suite of intelligent tools and systems, and that ultimately must be regulated as a set. Digital platforms, for example, generate the pools of big data on which AI tools operate and hence, the regulation of digital platforms and of big data is part of the challenge of governing AI. Many of the platform offerings are, in fact, deployments of AI tools. Hence, focusing on AI alone distorts the governance problem. 
 
• Simply declaring objectives – be they digital privacy, transparency, or avoiding bias – is not sufficient. We must decide what the goals actually will be in operational terms. 
 
• The issues and choices will differ by sector. The consequences, for example, of bias and error will differ from a medical domain or a criminal justice domain to one of retail sales. 
 
• The application of AI tools in public policy decision making, in the design of transport or waste disposal or policing, or in a whole variety of domains, requires great care. There is a substantial risk of confusing efficiency with public debate about what the goals should be in the first place. Indeed, public values evolve as part of social and political conflict. 
 
• The economic implications of AI applications are easily exaggerated. Should public investment be concentrated on advancing basic research or on the diffusion of tools and the user interfaces and training needed to implement them? 
 
As difficult as it will be to decide on goals and a strategy to implement the goals in one community, let alone regional or international communities, any agreement that goes beyond simple statements of hoped for outcomes is very unlikely.",,2020.0,10.2139/ssrn.3681088,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d198e46463e2c883a3f04a0aec699fc17296b55e,https://www.semanticscholar.org/paper/d198e46463e2c883a3f04a0aec699fc17296b55e,"Introduction to the Artificial Intelligence and Big Data Analytics Management, Governance, and Compliance Minitrack","Artificial Intelligence (AI) and Big Data applications are becoming increasingly important strategic assets as they enable organizations to differentiate from their competitors by offering new data-driven products and services, by achieving increased agility in operations and decision making, by enhancing the discovery of new business insights, and by augmenting decision making and acting on decisions in a faster, more streamlined manner. As organizations become more reliant on AI and data-driven models for insight, decision making and action, they need new theories, frameworks and methodologies that can help them in the following areas: • Deploying effective strategies and policies for managing AI and Big Data • Streamlining processes to develop and deploy analytical models and Machine Learning (ML) algorithms • Designing new KPIs and deploying actionable dashboards • Managing and staffing AI, ML and data science teams • Structuring analytics functions/capabilities within organizations • Designing, staffing and providing direction to AI, data and analytics governance committees • Managing AI and Big Data Analytics project and deployment risk and • Advancing AI and Big Data capability maturity",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
69f91edaf5abc963a9b9b121d1f8bc851333e133,https://www.semanticscholar.org/paper/69f91edaf5abc963a9b9b121d1f8bc851333e133,Active Learning in the Era of Big Data,"Active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning. Because of this, real-world testing and comparing active learning algorithms requires collecting new datasets (adaptively), rather than simply applying algorithms to benchmark datasets, as is the norm in (passive) machine learning research. To facilitate the development, testing and deployment of active learning for real applications, we have built an open-source software system for large-scale active learning research and experimentation. The system, called NEXT, provides a unique platform for realworld, reproducible active learning research. This paper details the challenges of building the system and demonstrates its capabilities with several experiments. The results show how experimentation can help expose strengths and weaknesses of active learning algorithms, in sometimes unexpected and enlightening ways.",,2015.0,10.2172/1225849,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6af4abe9f3360b2817db4a655e1d3486754ba6b0,https://www.semanticscholar.org/paper/6af4abe9f3360b2817db4a655e1d3486754ba6b0,Large-scale learning for media understanding,,EURASIP J. Image Video Process.,2015.0,10.1186/S13640-015-0080-7,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f353686614dcd8f1afe5e8c570b5612a18a473f3,https://www.semanticscholar.org/paper/f353686614dcd8f1afe5e8c570b5612a18a473f3,Releasing eHealth Analytics into the Wild: Lessons Learnt from the SPHERE Project,"The SPHERE project is devoted to advancing eHealth in a smart-home context, and supports full-scale sensing and data analysis to enable a generic healthcare service. We describe, from a data-science perspective, our experience of taking the system out of the laboratory into more than thirty homes in Bristol, UK. We describe the infrastructure and processes that had to be developed along the way, describe how we train and deploy Machine Learning systems in this context, and give a realistic appraisal of the state of the deployed systems.",KDD,2018.0,10.1145/3219819.3219883,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
641471adf1857a3110c3e37eb9c8c9b949923d56,https://www.semanticscholar.org/paper/641471adf1857a3110c3e37eb9c8c9b949923d56,High-throughput experiments facilitate materials innovation: A review,,Science China Technological Sciences,2019.0,10.1007/S11431-018-9369-9,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
17d429b405a0951ebf3c729570a476977e412000,https://www.semanticscholar.org/paper/17d429b405a0951ebf3c729570a476977e412000,#MeTooMaastricht: Building a chatbot to assist survivors of sexual harassment,,PKDD/ECML Workshops,2019.0,10.1007/978-3-030-43823-4_41,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b73946a81c35bee0e4fe5d2a28ff479abaac8721,https://www.semanticscholar.org/paper/b73946a81c35bee0e4fe5d2a28ff479abaac8721,A Genetic Algorithm-based AutoML Approach for Large-scale Traffic Speed Prediction,"With the continuous innovation of computer science as well as the big data acquisition technology, machine learning (ML), developed as a state-of-art framework, now has been comprehensively applied in speed prediction tasks. However, ML methods usually require intensive hyper-parameter tuning, which hinders the practical deployment of ML models. In view of this, this paper proposes an automated machine learning (AutoML) framework for speed prediction, which enables the prediction work to be accomplished in a much more timesaving and convenient way as well as in high prediction accuracy. The proposed framework utilizes the Genetic Algorithm (GA) following its four major procedures: Genome coding, Crossover, Mutation and Selection to automatically search for the optimal neural network architectures and hyperparameters. The proposed framework is examined on a real-world large-scale dataset in the city of Berlin, Germany. The experimental results demonstrate that the proposed method outperforms other benchmarking methods by a significant margin. Sensitivity analysis is also conducted to show the robustness of the proposed method. This study demonstrates the great penitential of using AutoML in traffic speed prediction and other related transportation applications.",2020 IEEE 5th International Conference on Intelligent Transportation Engineering (ICITE),2020.0,10.1109/ICITE50838.2020.9231486,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fed210352fe362d9962a9de21a9ec8e8fbd4f842,https://www.semanticscholar.org/paper/fed210352fe362d9962a9de21a9ec8e8fbd4f842,Characteristic Latent Features for Analyzing Digital Mental Health Interaction and Improved Explainability (Preprint),"
 BACKGROUND
 Using mobile health technology has sparked a broad engagement of data science and machine learning methods to leverage the complex, assorted amount of data for mental health purposes. Despite many studies, there is a reported underdevelopment of user engagement concepts, and the desire for high accuracy or significance has shown to lead to low explicability and irreproducibility.
 
 
 OBJECTIVE
 To overcome such reasons of poor analysis input and facilitate the reproducibility and credibility of artificial intelligence applications, we aim to explore principal characteristics of user interaction with digital mental health.
 
 
 METHODS
 We generated five latent features based on previous research, expert opinions from digital mental health, and informed by data. The features were analyzed with descriptive statistics and data visualization. We carried out two rounds of evaluations with data from 12,400 users of IntelliCare, a mental health platform with 12 apps. First, we focused to proof concept and second, we assessed reproducibility by drawing conclusion from distribution differences. User data was drawn from both research trials and public deployment on Google Play.
 
 
 RESULTS
 Our algorithms showed advantages over commonly used concepts and reproduce in our public data set with different underlying behavioral strategies. These measures relate to the distribution of a user’s allocated attention, users’ circadian behavior, their consecutive commitment to a specific strategy, and users’ interaction trajectory. Because distributions between research trial and public deployment were similar, consistency was implied regarding the underlying behavioral strategies: psychoeducation and goal setting are used as a catalyst to overcome the users’ primary obstacles, sleep hygiene is addressed most regularly, while regular self-reflective thinking is avoided. Relaxation as well as cognitive reframing have increased variance in commitment among public users, indicating the challenging nature of these apps. The relative course of users’ engagement is similar in research and public data.
 
 
 CONCLUSIONS
 We argue that deliberate, a-priori feature engineering is essential for reproducible, tangible, and explainable study analyses. Our features enable improved results as well as interpretability, providing an increased understanding of how people engage with multiple mental health apps over time. Since we based the generation of features on generic interaction, these methods are applicable to further methods of study analysis and digital health.
",,2020.0,10.21203/rs.3.rs-68076/v1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
16759109cab96eb9c15ce5dd9c1205ca6071c033,https://www.semanticscholar.org/paper/16759109cab96eb9c15ce5dd9c1205ca6071c033,Planting trees at the right places: Recommending suitable sites for growing trees using algorithm fusion,"Large-scale planting of trees has been proposed as a low-cost natural solution for carbon mitigation, but is hampered by poor selection of plantation sites, especially in developing countries. To aid in site selection, we develop the ePSA (e-Plantation Site Assistant) recommendation system based on algorithm fusion that combines physics-based/traditional forestry science knowledge with machine learning. ePSA assists forest range officers by identifying blank patches inside forest areas and ranking each such patch based on their tree growth potential. Experiments, user studies, and deployment results characterize the utility of the recommender system in shaping the long-term success of tree plantations as a nature climate solution for carbon mitigation in northern India and beyond.",ArXiv,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c26fc9a1ccc05d8e1a025a8866a14f29ef595b8c,https://www.semanticscholar.org/paper/c26fc9a1ccc05d8e1a025a8866a14f29ef595b8c,Artificial Intelligence for Social Good,"The Computing Community Consortium (CCC), along with the White House Office of Science and Technology Policy (OSTP), and the Association for the Advancement of Artificial Intelligence (AAAI), co-sponsored a public workshop on Artificial Intelligence for Social Good on June 7th, 2016 in Washington, DC. This was one of five workshops that OSTP co-sponsored and held around the country to spur public dialogue on artificial intelligence, machine learning, and to identify challenges and opportunities related to AI. In the AI for Social Good workshop, the successful deployments and the potential use of AI in various topics that are essential for social good were discussed, including but not limited to urban computing, health, environmental sustainability, and public welfare. This report highlights each of these as well as a number of crosscutting issues.",ArXiv,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1a529fe5a3a5c05ce2a363ba2370e552cd09ae4d,https://www.semanticscholar.org/paper/1a529fe5a3a5c05ce2a363ba2370e552cd09ae4d,MagneToRE: Mapping the 3-D Magnetic Structure of the Solar Wind Using a Large Constellation of Nanosatellites,"Unlike the vast majority of astrophysical plasmas, the solar wind is accessible to spacecraft, which for decades have carried in-situ instruments for directly measuring its particles and fields. Though such measurements provide precise and detailed information, a single spacecraft on its own cannot disentangle spatial and temporal fluctuations. Even a modest constellation of in-situ spacecraft, though capable of characterizing fluctuations at one or more scales, cannot fully determine the plasma’s 3-D structure. We describe here a concept for a new mission, the Magnetic Topology Reconstruction Explorer (MagneToRE), that would comprise a large constellation of in-situ spacecraft and would, for the first time, enable 3-D maps to be reconstructed of the solar wind’s dynamic magnetic structure. Each of these nanosatellites would be based on the CubeSat form-factor and carry a compact fluxgate magnetometer. A larger spacecraft would deploy these smaller ones and also serve as their telemetry link to the ground and as a host for ancillary scientific instruments. Such an ambitious mission would be feasible under typical funding constraints thanks to advances in the miniaturization of spacecraft and instruments and breakthroughs in data science and machine learning.",Frontiers in Astronomy and Space Sciences,2021.0,10.3389/fspas.2021.665885,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4cb659cf5a8dea8b86263c65ec613f48c2426c51,https://www.semanticscholar.org/paper/4cb659cf5a8dea8b86263c65ec613f48c2426c51,"Analyzing Dilemmas Posed by Artificial Intelligence and 4IR Technologies Requires using all Available Models, Including the Existing International Human Rights Framework and Principles of AI Ethics","We are living in the epoch referred to as the ‘4th industrial revolution'. The 4th industrial Revolution (4IR) is a development characterized by a fusion of technologies that blur the digital, physical, and biological spheres (e.g., cyberspace, virtual and augmented reality, body-machine interface and robotics). 
 
Certain is the guaranteed ubiquitous adoption of these technologies, and futurism. Where the former is a reference to the increasing use and normalization of such technologies in everyday life, government service provision and industry. The latter is a reference to the philosophical/science fiction discussions that are emerging as a result of these changes (e.g. debates around the ‘singularity’, transhumanism, and posthumanism – often presented in utopian/dystopia terms). As such, the definition of digital ethics can be expanded and expressed in terms of the impacts of new digital technologies, through analysis of potential opportunities and risks in contemporary and future contexts. 
 
Many are working on forward‑looking policy frameworks and governance protocols, with broad multistakeholder engagement and buy‑in, to accelerate the adoption of emerging technologies in the global public interest, such as artificial intelligence (AI) and machine learning (ML) blockchain, 5G, data analytics, quantum computing, autonomous vehicles, synthetic biology, the internet of things (IoT), and killer robots or autonomous weapons systems (AWS). We have gained insight into the unequal distribution of the positive and negative impacts of AI on human rights throughout society, and have begun to explore the power of the human rights framework to address these disparate impacts. 
 
Although internationally recognized laws and standards on human rights provide a common standard of achievement for all people in all countries, more work is needed to understand how they can be best applied in the context of disruptive technology. 
 
AI systems raise myriad questions for society and democracy, only some of which are covered or addressed by existing laws. In order to fill these perceived gaps, a vocal group of governments, industry players, academics, and civil society actors have been promoting principles or frameworks for ethical AI. 
 
COVID-19 accelerated the use of AI in all countries and all fields. The pandemic accelerated the transition to a society that is increasingly based on the use of AI. This also increased the threats new risks related to human rights in the context of AI deployment. The human rights implications of governments' aggressive measures targeting the spread of COVID-19-related misinforation is also discussed. 
 
The question of whether corporations can act ethically is particularly relevant for Big Tech. Many of these firms are oligopolies that individuals and governments alike depend on completely, though they have little to no capacity to independently remedy issues when they arise, as Project Maven showed. Artificial intelligence and automated decision-making tools are increasing in power and centrality, and technology companies retain large troves of private data that it sells. These companies are at the forefront of technological innovation and may be caught up with the factual question of what can be done rather than the normative question of whether it should be done. All these issues arise in a field where there is little to no government regulation or intervention. The threats AI poses to society are so new, that the legal system is struggling to impose sufficient values and restrictions. Thus, a coherent approach to addressing AI ethics, values and consequences is, indeed, urgently needed. 
 
In May 2019, 42 countries adopted the Organization for Economic Co-operation and Development (OECD) AI Principles, a legal recommendation that includes five principles and five recommendations related to the use of AI. To ensure the successful implementation of the Principles, the OECD launched the AI Policy Observatory in February 2020. The Observatory publishes practical guidance about how to implement the AI Principles, and supports a live database of AI policies and initiatives globally. It also compiles metrics and measurement of global AI development and uses its convening power to bring together the private sector, governments, academia, and civil society. 
 
The AI ethics and governance initiatives discussed are cause for optimism that the global community will use all available models and brainpower for analysis and ultimately global governance of AI.",SSRN Electronic Journal,2021.0,10.2139/ssrn.3874279,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e2ae8e565a6cc5f1cb206632bc837402a059be32,https://www.semanticscholar.org/paper/e2ae8e565a6cc5f1cb206632bc837402a059be32,Bail or Jail? Judicial Versus Algorithmic Decision-Making in the Pretrial System,"To date, there are approximately sixty risk assessment tools deployed in the criminal justice system. These tools aim to differentiate between low-, medium-, and high-risk defendants and to increase the likelihood that only those who pose a risk to public safety or who are likely to flee are detained. Proponents of actuarial tools claim that these tools are meant to eliminate human biases and to rationalize the decision-making process by summarizing all relevant information in a more efficient way than can the human brain. Opponents of such tools fear that in the name of science, actuarial tools reinforce human biases, harm defendants’ rights, and increase racial disparities in the system. The gap between the two camps has widened in the last few years. Policymakers are torn between the promise of technology to contribute to a more just system and a growing movement that calls for the abolishment of the use of actuarial risk assessment tools in general and the use of machine learning-based tools in particular. 
This paper examines the role that technology plays in this debate and examines whether deploying artificial intelligence (“AI”) in existing risk assessment tools realizes the fears emphasized by opponents of automation or improves our criminal justice system. It focuses on the pretrial stage and examines in depth the seven most commonly used tools. Five of these tools are based on traditional regression analysis, and two have a machine-learning component. This paper concludes that classifying pretrial risk assessment tools as AI-based tools creates the impression that sophisticated robots are taking over the courts and pushing judges from their jobs, but that impression is far from reality. Despite the hype, there are more similarities than differences between tools based on traditional regression analysis and tools based on machine learning. Robots have a long way to go before they can replace judges, and this paper does not argue for replacement. The long list of policy recommendations discussed in the last chapter highlights the extensive work that needs to be done to ensure that risk assessment tools are both accurate and fair toward all members of society. These recommendations apply regardless of whether machine learning or regression analysis is used. Special attention is paid to assessing how machine learning would impact those recommendations. For example, this paper argues that carefully detailing each of the factors used in the tools and including multiple options to choose from (i.e., not just binary “yes-or-no” questions) will be useful for both regression analysis and machine learning. However, machine learning would likely lead to more personalized and meaningful scoring of criminal defendants because of the ability of machine learning techniques to “zoom in” on the unique details of each individual case.",,2020.0,10.7916/STLR.V21I2.6838,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2e227dd6892fb34fbcd2af7e59c1d2c459f86e9a,https://www.semanticscholar.org/paper/2e227dd6892fb34fbcd2af7e59c1d2c459f86e9a,Estimating Principal Components under Adversarial Perturbations,"Robustness is a key requirement for widespread deployment of machine learning algorithms, and has received much attention in both statistics and computer science. We study a natural model of robustness for high-dimensional statistical estimation problems that we call the adversarial perturbation model. An adversary can perturb every sample arbitrarily up to a specified magnitude $\delta$ measured in some $\ell_q$ norm, say $\ell_\infty$. Our model is motivated by emerging paradigms such as low precision machine learning and adversarial training. 
We study the classical problem of estimating the top-$r$ principal subspace of the Gaussian covariance matrix in high dimensions, under the adversarial perturbation model. We design a computationally efficient algorithm that given corrupted data, recovers an estimate of the top-$r$ principal subspace with error that depends on a robustness parameter $\kappa$ that we identify. This parameter corresponds to the $q \to 2$ operator norm of the projector onto the principal subspace, and generalizes well-studied analytic notions of sparsity. Additionally, in the absence of corruptions, our algorithmic guarantees recover existing bounds for problems such as sparse PCA and its higher rank analogs. We also prove that the above dependence on the parameter $\kappa$ is almost optimal asymptotically, not just in a minimax sense, but remarkably for every instance of the problem. This instance-optimal guarantee shows that the $q \to 2$ operator norm of the subspace essentially characterizes the estimation error under adversarial perturbations.",COLT,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
066a1de44f2bea4eb897f5b7f313fa22311facee,https://www.semanticscholar.org/paper/066a1de44f2bea4eb897f5b7f313fa22311facee,The practice of developing the academic cloud using the Proxmox VE platform,"Cloud technologies provide users with efficient and secure tools for data management, computing, storage and other services. The article analyzes the projects for the introduction of cloud technologies in education and identifies the main advantages and risks in creating a cloud infrastructure for the university. Such startups contribute to the formation of a new paradigm of education. It involves the virtualization of education, the introduction of mobile and blended learning, ie the combination of cloud computing with modern learning concepts. In this paper, we highlight our experience in improving the academic cloud for the training of a bachelor's degree in computer science. This is through the integration of the Proxmox VE platform into existing computing power by deploying the Proxmox VE system. In the study, we reveal some technical and methodological aspects of the organization of the educational process using this corporate cloud platform. The scheme of the organization of physical components of cloud infrastructure (nodes, virtual networks, routers, domain controller, VPN server, backup system of students' virtual machines) is given. All characteristics of this environment and possibilities of their application are studied.",Educational Technology Quarterly,2022.0,10.55056/etq.36,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1a0cf3eba22c053ad21503e034baf3178899258a,https://www.semanticscholar.org/paper/1a0cf3eba22c053ad21503e034baf3178899258a,Design and Deployment of a Data Lake at a Pilot Plant Scale for a Smart Electropolishing Process,"In order to remain competitive and satisfy the demands of today’s customers in a timely manner, manufacturing industries are embracing the Industry 4.0 philosophy where automation is pushed beyond robotics to new technologies emerging from data science and artificial intelligence. The aim is to reduce time spent on none added value tasks and help learning from past experience in order to enhance efficiency and quality of manufacturing processes.
 Traditional industries, such as electropolishing, need to find ways to automate their, often heavily artisanal-based techniques and develop an intelligent network of machines and processes taking advantage of information and communication technology such as Big Data, IoT (Internet of Things), or Artificial Intelligence (AI). This digital transition can be realized through the application of an IIoT (Industrial Internet of Things) platform that constructs a massive, sophisticated information network of interconnected sensors, equipment, and processes known as cyber-physical systems.
 Within this network, large amounts of data (for example process bath attributes such as temperature or viscosity and part characteristics such as roughness or brightness) can be collected automatically via sensors and through user-friendly applications from manual measurements and observations. All data are uploaded automatically into a cloud-based data storage system. In order for this collected information to be useful, the data needs to be processed to allow pattern discovery and extraction of useful information regarding the system performance, probable faults in the process, and product quality. Besides others, machine learning algorithms play a key role in extracting useful information.
 Classification and processing of such massive, diverse, and rapidly arriving data sets are known to be challenging. As a result, the concept of data lake has arisen in the last decade as an appealing and cost-effective approach for companies to manage large amounts of data. It consists of a large repository of datasets designed to transform raw and unstructured data into structured, usable information to allow further processing. A data lake, organized typically in four layers (ingestion, distillation, processing, and insights layers), stores both old and near real-time data in one location for initial assessment, with comprehensive data organization, analysis, and visualization being performed only when necessary 1,2. This promotes agility by allowing data to be accessed by everyone in the company. 2
 
 In this work, a data lake is designed and implemented in conjunction with a pilot plant to demonstrate how in the electropolishing process of stainless-steel samples in an aging electrolyte, data can be collected and organized for further processing using machine learning techniques in order to optimize the process and part quality based on the data analysis results.
 References: 
 
 N. Miloslavskaya and A. Tolstoy, Procedia Comput. Sci., 88, 300–305 (2016).
 
 
 H. Fang, in 2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER),, p. 820–824 (2015).
 
 
 
 
 
 
 
 
 Figure 1
",ECS Meeting Abstracts,2022.0,10.1149/ma2022-01251219mtgabs,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
64ebaaddd9152703c174fe47107ebb399cf73923,https://www.semanticscholar.org/paper/64ebaaddd9152703c174fe47107ebb399cf73923,"Editorial on Special Issue: ""Trends and Developments on Type-2 Fuzzy Sets and Systems""",,Int. J. Fuzzy Syst.,2021.0,10.1007/s40815-021-01124-8,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a1d831cdd9ed4b4a7b56037d13b630bb5b272f1e,https://www.semanticscholar.org/paper/a1d831cdd9ed4b4a7b56037d13b630bb5b272f1e,A Dynamic Programming Heuristic for Dense Hessian Chain Bracketing,"Second derivatives of mathematical models for realworld phenomena are fundamental ingredients of a wide range of numerical simulation methods including parameter sensitivity analysis, uncertainty quantification, nonlinear optimization and model calibration. The evaluation of such Hessians often dominates the overall computational effort. Various combinatorial optimization problems can be formulated based on the highly desirable exploitation of the associativity of the chain rule of differential calculus. The fundamental Hessian Accumulation problem aiming to minimize the number of floating-point operations required for the computation of a Hessian turns out to be NP-complete. The restriction to suitable subspaces of the exponential search space proposed in this paper ensures computational tractability while yielding improvements by factors of ten and higher over standard approaches based on second-order tangent and adjoint algorithmic differentiation. Motivated by second-order parameter sensitivity analysis of surrogate numerical models obtained through training and pruning of deep neural networks this paper focusses on bracketing of dense Hessian chain products with the aim of minimizing the total number of floating-point operations to be performed. The results from a given dynamic programming algorithm for optimized bracketing of the underlying dense Jacobian chain product are used to reduce the computational cost of the corresponding Hessian. Minimal additional algorithmic effort is required. 1 Motivation and Introduction The increase in compute power due to the ongoing rapid evolution of (massively parallel) computer systems has been boosting numerical simulation as one of the fundamental branches of science and engineering. State of the art simulation software often operates at the limits of the available computer infrastructure. Simulation times as well as energy consumption can reach barely feasible levels. Triggered by the same progress in computer techcorresponding author: naumann@stce.rwth-aachen.de both: Informatik 12: Software and Tools for Computational Engineering, RWTH Aachen University, 52056 Aachen, Germany. nology stochastic machine learning (ML) methods have been gaining popularity over the past decade in particular [11]. Considerable research and development effort is put into the training and optimization of ML surrogates for computationally expensive numerical simulations [2, 18, 21]. For example, deep neural networks (DNN) are often used to substitute the target simulation. For the purpose of this paper a DNN is assumed to implement a twice (not necessarily continuously) differentiable multivariate vector function F : R → R : x 7→ y = F (x) as a sequence of evaluations of q > 0 elemental functions Fi : R ni−1 → Ri : vi−1 7→ vi = Fi(vi−1) for i = 1, . . . , q, v0 = x and y = vq yielding a layered structure of F as (1.1) F = Fq ◦ Fq−1 ◦ Fq−2 ◦ · · · ◦ F1 . Moreover, F[i,j) ≡ Fi◦· · ·◦Fj+1 implies Fi = F[i,i−1) and F = F[q,0). We use = to denote mathematical equality and ≡ in the sense of “is defined as.” The elemental Hessians F ′′ i = F ′′ i (vi−1) ≡ dFi dv2 i−1 (vi−1) ∈ R ni×ni−1×ni−1 are assumed to be dense. Potential symmetry in the two trailing dimensions due to second-order continuous differentiability is not exploited at this stage. Pruning methods for reducing the size of the DNN [6] often yield varying cardinalities ni, i = 0, . . . , q, n0 = n, nq = m, of the individual layers. For example, significance analysis based on interval expansions of adjoints computed by Algorithmic differentiation (AD) [14, 16] was applied successfully to the identification of insignificant intermediate values in [1]. The chain rule of differential calculus yields (1.2) F ′ = F (x) = F ′ q · F ′ q−1 · . . . · F ′ 1",ArXiv,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
46c5240831bc4a8c61a601f8ada58522a9b8f491,https://www.semanticscholar.org/paper/46c5240831bc4a8c61a601f8ada58522a9b8f491,Current Challenges and Future Research Directions in Augmented Reality for Education,"Innovation in formal and practical learning is an accepted progression and its adoption in learning methodologies is a sign that a respective society is open to new technologies, ideas, and, thus, to advancement. The latest innovation in teaching is the use of Augmented Reality. Applications using this technology have been deployed successfully in STEM(Science, Technology, Engineering, and Mathematics) education for delivering the practical and creative parts of the education. Since Augmented Reality technology has already a large volume of published studies about education that reports advantages, limitations, effectiveness, and challenges of AR in education, classifying these projects will allow for a review of the success of Augmented Reality integration in the different educational settings and discover current challenges and future research areas.The main findings of this paper are the generation of a detailed taxonomy of the current literature which outlines the current challenges, the use of this taxonomy to identify future research areas, and finally to report on the development of two case studies that can highlight the first steps need to address these research areas. The result of this research ultimately is to detail the research gap that is needed to be filled to facilitate real-time touchless hand interaction, kinesthetic learning, machine learning agents with a remote learning pedagogy.",,2021.0,10.36227/techrxiv.16369224.v1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b8e80588a5e6fe2557a6c664355d7fafb44487a5,https://www.semanticscholar.org/paper/b8e80588a5e6fe2557a6c664355d7fafb44487a5,TAMIZHİ: Historical Tamil-Brahmi Script Recognition Using CNN and MobileNet,"Computational epigraphy is the study of an ancient script where the computer science and mathematical model is relatively built for epigraphy. The Tamil-Brahmi inscriptions are the most ancient of the extant written of the Tamil. The inscriptions furnish valuable information on many aspects of life in the ancient Tamil country from a period anterior to the literary age of Sangam. The recognition of the script and systematic analysis of the script is required. The recognition of this script is complex, containing various curves for a single character and the style of writing overlap with curves and lines. Generating corpus of the script is necessary, since it is the initial step for computational epigraphy. The archaeological department has supported the raw data that helped to develop a corpus of Tamizhi. In this article, we have implemented a convolution neural network in various ways, i.e., (i) Training the CNN model from scratch a Softmax classifier in a sequential model (ii) using MobileNet: Transfer learning paradigm from a pre-trained model on a Tamizhi dataset (iii) Building Model with CNN and SVM (iv) SVM for evaluation of best accuracy to recognize handwritten Brahmi characters. To train the CNN Model an extensive TAMIZHİ handwritten Brahmi Dataset of 1lakh and 90,000 isolated samples for the character has been created and deployed. The designed dataset consists of 9 vowels and 18 consonants and 209 class so researchers can use machine learning. MobileNet outperformed among all the models implemented with the accuracy of 68.3%, whereas other algorithm ranges from 58% to 67% with respect to the Tamizhi dataset. MobileNet model is trained and tested for the dataset of vowels (8 class), consonants (18 class), and consonants vowels (26 class) with the accuracy of 98.1%, 97.7%, 97.5%, respectively.",ACM Transactions on Asian and Low-Resource Language Information Processing,2021.0,10.1145/3402891,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
974435ad374206b6107f47f63db707bfda7701f3,https://www.semanticscholar.org/paper/974435ad374206b6107f47f63db707bfda7701f3,INCEpTION - Corpus-based Data Science from Scratch,"In recent years, corpus-based data science has seen rapid adoption both in science and in industry. Developing corpusbased models for text mining from scratch has penetrated a huge number of application fields. This renders common approached to corpus annotation unscalable. Instead machineassisted annotation with a human-in-the loop is becoming crucial for the adoption of NLP by data scientists. INCEpTION [1] is a web-based annotation platform for machine-assisted annotation which provides such a tool. The platform targets users in any domain or application scenario that are in need of text that is annotated with specific categories and relations or linked to knowledge bases. It uses machine learning to provide annotation suggestions including activelearning driven guidance, thus improving annotator efficiency and quality. The modular architecture allows using different external annotation services to provide such suggestions. It supports entity disambiguation and linking, cross-document coreference, as well as fact linking using custom domainspecific RDF-based internal knowledge bases or using local or remote external knowledge bases through SPARQL. Annotation interoperability is ensured through the use of UIMA [2] as well as through the support of various annotation formats including CLARIN TCF [3]. At the level of the annotation scheme, the platform is compatible with the DKPro Core [4] type system facilitating interoperability with many of the NLP tools integrated within DKPro Core. INCEpTION is a multi-user platform. Users assume different roles (e.g. admin, project creator, normal user) on the platform as well as in individual projects (manager, annotator, adjudicator). User authorization can be delegated to an external mechanism users to be authenticated against infrastructure identity providers. This is essential for the deployment of the platform at the level of local or national infrastructures where it is used by users from many different organizations. Being a web-based tool these geographically distributed users can also conveniently collaborate on annotation projects within the platform. Further connectivity with other services is possible through a remote access API compatible with the OpenMinTeD AERO protocol1 that permits the automated setup and management of annotation projects. This allows projects to embed the annotation tool into a larger annotation campaign management",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d39c6aa002151972b0805a83e1b6f6d591885349,https://www.semanticscholar.org/paper/d39c6aa002151972b0805a83e1b6f6d591885349,Key Management and Governance Challenges when Executing Data Science / Analytics Projects,"Big data, data science and analytics have become increasingly important strategic assets because they can help organizations 
make better decisions, discover new insights, competitively differentiate, and they enable the embedding of intelligence into 
automated processes so organizations can efficiently respond at the speed of business. Effective organizational management 
and governance of data science practices are necessary in order to mitigate risks associated with analytics deployment. For 
example, organizations need to capture and manage critical meta-information detailing modeling and environmental 
assumptions underlying the analytics solutions, they also need to establish policies and a culture designed to ensure 
adherence to the highest ethical standards of data management and predictive model deployment. At a higher level, 
unleashing machine learning algorithms may require safeguards and risk mitigation monitoring to address these types of 
socio-technical challenges. This panel will foster a debate with respect to what are the most important concerns or potential 
issues that an organization should focus on while executing a data science/analytics project. Via a debate, the panel, along 
with the audience, will explore the field of data science and predictive analytics, and what are the key project risks that need 
to be mitigated.",AMCIS,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d7ba1ff78f3a0d45f1ca7429747ac346f0481614,https://www.semanticscholar.org/paper/d7ba1ff78f3a0d45f1ca7429747ac346f0481614,Introduction to the Minitrack on Implementation of Body Sensor Systems in Healthcare Practice,"Body sensor systems continue to evolve at a rapid pace. Their adoption into healthcare infrastructure is now beginning to parallel their speed of innovation. While most commercial systems have traditionally been consumer facing and centered around health and wellness, the COVID-19 pandemic has focused attention on the ability of these sensor systems to provide important health data in a new paradigm of medical care that emphasizes remote care. While the intention of this minitrack has always been to explore the integration and infrastructure required to deploy such body sensor systems in healthcare, these efforts have taken on a special emphasis in the setting of COVID-19. Pathways to adoption of health IT measures that may support many body sensor systems have been simplified, and there is increased interest even among traditionally conservative healthcare systems to pilot and adopt remote care systems. For example, to encourage remote care and maintain social distance in preventative healthcare visits, the United States Department of Health and Human Services issued a notice of enforcement discretion for telehealth remote communications during COVID-19 which has allowed healthcare providers to waive certain information security integration aspects of telehealth. The ultimate goal of this minitrack is to explore theoretical frameworks, formative qualitative work and demonstration projects that illustrate the integration of body sensors or health information technologies into clinical practice. Last year’s minitrack explored such deployments in innovative spaces to understand the contextual basis of activity and discover opportunities for academic engagement among secondary school students, as well as utilize a radar system adapted to locate and measure respiratory rate among patients in an emergency department. Both papers considered not only the design of such systems but also the multidisciplinary nature of teams required to bring such sensor systems into use. This year’s minitrack features three innovative papers that continue to describe the application of body sensor systems or information architecture to clinical care. While work in these papers was completed prior to pandemic spread of SARS-CoV-2, the findings around deployment of such systems and the application of machine learning algorithms is ever important and applicable to a new healthcare landscape focused on remote patient care. In our first paper, the authors present the construction of a probabilistic logic network trained on a portion of a typical medical toxicology practice to help diagnose common poisonings. Our second paper describes the use of a wearable biosensor to gather data around opioid use and the development of a machine learning algorithm that helps identify withdrawal from opioids. Finally, our third paper describes the technical deployment of a wearable sensor system to collect contactless vital signs in an emergency department observation unit. Overall, these three papers describe clinical applications of new technologies into healthcare. They all address specific aspects of implementation—training and development of artificial intelligence systems, identification of specific physiologic changes associated with medical conditions, and infrastructure requirements needed to isolate and discover technical challenges in the deployment of sensor systems. While we will miss the personal interactions and connections of HICSS this year, we hope that this minitrack continues to foster new ideas and pathways through which other researchers may consider developing body sensor systems. We look forward to fruitful discussion of the featured papers in this year’s minitrack and lively conversation around body sensor systems. We hope to re-convene in-person at HICSS55! Proceedings of the 54th Hawaii International Conference on System Sciences | 2021",HICSS,2021.0,10.24251/HICSS.2021.432,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10bbc5556fe6b79b18bb986af5e601b7ac7ab114,https://www.semanticscholar.org/paper/10bbc5556fe6b79b18bb986af5e601b7ac7ab114,LTF: A Label Transformation Framework for Correcting Target Shift,"Distribution shift is a major obstacle to the deployment of current deep learning models on realworld problems. Let Y be the target (label) and X the predictors (features). We focus on one type of distribution shift, target shift, where the marginal distribution of the target variable PY changes but the conditional distribution PX|Y does not. Existing methods estimate the density ratio between the sourceand target-domain label distributions by density matching. However, these methods are either computationally infeasible for large-scale data or restricted to shift correction for discrete labels. In this paper, we propose an end-to-end Label Transformation Framework (LTF) for correcting target shift, which implicitly models the shift of PY and the conditional distribution PX|Y using neural networks. Thanks to the flexibility of deep networks, our framework can handle continuous, discrete, and even multidimensional labels in a unified way and is scalable to large data. Moreover, for high dimensional X , such as images, we find that the redundant information in X severely degrades the estimation accuracy. To remedy this issue, we propose to match the distribution implied by our generative model and the target-domain distribution in a low-dimensional feature space that discards information irrelevant to Y . Both theoretical and empirical studies demonstrate the superiority of our method over previous approaches. UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW 2008, Australia School of Mathematics and Statistics, The University of Melbourne Department of Philosophy, Carnegie Mellon University. Correspondence to: Jiaxian Guo <jguo5934@uni.sydney.edu.au>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6405af1511b834653bf294fd6b44173fd9781d97,https://www.semanticscholar.org/paper/6405af1511b834653bf294fd6b44173fd9781d97,Interpreting Models of Student Interaction in Immersive Simulation Settings,"Immersive simulations are increasingly used for teaching and training in many societally important arenas including healthcare, disaster response and science education. The interactions of students in such settings leads to a complex array of emergent outcomes that present challenges for analysis. This paper studies a central element of such an analysis, namely the interpretability of models for inferring structure in time series data that are generated by the immersive simulations. This problem is explored in the context of modeling student interactions in an ecological-system simulation. Unsupervised machine learning is applied to data on system dynamics with the aim of helping teachers determine the effects of students’ actions on these dynamics. We address the question of choosing the optimal machine learning model, considering both statistical information criteria and interpretabilty quality. Our approach adapts two interpretability tests from the literature that measure the agreement between the model output and human judgment. The results of a user study show that the models that are the best understood by people are not those that optimize information theoretic criteria. This is a challenge for education settings as we cannot guarantee optimally interpretable models by choosing to optimise a statistical metric. We conclude that it is important to consider the interpretability of machine learning models as a separate optimization objective to statistical likelihood metrics when deploying models that hope to provide explanations of the complex dynamics occurring in rich embodied simulations.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d0f37dfa4d0ad91a0fc5957fa7b1c7a96015a791,https://www.semanticscholar.org/paper/d0f37dfa4d0ad91a0fc5957fa7b1c7a96015a791,Advances of Straddle Packer Microfrac Stress Measurements for Reservoir Development – The Pursuit of Subsurface Tectonic Strain Behavior,"
 The objective of this work is to highlight wireline straddle-packer microfrac testing is an underutilized technology today by the oil and gas industry despite these tests have evolved significantly in the last 10 years. This work also summaries the technological improvements and latest advances of microfrac service deployment in addition to share the future of in-situ reservoir stress monitoring from fiber-optic Distributive Strain Sensing (DSS).
 Over 500 microfrac tests and more than 30 decades of stress testing data are compiled and analyzed from science and data-collection pilot wells drilled around the world. The number of pressure tests collected by the industry is estimated by Baker Hughes’ database and competitor’s market share to compare the substantial difference between the number of reservoir pressure points and microfrac stress test collected every year for the last decade. Machine learning algorithms predict tectonic strain values to match microfrac formation breakdown and fracture closure using basic rock elastic properties to calculate the static stiffness of the formations where the stress tests are obtained.
 The microfrac success rate has increased from 20% to 85% in the last decade thanks to upgraded straddle packer tool capabilities and improved operational practices. The formation breakdown pressure data consistently indicates higher level of uncertainty than reservoir pore pressure. However, the industry collects several orders of magnitude more pore pressure points than microfrac stress tests every year. Possibly, this is the consequence of using basic effective in-situ stress ratio models by geomechanics practitioners that requires few calibration points from leak-off tests or borehole breakout modelling. This practice could treat microfracs as a nice-to-have calibration data rather than an essential subsurface tectonic stress information. A significant increase in microfrac testing is observed during the US shale gas revolution in order to calibrate stress profile models where basic effective stress ratio models failed to predict a lithology-dependent stress contrast between pay and non-pay intervals. The data shows the importance of using microfrac tests to calibrate subsurface tectonic strain values and predict accurate hydraulic fracture containment.
 The predicted tectonic strain data from microfrac testing shows values between 0.05 to 1.2 mStrain which can also be detected with current fiber optic technology using two centimeter grading and capable of detecting two micrometers of deformation. This new distributed strain sensing technology can be implemented to detect changes of stress and strain as the reservoir is developed by producer and injector wells. This technology may be the future of stress monitoring at the reservoir scale.",,2020.0,10.2523/iptc-20257-ms,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c83414b4deba5065606edb8a9ad2c4bd3aaa4fb2,https://www.semanticscholar.org/paper/c83414b4deba5065606edb8a9ad2c4bd3aaa4fb2,Application of Bellman's Equation in Ant-Like Robotic Device Path Decisions,"Swarm Intelligence is about emergency of collective intelligence from groups of homogeneous robotic devices deployed for a purpose. Ant Colony Systems, in particular, are inspiring. They commonly have drawn inspiration from the behaviors of real ants in nature in order to construct routes between the food sources and the nest. There are still gaps in alternative options for path decision in ant agents. Bellman's equation has been successfully used to solve path decision problems in machine learning. We proposed to investigate impact of a Bellman's equation inspired algorithm for path decision on stigmergic ant agent robotic devices. A design science research paradigm was used to design our research experiment in which a simulated environment was designed to simulate the behavior of ant agents when using a Bellman's equation inspired algorithm for path decision. We introduced a reward function to the orientation process of ant agents. Reward function rewards a decision made when an ant moves from one point to an adjacent cell. The Bellman's inspired algorithm for ant orientation led to convergence of ant agents even though there was reduced quality of convergence. Evaluation of results show that Bellman's equation can be used in path decision processes for ant agent robotic devices. Our results contributed to adding an alternative way of implementing path decision for ant agents. This will help in growing the knowledge around ant agents and finding better ways to implementing path decisions for ant agents.",2020 2nd International Multidisciplinary Information Technology and Engineering Conference (IMITEC),2020.0,10.1109/IMITEC50163.2020.9334099,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
edbcd0736eace4db643d48a1074d8d53c89b554a,https://www.semanticscholar.org/paper/edbcd0736eace4db643d48a1074d8d53c89b554a,Spatio-Temporal Variational Autoencoders,"The ubiquitous interest in deep latent variable models within the machine learning community has been fuelled by the development of the variational autoencoder (VAE). VAEs provide a framework for performing fast, scalable inference in deep latent variable models, facilitating their deployment on the large, multi-dimensional and richly structured datasets omnipresent in modern science and engineering. Increasingly large volumes of such data that also exhibit strong dependencies across space and time are arising from a wealth of domains, including environmental, social and earth sciences. Crucial to the advancement of these fields are the tools to effectively model spatio-temporal data. Despite their wide applicability, VAEs are ill-equipped to model such data. At the crux of this inadequacy is a deficiency in the probabilistic model, specifically, the assumption that observations are independent and identically distributed. In contrast to VAEs, Gaussian processes (GPs) are an extremely effective tool for modelling data that exhibits strong dependencies. Unfortunately, the power of GPs necessitates an often undermining computational burden scaling cubically with the number of data points and observed dimensions. This prohibits their application in the large data regime. Furthermore, GPs are comparatively inexpressive relative to ‘deep’ machine learning models, such as the VAE. Whilst the construction of more expressive GPs is possible, this is typically a hand-crafted process which only adds to the computational complexity, unlike the automatic feature learning intrinsic to deep models. This thesis seeks to unify the complementary strengths of VAEs and GPs, forming a novel family of VAEs for the effective modelling of spatio-temporal datasets. The amalgamation of the two models is natural; however, it requires careful consideration of approximate inference techniques to ensure the benefits of each are realised. We establish the theoretical framework for achieving this, paying particular attention to the preservation of structure in the approximate posterior and the principled handling of partially observed data. We provide an extension to the sparse GP literature, developing a scalable technique for the introduction of sparse approximations into our family of spatio-temporal VAEs. Finally, we demonstrate state-of-the-art performance relative to existing multi-output GP models and structured VAEs in a variety of experiments involving spatio-temporal datasets.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b50fe9e5c27e0af2a1d72c5a4576b7a36dc1bc82,https://www.semanticscholar.org/paper/b50fe9e5c27e0af2a1d72c5a4576b7a36dc1bc82,Azure Web Apps,,Microsoft Azure,2020.0,10.1007/978-1-4842-5958-0_12,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
558a9d5d37c7b3359b75a390e2f42286c69ea31c,https://www.semanticscholar.org/paper/558a9d5d37c7b3359b75a390e2f42286c69ea31c,Unlocking Completion Design Optimization Using an Augmented AI Approach,"
 An Augmented AI approach has been developed to optimize completion design parameters and access the full potential of unconventional assets by leveraging big data sculpting, domain-induced feature engineering, and robust and explainable machine learning models with quantified uncertainty. This method unlocks the full potential of a well using completion design parameters optimization that considers all the factors that impact well performance, geological characteristics, well trajectory, spacing, etc.
 By leveraging basin-level knowledge captured by big data sculpting with the use of uncertainty quantification, Augmented AI can provide quick and science-based answers for completion optimization, and also assess the full potential of an asset in unconventional reservoirs.
 By leveraging computer vision and natural language processing techniques, unstructured data from various sources were deciphered, combined and organized into a structured database. Imputation techniques were used to fill the gaps of missing data. With the Augmented AI approach, the median accuracies of IP and EUR predictions for new drills is around 90%, which often outperforms industry-standard type curving methods. With the explainable machine learning (ML) model, the direct impact of completion design parameters on well performance is deconvoluted among other parameters, such as engineering and geological attributes. The prediction also comes with an 80% confidence interval to quantify the prediction uncertainties, which allows for better risk management and confident business decision making. With the ML model and given economic inputs and metrics, many sensitivity analyses are performed to evaluate optimized completion design parameters. The proposed Augmented AI approach has been deployed to Eagle Ford wells.",,2020.0,10.2118/200000-ms,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
889bde2b576ccd2cb1de6b4b76104548abff8b5f,https://www.semanticscholar.org/paper/889bde2b576ccd2cb1de6b4b76104548abff8b5f,A Positive-Confidence based approach to Classifying Imbalanced data: A Case Study on Hepatitis,"With advancements in the fields of Information science and Machine Learning researchers are increasingly deploying prediction methods to biomedical data to aid with the decision-making process for the domain experts. A combined approach of utilising domain knowledge from experts and the information the data provides us can assist in better decision making. There are two major issues which arise with using such models for biomedical data, namely, class imbalance and lack of transparency in models. Another major issue is Feature selection also plays an important role in the prediction process as noisy data features can confuse the model and lead to a drop-in accuracy.",American Journal of Biomedical Science & Research,2020.0,10.34297/ajbsr.2020.08.001319,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f3ebbf27a0e37ab29a05df7fd75c7dc84928c284,https://www.semanticscholar.org/paper/f3ebbf27a0e37ab29a05df7fd75c7dc84928c284,Predictive Maintenance for Fault Diagnosis and Failure Prognosis in Hydraulic System,"The key role in developing and successfully achieving high-quality results in business is by minimising the cost and maximising the profits. This is possible only if proper resources are optimized and implemented. Failure prognosis is a part of predictive maintenance where data science field is involved in predicting the conditions of a system. With proper machine learning techniques, the monitoring devices can easily replace traditional monitoring devices. In this research, proper fault diagnosis is carried out on the components and the stable conditions of a hydraulic system. A dashboard is created where all the data point is explored by showing their distribution, the correlation matrix, their importance in predicting the conditions and the outliers. Chi-square test of independence is calculated to define the relationship between the categorical values. The scaling and dimensionality reduction step was done by using Quantile Transform Scalar and UMAP technique respectively. The model building and evaluating stages were implemented in Python where RandomisedSearchCV is used for hyperparameter optimization in six classification algorithms. Results showed that using gradient boosting decision trees algorithm helped in achieving greater accuracy than any other machine learning models. The web app was deployed for the research project using Heroku and the dashboard created for exploratory data analysis was published to web using Shiny apps in R.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
49a7d17490b6856769257c869c17539c471b59a2,https://www.semanticscholar.org/paper/49a7d17490b6856769257c869c17539c471b59a2,Forgotten AI? Advancing agent reasoning in models,"Understanding human intelligence has been the core driver of the field of AI/cognitive science and the accumulated knowledge, tools and algorithms are used in many ways. One of the oldest branches of AI concerns the development of agents that aim to reproduce human reasoning to learn more about human decision making and behaviours it leads to. Until this day these architectures, models and theories influence many scientific disciplines that operationalise them for advancing their field as well as informing policies. The rational actor model utilised in and deployed by economics (thus also called homo economicus) is a prominent example of this. The renewed interest in machine learning makes one almost forget the diversity that the label AI also encompasses and thus fail to make use of the complementary power of these diverse approaches and abilities in addressing real world problems. In this abstract, we illuminate a way for advancing human reasoning realism in models by describing the use of analytical tools can support agent reasoning to be context sensitive. We argue this supports the understanding of human reasoning as well as increases the awareness of advantages and dangers of using context sensitive (policy) models compared to the use of agents applying one reasoning mechanism.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7b172a138ddc3f668125edc7a2e095d64005ec03,https://www.semanticscholar.org/paper/7b172a138ddc3f668125edc7a2e095d64005ec03,Detection of frauds using local outlier factor and isolation algorithm for transaction information,"Several companies are able to identify fraudulent credit card transactions therefore that consumers not charged for objects that they look after not securing. Such problems can be attempted through Data Science and its significance, along with Machine Learning, cannot be excessive. In this paper proposes to illuminate the exhibiting of a information agreed expending machine learning through Credit Card Fraud Detection. In this Difficult comprises demonstrating earlier credit card transactions with the information of the ones that revolved out to be fraud. This prototypical is formerly used to distinguish whether a new transaction is fraudulent or not. Our impartial now is to perceive 100% of the fraudulent transactions while decreasing the improper deception arrangements. Fraud Detection is a representative sample of classification. In this procedure, we must focused on analyzing then pre-processing datasets as well as the deployment of several anomaly detection algorithms such as Local Outlier Factor and Isolation Forest algorithm for Transaction information. Keywords— Credit card fraud, applications of machine learning, data science, isolation forest algorithm, local outlier factor, automated fraud detection.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e2cc3dc022eab7ffe66bca920543cac0cb715280,https://www.semanticscholar.org/paper/e2cc3dc022eab7ffe66bca920543cac0cb715280,Advancing Sustainable Low-Carbon Energy through Convergence,"Accelerating the development and deployment of sustainable low-carbon energy solutions is required to continue to grow the availability of affordable, reliable and environmentally responsible energy for all, and make the US lead the energy transition world. The convergence of advancing the science and engineering of energy and sustainability, including carbon management, renewable energy, alternate fuels such as methanol and hydrogen, energy storage, and the circular economy, is necessary for driving this acceleration. Moreover, their alignment with policy and social drivers such as human behavior are crucial. Perhaps, the strongest accelerator for the transition relies on developing capabilities and competencies of the workforce with interdisciplinary tools. These include big data analytics, the integration of real-time artificial intelligence and machine learning with virtual and augmented reality to advance decision-making, and advanced technologies These technological and workforce assets must be complemented by aligning regulatory policy, public policy and business policy to improve and enhance the rapid adoption of such a sustainable energy paradigm.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e985b79a4a4efaca8e00ac840b16deb9f5edd671,https://www.semanticscholar.org/paper/e985b79a4a4efaca8e00ac840b16deb9f5edd671,Digging for the truth: the case for active annotation in evaluating the credibility of online medical information (Preprint),"
 BACKGROUND
 With the rapidly accelerating spread of dissemination of false medical information on the Web, the task of establishing the credibility of online sources of medical information becomes a pressing necessity. The sheer number of websites offering questionable medical information presented as reliable and actionable suggestions with possibly harmful effects poses an additional requirement for potential solutions, as they have to scale to the size of the problem. Machine learning is one such solution which, when properly deployed, can be an effective tool in fighting medical disinformation on the Web.
 
 
 OBJECTIVE
 We present a comprehensive framework for designing and curating of machine learning training datasets for online medical information credibility assessment. We show how the annotation process should be constructed and what pitfalls should be avoided. Our main objective is to provide researchers from medical and computer science communities with guidelines on how to construct datasets for machine learning models for various areas of medical information wars.
 
 
 METHODS
 The key component of our approach is the active annotation process. We begin by outlining the annotation protocol for the curation of high-quality training dataset, which then can be augmented and rapidly extended by employing the human-in-the-loop paradigm to machine learning training. To circumvent the cold start problem of insufficient gold standard annotations, we propose a pre-processing pipeline consisting of representation learning, clustering, and re-ranking of sentences for the acceleration of the training process and the optimization of human resources involved in the annotation.
 
 
 RESULTS
 We collect over 10 000 annotations of sentences related to selected subjects (psychiatry, cholesterol, autism, antibiotics, vaccines, steroids, birth methods, food allergy testing) for less than $7 000 employing 9 highly qualified annotators (certified medical professionals) and we release this dataset to the general public. We develop an active annotation framework for more efficient annotation of non-credible medical statements. The results of the qualitative analysis support our claims of the efficacy of the presented method.
 
 
 CONCLUSIONS
 A set of very diverse incentives is driving the widespread dissemination of medical disinformation on the Web. An effective strategy of countering this spread is to use machine learning for automatically establishing the credibility of online medical information. This, however, requires a thoughtful design of the training pipeline. In this paper we present a comprehensive framework of active annotation. In addition, we publish a large curated dataset of medical statements labelled as credible, non-credible, or neutral.
",JMIR Medical Informatics,2020.0,10.2196/preprints.25920,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
168734fcffe9606c16804ed1590dd117cdc1db77,https://www.semanticscholar.org/paper/168734fcffe9606c16804ed1590dd117cdc1db77,IEEE INFOCOM 2020 Workshops: IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS) - Program,"When accessing cloud-hosted modern applications, users often suffer a significant latency due to the long geo-distance to the central cloud. Edge computing thus emerges as an alternative paradigm that can reduce this latency by deploying services close to users. In this talk, we will analyze the methodology and limitations of popular approaches for supporting AI services on geo-distributed systems along the evolution from cloud computing to edge computing. In particular, we shall discuss how to deal with different sets of challenges in distributed machine learning over heterogeneous geo-distributed systems. We shall also present our recent studies on parameter-server based framework among networked collaborative edges. Monday, July 6 9:00 10:30 MobiSec_S1: Mobile Security Chairs: Lei Chen (Georgia Southern University, USA), Danda B. Rawat (Howard University, USA) A View-Invariant Feature Learning Model for Cross-View Security Authentication in Mobile Smart Devices Ao Li and Xin Liu (Harbin University of Science and Technology, China); Qiang Guan (Kent State University, USA); Deyun Chen (Harbin University of Science and Technology(HUST), China); Guanglu Sun (Harbin University of Science and Technology, China) Enterprise Mobile Device Management Requirements and Features Hina Batool and Ammar Masood (Air University, Pakistan) A Convolutional Neural Network-Based RF Fingerprinting Identification Scheme for Mobile Phones Sheng Wang, Linning Peng, Hua FU, Aiqun Hu and Xinyu Zhou (Southeast University, China) Comprehensive Detection of Vulnerable Personal Information Leaks in Android Applications Nattanon Wongwiwatchai, Phannawhat Pongkham and Kunwadee Sripanidkulchai (Chulalongkorn University, Thailand) Monday, July 6 9:00 9:15 OS: Opening Session Welcome from the TPC Co-Chairs Yanyong Zhang, Michael Zink Chairs: Yanyong Zhang (University of Science and Technology of China, China), Michael Zink (University of Massachsetts Amherst, USA) Monday, July 6 9:00 9:10 OS: Opening Session Message from Chairs Monday, July 6 9:00 10:40 S1: Network Planning A Dynamic Resource Allocation Scheme in Vehicular Communications Akinsola S Akinsanya and Manish Nair (University of Kent, United Kingdom (Great Britain)); Yijin Pan (Southeast University, China & University of Kent, United Kingdom (Great Britain)); Jiangzhou Wang (University of Kent, United Kingdom (Great Britain)) Resource Allocation in Drone-Assisted Emergency Communication System Tianqi Chen (University of Kent, United Kingdom (Great Britain)); Jian He (Shanghai Aerospace Electronic Technology Institute & Key Laboratory of Intelligent Computing Technology (SAST), China); Huiling Zhu (University of Kent, United Kingdom (Great Britain)); Lin Cai and Peng Yue (Xidian University, China); Jiangzhou Wang (University of Kent, United Kingdom (Great Britain)) Intelligent UAV Based Flexible 5G Emergency Networks: Field Trial and System Level Results Gao Yuan (Tsinghua University, China); Jiang Cao (Academy of Military Science of PLA, China); Ping Wang (Tsinghua University, China); Junsong Yin (Academy of Military Science of PLA, China); Ming He (CMCC, China); Ming Zhao (Tsinghua University, China); Mugen Peng (Beijing University of posts & Telecommunications, China); Su Hu (University of Electronic Science and Technology of China, China); Yunchuan Sun (Beijing Normal University, China); Jing Wang (Academy of Military Science of PLA, China); Shaochi Cheng, Yang Guo and Yanchang Du (CDSTIC, China); Yanxi Cai, Jinhui Huang and Kai Qiu (Academy of Military Science of PLA, China) An Integrated Platoon and UAV System for 3D Localization in Search and Rescue Hongming Zhang, Li Wang and Aiguo Fei (Beijing University of Posts and Telecommunications, China) Latency and Reliability Oriented Collaborative Optimization for Multi-UAV Aided Mobile Edge Computing System Xiangwang Hou and Zhiyuan Ren (Xidian University, China); Jingjing Wang (Tsinghua University, Beijing, China); Shuya Zheng and Hailin Zhang (Xidian University, China) Monday, July 6 9:00 9:05 WISARN-O: Opening Session Monday, July 6 9:05 10:00 Keynote Session 1: Tethered UAVs: Challenges, Potential, and Applications Prof. Mohamed-Slim Alouini (King Abdullah University of Science and Technology (KAUST), Saudi Arabia) Monday, July 6 9:05 10:20 S1: Traffic Optimization Algorithms Room: EdgeBlock Chairs: Tong Yang (Peking University, China), Qun Huang (Peking University, China), Haipeng Dai (Nanjing University & State Key Laboratory for Novel Software Technology, China), Yi Wang (Southern University of Science and Technology, China) S1.1 Dual Channel Per-packet Load Balancing for Datacenters Cong Xu, Tingqiu Yuan, Haibo Zhang and Tao Huang (HUAWEI Technologies Co., Ltd, China); Feilong Tang (Shanghai Jiao Tong University, China) S1.2 Predicting Traffic Demand Matrix by Considering Inter-flow Correlations Kaihui Gao and Dan Li (Tsinghua University, China); Li Chen (Huawei, Hong Kong); Jinkun Geng (Tsinghua University, China); Fei Gui (University of XiangTan, China); Yang Cheng and Yue Gu (Tsinghua University, China) S1.3 MFBBR:An Optimized Fairness-aware TCP-BBR Algorithm in Wired-cum-wireless Network Minghan Jia, Weifeng Sun, Zun Wang, Yaohua Yan, Hongyu Qin and Kelong Meng (Dalian University of Technology, China) Monday, July 6 9:05 10:00 Section 2: Blockchain Application 1 Room: EdgeBlock Chair: Yury Yanovich (Skolkovo Institute of Science and Technology, Russia) A Concurrent Weighted Communication Scheme for Blockchain Transaction Jiao Li (Xi'an Shiyou University, China); Li Chen (University of Louisiana at Lafayette, USA) Towards Blockchain-Based Reputation-Aware Federated Learning Muhammad Habib Ur Rehman and Khaled Salah (Khalifa University of Science and Technology, United Arab Emirates); Ernesto Damiani (Khalida University EBTIC, United Arab Emirates); Davor Svetinovic (Khalifa University of Science and Technology, United Arab Emirates) Monday, July 6 9:05 10:05 WISARN-K: Keynote Session Monday, July 6 9:10 9:50 S1: Data driven smart cities Network Flow based IoT Botnet Attack Detection using Deep Learning Sriram S (Amrita Vishwa Vidyapeetham, India); Vinayakumar R (Cincinnati Children's Hospital Medical Center, USA); Mamoun Alazab (Charles Darwin University, Australia); Soman K P (Amrita Vishwa Vidyapeetham, India) Blockchain-based E-waste Management in 5G Smart Communities Amit Dua (Birla Institute of Technology and Science (BITS) Pilani, India); Akash Dutta and Nishat Zaman (BITS Pilani, India); Neeraj Kumar (Thapar University Patiala, India) Monday, July 6 9:15 10:15 KN: Keynote FABRIC: Enabling Your Impossible Networking Experiments Paul Ruth, Renaissance Computing Institute (RENCI) Chair: Yanyong Zhang (WINLAB, Rutgers University, USA) Monday, July 6 9:15 10:30 S1: Physical Layer Room: TBD NLC: Natural Light Communication using Switchable Glass Changshuo Hu, Dong Ma and Mahbub Hassan (University of New South Wales, Australia); Wen Hu (the University of New South Wales (UNSW) & CSIRO, Australia) Smart User Pairing for Massive MIMO Enabled Industrial IoT Communications Jingjie Zong and Shuangzhi Li (Zhengzhou University, China); Di Zhang (Zhengzhou Univerisity, China); Gangtao Han and Xiaomin Mu (Zhengzhou University, China); Ali Kashif Bashir (Manchester Metropolitan University, United Kingdom (Great Britain)); Joel J. P. C. Rodrigues (Federal University of Piauí (UFPI), Brazil & Instituto de Telecomunicações, Portugal) Secure Backscatter Communications in Multi-Cell NOMA Networks: Enabling Link Security for Massive IoT Networks Wali Ullah Khan (School of Information Science and Engineering, Shandong University, Qingdao, China); Ju Liu (Shandong University, China); Furqan Jameel (Aalto University, Finland); Muhammad Toaha Raza Khan (Kyungpook National University, Korea (South)); Syed Hassan Ahmed (Department of Computer Science, Georgia Southern University, Statesboro, USA); Riku Jäntti (Aalto University, Finland) Monday, July 6 9:45 11:00 BlockSecSDN-1: Software-defined Networks Chair: Anish Jindal (University of Essex, United Kingdom (Great Britain)) BIND: Blockchain-Based Flow-Table Partitioning in Distributed Multi-Tenant Software-Defined Networks Ayan Mondal (Indian Institute of Technology, Kharagpur, India); Sudip Misra (Indian Institute of Technology-Kharagpur, India) DecOp: Decentralized Network Operations in Software Defined Networking using Blockchain Ephraim Moges and Tao Han (University of North Carolina at Charlotte, USA) Proof-of-Balance: Game-Theoretic Consensus for Controller Load Balancing of SDN Siyi Liao, Jun Wu and Jianhua Li (Shanghai Jiao Tong University, China); Ali Kashif Bashir (Manchester Metropolitan University, United Kingdom (Great Britain)) On the Design of Blockchain-Based Access Control Scheme for Software Defined Networks Durbadal Chattaraj (Indian Institute of Technology Kharagpur, India); Sourav Saha, Basudeb Bera and Ashok Kumar Das (International Institute of Information Technology, Hyderabad, India) Monday, July 6 10:00 11:00 K1: Keynote Session 1 Leveraging AI for Zero-Touch Automation in 6G: How to Address the Training Data Sparsity/Scarcity Challenge? Dr Ali Imran Despite the recent success of AI for enabling automation in other domains, in mobile networks attempts towards AI powered zero touch automation are hampered by a fundamental challenge: The sparsity and scarcity of the training data. Unlike many other applications of AI, real cellular data for training AI is both scarce and sparse. This is because operators generally do not test a wide range of parameters on live network, and whatever data they have cannot be extracted and shared easily. This limits the utility of some of the most powerful AI tools such as DNN for solving many practical problems in mobile networks. Without addressing this challenge explicitly and timely, despite the hype and hopes, full potential of AI may not be harnessed for emerging mobile networks. Leveraging insights from the speaker's involvement i",IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),2020.0,10.1109/infcomw.2019.8845217,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6f0f4fd6672af5f2aab9efaa2b6c7ebbe0f1d452,https://www.semanticscholar.org/paper/6f0f4fd6672af5f2aab9efaa2b6c7ebbe0f1d452,In-Platform CI/CD,,,2020.0,10.1007/978-1-4842-5611-4_4,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a2e91924c0bf7e38eecd0a357d4e1506e4c0b6c8,https://www.semanticscholar.org/paper/a2e91924c0bf7e38eecd0a357d4e1506e4c0b6c8,Evidence-Centered Design and Its Application to Collaborative Problem Solving in Practice-based Learning Environments,"Learning analytics is introducing a number of new techniques and frameworks for studying learning, including collaborative problem-solving processes. An increasing number of researchers are using data from students’ interactions with learning technologies to support the assessment of collaborative problem solving. Moreover, to create shared understanding among the multiple disciplines involved in learning analytics research (such as psychology, social psychology, the learning sciences, machine learning, statistics, and artificial intelligence) and those studying learning processes like collaborative problem solving, researchers recently started working on theoretical frameworks to share insights, receive feedback, and build on one another’s efforts. Evidence-centered design (ECD), an assessment design process that assessment designers use to articulate design goals and decisions, has been leveraged successfully as such a framework. ECD was shown to be very useful in framing large data sets generated in digital learning environments, offering data on students’ interactions with a system that made it possible to track and identify student learning processes such as “gaming the system”, “engagement”, “wheel spinning”, and “task persistence”. Although ECD has been shown to be useful for assessment design in digital learning environments, we argue that its potential is broader, that it is also applicable to multimodal learning analytics from practice-based learning environments. In this paper, we first present the practice-based learning environment that we are studying, and then we show how ECD could be deployed for the assessment of collaborative problem-solving​ processes in these learning environments.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
542438fa30d30cf9be1fd01b46ab1ad8dad1a06a,https://www.semanticscholar.org/paper/542438fa30d30cf9be1fd01b46ab1ad8dad1a06a,The Value of Big Data for Urban Science,"Introduction The past two decades have seen rapid advances in sensors, database technologies, search engines, data mining, machine learning, statistics, distributed computing, visualization, and modeling and simulation. These technologies, which collectively underpin ‘big data’, are allowing organizations to acquire, transmit, store, and analyze all manner of data in greater volume, with greater velocity, and of greater variety. Cisco, the multinational manufacturer of networking equipment, estimates that by 2017 there will be three networked devices for every person on the globe. The ‘instrumenting of society’ that is taking place as these technologies are widely deployed is producing data streams of unprecedented granularity, coverage, and timeliness. The tsunami of data is increasingly impacting the commercial and academic spheres. A decade ago, it was news that Walmart was using predictive analytics to anticipate inventory needs in the face of upcoming severe weather events. Today, retail (inventory management), advertising (online recommendation engines), insurance (improved stratification of risk), finance (investment strategy, fraud detection), real estate, entertainment, and political campaigns routinely acquire, integrate, and analyze large amounts of societal data to improve their performance. Scientific research is also seeing the rise of big data technologies. Large federated databases are now an important asset in physics, astronomy, the earth sciences, and biology. The social sciences are beginning to grapple with the implications of this transformation. The traditional data paradigm of social science relies upon surveys and experiments, both qualitative and quantitative, as well as exploitation of administrative records created for non-research purposes. Well-designed surveys generate representative data from comparatively small samples, and the best administrative datasets provide high-quality data covering a total population of interest. The opportunity now presents to understand how these traditional tools can be complemented by large volumes of ‘organic’ data that are being generated as a natural part of a modern, technologically advanced society. Depending upon how sampling errors, coverage errors, and biases are accounted for, we believe the combination can yield new insights into human behavior and social norms.",,2014.0,10.1017/CBO9781107590205.009,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
50b009de7c4ed0bcc492901da83ad2a9b867dc83,https://www.semanticscholar.org/paper/50b009de7c4ed0bcc492901da83ad2a9b867dc83,Another step toward demystifying deep neural networks,"The field of deep learning has positioned itself in the past decade as a prominent and extremely fruitful engineering discipline. This comeback of neural networks in the early 2000s swept the machine learning community, and soon after found itself immersed in practically every scientific, social, and technological front. A growing series of contributions established this field as leading to state-of-the-art results in nearly every task, recognizing image content, understanding written documents, exposing obscure connections in massive datasets, facilitating efficient search in large repositories, translating languages, enabling a revolution in transportation, revealing new scientific laws in physics and chemistry, and so much more. Deep neural networks not only solve known problems but offer, in addition, unprecedented results in deploying learning to problems that until recently were considered as hopeless or only weakly successful. These include automatically synthesizing text–media, creating musical art pieces, synthesizing realistic images and video, enabling competitive game-playing, and this list goes on and on.

Amazingly, all these great empirical achievements are obtained with hardly any theoretical foundations that could provide a clear justification for the architectures used, an understanding of the algorithms that accompany them, a clear mathematical reasoning behind the various tricks employed, and above all, the impressive results obtained. The quest for a theory that could explain these ingredients has become the Holy Grail of data sciences. Various impressive attempts to provide such a theory have started to appear, relying on ideas from various disciplines (see, e.g., refs. 1⇓⇓⇓⇓⇓⇓⇓–9). The paper by Papyan et al. (10) in PNAS adds an important layer to this vast attempt of developing a comprehensive theory that explains the behavior of deep learning solutions. In this Commentary, we provide a wide context to their results, highlight and clarify their contribution, and raise … 

[↵][1]1To whom correspondence may be addressed. Email: elad{at}cs.technion.ac.il.

 [1]: #xref-corresp-1-1",Proceedings of the National Academy of Sciences,2020.0,10.1073/pnas.2018957117,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0cfcc755f0290ee1110f5eb1fb61f48104a85f0d,https://www.semanticscholar.org/paper/0cfcc755f0290ee1110f5eb1fb61f48104a85f0d,Perspectives on Ethics of AI : Computer Science ∗,"AI is a collection of computational methods for studying human knowledge, learning, and behavior, including by building agents able to know, learn, and behave. Ethics is a body of human knowledge, far from completely understood, that helps agents (humans today, but perhaps eventually robots and other AIs) decide how they and others should behave. The ethical issues raised by AI fall into two overlapping groups. First, potential deployments of AI raise ethical questions about the impacts they may have on human well-being, just like other powerful tools or technologies such as nuclear power or genetic engineering. Second, unlike other technologies, intelligent robots and other AIs have the potential to be considered as members of our society. Since they will make their own decisions about the actions they take, it is appropriate for humans to expect them to behave ethically. This requires AI research with the goal of understanding the structure, content, and purpose of ethical knowledge, well enough to implement ethics in artificial agents. This chapter describes a computational view of the function of ethics in human society, and discusses its application to three diverse examples. ∗Draft chapter for the Oxford Handbook of Ethics of AI, edited by Markus Dubber, Frank Pasquale, and Sunit Das, to appear, 2019. †kuipers@umich.edu. Computer Science & Engineering, University of Michigan, Ann Arbor, Michigan 48109 USA 1 Why Is the Ethics of AI Important? AI uses computational methods to study human knowledge, learning, and behavior, in part by building agents able to know, learn, and behave. Ethics is a body of human knowledge that helps agents (humans today, but perhaps eventually robots and other AIs) decide how they and others should behave. The ethical issues raised by AI fall into two overlapping groups. First, like other powerful tools or technologies (e.g., genetic engineering or nuclear power), potential deployments of AI raise ethical questions about their impact on human well-being. Second, unlike other technologies, intelligent robots (e.g., autonomous vehicles) and other AIs (e.g., high-speed trading systems) make their own decisions about the actions they take, and thus could be considered as members of our society. Humans should be able to expect them to behave ethically. This requires AI research with the goal of understanding the function, structure, and content of ethical knowledge well enough to implement ethics in artificial agents. As the deployment of AI, machine learning, and intelligent robotics becomes increasingly widespread, these problems become increasingly urgent. 2 What is the Function of Ethics? “At the heart of ethics are two questions: (1) What should I do?, and (2) What sort of person should I be?”1 Ethics consists of principles for deciding how to act in various circumstances, reflecting what is right or wrong (or good or bad) to do in that situation. It is clear that ethics (and hence what is considered right or wrong, or good or bad) changes significantly over historical time. Over similarly long historical time-scales, despite discouraging daily news reports, it appears that the societies of our world are becoming stronger, safer, healthier, wealthier, and more just and inclusive for their members.2 Two important sources of concepts help make sense of these changes. First, game theory contributes the abstraction of certain types of interactions among people as games3, and behavioral economics shows that these games not only have winners and losers, but the overall impact on the players collectively can be described as positive-sum, zero-sum, or negative-sum.4 Second, the theory of evolution, as applied to human and great ape cognition and sociality, shows how a way of life that depends on positive-sum cooperation among individuals is likely to 1 Russ Shafer-Landau, editor. Ethical Theory: An Anthology. Wiley-Blackwell, second edition, 2013. p. xi. 2 Robert Wright, Nonzero: The Logic of Human Destiny, Pantheon, 2000. Steven Pinker, The Better Angels of Our Nature: Why Violence Has Declined, Viking Adult, 2011. Steven Pinker, Enlightenment Now: The Case for Reason, Science, Humanism, and Progress, Viking, 2018. 3 John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton University Press, 1953. 4 Samuel Bowles, The Moral Economy: Why Good Incentives are No Substitute for Good Citizens. Yale University Press, 2016. provide for its society greater fitness than less cooperative ways of life.5 We can therefore think of the function of ethics as promoting the survival and thriving of the society by influencing the behavior of its individual members, summarized as: Ethics is a set of beliefs that a society conveys to its individual members, to encourage them to engage in positive-sum interactions and to avoid negative-sum interactions. As a society prospers, survives, and thrives, its individual members benefit as well, so ethical behavior is “non-obvious self-interest” for the individual. Philosophers would consider this to be a rule consequentialist position6, but one where the relevant consequences are the survival and thriving of society, not the pleasures and pains of its individual members. It is consequentialist because actions are not evaluated according to whether they are intrinsically right or wrong (by some criterion), but according to their long-term good or bad consequences for the survival and thriving of society. This position is rule consequentialism because the unit that is evaluated is not the individual action decision, but the set of ethical principles (often rules) adopted by society. Positive-sum and negative-sum interactions. Commerce and cooperation are paradigm positivesum interactions. When one person voluntarily trades or sells something to someone else, each party receives something that they value more highly than what they gave. When cooperating on a project, partners contribute toward a common goal, and reap a benefit greater than either could achieve alone. Theft and violence are examples of negative-sum interactions. The thief gains something from the theft, but the loss to the victim is typically greater than the gain to the thief. Violent conflict is the paradigm negative-sum interaction, since both parties may be worse off afterwards than before, possibly much worse off. (These are not cleanly separated cases. Violence in defense against external attack may be necessary to avoid a catastrophic outcome, and that defense itself is likely to be a cooperative project.) Cooperation, trust, and social norms. Cooperative projects among individuals are a major source of positive-sum outcomes. However, cooperation requires vulnerability, and trust that the vulnerability will not be exploited.7 Trust is a psychological state comprising the intention to accept vulnerability based on positive expectations of the intentions or behavior of another.8 5 Michael Tomasello. A Natural History of Human Morality. Harvard University Press, 2016. 6 Walter Sinnott-Armstrong. Consequentialism. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Winter 2015 edition, 2015. 7 Michael Tomasello. A Natural History of Human Morality. Harvard University Press, 2016. 8 D. M. Rousseau, S. B. Sitkin, R. S. Burt, and C. Camerer. Not so different after all: a cross-discipline view of trust. Academy of Management Review, 23(3):393–404, 1998. As intelligent robots or large corporations increasingly act as autonomous goal-seeking agents and therefore as members of our society, then they, too, need to be subject to the requirements of ethics, and need to demonstrate that they can trust and be trustworthy. Successful cooperation demonstrates the trustworthiness of the partners and produces more trust while exploitation reduces trust. By trusting each other enough to pool their resources and efforts, individuals working together can often achieve much more than the sum of their individual efforts working separately. Large cooperative projects, from raising a barn, to digging a canal, to creating an Interstate Highway System, produce large benefits for everyone. But if I spend a day helping raise your barn, I trust that in due time, you will spend a day helping to raise mine. And if taxes help pay for New York’s Erie Canal or the Pennsylvania Turnpike, I trust that, in due time, taxes will also pay for the Panama Canal linking the East and West Coasts, and the St. Lawrence Seaway providing access to the Great Lakes. Some of the states in the USA emphasize this with the name “Commonwealth”, meaning that shared resources provide shared prosperity. Social norms are behavioral regularities that we as individual members of society can generally count on when planning our activities. By trusting these (near) invariants, many aspects of our lives become simpler, more efficient, and less risky and uncertain. Maintaining a social norm is a kind of cooperative project without specified partners. I accept certain minor sacrifices in return for similar behaviors by (almost) everyone else, providing a (near) invariant that we all can rely on. For example, when having lunch at a cafe, condiments are freely available for my convenience, but I know not to pocket the extras, so they will continue to be available. Likewise, I trust that a simple painted stripe in the middle of a road I am driving on securely separates me from drivers going in the opposite direction, so I accept the minor sacrifice of not crossing that stripe even when my side is congested. Like explicit cooperative projects, social norms provide positive-sum results for society, saving resources that would otherwise go toward protection and recovery, making us individually and collectively better off. Each requires trust: acceptance of vulnerability to the other partners, along with confidence that few others will exploit that vulnerability, even for individual gain. I use the term “social norm” in",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ed6ce30c5bec85fc992bd9e814e0f77ea76cd18d,https://www.semanticscholar.org/paper/ed6ce30c5bec85fc992bd9e814e0f77ea76cd18d,AI explainability 360: hands-on tutorial,"This tutorial will teach participants to use and contribute to a new open-source Python package named AI Explainability 360 (AIX360) (https://aix360.mybluemix.net), a comprehensive and extensible toolkit that supports interpretability and explainability of data and machine learning models. Motivation for the toolkit. The AIX360 toolkit illustrates that there is no single approach to explainability that works best for all situations. There are many ways to explain: data vs. model, direct vs. post-hoc explanation, local vs. global, etc. The toolkit includes ten state of the art algorithms that cover different dimensions of explanations along with proxy explainability metrics. Moreover, one of our prime objectives is for AIX360 to serve as an educational tool even for non-machine learning experts (viz. social scientists, healthcare experts). To this end, the toolkit has an interactive demonstration, highly descriptive Jupyter notebooks covering diverse real-world use cases, and guidance materials, all helping one navigate the complex explainability space. Compared to existing open-source efforts on AI explainability, AIX360 takes a step forward in focusing on a greater diversity of ways of explaining, usability in industry, and software engineering. By integrating these three aspects, we hope that AIX360 will attract researchers in AI explainability and help translate our collective research results for practicing data scientists and developers deploying solutions in a variety of industries. Regarding the first aspect of diversity, Table 1 in [1] compares AIX360 to existing toolkits in terms of the types of explainability methods offered. The table shows that AIX360 not only covers more types of methods but also has metrics which can act as proxies for judging the quality of explanations. Regarding the second aspect of industry usage, AIX360 illustrates how these explainability algorithms can be applied in specific contexts (please see Audience, goals, and outcomes below). In just a few months since its initial release, the AIX360 toolkit already has a vibrant slack community with over 120 members and has been forked almost 80 times accumulating over 400 stars. This response leads us to believe that there is significant interest in the community in learning more about the toolkit and explainability in general. Audience, goals, and outcomes. The presentations in the tutorial will be aimed at an audience with different backgrounds and computer science expertise levels. For all audience members and especially those unfamiliar with Python programming, AIX360 provides an interactive experience (http://aix360.mybluemix.net/data) centered around a credit approval scenario as a gentle and grounded introduction to the concepts and capabilities of the toolkit. We will also teach all participants which type of explainability algorithm is most appropriate for a given use case, not only for those in the toolkit but also from the broader explainability literature. Knowing which explainability algorithms apply to which contexts and understanding when to use them can benefit most people, regardless of their technical background. The second part of the tutorial will consist of three use cases featuring different industry domains and explanation methods. Data scientists and developers can gain hands-on experience with the toolkit by running and modifying Jupyter notebooks, while others will be able to follow along by viewing rendered versions of the notebooks. Here is a rough agenda of the tutorial: 1) Overture: Provide a brief introduction to the area of explainability as well as introduce common terms. 2) Interactive Web Experience: The AIX360 interactive web experience (http://aix360.mybluemix.net/data) is intended to show a non-computer science audience how different explainability methods may suit different stakeholders in a credit approval scenario (data scientists, loan officers, and bank customers). 3) Taxonomy: We will next present a taxonomy that we have created for organizing the space of explanations and guiding practitioners toward an appropriate choice for their applications. 4) Installation: We will transition into a Python environment and ask participants to install the AIX360 package on their machines using provided instructions. 5) Example Use Cases in Finance, Government, and Healthcare: We will take participants through three use-cases in various application domains in the form of Jupyter notebooks. 6) Metrics: We will briefly showcase the two explainability metrics currently available through the toolkit. 7) Future Directions: The final segment will be to discuss future directions and how participants can contribute to the toolkit.",FAT*,2020.0,10.1145/3351095.3375667,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4546399ebcef2da1294b07c1ce05ca7f23e6ed99,https://www.semanticscholar.org/paper/4546399ebcef2da1294b07c1ce05ca7f23e6ed99,UAV-Based Structural Damage Mapping: A Review,"Structural disaster damage detection and characterization is one of the oldest remote sensing challenges, and the utility of virtually every type of active and passive sensor deployed on various air- and spaceborne platforms has been assessed. The proliferation and growing sophistication of unmanned aerial vehicles (UAVs) in recent years has opened up many new opportunities for damage mapping, due to the high spatial resolution, the resulting stereo images and derivatives, and the flexibility of the platform. This study provides a comprehensive review of how UAV-based damage mapping has evolved from providing simple descriptive overviews of a disaster science, to more sophisticated texture and segmentation-based approaches, and finally to studies using advanced deep learning approaches, as well as multi-temporal and multi-perspective imagery to provide comprehensive damage descriptions. The paper further reviews studies on the utility of the developed mapping strategies and image processing pipelines for first responders, focusing especially on outcomes of two recent European research projects, RECONASS (Reconstruction and Recovery Planning: Rapid and Continuously Updated Construction Damage, and Related Needs Assessment) and INACHUS (Technological and Methodological Solutions for Integrated Wide Area Situation Awareness and Survivor Localization to Support Search and Rescue Teams). Finally, recent and emerging developments are reviewed, such as recent improvements in machine learning, increasing mapping autonomy, damage mapping in interior, GPS-denied environments, the utility of UAVs for infrastructure mapping and maintenance, as well as the emergence of UAVs with robotic abilities.",ISPRS Int. J. Geo Inf.,2019.0,10.3390/ijgi9010014,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e448592bb291099809c752a5ae4db837f8b38188,https://www.semanticscholar.org/paper/e448592bb291099809c752a5ae4db837f8b38188,Quantum key distribution and beyond: introduction,"This feature issue presents a collection of recent theoretical and experimental developments in the field of quantum key distribution (QKD) and its extension to other quantum cryptography protocols and devices. It encompasses work on a variety of QKD protocols, including continuous-variable, measurement-device independent, and twin-field QKD, as well as other newly proposed protocols, in platforms ranging from optical fiber through to wireless indoor and satellite links. It covers examples of hacking strategies and their countermeasures as well as applications of machine learning techniques in designing quantum networks. It also includes new developments in efficient superconducting photon-number resolving detectors as well as fast quantum random number generators. Distinctively, this feature issue demonstrates how different expertise in science and engineering can come together to produce an outcome that hopefully takes us one step closer to the wide-scale deployment of quantum communications technologies.",Journal of the Optical Society of America B,2019.0,10.1364/JOSAB.36.00QKD1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0cc577139c0649cb8ba6fc16ba09c784cd5a5c13,https://www.semanticscholar.org/paper/0cc577139c0649cb8ba6fc16ba09c784cd5a5c13,Towards Data-Driven Physics-Informed Global Precipitation Forecasting from Satellite Imagery,"Under the effects of global warming, extreme events such as floods and droughts are increasing in frequency and intensity. This trend directly affects communities and make all the more urgent widening the access to accurate precipitation forecasting systems for disaster preparedness. Nowadays, weather forecasting relies on numerical models necessitating massive computing resources that most developing countries cannot afford. Machine learning approaches are still in their infancy but already show the promise for democratizing weather predictions, by leveraging any data source and requiring less compute. In this work, we propose a methodology for data-driven and physics-aware global precipitation forecasting from satellite imagery. To fully take advantage of the available data, we design the system as three elements: 1. The atmospheric state is estimated from recent satellite data. 2. The atmospheric state is propagated forward in time. 3. The atmospheric state is used to derive the precipitation intensity within a nearby time interval. In particular, our use of stochastic methods for forecasting the atmospheric state represents a novel application in this domain. 1 Data-driven precipitation forecasting Climate change is already affecting precipitation events and how water is distributed over the planet. A warmer atmosphere has increased capacity at holding water vapor, resulting in increasing the frequency and intensity of heavy rainfall [6] and at the same time in draining water reserves. Direct consequences of this trend are the saturation of watersheds and sewer systems, with higher risks of flooding and landslides, and of polluting waters. This is particularly problematic for urban areas, but also for agriculture and aquaculture, endangering livelihoods and causing severe economic loss. Being able to predict these extreme events several days in advance would allow for mitigating their impact. However, currently deployed operational weather forecasting systems are based on numerical models with computing and data demands that most developing countries cannot afford. Furthermore, they have a prediction time lag of several hours[17], which delays prompt disaster response. Given the amount of Earth Observational data and numerical model outputs available nowadays, machine learning approaches have the potential to match operational systems’ performance and significantly reduce their compute needs and their inference time lags. Yet, due to the scale and complexity of the AI for Earth Sciences Workshop at NeurIPS 2020. Figure 1: System overview. The three steps are highlighted by different colours. Each step can be viewed as a self-contained task, to be finally linked and fine-tuned together. problem, state-of-the-art approaches in this field are limited to nowcasting (forecasting a few hours in advance), or to local geographical areas. In this paper, we propose a global, physics-informed and probabilistic Deep Learning system for predicting precipitation rates up to three days ahead of time. The global scale is enabled by the use of satellite imagery and allows for surpassing the nowcasting time barrier. Physics knowledge is implicitly injected by processing a numerical model product (ERA5 [7]) at training as an intermediate representation. Due to data availability constraints elucidated in Section 2, we decouple the system into three steps and provide preliminary results for each of them. Related Work. Our work is motivated by three areas of related work. First, our work adds to the domain of machine-learning-based precipitation forecasting, where a common approach is to use Recurrent Neural Networks for precipitation nowcasting (i.e. forecasting few hours ahead) [20]. Agrawal et al. [1] proposed a U-Net-based nowcasting model from radar images, while Sønderby et al. [17] proposes a network with spatial and temporal encoders to predict from both radar and satellite images, although on a limited regional area. Another related area is physics-informed machine learning, where a common approach is to incorporate physics constraints as an additional loss term [4]. In this work, we instead emulate the output of a physics-based model, an approach also seen in [12]. Lastly, our work incorporates probabilistic forecasts; relevant techniques can be found in stochastic video prediction [3, 9]. In precipitation nowcasting, [17] is an example of a probabilistic model, where uncertainty is modeled by outputting precipitation histograms. 2 A physics-informed and probabilistic approach To enable medium-range and global predictions, we incorporate physics knowledge into the precipitation forecasting system by leveraging the reanalysis dataset ERA5. This product is generated by a numerical model and provides a range of atmospheric-state variables appropriate for our precipitation estimation, e.g. specific humidity, temperature and geopotential height at different pressure levels [7]. By using this data source, we encourage the system to implicitly model physical laws. We further make use of SimSat [2] which contains simulated1 satellite imagery from European Centre for Medium-Range Weather Forecasts (ECMWF) at 3 h frequency for three spectral channels. Finally, as precipitation ground truth, we consider IMERG, a global half-hourly precipitation estimation product provided by NASA [8]. More details about each dataset can be found in Appendix A. Directly forecasting precipitation is an intrinsically difficult task, as these three data sources are not available at the same times. Training an end-to-end system, with all datasets simultaneously, would limit the data availability to less than 5 years, which would consequently limit the capacity of the trained model. We propose instead a three-step approach: 1) State Estimation: the weather satellite imagery is processed to infer the atmospheric state of the Earth, 2) State Forecasting: a sequence Using simulated satellite data in place of real ones minimizes data processing, as images are a global nadir view of Earth, avoiding issues of instrument error and large numbers of missing values.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f34727ce4be5da88d962e2e89e7205b83ee79c15,https://www.semanticscholar.org/paper/f34727ce4be5da88d962e2e89e7205b83ee79c15,The Future of Coding: A Comparison of Hand-Coding and Three Types of Computer-Assisted Text Analysis Methods,"Advances in computer science and computational linguistics have yielded new, and faster, computational approaches to structuring and analyzing textual data. These approaches perform well on tasks like information extraction, but their ability to identify complex, socially constructed, and unsettled theoretical concepts—a central goal of sociological content analysis—has not been tested. To fill this gap, we compare the results produced by three common computer-assisted approaches—dictionary, supervised machine learning (SML), and unsupervised machine learning—to those produced through a rigorous hand-coding analysis of inequality in the news (N = 1,253 articles). Although we find that SML methods perform best in replicating hand-coded results, we document and clarify the strengths and weaknesses of each approach, including how they can complement one another. We argue that content analysts in the social sciences would do well to keep all these approaches in their toolkit, deploying them purposefully according to the task at hand.",,2018.0,10.1177/0049124118769114,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5f4911ff36d8b00fc635c6c011e4e483509c7359,https://www.semanticscholar.org/paper/5f4911ff36d8b00fc635c6c011e4e483509c7359,"Future Challenges for the Science and Engineering of Learning July 23-25 , 2007 National Science Foundation Organizers","The goal of the workshop was to explore research opportunities in the broad domain of the Science and Engineering of Learning, and to provide NSF with this Report identifying important open questions. It is anticipated that this Report will to be used to encourage new research directions, particularly in the context of the NSF Science of Learning Centers (SLCs), and also to spur new technological developments. The workshop was attended by 20 leading international researchers. Half of the researchers at the workshops were from SLCs and the other half were experts in neuromorphic engineering and machine learning. The format of the meeting was designed to encourage open discussion. There were only relatively brief formal presentations. The most important outcome was a detailed set of open questions in the domains of both biological learning and machine learning. We also identified set of common issues indicating that there is a growing convergence between these two previously separate domains so that work invested there will benefit our understanding of learning in both man and machine. In this summary we outline a few of these important questions. Biological learners have the ability to learn autonomously, in an ever changing and uncertain world. This property includes the ability to generate their own supervision, select the most informative training samples, produce their own loss function, and evaluate their own performance. More importantly, it appears that biological learners can effectively produce appropriate internal representations for composable percepts-a kind of organizational scaffold-as part of the learning process. By contrast, virtually all current approaches to machine learning 2 typically require a human supervisor to design the learning architecture, select the training examples, design the form of the representation of the training examples, choose the learning algorithm, set the learning parameters, decide when to stop learning, and choose the way in which the performance of the learning algorithm is evaluated. This strong dependence on human supervision is greatly retarding the development and ubiquitous deployment autonomous artificial learning systems. Although we are beginning to understand some of the learning systems used by brains, many aspects of autonomous learning have not yet been identified. The mechanisms of learning operate on different time scales, from milliseconds to years. These various mechanisms must be identified and characterized. These time scales have practical importance for education. For example, the most effective learning occurs when practice is distributed over time such that learning experiences are separated …",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6e79b78112d526b64b2c491f27a1f00fb6b9f5b6,https://www.semanticscholar.org/paper/6e79b78112d526b64b2c491f27a1f00fb6b9f5b6,Fundamentals of Predictive Text Mining,,Texts in Computer Science,2010.0,10.1007/978-1-4471-6750-1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1b33b7d9efd360cc1bbfd76d18def9e9fd024943,https://www.semanticscholar.org/paper/1b33b7d9efd360cc1bbfd76d18def9e9fd024943,A public fMRI dataset of 5000 scenes: a resource for human vision science,"Vision science particularly machine vision is being revolutionized by large-scale datasets. State-of-the-art artificial vision models critically depend on large-scale datasets to achieve high performance. In contrast, although large-scale learning models (e.g., models such as Alexnet) have been applied to human neuroimaging data, the image datasets used on neural studies often rely on significantly fewer images. The small size of these datasets also translates to limited image diversity. Here we dramatically increase the image dataset size deployed in an fMRI study of visual scene processing: over 5,000 discrete image stimuli were presented to each of four participants. We believe this boost in dataset size will better connect the field of computer vision to human neuroscience. To further enhance this connection and increase image overlap with computer vision datasets, we include images from two standard artificial learning datasets in our stimuli: 2,000 images from COCO; 2 images per category from ImageNet (∼ 2000). Also included are 1,000 hand-curated scene images from 250 categories. The scale advantage of our dataset and the use of a slow event-related design enables, for the first time, joint computer vision and fMRI analyses that span a significant and diverse region of image space using high-performing models.",,2018.0,10.32470/CCN.2018.1140-0,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a6c7b3c141f6c604e08fee902fa268665b8a4221,https://www.semanticscholar.org/paper/a6c7b3c141f6c604e08fee902fa268665b8a4221,Simple linear classifiers via discrete optimization: learning certifiably optimal scoring systems for decision-making and risk assessment,"Scoring systems are linear classification models that let users make quick predictions by adding, subtracting, and multiplying a few small numbers. These models are widely used in applications where humans have traditionally made decisions because they are easy to understand and validate. In spite of extensive deployment, many scoring systems are still built using ad hoc approaches that combine statistical techniques, heuristics, and expert judgement. Such approaches impose steep trade-offs with performance, making it difficult for practitioners to build scoring systems that will be used and accepted. In this dissertation, we present two new machine learning methods to learn scoring systems from data: Supersparse Linear Integer Models (SLIM) for decision-making applications; and Risk-calibrated Supersparse Linear Integer Models (RiskSLIM) for risk assessment applications. Both SLIM and RiskSLIM solve discrete optimization problems to learn scoring systems that are fully optimized for feature selection, small integer coefficients, and operational constraints. We formulate these problems as integer programming problems and develop specialized algorithms to recover certifiably optimal solutions with an integer programming solver. We illustrate the benefits of this approach by building scoring systems for realworld problems such as recidivism prediction, sleep apnea screening, ICU seizure prediction, and adult ADHD diagnosis. Our results show that a discrete optimization approach can learn simple models that perform well in comparison to the state-ofthe-art, but that are far easier to customize, understand, and validate. Thesis Supervisor: Cynthia Rudin Title: Associate Professor of Computer Science Duke University",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2ad3ff2bc45526b4e6158072d4840147bd59051b,https://www.semanticscholar.org/paper/2ad3ff2bc45526b4e6158072d4840147bd59051b,Beginnings of Artificial Intelligence in Medicine (AIM): Computational Artifice Assisting Scientific Inquiry and Clinical Art – with Reflections on Present AIM Challenges,"Summary Background : The rise of biomedical expert heuristic knowledge-based approaches for computational modeling and problem solving, for scientific inquiry and medical decision-making, and for consultation in the 1970’s led to a major change in the paradigm that affected all of artificial intelligence (AI) research. Since then, AI has evolved, surviving several “winters”, as it has oscillated between relying on expensive and hard-to-validate knowledge-based approaches, and the alternative of using machine learning methods for inferring classification rules from labelled datasets. In the past couple of decades, we are seeing a gradual but progressive intertwining of the two. Objectives : To give an overview of early directions in AI in medicine and threads of some subsequent developments motivated by the very different goals of scientific inquiry for biomedical research, and for computational modeling of clinical reasoning and more general healthcare problem solving from the perspective of today’s “AI-Deep Learning Boom”. To show how, from the beginning, AI was central to Biomedical and Health Informatics (BMHI), as a field investigating how to understand intelligent thinking in dealing professionally with the practice for healthcare, developing mathematical models, technology, and software tools to aid human experts in biomedicine, despite many previous bouts of “exuberant optimism” about the methodologies deployed. Methods : An overview and commentary on some of the early research and publications in AI in biomedicine, emphasizing the different approaches to the modeling of problems involved in clinical practice in contrast to those of biomedical science. A concluding reflection of a few current challenges and pitfalls of AI in some biomedical applications. Conclusion : While biomedical knowledge-based systems played a critical role in influencing AI in its early days, 50 years later they have taken a back seat behind “Deep Learning” which promises to discover knowledge structures for inference and prediction, both in science and for clinical decision-support. Early work on AI for medical consultation turned out to be more useful for explanation and teaching than for clinical practice, as had been originally intended. Today, despite the many reported successes of deep learning, fundamental scientific challenges arise in drawing on models of brain science, cognition, and language, if AI is to augment and complement rather than replace human judgment and expertise in biomedicine while also incorporating these advances for translational medicine. Understanding clinical phenotypes and how they relate to precision and personalization of care requires not only scientific inquiry, but also humanistic models of treatment that respond to patient and practitioner narrative exchanges, since it is the stories and insights of human experts which encourage what Norbert Weiner termed the ethical “human use of human beings”, so central to adherence to the Hippocratic Oath",Yearbook of medical informatics,2019.0,10.1055/s-0039-1677895,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ad7777348a309d221b0f9cdcea070ca9ad03dae4,https://www.semanticscholar.org/paper/ad7777348a309d221b0f9cdcea070ca9ad03dae4,Mastering data analysis with R : gain clear insights into your data and solve real-world data science problems with R - from data munging to modeling and visualization,"Gain sharp insights into your data and solve real-world data science problems with R-from data munging to modeling and visualization About This Book * Handle your data with precision and care for optimal business intelligence * Restructure and transform your data to inform decision-making * Packed with practical advice and tips to help you get to grips with data mining Who This Book Is For If you are a data scientist or R developer who wants to explore and optimize your use of R's advanced features and tools, this is the book for you. A basic knowledge of R is required, along with an understanding of database logic. What You Will Learn * Connect to and load data from R's range of powerful databases * Successfully fetch and parse structured and unstructured data * Transform and restructure your data with efficient R packages * Define and build complex statistical models with glm * Develop and train machine learning algorithms * Visualize social networks and graph data * Deploy supervised and unsupervised classification algorithms * Discover how to visualize spatial data with R In Detail R is an essential language for sharp and successful data analysis. Its numerous features and ease of use make it a powerful way of mining, managing, and interpreting large sets of data. In a world where understanding big data has become key, by mastering R you will be able to deal with your data effectively and efficiently. This book will give you the guidance you need to build and develop your knowledge and expertise. Bridging the gap between theory and practice, this book will help you to understand and use data for a competitive advantage. Beginning with taking you through essential data mining and management tasks such as munging, fetching, cleaning, and restructuring, the book then explores different model designs and the core components of effective analysis. You will then discover how to optimize your use of machine learning algorithms for classification and recommendation systems beside the traditional and more recent statistical methods. Style and approach Covering the essential tasks and skills within data science, Mastering Data Analysis provides you with solutions to the challenges of data science. Each section gives you a theoretical overview before demonstrating how to put the theory to work with real-world use cases and hands-on examples.",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6a2a534a9d01b76ecb5caba20032022a904cb50d,https://www.semanticscholar.org/paper/6a2a534a9d01b76ecb5caba20032022a904cb50d,COSMIC Semantic Segmentation Framework,"Deep space missions such as the Mars Reconnaissance Orbiter collect more data than can be sent back to Earth due to limited communications bandwidth. Machine learning algorithms can be deployed on board orbiters to prioritize the downlink of scientifically interesting images, such as those including fresh impact craters, recurring slope lineae, or dust devils. However, basic machine learning research is necessary to boost realworld performance, and numerous possible convolutional neural network architectures must be evaluated in terms of accuracy and compute requirements. A framework is designed to reduce redundant development, to standardize the algorithm testing process, and to allow developers to focus on the implementation details of novel machine learning algorithms. Three convolutional neural network implementations are included with the framework, pending use in future research. 1Content-based Onboard Summarization to Monitor Infrequent Change 2CL #18-465",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e92cd6251a9d93628399db7de2f8f94d6f9b9725,https://www.semanticscholar.org/paper/e92cd6251a9d93628399db7de2f8f94d6f9b9725,Synchronized Pair Configuration in Virtualization-Based Lab for Learning Computer Networks,"Introduction Recently, practice labs in computer network courses have become essential for enhancing students' learning since they can enable students to apply, practice, and verify network concepts in real contexts. In general, network configuration is usually an individual task and is similar to tasks of famous networking certification examinations such as the Cisco Certified Network Associate and the Linux Professional Institute Certification. Each examinee must individually configure a network on these tests. However, implementation of a physical network lab on campus has limitations in terms of the amount of available hardware since hardware requires rapid upgrades, and the number of instructors seems insufficient to assist all students (Williams, 2010; Xu, Huang, & Tsai, 2014). Meanwhile, some studies have revealed other limitations when students work individually to solve problems in computer science; for instance, solo students had less self-confidence in doing assignments (Williams & Kessler, 2002), made more syntax and typing errors while programming (Lui & Chan, 2006), and required more assistance from instructors (Braught, Wahls, & Eby, 2011), thereby spending more time solving their problems (Williams, Wiebe, Yang, Ferzli, & Miller, 2002). In contrast, previous studies found that pairs outperformed individuals in the number of interactions (Williams, Wiebe, Yang, Ferzli, & Miller, 2002) and learning outcomes in computer programming (Layman, Williams, Slaten, Berenson, & Vouk, 2008). In computer network learning, as experienced teachers in computer networks (with more than 10 years teaching experience), we found insufficient networking hardware and a heavy teaching load when students were assigned to perform lab practices individually. Therefore, in our previous study, stand-alone desktop virtual machines (e.g., VMware and Oracle Virtual box) (Dobrilovic & Stojanov, 2006; Chen & Tao, 2012) were enhanced, and one distributed virtual machine (e.g., KVM cloud platform) (Hwang, Kongcharoen, & Ghinea, 2014) was proposed for solving the hardware limitation. However, after deep investigation, we found that some students still did not pay attention in lab practices even though they could observe and learn from their peers or group mates; moreover, they did not ask for much help from instructors. Therefore, we tried to discover an effective collaboration mechanism that would strengthen their collaborations in a lab class and thereby reduce the instructors' load. This study proposed one synchronized mechanism to help paired students to practice learning computer networks in the virtualization system to reduce class teaching effort and to facilitate sufficient networking lab installation. The virtualization system is a group of virtual machines (VMs) deployed on one server, and the VMs contain operating systems (OSs) for operating as networking equipment, such as hubs, networking switches, and routers. The synchronized mechanism is a group assignment, and it requires a pair of users to work simultaneously. Thus each user works in a pair and pays the same level of attention as his/her partner. The mechanism mentioned above is called synchronized pair configuration (SPC) in this study. Through SPC, paired students can input commands simultaneously from two computers to one terminal. SPC enables a pair of students to work simultaneously and reduces instructor effort. In addition, paired students can share ideas and knowledge during lab assignments without much help from instructors. Due to SPC, this study established a virtualization-based lab (VBLab) that allows conduction of labs in the virtualization system. This VBLab's design is improved from our previous version (Hwang et al., 2014), with a new feature for a web terminal that allows students simultaneously or separately to input commands to VMs. In addition to experiment procedures, university students were divided into two groups, as pairs and as individuals, and each pair accomplished network lab practices together using SPC while each individual accomplished them alone. …",J. Educ. Technol. Soc.,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4b8ab3bb19e64a931209e02b7449cec03da68fa2,https://www.semanticscholar.org/paper/4b8ab3bb19e64a931209e02b7449cec03da68fa2,Knowledge Organization Seventh Annual “Best Paper in KO Award” for Volume 46 (2019),"The paper traces the development of the discussion around ethical issues in artificial intelligence, and considers the way in which humans have affected the knowledge bases used in machine learning. The phenomenon of bias or discrimination in machine ethics is seen as inherited from humans, either through the use of biased data or through the semantics inherent in intellectually-built tools sourced by intelligent agents. The kind of biases observed in AI are compared with those identified in the field of knowledge organization, using religious adherents as an example of a community potentially marginalized by bias. A practical demonstration is given of apparent religious prejudice inherited from source material in a large database deployed widely in computational linguistics and automatic indexing. Methods to address the problem of bias are discussed, including the modelling of the moral process on neuroscientific understanding of brain function. The question is posed whether it is possible to model religious belief in a similar way, so that robots of the future may have both an ethical and a religious sense and themselves address the problem of prejudice. Vanda Broughton is Emeritus Professor of library and information studies at University College London. Her principal research interest is in the development of faceted classification, particularly as it affects different disciplines. She is editor of the second edition of the Bliss Bibliographic Classification (BC2), and an associate editor of the Universal Decimal Classification. In addition to the published volumes of BC2, she is author of several books on knowledge organization systems and numerous articles and conference papers. Smiraglia, Richard P. 2019. “Work.” Knowledge Organization 46, no. 4: 308-19. Abstract: A work is a deliberately created informing entity intended for communication. A work consists of abstract intellectual content that is distinct from any object that is its carrier. In library and information science, the importance of the work lies squarely with the problem of information retrieval. Works are mentefacts—intellectual (or mental) constructs that serve as artifacts of the cultures in which they arise. The meaning of a work is abstract at every level, from its creator’s conception of it, to its reception and inherence by its consumers. Works are a kind of informing object and are subject to the phenomenon of instantiation, or realization over time. Research has indicated a base typology of instantiation. The problem for information retrieval is to simultaneously collocate and disambiguate large sets of instantiations. Cataloging and bibliographc tradition stipulate an alphabetico-classed arrangement of works based on an authorship principle. FRBR provided an entity-relationship schema for enhanced control of works in future catalogs, which has been incorporated into RDA. FRBRoo provides an empirically more precise model of work entities as informing objects and a schema for their representation in knowledge organization systems. A work is a deliberately created informing entity intended for communication. A work consists of abstract intellectual content that is distinct from any object that is its carrier. In library and information science, the importance of the work lies squarely with the problem of information retrieval. Works are mentefacts—intellectual (or mental) constructs that serve as artifacts of the cultures in which they arise. The meaning of a work is abstract at every level, from its creator’s conception of it, to its reception and inherence by its consumers. Works are a kind of informing object and are subject to the phenomenon of instantiation, or realization over time. Research has indicated a base typology of instantiation. The problem for information retrieval is to simultaneously collocate and disambiguate large sets of instantiations. Cataloging and bibliographc tradition stipulate an alphabetico-classed arrangement of works based on an authorship principle. FRBR provided an entity-relationship schema for enhanced control of works in future catalogs, which has been incorporated into RDA. FRBRoo provides an empirically more precise model of work entities as informing objects and a schema for their representation in knowledge organization systems. Richard P. Smiraglia holds a PhD in information from the University of Chicago. He is Senior Fellow and Executive Director of the Institute for Knowledge Organization and Structure, Inc. and is Editor-in-Chief of this journal. He also is Professor Emeritus of the iSchool at the University of Wisconsin-Milwaukee. He was 2017-2018 KNAW Visiting Professor at DANS (Data Archiving and Networked Services division of the Royal Netherlands Academy of the Arts and Sciences), The Hague, The Netherlands, where he remains visiting fellow and was the 2018 recipient of the 2018 Frederick G. Kilgour Award for Research in Library and Information Technology. * Awards Committee for Volume 45 (2018): Ann Graf. chair; Jane Greenberg, Joseph Tennis, Daniel Martínez-Ávila and Yejun Wu",KNOWLEDGE ORGANIZATION,2020.0,10.5771/0943-7444-2020-8-714,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
afcdc4580f22a6dae089622257efab6720d33015,https://www.semanticscholar.org/paper/afcdc4580f22a6dae089622257efab6720d33015,Data Collection and Annotation Pipeline for Social Good Projects,"Vast amounts of data are generated during crisis events through both formal and informal sources, and this data can be used to make a positive impact in all phases of crisis events. However, collecting and annotating data quickly and effectively in the face of crises is a challenging task. Crises require quick, robust, and efficient annotation to best respond to unfolding events. Data must be accessed and aggregated across different platforms and sources, and annotation tools must be able to utilize this data effectively. This work describes an architecture built for rapid collection and annotation of data from multiple sources which can then be built into machine learning and data analysis models. We extract data from social media via multiple systems for Twitter data collection, as well as building architecture for the collection of news articles from diverse sources. These can then be input into the INCEpTION annotation framework, which has been adapted to allow for easy management of multiple annotators, aiming to improve functionality to facilitate the application of citizen science. This allows us to rapidly prototype new annotation schema across a diverse array of data sources, which can then be deployed for machine learning. As a use case, we explore annotation of COVID-19 related Tweets and news articles for case prediction.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ab790b32c23ad4921264756dbe69f630bc58c064,https://www.semanticscholar.org/paper/ab790b32c23ad4921264756dbe69f630bc58c064,Data Transfer via UAV Swarm Behaviours,"This paper presents an adaptive robotic swarm of Unmanned Aerial Vehicles (UAVs) enabling communications between separated non-swarm devices. The swarm nodes utilise machine learning and hyper-heuristic rule evolution to enable each swarm member to act appropriately for the given environment. The contribution of the machine learning is verified with an exploration of swarms with and without this module. The exploration finds that in challenging environments the learning greatly improves the swarm’s ability to complete the task. The swarm evolution process of this study is found to successfully create different data transfer methods depending on the separation of non-swarm devices and the communication range of the swarm members. This paper also explores the resilience of the swarm to agent loss, and the scalability of the swarm in a range of environment sizes. In regard to resilience, the swarm is capable of recovering from agent loss and is found to have improved evolution. In regard to scalability, the swarm is observed to have no upper limit to the number of agents deployed in an environment. However, the size of the environment is seen to be a limit for optimal swarm performance. Introduction Phillip C. Smith [1] Monash University Robert Hunjet [2] Defence Science and Technology Group, Edinburgh, South Australia Aldeida Aleti [3] Monash Swarm Robotics Laboratory Jan Carlo Barca [4] Deakin University AJTDE Vol 6, No 2 May 2018 [5]",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5e699360578d6ca55770376cb97ad021bcbad9e6,https://www.semanticscholar.org/paper/5e699360578d6ca55770376cb97ad021bcbad9e6,Geospatial Artificial Intelligence (GeoAI),"Introduction Nowadays artificial intelligence (AI) is bringing tremendous new opportunities and challenges to geospatial research. Its fast development is powered by theoretical advancement, big data, computer hardware (e.g, the graphics processing unit, GPU) and high-performance computing platforms that support the development, training, and deployment of AI models within reasonable amount of time. Recent years have witnessed significant advances in Geospatial Artificial Intelligence (GeoAI), which is the integration of geospatial studies and AI, especially machine learning and deep learning methods and latest AI technologies in both academia and industry. GeoAI can be regarded as a study subject to develop intelligent computer programs to mimic the processes of human perception, spatial reasoning, and discovery about geographical phenomena and dynamics, to advance our knowledge, and to solve problems in human environmental systems and their interactions with a focus on spatial contexts and roots in geography or geographic information science (GIScience). Thus, it would require the knowledge of AI theory, programming and computation practices as well as geographic domain knowledge to be competent in GeoAI research. There have already been increasingly collaborative GeoAI studies for GIScience, remote sensing, physical environment and human society. It is a good time to provide a key reference list for educators, students, researchers, and practitioners to keep up with the latest GeoAI research topics. This bibliographical entry will first review the historical roots for AI in geography and GIScience and then list up to 10 selective recent works with annotations that briefly describe their importance for each topic of interest in the GeoAI landscape, ranging from fundamental spatial representation learning to spatial predictions and to various advancements in cartography, earth observation, social sensing and geospatial semantics.",,2020.0,10.1093/obo/9780199874002-0228,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4c41daec0e70789e3499aca020a12b02f75b6c82,https://www.semanticscholar.org/paper/4c41daec0e70789e3499aca020a12b02f75b6c82,Sociotechnical Design in Legal Algorithmic Decision-Making,"Over the past decade, civil litigants in the U.S. have come to increasingly rely on machine learning (ML) systems to classify documents for discovery review and fact-finding, an approach now broadly referred to as Technology-Assisted Review (TAR). The transformation of legal discovery from a painstaking manual process to a sophisticated algorithm-driven methodology took place over a relatively short period of time, many years before controversies arose surrounding the use of automated risk assessment tools on the criminal side of the U.S. justice system. Introduced in 2008 to a handful of litigators in an experimental research setting hosted by the National Institute of Standards and Technology (NIST), TAR was first deployed live on an active litigation in 2012, and by 2015 a vocal and influential vanguard of judges was actively advocating for its use on cases involving large, complex document discovery. My research examines the cross-disciplinary experimentation and collaboration that took place across legal practitioners and computer scientists leading to ML becoming a judicially accepted solution in U.S. civil litigation practice. The aim of this research is to develop a comprehensive case study for how an expert professional field wrestled with the challenges of integrating ML into sensitive decision-making workflows. While deeply attentive to the unique and complex encounter between U.S. civil litigation practice and computer science, my work also aims to inform the practice of algorithmic system design, development, and governance across other high-stakes professional domains.",CSCW Companion,2020.0,10.1145/3406865.3418361,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4c41daec0e70789e3499aca020a12b02f75b6c82,https://www.semanticscholar.org/paper/4c41daec0e70789e3499aca020a12b02f75b6c82,Sociotechnical Design in Legal Algorithmic Decision-Making,"Over the past decade, civil litigants in the U.S. have come to increasingly rely on machine learning (ML) systems to classify documents for discovery review and fact-finding, an approach now broadly referred to as Technology-Assisted Review (TAR). The transformation of legal discovery from a painstaking manual process to a sophisticated algorithm-driven methodology took place over a relatively short period of time, many years before controversies arose surrounding the use of automated risk assessment tools on the criminal side of the U.S. justice system. Introduced in 2008 to a handful of litigators in an experimental research setting hosted by the National Institute of Standards and Technology (NIST), TAR was first deployed live on an active litigation in 2012, and by 2015 a vocal and influential vanguard of judges was actively advocating for its use on cases involving large, complex document discovery. My research examines the cross-disciplinary experimentation and collaboration that took place across legal practitioners and computer scientists leading to ML becoming a judicially accepted solution in U.S. civil litigation practice. The aim of this research is to develop a comprehensive case study for how an expert professional field wrestled with the challenges of integrating ML into sensitive decision-making workflows. While deeply attentive to the unique and complex encounter between U.S. civil litigation practice and computer science, my work also aims to inform the practice of algorithmic system design, development, and governance across other high-stakes professional domains.",CSCW Companion,2020.0,10.1145/3406865.3418361,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
afcdc4580f22a6dae089622257efab6720d33015,https://www.semanticscholar.org/paper/afcdc4580f22a6dae089622257efab6720d33015,Data Collection and Annotation Pipeline for Social Good Projects,"Vast amounts of data are generated during crisis events through both formal and informal sources, and this data can be used to make a positive impact in all phases of crisis events. However, collecting and annotating data quickly and effectively in the face of crises is a challenging task. Crises require quick, robust, and efficient annotation to best respond to unfolding events. Data must be accessed and aggregated across different platforms and sources, and annotation tools must be able to utilize this data effectively. This work describes an architecture built for rapid collection and annotation of data from multiple sources which can then be built into machine learning and data analysis models. We extract data from social media via multiple systems for Twitter data collection, as well as building architecture for the collection of news articles from diverse sources. These can then be input into the INCEpTION annotation framework, which has been adapted to allow for easy management of multiple annotators, aiming to improve functionality to facilitate the application of citizen science. This allows us to rapidly prototype new annotation schema across a diverse array of data sources, which can then be deployed for machine learning. As a use case, we explore annotation of COVID-19 related Tweets and news articles for case prediction.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
830f30fa1758649b51117a359028c3fd64b74c91,https://www.semanticscholar.org/paper/830f30fa1758649b51117a359028c3fd64b74c91,"Discussion of the Paper “Prediction, Estimation, and Attribution” by B. Efron","We enjoyed reading Professor Efron’s (Brad) paper just as much as we enjoyed listening to his June 2019 lecture in Leiden. One of the core values underlying statistical research is in how it enables scientific understanding and discoveries, and we appreciate that Efron has placed “science” at the center of his article. Or at least, the discoveries of associations that are not just valid here and now—in the particular dataset I have at hand—but are likely to be stable and replicated under slightly different circumstances. The successes of purely predictive algorithms (gradient boosting, deep learning) in object recognition, speech recognition, machine translation, and others, are real.1 Incidentally, when one examines these successes as reported in the media, one observes that they all have in common a nearly infinite sample size (consider that WhatsApp record billions of conversations every year) and a nearly infinite signal-to-noise ratio (SNR) or, put differently, very little uncertainty. If you give me a photograph, there is little debate as to whether there is a cow on it or not. These instances are far from the settings statisticians are accustomed to: moderate sample sizes, a higher degree of uncertainty, and variables/factors which have not been measured—prosaically, things we cannot see. Using Efron’s example, it is not because I know how much you comply with a treatment that I can be certain about how much it will be of benefit to you. Interestingly, Efron puts the pure predictive algorithms to the test in a scenario where the sample size is extremely moderate by today’s standards (nD 102). In the prostate cancer microarray study, he observes the following: split the sample at random as to create a training and test set with the same number of cases and controls, and train a random forest. Perhaps to his surprise, random forests perform extremely well correctly classifying all 51 test patients but one! Surely, we must have “learned” something valuable. Or have we? His second finding is this: if we split patients according to what we believe is the time of entry in the study, then the performance is spectacularly degraded; that is, he now records 12 wrong predictions corresponding to an error rate of about one in four. So what have we really learned? This is all the more discomforting since prediction rules are intended to be deployed in the second scenario, and not in the highly idealized setting where test and training samples are exchangeable or, said differently,",International Statistical Review,2020.0,10.1111/insr.12412,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a269f88459ac85f137bfca86f9332d81284fa310,https://www.semanticscholar.org/paper/a269f88459ac85f137bfca86f9332d81284fa310,Analyzing Objective and Subjective Data in Social Sciences: Implications for Smart Cities,"The ease of deployment of digital technologies and the Internet of Things gives us the opportunity to carry out large-scale social studies and to collect vast amounts of data from our cities. In this paper, we investigate a novel way of analyzing data from social sciences studies by employing machine learning and data science techniques. This enables us to maximize the insight gained from this type of studies by fusing both objective (sensor information) and subjective data (direct input from the users). The pilot study is concerned with better understanding the interactions between citizens and urban green spaces. A field experiment was carried out in Sheffield, U.K., involving 1870 participants for two different time periods (7 and 30 days). With the help of a smartphone app, both objective and subjective data were collected. Location tracking was recorded as people entered any of the publicly accessible green spaces. This was complemented by textual and photographic information that users could insert spontaneously or when prompted (when entering a green space). By employing data science and machine learning techniques, we identify the main features observed by the citizens through both text and images. Furthermore, we analyze the time spent by people in parks as well as the top interaction areas. This paper allows us to gain an overview of certain patterns and the behavior of the citizens within their surroundings and it proves the capabilities of integrating technology into large-scale social studies.",IEEE Access,2019.0,10.1109/ACCESS.2019.2897217,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
09a4ab6a61eaa69d1ce496a4f291d565329fc8d5,https://www.semanticscholar.org/paper/09a4ab6a61eaa69d1ce496a4f291d565329fc8d5,eResearch Collaboration Projects-supporting CSIRO's digital science and research,"Background CSIRO is Australia’s largest research agency and is a recognised leader in a diverse set of
science domains: Agricultural Sciences, Environment/Ecology, Plant and Animal Sciences,
Geosciences, Chemistry and Materials Science. CSIRO also manages research infrastructure
like the Australia Telescope National Facility (ATNF), the Marine Research Vessel RV
Investigator and the Pawsey Supercomputing Centre. For many years in Australia, and also worldwide [2], research and science have undergone
transformational changes with the introduction of new instruments and advanced facilities
with matching increases in storage and computing capabilities. Individual researchers were
taking a bespoke approach to matching these technologies and capabilities to the way that
research and science were carried out. Wider adoption of new practices required social
change (in the practice of science and research) and these changes remained fragmented
and tailored to specific sciences or even projects. Organisations, by and large, varied
enormously in their support of these new practices.As far back as 2007 [1], CSIRO eResearch practitioners advocated that science and research
practices within CSIRO adapt to deal with these challenges. Much like the rest of the world,
practices matured over the years: in CSIRO’s health and biosecurity, oceanographic and
atmospheric research, radio astronomy, agriculture and food as well as geological and
other earth sciences. However, a significant shift occured in 2018, with a formal recognition by the CSIRO Board
of the need to support the new “digital” science and research at an organisational level.
CSIRO developed strategic digital transformation initiatives, including CSIRO’s Managed
Data Ecosystem (MDE), Missions and the Digital Academy [4].The aim of the MDE is to connect current and new platforms in a seamless way and improve
interoperability between datasets so users will be able to easily find and work on multiple
datasets. It will provide a set of tools and approaches enabling CSIRO and partners to
improve our collaboration, mining and analysis of data. CSIRO Missions are major scientific and collaborative research programs aimed at making
significant breakthroughs in one of six major challenges facing Australia. They include the
resilient and valuable environments, food security and quality, health and well-being, future
industries, sustainable energy and resources, and regional security. CSIRO's Digital Academy is focused on investing in the digital capability of our staff and
involves a rethink in planning for a digitally driven research environment. It provides a
learning opportunity for our staff, helping define the digital talent, skills and new ways of
working. The Academy will help attract and retain new digital talent within the Australian
innovation system, develop new digital skills and mindsets in Australian’s scientists and
facilitate digital talent accessibility and collaboration across Australia’s innovation system.Existing Support for “Digital” Science through “eResearch” initiativesCSIRO Scientific Computing Services group has been providing a dedicated eResearch service
since 2011 [3] This service is delivered through ""eResearch Collaboration Projects” (eRCPs)
which now delivers specialist capabilities that includes Machine Learning, Data Analytics,
Scientific Visualisation, Workflow Management and Science Data Handling into research and
science projects. The eRCP process is run as a competitive grant process and continues to be very successful. In the latest cycle, forty Scientific Computing Services specialists successfully completed and
delivered over sixty eRCPs outcomes from a total of eighty submissions. The underlying
capabilities are delivered by members from each of teams in the Scientific Computing
Services group: Technical Solutions; Data Analytics and Visualisation; Research Software
Engineering; and Modelling and Dataflow. The eRCP process also provides a mechanism to
promote and introduce new tools and frameworks for consumption to CSIRO’s research
community eg Jupyter and R/Shiny. In the latest cycle, forty Scientific Computing Services specialists successfully completed and
delivered over sixty eRCPs outcomes from a total of eighty submissions. The underlying
capabilities are delivered by members from each of teams in the Scientific Computing
Services group: Technical Solutions; Data Analytics and Visualisation; Research Software
Engineering; and Modelling and Dataflow. The eRCP process also provides a mechanism to
promote and introduce new tools and frameworks for consumption to CSIRO’s research
community eg Jupyter and R/Shiny. Specialists from the Scientific Computing program are then assigned to work on one or more
approved eRCPs. Over the six-month cycle, the resource allocation is around 0.2 FTE, with
each staff member allocated 3 eRCP projects per cycle. Importantly, eRCPs are provided to
CSIRO researchers and scientists at no additional charge.The eRCP has been enormously successful over the years, with demand outstripping
capability to allocate staff to the projects. The program has demonstrated a range of useful
outcomes including – including for example - an augmented reality tool for analysing
bushfire plumes over Tasmania; a dashboard to interrogate cotton crop physiological
measurements and an online platform to monitor algal blooms for multiple water bodies.Scientific Computing specialists also provide dedicated support to CSIRO researchers, based
around the same set of core capabilities, via an entirely separate funding models known as
“pan deployments” as well as secondments. In both cases, CSIRO projects fund the specialists’
time at larger allocations, often extending over 12 months or more. In a sense, this acts like a
contractor service for Business Units, providing them with highly specialised skills but without
the need to recruit new staff of their own.Future PlansCSIRO Scientific Computing will respond to the major initiatives – MDE, Digital Academy and
Missions as follows:MDE - Redirect Scientific Computing expertise currently working on eRCPs and pan
deployments to MDE related activities. In the first instance, these specialists
will apply their skills and domain knowledge to one of several nominated
pilots, helping design and build foundational components of the MDE. - Over time, it is anticipated that those same specialists will contribute to the
ongoing development and enhancement of additional MDE components in
line with its progressive organisational rollout. Digital Academy - Develop/adapt training content as appropriate for the Digital Academy. For
example, making use of existing Software Carpentry material for HPC usage,
but customising appropriate aspects for our own computing environment.- Delivering training content to CSIRO staff. This has already proven very
successful in the machine learning area – with hundreds of staff attending
sessions - and will no doubt continue to grow over time.Missions - Scientific Computing will continue to provide CSIRO researchers with the
eResearch support they need in response to the significant scientific
challenges tackling Missions. REFERENCES 1. J. A. Taylor, J. Zic, and J. Morrissey, “Building CSIRO e-Research Capabilities,” in eResearch Australasia 2008.2. T. Hey, S. Tansley, and K. Tolle, “The Fourth Paradigm: Data-Intensive Scientific
Discovery,” Data-Intensive Sci. Discov. Microsoft Res., 2009.3. S. Moskwa, “The Accelerated Computing Initiative,” in eResearch Australasia, 2012.4. CSIRO Chief Executive's Report 2018-19: https://www.csiro.au/en/About/Ourimpact/Reporting-our-impact/Annual-reports/18-19-annual-report/part-1/chiefexecutive-reportABOUT THE AUTHOR(S) Dr John Zic is the Executive Manager of CSIRO’s Science Computing Services Mr Justin Baker is Leader of the Scientific Computing Data Analytics and Visualisation
Team.",,2020.0,10.6084/M9.FIGSHARE.11929647.V1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c600aac6059fbd3c387aedc34f4e9d8ff34c07bd,https://www.semanticscholar.org/paper/c600aac6059fbd3c387aedc34f4e9d8ff34c07bd,Benchmarks and Process Management in Data Science: Will We Ever Get Over the Mess?,"This panel aims to address areas that are widely acknowledged to be of critical importance to the success of Data Science projects and to the healthy growth of KDD/Data Science as a field of scientific research. However, despite this acknowledgement of their criticality, these areas receive insufficient attention in the major conferences in the field. Furthermore, there is a lack of actual actions and tools to address these areas in actual practice. These areas are summarized as follows: 1. Ask any data scientist or machine learning practitioner what they spend the majority of their time working on, and you will most likely get an answer that indicates that 80% to 90% of their time is spent on ""Data Chasing"", ""Data Sourcing"", ""Data Wrangling"", ""Data Cleaning"" and generally what researchers would refer to-often dismissively-as ""Data Preparation"". The process of producing statistical or data mining models from data is typically ""messy"" and certainly lacks management tools to help manage, replicate, reconstruct, and capture all the knowledge that goes in 90% of activities of a Data Scientists. The intensive Data Engineering work that goes into exploring and determining the representation of problem and the significant amount of ""data cleaning"" that ensues creates a plethora of extracts, files, and many artifacts that are only meaningful to the data scientist. 2. The severe lack of Benchmarks in the field, especially ones at big data scale is an impediment to true, objective, measurable progress on performance. The results of each paper are highly dependent on the large degree of freedom an author has on configuring competitive models and on determining which data sets to use (often Data that is not available to others to replicate results on) 3. Monitoring the health of models in production, and deploying models into production environments efficiently and effectively is a black art and often an ignored area. Many models are effectively ""orphans"" with no means of getting appropriate health monitoring. The task of deploying a built model to production is frequently beyond the capabilities of a Data Scientists and the understanding of the IT team.",KDD,2017.0,10.1145/3097983.3120998,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
307a2fa0c8e77f91ff1c0f7b2471c030078464ad,https://www.semanticscholar.org/paper/307a2fa0c8e77f91ff1c0f7b2471c030078464ad,Exergames in motor skill learning.,"IntroductionExergaming, by joining physical activity with Natural User Interfaces and Gesture Recognition technology, addresses the fundamental questions of perception and phenomenology in a digital context, with awareness that embodied actions within a digital media interface are ""fluid and functional crossings between virtual and physical realms."" (Hansen 2006)""Natural User Interface is the next metaphysical paradigm shiftin man machine interaction (MMI) also known as human computer interaction (HCI). Beginning with the Command Line Interface (CLI) and followed by the Graphical User Interface (GUI), we are now in the midst of discovering the next phase of a more organic interfaces which are based on more traditional human interaction paradigms such as touch, vision, speech and most importantly creativity."" (NUIGroup 2009)The Natural User Interfaces include input and output based on touch, voice, movements and move towards an efficient use of the senses in the interaction with machines.The origin of term ""exergame"" is uncertain: as noted Sinclair, earliest citation of ""exergame"", according to Wordspy website, is by RajuNudhar on the Toronto Star in 2004.In a bid to unite jocks with nerds in a way never seen before, Toronto-based Nexfit has introduced Canada's first exer-gaming bike. ...At its most basic, the product is an exercise bike that hooks up to a personal computer. The bike serves as a giant joystick. You use the handlebars as the controller, and above the right hand grip are the traditional joystick controls with a trigger button. For racing games, the pedals of the bike serve as the accelerator. There are also ""force feedback"" simulators so the rumble effect you get on a console's controller is emulated by the entire bike.RajuNudhar, ""In hot pursuit of the Holodeck,"" The Toronto Star, April 20, 2004Sinclair also noted as ""exergames are also referred to using the following terms; Activity promoting video games (Lanningham-Foster, Jensen et al.); interactive video games (Hoysniemi, 2006; Luke,2005), and exertion interfaces (Mueller, Agamanolis et al. 2003 2003, Yang, Smith et al. 2008).""(Sinclair 2011)In a study on the definition of exergames, Oh and Yang(Oh and Yang 2010) have proposed a classification of terms with which exergames are referred in scientific studies, distinguishing between health-related contexts and non health-related contexts. These terms are: Exergame, Exertainment, Dance simulation video game , Interactive video game, Activity promoting video game, Active video game, Physical gaming, Kinaesthetic of video gaming, Physical activity-change games.Given this state of the art, we agree with Yang (Yang, Smith et al. 2008), in affirming that this area of study ""is still in its infancy"".The definition of exergame used herein was provided by Bogost:""Exergaming is the combination of exercise and video games"" (Bogost 2007)It should be emphasized, however, as exergames involve the whole body of the player in the human-machine interaction process, making use of special devices such as MicrosoftKinect, the Nintendo Wiimote and BalanceBoard, just to mention the most well-known. (Di Tore and Raiola 2012)MethodsThis paper presents an excursus of the literature on exergames and summarizes the state of the art of research in this field, in an effort to identify the theoretical and didactic foundations of exergames design, in order to see if there are exergames that are attributable to a specific theoretical framework within Physical Education or that are specifically oriented to teaching or rehabilitation.This work intends to provide a preliminary analysis of the scientific literature on the subject of exergames, in order to verify whether there are scientific works in which have been studied and presented exergames specifically designed for educational purposes or rehabilitation purposes.ResultsSince the first deployment, exergames have received, in science, a positive reception, by virtue of two factors:involvement in the field of gaming the entire body of the player is a powerful tool in the longstanding battle against sedentary lifestyle (Chamberlin and Gallagher 2008);the level of user involvement, already high in traditional videogames, with exergames becomes even more valuable due to the NUIs (Baranowski, Buday et al. …",,2012.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d6473f67233f0a7e98139596b39106325ec79b28,https://www.semanticscholar.org/paper/d6473f67233f0a7e98139596b39106325ec79b28,Introduction to the Minitrack on Design and Appropriation of Knowledge and AI Systems,"The objective of this minitrack is to contribute to the body of knowledge that helps scholars and practitioners increase their collective understanding of (1) how knowledge and artificial intelligence (AI) systems are planned, designed, built, implemented, used, evaluated, supported, upgraded, and evolved; (2) how knowledge and AI systems impact the context in which they are embedded; and (3) the human behaviors reflected within and induced through both (1) and (2). By knowledge and AI systems, we mean systems in which human participants and/or machines perform work (processes and activities) related to the creation, retention, transfer and/or application of knowledge using information, technology, and other resources to produce informational products and/or services for internal or external customers. Such systems may include, but are not limited to, knowledge management systems, decision systems, social media, expert systems, machine learning systems, and other AI systems as well as any other IT-enabled knowledge processes. It is the ninth year of the minitrack. We received seven papers this year and after a rigorous review process, we accepted three for publication in the proceedings and online presentation at the conference. The first paper, co-authored by Ali Intezari, Morteza Namvar, and Ramin Taghinejad, aims at providing a theoretical framework that explains the complexity of knowledge in an organizational context, and how knowledge management (KM) can be integrated into an organization’s existing enterprise system. To do so, the authors introduce the concept of knowledge identity (KI) that refers to the collective construal that an organization’s members have about their previous, current and future knowledge needs. They also provide a critical perspective about the assumptions that underline organizations’ practice of KM and argue how these assumptions influence KI. In the second paper, which is nominated for the HICSS-54 2021 Best Paper Award, Camille Grange and Alain Pinsonneault develop a theoretical framework that emphasizes the socio-ethical factors surrounding the decision made by organizations to deploy a highly automated decision-making system. Previous literature on adoption and intention to adopt omits inquiring whether the adoption decision or the process leading to the decision is socially responsible. Thus, in developing their framework, the authors specify a new set of constructs to reflect the phenomenon and introduce the theoretical logic underlying the relationships between these constructs. The third paper, co-authored by Peter Hofmann, Philipp Stähle, Christoph Buck, and Harald Thorwarth, addresses the lack of guidance on the development and application of data-driven applications fostering an organization’s absorptive capacity. Based on a structured literature review, the authors derive seven data-driven application capabilities and match them with an established conceptualization of absorptive capacity. While previous literature did not allow for a specific analysis, the authors’ functional representation concretely demonstrates how data-driven applications composed of separate capabilities can foster absorptive capacity in different ways. We wish to thank all of the authors who submitted work for consideration in this minitrack. We also thank the dedicated reviewers for the time and effort they invested in reviewing the papers. We believe that the accepted papers contribute to furthering our understanding on the creation and appropriation of knowledge and AI systems. All author teams provide presentation slides and/or a presentation video that could be accessed during the conference in January 2021. We hope that all provided materials (papers, slides, videos) inspire discussions, networking, and new ideas and research endeavors. Proceedings of the 54th Hawaii International Conference on System Sciences | 2021",HICSS,2020.0,10.24251/hicss.2020.572,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9140a96f959b2c2b1ac74eb0f3caeccffcd0d17d,https://www.semanticscholar.org/paper/9140a96f959b2c2b1ac74eb0f3caeccffcd0d17d,Scope of Field: On Defining Big Data Analytics for Field Development in Research & Curricular Design,"Big data increasingly drives business models, marketing methods, regulatory techniques, novel intrusion methods and national security practices. Is big data analytics a “real discipline?” If so, it is appropriate to design an ontology, essentially a comprehensive schema acknowledging sub-topics of concern and their relationships. Taxonomies are the foundation for inspiring future research and designing curricula by identifying issues, themes and inevitable gaps. Technical disciplines are primary drivers defining current practice and inspiring innovation. However, technologists devote scant attention to public policy ramifications or predictable regulatory counter-measures. Big data is the primary raw material/feedstock to machine learning and artificial intelligence (AI). AI algorithms are generally opaque, black box decision-makers operating outside traditional democratic transparency. Further, Intellectual property (IP) is under-researched in metering how big data flows through the information supply chain. Big data analytics lie at the heart of financial technology (FinTech) and regulatory technologies (RegTech). Most technical innovations cause regulatory lag (RegLag) evidenced by delayed and reactionary public policy response: (i) phenomena discovery, (ii) theorizing, (iii) interpretation of control mechanism efficacy, and (iv) deployment of adjustments to rights, duties and opportunities. This paper reviews authoritative sources enabling synthesis of big data experience and predictions of future scholarly public policy research. Structural implications emerge for curricula design in public policy incidents of data science, big data analytics, AI and machine learning.",,2020.0,10.2139/ssrn.3631182,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d76a715a073fb3ada7b42864c58b42b13cb0ee92,https://www.semanticscholar.org/paper/d76a715a073fb3ada7b42864c58b42b13cb0ee92,NeuroKube: An Automated and Autoscaling Neuroimaging Reconstruction Framework using Cloud Native Computing and A.I.,"The Neuroscience domain stands out from the field of sciences for its dependence on the study and characterization of complex, intertwining structures. Understanding the complexity of the brain has led to widespread advances in the structure of large-scale computing resources and the design of artificially intelligent analysis systems. However, the scale of problems and data generated continues to grow and outpace the standards and practices of neuroscience. In this paper, we present an automated neuroscience reconstruction framework, called NeuroKube, for large-scale processing and labeling of neuroimage volumes. Automated labels are generated through a machine-learning (ML) workflow, with data-intensive steps feeding through multiple GPU stages and distributed data locations leveraging autoscalable cloud-native deployments on a multi-institution Kubernetes system. Leading-edge hardware and storage empower multiple stages of machine-learning, GPU-accelerated solutions. This demonstrates an abstract approach to allocating the resources and algorithms needed to elucidate the highly complex structures of the brain. We summarize an integrated gateway architecture, and a scalable workflow-driven segmentation and reconstruction environment that brings together image big data with state-of-the-art, extensible machine learning methods.",2020 IEEE International Conference on Big Data (Big Data),2020.0,10.1109/BigData50022.2020.9378053,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c6ce7fd941de2b1b9863cc8e43bc73a004343e52,https://www.semanticscholar.org/paper/c6ce7fd941de2b1b9863cc8e43bc73a004343e52,Human-Centered AI through Scalable Visual Data Analytics,"While artificial intelligence (AI) has led to major breakthroughs in many domains, understanding machine learning models remains a fundamental challenge. They are often used as ”black boxes,” which could be detrimental. How can we help people understand complex machine learning models, so that they can learn them more easily and use them more effectively? In this talk, I present my research that makes AI more accessible and interpretable, through a novel human-centered approach, by creating novel data visualization tools that are scalable, interactive, and easy to learn and to use. I present my work in two interrelated topics. (1) Visualization for Industry-scale Models: I present how to scale up interactive visualization tools for industry-scale deep learning models that use large datasets. I describe how the ActiVis system helps Facebook data scientists interpret deep neural network models by visually exploring activation flows. ActiVis is patent-pending, and has been deployed on Facebook’s ML platform. (2) Interactive Understanding of Complex Models: I show how visualization helps novices interactively learn complex concepts of deep learning models. I describe how I developed GAN Lab, a visual education system for Generative Adversarial Networks (GANs), one of the most popular, but hard-to-understand models. GAN Lab has been open-sourced in collaboration with Google Brain and used by over 30,000 people from 140 countries. I conclude with my vision to make AI more human-centered, to promote actionability for AI, stimulate a stronger ethical AI workforce, and foster healthy impacts of AI on broader society. Monday, April 1, 2019, 10:00 am Planetarium E300 MSC Computer Science Emory University",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
56c46da2a66e2685be88ce096e8495b3ca06be9b,https://www.semanticscholar.org/paper/56c46da2a66e2685be88ce096e8495b3ca06be9b,Deep Synthesis and De-Identification of Large Data Sets: A Comparative Analysis,"Embracing the latest innovations in data analytics has become critical to compete in both the private and public sectors. Unfortunately, the data which yields the most meaningful results is often the most sensitive, introducing a potential risk if proper security measures are not taken. Moreover, this data is aggregated in large sums. With the increased use of sensitive data, particularly in machine learning applications, advanced data augmentation techniques introduce a means by which advanced analytics can be practiced securely and effectively in sensitive domains. In this paper, we present two methods –Pseudonymization and Generative Adversarial Networks (GANs)– to deidentify data and protect the privacy of entities in data at rest on sensitive IT systems for secure use outside those systems. The former method is often used to test and optimize code by providing realistic values at the aggregate feature level. The latter can be used to train machine learning models and perform statistical analyses by learning the underlying distribution of specific observations in a manner that does not compromise the original records. The GANs in this study leverage Convolutional Neural Networks (CNNs) in the traditional arrangement of a generator and a discriminator, with a third CNN enforcing the syntactical relationship between features. We test our performance with a statistical evaluation of the underlying distributions of the synthetic features against the original feature vectors from which they were generated, visualizing high-dimensional relationships between data sets, and comparing supervised cross-validation scores on the synthetic data to those of models trained on the real data. Results showed strong statistical relationships between real and synthetic features but variable model performance across datasets. ABOUT THE AUTHORS Mr. Eric White (Booz Allen Hamilton) is a Data Engineer, providing analytical support and managing web presence. He specializes in building and optimizing scalable ETL pipelines from unstructured data and disaggregate systems. He holds a M.A. in Economics and Business Modelling and Simulation as well as a B.S. in International Business, both from Old Dominion University. Mr. Justin Whitlock (Booz Allen Hamilton) is a Data Scientist. Professional interests include signal processing and analysis, natural language processing, modeling, simulation and linguistics. He holds a M.S. in Computer Science from Old Dominion University, a B.A. in International Studies from University of Nebraska at Omaha, and an A.A. in Arabic Language & Middle Eastern Studies from Defense Language Institute at Presidio of Monterey. Mr. Andrew Turscak (Booz Allen Hamilton) is a Data Scientist, responsible for analytical support, model deployment, and R&D. Professional interests include natural language processing, network analytics, game theory, and simulation. He holds a M.S. in Computational Operations Research and B.A. in Economics, both from the College of William & Mary.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a110c204289f927158d87cd63b18b6c16bcf1dc5,https://www.semanticscholar.org/paper/a110c204289f927158d87cd63b18b6c16bcf1dc5,Accelerating Microstructural Analytics with Dask for Volumetric X-ray Images,"While X-ray microtomography has become indispensable in 3D inspections of materials, efficient processing of such volumetric datasets continues to be a challenge. This paper describes a computational environment for HPC to facilitate parallelization of algorithms in computer vision and machine learning needed for microstructure characterization and interpretation. The contribution is to accelerate microstructural analytics by employing Dask high-level parallel abstractions, which scales Numpy workflows to enable multi-dimensional image analysis of diverse specimens. We illustrate our results using an example from materials sciences, emphasizing the benefits of parallel execution of image-dependent tasks. Preliminary results show that the proposed environment configuration and scientific software stack deployed using JupyterLab at NERSC Cori enables near-real time analyses of complex, high-resolution experiments.",2020 IEEE/ACM 9th Workshop on Python for High-Performance and Scientific Computing (PyHPC),2020.0,10.1109/PyHPC51966.2020.00010,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8e4c45c6c6e59d803781ca6b3cb59ad3b5d036dc,https://www.semanticscholar.org/paper/8e4c45c6c6e59d803781ca6b3cb59ad3b5d036dc,Three Challenges in Building Industrial-Scale Recommender Systems,"We have seen astonishing progress of machine learning research in the last years. Unfortunately, it is often difficult to translate this academic progress into deployable applications, due to the constraints and challenges imposed by production settings. In this talk, I will present some of my recent research in the area of data management for machine learning, which tackles these problems. Furthermore, I will put a special focus on three challenges that I see for building industrial-scale recommender systems. In particular, I will outline ideas on how to scale to datasets with billions of interactions, understand the impact of response latency on the performance of a deployed recommender system, and make the ""right to be forgotten"" a first-class citizen in real-world ML systems. Reference Format: Sebastian Schelter. 2020. Three Challenges in Building Industrial-Scale Recommender Systems. In 3rd Workshop on Online Recommender Systems and User Modeling (ORSUM 2020), in conjunction with the 14th ACM Conference on Recommender Systems, September 25th, 2020, Virtual Event, Brazil. SPEAKER BIOGRAPHY Sebastian Schelter is an Assistant Professor with the University of Amsterdam, conducting research at the intersection of data management and machine learning. He manages the AI for Retail Lab Amsterdam, and has a joint appointment as Research Fellow at Ahold Delhaize, an international retailer based in the Netherlands. His work covers many aspects, such as automating data quality validation, optimizing programs that combine operations from linear and relational algebra or tracking the lineage of machine learning pipelines. In the past, he has been a Faculty Fellow with the Center for Data Science at New York University and a Senior Applied Scientist at Amazon Research, after obtaining his Ph.D. at the database group of TU Berlin with Volker Markl. He is active in open source as an elected member of the Apache Software Foundation, and has extensive experience in building real world systems from my time at Amazon, Twitter, IBM Research, and Zalando. ORSUM@ACM RecSys 2020, September 25th, 2020, Virtual Event, Brazil Copyright© 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",ORSUM@RecSys,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
646b53d162cf6754aff28c6923663d0eeeafa427,https://www.semanticscholar.org/paper/646b53d162cf6754aff28c6923663d0eeeafa427,Special issue on quality management for information systems,,Software Quality Journal,2020.0,10.1007/s11219-020-09516-z,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b01665121efe7b61bf8a5180cdb0decaf874f891,https://www.semanticscholar.org/paper/b01665121efe7b61bf8a5180cdb0decaf874f891,Adaptive Terminal Sliding Mode Control For Nonlinear,"Modelling and Control of Mechatronic and Robotic SystemsAdvanced Machine Learning Technologies and ApplicationsUnmanned Driving Systems for Smart TrainsInformatics in Control, Automation and RoboticsCyberspace Safety and SecurityAdaptive Identification and Control of Uncertain Systems with Non-smooth DynamicsElectrical Engineering and ControlSoft Computing: Theories and ApplicationsElectronics and Signal ProcessingAdvances in Reconfigurable Mechanisms and Robots IControl of Chaos in Nonlinear Circuits and SystemsSliding Modes after the first Decade of the 21st CenturyModeling and Control of Static Converters for Hybrid Storage SystemsInternational Conference on Electrical, Control and Automation (ICECA 2014)Sliding Mode Controllers for Power Electronic ConvertersIntelligent Computing Theories and ApplicationCognitive Systems and Signal ProcessingInertial Quasi-Velocity Based Controllers for a Class of VehiclesApplied Methods and Techniques for Mechatronic SystemsUnmanned Aerial SystemsSliding Mode Control (SMC)Variable Structure Systems: Towards the 21st CenturyControl and AutomationA Robust Adaptive Terminal Sliding Mode Control for Rigid Robotic ManipulatorsWind Turbine Control and MonitoringAdvances in Variable Structure Systems and Sliding Mode Control—Theory and ApplicationsDynamical SystemsMICAI 2004: Advances in Artificial IntelligenceControl of Power Electronic Converters and SystemsCommunications in MicrogridsElectrical, Information Engineering and Mechatronics 2011Advanced Robust Nonlinear Control Approaches for Quadrotor Unmanned Aerial VehicleProceedings of 2016 Chinese Intelligent Systems ConferenceStudy on Adaptive Fuzzy Terminal Sliding-Mode Control and Adaptive Terminal Sliding-Function Control2011 International Conference in Electrics, Communication and Automatic Control ProceedingsAdaptive Sliding Mode Neural Network Control for Nonlinear SystemsMem-elements for Neuromorphic Circuits with Artificial Intelligence ApplicationsProceedings of the International Conference on Advanced Intelligent Systems and Informatics 2019Mathematical Techniques of Fractional Order SystemsIntelligent Computing Methodologies Adaptive Identification and Control of Uncertain Systems with Nonsmooth Dynamics reports some of the latest research on modeling, identification and adaptive control for systems with nonsmooth dynamics (e.g., backlash, dead zone, friction, saturation, etc). The authors present recent research results for the modelling and control designs of uncertain systems with nonsmooth dynamics, such as friction, dead-zone, saturation and hysteresis, etc., with particular applications in servo systems. The book is organized into 19 chapters, distributed in five parts concerning the four types of nonsmooth characteristics, namely friction, dead-zone, saturation and hysteresis, respectively. Practical experiments are also included to validate and exemplify the proposed approaches. This valuable resource can help both researchers and practitioners to learn and understand nonlinear adaptive control designs. Academics, engineers and graduate students in the fields of electrical engineering, control systems, mechanical engineering, applied mathematics and computer science can benefit from the book. It can be also used as a reference book on adaptive control for servo systems for students with some background in control engineering. Explains the latest research outputs on modeling, identification and adaptive control for systems with nonsmooth dynamics Provides practical application and experimental results for robotic systems, and servo motorsAdvances in Reconfigurable Mechanisms and Robots I provides a selection of key papers presented in The Second ASME/IFToMM International Conference on Reconfigurable Mechanisms and Robots (ReMAR 2012) held on 9th -11th July 2012 in Tianjin, China. This ongoing series of conferences will be covered in this ongoing collection of books. A total of seventy-eight papers are divided into seven parts to cover the topology, kinematics and design of reconfigurable mechanisms with the reconfiguration theory, analysis and synthesis, and present the current research and development in the field of reconfigurable mechanisms including reconfigurable parallel mechanisms. In this aspect, the recent study and development of reconfigurable robots are further presented with the analysis and design and with their control and development. The bio-inspired mechanisms and subsequent reconfiguration are explored in the challenging fields of rehabilitation and minimally invasive surgery. Advances in Reconfigurable Mechanisms and Robots I further extends the study to deployable mechanisms and foldable devices and introduces applications of reconfigurable mechanisms and robots. The rich-content of Advances in Reconfigurable Mechanisms and Robots I brings together new developments in reconfigurable mechanisms and robots and presents a new horizon for future development in the field of reconfigurable mechanisms and robots.Mem-elements for Neuromorphic Circuits with Artificial Intelligence Applications illustrates recent advances in the field of mem-elements (memristor, memcapacitor, meminductor) and their applications in nonlinear dynamical systems, computer science, analog and digital systems, and in neuromorphic circuits and artificial intelligence. The book is mainly devoted to recent results, critical aspects and perspectives of ongoing research on relevant topics, all involving networks of mem-elements devices in diverse applications. Sections contribute to the discussion of memristive materials and transport mechanisms, presenting various types of physical structures that can be fabricated to realize mem-elements in integrated circuits and device modeling. As the last decade has seen an increasing interest in recent advances in mem-elements and their applications in neuromorphic circuits and artificial intelligence, this book will attract researchers in various fields. Covers a broad range of interdisciplinary topics between mathematics, circuits, realizations, and practical applications related to nonlinear dynamical systems, nanotechnology, analog and digital systems, computer science and artificial intelligence Presents recent advances in the field of mem-elements (memristor, memcapacitor, meminductor) Includes interesting applications of mem-elements in nonlinear dynamical systems, analog and digital systems, neuromorphic circuits, computer science and artificial intelligenceThis volume includes extended and revised versions of a set of selected papers from the International Conference on Electric and Electronics (EEIC 2011) , held on June 20-22 , 2011, which is jointly organized by Nanchang University, Springer, and IEEE IAS Nanchang Chapter. The objective of EEIC 2011 Volume 2 is to provide a major interdisciplinary forum for the presentation of new approaches from Electrical engineering and controls, to foster integration of the latest developments in scientific research. 133 related topic papers were selected into this volume. All the papers were reviewed by 2 program committee members and selected by the volume editor Prof. Min Zhu. We hope every participant can have a good opportunity to exchange their research ideas and results and to discuss the state of the art in the areas of the Electrical engineering and controls.Adaptive Sliding Mode Neural Network Control for Nonlinear Systems introduces nonlinear systems basic knowledge, analysis and control methods, and applications in various fields. It offers instructive examples and simulations, along with the source codes, and provides the basic architecture of control science and engineering. Introduces nonlinear systems' basic knowledge, analysis and control methods, along with applications in various fields Offers instructive examples and simulations, including source codes Provides the basic architecture of control science and engineeringThis book proposes a proportional integral type sliding function, which does not facilitate the finite reaching and hence the responses of the load voltage results in an exponential steady state. To facilitate finite time reaching, it also presents the new Integral Sliding Mode Control with Finite Time Reaching (ISMCFTR). The book also extends the application of the proposed controller to another type of PEC, the DC-DC Boost converter, and also proposes the PI type sliding surface for the Zeta converter, which is non-inverting type Buck Boost converter. An important source of practical implementations, it presents practical implementations as simulation and experimental results to demonstrate the efficacy of the converter.Control of Power Electronic Converters and Systems, Volume 3, explores emerging topics in the control of power electronics and converters, including the theory behind control, and the practical operation, modeling, and control of basic power system models. This book introduces the most important controller design methods, including both analog and digital procedures. This reference explains the dynamic characterization of terminal behavior for converters, as well as preserving the stability and power quality of modern power systems. Useful for engineers in emerging applications of power electronic converters and those combining control design methods into different applications in power electronics technology. Addressing controller interactions in light of increasing renewable energy integration and related challenges with stability and power quality is becoming more frequent in power converters and passive components. Discusses different applications and their control in integrated renewable energy systems Introduces the most important controller design methods, both in analog and digital Describes different important applications to be used in future industrial products Explains the dynamic characterization of terminal behavior for convertersThis book presents the refereed proceedin",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d90e2746a5a8e6a713aff7fcd8e2ab5fd64657f8,https://www.semanticscholar.org/paper/d90e2746a5a8e6a713aff7fcd8e2ab5fd64657f8,Introduction to the Minitrack on Design and Application of Body Sensor Systems in Healthcare,"Body sensor systems are becoming increasingly ubiquitous and have amazing potential to revolutionize healthcare delivery. Sensors come in a variety of form factors (e.g. wearable, ambient, ingestible or injectable) and can measure a plethora of physiologic and behavioral states in a near continuous fashion. Integrated into networks of sensors, these body sensor systems offer a mechanism to integrate technology into daily life. Perhaps most promising is the potential of such sensor systems to contextualize sensor data, to provide understanding of the milieu in which changes in health occur and to intervene in an automated, real time fashion to change behavior and promote health. Many challenges need to be overcome before body sensor systems can be widely adopted and highly effective. Designing sensors that are highly accurate while being non-obtrusive and aesthetically acceptable to users is a top priority for designers. Optimization of machine learning approaches, big data management, and privacy are other considerations that will influence success and uptake. In addition, the design process must involve the end users, maximize acceptability and intercalate with daily life. The ideal end product is a body sensor system that can track specific behaviors, but also create a new opportunity to deliver feedback, corrective interventions and other behavioral modifications in the setting of detection of specific parameters. Last year’s mini-track focused on the state of art of wearable sensor systems and generated recommendations regarding design of closed loop sensor systems and methods for evaluating sensor systems. This year’s mini-track builds upon last year’s work by discussing deployments of body sensor systems, interpretation of data generated from systems and the implications that body sensor systems have on understanding health, wellness and populations. This year, the mini-track has two exciting papers that describe practical and innovative approaches to body sensor system applications. In our first paper, the authors leverage off-the-shelf wearable devices and on-board smartphone sensors to contextualize the daily activity of adolescents and quantify opportunities to increase academic engagement during the daily activities of students. Our second paper discusses the use of contactless body sensors in the form of ultra-wide band radar to detect the presence of an individual in a room and measure respiratory rate noninvasively. The authors of both papers will describe novel applications of body sensor systems for unobtrusive detection of biometrics that may correlate with key features of daily life or health status. Importantly, these papers establish important preliminary data which can be used to develop automated interventions, whether the context of academic engagement or healthcare Following the paper presentations, we will open the forum for discussion on the material presented. We will use the challenges and successes from each paper to explore how this information can immediately inform research, commercial and clinical endeavors. We will also crowdsource ideas for a follow-up mini-track that will fill identified gaps and be highest yield for future HICSS participants. We anticipate that presentations and discussion will help attendees consider key design factors in their own work evaluating body sensors and generate new ideas for collaborations and applications in this space. Proceedings of the 53rd Hawaii International Conference on System Sciences | 2020",HICSS,2020.0,10.24251/hicss.2020.410,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0fa308b9025c563a9e33ad92e9b2cc5e8d00dafc,https://www.semanticscholar.org/paper/0fa308b9025c563a9e33ad92e9b2cc5e8d00dafc,Evaluating the Transferability of Personalised Exercise Recognition Models,,EANN,2020.0,10.1007/978-3-030-48791-1_3,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2a21fc370a7e0854d456c921d6a061a84c82ffee,https://www.semanticscholar.org/paper/2a21fc370a7e0854d456c921d6a061a84c82ffee,Letter from the Special Issue Editor,"As Artificial Intelligence (AI) and Machine Learning (ML) are increasingly being used to make consequential decisions that may critically impact individuals and many aspects of our society such as criminal justice, health care, education, and employment, there is a growing recognition that AI systems may perpetuate or, worse, exacerbate the unfairness of existing social systems. To tackle this problem, there has been a sharp recent focus on fair AI/ML research in the Machine Learning community. These efforts tend to focus on how to engineer fair AI models by (a) defining appropriate metrics of fairness and (b) designing new algorithms to mitigate bias and ensure fairness. It however frequently overlooks the messy, complex and ever changing contexts in which these systems are deployed. As AI fairness is a complex socio-technical issue which cannot be addressed by a purely technical solution, designing fair AI systems requires close collaboration across multiple disciplines to deeply integrate the social, historical, legal, and technical context and concerns in the design process. In this special issue on Interdisciplinary Perspectives on Fairness and Artificial Intelligence Systems, leading researchers from engineering, social science and humanities present their work on ethical considerations in developing fair AI systems specifically, and responsible socio-technical systems in general. We sought highquality contributions that integrate ideas from more than one field across the disciplines of technology, social science and the humanities. These papers will provide readers deep insights into the nature and complexity of AI biases, their manifestations in developing practical socio-technical systems and typical mitigation strategies. They also identify opportunities for the AI community to engage with the experts from the humanities. Safiya U. Noble and Sarah T. Roberts from UCLA seek to expand the conversations about socio-technical systems beyond individual, moral and ethical concerns. They believe that the field of “ethical AI” must contend with how it affects and is affected by power structures that encode systems of sexism, racism, and class. They advocate the need for independent research institutes, such as the UCLA Center for Critical Internet Inquiry (C2i2) to promote investigations into the politics, economics, and impacts of technological systems. Jennifer Keating from the University of Pittsburgh discusses the importance of incorporating ethical standards and responsible design features into the development of new technologies. Covid contact tracing was used as a case study to illustrate how rapidly developed tools can have unintended consequences if they are not carefully designed/monitored. She advocates that technologists should collaborate with experts from the humanities to integrate deeper cultural concerns and social/political context into technology development. Lisa Singh and her co-authors from Georgetown University overview the challenges associated with using social media data and the ethical considerations they create. They frame the ethical dilemmas within the context of data privacy and algorithmic fairness and show how and when different ethical concerns arise. Sebastian Schelter from University of Amsterdam and Julia Stoyanovich from NYU present a discussion on technical bias that arises in the data processing pipeline. A number of potential sources of bias during the preprocessing, model development and deployment phases of the ML development lifecycle are identified, with illustrative examples. They show how software support can help avoid these technical bias issues. The broader point of the work is to shed light on the challenge of bias introduced due to data engineering decisions, and to promote an emerging research direction on developing solutions to mitigate it. James Foulds and Shimei Pan from UMBC aim to shed light on whether parity-based metrics are valid measures of AI fairness. They consider the arguments both for and against parity-based fairness definitions and provide a set of guidelines on their use in different contexts. Finally, Jared Sylvester and Edward Raff from Booz Allen Hamilton argue that good-faith efforts toward implementing fairness in practical ML applications should be encouraged, even when the fairness interventions may not result from completely resolving thorny philosophical debates, as this represents progress over the (likely unfair) status quo. This viewpoint is discussed in several contexts: choosing the right fairness metric, solving trolley problems for self-driving cars, and selecting hyper-parameters for fair learning algorithms. James Foulds and Shimei Pan University of Maryland, Baltimore County",IEEE Data Eng. Bull.,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a2b1dbc0658c5e6fc26893f6a3fce8b2db872c1c,https://www.semanticscholar.org/paper/a2b1dbc0658c5e6fc26893f6a3fce8b2db872c1c,Building and deploying a cyberinfrastructure for the data-driven design of chemical systems and the exploration of chemical space,"Abstract The use of modern data science has recently emerged as a promising new path to tackling the complex challenges involved in the creation of next-generation chemistry and materials. However, despite the appeal of this potentially transformative development, the chemistry community has yet to incorporate it as a central tool in every-day work. Our research program is designed to enable and advance this emerging research approach. It is centred around the creation of a software ecosystem that brings together physics-based modelling, high-throughput in silico screening and data analytics (i.e. the use of machine learning and informatics for the validation, mining and modelling of chemical data). This cyberinfrastructure is devised to offer a comprehensive set of data science techniques and tools as well as a general-purpose scope to make it as versatile and widely applicable as possible. It also emphasises user-friendliness to make it accessible to the community at large. It thus provides the means for the large-scale exploration of chemical space and for a better understanding of the hidden mechanisms that determine the properties of complex chemical systems. Such insights can dramatically accelerate, streamline and ultimately transform the way chemical research is conducted. Aside from serving as a production-level tool, our cyberinfrastructure is also designed to facilitate and assess methodological innovation. Both the software and method development work are driven by concrete molecular design problems, which also allow us to assess the efficacy of the overall cyberinfrastructure.",,2018.0,10.1080/08927022.2018.1471692,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
bb9e275781c436c0b5a4bd58d77dd447b2372787,https://www.semanticscholar.org/paper/bb9e275781c436c0b5a4bd58d77dd447b2372787,A Survey on different techniques in Artificial Intelligence that can be enforced in cybersecurity,"AI is a branch of Computer Science concerned with the study and creation of computer systems.AI is a study of how to make computers do things which at a moment, people do better. Today businesses using modern technologies like cloud, big data, mobile, and social media. Although these technologies unlock a whole new set of capabilities and rewards for businesses, they also expose them to to hitherto unknown risks. when hackers would deploy adware, malware, Trojan viruses, phishing attacks or standard keyloggers on private systems for small gains, the focus of hackers and cybercriminals has shifted from individual users to big businesses and corporations since they make for more lucrative targets. But financial rewards are not the only motive behind cyber-attacks. Gaining access to sensitive data and using it for illegal purposes, cause enterprises far more damage, not only in terms of financial losses but also hurting the reputation they have painstakingly built over several years. now, before cyberattack occurs it will be prevented by using modern technology. This paper proposes techniques to prevent cyber attacks by the development of cybersecurity skills and how artificial intelligence can be implied to improve skills through the use of artificial neural networks and machine learning algorithms. Keywords— Artificial Intelligence, Cybercrime, Cyber-attacks, Cyber Security, Artificial Neural Networks and Machine Learning Algorithms.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0930bba136c44e29207e27ca5179b60f7004fe98,https://www.semanticscholar.org/paper/0930bba136c44e29207e27ca5179b60f7004fe98,Integration and Exploitation process of domain ontology into Machine Teaching PERO2,"The Interest to ""Machines Teaching"" these recent years receives a great importance in order to extend their deployment in educational institutions, by facilitating their appropriation by users. This research work is part of the PERO2 project (intelligent system for learning of reasoning and problem-solving in physics domain specifically the subdomain ""electricity"") conducted at Toulouse1 University [1] and Chouaib Doukkali University (LAROSERI, ""PERO2). The general aim is to improve the system by offering a new version PERO2 by means of integrating an ontological knowledge base of concepts instead of a relational database and extend this new version to other disciplines of physical sciences. A knowledge base includes knowledge specific to a domain (Concepts, theorems, lemmas, laws, ...), generally modeled and stored in a declarative manner in the form of ontology which can be exploited by PERO2, allowing its consultation and its use.",BDAW '16,2016.0,10.1145/3010089.3010120,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d3bb0a30584ceca6bc8f7296d55f86ad0163c2f6,https://www.semanticscholar.org/paper/d3bb0a30584ceca6bc8f7296d55f86ad0163c2f6,"DQA: Scalable, Automated and Interactive Data Quality Advisor","Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today’s world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.",2019 IEEE International Conference on Big Data (Big Data),2019.0,10.1109/BigData47090.2019.9006187,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
946d3f843ea93f22cc9c7e30af42a682139ad1e6,https://www.semanticscholar.org/paper/946d3f843ea93f22cc9c7e30af42a682139ad1e6,Performance of Neural Network Image Classification on Mobile CPU and GPU,"Aalto University, P.O. Box 11000, FI-00076 Aalto www.aalto.fi Author Sipi Seppälä Title Performance of Neural Network Image Classification on Mobile CPU and GPU School School of Science Master’s programme Computer, Communication and Information Sciences Major Computer Science Code SCI3042 Supervisor Professor Antti Ylä-Jääski Advisors M.Sc. Teemu Kämäräinen, Dr. Matti Siekkinen Level Master’s thesis Date 2018-04-20 Pages 86 Language English Abstract Artificial neural networks are a powerful machine learning method, with impressive results lately in the field of computer vision. In tasks like image classification, which is a well-known problem in computer vision, deep learning convolutional neural networks can even achieve human-level prediction accuracy. Although high-accuracy deep neural networks can be resource-intensive both to train and to deploy for inference, with the advent of lighter mobile-friendly neural network model architectures it is finally possible to achieve real-time on-device inference without the need for cloud offloading. The inference performance can be further improved by utilizing mobile graphics processing units which are already capable of general-purpose parallel computing. This thesis measures and evaluates the performance aspects – execution latency, throughput, memory footprint, and energy usage – of neural network image classification inference on modern smartphone processors, namely CPU and GPU. The results indicate that, if supported by the neural network software framework used, hardware acceleration with GPU provides superior performance in both inference throughput and energy efficiency – whereas CPU-only performance is both slower and more power-hungry. Especially when the inference computation is sustained for a longer time, running CPU cores at full speed quickly reaches the overheat-prevention temperature limits, forcing the system to slow down processing even further. The measurements show that this thermal throttling does not occur when the neural network is accelerated with a GPU. However, currently available deep learning frameworks, such as TensorFlow, not only have limited support for GPU acceleration, but have difficulties dealing with different types of neural network models because the field is still lacking standard representations for them. Nevertheless, both of these are expected to improve in the future when more comprehensive APIs are developed.Artificial neural networks are a powerful machine learning method, with impressive results lately in the field of computer vision. In tasks like image classification, which is a well-known problem in computer vision, deep learning convolutional neural networks can even achieve human-level prediction accuracy. Although high-accuracy deep neural networks can be resource-intensive both to train and to deploy for inference, with the advent of lighter mobile-friendly neural network model architectures it is finally possible to achieve real-time on-device inference without the need for cloud offloading. The inference performance can be further improved by utilizing mobile graphics processing units which are already capable of general-purpose parallel computing. This thesis measures and evaluates the performance aspects – execution latency, throughput, memory footprint, and energy usage – of neural network image classification inference on modern smartphone processors, namely CPU and GPU. The results indicate that, if supported by the neural network software framework used, hardware acceleration with GPU provides superior performance in both inference throughput and energy efficiency – whereas CPU-only performance is both slower and more power-hungry. Especially when the inference computation is sustained for a longer time, running CPU cores at full speed quickly reaches the overheat-prevention temperature limits, forcing the system to slow down processing even further. The measurements show that this thermal throttling does not occur when the neural network is accelerated with a GPU. However, currently available deep learning frameworks, such as TensorFlow, not only have limited support for GPU acceleration, but have difficulties dealing with different types of neural network models because the field is still lacking standard representations for them. Nevertheless, both of these are expected to improve in the future when more comprehensive APIs are developed.",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3708b183b9121199a352a50a3eb845f7ee786df5,https://www.semanticscholar.org/paper/3708b183b9121199a352a50a3eb845f7ee786df5,Graphical Potential Games,"Potential games, originally introduced in the early 1990’s by Lloyd Shapley, the 2012 Nobel Laureate in Economics, and his colleague Dov Monderer, are a very important class of models in game theory. They have special properties such as the existence of Nash equilibria in pure strategies. This note introduces graphical versions of potential games. Special cases of graphical potential games have already found applicability in many areas of science and engineering beyond economics, including artificial intelligence, computer vision, and machine learning. They have been effectively applied to the study and solution of important realworld problems such as routing and congestion in networks, distributed resource allocation (e.g., public goods), and relaxation-labeling for image segmentation. Implicit use of graphical potential games goes back at least 40 years. Several classes of games considered standard in the literature, including coordination games, local interaction games, lattice games, congestion games, and party-affiliation games, are instances of graphical potential games. This note provides several characterizations of graphical potential games by leveraging well-known results from the literature on probabilistic graphical models. A major contribution of the work presented here that particularly distinguishes it from previous work is establishing that the convergence of certain type of game-playing rules implies that the agents/players must be embedded in some graphical potential game.",ArXiv,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3bb2e87d338e3745f86fd62b3b29c2312155b20e,https://www.semanticscholar.org/paper/3bb2e87d338e3745f86fd62b3b29c2312155b20e,Evaluation of Classifier Complexity for Delay Tolerant Network Routing,"The growing popularity of small cost-effective satellites (SmallSats, CubeSats, etc.) creates the potential for a variety of new science applications involving multiple nodes functioning together to achieve a task, such as swarms and constellations. As this technology develops and is deployed for missions in Low Earth Orbit and beyond, the use of delay tolerant networking (DTN) techniques may improve communication capabilities within the network. In this paper, a network hierarchy is developed from heterogeneous networks of SmallSats, surface vehicles, relay satellites and ground stations which form an integrated network. There is a trade-off between complexity, flexibility, and scalability of user defined schedules versus autonomous routing as the number of nodes in the network increases. To address these issues, this work proposes a machine learning classifier based on DTN routing metrics. A framework is developed which will allow for the use of several categories of machine learning algorithms (decision tree, random forest, and deep learning) to be applied to a dataset of historical network statistics, which allows for the evaluation of algorithm complexity versus performance to be explored. We develop the emulation of a hierarchical network, consisting of tens of nodes which form a cognitive network architecture. CORE (Common Open Research Emulator) is used to emulate the network using bundle protocol and DTN IP neighbor discovery.",2019 IEEE Cognitive Communications for Aerospace Applications Workshop (CCAAW),2019.0,10.1109/CCAAW.2019.8904898,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b4376c5f5b536c29090b81a72c50d6aa312ddae7,https://www.semanticscholar.org/paper/b4376c5f5b536c29090b81a72c50d6aa312ddae7,Miniaturized Pervasive Sensors for Indoor Health Monitoring in Smart Cities,"Sensors and electronics technologies are pivotal in several fields of science and engineering, especially in automation, industry and environment monitoring. Over the years, there have been continuous changes and advancements in design and miniaturization of sensors with the growth of their application areas. Challenges have arisen in the deployment, fabrication and calibration of modern sensors. Therefore, although the usage of sensors has greatly helped improving the quality of life, especially through their employment in many IoT (Internet of Things) applications, some threats and safety issues still remain unaddressed. In this paper, a brief review focusing on pervasive sensors used for health and indoor environment monitoring is given. Examples of technology advancements in air, water and radioactivity are discussed. This bird’s eye view suggests that solid-state pervasive sensors have become essential parts of all emerging applications related to monitoring of health and safety. Miniaturization, in combination with gamification approaches and machine learning techniques for processing large amounts of captured data, can successfully address and solve many issues of massive deployment. The development paradigm of Smart Cities should include both indoor and outdoor scenarios.",Smart Cities,2021.0,10.3390/SMARTCITIES4010008,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d5f80c1bda049cec79e5996e6252056da7c4c972,https://www.semanticscholar.org/paper/d5f80c1bda049cec79e5996e6252056da7c4c972,MPP: Model Performance Predictor,"Operations is a key challenge in the domain of machine learning pipeline deployments involving monitoring and management of real-time prediction quality. Typically, metrics like accuracy, RMSE etc., are used to track the performance of models in deployment. However, these metrics cannot be calculated in production due to the absence of labels. We propose using an ML algorithm, Model Performance Predictor (MPP), to track the performance of the models in deployment. We argue that an ensemble of such metrics can be used to create a score representing the prediction quality in production. This in turn facilitates formulation and customization of ML alerts, that can be escalated by an operations team to the data science team. Such a score automates monitoring and enables ML deployments at scale.",OpML,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
56c051e6c95479dd3832b0bdac2f5ac6dc706e08,https://www.semanticscholar.org/paper/56c051e6c95479dd3832b0bdac2f5ac6dc706e08,Goal-Driven Learning: Fundamental Issues: A Symposium Report,"implicit in the user’s choice of training examples); and they address the question of how to learn by applying a single, fixed learning method. Although such systems provide a useful test bed for examining individual learning mechanisms, they are inadequate for use as real-world learners. The problem is that realworld situations offer countless opportunities for learning, and each of these opportunities licenses the learning of infinitely many concepts, few of which are actually useful. Consequently, an indiscriminat e learning system will expend enor■ In AI, psychology, and education, a growing body of research supports the view that learning is a goal-directed process. Psychological experiments show that people with varying goals process information differently, studies in education show that goals have a strong effect on what students learn, and functional arguments in machine learning support the necessity of goalbased focusing of learner effort. At the Fourteenth Annual Conference of the Cognitive Science Society, a symposium brought together researchers in AI, psychology, and education to discuss goaldriven learning. This article presents the fundamental points illuminated at the symposium, placing them in the context of open questions and current research directions in goal-driven learning. Learning is a central area of study for researchers interested in human cog",AI Mag.,1993.0,10.1609/aimag.v14i4.1069,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c7838cb476d870011654d6678999c1a96eb3d883,https://www.semanticscholar.org/paper/c7838cb476d870011654d6678999c1a96eb3d883,High-performance Hardware Architecture for Tensor Singular Value Decomposition: Invited Paper,"Tensor provides a brief and natural representation for large-scale multidimensional data by way of appropriate low-rank approximations, thus we can discover significant latent structures of complex data and generalize data representation. To date, tensor has gained tremendous success in various science and technology fields, especially in machine learning and big data applications. However, tensor computation, especially tensor decomposition, is usually expensive due to the inherent large-size characteristic of tensors, and hence would potentially hinder their future wide deployment. In this paper, we develop a hardware architecture to accelerate tensor singular value decomposition (t-SVD), which is a new tensor decomposition technique that has been successfully applied to high-dimensional data classification and video recovery. Specifically, design consideration of each key computing unit is analyzed and discussed. Then, the proposed t-SVD hardware architecture is implemented and synthesized using CMOS 28nm technology. Comparison with real-world CPU-based implementations shows that the proposed hardware accelerator is expected to provide average 14× speedup on various t-SVD workloads.",2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),2019.0,10.1109/iccad45719.2019.8942082,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e7858a0fb806c5a96418ae8984fd4e1abf0f3225,https://www.semanticscholar.org/paper/e7858a0fb806c5a96418ae8984fd4e1abf0f3225,Help Me to Help You,"The increasing size of datasets with which researchers in a variety of domains are confronted has led to a range of creative responses, including the deployment of modern machine learning techniques and the advent of large scale “citizen science projects.” However, the ability of the latter to provide suitably large training sets for the former is stretched as the size of the problem (and competition for attention amongst projects) grows. We explore the application of unsupervised learning to leverage structure that exists in an initially unlabelled dataset. We simulate grouping similar points before presenting those groups to volunteers to label. Citizen science labelling of grouped data is more efficient, and the gathered labels can be used to improve efficiency further for labelling future data. To demonstrate these ideas, we perform experiments using data from the Pan-STARRS Survey for Transients (PSST) with volunteer labels gathered by the Zooniverse project, Supernova Hunters and a simulated project using the MNIST handwritten digit dataset. Our results show that, in the best case, we might expect to reduce the required volunteer effort by 87.0% and 92.8% for the two datasets, respectively. These results illustrate a symbiotic relationship between machine learning and citizen scientists where each empowers the other with important implications for the design of citizen science projects in the future.",ACM Transactions on Social Computing,2019.0,10.1145/3362741,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7626d1712fff3456e415e0c5023b1fd113bb7b39,https://www.semanticscholar.org/paper/7626d1712fff3456e415e0c5023b1fd113bb7b39,Implementing Entrepreneurial-minded Learning (EML) in a Manufacturing Processes Course,"At Ohio Northern University, ME-3421 Manufacturing Processes is a technical elective course for juniors in the Mechanical Engineering discipline. Project-based learning techniques (PBL) have been known to underscore skill-based learning outcomes. For this course, PBL was enriched by including Entrepreneurial Minded Learning (EML) activities. EML activities are designed to inspire students’ curiosity about the world around them, teaches them to integrate information from many sources to gain insight, and encourages them to creating value by identifying opportunities and working in partnerships with fellow students. It also teaches them to explore a contrarian view of accepted solutions. In the Manufacturing Processes class, two EML modules were developed and deployed in stages. These were (i) a manufacturing process selection activity and (ii) an activity related to environmental and economic impact of manufacturing processes. Both activities included a stakeholder or a customer as well as unexpected design alternatives. In addition, unlike conventional PBL, the project information was kept ambiguous by design, and the customer’s requirements were not all clearly spelled out at the project start, and even changed as the project progressed. As an example, during the implementation of manufacturing process selection activity, students interacted with the customer and selected the best manufacturing process for a product based on quantity produced and properties (strength, finish, tolerances, etc.) needed. Students then presented their work to the customer. A few assessments were implemented including written reports, presentations, peer evaluation on teamwork, and a survey. By implementing entrepreneurial minded learning experiences in coursework, students will not only learn the technical theory, but they will be trained to identify problems and solve them in innovative ways. Introduction For this work, project-based learning activities (PBL) were modified to include entrepreneurial components. Two modules were developed and implemented in Manufacturing Processes class, an elective at Ohio Northern University. There were 7 students in the class. This course introduces different manufacturing techniques such as casting, rolling, forging, extrusion, drawing, sheet metal forming, machining, and welding. Theoretical as well as practical considerations are covered, including quality control and statistical methods. Subject-based learning (SBL), and Active Collaborative Learning (e.g. PBLs) have been widely used in the engineering curriculum [1]. Project/problem based activities have been implemented to help students learn new concepts faster. In this methodology, a question or problem is used to drive the students’ learning activities to produce a product that can be used in real world. PBL is used to prepare students with skills such as leadership, team building, ethical behavior, creativity, critical thinking, and problem solving [2]. PBL has been implemented as part of the curriculum or as a replacement of the traditional classroom. EMLs are either designed independently or used by modifying existing pedagogy techniques such as SBL or ACL. EMLs can be implemented either as single homework assignment or as a single week or multiple week-long activities [3]. With experience, an instructor can teach an entire course using such active learning techniques. It is important to understand that entrepreneurship, in this context, is not necessarily about teaching students how to start a new business, but rather to develop the mindset of innovation necessary to recognize opportunities and make the most of them [1,3]. An enterprise does not need to be created at the end of an EML activity. Entrepreneurial Minded Learning is designed to ● Stimulate curiosity. Students are encouraged to demonstrate constant curiosity about our changing world, and explore a contrarian view of accepted solutions. ● Make connections. Students integrate information from many sources to gain insight, and assess and manage risk. ● Creating value. They do this by identifying unexpected opportunities to create extraordinary value, and persisting through and learning from failure. [4-8]. The tenets above are called the three Cs of the entrepreneurial framework. The EML mindset is being promoted by Kern Entrepreneurship Education Network (KEEN) and implemented at 33 partner institutions [9]. Its goal is to encourage entrepreneurial mindset-based pedagogy within the undergraduate engineering curriculum. Fresh engineers with entrepreneurial mindset skills are prepared for today’s job market. In addition to above-mentioned skills, complementary skills such as (i) identifying an opportunity, (ii) developing partnerships and building teams, (iii) finding an engineering solution in terms of societal benefits etc. will also be developed. These skills will reinforce the development of an entrepreneurial mindset. In this work, first two modules will be presented and then the student assessment will be given. Entrepreneurially Minded (EM) Course Enhancements: Two EML modules were created by first looking at the course learning objectives and then modifying project-based learning activities by incorporating entrepreneurial mindset skills. Module 1: Souvenir Supply Contract Bid First activity was a classic materials/ manufacturing process selection activity for the students. It was introduced in the 7 week of the semester. This 5-week module required students to prepare a bid for a souvenir supply contract with the university. Just like with most project-based learning modules (PBLs), following was the hook statement – “Your university is planning to celebrate its 150 years of existence in few years. They are planning to sell souvenirs to market the brand and cover the cost of celebrations. Your startup company is bidding for the supply contract.” Student groups were required to select materials and manufacturing process for those articles. In addition, they were to compare the selected manufacturing process with other manufacturing process. Student teams were required to meet with the customer to find out type and number of souvenirs required. The customer in this case was instructor. Deliverables included written reports and a class presentation. The written report would be in the form of a bid document. Students were asked to prepare bid documents as if they were to be used by the university marketing and media department in evaluating potential future giveaway items. The module was designed with following course learning objectives in mind. • Identify and summarize the types of the materials used in the manufacturing of a product. • Apply mechanics of materials and engineering materials science to qualify or quantify the properties of different manufactured products. • Select the best manufacturing process for a product based on quantity produced and properties (strength, finish, tolerances, etc.) needed. • Understand the economic, environmental, and societal impact of manufacturing. For this activity, students formed themselves into teams of 2-3 students. Part of the lecture time was devoted in the beginning to explain students about the module, rubrics, assessments and deliverables. A feedback on student progress was given in week 3 during in class lecture time. Module activities were performed out of class majority of the time. The project stages were as follows: (i) Week 1-2Types of Souvenirs-Manufacturing process decided and e-mailed. (ii) Week 3-4the economic and environmental, impact of manufacturing –paper submission. (iii) Week 5Report and Presentation. Guidelines about how to write a bid were also given [9]. These are given in Appendix 1. To jump start student discussion, ice-breaking questions/suggestions were given to teams. These were: ● What type of and how many souvenirs will be required? ● Four different materials have to be used in the manufacturing. ● What is the expected cost of each souvenir? ● Is it going to be locally produced or sourced from outside (different state/country etc.)? ● What is the current state of the art for bulk manufacturing of each type of souvenir? ● The souvenirs should be lightweight and with no sharp corners. ● The materials should be non-toxic, water proof, self-supporting, and souvenirs should accurately represent beliefs of university. ● The manufacturing process selected for the souvenirs should waste minimal material and require least number of process steps. ● Two souvenirs should be new designs. ● The selected souvenirs should be able to bulk manufactured. ● One piece can be manufactured using 3-D printing. ● Your “how its manufactured” explanation will not be graded on accuracy, but rather on your thought process and thoroughness. Module 2: Sustainable Cafeteria Design This module was staged in two class periods spread over two weeks. It included a summary report with references and a gallery walk. Students were divided into groups of two. In the first class of the first week of the module, students were introduced to the “triple bottom line” concept and were told to summarize three articles related to triple bottom line in a 2-3-page report. The triple bottom line refers to a one method of articulating a sustainable business model: to focus not only on “profit,” but equally on “people” and the “planet.” Necessary resources such as regional Environmental Protection Agency (EPA) website address were provided. More details are given in Appendix 2. At the end of second week, once they submitted the summary report, following situation was presented to them, “An eccentric oil magnate is donating $2 million to your university to build a state-of-the-art cafeteria. The only caveat is that it has to be built and operated sustainably. The university is looking for ideas for the cafeteria design. Your company is going to participate in this process. Please select 3 products and/or manufacturing processes that are susta",,2018.0,10.18260/1-2--30621,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
75b3aab1efb1acc7f6f29084cbab04183237fe08,https://www.semanticscholar.org/paper/75b3aab1efb1acc7f6f29084cbab04183237fe08,Artificial Neural Network Fuzzy Inference System Anfis Epub File,"In this book a neural network learning method with type-2 fuzzy weight adjustment is proposed. The mathematical analysis of the proposed learning method architecture and the adaptation of type-2 fuzzy weights are presented. The proposed method is based on research of recent methods that handle weight adaptation and especially fuzzy weights.The internal operation of the neuron is changed to work with two internal calculations for the activation function to obtain two results as outputs of the proposed method. Simulation results and a comparative study among monolithic neural networks, neural network with type-1 fuzzy weights and neural network with type-2 fuzzy weights are presented to illustrate the advantages of the proposed method.The proposed approach is based on recent methods that handle adaptation of weights using fuzzy logic of type-1 and type-2. The proposed approach is applied to a cases of prediction for the Mackey-Glass (for ô=17) and Dow-Jones time series, and recognition of person with iris biometric measure. In some experiments, noise was applied in different levels to the test data of the Mackey-Glass time series for showing that the type-2 fuzzy backpropagation approach obtains better behavior and tolerance to noise than the other methods.The optimization algorithms that were used are the genetic algorithm and the particle swarm optimization algorithm and the purpose of applying these methods was to find the optimal type-2 fuzzy inference systems for the neural network with type-2 fuzzy weights that permit to obtain the lowest prediction error. Master's Thesis from the year 2017 in the subject Engineering Artificial Intelligence, grade: 9.00, Lovely Professional University, Punjab (Lovely professional university, Punjab), course: M.Tech, language: English, abstract: Residual life prediction is the technique which demonstrates how reliable a particular electronic system or component works under in specific operating conditions. The remaining useful life relies on the failure rate of a component and on the operating conditions of a device. This failure rate drifts for the duration of the life of the item with time. Life is an important aspect while choosing the electronic hardware. Residual life estimation and life prediction are two distinct terms. The importance of life estimation is to evaluate the remaining useful life of a specific component under the different stress parameters. As an increasing number of components are integrated on to a chip, the chances of failure increase, as the different parts have their own stress factors and different working conditions. So the condition monitoring strategies are utilized which enhances the reliability of a component and a suitable move to be made before any harmful breakdown happens. The electronic circuits need a failure estimation technique to protect the system from unavoidable failures. Residual life estimation of electronic components is an important fact these days as electronic components and devices becomes a great need of society. Residual life prediction is predicting the remaining useful life of a component or device based on various failure factors of any component and it also depends on the operating conditions. Many methods for predicting the life of electronic components have been developed. The life of electronic components can be predicted by creating an intelligent system for the failure analysis. The capability to predict the life of electronic components is a key to prevent the sudden costly failure and it will increase the overall performance and reliability of a system. So, remaining useful life prediction is an important factor for every active and passive electronic component such as resistor, capacitor and diode etc. Fuzzy Systems EngineeringTheory and PracticeSpringer Science & Business Media ""This book offers an outlook of the most recent works at the field of the Artificial Neural Networks (ANN), including theoretical developments and applications of systems using intelligent characteristics for adaptability""--Provided by publisher. This book describes hybrid intelligent systems using type-2 fuzzy logic and modular neural networks for pattern recognition applications. Hybrid intelligent systems combine several intelligent computing paradigms, including fuzzy logic, neural networks, and bio-inspired optimization algorithms, which can be used to produce powerful pattern recognition systems. Type-2 fuzzy logic is an extension of traditional type-1 fuzzy logic that enables managing higher levels of uncertainty in complex real world problems, which are of particular importance in the area of pattern recognition. The book is organized in three main parts, each containing a group of chapters built around a similar subject. The first part consists of chapters with the main theme of theory and design algorithms, which are basically chapters that propose new models and concepts, which are the basis for achieving intelligent pattern recognition. The second part contains chapters with the main theme of using type-2 fuzzy models and modular neural networks with the aim of designing intelligent systems for complex pattern recognition problems, including iris, ear, face and voice recognition. The third part contains chapters with the theme of evolutionary optimization of type-2 fuzzy systems and modular neural networks in the area of intelligent pattern recognition, which includes the application of genetic algorithms for obtaining optimal type-2 fuzzy integration systems and ideal neural network architectures for solving problems in this area. Soft computing is a consortium of computing methodologies that provide a foundation for the conception, design, and deployment of intelligent systems and aims to formalize the human ability to make rational decisions in an environment of uncertainty and imprecision. This book is based on a NATO Advanced Study Institute held in 1996 on soft computing and its applications. The distinguished contributors consider the principal constituents of soft computing, namely fuzzy logic, neurocomputing, genetic computing, and probabilistic reasoning, the relations between them, and their fusion in industrial applications. Two areas emphasized in the book are how to achieve a synergistic combination of the main constituents of soft computing and how the combination can be used to achieve a high Machine Intelligence Quotient. Over the past decades, fault diagnosis (FDI) and fault tolerant control strategies (FTC) have been proposed based on different techniques for linear and nonlinear systems. Indeed a considerable attention is deployed in order to cope with diverse damages resulting in faults occurrence. CD-ROM contains: BackProp -Data files -Display -Images -MATLAB examples. This book constitutes the refereed proceedings of the 8th International Conference on Neural Networks and Artificial Intelligence, ICNNAI 2014, held in Brest, Belarus, in June 2014. The 19 revised full papers presented were carefully reviewed and selected from 27 submissions. The papers are organized in topical sections on forest resource management; artificial intelligence by neural networks; optimization; classification; fuzzy approach; machine intelligence; analytical approach; mobile robot; real world application. Estimation of missing precipitation records is one of the most important tasks in hydro-logical and environmental study. The efficiency of hydrological and environmental models is sub-ject to the completeness of precipitation data. This study compared some basic soft computing techniques, namely, artificial neural network, fuzzy inference system and adaptive neuro-fuzzy inference system as well as the conventional methods to estimate missing monthly rainfall records in the northeast region of Thailand. Four cases studies are selected to evaluate the accuracy of the estimation models. The simultaneous rainfall data from three nearest neighbouring control stations are used to estimate missing records at the target station. The experimental results suggested that the adaptive neuro-fuzzy inference system could be considered as a recommended technique because it provided the promising estimation results, the estimation mechanism is transparent to the users, and do not need prior knowledge to create the model. The results also showed that fuzzy inference system could provide compatible accuracy to artificial neural",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8e2db36fc1de5c190338627c50d4d82cda5e664f,https://www.semanticscholar.org/paper/8e2db36fc1de5c190338627c50d4d82cda5e664f,Computational Models & Methods in Systems,"With the current emphasis on ‘big data’ marking a new stage in the advance of biomedical sciences, improvements in computational capability, together with the impact of high throughput techniques and genome-wide methods, mean that biological and medical fields are now data-rich to a degree that was unknown a few decades ago. Increased data availability has not only highlighted the complementarity needed between biology and computer science, but has served to emphasise interdisciplinary overlap with mathematical and physical sciences in the formulation of computational models, posing of hypotheses and statistical interpretation of results. Information derived from diverse sources means that linking system behaviour to changes at cellular and molecular scales has become a viable goal, facilitated by techniques such as network theory, stochastic processes and integrative data analysis. The studies of systems of biological components, their dynamic behaviour and reliance on wide-ranging data, together with translation to disease progression and treatment options, define systems biology and medicine. The IEEE International Conference on Bioinformatics and Biomedicine (IEEE BIBM) is a well-established research conference, providing a leading forum for disseminating the latest research in bioinformatics and health informatics. It attracts contributions from both academic and industrial scientists, which range from biology and medicine, to chemistry, computer science, mathematics and statistics. In particular, and in addition to its over-arching remit and general proceedings, IEEE BIBM provides an important platform for showcasing computational and mathematical modelling methods, together with the data integration, analysis and visualisation, which underpin these. Selected and extended papers from several of their important workshops thus provide the focus of this Special Issue. The papers deal with basic model formulation, data analysis and the incorporation of these analyses in the decision-making process for clinical treatment. The algorithms and methods include ant colony optimisation (Sapin et al.) of the interactions between a number of single-nucleotide polymorphisms (SNPs). Discriminatory performance of the algorithm is found to agree well with SNP identification from large-scale genome-wide association studies for Type II diabetes. Medical image analysis is the objective for applying both evolutionary and swarm intelligence algorithms. In the former case, data classification and monitoring in Parkinson’s disease in humans is considered, together with characterisation of genetic mutations in the fruit fly vector (Smith et al.). Deployment of swarm intelligence algorithms, such as stochastic diffusion search for CT scans and X-Rays, and learning vector quantisation for identification of abnormal tumour regions in MR segmentation (al-Rifaie et al.), also demonstrate the applicability of these methods. Wong-Lin & Cullen explore the importance of dopamine as a neurotransmitter for multiple brain functions using a computational model that spans multiple levels of function and different dynamics, and lay the foundation for an integrated approach to realistic in silico simulation of dopaminergic systems in neuropharmacology. Investigation, similarly, of the relative dynamics and structure of human intestinal crypts in malignant systems, provides the motivation for the formulation of an epigenetic model using the agent-based paradigm (Roznovat & Ruskin). Epigenetics, the additional layer of inherited genome regulation, together with epigenome-wide association studies, linking intra-individual epigenetic variation, are linked to the evolution of human diseases, such as cancer, and to autoimmune and neuropsychiatric disorders. The derived computational model enables comparative analysis on aberrant DNA methylation levels in cancer development and the investigation of the effect of potential methylation inhibitors during disease initiation. Inhibitors, both time-dependent and time-independent, merit important distinction in the characterisation of compound potency and drug-response. Reversibility properties for both are investigated by means of a simple kinetic model (Yue & You) and analysis of the outcomes and their contrast with supporting numerical studies. The complexity of the drug-receptor process in this case indicates the contribution that can be made by computational modelling, as well as the need to support formulation and parameterisation with good quality data. A further example of the flexibility and scope offered by the modelling approach is provided by studies of microtubule ordering and the way in which this is affected by collision and crossover. A 3-state model is used to determine the influence of spontaneous catastrophe, crossover and ketaninmediated severing on plant microtubule ordering across different temperatures (Mace & Wang). It is evident, however, that, while many dynamic biological systems share similar properties and constraints, model specification, particularly in the context of sparse or poor experimental data, is often anything but straightforward. Achieving an unambiguous estimation of the full set of parameters is frequently challenging and may be impossible. The identifiability in this context, of typical S-System models for dynamic biological systems, is discussed with respect to an application on yeast fermentation pathway determination (Li et al.). The authors note also that, even where data are available, these may be noisy or incomplete, which also affects the identification process. When analysing such data, a range of statistical and computational methods are available. These can be categorised by their level of automation, the sophistication of their algorithms, their data type and size and so on. Bioinformatics, often described as the intersection of mathematics, biology and computer science, typically involves processing large amounts of data, so methods are usually machine-based. The paper (by Akutekwe et al.) thus describes a two-stage optimisation process in the modelling of protein-protein networks. This is applied to complex diseases, such as colorectal cancer, and discusses the performance of machine learning methods and selected algorithms, such as particle swarm optimisation and differential evolution for parameter optimisation for classification and automatic discovery of biomarkers, with Bayesian network analysis used to predict their temporal linkage. Ultimately, systems modelling and simulation-optimisation, with appropriate data analysis, can play a major role in decision-support systems for biomedical applications. The Heartsearcher for patient risk classification is proposed by Park & Kang, while Bansal et al. describe a cardiac monitoring system which uses ECG signal analysis and pattern recognition provided through a mobile device, a remote server, and medical practitioner point-of-care communication. Evidently, the use of computational models and methods is widespread in Systems Biology and Medicine and, as datasets grow in size and complexity, biomedical systems increase in sophistication, and computing power escalates, this trend looks set to continue.",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
586d40cc9f8a88e469bf67d8ac29ce1706e212aa,https://www.semanticscholar.org/paper/586d40cc9f8a88e469bf67d8ac29ce1706e212aa,Automatic Speaker Recognition System Pdf Read,"This book constitutes the thoroughly refereed post-conference proceedings of the Second International Symposium on Intelligent Informatics (ISI 2013) held in Mysore, India during August 23-24, 2013. The 47 revised papers presented were carefully reviewed and selected from 126 initial submissions. The papers are organized in topical sections on pattern recognition, signal and image processing; data mining, clustering and intelligent information systems; multi agent systems; and computer networks and distributed systems. The book is directed to the researchers and scientists engaged in various fields of intelligent informatics. The conference will consist of various topics relating to power systems, robotics, mechatronics and pattern recognition Automatic Speech and Speaker RecognitionAdvanced TopicsSpringer Science & Business Media This book presents an overview of speaker recognition technologies with an emphasis on dealing with robustness issues. Firstly, the book gives an overview of speaker recognition, such as the basic system framework, categories under different criteria, performance evaluation and its development history. Secondly, with regard to robustness issues, the book presents three categories, including environment-related issues, speaker-related issues and application-oriented issues. For each category, the book describes the current hot topics, existing technologies, and potential research focuses in the future. The book is a useful reference book and self-learning guide for early researchers working in the field of robust speech recognition. Speech Recognition has a long history of being one of the difficult problems in Artificial Intelligence and Computer Science. As one goes from problem solving tasks such as puzzles and chess to perceptual tasks such as speech and vision, the problem characteristics change dramatically: knowledge poor to knowledge rich; low data rates to high data rates; slow response time (minutes to hours) to instantaneous response time. These characteristics taken together increase the computational complexity of the problem by several orders of magnitude. Further, speech provides a challenging task domain which embodies many of the requirements of intelligent behavior: operate in real time; exploit vast amounts of knowledge, tolerate errorful, unexpected unknown input; use symbols and abstractions; communicate in natural language and learn from the environment. Voice input to computers offers a number of advantages. It provides a natural, fast, hands free, eyes free, location free input medium. However, there are many as yet unsolved problems that prevent routine use of speech as an input device by non-experts. These include cost, real time response, speaker independence, robustness to variations such as noise, microphone, speech rate and loudness, and the ability to handle non-grammatical speech. Satisfactory solutions to each of these problems can be expected within the next decade. Recognition of unrestricted spontaneous continuous speech appears unsolvable at present. However, by the addition of simple constraints, such as clarification dialog to resolve ambiguity, we believe it will be possible to develop systems capable of accepting very large vocabulary continuous speechdictation. rd It is a pleasure and an honour both to organize ICB 2009, the 3 IAPR/IEEE Intertional Conference on Biometrics. This will be held 2–5 June in Alghero, Italy, hosted by the Computer Vision Laboratory, University of Sassari. The conference series is the premier forum for presenting research in biometrics and its allied technologies: the generation of new ideas, new approaches, new techniques and new evaluations. The ICB series originated in 2006 from joining two highly reputed conferences: Audio and Video Based Personal Authentication (AVBPA) and the International Conference on Biometric Authentication (ICBA). Previous conferences were held in Hong Kong and in Korea. This is the first time the ICB conference has been held in Europe, and by Programme Committee, arrangements and by the quality of the papers, ICB 2009 will continue to maintain the high standards set by its predecessors. In total we received around 250 papers for review. Of these, 36 were selected for oral presentation and 93 for poster presentation. These papers are accompanied by the invited speakers: Heinrich H. Bülthoff (Max Planck Institute for Biological Cybernetics, Tübgen, Germany) on “What Can Machine Vision Learn from Human Perception?”, daoki Furui (Department of Computer Science, Tokyo Institute of Technology) on “40 Years of Progress in Automatic Speaker Recognition Technology” and Jean-Christophe Fondeur (SAGEM Security and Morpho, USA) on “Large Scale Deployment of Biomrics and Border Control”. Automatic speech recognition and speaker recognition have a lot of applications in personal identification, access control and in the new manmachine-interface paradigm. The existing applications in voice-activated embedded systems solve the problem of recognition of the spoken words only or the problem of recognition of a speaker through the words uttered only. The goal of this project, therefore, is the development of a robust algorithm for both speech recognition and speaker verification. An example of a target application of this work is speech dialing of mobile phones with a speaker verification front-end in order to effect access control. In view of the memory and computational constraints of embedded systems, the dynamic time warping algorithm is used. This project only considers isolated spoken digits. The developed algorithm is coded in C language and can be ported to firmware for Arabic numeral digit recognition with a speaker verification front end for an embedded system like mobile phones. The system produced a FAR of 13.33% and a FRR of 24.3% for a total of 70 true claims and 30 false claims. It also had a word accuracy of 96.7%. The three volume set LNCS 7062, LNCS 7063, and LNCS 7064 constitutes the proceedings of the 18th International Conference on Neural Information Processing, ICONIP 2011, held in Shanghai, China, in November 2011. The 262 regular session papers presented were carefully reviewed and selected from numerous submissions. The papers of part I are organized in topical sections on perception, emotion and development, bioinformatics, biologically inspired vision and recognition, bio-medical data analysis, brain signal processing, brain-computer interfaces, brain-like systems, brain-realistic models for learning, memory and embodied cognition, Clifford algebraic neural networks, combining multiple learners, computational advances in bioinformatics, and computational-intelligent human computer interaction. The second volume is structured in topical sections on cybersecurity and data mining workshop, data mining and knowledge doscovery, evolutionary design and optimisation, graphical models, humanoriginated data analysis and implementation, information retrieval, integrating multiple nature-inspired approaches, Kernel methods and support vector machines, and learning and memory. The third volume contains all the contributions connected with multi-agent systems, natural language processing and intelligent Web information processing, neural encoding and decoding, neural network models, neuromorphic hardware and implementations, object recognition, visual perception modelling, and advances in computational intelligence methods based pattern recognition. The Defense Communications Division of ITT (ITTDCD) has developed an automatic speaker recognition (ASR) system that meets the functional requirements defined in NRL's Statement of Work. This report is organized as follows. Chapter 2 is a short history of the development of the ASR system, both the algorithm and the implementation. Chapter 3 describes the methodology of the system testing, while Chapter 4 summarizes the test results. In Chapter 5, we discuss some further testing that was performed using the GFM test material. Conclusions derived from the contract work are given Chapter 6. Speech recognition. (JES). This book is a collection research papers and articles from the 2nd International Conference on Communications and CyberPhysical Engineering (ICCCE – 2019), held in Pune, India in Feb 2019. Discussing the latest developments in voice and data communication engineering, cyber-physical systems, network science, communication software, imageand multimedia",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
31a12d8fbd1fcf955641192a0bd77f31485c5a93,https://www.semanticscholar.org/paper/31a12d8fbd1fcf955641192a0bd77f31485c5a93,OPTIMISATION OF SHAPE RECOGNITION AND CLASSIFICATION USING MACHINE VISION AND AN ARTIFICIAL NEURAL NETWORK,"Machine vision systems are being increasingly used for sophisticated applications such as classification and process control. Though there is significant potential for the increased deployment of industrial vision systems, a number of important problems have to be addressed to sustain growth in the area of industrial machine vision. Artificial neural networks coupled with machine vision systems offers a new methodology for solving difficult computational problems in many areas of science and engineering. Artificial neural networks, along with their varied learning techniques, have replaced complicated mathematical models and one area to benefit from these new computational techniques is machine vision. This paper presents a method of representing objects as planar shapes by generating a vector sequence of Euclidean distances between the shape centroid and each boundary pixel, which is translation invariant and can exhibit scale and rotation invariance if required. The sequence can be re-sampled to form a suitable input vector for an Artificial Neural Network. This method of representing planar shapes has been used to identify a suitable ANN architecture capable of acting as a classifier of manufactured products.",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1cbee22b2d9c84af78be4e6467ca2f01e7de0ee5,https://www.semanticscholar.org/paper/1cbee22b2d9c84af78be4e6467ca2f01e7de0ee5,Feature engineering workflow for activity recognition from synchronized inertial measurement units,,ACPR Workshops,2019.0,10.1007/978-981-15-3651-9_20,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9190a60e8258caf1a449cbe5fe6c485ab8ab2f20,https://www.semanticscholar.org/paper/9190a60e8258caf1a449cbe5fe6c485ab8ab2f20,Anode Quality Monitoring Using Advanced Data Analytics,,Light Metals 2019,2019.0,10.1007/978-3-030-05864-7_152,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2b03fa95a1233af6b413a7e2dc1c82c9f938049f,https://www.semanticscholar.org/paper/2b03fa95a1233af6b413a7e2dc1c82c9f938049f,Enhancing Usability for Automatically Structuring Digitised Dictionaries,The last decade has seen a rapid development of the number of NLP tools which have been made available to the community. The usability of several e-lexicography tools represents a serious obstacle for researchers with little or no background in computer science. We present in this paper our efforts to overcome this issue in the case of a machine learning system for the automatic segmentation and semantic annotation of digitised dictionaries. Our approach is based on limiting the burdens of managing the tool's setup in different execution environments and lightening the complexity of the training process. We illustrate the possibility to reach this goal through the adaptation of existing functionalities and through using out of the box software deployment technology. We also report on the community's feedback after exposing the new setup to real users of different professional backgrounds.,,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
50aa6f5a9a4e12cb2d3a4eeb02d1e8b428b3f984,https://www.semanticscholar.org/paper/50aa6f5a9a4e12cb2d3a4eeb02d1e8b428b3f984,A Path from Serial Execution to Hybrid Parallelization for Learning HPC,"Parallel and distributed computing are becoming necessary in almost all aspects of computation. Due to this growing demand, curriculum initiatives have been developed for integrating parallel and distributed computing into traditional undergraduate computer science programs. However, adoption has been slow resulting in many students lacking proper training for parallel and distributed computing. Two potential barriers for slow adoption are a deficiency in example programs that step students through the processes of parallelizing serial code, and the inaccessibility of dedicated machines to run highly parallel programs at scale within the confines of a course schedule. We have developed course material using a simple two-dimensional Lattice-Boltzmann Method Computational Fluid Dynamic simulation to walk students though shared memory parallelism, distributed memory parallelism, and hybrid parallel execution. We also created a custom mini-cluster comprised of 16 creditcard sized compute nodes, with a total of 288 cores, as an inexpensive solution for testing the scalability of different parallel models that can be deployed in a classroom setting.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cc6676cad602f85e194397bc17d23628ec22b7dd,https://www.semanticscholar.org/paper/cc6676cad602f85e194397bc17d23628ec22b7dd,Computational detection of socioeconomic inequalities,"Machine and deep learning advances have come to permeate modern sciences and have unlocked the study of numerous issues many deemed intractable. Social sciences have accordingly not been exempted from benefiting from these advances, as neural language model have been extensively used to analyze social and linguistic based phenomena such as the quantification of semantic change or the detection of the ideological bias of news articles, while convolutional neural networks have been used in urban settings to explore the dynamics of urban change by determining which characteristics predict neighborhood improvement or by examining how the perception of safety affects the liveliness of neighborhoods. In light of this fact, this dissertation argues that one particular social phenomenon, socioeconomic inequalities, can be gainfully studied by means of the above. We set out to collect and combine large datasets enabling 1) the study of the spatial, temporal, linguistic and network dependencies of socioeconomic inequalities and 2) the inference of socioeconomic status (SES) from these multimodal signals. This task is one worthy of study as previous research endeavors have come short of providing a complete picture on how these multiple factors are intertwined with individual socioeconomic status and how the former can fuel better inference methodologies for the latter. The study of these questions is important, as much is still unclear about the root causes of SES inequalities and the deployment of ML/DL solutions to pinpoint them is still very much in its infancy.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e318e554098224c9475dfc80765cbbb82fa4a409,https://www.semanticscholar.org/paper/e318e554098224c9475dfc80765cbbb82fa4a409,"Towards Fair, Equitable, and Efficient Peer Review","Peer review is the backbone of academia. The rapid growth of the number of submissions to leading publication venues has identified a need for automation of some parts of the peerreview pipeline and nowadays human referees are required to interact with various interfaces and technologies in this process. However, there exists evidence that if such interactions are not carefully designed, they can exacerbate various problems related to fairness and efficiency of the process. In my research, I aim to design a Human-AI collaboration pipeline in peer review to mitigate these issues and ensure that science progresses in a fair, equitable, and efficient manner. Despite peer review being the primary mechanism of science dissemination for decades, the rapid growth of the number of submissions to leading AI and ML conferences has challenged its sustainability in two ways: • It has brought up a call for automated tools to assist human decision-makers. • It has amplified the shortcomings of the peer-review procedure, making them more visible to the community and stressing the importance of research on peer review. These issues motivate my thesis research and I am passionate about working at the intersection of machine learning, operations research, social choice theory, and humancomputer interaction, to understand and develop a principled approach towards scientific peer review. Specifically, I believe that a carefully designed Human-AI collaboration is crucial for sustainability of peer review and in my work I aim at designing tools to support this collaboration. My research touches both algorithmic and human sides of the Human-AI collaboration and in the sequel I first describe my projects on supporting each of these sides. I then outline a direction for future work on bringing these sides to a closer interaction with a goal of improving the peer-review process. On a higher level, my work comprises novel theoretical and empirical contributions: I aim to design practical algorithms that are supported by strong theoretical guarantees and are evaluated in a carefully designed real-world experiments. The preliminary results I discuss below have already had a considerable impact in practice with some tools deployed in ICML 2020, and this inspires me to continue my work towards fair, equitable and efficient peer review. Copyright c © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Algorithmic Side. Past research in social science indicates that unfairness of the peer-review process may have farreaching consequences both on a development of research areas and on career trajectories of individual researchers. Therefore, my work on the algorithmic side is twofold: first, I aim to ensure that the algorithms used to automate peer review are themselves fair. Second, I aim at designing algorithms that help conference organizers to promote fairness. Fairness for Algorithms: The most automated part of the review process is the assignment of submissions to referees and most of the of the top AI and ML conferences rely on a simple and efficient matching algorithm developed by Charlin and Zemel (2013). Simultaneously, assignment is of the utmost importance: one cannot expect good reviews for papers that are assigned to unsuitable reviewers. In our past work (Stelmakh, Shah, and Singh 2018) we demonstrate that the state-of-the-art algorithm used by NeurIPS and ICML does not necessarily lead to a fair assignment, discriminating against some papers. More importantly, we design a novel assignment algorithm with provable guarantees on the fairness of the assignment that ensures that no paper is discriminated against to improve the assignment of more lucky counterparts. In addition to strong fairness guarantees, our algorithm is also optimal in terms of the accuracy of final decisions under a popular statistical model, that is, our algorithm theoretically outperforms the state-of-the-art algorithm both in terms of fairness and statistical accuracy. These guarantees are corroborated by an extensive empirical evaluation: in particular, our algorithm was tested and eventually deployed in the assignment of the ICML 2020 conference, improving the fairness by 15-30% while not trading off the conventional measure of the assignment quality. Algorithms for Fairness: While we can prove that algorithms employed to automate peer review satisfy the requirement of fairness, ensuring fairness of decisions made by humans is a more challenging task. An important direction that I am interested in is a use of algorithms to perform statistical testing for fairness and impartiality of final decisions. In our work (Stelmakh, Shah, and Singh 2019), we made the progress on this problem by contributing to the long-standing debate on the fairness of the decisions in single-blind peer review. In that, we design a novel semirandomized experimental procedure that allows to test for The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)",AAAI,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
75b21f9ef074a7385ef3837f63fcb0206006eb10,https://www.semanticscholar.org/paper/75b21f9ef074a7385ef3837f63fcb0206006eb10,COGNITIVE RISKS,"ABSTRACT Decision science have begun to enter the lexicon of risk professionals as the concepts from Prospect Theory become popular in media outlets who increasingly warn about the risk of human biases. Decision-making under uncertainty, popularized by Dan Kahneman, Amos Tversky, Paul Slovic, Herbert Simon and other economists, is more than an examination of human biases. Prospect Theory is a reexamination of the theory of choice and the causes of violations of utility theory that has blossomed into a broad and diverse body of research in behavioral and cognitive science. This paper is an outline for a proposed draft of a cognitive risk framework that will be developed to incorporate behavioral and cognitive science into an enterprise risk framework for cybersecurity and enterprise risk governance. Herbert Simon coined the term “Bounded Rationality” in his seminal book of the same name. “Broadly stated, the task is to replace the global rationality of economic man with the kind of rational behavior that is compatible with the access to information and the computational capacities that are actually possessed by organisms, including man, in the kinds of environments in which such organisms exist” (Simon 1955a: 99). Before the development of modern of PCs and even more powerful machine learning algorithms, Simon foresaw the opportunity at the intersection of human decision-making and technology. Since Simon, other economists and researchers have broaden insights from a multidisciplinary offering of academic studies into applied behavioral science. Notwithstanding these advances, only a few scientists have developed decision science solutions at scale at the enterprise level. Machine learning and other forms of artificial intelligence will require new rules of engagement and governance controls to ensure that bias and ethical use standards have been put in place. Data, the newest commodity in all digital strategies, must be better organized and structured in organizations to allow for efficacious information workflows needed to power organizations to higher performance. And lastly, the role of humans working with and alongside machines as decision-support tools are in the early stage of deployment. The research for the book, Cognitive Risks, will examine the last frontier in risk management – the role of human actors in a business environment that is transitioning to digital products and services. A new level of awareness is needed in a digital environment that differs from the physical world. We know this because of the advent of misinformation that now permeates the Internet. Nation states and Dark Web criminals have weaponized trust in the Internet through misinformation campaigns in social media sites by using behavioral science, or more specifically, cognitive hacks to change our behavior when surfing the web. These attacks are low cost and very effective because most observers are not aware of cognitive risks. There are many variations of “cognitive hacks” and “cognitive risks” which will be explained in detail in the book. Dimitry Kiselev, director general of Russia’s state-controlled Rossiyua Segodnya media conglomerate, “Objectivity is a myth which is proposed and imposed on us.” Today, thanks to the Internet and social media, the manipulation of our perception of the world is taking place on previously unimaginable scales of time, space and intentionality. Cognitive hacks and cognitive risks are part of a new lexicon of risks we must learn. Cognitive risks are commonly referred to as heuristic behavior. Heuristics is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision. Large swaths of the economy have already misjudged the potential, and the threats, of digital transformation. The questions explored in this paper and the subsequent book, Cognitive Risks, that will follow is why? Why do some leaders see opportunity when others only see problems? Why has the retail industry been blindsided by firms like Amazon, Google, Apple, and so many others? The research for the book will also include an exhaustive review of how applied behavioral science can be used to enhance organizational performance, risk management and cybersecurity in all organizations. Few, if any studies to date, have combined a multidisciplinary approach to enterprise risk management and organizational performance. This will be the first study that builds on a 2020 study of advancements in enterprise risk and board governance to provide a comprehensive analysis of methods and processes to apply behavioral science to address a range of risks facing organizations as they transition to a digital economy.",EDPACS,2021.0,10.1080/07366981.2020.1840020,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a15a9db724bc3ac593637cbaccbe3efc5f6e8f27,https://www.semanticscholar.org/paper/a15a9db724bc3ac593637cbaccbe3efc5f6e8f27,Learning through artifacts in engineering education,,,2012.0,10.1007/978-1-4419-1428-6,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1263dd580be3120198731ac09fb868561a8ae81d,https://www.semanticscholar.org/paper/1263dd580be3120198731ac09fb868561a8ae81d,Data-Driven Insights from Predictive Analytics on Heterogeneous Experimental Data of Industrial Magnetic Materials,"Data-driven methods are becoming increasingly popular in the field of materials science. While most data-driven models are trained on simulation data as it is relatively easier to collect a large amount of data from physics-based simulations, there are many challenges in applying data-driven methods on experiments: 1) experimental data is usually not clean; and 2) it generally has a greater degree of heterogeneity. In this project, we have developed a data-driven methodology to address these challenges on an industrial magnet dataset, where the goal is to predict magnetic properties (forward models) at different stages of the experimental workflow. The data-driven methodology consists of data cleaning, data preprocessing, feature extraction, and model development using traditional machine learning and deep learning methods to accurately predict magnet properties. In particular, we have developed three different types of predictive models: 1) numerical model using only numerical data containing composition and processing information; 2) image model using image data representing structure information; and 3) combination model using both types of data together. In addition to predictive models, the analysis and comparison of results across the models provide several interesting data-driven insights. Such data-driven analytics has the potential to help guide future experiments and realize the inverse models, which could significantly reduce costs and accelerate the discovery of new magnets with superior properties. The proposed models are already deployed in Toyota Motor Corporation.",2019 International Conference on Data Mining Workshops (ICDMW),2019.0,10.1109/ICDMW.2019.00119,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c62e33d4b9896f0f90aa8af743254baf5f3dd8aa,https://www.semanticscholar.org/paper/c62e33d4b9896f0f90aa8af743254baf5f3dd8aa,Towards Explainable Social Agent Authoring tools: A case study on FAtiMA-Toolkit,"Disclaimer: At the moment this article is currently awaiting submission review. Since this process usually takes a lot of time, feel free to use it this version as a reference until it is published. Thank you for reading, if you have any feedback please send us a message. Enjoy. The deployment of Socially Intelligent Agents (SIAs) in learning environments has proven to have several advantages in different areas of application. Social Agent Authoring Tools allow scenario designers to create tailored experiences with high control over SIAs behaviour, however, on the flip side, this comes at a cost as the complexity of the scenarios and its authoring can become overbearing. In this paper we introduce the concept of Explainable Social Agent Authoring Tools with the goal of analysing if authoring tools for social agents are understandable and interpretable. To this end we examine whether an authoring tool, FAtiMA-Toolkit, is understandable and its authoring steps interpretable, from the point-of-view of the author. We conducted two user studies to quantitatively assess the Interpretability, Comprehensibility and Transparency of FAtiMA-toolkit from the perspective of a scenario designer. One of the key findings is the fact that FAtiMA-Toolkit’s conceptual model is, in general, understandable, however the emotional-based concepts were not as easily understood and used by the authors. Although there are some positive aspects regarding the explainability of FAtiMA-Toolkit, there is still progress to be made to achieve a fully explainable social agent authoring tool. We provide a set of key concepts and possible solutions that can guide developers to build such tools. SectionionIntroduction Socially Intelligent Agents (SIAs) have an ever increasing range of applications from conversational interfaces on websites to tutors or teammates in educational environments [1, 2], where they are equipped with tools to conduct human-like interactions. Amongst the most promising applications of SIAs are serious games and social skills training environments. In these virtual environments SIAs behaviours can range from reactive wandering in the background of a scenario to complex social interactions that provide social support or assist the player in some skill training [3]. These autonomous agents sense the environment and act intelligently and independently from the user, allowing them to train and adapt specific verbal and nonverbal behaviors in socially challenging situations [4]. ar X iv :2 20 6. 03 36 0v 1 [ cs .M A ] 7 J un 2 02 2 A PREPRINT JUNE 8, 2022 Agent-based frameworks allow to simulate agent’s cognitive and affective processes [5, 6, 7, 8], in such environments, and can produce intelligent and emotional behaviour, in an unbounded number of situations. Yet, it is up to the author of a scenario – typically instructors, therapists, or researchers – to manually describe how individual traits, goals, beliefs and actions interact, and guarantee character adaptability and consistency as events unfold. This includes defining a plot, writing rules of behaviour, creating dialogues, keeping track of the possible outcomes, among other things. While this can be manageable in narrow domains of application, for a serious game or social skills training content designer, using an intelligent agent framework can quickly become an overbearing task. As a response to those difficulties, data-driven approaches to automatic content and agent creation have become very attractive and are pursued widely in academic research and industry [9]. Yet, these approaches require large datasets tailored to the domain1 and offer little control to the scenario designer. The main advantage of agent-based authoring tools is that it allows a scenario designer to have high control over content creation and target specific learning needs. Although previous research survey a set of challenges associated with the complexity and accessibility of the these tools [10, 11], we argue that tools scaffolded by understandable meta-models and metaphors will empower scenario designers and make them trust that they can easily create complex SIA interactions. Additionally, the way the author and the tool communicate should help the designer understand what comes next in the authoring workflow, and help them achieve their authoring goals. We refer to tools governed by these principles as Explainable Social Agent Authoring Tools. Explainability of social agent authoring tools not only is the extent to which a strong conceptual model can be easily understandable by authors, but also the extent to which a set of mechanisms can help the author, through interaction, explain the cause and effect of their actions. [12] explored this view of explanation-as-interaction making a parallel with Intelligent Tutoring System (ITS) design. Their view supports that XAI2 methods should drop the assumption that providing an explanation consists is summarizing a complex process in a few lines of text or simply showing a graphic illustrating the reasoning process of an algorithm3. They advocate that the tools should promote understanding by interaction, by applying similar methods to those applied in ITS research: highlight important concepts, integration of fragmentary knowledge, reflect on previous experience, etc. We argue that understanding how an AI framework works requires encoding knowledge explicitly in a framework of knowledge (meta-model) and that will allow to create context-aware authoring experiences that promote communication between the system and the user4. That implies design for explainability [13]. The contributions of this paper are fourfold. First, we introduce the concept of Explainable Social Agent Authoring Tools and we frame it within explainable AI (XAI) literature. Second, we examine whether an authoring tool, based on theoretical concepts is understandable and its authoring steps interpretable. Although there is an established assumption that theory-based architectures are understandable, we investigate if this is true from the point-of-the-view of the user, which has not been done before. We present data from two user studies, where we investigate whether FAtiMA-toolkit, which is grounded on Dennett’s Intentional Stance [14], is understandable. Third, data shows that while some artefacts are understandable other underlying concepts, namely emotions, that are not as easily understood. Furthermore, users choose between design long interactions or create scenario ramifications, due to the resulting complexity. Finally, we draw from the data and present a set of suggestions for leveraging machine learning approaches to drive explanation-asinteraction in agent-based authoring tools. It is our stance, that an interactive hybrid approach to authoring is necessary to make it a smooth process that users recognize as trustworthy. SectionionApproaches to Create Socially Intelligent Agents The design rational behind Socially Intelligent Agents (SIAs) represent decades of work across different fields such as Social Sciences, Cognitive Science and Human Computer Interaction [15]. Currently, there are two main approaches used to create SIAs: theory-driven and data-driven, both come with advantages and disadvantages. The former is a top-down approach that consists in developing computational models grounded on theoretical principles from the social sciences literature. The latter is bottom-up approach where the behaviour of the agents is generated/created from a large collection of examples of humans reacting and acting in different contexts. A less active line of research is the use of ontologies to support the decision-making of SIAs. Yet, they have gained a new momentum with newly datadriven models capable of making commonsense inferences about entities and events [16]. In this section, we describe characteristics of the tools under these two umbrella term and we provide a critical analysis these two approaches reflect transparency, complexity and interpretability the core concepts of an understandable tool (refer to Section 0.4 for more details). Furthermore, we also analyse the systems in terms of control and auditability as these concepts are central to social agents authoring tools. As a result, they struggle in generating content for open-ended worlds Explainable Artificial Intelligence These methods are usually referred as post-hoc explainability techniques in the literature in the form of authoring assistant, for instance.",ArXiv,2022.0,10.48550/arXiv.2206.03360,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3d57a8c1737d6fbc61343e918b64f2276e908eb8,https://www.semanticscholar.org/paper/3d57a8c1737d6fbc61343e918b64f2276e908eb8,"Automatic taxonomic identification based on the Fossil Image Dataset (>415,000 images) and deep convolutional neural networks","
 The rapid and accurate taxonomic identification of fossils is of great significance in paleontology, biostratigraphy, and other fields. However, taxonomic identification is often labor-intensive and tedious, and the requisition of extensive prior knowledge about a taxonomic group also requires long-term training. Moreover, identification results are often inconsistent across researchers and communities. Accordingly, in this study, we used deep learning to support taxonomic identification. We used web crawlers to collect the Fossil Image Dataset (FID) via the Internet, obtaining 415,339 images belonging to 50 fossil clades. Then we trained three powerful convolutional neural networks on a high-performance workstation. The Inception-ResNet-v2 architecture achieved an average accuracy of 0.90 in the test dataset when transfer learning was applied. The clades of microfossils and vertebrate fossils exhibited the highest identification accuracies of 0.95 and 0.90, respectively. In contrast, clades of sponges, bryozoans, and trace fossils with various morphologies or with few samples in the dataset exhibited a performance below 0.80. Visual explanation methods further highlighted the discrepancies among different fossil clades and suggested similarities between the identifications made by machine classifiers and taxonomists. Collecting large paleontological datasets from various sources, such as the literature, digitization of dark data, citizen-science data, and public data from the Internet may further enhance deep learning methods and their adoption. Such developments will also possibly lead to image-based systematic taxonomy to be replaced by machine-aided classification in the future. Pioneering studies can include microfossils and some invertebrate fossils. To contribute to this development, we deployed our model on a server for public access at www.ai-fossil.com.",Paleobiology,2022.0,10.1017/pab.2022.14,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
aa4f3540971b416400a5d95274a8215938677afc,https://www.semanticscholar.org/paper/aa4f3540971b416400a5d95274a8215938677afc,"Exploration of Knowledge Engineering Paradigms for Smart Education: Techniques, Tools, Benefits and Challenges","Knowledge engineering paradigms (KEPs) deal with the development of intelligent systems in which reasoning and knowledge play pivotal role. Recently, KEPs receive increasing attention within the fields of smart education. Researchers have been used the knowledge engineering (KE) techniques, approaches and methodologies to develop a smart tutoring systems (STSs). The main characteristics of such systems are the ability of reasoning, inference and based on static and heuristic knowledge. On the other side, the convergence of artificial intelligence (AI), web science (WS) and data science (DS) is enabling the creation of a new generation of web-based smart systems for all educational and learning tasks. This paper discusses the KEPs techniques and tools for developing the smart educational and learning systems. Four most popular paradigms are discussed and analyzed namely; case-based reasoning, ontological engineering, data mining and intelligent agents. The main objective of this study is to determine and exploration the benefits and advantages of such computational paradigms to increase the effectiveness and enhancing the efficiency of the smart tutoring systems. Moreover, the paper addresses the challenges faced by the application developers and knowledge engineers in developing and deploying such systems. In addition to institutional and organizational aspects of smart educational technologies development and application. Key-Words: Knowledge engineering and management, Artificial intelligence in education, Smart tutoring systems, Computational intelligence, Machine learning",,2020.0,10.37394/232010.2020.17.1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6787d3ddf6e2e5400d2e815c04c0c216740bab03,https://www.semanticscholar.org/paper/6787d3ddf6e2e5400d2e815c04c0c216740bab03,Technology Focus: Production Monitoring (March 2022),"We are seeing an uptrend in the instrumentation of legacy wells across the world, as costs lower and business cases become obvious. Several interesting technology applications have been showcased lately about retrofitting instrumentation and control technology in legacy wells. Advances on the digital front, where data science and engineering analytics are becoming more embedded in regular production monitoring and optimization processes, have been widespread.
 On the gas lift side, developments in surface controllable gas lift valves, which can be deployed during the completion phase, or retrofitted in existing mandrels through well intervention, have proceeded apace. This technology can bring significant improvement in the monitoring and management of wells because the operating envelope can be significantly expanded.
 Other technology developments involve the use of alternative data. As the saying goes, somebody’s noise is somebody else’s data. A use case of alternative data for rod pumps uses edge computing to process electric measurements and extract features using signal processing and machine learning. This can be used to create synthetic dynamometer cards used to optimize the wells in real time and predict failures ahead of time. Similar techniques could be applicable for electric submersible pumps.
 Distributed acoustic sensing (DAS) has many applications, but it is challenging to fully exploit the capability because of the sheer amount of raw data coming from these sensors. The data need to be compressed using feature-extraction algorithms. Each use case may require data over different regions of the frequency spectrum covered by the sensors, so how can feature extraction be set to account for all present and future use cases? Significant advances in inflow profiling have been achieved over the past few years, correlating DAS signals against flow-loop measurements and transient simulations. These technologies are very promising, especially now that wet connect and pumpdown technology for fiber optics is gaining more attention.
 Digital technology for orchestrating production-optimization and reservoir-management work flows has been increasingly embedding machine-learning functionality. Despite these advances, it remains a challenge to maintain these digital solutions over the long term. The most-successful companies in this area ensure the systems are tightly integrated with business-critical work flows, such as integrated activity planning, loss management, locked-in potential management, reservoir-surveillance planning, well-work-opportunity identification, production forecasting, and production back allocation. This involves a significant management of change process. It takes time and effort, but the rewards are worth it.
 Recommended additional reading at OnePetro: www.onepetro.org.
 SPE 207879 - Expert Advisory System for Production Surveillance and Optimization Assisted by Artificial Intelligence by Carlos Mata, ADNOC Upstream, et al.
 SPE 201313 - Production Rate Measurement Optimization Using Test Separator and In-Well Sound Speed by Ö. Haldun Ünalmis, Weatherford
 SPE 203119 - Wireless Completion Monitoring and Flow Control: A Hybrid Solution for Extended Capabilities by Marcel Bouman, Emerson Automation Solutions",Journal of Petroleum Technology,2022.0,10.2118/0322-0068-jpt,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f638a25e5c249cf9a782ad23ef4110d0742973f1,https://www.semanticscholar.org/paper/f638a25e5c249cf9a782ad23ef4110d0742973f1,Resource Aware Fog Based Remote Health Monitoring System,"In today’s world of medical science, remote patient monitoring devices are becoming more important and a future need particularly in the present COVID-19 situation as individuals are preferred to be kept isolated. Patients would be benefited from a suitable monitoring system that measures their important medical parameters such as pulse rate, oxygen saturation or SpO2, body temperature, blood pressure, and Galvanized Skin Response (GSR). This system can increase the medical staff efficiency by drastically decreasing their duties in hospitals and the need to attend to them individually. Patients in their home isolation may utilize the device as well, and their vital indicators may be checked by doctors remotely. In this work, we are prototyping a powerefficient, wearable medical kit and a resource-aware fog network set up to handle the Internet of Things (IoT) data traffic. The idea behind the design is to process the critical medical sensors’ data in the fog nodes which are deployed at the edge of the network. The data thus received, is used for a machine learning-based solution for personal health anomalies and COVID-19 infection risk analysis.",IEEE INFOCOM 2022 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),2022.0,10.1109/infocomwkshps54753.2022.9798058,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e1036088d1820f7aa5a029cb92e4a642849473d8,https://www.semanticscholar.org/paper/e1036088d1820f7aa5a029cb92e4a642849473d8,Internet of things–Enabled technologies as an intervention for childhood obesity: A systematic review,"Childhood obesity is one of the most serious public health challenges of the 21st century, with consequences lasting into adulthood. Internet of Things (IoT)-enabled devices have been studied and deployed for monitoring and tracking diet and physical activity of children and adolescents as well as a means of providing remote, ongoing support to children and their families. This review aimed to identify and understand current advances in the feasibility, system designs, and effectiveness of IoT-enabled devices to support weight management in children. We searched Medline, PubMed, Web of Science, Scopus, ProQuest Central and the IEEE Xplore Digital Library for studies published after 2010 using a combination of keywords and subject headings related to health activity tracking, weight management, youth and Internet of Things. The screening process and risk of bias assessment were conducted in accordance with a previously published protocol. Quantitative analysis was conducted for IoT-architecture related findings and qualitative analysis was conducted for effectiveness-related measures. Twenty-three full studies are included in this systematic review. The most used devices were smartphone/mobile apps (78.3%) and physical activity data (65.2%) from accelerometers (56.5%) were the most commonly tracked data. Only one study embarked on machine learning and deep learning methods in the service layer. Adherence to IoT-based approaches was low but game-based IoT solutions have shown better effectiveness and could play a pivotal role in childhood obesity interventions. Researcher-reported effectiveness measures vary greatly amongst studies, highlighting the importance for improved development and use of standardised digital health evaluation frameworks.",PLOS Digital Health,2022.0,10.1371/journal.pdig.0000024,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8061d521fe7c62430e1ba2cb0b8f4d19f7366e6f,https://www.semanticscholar.org/paper/8061d521fe7c62430e1ba2cb0b8f4d19f7366e6f,Multi Uav Cooperative Surveillance With Spatio Temporal,"Deep Learning for Unmanned SystemsMultiple Heterogeneous Unmanned Aerial VehiclesAdvanced Mobile RoboticsSafe Robot Navigation Among Moving and Steady ObstaclesComputer Safety, Reliability, and SecurityAdvances in Swarm IntelligenceHolonic and Multi-Agent Systems for ManufacturingAdvances in Artificial Intelligence and Applied Cognitive ComputingUnmanned Aircraft SystemsIntelligent Computing Theories and ApplicationAutonomous Airborne Wireless NetworksAd Hoc NetworksEnabling Blockchain Technology for Secure Networking and CommunicationsUAV Sensors for Environmental MonitoringUnmanned Aerial Vehicles: Breakthroughs in Research and PracticeComputational Collective IntelligenceTime-Critical Cooperative Control of Autonomous Air VehiclesAdvances in Cooperative Control and OptimizationCooperative Robots and Sensor Networks 2015Artificial Intelligence and SecurityPRICAI 2016: Trends in Artificial IntelligenceClosing the Gap Between Research and Field Applications for Multi-UAV Cooperative MissionsMulti-rotor Platform Based UAV SystemsProceedings of the Future Technologies Conference (FTC) 2020, Volume 1Unmanned Aerial SystemsAdvanced Distributed Consensus for Multiagent SystemsCooperative Control of MultiAgent SystemsMulti-UAV Planning and Task AllocationMobile Internet SecurityCooperative Control of Multiple Unmanned Aerial Vehicles with Application to Forest Fire Detection and FightingMulti UAV Systems with Motion and Communication ConstraintsIntelligent Autonomy of UAVsIntelligent and Fuzzy Techniques in Big Data Analytics and Decision MakingIntelligent Autonomy of UAVsUAV Cooperative Decision and ControlCooperative Localization and NavigationAdvances in Guidance, Navigation and ControlMachine Learning and Intelligent CommunicationsUnmanned Aerial VehiclesThe Cognitive Approach in Cloud Computing and Internet of Things Technologies for Surveillance Tracking Systems Ad hoc networks, which include a variety of autonomous networks for specific purposes, promise a broad range of civilian, commercial, and military applications. These networks were originally envisioned as collections of autonomous mobile or stationary nodes that dynamically auto-configure themselves into a wireless network without relying on any existing network infrastructure or centralized administration. With the significant advances in the last decade, the concept of ad hoc networks now covers an even broader scope, referring to the many types of autonomous wireless networks designed and deployed for a specific task or function, such as wireless sensor networks, vehicular networks, home networks, and so on. In contrast to the traditional wireless networking paradigm, such networks are all characterized by sporadic connections, highly error-prone communications, distributed autonomous operation, and fragile multi-hop relay paths. The new wireless networking paradigm necessitates reexamination of many established concepts and protocols, and calls for developing a new understanding of fundamental problems such as interference, mobility, connectivity, capacity, and security, among others. While it is essential to advance theoretical research on fundamental and practical research on efficient policies, algorithms and protocols, it is also critical to develop useful applications, experimental prototypes, and real-world deployments to achieve an immediate impact on society for the success of this wireless networking paradigm.A comprehensive review of the state of the art in the control of multi-agent systems theory and applications The superiority of multi-agent systems over single agents for the control of unmanned air, water and ground vehicles has been clearly demonstrated in a wide range of application areas. Their large-scale spatial distribution, robustness, high scalability and low cost enable multi-agent systems to achieve tasks that could not successfully be performed by even the most sophisticated single agent systems. Cooperative Control of Multi-Agent Systems: Theory and Applications provides a wide-ranging review of the latest developments in the cooperative control of multi-agent systems theory and applications. The applications described are mainly in the areas of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). Throughout, the authors link basic theory to multi-agent cooperative control practice — illustrated within the context of highly-realistic scenarios of high-level missions — without losing site of the mathematical background needed to provide performance guarantees under general working conditions. Many of the problems and solutions considered involve combinations of both types of vehicles. Topics explored include target assignment, target tracking, consensus, stochastic game theory-based framework, event-triggered control, topology design and identification, coordination under uncertainty and coverage control. Establishes a bridge between fundamental cooperative control theory and specific problems of interest in a wide range of applications areas Includes example applications from the fields of space exploration, radiation shielding, site clearance, tracking/classification, surveillance, search-and-rescue and more Features detailed presentations of specific algorithms and application frameworks with relevant commercial and military applications Provides a comprehensive look at the latest developments in this rapidly evolving field, while offering informed speculation on future directions for collective control systems The use of multi-agent system technologies in both everyday commercial use and national defense is certain to increase tremendously in the years ahead, making this book a valuable resource for researchers, engineers, and applied mathematicians working in systems and controls, as well as advanced undergraduates and graduate students interested in those areas.Time-Critical Cooperative Control of Autonomous Air Vehicles presents, in an easy-to-read style, the latest research conducted in the industry, while also introducing a set of novel ideas that illuminate a new approach to problem-solving. The book is virtually self-contained, giving the reader a complete, integrated presentation of the different concepts, mathematical tools, and control solutions needed to tackle and solve a number of problems concerning time-critical cooperative control of UAVs. By including case studies of fixed-wing and multirotor UAVs, the book effectively broadens the scope of application of the methodologies developed. This theoretical presentation is complemented with the results of flight tests with real UAVs, and is an ideal reference for researchers and practitioners from academia, research labs, commercial companies, government workers, and those in the international aerospace industry. Addresses important topics related to time-critical cooperative control of UAVs Describes solutions to the problems rooted in solid dynamical systems theory Applies the solutions developed to fixed-wing and multirotor UAVs Includes the results of field tests with both classes of UAVsThis book provides the state-of-the-art intelligent methods and techniques for solving realworld problems along with a vision of the future research. The fifth 2020 Future Technologies Conference was organized virtually and received a total of 590 submissions from academic pioneering researchers, scientists, industrial engineers, and students from all over the world. The submitted papers covered a wide range of important topics including but not limited to computing, electronics, artificial intelligence, robotics, security and communications and their applications to the real world. After a double-blind peer review process, 210 submissions (including 6 poster papers) have been selected to be included in these proceedings. One of the meaningful and valuable dimensions of this conference is the way it brings together a large group of technology geniuses in one venue to not only present breakthrough research in future technologies, but also to promote discussions and debate of relevant issues, challenges, opportunities and research findings. The authors hope that readers find the book interesting, exciting and inspiringAdvanced Distributed Consensus for Multiagent Systems contributes to the further development of advanced distributed consensus methods for different classes of multiagent methods. The book expands the field of coordinated multiagent dynamic systems, including discussions on swarms, multi-vehicle and swarm robotics. In addition, it addresses advanced distributed methods for the important topic of multiagent systems, with a goal of providing a high-level treatment of consensus to different versions while preserving systematic analysis of the material and providing an accounting to math development in a unified way. This book is suitable for graduate courses in electrical, mechanical and computer science departments. Consensus control in multiagent systems is becoming increasingly popular among researchers due to its applicability in analyzing and designing coordination behaviors among agents in multiagent frameworks. Multiagent systems have been a fascinating subject amongst researchers as their practical applications span multiple fields ranging from robotics, control theory, systems biology, evolutionary biology, power systems, social and political systems to mention a few. Gathers together the theoretical preliminaries and fundamental issues related to multiagent systems and controls Provides coherent results on adopting a multiagent framework for critically examining problems in smart microgrid systems Presents advanced analysis of multiagent systems under cyberphysical attacks and develops resilient control strategies to guarantee safe operationComplete with online files and updates, this cutting-edge text looks at the next generation of unmanned flying machines. Aerial robots can be considered as an evolution of the Unmanned Aerial Vehicl",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c59dc28f30409531bae0cd0b99969070ecd3ed01,https://www.semanticscholar.org/paper/c59dc28f30409531bae0cd0b99969070ecd3ed01,The Application of English Teaching Based on Cloud Network with Virtual Machine Technology,"The research and practice of virtual machine technology in English teaching is in its infancy, research involving the production and use of the teaching courseware, the use of existing educational software, and the use of computer networks classrooms, the campus network, the Internet and distance learning utility. These research and practice indicates the trend of the future development of foreign language teaching. This article focuses on the discusses of the popularity of cloud computing and virtual machine instruction, by introducing the technology of VMware virtualization to solve problems encountered in the process of teaching cloud computing, fully demonstrated the widely use of virtualization technology and its irreplaceable role . Introduction The development of information technology and computer applications has had a tremendous impact on foreign language teaching. Multimedia technology can be used to design a new process of teaching and interactive, personalized training methods , which tightly integrated English teachers’ teaching process and students learning process, and prompting the English teachers to generate new ideas in teaching, promoting the teaching process of fundamental change, prompting the students to change the traditional passive learning style [1,2]. This paper aims to present their views on the problems encountered in the promotion process and teaching process, through traditional and relatively mature virtualization technology to solve problems encountered in teaching ""cloud computing"" [3]. In fact, this is also a kind of cloud computing solutions, more specifically, an application of virtualization technology, a combination of virtualization technology and teaching application of cloud computing, a platform to promote cloud computing with VMware technology built. Advantage of virtual machine Convenient and safe use of a computer through the virtual machine to install more than one operating system to learn; portability of software test platform migration process; develop cross-platform system software for cross-platform testing. For example, mission-critical Windows and Linux-based [4,5] application development, virtual machines can take advantage of cross-platform development. The use of virtual machines in a computer at the same time enables multiple clients connected into a network, completely realistic simulated environment for testing or learning. Noting that virtualization is the logical representation of resource. Cloud computing introduction Although very young, cloud computing has become a broader application of technology, and various cloud emerging in the IT sector, some analysts believe that cloud computing represents a change in the way of enterprise computing. Expected that over the next five years, many giant manufacturers around the world, such as IBM, Dell, and Hewlett-Packard will transfer its own product line to cloud computing. With more and more enterprises turning to cloud computing, the traditional CPU chip chase higher performance, the pursuit of more large-scale supercomputers tirelessly to improve the performance of a single system industry development model will be slowly 2nd International Conference on Management Science and Industrial Engineering (MSIE 2013) © 2013. The authors Published by Atlantis Press 627 replacing. Cloud computing course, have great vitality and represent the future direction of development of the IT industry, on behalf of the people’s target in the IT industry, but this is also need to spread cloud computing in teaching which not only needs to implement a solid theoretical foundation for students, but also requires a combination of practice and more experimental, and a deep understanding of cloud computing. PM-LB algorithm for virtual machine deployment based on the performance of vector The study of deployment algorithm should fully considering the cloud computing’s multi-user and multi-service environment, the reason is that the system is based on a virtual machine hosted business whose dependence is different for different resources, mainly dependent on the performance of the virtual machine as a user preference for performance and making resource allocation is given adequate resources to reserve space, the side of the user is designed to obtain a better user experience, admittedly. When dealing with the virtual machine deployment, we need first to effectively monitor the performance of the virtual machines. For that the virtual machine hardware resources generally consist primarily of CPU performance, memory utilization, network connectivity and configuration state of the virtual machine on the host operating status, etc. Standardization of performance characteristics herein by reference Virtual Machine Manager 2008 technical report performance evaluation criteria for the physical servers , the four basic performance of the CPU, memory, substitution, and a hard disk , for example, per 10min to extract the average value of the condition of use , according to resource characteristics calculated under treatment:",,2013.0,10.2991/MSIE-13.2013.138,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0a382216eaa782e1832447c73594d98a0f1782d1,https://www.semanticscholar.org/paper/0a382216eaa782e1832447c73594d98a0f1782d1,The journal of knowledge engineering special issue on WorldCist'19—Seventh World Conference on Information Systems and Technologies,"The constant growth of technology leads to the development of expert systems that serve to support critical decision-making and have applications in many areas, such as healthcare, business, chemistry, financial decision-making, and engineering. These systems are computer programs derived from a computer science research branch called Artificial Intelligence (AI) and use human knowledge intensively in problem-solving. These programs combine expert knowledge and use the knowledge necessary to solve problems (Kidd, 2012). In this special issue, we present a range of papers covering some of the subareas of expert systems such as intelligent and decision support systems, ethics, computers, and security, health informatics, simulations, and big-data analytics. This special issue comprises six research papers. All manuscripts are extended versions of selected papers from WorldCIST'19 - 7th World Conference on Information Systems and Technologies, held in at La Toja Island, Galicia, Spain, April 2019. The WorldCIST conference have become a global forum for researchers and practitioners to present and discuss the most recent innovations, trends, results, experiences, and con-cerns in the several perspectives of Information Systems and Technologies, as well as computer science in general. The six selected papers in this special section include a Virtual Programming Lab (VPL), a model's predictions, a novel information systems architecture for the agri-food sector, various approaches for detection of malware, an intelligent system to assess, in real-time, potential HRV indices, that can predict HRQoL in lymphoma patients throughout chemotherapy treatment, as well as an expert system comprising a self-aware framework for resource-efficient and accurate data transmission within a low-power lossy sensor network (LLN) deployed for indoor monitoring. Cardoso et al. (2020) present the VPL, a Moodle plugin that allows students to submit their code and get prompt feedback without the teacher's intervention. To test this concept, an experiment was performed with several classes of beginner programming students, in two editions of Algorithms and Programming course unit of the degree in Informatics Engineering lectured at the Informatics Engineering Department at the School of Engineering, Polytechnic Institute of Porto. on sig-natures and are error-prone. Traditional machine learning techniques are based on static, dynamic, and hybrid analysis; however, for large scale Android malware analysis, these approaches are not feasible. Deep neural architectures can analyze large scale static details of the applications, but static analysis techniques can ignore many malicious behaviours of applications. The study contributes to the documentation of various constructing a 6LoWPAN network in the Contiki Cooja simulator. MCDM is applied to generate an adaptive objective function for the IPv6 routing protocol for the LLN (RPL) and to aid in ranking the nodes to select the best available neighbouring node, while the data accuracy is ensured by the cluster head through data corre-lation among its associated members. The network performance is assessed by analyzing the packet delivery rate, throughput and energy con-sumption against varying sensors and by comparing our proposed MCDM-RPL with a standard RPL and a fuzzy-based RPL, where the results show that our framework is found to be better with gains of 13%, 25% and 13%, respectively.",Expert Syst. J. Knowl. Eng.,2021.0,10.1111/exsy.12711,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
00a1cb012395458fb5c63eb86cb88d8b116b2449,https://www.semanticscholar.org/paper/00a1cb012395458fb5c63eb86cb88d8b116b2449,Resumé,"Herman Lam is an Associate Professor of Electrical and Computer Engineering at the University of Florida. He has over 25 years of research and development experience in the areas of distributed computing, service-oriented computing, and database management. Currently, Dr. Lam’s main research interest is in reconfigurable computing (RC), focused upon methods and tools for the acceleration and deployment of scientifically impactful applications on scalable RC systems like the Novo-G reconfigurable supercomputer. He was a Co-PI of the 2012 Alexander Schwarzkopf Prize for Technology Innovation from the National Science Foundation for “Novo-G: An innovative and synergistic research project and the world’s most powerful reconfigurable supercomputer”. Dr. Lam also led a team of graduate students at the University of Florida to win the 2018 Dell EMC AI Challenge, recognized for developing and demonstrating a heterogeneous computing (HGC) system that can support a complete workflow–data analysis and pre-processing, model training, and deployment and inferencing–for machine learning.",Devenir prêtre ou prêtresse vodou en Haïti,1929.0,10.1111/j.1651-2227.1929.tb06298.x,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
71156ebc6f4a94cf0834e8ef6bd0a865463ed9da,https://www.semanticscholar.org/paper/71156ebc6f4a94cf0834e8ef6bd0a865463ed9da,Accessing spatial knowledge networks with maps,"ABSTRACT Currently, knowledge networks develop to establish common data spaces. A common data-space offers mutual exchange and reusability for data sources and their derived information and provides access to structured knowledge and even creates wisdom. The geospatial domain becomes included in those knowledge networks and, therefore, creates spatial knowledge networks. ‘Geospatial’ is moving from a special expert domain to a ‘normal’ common data source that is processed for specific data science use cases. Maps with their different levels of abstraction according to its transmission task may offer (1) strategies to enhance processing performance, due to its abstraction, (2) persistent references of map features throughout different scales (abstractions) and (3) improvement of the transmission of spatial information, which includes the transmission interfaces as well as geo-communication. This paper tries to identify new functions for maps in new developing application areas. For example, a ‘universal semantic structure of topographic content’ could help to establish relations/links across domains that only have their own feature keys. We try to set the scene of cartography in a common data-space and highlight some requirements in the world of spatial knowledge networks, which are needed for automatization, machine learning and AI. According to Gordon and de Souza location matters: ‘Mapping is not simply a mode of visualisation, but a “central organizational device for networked communications”, an adaptive interface through which users can access, alter and deploy an expansive database of information, and a platform to socialize spatial information through collective editing, annotations, discussion, etc.’ [Gordon, E., & de Souza e Silva, A. (2011). Net locality: Why location matters in a networked world. John Wiley & Sons, p. 28].",International Journal of Cartography,2021.0,10.1080/23729333.2021.1972910,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
76bc6b1bee867a84fa336c7d64fc466487af43b6,https://www.semanticscholar.org/paper/76bc6b1bee867a84fa336c7d64fc466487af43b6,Cyber-Physical Systems for Smart Water Networks: A Review,"There is a growing demand to equip Smart Water Networks (SWN) with advanced sensing and computation capabilities in order to detect anomalies and apply autonomous event-triggered control. Cyber-Physical Systems (CPSs) have emerged as an important research area capable of intelligently sensing the state of SWN and reacting autonomously in scenarios of unexpected crisis development. Through computational algorithms, CPSs can integrate physical components of SWN, such as sensors and actuators, and provide technological frameworks for data analytics, pertinent decision making, and control. The development of CPSs in SWN requires the collaboration of diverse scientific disciplines such as civil, hydraulics, electronics, environment, computer science, optimization, communication, and control theory. For efficient and successful deployment of CPS in SWN, there is a need for a common methodology in terms of design approaches that can involve various scientific disciplines. This paper reviews the state of the art, challenges, and opportunities for CPSs, that could be explored to design the intelligent sensing, communication, and control capabilities of CPS for SWN. In addition, we look at the challenges and solutions in developing a computational framework from the perspectives of machine learning, optimization, and control theory for SWN.",IEEE Sensors Journal,2021.0,10.1109/jsen.2021.3121506,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d53d6e5f19a3a5934a335f03d9a4f63fc55142bb,https://www.semanticscholar.org/paper/d53d6e5f19a3a5934a335f03d9a4f63fc55142bb,Virtual European Solar & Planetary Access (VESPA)  2021: consolidation,"<p>VESPA (Virtual European Solar and Planetary Access) has been focusing for nearly 10 years on adapting Virtual Observatory (VO) techniques to handle Planetary Science data [1] [2]. The objective of this activity is to build a contributive data distribution system where data services are located and maintained in research institutes, as well as in space agencies and observatories. This system is responsive to the new paradigm of Open Science and FAIR access to the data.</p>
<p>During the previous Europlanet-2020 program, VESPA has defined an architecture adapted from the astronomy VO, incorporating concepts and standards from other areas (Earth observation, Heliophysics, etc). The basic system uses the VO infrastructure: data services are installed in any location but are declared in a system of harvested registries with identifiers, end-point (URL), mention of supported access protocols, and rough description of content. Such services are interoperable via clients and tools, which also provide visualization and analysis functions.</p>
<p>The activity in Europlanet-2024 focuses on expanding this environment, enforcing sustainability, and opening new possibilities to improve data processing &#8211; such as workflows, cloud-based computation, and readiness for exploitation through Machine Learning techniques.</p>
<p><strong>Data access</strong>. VESPA has defined a specific access protocol called EPN-TAP which at the time of writing is a Working Draft of the Internal Virtual Observatory Alliance (IVOA), and expected to become a Recommendation in the coming months [3]. The EPN-TAP metadata system provides uniform description of datasets not only to access data in a VO context, but also for research projects. EPN-TAP is compliant with the general TAP protocol, allowing usage of existing VO tools and communication protocols with data services pertaining to Solar System studies. Some VO tools (TOPCAT, Aladin, CASSIS) were also adapted to improve the handling of such data.</p>
<p>The VESPA portal, intended as a discovery tool to browse the EPN-TAP services, is under study to improve the user experience. ElasticSearch capacities are being implemented, and all interface mechanisms are being evaluated. Other, more specific access modes (via script, web services, VO tools, etc) are also being reviewed.</p>
<p><strong>Data services</strong>. There are currently 55 EPN-TAP data services published in the IVOA registry, and about 20 in development phase. Most of them are implemented on DaCHS, a VO data server provided by Heidelberg University. A major upgrade of DaCHS published last year implements recent evolutions of IVOA standards. Existing data services are currently reviewed for compliance, and upgraded to benefit from the latest developments. In many cases, this is also an occasion to extend their content with new data. This upgrade also addresses low-level technical aspects, e.g. related to declaration in the IVOA registry.</p>
<p>Larger data infrastructures with EPN-TAP interface (AMDA, SSHADE, PVOL) also continue to develop their content and capacities, e.g. band lists have been implemented in SSHADE this year.</p>
<p><strong>Sustainability</strong>. This major update relies on the VESPA hubs activity: definition files of all services are stored in a unique gitlab for preservation and maintenance by several VESPA teams. Authentication is granted by G&#201;ANT/eduTEAMS. This is a simple and efficient way to share the technical expertise among services and teams, and to improve sustainability.</p>
<p><strong>New environments</strong>. VESPA-cloud was a project supported by EOSC-Hub, through its 2nd Early Adopter Program (2020-21). It was an assessment of the deployment of EPN-TAP services on EOSC (the recent European Open Science Cloud) inside Virtual Machines or Docker containers, from the same gitlab installation used to preserve the services. The assessment was successful and opens up new solutions and opportunities for future VESPA service implementations. It will provide a workaround to services temporary unavailability, for performing cloud-based computation on data services, and a solution for data providers who are not able or not willing to host a VESPA server for a long period of time.</p>
<p><strong>New services</strong>. Implementation of new services has been going on with internal projects. External ones will restart with an on-line implementation workshop before the end of the year. A VizieR EPN-TAP service will provide access to the data content of articles related to the Solar System and exoplanets (hopefully ready at the time of the conference). An interface with space agency archives will make use of the recent PDS4 dictionary for EPN-TAP (in addition to the existing EPN-TAP interface on ESA&#8217;s PSA).</p>
<p>Discussions have started with other WP producing data in Europlanet-2024 to start distributing their results using the VESPA infrastructure: other VAs (SPIDER, GMap, ML), NA2 (telescope network), and TAs (lab experiments and field studies). VESPA is of course also available to distribute data from other H2020 programmes in the field.</p>
<p><strong>Prospects</strong>. Detailed examples of recent VESPA developments are provided in this session and related ones. The focus will shift again next year to new data services, with the finalization of several projects, in particular related to the Moon, Mercury, and exoplanets. A workflow platform will also be connected to perform run-on-demand (the OPUS system also used by the ESCAPE H2020 programme) and cloud-based activity will expand.</p>
<p>&#160;</p>
<p>The Europlanet-2024 Research Infrastructure project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreements No 871149.</p>
<p>&#160;[1]&#160;Erard et al 2018,<em> Planet. Space Sci. </em><strong>150</strong>, 65-85. 10.1016/j.pss.2017.05.013. ArXiv 1705.09727&#160;&#160;</p>
<p>&#160;[2]&#160;Erard et al. 2020, <em>Data Science Journal</em> <strong>19</strong>, 22. doi: 10.5334/dsj-2020-022.</p>
<p>&#160;[3]&#160;https://ivoa.net/documents/EPNTAP/20201027/index.html (still open for comments)</p>",,2021.0,10.5194/epsc2021-506,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cf2c515836d3a8d9938b77a8dfa1b937bc32640e,https://www.semanticscholar.org/paper/cf2c515836d3a8d9938b77a8dfa1b937bc32640e,"Editorial: Scalable Bioinformatics: Methods, Software Tools, and Hardware Architectures","Advances in DNA sequencing technology have contributed to the accumulation of molecular sequence data at an unprecedented pace, since whole genomes can now be sequenced rapidly, accurately, and cost effectively. When methods and tools are not specifically designed to handle big volumes of data efficiently, large-scale analyses practically become infeasible due to the explosion in processing and memory requirements. Bioinformatics algorithms frequently rely on approximations and heuristics to yield computationally tractable implementations, at the cost of performing less thorough analyses. This Research Topic presents a series of works that connect computational problems in the fields of Bioinformatics and Computational Biology with software and hardware solutions from the fields of Computer Science and Computer Engineering to address scalability issues across a variety of Bioinformatics problems. The Basic Local-Alignment Search Tool, BLAST (Altschul et al., 1990), is one of the most widely used algorithms to search for sequence similarities in Bioinformatics. Gálvez et al. introduce BLVector, a heuristic algorithm that adapts high-level concepts of BLAST+ to many-core x86 architectures with Single-Instruction Multiple Data (SIMD) vector instructions of the Advanced Vector eXtensions (AVX)-512 instruction set. BLVector outperforms BLAST+ for mid-size protein sequences (∼750 amino acids), and retrieves a much larger set of results than BLAST+ when applied to longer proteins, at the cost of a longer execution time. BLVector can be up to an order of magnitude faster than BLAST+ on various many-core processor architectures, and the authors suggest that BLVector and BLAST+ can be considered as complementary tools. Autism spectrum disorder (ASD) is a neurodevelopmental disorder that has been extensively studied over the past decades (Cox et al., 1999; Marshall et al., 2008; Ozonoff and Iosif, 2019). Garbulowski et al. developed an analysis pipeline to construct interpretable machine learning models and performed an analysis of multiple cohorts of control-case studies of Autism Spectrum Disorder (ASD). The analysis revealed that autism is the most severe subtype of ASD, while pervasive developmental disorder-not otherwise specified (PDD-NOS) and Asperger syndrome are closely related and milder subtypes of ASD. Additionally, the authors analyzed the most important ASDrelated features described in terms of gene co-predictors, finding a strong co-predictive mechanism and possible co-regulation between genes EMC4 and TMEM30A. This study showcases one more application of Machine Learning and outlines its potential in providing insights into important medical and biological questions while encouraging the deployment of other techniques such as deep learning (LeCun et al., 2015) as well. Edited and reviewed by: Richard D. Emes, University of Nottingham, United Kingdom",Frontiers in Genetics,2022.0,10.3389/fgene.2021.822986,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7d9db17ca7805f0d884fe485f38392b8033f5301,https://www.semanticscholar.org/paper/7d9db17ca7805f0d884fe485f38392b8033f5301,Augmented intelligence: The new world of surveys at work,"The goal of this article is to inform science, practice, and debate that are related to modern applications of organizational survey research, particularly with respect to the technology-assisted pursuit of happiness and success at work. Our hope is to inspire researchers, practitioners, and HR leaders to start building now for the world of organizational surveys we will live in 15 years from now. The observations and conclusions that are made in this article are drawn from three primary sources: the literature on organizational surveys and employee attitudes, the literature on computational methods, and our personal experience with designing and deploying technology-assisted survey insights at hundreds of large organizations over the past 6 years. Producing action from organizational surveys remains one of the biggest challenges with them, even though action is generally what drives positive change (Church & Oliver, 2006; Donovan & Brooks, 2006). Frankly, we experts have underperformed on our goal to use surveys to focus and accelerate change, largely due to systems that rely too heavily on administrative involvement, especially from often underresourced human resources (HR) departments/people teams. We believe experts play a critical role in the process (Church & Oliver, 2006) and that expertise can be delivered at greater scale than it is today in terms of both the number of people affected and the size of the effects. Data science, learning and development, and design science are converging to scale personalized insights for leaders, using expert-built algorithms to suggest focus areas and serving up expert-vetted tools and resources through intuitive online experiences. Given that people will increasingly coexist with machines at work, it is important that our field engages more actively with what we call augmented intelligence, or highly specific machineassisted insights that are faster and smarter than a human could produce. Augmented intelligence is differentiated from artificial intelligence (AI) in that the latter is often operationalized as a broad and complex set of machine-based operations that are organized via an executive function so as to mimic human intelligence. Many models of AI suggest that its purpose is to wholly supplant human intelligence. Even more nuanced models like Searle’s (1980) strong and weak AI propose that both the “stronger” and “weaker” forms of AI serve to replace rather than support or augment human intellect. Simply put, augmented intelligence serves to support the human ability to produce valuable insights. In doing so, augmented intelligence augments one small aspect of human intelligence, like the cognitive processing of topics and sentiment from open-ended survey comments, to enrich (rather than fully write) the stories we can tell through data. This article provides the context for and implications of that engagement.",Industrial and Organizational Psychology,2021.0,10.1017/iop.2021.89,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
83f5e5655e3b32dd58c58f2f63edb63c21870f27,https://www.semanticscholar.org/paper/83f5e5655e3b32dd58c58f2f63edb63c21870f27,BLACK BOX ARTIFICIAL INTELLIGENCE AND THE RULE OF LAW,"It seems fitting to explore issues of emerging uses of algorithmic decisionmaking and artificial intelligence (AI) through an interdisciplinary publication like Law & Contemporary Problems. After all, the AI tools at the heart of these articles are being deployed in nearly every industry and in every corner of the globe. This small volume brings together leading thinkers in philosophy, ethics, data science, computer science, and law, who connect with us from Germany, Belgium, England, Columbia, and the United States. This cosmopolitan and cross-disciplinary approach offers particular value for the exploration of socio-technical systems where AI influences meaningful determinations, distributions, and allocations of rights and responsibilities. Precisely because AI affects personal and professional opportunities, due process, and the rule of law, any narrow exploration set apart from the systems it shapes—where myopically technological inquiries might fail to include broader ethical and sociological scrutiny—could be misguided and potentially harmful. Such narrow explorations might not only fail to prioritize the rights and values we hold dear but might also undermine our abilities then to govern AI and the effects it has on the social and political systems we aim to protect. As such, ideal is a forum like Law & Contemporary Problems that brings together lawyers, ethicists, technologist, engineers, and others to consider these socio-technical systems across disciplines. When seeking a positive AI future, it will take a village. Such a sprawling topic, though, also requires some constraints. As this volume’s title “Black Box Algorithms and the Rule of Law” suggests, we have imposed two constraints here. First, we focus on a particular subset of AI characterized as “black box AI.” In his article The Black Box Society: The Secret Algorithms that Control Money and Information, contributing author Frank Pasquale showed that black-box systems are those “colonized by the logic of secrecy.” His article in this volume adds that “‘black box AI’ refers to any natural language processing, machine learning, textual analysis, or similar software which uses data which are not accessible to the data subject, or which deploys algorithms which are either similarly inaccessible, or so complex that they cannot be reduced to a series of rules and rule applications comprehensible to the data subject.” In other words,",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
70a6876d2f5b001be38608182de55e7d5a75cbfd,https://www.semanticscholar.org/paper/70a6876d2f5b001be38608182de55e7d5a75cbfd,The Human-Side of Service Engineering: Advancing Technology's Impact on Service Innovation,,AHFE,2021.0,10.1007/978-3-030-80840-2_1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c17d2d329f367c18c091afa4ebe046c2b2b1ac47,https://www.semanticscholar.org/paper/c17d2d329f367c18c091afa4ebe046c2b2b1ac47,Frame Work to Classify Data in Interactive System to Enhance Decision Making,"Decision-making is a process of choosing among alternative courses of action for the purpose of attaining a goal or goals. The ultimate objective of data analytics is to ease the decision making process but this has lot of challenges and proper planning is the only way to overcome. The idea behind this research work is to propose a novel framework for data analytics to make effective decisions in an organization by aiding the various stages of the decision making mechanism. According to the design science methodology, the research has been formulated and used in the frame work design process. A novel framework was proposed that combines different aspects of data analytics, needed architectures and tools are incorporated in the various stages of decision making process. Based on the Simons the decision-making process, a new framework was designed with 4 phases namely Data, analytical, model deployment and visualization. The decisive objective of the proposed framework is to ease the process of decision making and also to take effective decision. 
In the process of future planning by the organization, it needs simple accurate estimation techniques for predictions to make effective decisions. Predictions always deal with the future events based on past incidents or records. Different kinds of predictions have been done regularly in many fields for the benefit of an individual, a group of people, an organization or a country. Support Vector Machines can be used to create a powerful prediction model because of its capability in classification and regression. The purpose of this research work is to develop a decision support system model was developed using novel algorithm. The newly developed framework has been proposed for the purpose of data analytics and for prediction.  In this work, the machine learning algorithms Support Vector Machines (SVM), Random Forest, Decision Tree, Naive Bayes and the newly proposed algorithm has been analyzed and results are compared. The outcome of this research work proves that the proposed framework model provides better result than other model. 
The objective of designing and developing the proposed framework is to ease the process of decision making in the scenario of interactive system. A student performance assessment is used to evaluate the proposed framework using real data. To test the correctness of proposed framework, an experiment was done with the student data to predict student performance using newly proposed machine learning framework. The result justifies the proposed framework for the decision making process, gives added value.",,2021.0,10.17762/TURCOMAT.V12I7.2670,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6e3e2bd6fc813e5e8a59aa0e89a254522e4324bb,https://www.semanticscholar.org/paper/6e3e2bd6fc813e5e8a59aa0e89a254522e4324bb,Recognition of Fake Currency Note using Convolutional Neural Networks,"58 Published By: Blue Eyes Intelligence Engineering & Sciences Publication Retrieval Number: E2857038519/19©BEIESP  Abstract: In this paper, the Automatic Fake Currency Recognition System (AFCRS) is designed to detect the counterfeit paper currency to check whether it is fake or original. The existing counterfeit problem due to demonetization effects the banking system and also in other fields. A new approach of Convolution Neural Network towards identification of fake currency notes through their images is examined in this paper which is comparatively better than previous image processing techniques. This method is based on Deep Learning, which has seen tremendous success in image classification tasks in recent times. This technique can help both people and machine in identifying a fake currency note in real time through an image of the same. The proposed system, AFCRS can also be deployed as an application in the smartphone which can help the society to distinguish between the fake and original currency notes. The Accuracy in the proposed system can be increased through the original fake notes, where as the proposed system contains the images from children’s bank churan label.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0661300551477d3ab320164c4f57a4bcd1b3f59e,https://www.semanticscholar.org/paper/0661300551477d3ab320164c4f57a4bcd1b3f59e,Social Media as a Catalyst for Policy Action and Social Change for Health and Well-Being: Viewpoint,"This viewpoint paper argues that policy interventions can benefit from the continued use of social media analytics, which can serve as an important complement to traditional social science data collection and analysis. Efforts to improve well-being should provide an opportunity to explore these areas more deeply, and encourage the efforts of those conducting national and local data collection on health to incorporate more of these emerging data sources. Social media remains a relatively untapped source of information to catalyze policy action and social change. However, the diversity of social media platforms and available analysis techniques provides multiple ways to offer insight for policy making and decision making. For instance, social media content can provide timely information about the impact of policy interventions. Social media location information can inform where to deploy resources or disseminate public messaging. Network analysis of social media connections can reveal underserved populations who may be disconnected from public services. Machine learning can help recognize important patterns for disease surveillance or to model population sentiment. To fully realize these potential policy uses, limitations to social media data will need to be overcome, including data reliability and validity, and potential privacy risks. Traditional data collection may not fully capture the upstream factors and systemic relationships that influence health and well-being. Policy actions and social change efforts, such as the Robert Wood Johnson Foundation’s effort to advance a culture of health, which are intended to drive change in a network of upstream health drivers, will need to incorporate a broad range of behavioral information, such as health attitudes or physical activity levels. Applying innovative techniques to emerging data has the potential to extract insight from unstructured data or fuse disparate sources of data, such as linking health attitudes that are expressed to health behaviors or broader health and well-being outcomes.",Journal of medical Internet research,2018.0,10.2196/jmir.8508,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
aa30901b02e9923851d507e1f2726bcf9ef1b068,https://www.semanticscholar.org/paper/aa30901b02e9923851d507e1f2726bcf9ef1b068,OccuSpace: Towards a Robust Occupancy Prediction System for Activity Based Workplace,"Workplace occupancy detection is becoming increasingly important in large Activity Based Work (ABW) environments as it helps building and office management understand the utilisation and potential benefits of shared workplace. However, existing sensor-based technologies detect workstation occupancy in indoor spaces require extensive installation of hardware and maintenance incurring ongoing costs. Moreover, accuracy can depend on the specific seating styles of workers since the sensors are usually placed under the table or overhead. In this research, we provide a robust system called OccuSpace to predict occupancy of different atomic zones in large ABW environments. Unlike fixed sensors, OccuSpace uses statistical features engineered from Received Signal Strength Indicator (RSSI) of Bluetooth card beacons carried by workers while they are within the ABW environment. These features are used to train state-of-the-art machine learning algorithms for prediction task. We setup the experiment by deploying our system in a realworld open office environment. The experimental results show that OccuSpace is able to achieve a high accuracy for workplace occupancy prediction.",2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),2019.0,10.1109/PERCOMW.2019.8730762,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
af7fb5305e2c6dbbc08167a4d442e7449761de5d,https://www.semanticscholar.org/paper/af7fb5305e2c6dbbc08167a4d442e7449761de5d,Computerize the Race Problem?: Why We Must Plan for a Just AI Future,"1960s civil rights and racial justice activists tried to warn us about our technological ways, but we didn't hear them talk. The so-called wizards who stayed up late ignored or dismissed black voices, calling out from street corners to pulpits, union halls to the corridors of Congress. Instead, the men who took the first giant leaps towards conceiving and building our earliest ""thinking"" and ""learning"" machines aligned themselves with industry, government and their elite science and engineering institutions. Together, they conspired to make those fighting for racial justice the problem that their new computing machines would be designed to solve. And solve that problem they did, through color-coded, automated, and algorithmically-driven indignities and inumahities that thrive to this day. But what if yesterday's technological elite had listened to those Other voices? What if they had let them into their conversations, their classrooms, their labs, boardrooms and government task forces to help determine what new tools to build, how to build them and - most importantly - how to deploy them? What might our world look like today if the advocates for racial justice had been given the chance to frame the day's most preeminent technological question for the world and ask, ""Computerize the Race Problem?"" Better yet, what might our AI-driven future look like if we ask ourselves this question today?",AIES,2020.0,10.1145/3375627.3377140,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e8ebe4d067d01bc038d045f92d35212ccfc3efdd,https://www.semanticscholar.org/paper/e8ebe4d067d01bc038d045f92d35212ccfc3efdd,"Crowdsourcing and Human‐in‐the‐Loop for
 IoT","Internet of Things (IoT) networks of sensors, mobile phones and other smart devices are providing researchers, practitioners, and end users with an unprecedented amount of data to enable new services, inform decisions and create added value. According to [36] the number of smart phone users was predicted to top three billion by the end of 2018. Other wearable devices such as watches, eyewear, and garments have become increasingly ubiquitous, with a projected 245 million units expected be sold in 2019 alone [22]. In the public sector, smart cities leverage IoT technologies to design better policies, create efficiencies, and manage growth sustainably [31]. Urban areas around the world have made substantial investments to deploy ‘smart connections’ for everything from buses to street lights to buildings, which fuel data analytics. While developers have focused on improving sensor accuracy and devising advanced methods to store, manage and analyse IoT data, public authorities soon realised that technology is just one, albeit a crucial component of their smart city strategy, which could help them achieve their wider development goals and be more responsive towards residents’ needs [35]. A smart city is hence commonly understood as a people-centric city, delivering services that matter to citizens and empowering communities and businesses to engage in decisions that will affect them. Human involvement enhances technology as well. More than a decade of development in big data and data science, experts agree that the best solutions employ a combination of machine and manual processes [9] for example, a state-of-the-art machine learning model can handle roughly 80% of a problem, while approximately 19% of cases require some form of human input, and the remaining 1% is random [4]. Augmenting technology is particularly helpful to:",,2020.0,10.1002/9781119545293.CH8,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0824e6d300ce2717916372cf459c32eadc4c37c8,https://www.semanticscholar.org/paper/0824e6d300ce2717916372cf459c32eadc4c37c8,Deploying Analytics with the Portable Format for Analytics (PFA),"We introduce a new language for deploying analytic models into products, services and operational systems called the Portable Format for Analytics (PFA). PFA is an example of what is sometimes called a model interchange format, a language for describing analytic models that is independent of specific tools, applications or systems. Model interchange formats allow one application (the model producer) to export models and another application (the model consumer or scoring engine) to import models. The core idea behind PFA is to support the safe execution of statistical functions, mathematical functions, and machine learning algorithms and their compositions within a safe execution environment. With this approach, the common analytic models used in data science can be implemented, as well as the data transformations and data aggregations required for pre- and post-processing data. PFA compliant scoring engines can be extended by adding new user defined functions described in PFA. We describe the design of PFA. A Data Mining Group (DMG) Working Group is developing the PFA standard. The current version is 0.8.1 and contains many of the commonly used statistical and machine learning models, including regression, clustering, support vector machines, neural networks, etc. We also describe two implementations of Hadrian, one in Scala and one in Python. We discuss four case studies that use PFA and Hadrian to specify analytic models, including two that are deployed in operations at client sites.",KDD,2016.0,10.1145/2939672.2939731,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f3dd7d67ac4d7d6843f326ac510d49926a7af934,https://www.semanticscholar.org/paper/f3dd7d67ac4d7d6843f326ac510d49926a7af934,Social Network Mining for Analysis of Social Phenomena,"In the last decade, a large amount of human interaction data has become available that either comes from online social networks or was captured using wearable devices. Traditional social science does not provide tools for conducting data-driven research using such data. Computational social science aims to close this gap by developing data mining and machine learning approaches for utilizing this data. Toward this end, in this thesis we explore different ways of studying social phenomena by applying data mining techniques to social network data. We use two different types of data: The first type is face-to-face interaction networks data obtained using novel wearable radio-frequency identification (RFID) technology, which represents high-resolution social networks. These networks allow researchers to study human interaction and social phenomena, such as community formation, on a micro level. The second type of data is the data available from online social networks and media. These data enable the study of social phenomena on a large scale, such as the study of changes in human mobility during disasters. It is important to consider both data types, as only in this way may we present a complete picture of the process of mining social network for analysis of social phenomena. We structure this work following the cross-industry standard process for data mining (CRISP-DM) framework. First, in CRISP-DM’s data understanding phase, we aim to analyze face-to-face interaction networks and especially the process of community formation in those networks. We also validate whether online social networks can be used as a proxy for offline activities. The CRISP-DM framework’s subsequent data preparation, modeling and evaluation phases are presented in three different types of studies. In this manner, we demonstrate the scope of what is possible in using social network mining to help us understand social phenomena and activities. The first study, conducted in collaboration with sociologists, shows how analysis of face-to-face interactions may enhance sociological studies. In the second study, which was carried out together with the United Nations Pulse Lab Jakarta, we show how forest fires are discussed in social media and how changes in the mobility of populations can be observed using online social media data. In the third study, we propose an improvement to the k-nearest neighbors classification algorithm for data with non-uniform geospatial distribution, a characteristic often observed in data from online social networks. These three studies show different possibilities when studying social phenomena and developing applications based on social network data. Finally, when discussing the deployment phase of the CRISP-DM framework, we present the Ubicon platform, which we used to collect data and to deploy some of our results. Altogether, this thesis shows different approaches for exploring social phenomena using data mining and machine learning techniques applied to social network data. The presented approaches have different potential use cases, from practical applications to more theoretical sociological work. The insights presented in this thesis are relevant for researchers from different disciplines, including computer science and sociology, interested in studying and developing methods to work with social network data.",,2019.0,10.17170/KOBRA-20190815628,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f525aa615c87fadf9eb1aebaf6058d2c05adc7eb,https://www.semanticscholar.org/paper/f525aa615c87fadf9eb1aebaf6058d2c05adc7eb,Letter from the Special Issue Editor,"Scientific computing used to be based on numerical simulations run on mid-range warehouse scale computers. This is no longer the case, due to the combination of strong application pull and technology push. In order to get realistic models of a phenomenon in natural or engineered systems, scientists must analyze unprecedented volumes of data generated by new generations of instruments and experiments. In addition, they must run simulations at higher spatial resolutions, for longer simulation times and with higher dimension models, possibly combining multiple physical models of a phenomenon, or study multiple simultaneous phenomena. These new computational challenges stemming from scientific applicatins have triggered a convergence of traditional numerical simulation with machine learning and high-performance data analytics. Put differently, data science and eScience are merging. The technology push is due to the planned transition to Exascale systems. Strictly defined, Exascale computers are capable of 1018 floating points operations per second (flops). More interestingly, they are three orders of magnitude faster than the High-Performance Computers deployed a decade ago. The first Exascale systems are expected in the coming year. In the US, three systems are being deployed: Aurora at Argonne National Lab, Frontier at Oak Ridge National Lab and El Capitan at Lawrence Livermore Lab. In China three existing pre-exascale systems are being extended: Sunway at the National Research Center of Parallel Computer Engineering and Technology (NRCPC in Wuxi, Jiangsu), Sugon (installed at the Shanghai Supercomputer Center) and Tianhe at the National Center of Defense Technology (NUDT in Changsha, Hunan). In Japan, Riken and Fujitsu have designed the Fugaku Exascale computer, which has been announced for 2021, 2022. It will be hosted at the RIKEN Center for Computational Science in Kobe. In Europe, three pre-exascale computers are under construction: Mare Nostrum 5 at the Barcelona Supercomputing Center, Leonardo at Bologna’s CINECA and LUMI at the CSC Data Center in Kaajani, Finland. In 2008, Kogge et al. surveyed the technology challenges in achieving Exascale systems. The main roadblock they identified was transporting data from one site to another: on the same chip, between closely coupled chips in a common package, or between different racks on opposite sides of a large machine room. Put differently, minimizing data movement is the key challenge on Exascale systems. This is a challenge in terms of architecture, but it is also a challenge for data management. In this issue, leading researchers from the HPC and database communities present their work on data management at Exascale. The papers will give readers an insight in the nature of the application pull and technology push sketched above. They contain the lessons learnt at the forefront of scientific data management. They are very interesting points of departure for future work. Mario Lassnig from CERN and his co-authors review their experience with the Rucio system, developed at CERN, to handle data in the ATLAS experiment. They detail the challenges they faced and how Rucio addresses them. They report on recent efforts to adapt Rucio in the context of other large-scale scientific projects. Jerome Soumagne from HDF Group and his co-authors tackle the issue of performance and resilience for data services at Exascale. They propose Remote Procedure Call as a building block for such data services. The paper describes the design of Mercury, a new form of Remote Procedure Call adapted to large data transfers on low-latency network fabrics. Jeremy Logan from Oak Ridge National Lab and his co-authors focus on ADIOS, the Adaptable I/O System, that provides a publish/subscribe abstraction for high-performance data services. The paper describe its design and its use in the context of near Exascale use cases. Based on lessons learnt and examples from a range of different projects, the authors discuss challenges and opportunities for future work on data management at Exascale. Noel Moreno Lemus from LNCC (National Lab for Scientific Computing in Rio de Janeiro, Brazeil) and his co-authors tackle the issue of large-scale spatio-temporal simulations. More specifically, they focus on answering uncertainty quantification queries over such simulation results. This is a great example of the convergence of numerical simulation and query processing.",IEEE Data Eng. Bull.,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fb20b5f5053e98c18ee8e452ad645cedbcbfe5a1,https://www.semanticscholar.org/paper/fb20b5f5053e98c18ee8e452ad645cedbcbfe5a1,Message from the Program Committee Co-Chairs,"Welcome to the 6th IEEE International Conference on Data Science and Advanced Analytics (DSAA’2019), the flagship annual meeting that spans the interdisciplinary fields of Data Science and Advanced Analytics. DSAA brings together researchers, industry and government practitioners, as well as developers and users of data science solutions. This creates a premier forum for an exchange of ideas on the latest theoretical developments in Data Science and on the best practice for a wide range of applications. DSAA focuses on the science of data science, as well as the implications of the science to industry, government, and society. On the science side, DSAA spans all the component fields of data science, including statistics, probabilistic and mathematical modeling, machine learning, data mining and knowledge discovery, complexity science, network science, business analytics, data management, infrastructure and storage, retrieval and search, security, privacy and ethics. On the applications side, DSAA highlights case studies and poses research obstacles motivated by applied work. DSAA showcases applications impacted by data science, presents tools and platforms that enable deployed data science solutions, and exposes researchers to challenges motivated by the application domains. As an alternative to the highly specialized disciplinary conferences, DSAA reflects the interdisciplinary nature of data science and analytics.",2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),2019.0,10.1109/wdfia.2008.4,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ce277faafe0f45d1bcf267025d283bdc9112df63,https://www.semanticscholar.org/paper/ce277faafe0f45d1bcf267025d283bdc9112df63,"Alternating Optimization: Constrained Problems, Adversarial Networks, and Robust Models","Title of dissertation: Alternating Optimization: Constrained Problems, Adversarial Networks, and Robust Models Zheng Xu Doctor of Philosophy, 2019 Dissertation directed by: Professor Tom Goldstein Department of Computer Science Data-driven machine learning methods have achieved impressive performance for many industrial applications and academic tasks. Machine learning methods usually have two stages: training a model from large-scale samples, and inference on new samples after the model is deployed. The training of modern models relies on solving difficult optimization problems that involve nonconvex, nondifferentiable objective functions and constraints, which is sometimes slow and often requires expertise to tune hyperparameters. While inference is much faster than training, it is often not fast enough for real-time applications. We focus on machine learning problems that can be formulated as a minimax problem in training, and study alternating optimization methods served as fast, scalable, stable and automated solvers. First, we focus on the alternating direction method of multipliers (ADMM) for constrained problem in classical convex and nonconvex optimization. Some popular machine learning applications including sparse and low-rank models, regularized linear models, total variation image processing, semidefinite programming, and consensus distributed computing. We propose adaptive ADMM (AADMM), which is a fully automated solver achieving fast practical convergence by adapting the only free parameter in ADMM. We further automate several variants of ADMM (relaxed ADMM, multi-block ADMM and consensus ADMM), and prove convergence rate guarantees that are widely applicable to variants of ADMM with changing parameters. We release the fast implementation for more than ten applications and validate the efficiency with several benchmark datasets for each application. Second, we focus on the minimax problem of generative adversarial networks (GAN). We apply prediction steps to stabilize stochastic alternating methods for the training of GANs, and demonstrate advantages of GAN-based losses for image processing tasks. We also propose GAN-based knowledge distillation methods to train small neural networks for inference acceleration, and empirically study the trade-off between acceleration and accuracy. Third, we present preliminary results on adversarial training for robust models. We study fast algorithms for the attack and defense for universal perturbations, and then explore network architectures to boost robustness. Alternating Optimization: Constrained Problems, Adversarial Networks, and Robust Models",,2019.0,10.13016/XGWP-EDLC,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f3408b6978ffc065b0c764904197b86d2d32f4cd,https://www.semanticscholar.org/paper/f3408b6978ffc065b0c764904197b86d2d32f4cd,Real-time DDoS Attack Detection Method for Programmable Device,"* This work is supported by the National Key R&D Program of China under Grant No. 2017YFB0801703, the National Natural Science Foundation of China under Grant No. 61602114, CERNET Innovation Project No. NGII20170406, and the Natural Science Foundation of Jiangsu Province (NO. BK20151416). Abstract Aiming at the problem that DDoS (Distributed Denial of Service) attacks are difficult to identify in real time with high accuracy and low energy consumption, a real-time DDoS attack detection method based on programmable device OpenBox is proposed. The method adopts a combination of software and hardware. On the hardware side, OpenBox updates the counter when forwarding packets, and supports user-defined hardware actions. It can provide the feature values required for detection at the hardware level. On the software side, it runs a hardware awareness module based on sliding window and an online detection module based on machine learning. The hardware awareness module senses the network status in real time according to the threshold. When the network is abnormal, the online detection module is launched to detect DDoS attack. A DDoS attack detection prototype system based on this method is implemented, and deployed on OpenBox. Experiments show that the method can detect DDoS attack in real time with low resource occupancy and high accuracy.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4af35db21dc1eb49d10716f84db6d6fb6a761639,https://www.semanticscholar.org/paper/4af35db21dc1eb49d10716f84db6d6fb6a761639,BUILDING AN AUTOMATIC DOOR SYSTEM USING FACE RECOGNITION,"The sustainable development goal of Tra Vinh University is the quality of training combined with science and technology to gradually bring the university reaching the world development. In order to promote the development of scientific research, it is necessary to create practical applications at Tra Vinh University and the research can be deployed to businesses and households. Currently, in contributing to the implementation of the above objectives, our team  focuses on study the deployment models of using machine learning methods in combination with supporting frameworks to create applications that can automatically open and close the door by face recognition. In this study, we have collected videos as data for face and facial gestures recognition.  ",Scientific Journal of Tra Vinh University,2019.0,10.35382/18594816.1.1.2019.81,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
26eaeead7a9c23088156335e1c420fd72d511375,https://www.semanticscholar.org/paper/26eaeead7a9c23088156335e1c420fd72d511375,On Countering Recurring Problems that Impede Equitable Development,"The motivation for my research lies in bringing social development impact through information technology. My research method is centred in development practice, where I not just build new information systems but I follow through with deploying and scaling them as well. As part of this process, I have contributed to research at multiple levels. I work at the technological level of building innovative ICT systems aligned with standard CS research streams such as computer networks, data science and machine learning, and information retrieval. I then work at the practice level of deploying and scaling these systems over many years using action research principles, and have contributed to insights in the use of information systems for governance, social and behavior change, and news media. Finally, I work at the theory level to explain observations arising from the interactions between technology and society, based on the learning gained from long-term deployment of these ICT systems. The same perspectives are reflected in my teaching and research, where I encourage students to take a dialectical approach towards their work, so as to appreciate a broader worldview of technology beyond just production of the technological artefact itself. I have recently also put together a course on ethics in applied CS, to take this view to undergraduate students as well so that they strive to ensure that responsible outcomes arise from the technology they are likely develop and manage in the near future after they enter the information technology industry.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7094888371de5f79d01d2733e8d4e689da41f2cf,https://www.semanticscholar.org/paper/7094888371de5f79d01d2733e8d4e689da41f2cf,"Scheduling, Characterization and Prediction of HPC Workloads for Distributed Computing Environments","Author(s): Naghshnejad, Mina | Advisor(s): Singhal, Mukesh | Abstract: As High Performance Computing (HPC) has grown considerably and is expected to grow even more, effective resource management for distributed computing sys- tems is motivated more than ever. As the computational workloads grow in quantity, it is becoming more crucial to apply efficient resource management and workload scheduling to use resources efficiently while keeping the computational performance reasonably good. The problem of efficiently scheduling workloads on resources while meeting performance standards is hard. Additionally, non-clairvoyance of job dimen- sions makes resource management even harder in real-world scenarios. Our research methodology investigates the scheduling problem compliant for HPC and researches the challenges for deploying the scheduling in real world-scenarios using state of the art machine learning and data science techniques.To this end, this Ph.D. dissertation makes the following core contributions: a) We perform a theoretical analysis of space-sharing, non-preemptive scheduling: we studied this scheduling problem and proposed scheduling algorithms with polyno- mial computation time. We also proved constant upper-bounds for the performance of these algorithms. b) We studied the sensitivity of scheduling algorithms to the accuracy of runtime and devised a meta-learning approach to estimate prediction accuracy for newly submitted jobs to the HPC system. c) We studied the runtime prediction problem for HPC applications. For this purpose, we studied the distri- bution of available public workloads and proposed two different solutions that can predict multi-modal distributions: switching state-space models and Mixture Density Networks. d) We studied the effectiveness of recent recurrent neural network models for CPU usage trace prediction for individual VM traces as well as aggregate CPU usage traces. In this dissertation, we explore solutions to improve the performance of scheduling workloads on distributed systems.We begin by looking at the problem from the theoretical perspective. Modeling the problem mathematically, we first propose a scheduling algorithm that finds a constant approximation of the optimal solution for the problem in polynomial time. We prove that the performance of the algorithm (average completion time is the constant approximation of the performance of the optimal scheduling. We next look at the problem in real-world scenarios. Considering High-Performance Computing (HPC) workload computing environments as the most similar real-world equivalent of our mathematical model, we explore the problem of predicting application runtime. We propose an algorithm to handle the existing uncertainties in the real world and show-case our algorithm with demonstrative effectiveness in terms of response time and resource utilization. After looking at the uncertainty problem, we focus on trying to improve the accuracy of existing prediction approaches for HPC application runtime. We propose two solutions, one based on Kalman filters and one based on deep density mixture networks. We showcase the effectiveness of our prediction approaches by comparing with previous prediction approaches in terms of prediction accuracy and impact on improving scheduling performance. In the end, we focus on predicting resource usage for individual applications during their execution. We explore the application of recurrent neural networks for predicting resource usage of applications deployed on individual virtual machines. To validate our proposed models and solutions, we performed extensive trace-driven simulation and measured the effectiveness of our approaches.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7874e8ddabfb2f10efd8712b2cc25c27360f2c01,https://www.semanticscholar.org/paper/7874e8ddabfb2f10efd8712b2cc25c27360f2c01,Essays on reporting and information processing,"The three essays collected in this PhD thesis concern internal and external reporting practices, narrative disclosures, recent advancements in reporting technologies, and the role of reporting in emerging markets. These essays utilize state-of-the-art empirical techniques drawn from computer science along with new data sources to study fundamental accounting questions. The first essay studies the relationship between reporting frequency and market pressure over social media in crowdfunding markets. The second essay studies the use of soft information in the context of internal bank lending decisions, in particular during a scenario of mandated changes to the location of decisions rights. The third essay studies the information retrieval process for narrative disclosures for users that vary in their financial literacy by combining innovative tracking techniques deployed on Amazon Mechanical Turk with state-of-the-art machine learning techniques.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0ad9b9912bfe9c19191dae430833c70f137039d4,https://www.semanticscholar.org/paper/0ad9b9912bfe9c19191dae430833c70f137039d4,Dynamic security management driven by situations: An exploratory analysis of logs for the identification of security situations,"Situation awareness consists of ""the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future"". Being aware of the security situation is then mandatory to launch proper security reactions in response to cybersecurity attacks. Security Incident and Event Management solutions are deployed within Security Operation Centers. Some vendors propose machine learning based approaches to detect intrusions by analysing networks behaviours. But cyberattacks like Wannacry and NotPetya, which shut down hundreds of thousands of computers, demonstrated that networks monitoring and surveillance solutions remain insufficient. Detecting these complex attacks (a.k.a. Advanced Persistent Threats) requires security administrators to retain a large number of logs just in case problems are detected and involve the investigation of past security events. This approach generates massive data that have to be analysed at the right time in order to detect any accidental or caused incident. In the same time, security administrators are not yet seasoned to such a task and lack the desired skills in data science. As a consequence, a large amount of data is available and still remains unexplored which leaves number of indicators of compromise under the radar. Building on the concept of situation awareness, we developed a situation-driven framework, called dynSMAUG, for dynamic security management. This approach simplifies the security management of dynamic systems and allows the specification of security policies at a high-level of abstraction (close to security requirements). This invited paper aims at exposing real security situations elicitation, coming from networks security experts, and showing the results of exploratory analysis techniques using complex event processing techniques to identify and extract security situations from a large volume of logs. The results contributed to the extension of the dynSMAUG solution.",2019 3rd Cyber Security in Networking Conference (CSNet),2019.0,10.1109/CSNet47905.2019.9108976,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
80fc0f17be6a9939e33c47f42bd8e847b5e666f0,https://www.semanticscholar.org/paper/80fc0f17be6a9939e33c47f42bd8e847b5e666f0,Intelligent Nutrition in Healthcare and Continuous Care,"In the healthcare industry, the patient's nutrition is a key factor in their treatment process. Every user has their own specific nutritional needs and requirements. An appropriate nutrition policy can therefore help the patient's recovery process and alleviate possible symptoms. Food recommender systems are platforms that offer personalised suggestions of recipes to users. However, there is a lack of usage of recipe recommendation systems in the healthcare sector. Multiple challenges in representing the domain of food and the patient's needs make it complicated to implement these systems. The present project aims to develop a platform for an intelligent planning of the user's meals, based on their clinical conditions. The application of machine learning algorithms on nutrition, in healthcare services and continuous care is thus a key topic of research. This platform will be tested and deployed at the Social Cafeteria of Vila Verde (Cantina Social da Santa Casa da Misericórdia de Vila Verde). The development of this project will use the Design Science Research (DSR) investigation methodology, ensuring that the solution to the problem accomplishes all needs and requirements of the professionals, while elucidating new knowledge both for the institution and the scientific community.",2019 International Conference in Engineering Applications (ICEA),2019.0,10.1109/CEAP.2019.8883496,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
706567d9d1b838a01ec513c922eaa49d745216a9,https://www.semanticscholar.org/paper/706567d9d1b838a01ec513c922eaa49d745216a9,Autonomous and Real-time Rock Image Classification using Convolutional Neural Networks,"Autonomous image recognition has numerous potential applications in the field of planetary science and geology. For instance, having the ability to classify images of rocks would allow geologists to have immediate feedback without having to bring back samples to the laboratory. Also, planetary rovers could classify rocks in remote places and even in other planets without needing human intervention. Shu et al. classified 9 different types of rock images using a Support Vector Machine (SVM) with the image features extracted autonomously. Through this method, the authors achieved a test accuracy of 96.71%. In this research, Convolutional Neural Networks(CNN) have been used to classify the same set of rock images. Results show that a 3-layer network obtains an average accuracy of 99.60% across 10 trials on the test set. A version of Self-taught Learning was also implemented to prove the generalizability of the features extracted by the CNN. Finally, one model has been chosen to be deployed on a mobile device to demonstrate practicality and portability. The deployed model achieves a perfect classification accuracy on the test set, while taking only 0.068 seconds to make a prediction, equivalent to about 14 frames per second.",,2021.0,10.20944/preprints202109.0285.v1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,https://www.semanticscholar.org/paper/cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,"Summary for CIFE Seed Proposals for Academic Year 2020-21 Proposal number: 2020-04 Proposal title: Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing","up to 150 words) Increasing evidence suggests built office features (e.g., lighting, materials, and ventilation) have substantial impacts on occupant wellbeing. A key next direction is field studies at industry partner sites to examine real-world workplaces. We propose to develop innovative Internet of Things (IoT) techniques that integrate data from building instrumentation, personal device sensors, and self-report interfaces and then deploy this platform in-the-wild to capture rich, longitudinal, ecologically-valid data about the status of office workers and the spaces they occupy. Insights will advance scientific knowledge of how buildings impact wellbeing as well as produce practical implications for building designers and operators. A timely component will explore how covid-19 has temporally or fundamentally changed occupant behaviors and operational decisions (e.g., physical distancing desks and ventilation settings that reduce pathogen spread). Overall, our proposed research has the potential to transform the industry’s thinking on how built environments can be designed, operated, and experienced. Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing Problem and Significance Considering that people in the U.S. spend 87% of their time in indoor spaces , we assert that 1 buildings are powerful yet underleveraged loci for promoting human wellbeing. Imagine an intelligent office that could adapt soundscape systems to manage noise in open floor plans, optimize space reservation or utilization to foster collaborations and save energy, or provide digital information displays that promote employee connectedness and physical activity. Towards actualizing our vision of such hybrid physical-digital spaces, our proposal strives to develop, apply, and evaluate novel scientific and engineering approaches that will transform the industry’s thinking around how built environments can be designed, operated, and experienced. Increasingly, hypotheses suggest that built features of indoor environments (e.g., lighting, materials, and ventilation) have substantial impacts on occupants (e.g., employee recruitment and retention, absenteeism, cognition, creativity, productivity, social interactions, physical activity and health, and psychological wellbeing). In turn, these individual outcomes also drive pivotal organizational outcomes such as product innovation, workforce diversity, employee turnover, market share, and profitability. Examples illustrate how building interventions can have huge impacts : enhancing employee exposure to daylight can save businesses ~$2,000/yr per capita 2 , better air quality can raise cognitive scores of workers by 101% 3 , and increasing indoor access to biophilic elements could recoup $23 billion considering 10% of workplace absenteeism (a $226 billion dollar problem) is attributable to architecture that inadequately connects to nature 4 . However, few of these hypotheses have been tested at scale, over time, and in real world conditions . Instead, most prior efforts are small sample, short-term correlational studies based on potentially biased and sparse self-reported data. A more rigorous, scientific, and human-centered approach to study and engineer buildings that promote wellbeing can have major implications at individual, organizational, and societal levels (see Figure 1), offering both foundational theoretical knowledge as well as practical strategies for building designers and operators. Figure 1. Relations among building features and human outcomes at various levels. Further, “smart buildings” today typically focus on basic sensing and control for energy savings, thermal comfort, and security. Connecting to CIFE’s Vision for the Future of Building Users, we argue buildings of the future can go beyond such bottom line outcomes to be more interactive and human-centered: aware of and responsive to occupants’ cognitive, mental, and physical feelings and needs, while respecting privacy and promoting positive indoor experiences . 1 Klepeis, et al., 2001; 2 Heschong & Mahone, 2003; 3 Allen et al., 2016; 4 Elzeyadi, 2011. <Landay-Billington> < Hybrid Physical-Digital Spaces> 1 Theoretical and Practical Points of Departure It is imperative to increase understanding of exactly what built attributes have what impacts and on whom, in a scalable, longitudinal, and inclusive manner. Thus through technology-driven assessment and hybrid physical-digital interventions, we aim to (a) fundamentally advance the science on how built environments impact human wellbeing and, in turn, (b) generate guidelines that can revolutionize the way spaces are designed, operated, and experienced . Our current scope focuses on office spaces and workers; though an overarching goal is for our developed approaches and insights to establish a foundation that enables future research with additional populations and environments (e.g., physicians and patients in clinical settings, students and teachers in classrooms, and traditionally marginalized shift and temporary workers). In particular, our reusable platform will help others study this wider range of buildings and occupants; and combining these approaches with emerging endeavors such as biophilic design and precision interventions provides a novel opportunity to not only more deeply investigate but also address long-running public health challenges and systemic inequities facing society. In these ways, we hope to positively impact a broad cross-section of stakeholders at individual, organizational, and institutional levels. Moreover, this project will support interdisciplinary fertilization across engineering, computing, psychology, law, and medicine . Research Methods and Work Plan Our research agenda is to support the design and operation of built facilities that augment human capabilities and wellbeing — and have a fundamental positive change on the way indoor spaces are experienced by the people that occupy them. By introducing intelligent systems capable of gathering and interpreting building and occupant data as well as delivering adaptive interventions in response, novel roles will also emerge for managing buildings and the activities that take place inside them. To achieve these goals, our research will comprise three main activities: 1. Developing an extensible and secure data collection and machine learning platform . A key aim of this research is scientifically examining how built spaces impact human wellbeing. To pursue this investigation and develop methods that enable buildings to be more aware of occupants’ states and needs, we have been developing pattern detection software that integrates data from (a) personal devices (smartphones, smartwatches, fitness trackers), (b) building instrumentation or portable environmental sensors (light levels, air quality), and (c) experience sampling interfaces that prompt occupants for subjective information through quick, validated self-report techniques. Figure 2 illustrates examples of these assessment components. This work involves addressing a number of technical challenges, such as selecting sampling rates and window sizes to maximize efficiency, developing methods for analyzing asynchronous and sparse sensor data, and developing privacy-sensitive feature engineering strategies for detecting and predicting wellbeing outcomes of interest. We also plan to package our platform as a reusable toolkit that can be applied by other researchers and building operators. This work is ongoing and a basic version will be ready by summer. Once development is complete, CIFE support would allow us to move onto the next critical phase: moving out of the lab and into the field. <Landay-Billington> < Hybrid Physical-Digital Spaces> 2 Figure 2. Platform to integrate data from personal devices, building sensors, and subjective self-report. 2. Deploying the platform through a mixed-method study with industry partners . The next step in our research is to deploy this platform at field sites in partnership with View, Inc. (specifically, at TIAA offices in Manhattan, this summer/fall) to capture rich, longitudinal, ecologically-valid data about behavioral, psychological, and physiological states of occupants and their everyday work environments. Our plan is to recruit a sample of approximately 150 employees for a period of 18 weeks, which will involve a baseline phase followed by systematic variation of built features (Views/No Views, Plants/No Plants, and Diversity/No Diversity in artwork) and measurement of indicators hypothesized to promote both personal wellbeing and organizational performance, based on the literature and our formative online and lab studies, described below. In combination with the engineering-focused activities to implement and install the platform, deployment will occur in tandem with ethnographic work (e.g., observations, interviews, and surveys) to manually validate reliability of the system’s automated inferences as well as gain a more qualitative portrait of occupant experiences in various spaces. Privacy-centric engagements will additionally investigate stakeholders’ attitudes regarding the capture of various types of information to derive implications about informed consent and personal data management. Along similar lines, it will be critical to responsibly manage captured data, especially potentially sensitive and exploitable data about wellness or performance. Therefore all studies will be conducted with oversight and approval from the Stanford Institutional Review Board (IRB). In addition to obtaining participants’ informed consent, we will also design sensor and data collection mechanisms to use an opt-in model, including partial participation. Our data management systems can also allow individuals to view and delete their personal data, including if purging is desired in the event of study withdrawal. Our research team has exp",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f44295dd83849f4f75cf6068bddd0bbbcbcd04b5,https://www.semanticscholar.org/paper/f44295dd83849f4f75cf6068bddd0bbbcbcd04b5,Development of a Computational and Data-Enabled Science and Engineering Ph.D. Program,"The previous two decades have seen the successful deployment of Computational Science programs in universities across the globe. These programs are aimed at training scientists and engineers to tackle problems requiring interdisciplinary approaches to finding solutions to scientific and engineering problems and the development of new computing, as exemplified by the co-design approach to exascale architectures and applications. Thus, the programs emphasize preparation in applied mathematics, numerical analysis, and scientific computing in addition to science and engineering work relevant to the target application. The rise of so-called ""Big-Data"" applications and the use of large data in business decision support and even in computational science workflows like uncertainty analysis are driving a need for training in data sciences. This paper makes the argument that, rather than treating topics in machine learning, statistics, etc. as stand-alone fields of study that students learn as electives, data-science should be an integral part of interdisciplinary training for future researchers. This approach is at the core of the newly developed Computational and Data-Enabled Science and Engineering (CDSE) Ph.D. program at the University of Buffalo. This paper describes the development of the Ph.D. program, the target student audience, and strategies for effectively executing the proposed curriculum.",2014 Workshop on Education for High Performance Computing,2014.0,10.1109/EduHPC.2014.8,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8c75a9d15d77b3d2119e655d9d83f00825572cc7,https://www.semanticscholar.org/paper/8c75a9d15d77b3d2119e655d9d83f00825572cc7,Machine Cognition Models: EPAM and GPS,"Through history, the human being tried to relay its daily tasks to other creatures, which was the main reason behind the rise of civilizations. It started with deploying animals to automate tasks in the field of agriculture(bulls), transportation (e.g. horses and donkeys), and even communication (pigeons). Millenniums after, come the Golden age with ""Al-jazari"" and other Muslim inventors, which were the pioneers of automation, this has given birth to industrial revolution in Europe, centuries after. At the end of the nineteenth century, a new era was to begin, the computational era, the most advanced technological and scientific development that is driving the mankind and the reason behind all the evolutions of science; such as medicine, communication, education, and physics. At this edge of technology engineers and scientists are trying to model a machine that behaves the same as they do, which pushed us to think about designing and implementing ""Things that-Thinks"", then artificial intelligence was. In this work we will cover each of the major discoveries and studies in the field of machine cognition, which are the ""Elementary Perceiver and Memorizer""(EPAM) and ""The General Problem Solver""(GPS). The First one focus mainly on implementing the human-verbal learning behavior, while the second one tries to model an architecture that is able to solve problems generally (e.g. theorem proving, chess playing, and arithmetic). We will cover the major goals and the main ideas of each model, as well as comparing their strengths and weaknesses, and finally giving their fields of applications. And Finally, we will suggest a real life implementation of a cognitive machine.",ArXiv,2012.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
df00993eaefe8b3a24c979a18b1a227106513fc3,https://www.semanticscholar.org/paper/df00993eaefe8b3a24c979a18b1a227106513fc3,AI for Materials Science: Tuning Laser-Induced Graphene Production and Beyond,"AI has advanced the state of the art in many application domains, including ones not ordinarily associated with computer science. We present an application of automated parameter tuning to materials science, in particular, we use surrogate models for automated parameter tuning to optimize the fabrication of laser-induced graphene. This process allows to create microscopic conductive lines in thin layers of insulating material, enabling the development of next-generation nano-circuits. Optimizing the parameters that control the laser irradiation process is crucial to creating high-quality graphene that is suitable for this purpose. Through the application of state-of-the-art parameter tuning techniques, we are able to achieve improvements of up to a factor of two compared to existing approaches in the literature and to what human experts are able to achieve. Our results are reproducible across different experimental specimen and the deployed application can be used by domain scientists without a background in AI or machine learning.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
11000db051b912a92eb2d31aad5a4d00b456819f,https://www.semanticscholar.org/paper/11000db051b912a92eb2d31aad5a4d00b456819f,"Exploration of Knowledge Engineering Paradigms for Smart Education: Techniques, Tools, Benefits and Challenges","Knowledge engineering paradigms (KEPs) deal with the development of intelligent systems in which reasoning and knowledge play pivotal role. Recently, KEPs receive increasing attention within the fields of smart education. Researchers have been used the knowledge engineering (KE) techniques, approaches and methodologies to develop a smart tutoring systems (STSs). The main characteristics of such systems are the ability of reasoning, inference and based on static and heuristic knowledge. On the other side, the convergence of artificial intelligence (AI), web science (WS) and data science (DS) is enabling the creation of a new generation of web-based smart systems for all educational and learning tasks. This paper discusses the KEPs techniques and tools for developing the smart educational and learning systems. Four most popular paradigms are discussed and analyzed namely; case-based reasoning, ontological engineering, data mining and intelligent agents. The main objective of this study is to determine and exploration the benefits and advantages of such computational paradigms to increase the effectiveness and enhancing the efficiency of the smart tutoring systems. Moreover, the paper addresses the challenges faced by the application developers and knowledge engineers in developing and deploying such systems. In addition to institutional and organizational aspects of smart educational technologies development and application. Key-Words: Knowledge engineering and management, Artificial intelligence in education, Smart tutoring systems, Computational intelligence, Machine learning Received: July 27, 2019. Revised: December 2, 2019. Accepted: January 13, 2020. Published: January 27, 2020.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f3829f50a1e34db72ad6eb4adefa82897498a3e3,https://www.semanticscholar.org/paper/f3829f50a1e34db72ad6eb4adefa82897498a3e3,"Big Data Driven Clinical Informatics & Surveillance (BDD_CIS) – A Multimodal Database Focused Clinical, Community, and Multi-Omics Surveillance Plan for COVID-19: A study Protocol (Preprint)","
 BACKGROUND
 The Coronavirus Disease 2019 (COVID-19) caused by the severe acute respiratory syndrome coronavirus (SARS-CoV-2) remains a serious global pandemic. Currently, all age groups are at risk for infection but the elderly and persons with underlying health conditions are at higher risk of severe complications. In the United States (US), the pandemic curve is rapidly changing with over 6,786,352 cases and 199,024 deaths reported. South Carolina (SC) as of 9/21/2020 reported 138,624 cases and 3,212 deaths across the state.
 
 
 OBJECTIVE
 The growing availability of COVID-19 data provides a basis for deploying Big Data science to leverage multitudinal and multimodal data sources for incremental learning. Doing this requires the acquisition and collation of multiple data sources at the individual and county level.
 
 
 METHODS
 The population for the comprehensive database comes from statewide COVID-19 testing surveillance data (March 2020- till present) for all SC COVID-19 patients (N≈140,000). This project will 1) connect multiple partner data sources for prediction and intelligence gathering, 2) build a REDCap database that links de-identified multitudinal and multimodal data sources useful for machine learning and deep learning algorithms to enable further studies. Additional data will include hospital based COVID-19 patient registries, Health Sciences South Carolina (HSSC) data, data from the office of Revenue and Fiscal Affairs (RFA), and Area Health Resource Files (AHRF).
 
 
 RESULTS
 The project was funded as of June 2020 by the National Institutes for Health.
 
 
 CONCLUSIONS
 The development of such a linked and integrated database will allow for the identification of important predictors of short- and long-term clinical outcomes for SC COVID-19 patients using data science.
",,2020.0,10.2196/preprints.24504,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d82ec7a4f9e057d400272c0a7840b842bf2e71e1,https://www.semanticscholar.org/paper/d82ec7a4f9e057d400272c0a7840b842bf2e71e1,DeepImageJ: Bridging Deep Learning to ImageJ,"Abstracts are to be limited to a minimum of 100 words and a maximum of 1000 words including spaces. ) In the last decade, the use of Deep Learning (DL) methodologies has made a vast improvement in the solution of several bioimage analysis tasks such as denoising, super-resolution, segmentation, detection, tracking, response prediction, or computer-aided diagnosis [2]. These techniques support automatic image processing workflows and have demonstrated potential to surpass human-level performance in common tasks. Consequently, they have a profound impact on the way life-science researchers conduct their bioimage data analysis [1]. Nonetheless, the integration of this breakthrough technology into life-science research pipelines remains still a challenge for the scientific community. Training and evaluating DL models requires previous programming expertise and technical knowledge about Machine Learning. Therefore, the transfer of this technology to the daily practice of life sciences researchers remains a bottleneck. Aware of the latter situation, there are already some pioneer works that target the very need of making DL solutions accessible through user-friendly software [3-7]. Moreover, there is an increasing interest in the bioimage analysis community to teach and learn some general knowledge about DL and democratize it. Figure 1: DeepImageJ bridges the gap between Deep Learning (DL) model developers and bioimage analysts. The model developers train their DL models for bioimage processing tasks and export them as bundled models. The format of the bundled model allows uploading it to the BioImage Model Zoo. ImageJ/Fiji users can download any deepImageJ compatible DL model at the BioImage Model Zoo and use it for their image processing tasks. DeepImageJ Run, the ImageJ/Fiji plugin, guides the user in a zero-code fashion through the DL bioimage processing workflow: image pre-processing, prediction, and post-processing are employed as specified by the model developer. The image pre-and post-processing routines are written in ImageJ macros and the model inference, the main functionality of DeepImageJ Run, is programmed in Java. However, the user will process the image with a single click. We present deepImageJ [8], a user-friendly plugin of ImageJ/Fiji [9, 10] to run trained DL models in a one-click. This is regardless of the DL models architecture or the task for which they were trained. DeepImageJ is designed as a standard ImageJ plugin to deploy DL models through a user-friendly interface. The deepImageJ bundled model format is subject to the specifications defined in the BioImage Model Zoo (https://bioimage.io/) which seeks to define DL models in a standard manner, and hence, contribute to the democratization of DL in the bioimage analysis. This format gathers all the technical information provided by the model developer in terms of image format, shape, pre-and post-processing steps, or tiling strategies. Moreover, it brings a plain description of the task that the model was trained for and an example image to test its performance. The deepImageJ interface renders all this information in a user-friendly fashion so it can guide the user through the application of trained DL models, Figure 1. In most cases, the proper use of DL techniques relies on image pre-and post-processing steps. Indeed, the potential of trained DL models is often exploited by additional steps such as in StarDist [5], SplineDist [11], DeepSTORM [12], or DeepWater [13] methods. In deepImageJ, the pre-and post-processing can be written in ImageJ macros and Java. Furthermore, deepImageJ is macro recordable. Hence it can be directly connected to any of the standard and sophisticated bioimage analysis methods available in the ImageJ/Fiji ecosystem, which allows the integration of DL methodologies into more complex bioimage analysis pipelines. A straightforward example of the latter is the recent integration of DeepSTORM [12] for full stacks, DeepWater [13], or Usiigaci [14]. Thanks to the collaboration with the BioImage Model Zoo, life-scientists have access to a wide range of trained models which can be also fine-tuned with existing solutions such as ZeroCostDL4Mic [3] or ImJoy [7], and then, used locally with deepImageJ. Noteworthiness, deepImageJ has been developed to import TensorFlow (Keras) and PyTorch models, covering the most extended DL environments among bioimage processing developers. To the best of our knowledge, deepImageJ is the only existing solution for the use of generic DL models. Hence, it has the potential to make available many of the powerful algorithms for microscopy image processing that are continuously being developed and published, enhancing research. Overall, we think that deepImageJ fosters a better integration and full exploitation of DL models.",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
26f135f5958761d92f8fc8f871230704fc17cd42,https://www.semanticscholar.org/paper/26f135f5958761d92f8fc8f871230704fc17cd42,Super-Scalable Computation Framework for Automated Terrain Identification,"Automated labeling of very large-scale and high-resolution remote sensing imagery is an important application in machine learning and data science. We develop a parallel computation framework to accomplish pixel-level classification by fully utilizing the available resources. An effective superpixel tessellation is employed to group pixels into homogeneous regions and derive discriminating multi-pixel features. We deploy this segmentation method on multiple computing nodes instead of a single machine, thereby creating an efficient hierarchical representation for each image while drastically reducing performance bottlenecks. In the experiments, we further demonstrate that the identification accuracy of this approach is merely affected in a small range relative to that of a single machine, while the overall running time and handling capacity are significantly improved by using our super-scalable scheme.","2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)",2017.0,10.1109/DASC-PICom-DataCom-CyberSciTec.2017.182,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0848611eddf7653cad91081764fe5404c7705661,https://www.semanticscholar.org/paper/0848611eddf7653cad91081764fe5404c7705661,Soft computing applications in the field of industrial and environmental enterprises,"Artificial Intelligence (AI) has proved to be a promising field of study that can greatly contribute to the development of computational systems that successfully address an increasing number of challenges. From the very beginning, when the term was coined in 1956 during the Dartmouth Summer Research Project, researchers from this field have shared a vision that computers can be made to perform intelligent tasks (Moor, 2006). A vast array of different techniques has been proposed so far to implement such intelligent systems, ranging from expert or knowledge‐based systems (some of the earlier products of AI) to cutting‐edge proposals such us deep and machine learning. Among all the branches of the AI tree, in last decades, there has been significant progress in soft computing (Karray & De Silva, 2004). As pointed out by Zadeh (1994), while hard computing focuses on precision, certainty, and rigour, soft computing requires that computation, reasoning, and decision making exploit a tolerance for imprecision and uncertainty wherever possible (Zadeh, 1994). Soft computing can be seen as a family consisting of many members, including evolutionary computation, fuzzy logic, and connectionist models among others (Pratihar, 2007). Its fascinating ability to simulate human intelligence has made it favourable in various scientific and technological disciplines (Chakraverty, Sahoo, & Mahato, 2019) such as computer science, mathematics, control theory, structural engineering, medical, and psychology. One of the domains where soft computing can arguably be of greatest utility is in the field of business. The current environment for businesses is increasingly complex and demanding. Companies are affected by multiple internal and external variables that have an impact on the various areas that make up the value chain, and this is exacerbated for those managing operations in multiple countries (i.e., exports, imports, and foreign direct investments). Decision takers within the companies can significantly benefit from and demand tools to analyse and extract the most knowledge from the large datasets generated by the interrelated activities that companies conduct. Soft computing can be of particular value for industrial and environmental companies. Compared to other sectors such as retailing, where individual‐level determinants are key to understand the buyer, industrial companies are characterized by a more critical role of operations efficiency in order to succeed in the market. Likewise, given the large volume of information that is now available for environmental firms, competition in this field is also more and more determined by a strategic and sophisticated use of data analytics. Overall, soft computing represents a potentially key source of competitive advantage sustainable over time that organizations, especially in the industrial and environmental field, can employ to outperform competitors, achieve higher returns, and ensure their own viability (Wernerfelt, 1984; Barney, 1991). By complementing broad analytical tools (such as Porter's five forces PESTLE or Blue Ocean; Porter, 1979; Kim & Mauborgne, 2014), soft computing may allow firms to materialize tangible resources from strategic advantages. This special issue compiles recent applications of soft computing techniques to the management of enterprise within industrial and environmental sectors. It is aimed at both researchers and practitioners from academia and industry who are engaged in deploying soft computing solutions for real‐life enterprise problems. Five papers are included in this special issue, covering a wide variety of case studies, ranging from photovoltaic solar systems to forest biomass estimation. For each different soft computing, techniques have been applied, namely, unsupervised neural networks, deep learning, regression models, multiagent systems, and case‐based reasoning. Thanks to the wide‐range panoramic view presented by these complementary works, readers can get a feel for developing up‐to‐date soft computing systems to solve present problems in enterprise contexts.",Expert Syst. J. Knowl. Eng.,2019.0,10.1111/exsy.12456,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
03ac8180391e6c8f064dad559e5e4c2350af827b,https://www.semanticscholar.org/paper/03ac8180391e6c8f064dad559e5e4c2350af827b,Staged deployment of interactive multi-application HPC workflows,"Running scientific workflows on a supercomputer can be a daunting task for a scientific domain specialist. Workflow management solutions (WMS) are a standard method for reducing the complexity of application deployment on high performance computing (HPC) infrastructure. We introduce the design for a middleware system that extends and combines the functionality from existing solutions in order to create a high-level, staged usercentric operation/deployment model. This design addresses the requirements of several use cases in the life sciences, with a focus on neuroscience. In this manuscript we focus on two use cases: 1) three coupled neuronal simulators (for three different space/time scales) with in-transit visualization and 2) a closed-loop workflow optimized by machine learning, coupling a robot with a neural network simulation. We provide a detailed overview of the application-integrated monitoring in relationship with the HPC job. We present here a novel usage model for large scale interactive multi-application workflows running on HPC systems which aims at reducing the complexity of deployment and execution, thus enabling new science.",2019 International Conference on High Performance Computing & Simulation (HPCS),2019.0,10.1109/HPCS48598.2019.9188104,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ce593a72c7c710e084e3111ca5a44da7fbc4083e,https://www.semanticscholar.org/paper/ce593a72c7c710e084e3111ca5a44da7fbc4083e,C# 8.0 and .NET Core 3.0 - Modern Cross-Platform Development,"Learn the fundamentals, practical applications, and latest features of C# 8.0 and .NET Core 3.0 from expert teacher Mark J. Price.
Key Features
Build modern, cross-platform applications with .NET Core 3.0

Get up to speed with C#, and up to date with all the latest features of C# 8.0

Start creating professional web applications with ASP.NET Core 3.0
Book Description
In C# 8.0 and .NET Core 3.0 – Modern Cross-Platform Development, Fourth Edition, expert teacher Mark J. Price gives you everything you need to start programming C# applications.


This latest edition uses the popular Visual Studio Code editor to work across all major operating systems. It is fully updated and expanded with new chapters on Content Management Systems (CMS) and machine learning with ML.NET.


The book covers all the topics you need. Part 1 teaches the fundamentals of C#, including object-oriented programming, and new C# 8.0 features such as nullable reference types, simplified switch pattern matching, and default interface methods. Part 2 covers the .NET Standard APIs, such as managing and querying data, monitoring and improving performance, working with the filesystem, async streams, serialization, and encryption. Part 3 provides examples of cross-platform applications you can build and deploy, such as web apps using ASP.NET Core or mobile apps using Xamarin.Forms. The book introduces three technologies for building Windows desktop applications including Windows Forms, Windows Presentation Foundation (WPF), and Universal Windows Platform (UWP) apps, as well as web applications, web services, and mobile apps.
What you will learn
Build cross-platform applications for Windows, macOS, Linux, iOS, and Android

Explore application development with C# 8.0 and .NET Core 3.0

Explore ASP.NET Core 3.0 and create professional web applications

Learn object-oriented programming and C# multitasking

Query and manipulate data using LINQ

Use Entity Framework Core and work with relational databases

Discover Windows app development using the Universal Windows Platform and XAML

Build mobile applications for iOS and Android using Xamarin.Forms
Who this book is for
Readers with some prior programming experience or with a science, technology, engineering, or mathematics (STEM) background, who want to gain a solid foundation with C# 8.0 and .NET Core 3.0.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c4bd40a5196486d03c4706f7ecd61234253c4f87,https://www.semanticscholar.org/paper/c4bd40a5196486d03c4706f7ecd61234253c4f87,International Journal of Advance Research in Computer Science and Management Studies,"Security in communication has become a major concern. A high level of security is required in the area of wireless sensor networks. The field of network security faces many challenges i.e. the ability to identify and prevent attacks on the network. Wireless sensor networks (WSN) consist of sensor nodes deployed in a manner to collect information about the surrounding environment. Due to their distributed nature, multi hop data forwarding and open wireless medium are the factors that make wireless sensor networks highly vulnerable to security attacks at various levels. An effective intrusion detection system can play an important role in identifying and preventing attacks which is needed to ensure the network against security breaches. Intrusion detection systems include pattern analysis techniques to discover useful patterns of system features. The derived patterns comprise inputs of classification systems, which are based on statistical and machine learning techniques. Clustering methods are used to detect unknown attacks. Elimination of insignificant features is essential for simplified, faster and more accurate detection of attacks. We present a conceptual framework for identifying attacks for intrusion detection by applying genetic k-means algorithm.",,2014.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7c50e55ef8c9f6ec04e48f0f6e75e3d08a40fa11,https://www.semanticscholar.org/paper/7c50e55ef8c9f6ec04e48f0f6e75e3d08a40fa11,PerAwareCity 2021: 6th IEEE International Workshop on Pervasive Context-Aware Smart Cities and Intelligent Transport System — Welcome and Committees,"This workshop focuses on bringing together researchers and practitioners working on systems and applications in the Smart City domain. Since nearly 60% of the world population lives in urban areas, the majority of the planet's resources are consumed in cities. As a consequence, applications targeting urban areas can positively impact a significant fraction of the population. Context-aware applications in the smart city range from transportation, energy, safety to low-carbon living. The ongoing advances in Pervasive Computing and Wireless Communication technology have delivered interesting hardware platforms that are suitable for city-scale infrastructure deployments and networking. Also, novel techniques using Artificial Intelligence (AI) and Machine Learning (ML) are now available for intelligent (real time) analytics and decision making by processing streaming sensor data collected from the surrounding environment. Fleets of smart cars can be distributed computational and sensing resource along with increasingly large crowds of people equipped with sensor-rich mobile and wearable devices, which can facilitate wide-scale crowd-sensing and crowd-sourcing. We are pleased to notify that this year we have accepted 6 highly exciting papers to be presented at the workshop. The accepted papers shall have significant contribution in city-scale smart system development. We are also looking forward to the keynote speech by Prof. Hirozumi Yamaguchi from the Mobile Computing Laboratory of the Graduate School of Information Science and Technology at Osaka University, Japan. This year, we plan to introduce an open discussion forum for identifying novel issues and challenges associated with Pervasive Context-Aware Smart Cities and Intelligent Transport Systems. The authors of the accepted papers have addressed various challenges to enable pervasive context-aware smart cities. Hyper-local Urban Contextual Awareness through Open Data Integration and provide an overview of a large-scale urban data collection in New York City. In their paper, et introduced BITS, a Blockchain based Intelligent Transportation System with Outlier Detection for Smart City. The system reveals malicious behavior in perfectly fits in the scope of the workshop. Dr. Srivastava et al. will present their approach to Building an Open, Multi-Sensor, Dataset of Water Pollution of Ganga Basin and Application to Assess Impact of Large Religious Gatherings. This contribution does not only contain exciting contributions but also sheds light on problems associated with increasing population density in urban areas. Dr. Montori et al. have worked on an approach to deliver IoT Smart Services through Collective Awareness, Mobile Crowdsensing and Open Data. The authors strive to use existing data and make it available to a larger group. They also implemented measures to allow data owners to easily share their data. Finally, Mr. Übelmesser et al. have compared Smart Cities Concepts to better understand stakeholders, driving forces, and goals during the digitalization process to smart cities. The paper also shows a path towards a unified smart energy city model for a sustainable transformation. We welcome every attendee to the workshop and urge you to actively participate in all the networking and discussions for advancing smart city research and development.",2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),2019.0,10.1109/percomw.2019.8730579,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ffa495659e271d3c4c4fddda50d42caa86be6b23,https://www.semanticscholar.org/paper/ffa495659e271d3c4c4fddda50d42caa86be6b23,"Astro 2020 Activity , Project or State of the Profession ( APC ) White Paper The Breakthrough","The discovery of the ubiquity of habitable extrasolar planets, combined with revolutionary advances in instrumentation and observational capabilities, have ushered in a renaissance in the millenia-old quest to answer our most profound question about the universe and our place within it − Are we alone? The Breakthrough Listen Initiative, announced in July 2015 as a 10-year 100M USD program, is the most comprehensive effort in history to quantify the distribution of advanced, technologically capable life in the universe. In this white paper, we outline the status of the on-going observing campaign with our primary observing facilities, as well as planned activities with these instruments over the next few years. We also list collaborative facilities which will conduct searches for technosignatures in either primary observing mode, or commensally. We highlight some of the novel analysis techniques we are bringing to bear on multi-petabyte data sets, including machine learning tools we are deploying to search for a broader range of technosignatures than was previously possible. 1 Key Science Goals and Objectives The search for life beyond Earth seeks to answer one of the most profound questions humans can ask about our place in the universe − Are we alone? Recent discoveries of thousands of exoplanets, including many Earth-like planets (Howell et al., 2014; Dressing & Charbonneau, 2015), point towards abundant targets of potential interest. Experiments to scan biospheres of these exo-worlds are already in the design phase and will likely start operating in the next decade (Schwieterman et al., 2018). It is possible that some fraction of these planets host life sufficiently advanced to be capable of communicating using electromagnetic waves. The Breakthrough Listen Initiative (hereafter, BL) is a US$100M 10-year effort to conduct the most sensitive, comprehensive, and intensive search for such advanced life on other worlds (Worden et al., 2017; Isaacson et al., 2017). BL currently has dedicated time on three telescopes: the Green Bank Telescope (GBT; MacMahon et al., 2017) and the Parkes Observatory (Price et al., 2018) in the radio, and Automated Planet Finder (APF; Lipman et al., 2018) in the optical. Additionally, commensal observations will soon begin in the radio at the MeerKAT radio telescope in South Africa. Technosignature searches take place in a large, multidimensional parameter space. As the luminosity function of the putative ETI transmitter is not known, deep, long observations from single pixel receivers must be complemented by wide-field surveys conducted across a significant portion of the electromagnetic spectrum. Other unknown signal characteristics include strength, intermittency, polarization, and modulation types (Tarter, 2003; Wright et al., 2018). Narrow-band (∼ Hz) radio signals are one of the most common signal types searched for in radio SETI (Drake, 1961; Verschuur, 1973; Tarter et al., 1980; Horowitz et al., 1986; Horowitz & Sagan, 1993; Oliver & Billingham, 1971; Siemion et al., 2013). Such signals are ubiquitous features of early terrestrial communication systems, transit the interstellar medium easily, are readily distinguished from natural astrophysical sources and can be produced with relatively low energy (Cocconi & Morrison, 1959). The BL team has also developed a software package to search for these narrow-band signals, turboSETI(Enriquez et al., 2017). Broad-band turboSETI:https://github.com/UCBerkeleySETI/turbo_seti",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6341953b36036a25c77e2b180f73960c1b3b9737,https://www.semanticscholar.org/paper/6341953b36036a25c77e2b180f73960c1b3b9737,One Ultimate Journey? AKA the Huxley’s Method: Perspectives of (Ab)Users of Hallucinogens and Entheogens on Having Planned Pre-Mortem Psychedelic Trip,"Background: The surface web is a rich source of extensive data on populations of users and misusers of psychoactive substances including substances known as hallucinogens and entheogens. The internet and its social media websites can serve as a database upon which several hypotheses are applicable via thematic analytic and psychoanalytic studies. Materials and Methods: This study will deploy the use of an internet snapshot by inspecting, via thematic analysis, the comments of a population of psychedelic (ab)users existing on the Facebook social platform. The snapshot will dare to answer an existing question in connection with the concept of using psychedelics and entheogens during the moments preceding death. Several demographics will be explored including ethnic-national and socio-cultural parameters to test several hypotheses about the tendencies for having an ultimate pre-mortem psychedelic trip towards the ambiguous afterlife. Results: Most of the psychedelic users recommended the use of DMT for the final journey. Others have suggested tripping on LSD, Psilocybin and Psilocybin mushrooms, NBOMe compounds, and even opiates. Based on inferential models, it seems that tendencies for the pre-mortem trip are not affected by the status of social relations, ethnicity, nationality, age, or sex. However, it appears to be based on the individualistic build-up. Religious affiliations and other cultural norms represent potential confounding variables. Hence, these must be explored in subsequent studies. Conclusion: In the future and to keep in pace with the logarithmic growth and arachnoid expansion of the web and its appendages, ambitious studies has to deploy the use of concepts of automation in data science via the exploitation of principles of machine learning and deep thinking. The aim is to achieve statistical inference in real-time and accurate predictions when it comes to running analytics on big data. If successfully applied, the benefits for the public health should be monumental.",Modern Applied Science,2019.0,10.5539/mas.v13n3p13,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d5af86aa820f994ccd5dce7fed8898a65ba87441,https://www.semanticscholar.org/paper/d5af86aa820f994ccd5dce7fed8898a65ba87441,Robot Navigation in Distorted Magnetic Fields,"This thesis investigates the utilization of magnetic field distortions for the localization and navigation of robotic systems. The work comprehensively illuminates the various aspects that are relevant in this context. Among other things, the characteristics of magnetic field environments are assessed and examined for their usability for robot navigation in various typical mobile robot deployment scenarios. A strong focus of this work lies on the self-induced static and dynamic magnetic field distortions of complex kinematic robots, which could hinder the use of magnetic fields because of their interference with the ambient magnetic field. In addition to the examination of typical distortions in robots of different classes, solutions for compensation and concrete tools are developed both in hardware (distributed magnetometer sensor systems) and in software. In this context, machine learning approaches for learning static and dynamic system distortions are explored and contrasted with classical methods for calibrating magnetic field sensors. In order to extend probabilistic state estimation methods towards the localization in magnetic fields, a measurement model based on Mises-Fisher distributions is developed in this thesis. Finally, the approaches of this work are evaluated in practice inside and outside the laboratory in different environments and domains (e.g. office, subsea, desert, etc.) with different types of robot systems. Zusammenfassung Diese Arbeit beschäftigt sich mit der Nutzbarmachung der Verzerrungen von Magnetfeldern für die Lokalisierung und Navigation von robotischen Systemen. Die Arbeit beleuchtet dabei umfassend die verschiedenen Aspekte, die hierbei relevant werden können. Unter anderem werden die Charakteristiken von Magnetfeldumgebungen in verschiedenen Szenarien untersucht, in denen Roboter typischerweise zum Einsatz kommen und auf ihre Nutzbarkeit für die Navigation hin untersucht. Einen großen Teil nimmt weiterhin die Untersuchung der selbstinduzierten statischen wie dynamischen Magnetfeldverzerrungen von komplexen kinematischen Robotern ein, die der Nutzung von Magnetfeldern entgegenstehen könnten. Hierzu werden im Rahmen der Arbeit neben der Ermittlung von typischen Verzerrungen Roboter verschiedener Klassen auch Lösungsansätze zur Kompensation und konkrete Werkzeuge sowohl in Hardware (verteilte MagnetometersensorikSysteme) als auch in Software entwickelt. Dabei werden unter anderem Ansätze des maschinellen Lernens zur Erfassung der statischen und dynamischen Verzerrungen verfolgt und klassischen Methoden zur Kalibrierung von Magnetfeldsensoren gegenübergestellt. Um die Zustandsschätzung mittels probabilistischer Methoden um die Möglichkeiten der Lokalisierung in Magnetfeldern zu erweitern, wird darüber hinaus in dieser Arbeit ein auf von Mises-Fisher-Verteilungen basierendes Messmodell entwickelt. Abschließend werden die Ansätze dieser Arbeit im konkreten Einsatz innerhalb und außerhalb des Labors in unterschiedlichen Umgebungen und Domänen (u.a. Unterwasser, Wüste) mit verschiedenen Arten von Robotersystemen evaluiert. Acknowledgements First and foremost, I would like to thank my colleagues who shared the very productive and stimulating PhD student retreats over the last years with me. The discussions and the suggestions given there were invaluable. In particular, I would like to thank my supervisor Prof. Dr. Frank Kirchner, who has always encouraged me to follow the not so well-trodden path of using distorted magnetic fields for localization purposes. I am very grateful for the opportunities that the German Research Center for Artificial Intelligence Robotics Innovation Center offered me with its unique collection of different types of robots and infrastructure, that I was fortunate to be able to use during the course of my work. Without this large variety of robots at the institute and the support of their system administrators, this thesis wouldn’t have been possible. Furthermore, I would like to thank my former team leaders Dr. Jan Albiez, Dr. Jakob Schwendner and Dr. Stefan Stiene, who not only backed me up in my daily work in order to pursue my goal, but also gave me valuable advice for my doctoral thesis due to their long experience in the field of robotics science. I would also like to thank Dr. Hendrik Müller from the Federal Institute for Geosciences and Natural Ressources (BGR), who, at the very beginning of my work, provided valuable insights into the world of magnetism and magnetometers from the perspective of a physicist. The extensive field trials campaign in the Mars-like environment of the desert of Utah (US) was a particularly one-of-a-kind experience that pushed me forward in my work. In this context I would like to say a special thank you to the whole field trials Utah team. I wish to thank Dr. Johannes Lemburg, Martin Fritsche and Christopher Gaudig, who always provided thematic as well as moral support when I felt a little bit lost in my endeavor. Special thanks go to Dr. Florian Cordes, who took it upon him to read through the entire thesis and mercilessly pointed out to me not only mistakes in sentence construction. Finally and above all, I would like to thank my family. This work is dedicated to them.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
20f13cfee92ad88820bec6741a5845815a6729be,https://www.semanticscholar.org/paper/20f13cfee92ad88820bec6741a5845815a6729be,Can We Stop the Academic AI Brain Drain?,,KI - Künstliche Intelligenz,2019.0,10.1007/s13218-019-00577-2,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c8373eb721d292f33176c73e813c32eac97ecb5c,https://www.semanticscholar.org/paper/c8373eb721d292f33176c73e813c32eac97ecb5c,Conversational Artificial Intelligence Development Tools for K-12 Education,"Artificial intelligence (AI) education is becoming increasingly important as the adoption of AI technology, including conversational agents such as Amazon Alexa, Siri, and the Google Assistant, increases. Current educational and programming tools enable non-programmers to develop simple conversational agents, or advanced programmers to develop complex agents. However, to the author’s knowledge, there are no tools for nonor novice programmers to develop conversational agents for the purpose of learning AI and programming skills. This paper describes AI curriculum that includes content about conversational agents, machine learning (ML), and AI ethics, as well as a blocks-based conversational AI interface developed within MIT App Inventor. During a series of six workshops, students used this interface to develop conversational agents for social good, including a memory aide, math tutor, speech visualizer, and recycling assistant. In this paper, I present (1) the blocks-based interface, (2) the conversational AI curriculum, (3) how conversational AI directly relates to computational thinking skills, and (4) results from an initial small-scale study. The results show that through the curriculum and using the blocksbased conversational AI interface, students learned AI and ML concepts, programming skills, and to develop conversational agents for social good. Introduction and Related Work The importance of artificial intelligence (AI) education is becoming more evident with AI’s increasing ubiquity. For instance, in one study where children and adults observed a robotic toy navigating a maze, the majority of participants indicated they thought the toy was smarter than they (Druga et al. 2018). In addition to the relevance of teaching about AI technology, it is important to emphasize AI’s social implications. For instance, conversational agents may be used positively to help drivers navigate the bewildering streets of Boston, or negatively to phish for confidential information. The need for AI education is especially apparent considering AI democratization tools, like Google’s AIY, Anki’s programmable Cozmo robots, and Scratch’s Cognimates extensions (Touretzky et al. 2019), which enable people without Copyright c © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. extensive training in computer science to meaningfully participate in AI development. Other tools to democratize and teach AI include MIT App Inventor extensions, such as the image recognition, speech processing and text analysis extensions (Zhu 2019); PopBots, a platform for preschoolers to train and interact with AI (Williams, Park, and Breazeal 2019); and Machine Learning for Kids, a platform to generate machine learning (ML) models and develop web or mobile apps using Scratch or MIT App Inventor (Lane 2018). Each of these tools focus on teaching essential ML and AI concepts through empowering learners to develop AI-enabled projects. Nonetheless, there are relatively few AI democratization tools that address conversational AI (the ability of a machine to interact with humans using natural language), which is rapidly becoming ubiquitous with devices like the Amazon Echo (as well as becoming a hot area of research). Furthermore, the few tools addressing this area are typically created for one of two purposes: (1) to enable non-programmers to create standardized apps with little programming knowledge (e.g., fill-in-the-blank Alexa Skill Blueprints (Amazon 2019)) or (2) to enable skilled developers to create complex conversational agents (e.g., Google Actions Console (Google 2018)). These tools do not provide scaffolding for the beginneror non-programmer to learn AI development skills, or purposeful educational opportunities about the implications of conversational AI. To address this lack of educational, conversational AI development tools, I created a blocks-based programming interface in MIT App Inventor, as shown in Figure 1. This visual coding platform enables a range of development (from simple to highly complex apps), simplifies the programming process, and promotes computational thinking (CT) skills. More specifically, such blocks-based interfaces lower the barrier of entry to programming, enable quick prototyping, and encourage learning gains in computer science (Weintrop and Wilensky 2017). Furthermore, conversational AI programming involves complex technical terminology (e.g., intents, slots, long short-term memory (LSTM) networks, etc.); thus, to enable students to learn such complex concepts, it is helpful to ease learning in other areas (e.g., through a straightforward blocks-based development environment). Figure 1: User workflow to create a conversational AI agent. The user first implements the Voice User Interface (VUI) and endpoint function using a blocks-based interface. The blocks are converted to JSON and JavaScript, which define the agent’s functionality on Alexa devices. Additionally, the platform provides students with highagency, project-based learning opportunities. This is in contrast to tools such as Zhorai and PopBots, which are aimed at a younger audience, and are less focused on project development, but rather on exposure through conversational agent interaction (Van Brummelen, Lukin, and Lin 2019; Williams, Park, and Breazeal 2019). Research suggests that curricula in which students develop STEM projects increases students’ creative thinking skills, STEM skills efficacy, and STEM career aspirations (Lestari, Sarwi, and Sumarti 2018; Beier et al. 2019); thus, for the purpose of STEM and CT skill development, I created a project-based curriculum. This curriculum involves brainstorming realworld problem solutions, generating designs, and developing conversational agents, as described in the Curriculum section. I investigate the effectiveness of this curriculum and the conversational AI interface through a workshop-based, small-scale study, as outlined in the Results section. Motivational Scenario To provide a basis for understanding (1) the components of the conversational AI interface, (2) how someone would use the interface, and (3) the capabilities and limitations of the system, I present a scenario about how “Sheila” created a conversational AI app for her cousin “Jaidon”. Although the scenario is fictional, a version of Sheila’s Storybook App was implemented using the interface and presented to students in the workshops. Furthermore, actual applications students developed are discussed in the Results section. Sheila’s Storybook App Sheila, a seventh grade student, loves stories. When she was younger, she imagined jumping into the pages of her storybook and interacting with the characters. During a computer lesson, she heard about MIT App Inventor’s conversational AI interface and had a brilliant idea: to create a talking storybook. Sheila would create the storybook app using MIT App Inventor, run it on her tablet, and enable conversation using the Alexa app. The storybook would have the following main features: • You could swipe through “pages” of the storybook while reading and viewing illustrations on-screen • You could ask Alexa about the characters, setting, and narrative (e.g., Figure 2) • You could ask Alexa to read you the story, and as Alexa reads, the sentence on the app’s page would be highlighted • You could have “conversations” with the storybook characters, and when you ask a character a question, a response would be automatically generated Figure 2: Speaking with Alexa contextually with Sheila’s storybook. Modified from (Van Brummelen 2018). To implement the storybook app, Sheila first uploaded illustrations to MIT App Inventor and implemented pageflipping functionality by creating events in the blocks-based interface. When the “next” button was pressed, a counter would increase, and the next illustration would pop up on screen. The opposite event would occur when the “previous” button was pressed. After creating the mobile app (as shown in Figure 3), Sheila moved onto the conversational AI portion of the app. Figure 3: The storybook mobile app being developed on the MIT App Inventor website. Sheila added a new Alexa Skill to the project by clicking the Add Skill button. This brought her to a page where she could drag-and-drop and connect blocks together to create a conversational agent. First, she dragged out a define intent block so that the Voice User Interface (VUI) would recognize when someone said, “Tell me a story”. She wanted the agent to respond by telling a story about zorillas (a littleknown animal that Sheila absolutely loved!), so she also dragged out a when intent spoken block. She connected the when intent spoken block to a say block containing a text block with the first line of the story. Sheila also wanted people to be able to speak with the main character, Karabo the Zorilla. She didn’t want to write out all the possible answers to people’s questions though (that would take forever), so she decided to use a generate text block to generate Karabo’s answers instead. According to her seventh grade teacher, this block used machine learning to generate sentences sounding kind of like the stories in the block’s drop-down menu. Sheila imagined Karabo speaking kind of like Dr. Seuss, so she chose that one from the menu. After adding some additional functionality using blocks, as shown in Figure 4, Sheila sent the Alexa Skill and mobile app to her cousin Jaidon. Jaidon downloaded the app and started flipping through the pages, talking to the storybook as he went along. Jaidon was thrilled that he could listen to Alexa read him the story, especially since he didn’t know how to read himself. He also had a blast asking Karabo questions and hearing the infinite different ways Karabo would respond (despite the sentences not always being logical). He laughed when Karabo said, “Little cat in the hat, they can not eat them in your snow”. It sounded quite li",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4daa0ddd214535480d87052a48190b5811eaf399,https://www.semanticscholar.org/paper/4daa0ddd214535480d87052a48190b5811eaf399,A computer-supported cooperative learning system with multiagent intelligence,"In this paper, we describe an innovative infrastructure to support student participation and collaboration and help the instructor manage large or distance classrooms using multiagent system intelligence. The system, called I-MINDS, has a host of intelligent agents for each classroom: a teacher agent ranks and categorizes real-time questions from the students and collects statistics on student participation, a number of group agents that each maintains a collaborative group and facilitate student discussions, and a student agent for each student that profiles a student and finds compatible students to form the student's ""buddy group"". Each agent is capable of machine learning, thus improving its performance and services over time. These agents also interact and collaborate among themselves to exchange information and form coalitions dynamically to better serve the users. We have pilot-tested I-MINDS in GIS lectures, deployed I-MINDS in an introductory computer science course (CS1)'s laboratory, and evaluated the impact of I-MINDS based on student assessment. The results showed that students using I-MINDS performed (and outperformed in some aspects) as well as students in traditional settings.",AAMAS '06,2006.0,10.1145/1160633.1160933,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e79e0b2a5a279b1c6fd39a40188bb260f1fe3297,https://www.semanticscholar.org/paper/e79e0b2a5a279b1c6fd39a40188bb260f1fe3297,Templet Web: the use of volunteer computing approach in PaaS-style cloud,"Abstract This article presents the Templet Web cloud service. The service is designed for high-performance scientific computing automation. The use of high-performance technology is specifically required by new fields of computational science such as data mining, artificial intelligence, machine learning, and others. Cloud technologies provide a significant cost reduction for high-performance scientific applications. The main objectives to achieve this cost reduction in the Templet Web service design are: (a) the implementation of “on-demand” access; (b) source code deployment management; (c) high-performance computing programs development automation. The distinctive feature of the service is the approach mainly used in the field of volunteer computing, when a person who has access to a computer system delegates his access rights to the requesting user. We developed an access procedure, algorithms, and software for utilization of free computational resources of the academic cluster system in line with the methods of volunteer computing. The Templet Web service has been in operation for five years. It has been successfully used for conducting laboratory workshops and solving research problems, some of which are considered in this article. The article also provides an overview of research directions related to service development.",,2018.0,10.1515/eng-2018-0007,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1fc0924a60ccb295e77010052698ebd73a5073f8,https://www.semanticscholar.org/paper/1fc0924a60ccb295e77010052698ebd73a5073f8,Artificial Intelligence in Disaster Risk Communication : A Systematic Literature Review,"Effective communication of disaster risks is crucial to provoking appropriate responses from citizens and emergency operators. With recent advancement in Artificial Intelligence (AI), several researchers have begun exploring machine learning techniques in improving disaster risk communication. This paper adopts a systematic literature approach to report on the various research activities involving the application of AI in disaster risk communication. The study found that research activities focus on two broad areas: (1) prediction and monitoring for early warning, and (2) information extraction and classification for situational awareness. These broad areas are discussed, including background information to help establish future applications of AI in disaster risk communication. The paper concludes with recommendations of several ways in which AI applications can have a broader role in disaster risk communication. Disciplines Engineering | Physical Sciences and Mathematics Publication Details Ogie, R. Ighodaro., Castilla Rho, J. & Clarke, R. J. (2019). Artificial Intelligence in Disaster Risk Communication: A Systematic Literature Review. 2018 5th International Conference on Information and Communication Technologies for Disaster Management, ICT-DM 2018 (pp. 1-8). United States: IEEE. This conference paper is available at Research Online: https://ro.uow.edu.au/smartpapers/272 An ontology based context-aware architecture for smart campus applications Nicolas Verstaevel1, Guillaume Garzone2, Thierry Monteil2, Nawal Guermouche2, Johan Barthelemy1, Pascal Perez1 1SMART Infrastructure Facility, University of Wollongong, Wollongong, Australia 2LAAS-CNRS, Université de Toulouse, INSA Toulouse, France Abstract—With an estimated number of more than 15 billion objects, the management of architectures for the Internet of Things is a veritable challenge. The inherent mobility (in terms of devices and users) of the IoT means that the architecture has to be resilient to appearance and disappearance of devices. In this paper, we address the problem of autonomic management of IoT architecture by the means of ontologies. The problem we address is that given a dynamic system which is built upon a multitude of entities abstracted as services and characterized by their inputs and outputs, evolving targets that aim to provide services in terms of data or in terms of control, our goal is to enable autonomic management of these kinds of systems to cope with changes and evolutions so that the specified targets are fulfilled throughout the execution according to the specifics and dynamic needs of the system’s users. We propose an innovative architecture that relies on ontologies to enable context-aware application to selfcompose on demand. This architecture is being deployed to two smart campuses in two universities from Toulouse, France and Wollongong, Australia.With an estimated number of more than 15 billion objects, the management of architectures for the Internet of Things is a veritable challenge. The inherent mobility (in terms of devices and users) of the IoT means that the architecture has to be resilient to appearance and disappearance of devices. In this paper, we address the problem of autonomic management of IoT architecture by the means of ontologies. The problem we address is that given a dynamic system which is built upon a multitude of entities abstracted as services and characterized by their inputs and outputs, evolving targets that aim to provide services in terms of data or in terms of control, our goal is to enable autonomic management of these kinds of systems to cope with changes and evolutions so that the specified targets are fulfilled throughout the execution according to the specifics and dynamic needs of the system’s users. We propose an innovative architecture that relies on ontologies to enable context-aware application to selfcompose on demand. This architecture is being deployed to two smart campuses in two universities from Toulouse, France and Wollongong, Australia.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
970779f04b4bdd4404ce75a1f888f354aa10fe22,https://www.semanticscholar.org/paper/970779f04b4bdd4404ce75a1f888f354aa10fe22,SHiPCC—A Sea-going High-Performance Compute Cluster for Image Analysis,"Marine image analysis faces a multitude of challenges: data set size easily reaches Terabyte-scale; the underwater visual signal often is impaired to the point where information content becomes negligible; interpreters are scarce and can only focus on subsets of the available data due to the annotation effort involved. Solutions to speed-up the analysis process have been presented in the form of semi-automation with artificial intelligence methods like machine learning. But the algorithms employed to automate the analysis commonly rely on large-scale compute infrastructure. So far, such an infrastructure has only been available on-shore. Here, a mobile compute cluster is presented to bring big image data analysis capabilities out to sea. The Sea-going High-Performance Compute Cluster (SHiPCC) units are mobile, robustly designed to operate with impure ship-based power supplies and based on off-the-shelf computer hardware. Each unit comprises of up to eight compute nodes with graphics processing units for efficient image analysis and an internal storage to manage the big image data sets. The first SHiPCC unit has been successfully deployed at sea. It allowed to extract semantic and quantitative information from a Terabyte-sized image data set within 1.5 hours (a relative speedup of 97\% compared to a single four-core CPU computer). Enabling such compute capability out at sea allows to include image-derived information into the cruise research plan, for example by determining promising sampling locations. The SHiPCC units are envisioned to generally improve the relevance and importance of optical imagery for marine science.",Front. Mar. Sci.,2019.0,10.3389/fmars.2019.00736,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8b3e620a8c44b956c268807079f1938bda29fc65,https://www.semanticscholar.org/paper/8b3e620a8c44b956c268807079f1938bda29fc65,"22nd Conference on Innovation in Clouds, Internet and Networks and Workshops, ICIN 2019, Paris, France, February 19-21, 2019","Networking technologies have been fast evolving over the past two decades leading to a broad range of technologies (numerous PHY/MACs, routing, QoS, high availability, security, ....) while requiring increasingly stringent requirements (from best effort to deterministic). Advanced analytics with Machine Learning is already playing a key role in today’s networks, a trend that will undoubtedly increase very quickly in the coming years. That being said, the gap between what is being claimed in research papers and implementable at scale in commercial products keeps increasing, at the risk of disillusions in a near future. This talk will take a pragmatic approach sharing almost decade of experience implementing ML/AI product for networking, beyond “Proof of Concepts”. Several breakthrough networking ML/AI applications (IoT, Wireless/Wired networks) will be discussed, providing fascinating results. The last part will be dedicated to the main challenges implementing ML/AI. Mérouane Debbah, is a Full Professor at CentraleSupelec (Gif-sur-Yvette, France) and the Director of the Mathematical and Algorithmic Sciences Lab, Huawei. His research interests lie in fundamental mathematics, algorithms, statistics, information & communication sciences research. He is an IEEE Fellow, a WWRF Fellow and a member of the academic senate of Paris-Saclay. He is a leading researcher in wireless communications and recipient of several prestigious awards. Mérouane Debbah entered the Ecole Normale Supérieure Paris-Saclay (France) in 1996 where he received his M.Sc and Ph.D. degrees respectively. He worked for Motorola Labs (Saclay, France) from 1999-2002 and the Vienna Research Center for Telecommunications (Vienna, Austria) until 2003. From 2003 to 2007, he joined the Mobile Communications department of the Institut Eurecom (Sophia Antipolis, France) as an Assistant Professor. Since 2007, he is a Full Professor at CentraleSupelec (Gif-sur-Yvette, France). From 2007 to 2014, he was the director of the Alcatel-Lucent Chair on Flexible Radio. Since 2014, he is the director of the Mathematical and Algorithmic Sciences Lab. Abstract: Mobile cellular networks are becoming increasingly complex to manage while classical deployment/optimization techniques are costineffective and thus seen as stopgaps. This is all the more difficult considering the extreme constraints of 5G networks in terms of data rate (more than 10 Gb/s), massive connectivity (more than 1,000,000 devices per km2), latency (under 1ms) and energy efficiency (a reduction by a factor of 100 with respect to 4G network). Unfortunately, the development of adequate solutions is severely limited by the scarcity of the actual resources (energy, bandwidth and space). Recently, the community has turned to a new resource known as Artificial Intelligence at all layers of the network to exploit the increasing computing power afforded by the improvement in Moore's law in combination with the availability of huge data in 5G networks. This is an important paradigm shift which considers the increasing data flood/huge number of nodes as an opportunity rather than a curse. In this talk, we will discuss through various examples how the recent advances in big data algorithms can provide an efficient framework for the design of Next Generation Intelligent Networks. ICIN 2019 Keynote Speakers Mobile cellular networks are becoming increasingly complex to manage while classical deployment/optimization techniques are costineffective and thus seen as stopgaps. This is all the more difficult considering the extreme constraints of 5G networks in terms of data rate (more than 10 Gb/s), massive connectivity (more than 1,000,000 devices per km2), latency (under 1ms) and energy efficiency (a reduction by a factor of 100 with respect to 4G network). Unfortunately, the development of adequate solutions is severely limited by the scarcity of the actual resources (energy, bandwidth and space). Recently, the community has turned to a new resource known as Artificial Intelligence at all layers of the network to exploit the increasing computing power afforded by the improvement in Moore's law in combination with the availability of huge data in 5G networks. This is an important paradigm shift which considers the increasing data flood/huge number of nodes as an opportunity rather than a curse. In this talk, we will discuss through various examples how the recent advances in big data algorithms can provide an efficient framework for the design of Next Generation Intelligent Networks. ICIN 2019 Keynote Speakers",ICIN,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
399c392f96fda7b76d755a7d14e3595df7d2b1ac,https://www.semanticscholar.org/paper/399c392f96fda7b76d755a7d14e3595df7d2b1ac,Digital Analytical Sciences,"Based on inductive reasoning scientific inquiry in Analytical Sciences has a long tradition. The talk addresses in which way the digital transformation will change the scientific paradigm in Analytical Sciences to a data-driven discipline. Three main areas, integration, instruments, and methods, are discussed in their transformation through machine learning and other digital tools. Finally, some shortcomings and pitfalls in the deployments of algorithmic shortcuts are illustrated.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c3da4a75d102951d3566893e972b6fe549121a81,https://www.semanticscholar.org/paper/c3da4a75d102951d3566893e972b6fe549121a81,A Tutorial on Software Engineering Intelligence: Case Studies on Model-Driven Engineering,"The recent advances in Artificial Intelligence (AI) are dramatically impacting the way we are modelling software systems. A large number of computational intelligence based approaches and tools, combining computational search and machine learning, proved to be successful in automating and semiautomating several activities to support developers. However, the adoption of computational intelligence to address model driven engineering problems is still under-explored. In this tutorial, we will give an overview about computational intelligence, why model-driven engineering is a suitable paradigm for computational intelligence and how computational intelligence could benefit from the recent advances in model-driven engineering. Then, we will focus on some case studies that we published around the adaptation of a variety of computational intelligence techniques for model transformations, models evolution, model changes detection, co-evolution, model/metamodel refactoring, models merging, models quality and metamodels matching. To make the tutorial interactive, the participants will have the opportunity to practice our interactive intelligent MDE tools during the tutorial. Finally, we will conclude the tutorial with different suggestions to enhance the adoption of MDE intelligence research into industry, and the lessons that we learned along this journey and our vision about the future of MDE intelligence. The event will target a wide range of researchers and practitioners from both the model-driven engineering and computational intelligence communities and will reduce the gap between them. The participants will learn about the recent advances in computational intelligence and acquire the required skills to apply them for relevant MDE problems. Model-Driven Engineering, computational intelligence, model transformations, models refactoring, machine learning I. SHORT BIO OF THE PRESENTERS The three organizers of the tutorial have extensive experience and complementary expertise on both MDE and Computational Intelligence. Dr. Kessentini is the PC chair of the foundations track of MODELS2019, he will be the general chair of ASE2021 and he organized with Dr. Deb the search based software engineering symposium (SSBSE2016). Dr. Marouane Kessentini is a recipient of the prestigious 2018 President of Tunisia distinguished research award, the University of Michigan distinguished teaching award, the University of Michigan distinguished digital education award, the Collge of Engineering and Computer Science distinguished research award, 4 best paper awards, and his AI-based software refactoring invention, licensed and deployed by industrial partners, is selected as one of the Top 8 inventions at the University of Michigan for 2018, among over 500 inventions, by the UM Technology Transfer Office. Prior to joining UM in 2013, He received his Ph.D. from the University of Montreal in Canada in 2012. He received several grants from both industry and federal agencies and published over 110 papers in top journals and conferences. Dr. Kessentini has several collaborations with industry on the use of computational search, machine learning and evolutionary algorithms to address software engineering and services computing problems. He is the co-founder of IWoR and NasBASE, General Chair of SSBSE16 and ASE21, and PC chair of MODELS19, GECCO14-15. He served as invited speaker at SSBSE and WCCI, graduated 13 Ph.D. student as a chair and serving as associate editor in 7 journals and PC member of over 100 conferences. He organized tutorials on Search based Software Engineering at SSBSE2018, WCCI2016, ASE2016, etc. He has extensive publications on adopting computational intelligence in MDE [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13] and organized several workshops at MODELS (2013, 2014 and 2015) around this topic. Xin Yao is a Chair Professor of Computer Science at the Southern University of Science and Technology, Shenzhen, China, and a part-time professor at the University of Birmingham, UK. His major research interests include evolutionary computation, ensemble learning and search-based software engineering. His work won the 2001 IEEE Donald G. Fink Prize Paper Award, 2010, 2015 and 2017 IEEE Transactions on Evolutionary Computation Outstanding Paper Awards, 2010 BT Gordon Radley Award for Best Author of Innovation (Finalist), 2011 IEEE Transactions on Neural Networks Outstanding Paper Award, and many other best paper awards. He received the prestigious Royal Society Wolfson Research Merit Award in 2012 and the IEEE CIS Evolutionary Computation Pioneer Award in 2013. He has extensive publications on search based software engineering. Kalyanmoy Deb is Koenig Endowed Chair Professor at Department of Electrical and Computer Engineering in Michigan State University, USA. Prof. Deb’s research interests are in evolutionary optimization and their application in multicriterion optimization, modeling, and machine learning. He was awarded IEEE EC Pioneer award, Infosys Prize, TWAS Prize in Engineering Sciences, CajAstur Mamdani Prize, Distinguished Alumni Award from IIT Kharagpur, EdgeworthPareto award, Bhatnagar Prize in Engineering Sciences, and Bessel Research award from Germany. He is fellow of IEEE and ASME. He has published over 500 research papers with Google Scholar citation of over 122,000 with h-index 112. He is in the editorial board on 18 major international journals.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
226dc83a92162a3fdd63466d2ada9559c7fbb2d4,https://www.semanticscholar.org/paper/226dc83a92162a3fdd63466d2ada9559c7fbb2d4,Law and Policy for the Second Quantum Revolution,"We are living in the “second quantum revolution.” Using theoretical insights from the first quantum revolution of the early 20th century, multidisciplinary teams have achieved fantastic advances in quantum metrology and sensing, in quantum communications, and in quantum computing. Metrology and sensing will enable high-resolution imaging, with attendant effects on everything from medicine to battlespace conflicts through enhanced sonar and radar. Quantum communications raise the specter of networks invulnerable to spying, and the fundamentals of such networks are already in place, with some technologies available commercially. Quantum computing, as many have observed, will degrade and in some cases render useless, the encryption that everyday commerce relies upon. But it will likely also enable simulation of complex systems and contribute to advances in machine learning. The affordances and limitations of quantum technologies will shape who can access and use these innovations. Furthermore, quantum technologies will arrive at different times and thus create surprising path dependencies. For instance, many hold out quantum computing as a doomsday technology for privacy, yet, some doubt that general purpose quantum computers necessary for the privacy apocalypse can even be built. Even if built, only nation states and large companies will have access to the technology, and these technical and economic constraints will shape both how quantum computers might be misused and how regulation might work. Excitement surrounding quantum computing should not cause us to overlook the advances in metrology, sensing, and communications that are already here and likely to be miniaturized and commercialized in ways quantum computers will not be for the foreseeable future. Indeed, in the short term, quantum may contribute to advances in communications integrity, confidentiality, and authenticity. There is no legal literature on the consequences of quantum technologies broadly and only a thin exploration of it in the ethics literature. Thus, this article starts a policy conversation on the high-level issues raised by quantum technologies. Quantum technologies will create strategic concerns for national security and for the intelligence community. Already China and Europe have made large investments into quantum communications technologies in explicit attempts to create surveillance-detecting and surveillance-invulnerable networks, no doubt motivated by revelations of the National Security Agency’s 1 Thank you footnote: Lily Lin, Evan Wolff. Draft, do not cite or distribute 2 spying power. Quantum metrology and sensing raises similar strategic concerns, from the uncloaking of submarine movements and thus unsettling the balance of power reached through the nuclear triad to development of electronic-warfare resistant weapons. Combined these developments might mean that the golden age of signals intelligence may be yielding to a golden age of measurement and signature intelligence. Responsive policy options could take many forms, from export control efforts and industrial policy to aggressive immigration policy aimed at attracting and retaining the best minds of the field. Steps can also be taken now to avoid meltdowns in confidentiality, integrity, and authenticity of data made possible if a general-purpose quantum computer is achieved. For instance, it is important to advance password complexity and to find more secure ways to sign software and digital certificates, because these technologies will be both made vulnerable by quantum computing, and be the kinds of attacks of most interest to entities likely to develop quantum computers. The internet revolution arrived with no coherent legal regime or strategy. We need not be unprepared for the quantum revolution. As quantum technologies reach deployment readiness, we can make fundamental decisions on how policy should complement or inhibit them. At the highest level, we should promote quantum in the many ways it could contribute to human flourishing. These include medical diagnostics, advances in materials science and design, and drug discovery. But it would be naïve to overlook how quickly governments are adopting these technologies for military purposes, and in doing so, perhaps even creating a quantum “taboo.” Thus, realists need to contemplate how quantum will affect nation-state conflict, whether and how quantum technologies should be commercialized, and what steps can be taken today to prevent quantum from being a destabilizing technology. Draft, do not cite or distribute 3 Abstract ......................................................................................................................................",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
aef58b6026577b5a42d38ab2363f274eaa69f335,https://www.semanticscholar.org/paper/aef58b6026577b5a42d38ab2363f274eaa69f335,Law and Policy for the Second Quantum Revolution,"We are living in the “second quantum revolution.” Using theoretical insights from the first quantum revolution of the early 20th century, multidisciplinary teams have achieved fantastic advances in quantum metrology and sensing, in quantum communications, and in quantum computing. Metrology and sensing will enable high-resolution imaging, with attendant effects on everything from medicine to battlespace conflicts through enhanced sonar and radar. Quantum communications raise the specter of networks invulnerable to spying, and the fundamentals of such networks are already in place, with some technologies available commercially. Quantum computing, as many have observed, will degrade and in some cases render useless, the encryption that everyday commerce relies upon. But it will likely also enable simulation of complex systems and contribute to advances in machine learning. The affordances and limitations of quantum technologies will shape who can access and use these innovations. Furthermore, quantum technologies will arrive at different times and thus create surprising path dependencies. For instance, many hold out quantum computing as a doomsday technology for privacy, yet, some doubt that general purpose quantum computers necessary for the privacy apocalypse can even be built. Even if built, only nation states and large companies will have access to the technology, and these technical and economic constraints will shape both how quantum computers might be misused and how regulation might work. Excitement surrounding quantum computing should not cause us to overlook the advances in metrology, sensing, and communications that are already here and likely to be miniaturized and commercialized in ways quantum computers will not be for the foreseeable future. Indeed, in the short term, quantum may contribute to advances in communications integrity, confidentiality, and authenticity. There is no legal literature on the consequences of quantum technologies broadly and only a thin exploration of it in the ethics literature. Thus, this article starts a policy conversation on the high-level issues raised by quantum technologies. Quantum technologies will create strategic concerns for national security and for the intelligence community. Already China and Europe have made large investments into quantum communications technologies in explicit attempts to create surveillance-detecting and surveillance-invulnerable networks, no doubt motivated by revelations of the National Security Agency’s 1 Thank you footnote: Lily Lin, Evan Wolff. Draft, do not cite or distribute 2 spying power. Quantum metrology and sensing raises similar strategic concerns, from the uncloaking of submarine movements and thus unsettling the balance of power reached through the nuclear triad to development of electronic-warfare resistant weapons. Combined these developments might mean that the golden age of signals intelligence may be yielding to a golden age of measurement and signature intelligence. Responsive policy options could take many forms, from export control efforts and industrial policy to aggressive immigration policy aimed at attracting and retaining the best minds of the field. Steps can also be taken now to avoid meltdowns in confidentiality, integrity, and authenticity of data made possible if a general-purpose quantum computer is achieved. For instance, it is important to advance password complexity and to find more secure ways to sign software and digital certificates, because these technologies will be both made vulnerable by quantum computing, and be the kinds of attacks of most interest to entities likely to develop quantum computers. The internet revolution arrived with no coherent legal regime or strategy. We need not be unprepared for the quantum revolution. As quantum technologies reach deployment readiness, we can make fundamental decisions on how policy should complement or inhibit them. At the highest level, we should promote quantum in the many ways it could contribute to human flourishing. These include medical diagnostics, advances in materials science and design, and drug discovery. But it would be naïve to overlook how quickly governments are adopting these technologies for military purposes, and in doing so, perhaps even creating a quantum “taboo.” Thus, realists need to contemplate how quantum will affect nation-state conflict, whether and how quantum technologies should be commercialized, and what steps can be taken today to prevent quantum from being a destabilizing technology. Draft, do not cite or distribute 3 Chris Jay Hoofnagle [Word Count: X] ........................................................................................... 1 Abstract ......................................................................................................................................",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6a4f014ac3a5ac7607637574ae7f293ea9f7e786,https://www.semanticscholar.org/paper/6a4f014ac3a5ac7607637574ae7f293ea9f7e786,The Challenges for Interpretable AI for Well-being,"In this AAAI Spring symposium 2019, we discuss interpretable AI in the context of well-being AI. Interpretable AI is an artificial intelligence methods and systems, of which outputs can be easily understood by humans. Especially in the human health and wellness domains, making wrong predictions may lead to critical judgements in life or death situations. AI based systems must be well-understood. We define “well-being AI” as an AI research paradigm for promoting psychological well-being and maximizing human potential. Interpretable AI is important for well-being AI in senses that (1) to understand how our digital experience affects our health and our quality of life and (2) to design well-being systems that put humans at the center. One of the important keywords in understanding machine intelligence in human health and wellness is cognitive bias. Advances in big data and machine learning should not overlook some new threats to enlightened thought, such as the recent trend of social media platforms and commercial recommendation systems being used to manipulate people's inherent cognitive bias. The second important keyword is “social embeddedness”. Cognitive bias will be affected by how the AI is perceived particularly at the community or social level. Social embeddedness is the social science idea that actions of individuals are refracted by the social relations within their community. In our contexts, understanding relationships between AI and society is very important, which includes the issues on AI and future economics (such as basic income, impact of AI on GDP), or “well-being society (such as happiness of citizen life quality). This paper describes the detailed motivation, important keywords, the scope of interests and research questions in this symposium. Motivation for Interpretable AI for well-being Interpretable AI is an artificial intelligence methods and systems, of which outputs can be easily understood by humans. Recently, the European Union’s new General Data Protection Regulation (GDPR) has raised concerns about the emerging tools for automated individual decisionmaking. These tools use algorithms to make decisions based on user-level profiles, with the potential to significantly affect users. Recent AI technologies (e.g.: Deep Learning and other advanced machine learning methods) will change the world. However, excessive expectations for AI (e.g., the representation of general-purpose AI in science fiction) and threat theory (e.g. AI will lead to unemployment) distort the judgment of many people. Understanding both the potential and the limitations of the current AI technologies is therefore very important. Especially in the human health and wellness domains, interpretable AI remains a huge challenge. For example, “evidence-based medicine” requires us to show the current best evidence in making decisions about the care of patients. “Why did the system make this prediction?” will be a key question. Even if the system is not accurate, it must be explainable and predictable. Although statistical machine learning predicts the future based on past data, it is difficult to respond to a new event which has never seen in the past. Training data that has outliers or adversarially generated data may lead an AI-based system to make wrong predictions (sometimes with high confidence) in life or death situations in medical diagnoses. For AI to be safely deployed, these systems must be well-understood. One of the important goals in this year's symposium is to discuss the technical and philosophical challenges of interpretability for well-being AI. Understanding Cognitive Bias and Social Embeddedness AI also provides the new risk of amplifying our “cognitive bias” through machine learning, as we discussed in our previous AAAI18 Spring symposium on “beyond machine intelligence” (Kido and Takadama, 2018). In the recent trend of big data becoming personalized, corresponding AI technologies for manipulating one’s cognitive bias are starting to evolve; examples of this include social media platforms such as Twitter and Facebook, and commercial recommendation systems. According to the “Echo chamber effect,” people with the same opinion tend to form communities, which makes it felt that everyone else also shares the same opinion. Recently, there has also been a movement to use such cognitive bias in the political world. We welcome discussions on “cognitive bias” in human or personal robot communications. “Social embeddedness” of AI is also an important keyword in this symposium. We welcome diverse discussions on the relationships between AI and society. The topics on social embeddedness of AI may include such issues as “AI and future economics (such as basic income, impact of AI on GDP)” or “well-being society (such as happiness of citizen, life quality)”, etc. Cognitive Bias will be affected by how the AI is perceived particularly at the community (or societal) level. “Social embeddedness of AI” seems likely to become a significant area as AI continues to develop. Our Scope of Interests and Research Questions. We expect to discuss important interdisciplinary challenges for guiding future advances in well-being AI. We will have the following scope of interests in this symposium: (1) ""Excessive expectation for AI understanding possibilities and limitations of the current AI technologies"", (2) ""Technical and philosophical challenges on interpretability for well-being AI"" (3) ""Cognitive bias"" and ""social embeddedness of AI"" in human/robot communications, from the sociocultural/political aspects to the technical/practical, accuracy and efficiency issues in health, economics, and other fields. More technically, we have the following research questions in Interpretable AI for well-being. We need to deepen the understandings of the possibilities and limitations of the Machine Learning and other advanced analyses for Health & Wellness. 1 Interpretable AI/ML  How can we develop interpretable machine learning methods in well-being AI that provide ways to manage the complexity of a model and/or generate meaningful explanations?  How can we use the tools of causal inference to reason about fairness in well-being AI? Can causal inference lead to actionable recommendations and interventions? How can we design and evaluate the effect of interventions?  What are the societal implications of algorithmic exploration? How can we manage the cost that such exploration might pose to individuals? 2 Unintended consequence of algorithms in well-being AI  Can we use adversarial conditions to learn about the inner workings of algorithms?  Can we learn from the ways they fail on edge cases?  Can we achieve accountability in well-being AI?  How can we conduct reliable empirical black-box testing for ethically salient differential treatment?  How can we manage the risks that such unintended consequence might pose to users?",AAAI Spring Symposium: Interpretable AI for Well-being,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0f850fc79214eb9a91df365a849e7516d5a6cc3b,https://www.semanticscholar.org/paper/0f850fc79214eb9a91df365a849e7516d5a6cc3b,Noytext: A Web platform to annotate social media documents on noise perception for their use in opinion mining research,"espanolLa explosion de las redes sociales online ha facilitado que la ciudadania comparta su punto de vista sobre los problemas que sufre en su dia a dia, incluyendo las molestias hacia el ruido. La evolucion tecnologica ha facilitado la implantacion de tecnologias como el Procesado de Lenguaje Natural y el Machine Learning, que han permitido analizar los textos provenientes de redes sociales online y detectar la opinion que expresa la poblacion sobre el ruido de su entorno en este canal de comunicacion. Algunos de los algoritmos utilizados, como los basados en redes neuronales profundas, son algoritmos de aprendizaje automatico supervisado. Esto significa que los investigadores tienen que proporcionar un conjunto de datos etiquetados para construir nuevos modelos de clasificacion de textos. El proceso de anotacion de datos es uno de los trabajos mas laboriosos en un proyecto de ciencia de datos, ya que los investigadores, entre otras cosas, tienen que probar la conformidad entre los anotadores y comprobar que las categorias pre-definidas para clasificar los datos han sido definidas correctamente. Con intencion de facilitar dicha tarea en este articulo se presenta Noytext, una herramienta web personalizable que permite anotar textos cortos provenientes de redes sociales online, que puede ser desplegada facilmente en servidores propios y con la que se puede solicitar la ayuda de colegas y colaboradores en el proceso de anotacion. EnglishBoost of online social networks has demonstrated that some people are willing to share their views about everyday problems, including noise. With the advent of Natural Language Processing and Machine Learning technologies to the majority of the scientific fields, we have begun to analyze the textual content of social media, and more specifically online social networks, to extract insights about the noise attitude of the population that uses this channel to express their opinion in this matter. Some of the state-of-the-art algorithms, such as deep neural networks, are supervised machine learning algorithms. This means that researchers have to provide a set of labelled training data to build new models. The annotation process is known as one of the most time-costly tasks in a data science pipeline, since researchers among other thigs have to test the agreement between annotators and to measure the quality of the categories they had previously defined. For that reason in this paper, we introduce Noytext which is a customizable web tool to annotate texts from your database, that can be deployed in your own webserver and you can use to request help from colleagues and collaborators in the annotation process in a friendly way.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6edc57265161a789cdcf10d687285c7c37993f2b,https://www.semanticscholar.org/paper/6edc57265161a789cdcf10d687285c7c37993f2b,Research Data Alliance: Understanding Big Data Analytics Applications in Earth Science,"The Research Data Alliance (RDA) enables data to be shared across barriers through focused working groups and interest groups, formed of experts from around the world - from academia, industry and government. Its Big Data Analytics (BDA) interest groups seeks to develop community based recommendations on feasible data analytics approaches to address scientific community needs of utilizing large quantities of data. BDA seeks to analyze different scientific domain applications (e.g. earth science use cases) and their potential use of various big data analytics techniques. These techniques reach from hardware deployment models up to various different algorithms (e.g. machine learning algorithms such as support vector machines for classification). A systematic classification of feasible combinations of analysis algorithms, analytical tools, data and resource characteristics and scientific queries will be covered in these recommendations. This contribution will outline initial parts of such a classification and recommendations in the specific context of the field of Earth Sciences. Given lessons learned and experiences are based on a survey of use cases and also providing insights in a few use cases in detail.",,2014.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c3a3bea63336e13c5c597c8061920c04d7715e8d,https://www.semanticscholar.org/paper/c3a3bea63336e13c5c597c8061920c04d7715e8d,Spatio-Temporal Locality in Hash Tables,"The overall theme of this Ph.D. is looking at ways to use emerging NVM (Non-Volatile Memory) technologies in realworld data-science scenarios. It is hoped that the exploitation of the characteristics of the technology will result in performance improvements, defined as being either/or an increase in computational throughput and energy-use reduction. Primarily, this has been through the inclusion of temporal locality into HopH (Hopscotch Hashing) by [2]. The problem of highly-skewed access patterns affecting lookup time and required computation is shown through a simple model. A simulator is then used to measure the expected performance gains of incorporating temporal locality, given different HT (Hash Table) configurations. This work was originally motivated by NVM, as a way to mask the extra latency anticipated, but the work is applicable to HTs in DRAM (Dynamic Random-Access Memory) also. The second area of interest in the Ph.D. is looking at exploiting the characteristics of NVM for different families of machine learning algorithms, though this paper focuses solely on the former.",PhD@VLDB,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d36a6cec7a0f058f3e3e8788c4c32ec8111c5ee4,https://www.semanticscholar.org/paper/d36a6cec7a0f058f3e3e8788c4c32ec8111c5ee4,"A Review of Holography in the Aquatic Sciences: In situ Characterization of Particles, Plankton, and Small Scale Biophysical Interactions","The characterization of particle and plankton populations, as well as microscale biophysical interactions, is critical to several important research areas in oceanography and limnology. A growing number of aquatic researchers are turning to holography as a tool of choice to quantify particle fields in diverse environments, including but not limited to, studies on particle orientation, thin layers, phytoplankton blooms, and zooplankton distributions and behavior. Holography provides a non-intrusive, free-stream approach to imaging and characterizing aquatic particles, organisms, and behavior in situ at high resolution through a 3-D sampling volume. Compared to other imaging techniques, e.g., flow cytometry, much larger volumes of water can be processed over the same duration, resolving particle sizes ranging from a few microns to a few centimeters. Modern holographic imaging systems are compact enough to be deployed through various modes, including profiling/towed platforms, buoys, gliders, long-term observatories, or benthic landers. Limitations of the technique include the data-intensive hologram acquisition process, computationally expensive image reconstruction, and coherent noise associated with the holograms that can make post-processing challenging. However, continued processing refinements, rapid advancements in computing power, and development of powerful machine learning algorithms for particle/organism classification are paving the way for holography to be used ubiquitously across different disciplines in the aquatic sciences. This review aims to provide a comprehensive overview of holography in the context of aquatic studies, including historical developments, prior research applications, as well as advantages and limitations of the technique. Ongoing technological developments that can facilitate larger employment of this technique toward in situ measurements in the future, as well as potential applications in emerging research areas in the aquatic sciences are also discussed.",Frontiers in Marine Science,2021.0,10.3389/fmars.2020.572147,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
517a6b1f19339ebf151682da8058080970973464,https://www.semanticscholar.org/paper/517a6b1f19339ebf151682da8058080970973464,A contextualized project-based approach for improving student engagement and learning in AI courses,"Project MLeXAI, Machine Learning Experiences in Artificial Intelligence, is a multi-institutional project that has been funded by a grant from the National Science Foundation. The goal is to develop a framework for teaching core Artificial Intelligence (AI) topics with a unifying theme of machine learning. The objectives are to enhance student learning experiences in the AI course by implementing a unifying theme of machine learning to tie together the diverse and seemingly disconnected topics in the AI course, to increase student interest and motivation to learn AI, and to introduce students to an increasingly important research area. To that end, a suite of hands-on term-long projects and associated curricular modules have been developed. Each project involves the design and implementation of a learning system which enhances a particular commonly-deployed AI application. In addition, the projects provide students with an opportunity to address not only core AI topics but also many of the issues central to computer science, including algorithmic complexity and scalability problems.
 Phase I of the project involved the development and testing of a prototype at three institutions: a small liberal arts college, a medium comprehensive university, and a state university. Phase II builds on phase I work and involves further development and testing of an adaptable framework for the presentation of core AI through a unifying theme of machine learning. The goal of phase II was to increase the number of modules and expand the implementation and testing to include a larger and more diverse set of colleges and universities.
 In this paper we present results of phase II assessment of the collaborative development, dissemination, and separate testing of the projects at the institutions of twenty participating faculty members. The institutions are geographically dispersed and represent a broad range of institutions and student body.",ITiCSE '11,2011.0,10.1145/2421277.2421278,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
961e4d114da5085b4ca8a78d989bbfea27b7383e,https://www.semanticscholar.org/paper/961e4d114da5085b4ca8a78d989bbfea27b7383e,Towards Teacher-Managed Deployment and Integration of Non-SaaS Tools in Virtual Learning Environments,,EC-TEL,2015.0,10.1007/978-3-319-24258-3_60,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d72cdf30b449b2d3474d4830751bf21b81fb3093,https://www.semanticscholar.org/paper/d72cdf30b449b2d3474d4830751bf21b81fb3093,Technology Developments for Biodiversity Monitoring and Conservation,"Over the next 5 years major advances in the development and application of numerous technologies related to computing, mobile phones, artificial intelligence (AI), and augmented reality (AR) will have a dramatic impact in biodiversity monitoring and conservation. Over a 2-week period several of us had the opportunity to meet with multiple technology experts in the Silicon Valley, California, USA to discuss trends in technology innovation, and how they could be applied to conservation science and ecology research. Here we briefly highlight some of the key points of these meetings with respect to AI and Deep Learning. Computing: Investment and rapid growth in AI and Deep Learning technologies are transforming how machines can perceive the environment. Much of this change is due to increased processing speeds of Graphics Processing Units (GPUs), which is now a billiondollar industry. Machine learning applications, such as convolutional neural networks (CNNs) run more efficiently on GPUs and are being applied to analyze visual imagery and sounds in real time. Rapid advances in CNNs that use both supervised and unsupervised learning to train the models is improving accuracy. By taking a Deep Learning approach where the base layers of the model are built upon datasets of known images and sounds (supervised learning) and later layers relying on unclassified images or sounds (unsupervised learning), dramatically improve the flexibility of CNNs in perceiving novel stimuli. The potential to have autonomous sensors gathering biodiversity data in the same way personal weather stations gather atmospheric information is close at hand. ‡ © Kelling S. This is an open access article distributed under the terms of the Creative Commons Attribution License (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Mobile Phones: The phone is the most widely used information appliance in the world. No device is on the near horizon to challenge this platform, for several key reasons. First, network access is ubiquitous in many parts of the world. Second, batteries are improving by about 20% annually, allowing for more functionality. Third, app development is a growing industry with significant investment in specializing apps for machine-learning. While GPUs are already running on phones for video streaming, there is much optimism that reduced or approximate Deep Learning models will operate on phones. These models are already working in the lab, with the biggest hurdle being power consumption and developing energy efficient applications and algorithms to run complicated AI processes will be important. It is just a matter of time before industry will have AI functionality on phones. These rapid improvements in computing and mobile phone technologies have huge implications for biodiversity monitoring, conservation science, and understanding ecological systems. Computing: AI processing of video imagery or acoustic streams create the potential to deploy autonomous sensors in the environment that will be able to detect and classify organisms to species. Further, AI processing of Earth spectral imagery has the potential to provide finer grade classification of habitats, which is essential in developing fine scale models of species distributions over broad spatial and temporal extents. Mobile Phones: increased computing functionality and more efficient batteries will allow applications to be developed that will improve an individual’s perception of the world. Already AI functionality of Merlin improves a birder’s ability to accurately identify a bird. Linking this functionality to sensor devices like specialized glasses, binoculars, or listening devises will help an individual detect and classify objects in the environment. In conclusion, computing technology is advancing at a rapid rate and soon autonomous sensors placed strategically in the environment will augment the species occurrence data gathered by humans. The mobile phone in everyone’s pocket should be thought of strategically, in how to connect people to the environment and improve their ability to gather meaningful biodiversity information.",,2018.0,10.3897/BISS.2.25833,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
66ee88df2211b093445f6ac183fda9195ebc1673,https://www.semanticscholar.org/paper/66ee88df2211b093445f6ac183fda9195ebc1673,Quality Classification of Scientific Publications Using Hybrid Summarization Model,,ICADL,2018.0,10.1007/978-3-030-04257-8_6,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9efc2ecf2efc6121c6b3489f829b74ecaa446213,https://www.semanticscholar.org/paper/9efc2ecf2efc6121c6b3489f829b74ecaa446213,"Smart Manufacturing: State-of-the-Art Review in Context of Conventional and Modern Manufacturing Process Modeling, Monitoring and Control","With the advances in automation technologies, data science, process modeling and process control, industries worldwide are at the precipice of what is described as the fourth industrial revolution (Industry 4.0). This term was coined in 2011 by the German federal government to define their strategy related to high tech industry [1], specifically multidisciplinary sciences involving physics-based process modeling, data science and machine learning, cyber-physical systems, and cloud computing coming together to drive operational excellence and support sustainable manufacturing. The boundaries between Information Technologies (I.T.) and Operation Technologies (O.T.) are quickly dissolving and the opportunities for taking lab-scale manufacturing science research to plant and enterprise wide deployment are better than ever before. There are still questions to be answered, such as those related to the future of manufacturing research and those related to meeting such demands with a highly skilled workforce. Furthermore, in this new environment it is important to understand how process modeling, monitoring, and control technologies will be transformed. The aim of the paper is to provide state-of-the-art review of Smart Manufacturing and Industry 4.0 within scope of process monitoring, modeling and control. This will be accomplished by giving comprehensive background review and discussing application of smart manufacturing framework to conventional (machining) and advanced (additive) manufacturing process case studies. By focusing on process modeling, monitoring, analytics, and control within the larger vision of Industry 4.0, this paper will provide a directed look at the efforts in these areas, and identify future research directions that would accelerate the pace of implementation in advanced manufacturing industry.",Volume 3: Manufacturing Equipment and Systems,2018.0,10.1115/MSEC2018-6658,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c515cb097a52d692a8c3cffc9a4dbeda3f60875f,https://www.semanticscholar.org/paper/c515cb097a52d692a8c3cffc9a4dbeda3f60875f,Ligo: An Open Source Application for the Management and Execution of Administrative Data Linkage,"IntroductionLigo is an open source application that provides a framework for managing and executing administrative data linking projects. Ligo provides an easy-to-use web interface that lets analysts select among data linking methods including deterministic, probabilistic and machine learning approaches and use these in a documented, repeatable, tested, step-by-step process. 
Objectives and ApproachThe linking application has two primary functions: identifying common entities in datasets [de-duplication] and identifying common entities between datasets [linking]. The application is being built from the ground up in a partnership between the Province of British Columbia’s Data Innovation (DI) Program and Population Data BC, and with input from data scientists. The simple web interface allows analysts to streamline the processing of multiple datasets in a straight-forward and reproducible manner. 
ResultsBuilt in Python and implemented as a desktop-capable and cloud-deployable containerized application, Ligo includes many of the latest data-linking comparison algorithms with a plugin architecture that supports the simple addition of new formulae. Currently, deterministic approaches to linking have been implemented and probabilistic methods are in alpha testing. A fully functional alpha, including deterministic and probabilistic methods is expected to be ready in September, with a machine learning extension expected soon after. 
Conclusion/ImplicationsLigo has been designed with enterprise users in mind. The application is intended to make the processes of data de-duplication and linking simple, fast and reproducible. By making the application open source, we encourage feedback and collaboration from across the population research and data science community.",International Journal of Population Data Science,2018.0,10.23889/IJPDS.V3I4.749,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
47c29c305d0ec04a8c8222b09141244ad759cc49,https://www.semanticscholar.org/paper/47c29c305d0ec04a8c8222b09141244ad759cc49,Lifelong Learning : A Reinforcement Learning Approach,"s (9): Abstract 3: Efficient deep neural networks for perception in autonomous driving (Jose M. Alvarez, TRI) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:00 AM3: Efficient deep neural networks for perception in autonomous driving (Jose M. Alvarez, TRI) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:00 AM Abstract Convolutional neural networks have achieved impressive success in many tasks in computer vision such as image classification, object detection / recognition or semantic segmentation. While these networks have proven effective in all these applications, they come at a high memory and computational cost, thus not feasible for applications where power and computational resources are limited. In addition, the process to train the network reduces productivity as it not only requires large computer servers but also takes a significant amount of time (several weeks) with the additional cost of engineering the architecture. In this talk, I first introduce our efficient architecture based on filter-compositions and then, a novel approach to jointly learn the architecture and explicitly account for compression during the training process. Our results show that we can learn much more compact models and significantly reduce training and inference time.Convolutional neural networks have achieved impressive success in many tasks in computer vision such as image classification, object detection / recognition or semantic segmentation. While these networks have proven effective in all these applications, they come at a high memory and computational cost, thus not feasible for applications where power and computational resources are limited. In addition, the process to train the network reduces productivity as it not only requires large computer servers but also takes a significant amount of time (several weeks) with the additional cost of engineering the architecture. In this talk, I first introduce our efficient architecture based on filter-compositions and then, a novel approach to jointly learn the architecture and explicitly account for compression during the training process. Our results show that we can learn much more compact models and significantly reduce training and inference time. Bio: Dr. Jose Alvarez is a senior research scientist at Toyota Research Institute. His main research interests are in developing robust and efficient deep learning algorithms for perception with focus on autonomous vehicles. Previously, he was a researcher at Data61 / CSIRO (formerly NICTA), a Postdoctoral researcher at the Courant Institute of Mathematical Science, New York University, and visiting scholar at University of Amsterdam and Group Research Electronics at Volkswagen. Dr. Alvarez graduated in 2012 and he was awarded the best Ph.D. Thesis award. Dr. Alvarez serves as associate editor for IEEE Trans. on Intelligent Transportation Systems. Abstract 4: Visual 3D Scene Understanding and Prediction for ADAS (Manmohan Chandraker, NEC Labs) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:30 AM4: Visual 3D Scene Understanding and Prediction for ADAS (Manmohan Chandraker, NEC Labs) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:30 AM Abstract: Modern advanced driver assistance systems (ADAS) rely on a range of sensors including radar, ultrasound, LIDAR and cameras. Active sensors have found applications in detecting traffic participants (TPs) such as cars or pedestrians and scene elements (SEs) such as roads. However, camera-based systems have the potential to achieve or augment these capabilities at a much lower cost, while allowing new ones such as determination of TP and SE semantics as well as their interactions in complex traffic scenes. Modern advanced driver assistance systems (ADAS) rely on a range of sensors including radar, ultrasound, LIDAR and cameras. Active sensors have found applications in detecting traffic participants (TPs) such as cars or pedestrians and scene elements (SEs) such as roads. However, camera-based systems have the potential to achieve or augment these capabilities at a much lower cost, while allowing new ones such as determination of TP and SE semantics as well as their interactions in complex traffic scenes. In this talk, we present several technical advances for vision-based ADAS. A common theme is to overcome the challenges posed by lack of large-scale annotations in deep learning frameworks. We introduce approaches to correspondence estimation that are trained on purely synthetic data but adapt well to real data at test-time. We introduce object detectors that are light enough for ADAS, trained with knowledge distillation to retain accuracies of deeper architectures. Our semantic segmentation methods are trained on weak supervision that requires only a tenth of conventional annotation time. We propose methods for 3D reconstruction that use deep supervision to recover fine TP part locations while relying on purely synthetic 3D CAD models. We develop deep learning frameworks for multi-target tracking, as well as occlusion-reasoning in TP localization and SE layout estimation. Finally, we present a framework for TP behavior prediction in complex traffic scenes that accounts for TP-TP and TP-SE interactions. Our approach allows prediction of diverse multimodal outcomes and aims to account for long-term strategic behaviors in complex scenes. ICML 2017 Workshop book Generated Mon Aug 13, 2018 Page 4 of 20 Bio: Manmohan Chandraker is an assistant professor at the CSE department of the University of California, San Diego and leads the computer vision research effort at NEC Labs America in Cupertino. He received a B.Tech. in Electrical Engineering at the Indian Institute of Technology, Bombay and a PhD in Computer Science at the University of California, San Diego. His personal research interests are 3D scene understanding and reconstruction, with applications to autonomous driving and human-computer interfaces. His works have received the Marr Prize Honorable Mention for Best Paper at ICCV 2007, the 2009 CSE Dissertation Award for Best Thesis at UCSD, a PAMI special issue on best papers of CVPR 2011 and the Best Paper Award at CVPR 2014. Abstract 6: 2 x 15 Contributed Talks on Datasets and Occupancy Maps in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 10:30 AM6: 2 x 15 Contributed Talks on Datasets and Occupancy Maps in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 10:30 AM Jonathan Binas, Daniel Neil, Shih-Chii Liu, Tobi Delbruck, DDD17: End-To-End DAVIS Driving Dataset Ransalu Senanayake and Fabio Ramos, Bayesian Hilbert Maps for Continuous Occupancy Mapping in Dynamic Environments Abstract 7: Beyond Hand Labeling: Simulation and Self-Supervision for Self-Driving Cars (Matt Johnson, University of Michigan) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:00 AM7: Beyond Hand Labeling: Simulation and Self-Supervision for Self-Driving Cars (Matt Johnson, University of Michigan) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:00 AM Self-driving cars now deliver vast amounts of sensor data from large unstructured environments. In attempting to process and interpret this data there are many unique challenges in bridging the gap between prerecorded data sets and the field. This talk will present recent work addressing the application of deep learning techniques to robotic perception. We focus on solutions to several pervasive problems when attempting to deploy such techniques on fielded robotic systems. The themes of the talk revolve around alternatives to gathering and curating data sets for training. Are there ways of avoiding the labor-intensive human labeling required for supervised learning? These questions give rise to several lines of research based around self-supervision, adversarial learning, and simulation. We will show how these approaches applied to self-driving car problems have great potential to change the way we train, test, and validate machine learning-based systems. Bio: Matthew Johnson-Roberson is Assistant Professor of Engineering in the Department of Naval Architecture & Marine Engineering and the Department of Electrical Engineering and Computer Science at the University of Michigan. He received a PhD from the University of Sydney in 2010. He has held prior postdoctoral appointments with the Centre for Autonomous Systems CAS at KTH Royal Institute of Technology in Stockholm and the Australian Centre for Field Robotics at the University of Sydney. He is a recipient of the NSF CAREER award (2015). He has worked in robotic perception since the first DARPA grand challenge and his group focuses on enabling robots to better see and understand their environment. Abstract 8: Learning Affordance for Autonomous Driving (JianXiong Xiao, AutoX) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:30 AM Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception based approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. We evaluate our approach in a virtual racing game as well as real world driving and show that our model can work well to drive a car in a very diverse set of virtual and ",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
08636d97e46cf246bd0e15eaea7daa1196e0982f,https://www.semanticscholar.org/paper/08636d97e46cf246bd0e15eaea7daa1196e0982f,Theoretical Foundations of Reinforcement Learning,"s (1): Abstract 1: Introductory notes in Beyond first order methods in machine learning systems, 08:00 AM1: Introductory notes in Beyond first order methods in machine learning systems, 08:00 AM Introductory notes for the workshop Law & Machine Learning CÃ©line Castets-Renard, Sylvain Cussat-Blanc, Laurent Risser Fri Jul 17, 08:30 AM ICML 2020 Workshop book Generated Sun Jun 28, 2020 Page 4 of 37 Description: the workshop proposal in â€œLaw and Machine Learningâ€■ aims to contribute to the research on social and legal risks of the deployment of AI systems using machine learning based decisions. Today, algorithms have been infiltrating and governing every aspect of our lives as individuals and as a society. Specifically, Algorithmic Decision Systems (ADS) are involved in many social decisions. For instance, such systems are increasingly used to support decision-making in fields, such as child welfare, criminal justice, school assignment, teacher evaluation, fire risk assessment, homelessness prioritization, healthcare, Medicaid benefit, immigration decision systems or risk assessment, and predictive policing, among other things. Law enforcement agencies are increasingly using facial recognition, algorithmic predictive policing systems to forecast criminal activity and allocate police resources. However, these predictive systems challenge fundamental rights and guarantees of the criminal procedure. For several years, numerous studies have revealed, social risks of ML, especially the risks of opacity, bias, manipulation of information. While it is only the starting point of the deployment of such systems, more interdisciplinary research is needed. Our purpose is to contribute to this new field which brings together legal researchers, mathematicians and computer scientists, by bridging the gap between the performance of algorithmic systems and legal standards. For instance, notions like â€œprivacyâ€■ or â€œfairnessâ€■ are formulated in law, as well as mathematical definitions in computer science. However, the meaning and the impact of such requirements are not necessarily identical. Besides, legal norms to regulate AI systems appear in certain national laws but have to be relevant and compatible with technical requirements. Furthermore, these standards must be checked by legal experts and regulators, which presupposes that AI systems are sufficiently meaningful and transparent. These issues emerge in different topics, such as privacy in data analysis and fairness in algorithmic decision-making. The topic will cover the research that denounces the risks and, above all, multidisciplinary research that proposes solutions, especially legal and technical solutions.",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
912c58a110bced35af73a60593540096520287cc,https://www.semanticscholar.org/paper/912c58a110bced35af73a60593540096520287cc,Smart Intrusion Detection Model for the Cloud Computing,,,2017.0,10.1007/978-3-319-46568-5_42,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
817d86036006ed080bf4c28df68a23c7a9000457,https://www.semanticscholar.org/paper/817d86036006ed080bf4c28df68a23c7a9000457,Subdeterminant Maximization via Nonconvex Relaxations and Anti-Concentration,"Several fundamental problems that arise in optimization and computer science can be cast as follows: Given vectors v_1,...,v_m in R^d and a constraint family B of subsets of [m], find a set S in B that maximizes the squared volume of the simplex spanned by the vectors in S. A motivating example is the ubiquitous data-summarization problem in machine learning and information retrieval where one is given a collection of feature vectors that represent data such as documents or images. The volume of a collection of vectors is used as a measure of their diversity, and partition or matroid constraints over [m] are imposed in order to ensure resource or fairness constraints. Even with a simple cardinality constraint, the problem becomes NP-hard and has received much attention starting with a result by Khachiyan who gave an r^{O(r)} approximation algorithm for this problem. Recently, Nikolov and Singh presented a convex program and showed how it can be used to estimate the value of the most diverse set when there are multiple cardinality constraints (i.e., when B corresponds to a partition matroid). Their proof of the integrality gap of the convex program relied on an inequality by Gurvits, and was recently extended to regular matroids. The question of whether these estimation algorithms can be converted into the more useful approximation algorithms &#x2013; that also output a set &#x2013; remained open.The main contribution of this paper is to give the first approximation algorithms for both partition and regular matroids. We present novel formulations for the subdeterminant maximization problem for these matroids; this reduces them to the problem of finding a point that maximizes the absolute value of a nonconvex function over a Cartesian product of probability simplices. The technical core of our results is a new anti-concentration inequality for dependent random variables that arise from these functions which allows us to relate the optimal value of these nonconvex functions to their value at a random point. Unlike prior work on the constrained subdeterminant maximization problem, our proofs do not rely on real-stability or convexity and could be of independent interest both in algorithms and complexity where anti-concentration phenomena has recently been deployed.",2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS),2017.0,10.1109/FOCS.2017.98,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
62edbe9b1148f4874042aa2863c9795d5a678080,https://www.semanticscholar.org/paper/62edbe9b1148f4874042aa2863c9795d5a678080,DevOps - Preparing Students for Professional Practice,"This work in progress paper presents a course on DevOps which is a combination of software development skills and software operations skills. This new course is for sophomores and juniors in the computer science program who want to be prepared for professional software engineering careers. Introduction to DevOps Is a hands-on laboratory course that brings students through Git for source code management, Capybara for automated testing, AWS, Docker, and Ansible for automated virtual machine provisioning and configuration, and Jenkins for Continuous Integration. Unlike our current course offerings which primarily focus on the single developer context in a localized environment, this course prepares students for highly collaborative, team-based projects that use cloud resources to facilitate management of the software deployment pipeline. We developed this course based on feedback from our external advisory board and under consultation from a number of industrial partners. This is complementary to our current offerings in software engineering which focus on Agile software practices. In this paper we describe the core concepts, the design, learning experiences, technologies, and lessons learned through developing and conducting this course. In future work we hope to present student perceptions of learning and provide data collected through direct assessment of student outcomes.",2019 IEEE Frontiers in Education Conference (FIE),2019.0,10.1109/FIE43999.2019.9028598,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8f671c710996d49d65e9a9109923c41e01ce6fbb,https://www.semanticscholar.org/paper/8f671c710996d49d65e9a9109923c41e01ce6fbb,Software Architecture of a Learning Apprentice System in Medical Billing,"Machine learning is an emerging field of computer science concerned with the learning of knowledge from exploration of already stored data. However, effective utilization of extracted knowledge is an important issue. Extracted knowledge may be best utilized via feeding to knowledge based system. To this end, the work reported in this paper is based on a novel idea to enhance the productivity of the previously developed systems. This paper presents the proposed architecture of a Learning Apprentice System in Medical Billing system being developed for medical claim processing. A new dimension is added whereby, the process of extracting and utilization of knowledge are implemented in relational database environment for improved performance. It opens enormous application areas as most business data is in relational format managed by some relational database management server. The major components of the proposed system include knowledge base, rule engine, knowledge editor, and data mining module. Knowledge base consists of rules, meta rules and logical variables defined in the form of SQL queries stored in relational tables. Rule engine has been successfully developed and deployed in the form of SQL stored procedures. Knowledge editor and data mining modules are under development. Given architecture depicts over all business process of medical billing along with major components of the system. The proposed architecture effectively integrates all three pertinent components given by data mining (production rule discovery), rule based systems technology and database systems environment.",,2010.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4d7ba7116cc8b9fbf5638b3d6ce6092d5ca7dcdc,https://www.semanticscholar.org/paper/4d7ba7116cc8b9fbf5638b3d6ce6092d5ca7dcdc,Distributed C++-Python embedding for fast predictions and fast prototyping,"Python has evolved to become the most popular language for data science. It sports state-of-the-art libraries for analytics and machine learning, like Sci-Kit Learn. However, Python lacks the computational performance that a industrial system requires for high frequency real time predictions. Building upon a year long research project heavily based on SciKit Learn (sklearn), we faced performance issues in deploying to production. Replacing sklearn with a better performing framework would require re-evaluating and tuning hyperparameters from scratch. Instead we developed a python embedding in a C++ based server application that increased performance by up to 20x, achieving linear scalability up to a point of convergence. Our implementation was done for mainstream cost effective hardware, which means we observed similar performance gains on small as well as large systems, from a laptop to an Amazon EC2 instance to a high-end server.",DIDL@Middleware,2018.0,10.1145/3286490.3286560,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e1d12f651c3028be03b3b4e9f3eabbad9bba5f26,https://www.semanticscholar.org/paper/e1d12f651c3028be03b3b4e9f3eabbad9bba5f26,A Communication Paradigm Using Subvocalized Speech: Translating Brain Signals into Speech,,Augmented Human Research,2016.0,10.1007/S41133-016-0001-Z,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f257c24f1c655790913f26c58fe390d1f0bdadc7,https://www.semanticscholar.org/paper/f257c24f1c655790913f26c58fe390d1f0bdadc7,Biomarker imprecision in precision medicine,"Precision medicine is a conceptual model for disease taxonomy, formally devised in 2011, that extends the long-established clinical pathologic correlation disease model to include and emphasize genetics, genomics, and systems biology knowledge [1]. Precision medicine has the goal to provide more effective therapy for individuals guided by more precise diagnostics. This includes imaging, molecular diagnostics, and other ‘omics’ data as well as the analysis of large data sets by advanced informatics tools including datamining, machine learning, and other aspects of artificial intelligence. Accordingly, precision medicine is highly dependent on the adequacy of biomarkers used to classify disease, determine prognosis, guide treatment, and assess therapy response. Biomarkers for precision medicine are fraught with both conceptual and implementation challenges. Conceptually, precision medicine is predicated on ideas that application of the vast amount of genomics and other biochemical data generated by life sciences research over the past 60 years, with appropriate detection and data mining, can yield more precise diagnostics and therapies. So far, this hope has been only partially realized and critical reaction is now appearing [2–4]. Despite some important and well-documented successes particularly in cancer [5,6], increasingly, molecular pathways are usually discovered to be more complex upon further investigation. Patients exhibiting similar molecular patterns of disease continue to have a range of responses beyond what the molecular mechanisms would have predicted, and targeted drugs are often effective on only a portion of patients deemed susceptible and often only for short periods, rendering less precision to disease categories at high cost. Genomics has long been known to affect strongly drug responses in certain individuals and families but clinical applications of pharmacogenomics, a clear subject for early deployment of precision medicine, have been slow to develop because of practical implementation difficulties [7,8]. In part, the difficulties with the precision medicine model relates to its overemphasis on genomic changes as disease determinants and lack of adequate recognition that gene expression may or may not translate to protein expression. As well, physiologic reactions and adaptions to molecular changes that counteract specific gene and protein expression can affect disease phenotype profoundly. These difficulties, which impact adversely on biomarker clinical validity, are likely to be resolved only in part over time by advances in understanding basic biology. One promising approach, the Immunoscore, standardizes the assessment of immunologic reaction within the tumor as a prognostic indicator for colon cancer [9]. This suggests that the precision of precision medicine can be improved by assessing key indicators of both the disease itself and the reaction to disease [10,11]. Meanwhile, practical obstacles to biomarker implementation that relate to biomarker diagnostic reliability are the major factors that limit precision medicine advances at present. Many of these factors are addressable by care in applying laboratory diagnostic technologies. These factors include specimen acquisition, biomarker preservation, biomarker analysis, and biomarker informatics. Specimens for cell, genomic, proteomic, and metabolomic biomarkers are acquired either directly from affected tissue by biopsy usually needle biopsy, or indirectly from blood, ‘liquid biopsy.’ Both methods have limitations of specimen adequacy, and representativeness that limit diagnostic precision. While tissue biopsy provides localization, and with imaging, tissue selectivity, accessing deep tissue sites can be difficult, expensive, and present some patient risk. Further, it is likely that some biomarkers can be degraded during specimen acquisition by insertion force energy such as that induced by core biopsy guns [12]. Alternatively, while intrinsically less invasive, liquid biopsy can be imprecise if marker concentrations in blood are inadequate or not representative of the local lesion. Variable biomarker preservation is a major source of diagnostic imprecision which is often not sufficiently recognized. Specimen fixation for biomarkers is time dependent, tissue mass dependent as it takes time for fixative to penetrate issue, and analyte dependent. Each biomarker class, DNA, RNA, lipid, protein has different optimum fixation conditions. Particularly for retrospective studies, it is common to compromise fixation by using ultrasensitive probes that can detect and measure biomarkers in the common fixation process for histopathology, formalin fixed, paraffin embedding. Formalin aqueous solution degrades biomarkers as does protein crosslinking, subsequent dehydration, heat, and lipid removal. In this process, genomic markers can not only be lost but artefactual mutations generated [13]. Biomarker imprecision related to lack of analytic rigor is well known. However, the extensive requirement for controls",Expert review of molecular diagnostics,2018.0,10.1080/14737159.2018.1493379,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
dc1e2ca531d69e95c8618b7120731a5f70e7c5ab,https://www.semanticscholar.org/paper/dc1e2ca531d69e95c8618b7120731a5f70e7c5ab,Computational cognitive neuroscience,"This chapter provides an overview of the basic research strategies and analytic techniques deployed in computational cognitive neuroscience. On the one hand, “top-down” (or reverse-engineering) strategies are used to infer, from formal characterizations of behavior and cognition, the computational properties of underlying neural mechanisms. On the other hand, “bottom-up” research strategies are used to identify neural mechanisms and to reconstruct their computational capacities. Both of these strategies rely on experimental techniques familiar from other branches of neuroscience, including functional magnetic resonance imaging, single-cell recording, and electroencephalography. What sets computational cognitive neuroscience apart, however, is the explanatory role of analytic techniques from disciplines as varied as computer science, statistics, machine learning, and mathematical physics. These techniques serve to describe neural mechanisms computationally, but also to drive the process of scientific discovery by influencing which kinds of mechanisms are most likely to be identified. For this reason, understanding the nature and unique appeal of computational cognitive neuroscience requires not just an understanding of the basic research strategies that are involved, but also of the formal methods and tools that are being deployed, including those of probability theory, dynamical systems theory, and graph theory.",The Routledge Handbook of the Computational Mind,2018.0,10.4324/9781315643670-27,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
64249b2e25af8165947d5e6d1ed8e87b40585197,https://www.semanticscholar.org/paper/64249b2e25af8165947d5e6d1ed8e87b40585197,Data-Mining Driven Design for Novel Perovskite-type Piezoceramics,"Materials Genome Initiative is envisioning the discovery, development, manufacturing and deployment of advanced materials twice as fast and at a fraction of cost. High throughput computation and experimentation will generate big data, which underscores the emergence of the fourth paradigm data science. In contrast to machine-learning needing big-data, data-mining assisted by domain knowledge and expertise works well with a limited number of data. In this presentation, data-mining based on material genome approach were performed in field of perovskite-type oxides. New ferroelectric ceramics based on BiFeO3 for high temperature piezoelectric applications are realized with piezoresponse of 1.5~4.0 times the present commercial non-perovskite counterpart. Our essay demonstrates data-mining driven searching sure able to reduce time-to-insight and human effort on synthesization, accelerating new materials discovery and deployment.",2018 IEEE ISAF-FMA-AMF-AMEC-PFM Joint Conference (IFAAP),2018.0,10.1109/ISAF.2018.8463245,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7caff978ec23a8cedfdcec58fe2a362cb8506978,https://www.semanticscholar.org/paper/7caff978ec23a8cedfdcec58fe2a362cb8506978,EDUCATION IN PROCESS SYSTEMS ENGINEERING FOR PRESCRIPTIVE AND PREDICTIVE ANALYTICS IN INDUSTRY,"We present an initiative for education in Process System Engineering (PSE) covering industrial applications in both prescriptive and predictive analytics. Prescriptive analytics or decision-automation is the science of automating the decisionmaking of any physical system with respect to its design, planning, scheduling, control and operation using any combination of optimization, heuristic, machine-learning and cyber-physical algorithms. Predictive analytics or data-analytics is the science of examining raw data with the purpose of drawing conclusions on the behavior of the systems using data reconciliation and parameter estimation techniques within real-time optimization and control environments. Examples for beginner, intermediate and advanced levels guide the open-users of this educational platform in PSE to evolve toward more complex problems for research, development and deployment of industrial applications in the chemical engineering and multi-related fields.",Blucher Chemical Engineering Proceedings,2018.0,10.5151/COBEQ2018-CO.174,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f9f55f78d732323798c5b1b7aac4cac5cd737f80,https://www.semanticscholar.org/paper/f9f55f78d732323798c5b1b7aac4cac5cd737f80,Proposed Computational Classification System of Human Cognitive Biases,"Despite our aspirations to do so, we humans don't always make optimal or rational decisions. Researchers from psychology, behavioral economics, anthropology, decision sciences, and other related fields have described many human cognitive biases which help to explain such decisions. Most of the time, these cognitive biases are relatively harmless and relatively costless. However, sometimes they do result in significant costs to individuals, companies, governments and societies in the form of wasted or misdirected money, time, effort, and sometimes even in the form of lives lost. The antidote to such decisions has long been recognized to lie in algorithmic decision making. Until relatively recently, though, requirements and complexity of such algorithms have limited their deployment in real-world situations. However, we now enjoy a convergence of computing power, decrease in computing costs, and computational and predictive methods born of data science, artificial intelligence (AI), and machine learning (ML), such that we can begin to mitigate some of the most negative effects of some of these cognitive biases. This paper proposes a method for classifying these human cognitive biases for purposes of mitigation by means of computing methods, describes some of these biases that are most ripe for mitigation through computing, and proposes future research directions that build upon this work.",2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI),2018.0,10.1109/WI.2018.00013,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
25b32197d49fed476fdbb3099c84550d6f84c42f,https://www.semanticscholar.org/paper/25b32197d49fed476fdbb3099c84550d6f84c42f,"Snapshot Serengeti, high-frequency annotated camera trap images of 40 mammalian species in an African savanna",,Scientific data,2015.0,10.1038/sdata.2015.26,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0419ba0310ac083bdb277238c5800a059ccd142c,https://www.semanticscholar.org/paper/0419ba0310ac083bdb277238c5800a059ccd142c,The introduction of laptops in the teaching of mathematics and science in English under the Teaching and Learning of Science and Mathematics in English Programme ( Pengajaran dan,"The introduction of laptops in the teaching of mathematics and science in English under the Teaching and Learning of Science and Mathematics in English Programme (Pengajaran dan Pembelajaran Sains dan Matematik dalam Bahasa Inggeris, PPSMI) has been implemented by the Ministry of Education since 2003. The preliminary observations found that teachers are not fully utilising these facilities in their teaching. A survey was conducted to study the barriers preventing the integration and adoption of information and communication technology (ICT) in teaching mathematics. Six major barriers were identified: lack of time in the school schedule for projects involving ICT, insufficient teacher training opportunities for ICT projects, inadequate technical support for these projects, lack of knowledge about ways to integrate ICT to enhance the curriculum, difficulty in integrating and using different ICT tools in a single lesson and unavailability of resources at home for the students to access the necessary educational materials. To overcome some of these barriers, this paper proposes an e-portal for teaching mathematics. The e-portal consists of two modules: a resource repository and a lesson planner. The resource repository is a collection of mathematical tools, a question bank and other resources in digital form that can be used for teaching and learning mathematics. The lesson planner is a user friendly tool that can integrate resources from the repository for lesson planning. INTRODUCTION During the 2003 budget speech, the then Prime Minister of Malaysia, YAB Dato Seri Dr. Mahathir Mohamad, announced that the government had decided to implement the teaching of science and mathematics using the English language as the medium of instruction in school. The government allocated about 5 billion Ringgit from 2002 to 2008 for the above project. This allocation was to be used for teacher training, providing launching grants for schools as well as for educational aids which would include ICT equipment. In order to implement the project smoothly, the sum of 978.7 million Ringgit was spent in the year 2003 to purchase notebook computers, LCD projectors and other related equipment (Mahathir, 2002). Ittigson & Zewe (2003) cited that technology is essential in teaching and learning mathematics. ICT improves the way mathematics should be taught and enhances student understanding of basic concepts. Many researchers have carried out studies to evaluate the benefits of using ICT in mathematics. Becta (2003) summarised the key benefits – ICT promotes greater collaboration among students and encourages communication and the sharing of knowledge. ICT gives rapid and accurate feedbacks to students and this contributes towards positive motivation. It also allows them to focus on strategies and interpretations of answers rather than spend time on tedious computational calculations. ICT also supports constructivist pedagogy, wherein students use technology to explore and reach an understanding of mathematical concepts. This approach promotes higher order thinking and better problem solving strategies which are in line with the recommendations forwarded by the National Council of Teachers of Mathematics (NCTM); students would then use technology to concentrate on problem-solving processes rather than on calculations related to the problems (Ittigson & Zewe, 2003). MOJIT A Study on the Use of ICT in Mathematics Teaching 44 For a successful integration of ICT into the mathematics curriculum, it is essential to have knowledge of the existing software that is used by mathematics teachers. A survey carried out by Forgasz & Prince (2002) found that 61% of the respondents (teachers) used spreadsheets, 45% used word processing and 30% used Internet browsers. In the same survey, it was found that 19% used Geometer’s sketchpads, 19% used CD-ROMs that accompanied mathematics textbooks, 18% used Graphmatica, 14% used Maths Blaster and 8% used other mathematics-specific software. Knowledge of the use of software on the part of the teachers is not the only criterion for integrating ICT into mathematics lessons; a sound pedagogical knowledge on how to integrate it is another critical success factor. In a separate study, Jones (2004) found that seven barriers existed while integrating ICT into lessons. These barriers were (i) lack of confidence among teachers during integration (21.2% responses), (ii) lack of access to resources (20.8%), (iii) lack of time for the integration (16.4%), (iv) lack of effective training (15.0%), (v) facing technical problems while the software is in use (13.3%), (vi) lack of personal access during lesson preparation (4.9%) and (vii) the age of the teachers (1.8%). An analysis of the preventive maintenance record in one of the rural secondary schools revealed that the use of ICT equipment was too low, despite a large sum of public funds being used for the purchase. It was found that although nine LCD projectors were procured by the school, only six were available at the time of inspection. The total number of operation hours recorded by the machines was 174 hours for a period of two years. On average, each projector was then only used for about 29 hours in the two year duration which is considered to be very low (MHS, 2005). The main objective of this study was to help mathematics teachers in the integration of ICT into their teaching. The study aimed at identifying the most common ICT applications used by these teachers and how ICT was used in the class. It also aimed at understanding how the Internet was used by teachers, analysed their training needs and further assessed the level of ICT usage in instructional programmes. The barriers faced by teachers during the integration of ICT into mathematics lessons and their perception of the usefulness of an e-portal were also investigated. METHODOLOGY This research deployed a survey method to investigate the use of ICT and the barriers of integrating ICT into the teaching of mathematics. The survey was carried out during a mathematics in-service course conducted by the State Education Department. Before the commencement of the survey, the respondents were given a briefing on the purpose of the survey. A total of 111 responses was received and they were analysed using the SPSS statistical package. A questionnaire was adapted from the Teacher Technology Survey by the American Institute for Research (AIR, 1998). The questionnaire was divided into seven areas, i.e., (A) the teacher’s profile, (B) how teachers use ICT, (C) professional development activities, (D) the teacher’s ICT experience, (E) the level of use in ICT, (F) the barriers faced by teachers and (G) the proposed solution. RESULTS AND DISCUSSION ICT applications in general In general, a total of 71.1% of the respondents used computers on a regular basis. Table 1 depicts the percentage of usage by teachers in the various ICT applications: word processing packages (71.1%), spreadsheets (51.2%), Internet activity (44.1%), search engines (44.1%), presentation software (36.9%), drill and practice (24.3%), hypermedia/multimedia (22.5%), databases (21.6%), graphical applications (19.8%), simulation programmes (17.1%), desktop publishing (12.6%), Flash presentations (11.7%) and Java applets (6.3%). These percentages show that the computer literacy rate among secondary school mathematics teachers has been high. MOJIT A Study on the Use of ICT in Mathematics Teaching 45 Table 1: Common ICT Applications by Teachers Application Daily (%) Weekly (%) Monthly (%) 1 or 2 times a year (%) Never (%) NA (%) NR (%) Computers in general 25.2 22.5 23.4 17.1 2.7 3.6 5.4 Word processing packages 21.6 27.0 22.5 9.9 5.4 3.6 9.9 Spreadsheets 9.8 22.5 18.9 22.5 11.7 3.6 10.8 Databases 2.7 5.4 13.5 18.0 30.6 11.7 18.0 Graphical applications 1.8 6.3 11.7 20.7 27.9 9.9 21.6 Presentation software 2.7 12.6 21.6 28.8 18.9 6.3 9.0 Desktop publishing 1.8 3.6 7.2 18.9 38.7 8.1 21.6 Any Internet activity 12.6 13.5 18.0 20.7 18.9 5.4 10.8 Search engines for Internet 12.6 14.4 17.1 20.7 18.0 6.3 10.8 Hypermedia / Multimedia 3.6 8.1 10.8 18.9 32.4 8.1 18.0 Simulation programmes 0.9 3.6 12.6 12.6 38.7 10.8 20.7 Drill / Practice tutorials 1.8 6.3 16.2 16.2 34.2 9.0 16.2 Java applets 1.8 0.0 4.5 9.9 50.5 17.1 16.2 Flash presentations 1.8 0.9 9.0 17.1 45.0 12.6 13.5 NA – Not Available NR – No Response ICT applications in class 49.5% of the respondents used courseware in the class, 40.5% used ICT as presentation tools , 8.1% used ICT as a graphical visualising tool, 6.3% used ICT as an online demonstration tool and 3.6% used it for other purposes in class. About 29.7% of the respondents did not use ICT in the classroom. Table 2 shows the percentage distribution of ICT uses in the class Table 2: ICT Uses in the Class Application Using presentation tools Using courseware Using graphical visualising tools Online demos Others None Response (%) 40.5 49.5 8.1 6.3 3.6 29.7 Uses of Internet The Internet was used for various purposes. 68.5% respondents used it for browsing, 44.1% used the e-mail facility, 10.8% used chat rooms, 9.9% used IRC, 7.2% used it in discussion forums and 1.8% for other purposes. 17.1% respondents did not use the Internet. Table 3 depicts the details. Table 3: Use of Internet by Teachers Activity Browsing e-mail IRC Discussion forums Chat rooms Others None Response (%) 68.5 44.1 9.9 7.2 10.8 1.8 17.1 Professional development and training needs A total of 42.3% respondents indicated that they had received ICT training during 2002-2004. 71.2% of the respondents demonstrated that they had found the training to be generally useful while 64.9% said that they had not received training on how to integrate ICT into mathematics teaching. According to 33.3% of the respondents, mathematics teachers require training on how to integrate ICT into their teaching while 59.5% of them stated that they needed a combination of various types of training. Table 4 depicts the",,2005.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b52a60ec1de00a81b6d09155f4587daa65fa067d,https://www.semanticscholar.org/paper/b52a60ec1de00a81b6d09155f4587daa65fa067d,Waggle: An open sensor platform for edge computing,"Many advanced sensors are capable of producing extremely large and continuous data streams. Hyperspectral imagers, microphones, high-resolution cameras, and 3D scanning devices can easily generate gigabytes of data per day, making it impractical for many wireless sensor platforms to stream all collected data to the cloud for analysis. Furthermore, in some sensor deployments, privacy concerns may restrict the resolution or content of data leaving the devices and being routinely stored in the cloud. To address this situation, sensor platforms must reduce or transform the data in situ, sending only the analyzed results to a central server. In designing an open sensor platform capable of leveraging advances in machine learning to support edge computing, several challenges arise, including resilience, performance isolation, and data privacy. This paper describes the architecture of the Waggle platform developed at Argonne National Laboratory. As an open platform, Waggle supports a wide range of sensors, including experimental sensors to measure airborne pollutants such as hydrogen sulfide and ozone, as well as cameras intended to detect urban flooding and automobile traffic. The Waggle platform is used by the Array of Things, a National Science Foundation project to deploy 500 sensor platforms in the city of Chicago, beginning in mid-2016.",2016 IEEE SENSORS,2016.0,10.1109/ICSENS.2016.7808975,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8603d37a2a46688c4c6f4466788fcdca56d238b5,https://www.semanticscholar.org/paper/8603d37a2a46688c4c6f4466788fcdca56d238b5,Connectionist Theories of Learning.,,,2011.0,10.1007/springerreference_302304,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9b678b0fe7f467b726a33dce33a1b84396372945,https://www.semanticscholar.org/paper/9b678b0fe7f467b726a33dce33a1b84396372945,The social lives of generative adversarial networks,"Generative adversarial networks (GANs) are a genre of deep learning model of significant practical and theoretical interest for their facility in producing photorealistic 'fake' images which are plausibly similar, but not identical, to a corpus of training data. But from the perspective of a sociologist, the distinctive architecture of GANs is highly suggestive. First, a convolutional neural network for classification, on its own, is (at present) popularly considered to be an 'AI'; and a generative neural network is a kind of inversion of such a classification network (i.e. a layered transformation from a vector of numbers to an image, as opposed to a transformation from an image to a vector of numbers). If, then, in the training of GANs, these two 'AIs' interact with each other in a dyadic fashion, shouldn't we consider that form of learning... social? This observation can lead to some surprising associations as we compare and contrast GANs with the theories of the sociologist Pierre Bourdieu, whose concept of the so-called habitus is one which is simultaneously cognitive and social: a productive perception in which classification practices and practical action cannot be fully disentangled. Bourdieu had long been concerned with the reproduction of social stratification: his early works studied formal public schooling in France not as an egalitarian system but instead as one which unintentionally maintained the persistence of class distinctions. It was, he argued, through the cultural inculcation of an embodied and partially unconscious habitus---a ""durably installed generative principle of regulated improvisations""---that, he argued, students from the upper classes are given an advantage which is only further reinforced throughout their educational trajectories. For Bourdieu, institutions of schooling instill ""deeply interiorized master patterns"" of behavior and thought (and classification) which in turn direct the acquisition of subsequent patterns, whose character is determined not simply by this cognitive layering but by their actual use in lived practice, especially early in childhood development. In this work I develop a productive analogy between the GAN architecture and Bourdieu's habitus, in three ways. First, I call attention to the fact that connectionist approaches and Bourdieu's theories were both conceived as revolts against rule-bound paradigms. In the 1980s, Rumelhart and McClelland used a multilayer neural network to learn the phonology of English past-tense verbs because ""sometimes we don't follow the rules... language is full of exceptions to the rules""; and in the case of Bourdieu, the habitus was an answer to a long-standing question: ""how can behaviour be regulated without being the product of obedience to rules?"" Bourdieu strove to transgress what was then seen in the social sciences as a conceptual opposition between structure-based theories of social life and those which emphasized an embodied agency. Second, I suggest that concerns about bias and discrimination in machine learning in recent years can in part be attributed due to the increased use of ML models not just for static classification but for practical action. Similarly, the habitus for Bourdieu is simultaneously durable and transposable: its judgments may be relatively stable, but are capable of being deployed dynamically in novel and varying social situations---or what ML practitioners might call generalizability. We can thus theorize generative models (including GANs) as biased not just in their stereotyped classifications, but through their potential for actively generating new biased data. These generated actions then recursively become part of the social arena Bourdieu called the field, into which new agents are 'born' and for which they may know few alternatives. Finally, it is intriguing that GAN researchers and Bourdieu both extensively use metaphors from game theory. Goodfellow described the GAN architecture as a ""two-player minimax game with value function V(G,D)"", meaning that there is a single abstract function whose output value the discriminator is trying to maximize and which the generator is trying to minimize; but the dynamic nature of the GAN training process means that convergence to Nash equilibrium is nontrivial. But for Bourdieu, such a utility-based approach to artistic creation could not be more crude when compared to the social reality of art worlds: utilitarianism is, for him, ""the degree zero of sociology"", by which he means an isolated, inert, and amodal---and therefore not particularly sociological---starting point. Moreover, 19th-century bohemian culture was characterized primarily by its inversion of financial incentives, in which failure is a kind of success, and ""selling out"" (i.e. maximizing profit) worst of all; and thus the relentless optimization of neural networks may be fundamentally at odds with the ""value functions"" of many human artists. I conclude that deep learning, while primarily understood as a scientific and technical achievement, may also intentionally or unintentionally constitute a nascent, independent reinvention of social theory.",FAT*,2020.0,10.1145/3351095.3373156,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3d8b1c2a8d2c7dc70db3eebf638ce5826444639b,https://www.semanticscholar.org/paper/3d8b1c2a8d2c7dc70db3eebf638ce5826444639b,Mosquito detection with low-cost smartphones: data acquisition for malaria research,"Mosquitoes are a major vector for malaria, causing hundreds of thousands of deaths in the developing world each year. Not only is the prevention of mosquito bites of paramount importance to the reduction of malaria transmission cases, but understanding in more forensic detail the interplay between malaria, mosquito vectors, vegetation, standing water and human populations is crucial to the deployment of more effective interventions. Typically the presence and detection of malaria-vectoring mosquitoes is only quantified by hand-operated insect traps or signified by the diagnosis of malaria. If we are to gather timely, large-scale data to improve this situation, we need to automate the process of mosquito detection and classification as much as possible. In this paper, we present a candidate mobile sensing system that acts as both a portable early warning device and an automatic acoustic data acquisition pipeline to help fuel scientific inquiry and policy. The machine learning algorithm that powers the mobile system achieves excellent off-line multi-species detection performance while remaining computationally efficient. Further, we have conducted preliminary live mosquito detection tests using low-cost mobile phones and achieved promising results. The deployment of this system for field usage in Southeast Asia and Africa is planned in the near future. In order to accelerate processing of field recordings and labelling of collected data, we employ a citizen science platform in conjunction with automated methods, the former implemented using the Zooniverse platform, allowing crowdsourcing on a grand scale.",ArXiv,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
785ff63be35cfa2d65b4627b9eda319c4332e845,https://www.semanticscholar.org/paper/785ff63be35cfa2d65b4627b9eda319c4332e845,Artificial Intelligence to Manage Network Traffic of 5 G Wireless Networks,"The deployment of the fifth generation (5G) wireless communication systems is projected for 2020. With new scenarios, new technologies, and new network architectures, the traffic management for 5G networks will present significant technical challenges. In recent years, artificial intelligence (AI) technologies, especially machine learning (ML) technologies, have demonstrated significant successes in many application domains, suggesting its potential in helping to solve the problem of 5G traffic management. In this article, we investigate the new characteristics of 5G wireless network traffic, and discuss challenges they present for 5G traffic management. Potential solutions and research directions for the management of 5G traffic, including distributed and light-weight ML algorithms and a novel AI assistant content retrieval algorithm framework are discussed. Y. Fu, S. Wang, C.-X. Wang (corresponding author), and S. McLaughlin are with the Institute of Sensors, Signals and Systems, School of Engineering and Physical Sciences, Heriot-Watt University, Edinburgh, EH14 4AS, UK. (E-mail: {y.fu, s.wang, cheng-xiang.wang, s.mclaughlin}@hw.ac.uk). X. Hong is with the Key Laboratory of Underwater Acoustic Communication and Marine Information Technology, Ministry of Education of China, Xiamen University, Xiamen 361005, Fujian, P.R. China. Email: xuemin.hong@xmu.edu.cn. IEEE NETWORK, VOL. XX, NO. YY, MONTH 2018 1",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
49ad4917e194642a76b507776940bf8091f2b70d,https://www.semanticscholar.org/paper/49ad4917e194642a76b507776940bf8091f2b70d,How to translate artificial intelligence? Myths and justifications in public discourse,"Automated technologies populating today’s online world rely on social expectations about how “smart” they appear to be. Algorithmic processing, as well as bias and missteps in the course of their development, all come to shape a cultural realm that in turn determines what they come to be about. It is our contention that a robust analytical frame could be derived from culturally driven Science and Technology Studies while focusing on Callon’s concept of translation. Excitement and apprehensions must find a specific language to move past a state of latency. Translations are thus contextual and highly performative, transforming justifications into legitimate claims, translators into discursive entrepreneurs, and power relations into new forms of governance and governmentality. In this piece, we discuss three cases in which artificial intelligence was deciphered to the public: (i) the Montreal Declaration for a Responsible Development of Artificial Intelligence, held as a prime example of how stakeholders manage to establish the terms of the debate on ethical artificial intelligence while avoiding substantive commitment; (ii) Mark Zuckerberg’s 2018 congressional hearing, where he construed machine learning as the solution to the many problems the platform might encounter; and (iii) the normative renegotiations surrounding the gradual introduction of “killer robots” in military engagements. Of interest are not only the rational arguments put forward, but also the rhetorical maneuvers deployed. Through the examination of the ramifications of these translations, we intend to show how they are constructed in face of and in relation to forms of criticisms, thus revealing the highly cybernetic deployment of artificial intelligence technologies.",,2020.0,10.1177/2053951720919968,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a812368fe1d4a186322bf72a6d07e1cf60067234,https://www.semanticscholar.org/paper/a812368fe1d4a186322bf72a6d07e1cf60067234,Gaussian processes for modeling of facial expressions,"Automated analysis of facial expressions has been gaining significant attention over the past years. This stems from the fact that it constitutes the primal step toward developing some of the next-generation computer technologies that can make an impact in many domains, ranging from medical imaging and health assessment to marketing and education. No matter the target application, the need to deploy systems under demanding, realworld conditions that can generalize well across the population is urgent. Hence, careful consideration of numerous factors has to be taken prior to designing such a system. The work presented in this thesis focuses on tackling two important problems in automated analysis of facial expressions: (i) view-invariant facial expression analysis; (ii) modeling of the structural patterns in the face, in terms of well coordinated facial muscle movements. Driven by the necessity for efficient and accurate inference mechanisms we explore machine learning techniques based on the probabilistic framework of Gaussian processes (GPs). Our ultimate goal is to design powerful models that can efficiently handle imagery with spontaneously displayed facial expressions, and explain in detail the complex configurations behind the human face in real-world situations. To effectively decouple the head pose and expression in the presence of large outof-plane head rotations we introduce a manifold learning approach based on multi-view learning strategies. Contrary to the majority of existing methods that typically treat the numerous poses as individual problems, in this model we first learn a discriminative manifold shared by multiple views of a facial expression. Subsequently, we perform facial expression classification in the expression manifold. Hence, the pose normalization problem is solved by aligning the facial expressions from different poses in a common latent space. We demonstrate that the recovered manifold can efficiently generalize to various poses and expressions even from a small amount of training data, while also being largely robust to corrupted image features due to illumination variations. State-of-the-art performance is achieved in the task of facial expression classification of basic emotions. The methods that we propose for learning the structure in the configuration of the muscle movements represent some of the first attempts in the field of analysis and intensity estimation of facial expressions. In these models, we extend our multi-view approach to exploit relationships not only in the input features but also in the multi-output labels. The structure of the outputs is imposed into the recovered manifold either from heuristically defined hard constraints, or in an auto-encoded manner, where the structure is learned automatically from the input data. The resulting models are proven to be robust to data with imbalanced expression categories, due to our proposed Bayesian learning of the target manifold. We also propose a novel regression approach based on product of GP experts where we take into account people’s individual expressiveness in order to adapt the learned models on each subject. We demonstrate the superior performance of our proposed models on the task of facial expression recognition and intensity estimation.",,2016.0,10.25560/44106,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e6d9121158ae0377ef1dcb6c62f391ce0c112bd2,https://www.semanticscholar.org/paper/e6d9121158ae0377ef1dcb6c62f391ce0c112bd2,Predictive Analysis for the Arbovirus-Dengue using SVM Classification,"Data mining in biology and medicine is a core component of biomedical informatics, and one of the first intensive applications of computer science to this field. Today’s biomedical data mining appears more multifaceted with advances in knowledge discovery in databases as well as machine learning approaches. This paper explores the application of machine learning techniqueSVM for the identification of one of the Arboviral disease – Dengue. This paper reports novel biological discovery through nontrivial data mining process by using existing computational techniques. The goal of the system is to support the collection, and retrieval of public health documents, data, learning objects, and tools. We have deployed this generic infrastructure to facilitate data integration and knowledge sharing in the domain of dengue, which is one of the most prevalent viral diseases. This paper proposed an effort to apply the svm classification with the Radial basis function to classify the viral data and the model exhibits highly precise prediction rate.",,2012.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
59d4fe962e34ea2b80c55b5ffbfc000cbff61de1,https://www.semanticscholar.org/paper/59d4fe962e34ea2b80c55b5ffbfc000cbff61de1,"Real‐Time Orbital Image Analysis Using Decision Forests, with a Deployment Onboard the IPEX Spacecraft","Automatic cloud recognition promises significant improvements in Earth science remote sensing. At any time, more than half of Earth's surface is covered by clouds, obscuring images and atmospheric measurements. This is particularly problematic for CubeSats, a new generation of small, low‐orbiting spacecraft with very limited communications bandwidth. Such spacecraft can use image analysis to autonomously select clear scenes for prioritized downlink. More agile spacecraft can also benefit from cloud screening by retargeting observations to cloud‐free areas. This could significantly improve the science yield of instruments such as the Orbiting Carbon Observatory 3 mission. However, most existing cloud detection algorithms are not suitable for these applications, because they require calibrated and georectified spectral data, which is not typically available onboard. Here, we describe a statistical machine‐learning method for real‐time autonomous scene interpretation using a visible camera with no radiometric calibration. A random forest classifies cloud and clear pixels based on local patterns of image texture. We report on experimental evaluation of images from the International Space Station (ISS) and present results from a deployment onboard the IPEX spacecraft. This demonstrates actual execution in flight and provides some preliminary lessons learned about operational use. It is a rare example of a machine‐learning system deployed to an autonomous spacecraft. To our knowledge, it is also the first instance of significant artificial intelligence deployed on board a CubeSat and the first ever deployment of visible image‐based cloud screening onboard any operational spacecraft.",J. Field Robotics,2016.0,10.1002/rob.21627,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
614bbef4a5f1253c14e5ce17c5bae8ef0be7fa6c,https://www.semanticscholar.org/paper/614bbef4a5f1253c14e5ce17c5bae8ef0be7fa6c,Deploying Graph Algorithms on GPUs: An Adaptive Solution,"Thanks to their massive computational power and their SIMT computational model, Graphics Processing Units (GPUs) have been successfully used to accelerate a wide variety of regular applications (linear algebra, stencil computations, image processing and bioinformatics algorithms, among others). However, many established and emerging problems are based on irregular data structures, such as graphs. Examples can be drawn from different application domains: networking, social networking, machine learning, electrical circuit modeling, discrete event simulation, compilers, and computational sciences. It has been shown that irregular applications based on large graphs do exhibit runtime parallelism; moreover, the amount of available parallelism tends to increase with the size of the datasets. In this work, we explore an implementation space for deploying a variety of graph algorithms on GPUs. We show that the dynamic nature of the parallelism that can be extracted from graph algorithms makes it impossible to find an optimal solution. We propose a runtime system able to dynamically transition between different implementations with minimal overhead, and investigate heuristic decisions applicable across algorithms and datasets. Our evaluation is performed on two graph algorithms: breadth-first search and single-source shortest paths. We believe that our proposed mechanisms can be extended and applied to other graph algorithms that exhibit similar computational patterns.",2013 IEEE 27th International Symposium on Parallel and Distributed Processing,2013.0,10.1109/IPDPS.2013.101,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9a762006e2e410c29d8b2d17145fcd9d7f3b44b6,https://www.semanticscholar.org/paper/9a762006e2e410c29d8b2d17145fcd9d7f3b44b6,Structural Health Monitoring and Prognostic of Industrial Plants and Civil Structures: A Sensor to Cloud Architecture,"The deployment of Structural Health Monitoring (SHM) systems is a natively interdisciplinary task that involves joint research contributions from sensing technologies, data science and civil engineering. The capability to assess, also from remote stations, the working conditions of industrial plants or the structural integrity of civil buildings is widely requested in many application fields. The technological development aims to continuously provide innovative tools and approaches to satisfy these demands. As a first instance, reliable monitoring strategies are needed to detect structural damages while filtering out environmental noise. Ongoing solutions to tackle these topics are based on the exploitation of highly customized sensing technologies, such as shaped transducers for Acoustic Emission (AE) testing or Micro-Electro-Mechanical System (MEMS) accelerometers for Operational Modal Analysis (OMA) [1]. On the other hand, effective data acquisition and storage techniques must be employed to cope with the heterogeneity of the sensing devices and with the amount of data produced by collecting raw measured signals. Finally, damage detection and prediction tasks should be computed via data-driven algorithms that can complement the model-based alternatives traditionally used in civil engineering. Layered SHM architectures [2] represent straightforward approaches to address the system complexity originated by this interdisciplinary design; however, few real-world implementations have been presented so far in the literature. In this paper, we overcome these limitations by presenting an Internet of Things (IoT)-based SHM architecture for the predictive maintenance of industrial sites and civil engineering structures and infrastructures. The proposed cyber-physical system includes a monitoring layer, that consists of accelerometer-based sensor networks, a data acquisition layer, built on the recent W3C Web of Things standard [3], and a data storage and analytics layer, which leverages distributed database and Machine Learning tools. We extensively discuss the hardware/software components of the proposed SHM architecture, by stressing its advantages in terms of device versatility, data scalability and interoperability support. Finally, the effectiveness of the system is validated on a real-world use-case, i.e., the monitoring of a metallic frame structure located at the SHM research labs of the University of Bologna, Italy, within the MAC4PRO project [4].",IEEE Instrumentation & Measurement Magazine,2020.0,10.1109/MIM.2020.9289069,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b00106a36f5009ec4ee70ff02ec004dbe83d7438,https://www.semanticscholar.org/paper/b00106a36f5009ec4ee70ff02ec004dbe83d7438,Measuring operational quality of recommendations: industry talk abstract,"With the rise of machine learning in production, we need to talk about operational data science. The talk introduces a pragmatic method on how to measure the response quality of a recommendation service. To that end the definition of a successful response is introduced and guidelines how to capture the rate of successful responses are presented. There are several changes that can happen during the serving phase of a model which negatively affect the quality of the algorithmic response. A few examples are: • The model is updated and the new version is inferior to the previous one. • The latest deployment of the stack that processes the request and serves the model contains a bug. • Changes in the infrastructure lead to performance loss. An example in an e-commerce setting is switching to a different microservice to obtain article metadata used for filtering the recommendations. • The input data changes. Typical reasons might be a client application that releases a bug (e.g., lowercasing a case sensitive identifier) or changes a feature in a way that affects the data distribution such as allowing all users to use the product cart instead of previously allowing it only for logged in users. If the change is not detected training data and serving data diverge. Current monitoring solutions mostly focus on the completion of a request without errors and the request latency. That means the mentioned examples would be hard to detect despite the response quality being significantly degraded, sometimes permanently. In addition to not being able to detect the mentioned changes, it can be argued that current monitoring practices are not sufficient to capture the performance of a recommender system or any other data driven service in a meaningful way. We might for instance have returned popular articles as a fallback in a case where personalized recommendations were requested. We should record that response as unsuccessful. A new paradigm for measuring response quality should fulfil the following criteria: • comparable across models • simple and understandable metrics • measurements are collected in real time • allows for actionable alerting on problems The response quality is defined as an approximation of how well the response fits the defined business and modelling case. The goal is to bridge the gap between metrics used during model learning and technical monitoring metrics. Ideally we would like to obtain Service Level Objectives (SLO)[1] that contain this quality aspect and can be discussed with the different client applications based on the business cases, e.g., ""85% of the order confirmation emails contain personalized recommendations based on the purchase."" A case study will illustrate how algorithmic monitoring was introduced in the recommendation team at Zalando. Zalando is one of Europe's largest fashion retailers and multiple recommendation algorithms serve many online and offline use cases. You will see several examples of how the monitoring helped to identify bugs or diagnose quality problems.",RecSys,2018.0,10.1145/3240323.3241725,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b2ba38af34c19c933be605b5552b8db4cab73ecd,https://www.semanticscholar.org/paper/b2ba38af34c19c933be605b5552b8db4cab73ecd,Introduction,"Computational propaganda is an emergent form of political manipulation that occurs over the Internet. The term describes the assemblage of social media platforms, autonomous agents, algorithms, and big data tasked with manipulating public opinion. Our research shows that this new mode of interrupting and influencing communication is on the rise around the globe. Advances in computing technology, especially around social automation, machine learning, and artificial intelligence, mean that computational propaganda is becoming more sophisticated and harder to track. This introduction explores the foundations of computational propaganda. It describes the key role of automated manipulation of algorithms in recent efforts to control political communication worldwide. We discuss the social data science of political communication and build upon the argument that algorithms and other computational tools now play an important political role in news consumption, issue awareness, and cultural understanding. We unpack key findings of the nine country case studies that follow—exploring the role of computational propaganda during events from local and national elections in Brazil to the ongoing security crisis between Ukraine and Russia. Our methodology in this work has been purposefully mixed, using quantitative analysis of data from several social media platforms and qualitative work that includes interviews with the people who design and deploy political bots and disinformation campaigns. Finally, we highlight original evidence about how this manipulation and amplification of disinformation is produced, managed, and circulated by political operatives and governments, and describe paths for both democratic intervention and future research in this space.",Oxford Scholarship Online,2018.0,10.1093/oso/9780190931407.003.0001,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
966da1d19db605f438c6cec6e5e3573d5a63f517,https://www.semanticscholar.org/paper/966da1d19db605f438c6cec6e5e3573d5a63f517,Image Recognition in Wildlife Applications,,,2018.0,10.1007/978-3-319-96978-7_14,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5b0ffc40f91580ec9b5cff2b2e27b51b6f1a6c24,https://www.semanticscholar.org/paper/5b0ffc40f91580ec9b5cff2b2e27b51b6f1a6c24,Lawrence Berkeley National Laboratory Recent Work Title Accelerating the discovery of materials for clean energy in the era of smart automation Permalink,"| The discovery and development of novel materials in the field of energy are essential to accelerate the transition to a lowcarbon economy. Bringing recent technological innovations in automation, robotics and computer science together with current approaches in chemistry , materials synthesis and characterization will act as a catalyst for revolutionizing traditional research and development in both industry and academia. This Perspective provides a vision for an integrated artificial intelligence approach towards autonomous materials discovery , which, in our opinion, will emerge within the next 5 to 10 years. The approach we discuss requires the integration of the following tools, which have already seen substantial development to date: highthroughput virtual screening, automated synthesis planning, automated laboratories and machine learning algorithms. In addition to reducing the time to deployment of new materials by an order of magnitude, this integrated approach is expected to lower the cost associated with the initial discovery. Thus, the price of the final products (for example, solar panels, batteries and electric vehicles) will also decrease. This in turn will enable industries and governments to meet more ambitious targets in terms of reducing greenhouse gas emissions at a faster pace. volume 3 | mAY 2018 | 5 PERSPECTIVES © 2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. NAture reviews | MAtEriAlS First, we briefly discuss advances in AI and then provide an overview of the main applications of materials for the clean energy sector. Next, we explore stateof-theart automated procedures for materials discovery, with a focus on machine learning. The field of organic materials is the farthest along in many of the areas required for an integrated platform, but along the way, we point out some of the notable advances in both inorganic materials and nanomaterials. Finally, we conclude and provide our vision for the next generation of integrated AI approaches towards autonomous materials discovery, which will emerge within the next 5–10 years. Advances in AI Scientific discoveries are usually associated with an insight: the act of intuitively seeing a phenomenon, which contrasts with systematic mechanistic learning. This is despite the fact that most of our discoveries are based on extensive preliminary studies. Insight is considered to be an exclusively human attribute, whereas systematic exploration is connected with automated platforms. However, this gap between a creative and intuitive targeted search and a systematic exploration continues to narrow as automated platforms become increasingly sophisticated and are able to process more complex information17,18. In addition to systematic screenings of large databases and building chemical structures according to a set of preprogramed rules, today's platforms can update the rules for analysing the available information and even search for more information that helps them to make specific decisions19. In this case, it is natural to expect that in the very near future such platforms will be able to not only predict the properties of materials but also test hypotheses by designing structures and characterizing them, becoming autonomous. It has already been demonstrated that combinatorial optimization procedures provide faster screening of the molecular space than traditional approaches based on intuition20. The pharmaceutical and chemical industries, as well as academic research environments, use these methods for the design of new molecules, reactions and materials21–26. However, combinatorial chemical synthesis makes an exhaustive search of the multidimensional molecular space out of reach26. As such, the community needs a more rational approach for exploration of this large space; this is where machine learning comes into play. The recent progress in machine learning and statistical inference methods can be viewed as a revolution in AI. Most machine learning methods, such as neural networks and Bayesian optimization, were developed decades ago but have not found widespread use until recently27,28. Basic research in AI continues to be backed by governments, industry, and public and private research institutes29. Today, machine learning methods are behind many commercial applications, such as Internet searches, natural language translation, and image and speech recognition. Recently, an upgraded version of AlphaGo, the Goplaying program from Google, which in part uses deep neural networks (DNNs) and reinforcement learning as key algorithms, beat the top human Go player30. Moreover, its playing style inspired other Go players. Since mid-2017, Cisco has used a multilayer supervised algorithm based on machine learning to analyse encrypted traffic (that is, HTTPS). This algorithm helps identify malware communication through passive monitoring and yields enhanced incident responses31. In economics, machine learning has also started to emerge, notably to predict economic growth32, to quantify predictive performance33 and to anticipate customer behaviour34. While recent progress is making its mark in nonscientific endeavours, the application of machine learning in science and medicine is also emerging: assisting physicians in interpreting computerbased medical images, processing biomedical signals and learning from patient data35–37. In late 2017, a DNN was successfully shown to enable the reconstruction of perceptual and subjective images from the activity of human brains38. Within the context of this Perspective, the application of several machine learning methods to computational chemistry has recently bourgeoned39–41. From the representation of aromaticity and conjugation in general42 to the prediction of protein–ligand affinities43, there is increasing interest in using DNNs and convolutional neural networks in a wide range of applications. Hybrid learning models, which combine different approaches to leverage their respective strengths, have shown great promise. Examples of hybrid learning models include Bayesian deep learning44, Bayesian conditional generative adversarial networks45 and deep Bayesian optimization46. The latter has been successfully applied to reverse engineer chemical reactions to quantitatively and qualitatively reproduce observed behaviours46. Machinelearning-based algorithms have also been intensively used to bypass expensive static47 and dynamic48–50 ab initio electronic structure calculations. Although exploration and discovery are more challenging than interpolation or optimization for AI, recent algorithmic developments show substantial promise for making advances in these areas. To overcome these challenges of inverse design in computational chemistry and to explore the openended chemical space, autoencoders and generative adversarial networks have emerged as powerful tools to generate novel molecular structures with desirable properties tailored to specific needs51–55. This progress is only the beginning of the integrated materials discovery revolution. The key component of an autonomous discovery approach lies in the synergy between machine learning and robotics. One might ask, “Why are robots better?” A cursory analysis of humans versus robots shows some clear advantages for the latter. First, robots can operate in more adverse conditions. This can be seen even outside chemistry; for example, robots were sent to Mars decades before humans. For chemistry, this ability can translate to procedures that are subject to high temperatures and/or pressures, toxic solvents and highly exothermic processes. Robots also excel at providing unbiased and reproducible routes towards materials discovery. For example, it was recently demonstrated that not only do machinelearning-based algorithms cover an application space of polyoxometalates approximately six times larger than a human approach but they also increase the accuracy of prediction by a relative value of 6.9%56. Recently, an autonomous infrastructure for the optimization of chemical reaction conditions through the Deep Reaction Optimizer (DRO), an algorithm based on deep reinforcement learning, was reported57. Furthermore, robots are better at recording reaction procedures independently of their outcome and reduce waste by rigorously following the stoichiometry of the experiments18. Robots also provide a natural platform for scaling chemical experiments, reducing the cost per experiment. In 2009, a hypotheticodeductive ‘robot scientist’, named Adam, was developed that could autonomously perform experiments, devise hypotheses and design experiments to validate the hypotheses in the area of functional genomics58. 6 | mAY 2018 | volume 3 © 2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. www.nature.com/natrevmats P e r s P e c t i v e s",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5db60e5fd5089dfc3df4c8f645a925ac20e290ab,https://www.semanticscholar.org/paper/5db60e5fd5089dfc3df4c8f645a925ac20e290ab,In Resonance with Technology's Cutting Edge [President's Message],"4 IEEE SyStEmS, man, & CybErnEtICS magazInE July 2018 T oday’s discourse among both technical professionals and lay technology enthusiasts is teeming with subject matter focused on innovations resulting from the research and practice of systems science and engineering, human– machine systems, and cybernetics. Whether it is complex systems en ­ abled by cybernetics, intelligence for robotic and vehicular autonomy, new capabilities enabled by advanc­ es in machine learning, augmented humans, human–machine fusion, or other forms of human–machine symbiosis, the dialog is vibrant in the technical and nontechnical sectors of society alike. My conversations with colleagues, laypersons, family, and friends consistently touch on various exciting aspects of science and tech­ nology, how quickly it is advancing, what it is enabling, where it is head­ ed, and its implications. Invariably, I find myself discussing ideas that all relate in some way to the fields of interest and focal activities of the IEEE Systems, Man, and Cybernetics Society (SMCS). Surely, this experi­ ence is also shared by the members of our Society, who must feel the same sense of powerful resonance that I do between the SMCS and the current frontiers of technology. Members of our Society do not share this sense of resonance in iso­ lation but may identify more strongly with it since we are directly engaged in our fields. The confluence of systems science and engineering, human–ma­ chine systems, and cybernetics is prev­ alent at the present­ day cutting edge of engineering. In fact, year after year, lead­ ing research organi­ zations that report on emerging and disrup­ tive technology trends consistently identify technologies that cor­ respond to areas of active research in the SMCS. Thus, this is an era of major impact for the Society, and I am excited to be sharing this time with our membership as we continue to advance technology for the benefit of humanity through our individual work and collaborations. As technologies and methodolo­ gies mature to meet new challenges, there is an increasing motivation to incorporate them into real­world sys­ tems. This is where our knowledge and tools for systems science and systems engineering are brought to bear. Additionally, at this time, there is a pronounced emphasis on human factors and the human relationships with the technologies that compose complex systems—to which we in the SMCS are well equipped to respond with appropriate human­centric solu­ tions. With cybernetics as a science of, and a transdisciplinary approach to, studying control and communica­ tions in machines and living things, we are well equipped to apply its ele­ ments to enable complex and increas­ ingly intelligent systems that interact with humans in a symbiotic or collab­ orative fashion. Interestingly, the hot topical areas referred to by the many specialized terms in SMCS member vernacular are often lumped by popular culture within the currently conflated term of artificial intelligence (AI) to the extent that any instance of soft­ ware performing intel­ ligent data pro cessing is even being referred to in pro noun form as an AI. The same can be said of terms of reference like cyberphysical sys tems. This situation presents an opportunity for our members to articulate our research in more specific terms, beyond our inner technical circles, asserting a claim on the inte­ gral ingredients of what are becoming known as AI, cyberphysical systems, and related capabilities. SMCS mem­ bers are engaged in the many facets of research that are collectively defining an exciting future and should be proud contributors to this, taking appropriate ownership and responsibility. Indeed, the Society is a nexus of technical talent, research activity, and advances for a large share of what is fueling today’s discourse. Students studying in our fields of interest (http://ieeesmc.org/about­smcs), their professors, and engaged professionals worldwide should see the SMCS as a home and community supporting and advancing their interests and career. In Resonance with Technology’s Cutting Edge","IEEE Systems, Man, and Cybernetics Magazine",2018.0,10.1109/msmc.2018.2833418,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0ee984527ec62d1126357ca50c7f7ea7ce3a17ce,https://www.semanticscholar.org/paper/0ee984527ec62d1126357ca50c7f7ea7ce3a17ce,Construction et déploiement d'applications web basées sur R,"La Data Science se developpe a grand pas. Le langage R, precurseur dans l'implementation d'algorithmes issus du monde de la statistique et du Machine Learning, beneficie d'une certaine avance dans ce domaine compare a la plupart des langages de programmation. Cependant, vu le besoin grandissant d'embarquer de l'intelligence artificielle dans les applications web, la course est aujourd'hui a la capacite a construire et deployer des applications professionnelles, robustes et fiables, capables d'analyse et d'apprentissage sur les donnees diverses auxquelles elles peuvent avoir acces. Grâce a sa large communaute de developpeurs a travers le monde, le langage R est en train de se doter de plus en plus d'extensions lui permettant de rivaliser avec des langages traditionnels tels que Java, Python ou JavaScript avec NodeJS, qui ont fait leurs preuves sur la construction et le deploiement d'applications web professionnelles. Au sein des services de recherche et developpement d'Orange, nous sommes souvent confrontes a des problematiques necessitant de construire, d'automatiser et de deployer des outils d'analyse, de visualisation et d'apprentissage de donnees sous forme d'applications web. Le langage R et plusieurs de ses extensions telles que les packages shiny et plumber nous ont largement servi dans ce cadre. Les reponses aux questionnements qui ont emerge tout au long des processus de developpement, d'integration et de deploiement de ces applications ont fait l'objet de passionnants travaux. Ces derniers ont d'ailleurs donne naissance a un package R nomme Rapp. Ce dernier vise en particulier a faciliter le deploiement sur une machine distante de toute application R, qu'elle soit une application Shiny (e.g. [1]) ou une API Plumber (e.g. [2]). La presentation proposee consistera d'abord en un panorama de packages R et d'outils complementaires interessants montrant le potentiel de ce langage pour supporter les besoins actuels de deploiement de Data Science et d'intelligence artificielle en tant que service web. Nous parlerons entre autres d'integration avec des bases de donnees ou des APIs tierces, de developpement d'API et d'interface web sous R, ainsi que de containerisation des applications R. Par la suite, nous presenterons des approches inspirees du monde DevOps que nous utilisons pour construire et deployer des applications web basees entierement ou en partie sur R. Enfin, nous introduirons le package Rapp que nous proposons pour partager ces approches avec les communautes d'utilisateurs de R. References [1] https://tomyardstick.sigmant.net/ [2] https://api.tomyardstick.sigmant.net/swagger/ 1",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
195052e4acad1c3e861ee566bb6187c2faebaf1e,https://www.semanticscholar.org/paper/195052e4acad1c3e861ee566bb6187c2faebaf1e,Final Report 5 / 3 / 2018 Implementation Part 1 : Data Scraping and Parsing,"The Yale Investments Office (YIO) selects investment managers to actively manage the university’s endowment. To achieve the best returns, the YIO’s team of investment professionals works year-round to identify new talent. By finding relatively new firms, the office can help promising investors build a strong infrastructure, establish effective long-term principles, and shape a healthy partnership. At present, few of the office’s methods leverage the capabilities of modern data science or programming techniques. To aid in processing the volume of publicly available data on investment firms, this paper presents a system for automatically identifying prospective firms using machine learning. The system implements three components. First, the system provides a data scraping and parsing library for SEC Form D data, which offers some of the earliest available about the existence of new firms. Second, we build a decision tree classifier to evaluate the attractiveness of firms based on the information reported in recently filed Form Ds. This report discusses the design and analysis of the decision tree model. Third, a simple web app interface allows non-technical users easy access to the classification system. The web app will also allow for convenient, centralized deployment on an office’s internal network. Based on the results of this report, implementing such a system at the YIO could meaningfully augment the breadth and speed of the manager selection process. The final section of this report outlines step for further exploration of machine classification of investment firms, including approaches to improve the quality of this analysis and expand its scope. Michael Byrnes CPSC 490 Final Report 5/3/2018",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fd360f941d85123a026c78504e350ff2073cbc1e,https://www.semanticscholar.org/paper/fd360f941d85123a026c78504e350ff2073cbc1e,Recent Advances in Metamaterials and Metasurfaces,"submission website: https://mc.manuscriptcentral.com/tap-ieee Authors should indicate in their cover letter that their submission is intended for the special issue on “Recent Advances in Metamaterials and Metasurfaces” and indicate “Special Issue” as the type of their manuscript during the submission process. description: In the 15 years since the last (and first) IEEE T-AP Special Issue on Metamaterials guest-edited by R. Ziolkowski and N. Engheta, the fields of metamaterials and, recently, metasurfaces, have evolved tremendously: they have seen development across the electromagnetic spectrum from RF to optical frequencies and found useful counterparts in acoustics and mechanics; they have pushed the boundaries of fabrication technologies and gained a foothold in a multitude of related disciplines from materials science to machine learning; concepts and terminology have been continuously refined, and the resolution of old controversies through rigorous theoretical and experimental validations has yielded a wealth of new and useful perspectives; they have enabled novel functionalities in a host of devices, with intriguing developments in antenna design; and they are now seeing successful practical deployments in a variety of industries. The purpose of this special issue is to draw attention to the latest progress in the understanding, development, and deployment of electromagnetic metamaterials and metasurfaces. Potential topics include, but are not limited to, the following:",IEEE Antennas and Propagation Magazine,2018.0,10.1109/map.2018.2875252,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4d60d7952308708523a95e179f4e0805452a2141,https://www.semanticscholar.org/paper/4d60d7952308708523a95e179f4e0805452a2141,Open Geospatial Data Contribution Towards Sentiment Analysis Within the Human Dimension of Smart Cities,,,2020.0,10.1007/978-3-030-58232-6_5,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e7334bd0ca86b145f072c966b27fb4d990fea768,https://www.semanticscholar.org/paper/e7334bd0ca86b145f072c966b27fb4d990fea768,Optimizing High-Performance Computing Systems for Biomedical Workloads,"The productivity of computational biologists is limited by the speed of their workflows and subsequent overall job throughput. Because most biomedical researchers are focused on better understanding scientific phenomena rather than developing and optimizing code, a computing and data system implemented in an adventitious and/or non-optimized manner can impede the progress of scientific discovery. In our experience, most computational, life-science applications do not generally leverage the full capabilities of high-performance computing, so tuning a system for these applications is especially critical. To optimize a system effectively, systems staff must understand the effects of the applications on the system. Effective stewardship of the system includes an analysis of the impact of the applications on the compute cores, file system, resource manager and queuing policies. The resulting improved system design, and enactment of a sustainability plan, help to enable a long-term resource for productive computational and data science. We present a case study of a typical biomedical computational workload at a leading academic medical center supporting over $100 million per year in computational biology research. Over the past eight years, our high-performance computing system has enabled over 900 biomedical publications in four major areas: genetics and population analysis, gene expression, machine learning, and structural and chemical biology. We have upgraded the system several times in response to trends, actual usage, and user feedback. Major components crucial to this evolution include scheduling structure and policies, memory size, compute type and speed, parallel file system capabilities, and deployment of cloud technologies. We evolved a 70 teraflop machine to a 1.4 petaflop machine in seven years and grew our user base nearly 10-fold. For long-term stability and sustainability, we established a chargeback fee structure. Our overarching guiding principle for each progression has been to increase scientific throughput and enable enhanced scientific fidelity with minimal impact to existing user workflows or code. This highly-constrained system optimization has presented unique challenges, leading us to adopt new approaches to provide constructive pathways forward. We share our practical strategies resulting from our ongoing growth and assessments.",2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),2020.0,10.1109/IPDPSW50202.2020.00040,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,https://www.semanticscholar.org/paper/cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,Comparing Human-Robot Proxemics between Virtual Reality and the Real World,"Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of Human-Robot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. Comparing Human-Robot Proxemics between Virtual Reality and the Real World Rui Li KTH Royal Institute of Technology Stockholm, Sweden Rui3@kth.se ABSTRACT Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other.Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. INTRODUCTION Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI) [1][2][3][4]. VR has been used to test teleoperation and collect demonstration data to train machine learning algorithms, which showcased the effectiveness of learning visuomotor skills using data collected by consumer-grade devices [1]. VR teleoperation systems were proposed to crowdsource robotic demonstrations at scale [2]. A VR simulation framework was also proposed to replace the physical robot, as VR can enable high level abstraction in embodiment and multimodal interaction [3]. VR has also been used as a rapid prototyping tool to design in-vehicle interactions and interfaces for self-driving cars, which showed the evocation to genuine responses from test participants [4]. Compared to other HRI experiment methods, VR as an emerging interactive media provides unique advantages. VR HRI has the potential of having higher immersion and fidelity than picture based HRI, video-based HRI and simulated HRI. In situations where the perception of the robot is challenging, compared to on-screen viewing, VR display showed significant improvement on collaborative tasks [5]. When comparing VR HRI to the physical, realworld interaction (Live HRI), there is a trade-off between the two. VR experiences still cannot replace physical experiences due to system limitation, and limited interaction modalities etc. [6]. For example, system limitations such as limited field of view and low display resolution could reduce immersion and presence of the VR experience, resulting in different behaviors from Live experiments. Limited interaction modalities, such as the absence of touch, means that the participant could not feel the robot or even go through the robot, which could potentially break the entire interaction. Figure 1: Photograph of the Live experiment setting However, with the help of the distribution of consumer-grade VR devices and online crowdsourcing platforms, VR HRI has the potential to gain massive data for training robotic behavior and studying HRI related issues. Data collection through VR can also reduce noise and improve the data quality [1], which help to ease data processing and algorithm training. Furthermore, VR HRI experiments can test concepts and interactions without physical robots, making it more resource efficient and less expensive than Live HRI. Less hardware also means that the experiment will be less cumbersome to set up, easier to be reproduced and to ensure experiment quality. In this study, HRI Proxemics (the preferred personal space between a human and a robot) was compared to give a better justification and more basic understanding of the relationship between Live and VR. Proxemics preferences rely on lower level intuition [7], therefore, reflect the differences in the perceptions between Live and VR better. Compared to other HRI subject such as conversational (audio) or gaze behavior (visual), which are more modality dependent, proxemics can give a comprehensive understanding of the human responses. In addition, variations of modalities in VR can greatly influence human perception. For example, a higher visual familiarity of the physical environment in VR can decrease the effect of distance distortion [8]. Auditory inputs play another important role in VR, the addition of spatial sound can increase the sense of presence in VR and provide sound localization [9]. Thus, this work also compares VR settings with variance in modalities to evaluate the impacts of visual familiarity and spatial sound on VR HRI experiments. A 2 x 3 mixed design experiment was conducted to evaluate the differences between Live and VR HRI, as well as the influence of visual familiarity and spatial sound in VR. For the Live HRI, the pepper robot from Softbank Robotics was used (Figure 1). In the VR HRI, a 3D model of the same robot was used. To measure visual familiarity, the VR scene was created in Blender based on a 3D scan of the physical lab. The spatial sound was created by enabling the movement of the physical robot, due to the difficulties of engineering spatial sound. The interaction was implemented in Unity. As an objective measurement for proxemics preference, the minimum comfort distance (MCD) was measured. In addition, for the psychological perception of the experience, the feeling of presence was measured with the SUS questionnaire. For the perception of the robot, two relevant factors, competence and discomfort was measured with the ROSAS questionnaire.",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a7aa6451c49ff7f92746b10f9e723aa9e9251343,https://www.semanticscholar.org/paper/a7aa6451c49ff7f92746b10f9e723aa9e9251343,A Methodology for the Investigation of the Producer-Consumer Problem,"The analysis of hierarchical databases is a confusing issue. In fact, few steganographers would disagree with the analysis of multi-processors, which embodies the confusing principles of machine learning. Here we motivate an analysis of gigabit switches (DimLalo), which we use to show that the little-known unstable algorithm for the emulation of Byzantine fault tolerance by Noam Chomsky is optimal. Introduction Electrical engineers agree that distributed epistemologies are an interesting new topic in the field of saturated cyberinformatics, and end-users concur. Although such a hypothesis at first glance seems perverse, it is supported by existing work in the field. A confusing grand challenge in cryptoanalysis is the analysis of wide-area networks. Given the current status of heterogeneous information, researchers clearly desire the understanding of the Ethernet. The exploration of write ahead logging would greatly degrade heterogeneous algorithms. DimLalo, our new algorithm for pervasive theory, is the solution to all of these grand challenges. Indeed, semaphores and the UNIVAC computer have a long history of cooperating in this manner. Indeed, extreme programming and suffix trees have a long history of agreeing in this manner. Obviously, we see no reason not to use constant-time configurations to analyze secure configurations. This work presents three advances above existing work. For starters, we validate that extreme programming and e-business are often incompatible. Second, we describe an algorithm for spreadsheets (DimLalo), which we use to verify that consistent hashing [14] can be made linear-time, lossless, and concurrent. We explore an analysis of consistent hashing (DimLalo), which we use to validate that spreadsheets and access points can collaborate to overcome this obstacle. The rest of the paper proceeds as follows. We motivate the need for Internet QoS. Similarly, we place our work in context with the previous work in this area. Similarly, to accomplish this purpose, we disconfirm not only that lambda calculus and gigabit switches are always incompatible, but that the same is true for IPv6. Finally, we conclude the effective models introduced in this paper. Model Continuing with this rationale, the design for our algorithm consists of four independent components: the simulation of vacuum tubes, wearable technology, IPv4, and DHTs. Our purpose here is to set the record straight. Furthermore, the architecture for our method consists of four independent components: scalable theory, red-black trees, I/O automata, and the Internet. Any extensive deployment of the location-identity split will clearly require that the famous cooperative algorithm for the deployment of cache coherence by Takahashi and Bhabha [12] is recursively enumerable; our heuristic is no different. Consider the early methodology by Martin; our framework is similar, but will actually surmount this question. This is a compelling property of DimLalo. We use our previously refined results as a basis for all of these assumptions. International Conference on Education Reform and Management Science (ERMS 2018) Copyright © 2018, the Authors. Published by Atlantis Press. This is an open access article under the CC BY-NC license (http://creativecommons.org/licenses/by-nc/4.0/). Advances in Social Science, Education and Humanities Research, volume 177",,2018.0,10.2991/ERMS-18.2018.53,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4e50e610a2dd0933ddbf47ba3e22e53e8131b34a,https://www.semanticscholar.org/paper/4e50e610a2dd0933ddbf47ba3e22e53e8131b34a,Human Language Technology,"Human language technology encompasses a wide array of speech and text processing capabilities. The Defense Advanced Research Projects Agency’s pioneering research on automatic transcription, translation, and content analysis were major artificial intelligence success stories that changed science fiction into social fact. During a 40-year period, 10 seminal DARPA programs produced breakthrough capabilities that were further improved and widely deployed in popular consumer products, as well as in many commercial, industrial, and governmental applications. The Defense Advanced Research Projects Agency produced the core enabling technologies by setting crisp, aggressive, and quantitative technical objectives; by providing strong multiyear funding; and by using the Defense Advanced Research Projects Agency’s Common Task Method, which was powerful, efficient, and easy to administer. To achieve these breakthroughs, multidisciplinary academic and industrial research teams working in parallel took advantage of increasingly large and diverse sets of linguistic data and rapidly increasing computational power to develop and use increasingly sophisticated forms of machine learning. This article describes the progression of technical advances underlying key successes and the seminal programs that produced them.",AI Mag.,2020.0,10.1609/aimag.v41i2.5297,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
59e10d1d4cd454635914cfd0ac5160a318fd0473,https://www.semanticscholar.org/paper/59e10d1d4cd454635914cfd0ac5160a318fd0473,UB09 Session 9,"In the domain of Wireless Sensor Networks (WSN), providing an effective security solution to protect the motes and their communications is challenging. Due to the hard constraints on performance, storage and energy consumption, normal network-security related techniques cannot be applied. Focusing on the ""Intrusion Detection"" problem, we propose a realworld application of our WSN Intrusion Detection System (WIDS). WIDS exploits the Weak Process Models to classify potential security issues in the WSN and to notify the operators when an attack tentative is detected. In this demonstration, we show how our IDS works, how it detects some basic attacks and how the IDS can evolve to fullfil the needs of secure WSN deployments. Download Paper (PDF) UB09.2 RESCUE: EDA TOOLSET FOR INTERDEPENDENT ASPECTS OF RELIABILITY, SECURITY AND QUALITY IN NANOELECTRONIC SYSTEMS DESIGN Authors: Cemil Cem Gürsoy1, Guilherme Cardoso Medeiros2, Junchao Chen3, Nevin George4, Josie Esteban Rodriguez Condia5, Thomas Lange6, Aleksa Damljanovic5, Raphael Segabinazzi Ferreira4, Aneesh Balakrishnan6, Xinhui Anna Lai1, Shayesteh Masoumian7, Dmytro Petryk3, Troya Cagil Koylu2, Felipe Augusto da Silva8, Ahmet Cagri Bagbaba8 and Maksim Jenihhin1 1Tallinn University of Technology, EE; 2Delft University of Technology, NL; 3IHP, DE; 4BTU Cottbus-Senftenberg, DE; 5Politecnico di Torino, IT; 6IROC Technologies, FR; 7Intrinsic ID B.V., NL; 8Cadence Design Systems GmbH, DE Abstract The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF)The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF) UB09.3 ASAM: AUTOMATIC SYNTHESIS OF ALGORITHMS ON MULTI CHIP/FPGA WITH COMMUNICATION CONSTRAINTS Authors: Amir Masoud Gharehbaghi, Tomohiro Maruoka, Yukio Miyasaka, Akihiro Goda, Amir Masoud Gharehbaghi and Masahiro Fujita, The University of Tokyo, JP Abstract Mapping of large systems/computations on multiple chips/multiple cores needs sophisticated compilation methods. In this demonstration, we present our compiler tools for multi-chip and multi-core systems that considers communication architecture and the related constraints for optimal mapping. Specifically, we demonstrate compilation methods for multi-chip connected with ring topology, and multi-core connected with mesh topology, assuming fine-grained reconfigurable cores, as well as generalization techniques for large problems size as convolutional neural networks. We will demonstrate our mappings methods starting from data-flow graphs (DFGs) and equations, specifically with applications to convolutional neural networks (CNNs) for convolution layers as well as fully connected layers. Download Paper (PDF) UB09.4 HEPSYCODE-MC: ELECTRONIC SYSTEM-LEVEL METHODOLOGY FOR HW/SW CO-DESIGN OF MIXED-CRITICALITY EMBEDDED SYSTEMS Authors: Luigi Pomante1, Vittoriano Muttillo1, Marco Santic1 and Emilio Incerto2 1Università degli Studi dell'Aquila DEWS, IT; 2IMT Lucca, IT Abstract Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF)Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF) UB09.5 CS: CRAZYSQUARE Authors: Federica Caruso1, Federica Caruso1, Tania Di Mascio1, Alessandro D'Errico1, Marco Pennese2, Luigi Pomante1, Claudia Rinaldi1 and Marco Santic1 1University of L'Aquila, IT; 2Ministry of Education, IT Abstract CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF)CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF) UB09.6 LABSMILING: A SAAS FRAMEWORK, COMPOSED OF A NUMBER OF REMOTELY ACCESSIBLE TESTBEDS AND RELATED SW TOOLS, FOR ANALYSIS, DESIGN AND MANAGEMENT OF LOW DATA-RATE WIRELESS PERSONAL AREA NETWORKS BASED ON IEEE 802.15.4 Authors: Carlo Centofanti, Luigi Pomante, Marco Santic and Walter Tiberti, University of L'Aquila, IT Abstract Low data-rate wireless personal area networks (LR-WPANs) are constantly increasing their presence in the fields of IoT, wearable, home automation, health monitoring. The development, deployment and testing of SW based on IEEE 802.15.4 standard (and derivations, e.g. 15.4e), require the exploitation of a testbed as the network grows in complexity and heterogeneity. This demo shows LabSmiling: a SaaS framework which connects testbeds deployed in a real-world-environment and the related SW tools that make available a meaningful (but still scalable) number of physical devices (sensor nodes) to developers. It provides a comforta",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9b43dade854555fdd06485a904bed119dbd77db6,https://www.semanticscholar.org/paper/9b43dade854555fdd06485a904bed119dbd77db6,An Exploratory Study of Sequence Alignment for Improved Sensor-Based Human Activity Recognition,"Sequence alignment (SA) is a well-established technique in bioinformatics for analyzing deoxyribonucleic acid (DNA), ribonucleic acid (RNA), or protein sequences and identifying regions of similarity. The main goal of SA is to discover relationships between strings of data by deploying a series of heuristic or probabilistic methods to align a new string (e.g., DNA of a new species) with an existing string (DNA of a known species). SA has also been used sporadically in linguistics, social sciences, and finance. In this paper, the authors explore the prospect of coupling machine learning (ML) and SA to improve the output of human activity recognition (HAR) methods. In particular, several field experiments are conducted to collect heterogeneous human motion data via wearable sensors. Collected data is further mined using ML to identify sequences of activities performed in each experiment. Given the inaccuracy of sensor readings and the limitations of ML algorithms especially in handling datasets from complex human activities such as those performed by construction workers, it is expected that the resulting activity sequences not fully match actual activity sequences as observed in the field. To further clean up this inherent noise, SA is deployed to refine imperfections in the resulting activity sequences by manipulating the output of HAR and ultimately aligning noisy activity sequences with ground truth sequences. The outcome of this work is a systematic method to improve the reliability of HAR from sensor readings, which can benefit decision-making as related to task planning, resource management, productivity monitoring, and ergonomic assessment. INTRODUCTION The sequence alignment (SA) technique was first developed during the 1980s by biochemists. SA primarily relies on a series of applied mathematical algorithms (Sankoff and Kruskal 1983) for holistic sequential analyses that could provide insight into long sequences of protein and deoxyribonucleic acid (DNA). The use of SA was expanded to other domains in the late 1990s (Abbott and Tsay 2000; Wilson 1998) primarily by social scientists (Abbott and Forrest 1986) to advance the analysis of socio-economic data by producing normalized data trends and comparing each data Construction Research Congress 2018 347",,2018.0,10.1061/9780784481264.034,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d9925b5cd5afb276414b9cb4a106de6435fccf9a,https://www.semanticscholar.org/paper/d9925b5cd5afb276414b9cb4a106de6435fccf9a,Plausible Cause: Explanatory Standards in the Age of Powerful Machines,"INTRODUCTIONSuppose, in the near future, that police start using an algorithmic tool-the Contraband Detector-to locate residences likely to contain illegal weapons. When the tool was first developed, its outputs were thirty percent accurate. With time, however, machine learning refined the tool.1 Now its accuracy rate hovers around eighty percent, and data scientists, having recently “audited” the Contraband Detector,2 report that the tool’s performance will only continue to improve. When the tool locates a suspicious residence, it does not explain why; it simply displays an address. And because of the tool’s complexity-it draws on more than one hundred input-variables- officers have no idea which variables are determinative in a given case.3Here is the puzzle. Imagine the Contraband Detector, deployed in New York City, turns up “285 Court St., Apt. 2L,” prompting the NYPD to seek a search warrant. When the judge asks about probable cause, the officers point to one, and only one, fact: the tool’s performance rate.4 Should the judge sign the warrant? Or better yet: Could the judge’s role in the process simply be eliminated-at least in principle-such that any time the tool identifies a suspicious residence, a search warrant issues automatically?5 In other words, suppose the next generation of tool, operating on the same logic, is not a Contraband Detector, but an Automatic Warrant Machine. Assuming the tool continues to perform at a high level of statistical precision, would its use-in lieu of judicial oversight-be consistent with the Fourth Amendment?There is a powerful and widespread intuition that the answer to these questions is no.6 Performance aside, blind reliance on an algorithmic tool feels uncomfortable. It misses the point of particularized suspicion.7 But why? On its face, probable cause would seem to depend on the probability that a “person[ ], house[ ], paper[ ] or effect[ ]” is linked to wrongdoing.8 In the example, it is eighty percent probable that 285 Court St., Apt. 2L contains an illegal weapon. So probable cause, literally construed, should be satisfied.I propose a simple solution to this puzzle. For probable cause to be satisfied, an inference of wrongdoing must be plausible-the police must be able to explain why observed facts give rise to the inference.9 And judges must have an opportunity to scrutinize that explanation: to test its overall intelligibility; to weigh it against the best innocent account on the other side; and to evaluate its consistency with background values, flowing from the Constitution, from general legality principles, and from other sources of positive law.10This hardly means that prediction tools have no place in policing or in other areas of governance. It means, rather, that their role is to aid human reasoning, not to supplant it.11 Outputs from prediction tools, like outputs from other detection instruments, such as drug dogs,12 can certainly be among the facts that police adduce-in an explanatory fashion-to anchor claims of wrongdoing. For that process to work, however, a tool’s outputs must be intelligible. Black-box tools will not do. Nor will transparent tools with outputs too complex for a human to trace.13Although the Contraband Detector, as imagined, exceeds current technology, the trend it reflects-the blossoming of data-driven prediction tools in the criminal justice system-is hardly science fiction. In many jurisdictions, judges have already begun to rely heavily on prediction tools that predict the likelihood of flight or recidivism for bail and sentencing purposes,14 a practice recently upheld by the Wisconsin Supreme Court.15 Likewise, the first wave of suspicion tools have recently been adopted by police departments, often to help officers assess individuals’ “threat scores” while on patrol.16 At present, the technology is crude; no hyper-precise detection tool, able to predict the presence of contraband eighty percent of the time, yet exists. …",,2016.0,10.2139/SSRN.2827733,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ee7cc3d2bee64e22248fd651788b98f2d1cf7f19,https://www.semanticscholar.org/paper/ee7cc3d2bee64e22248fd651788b98f2d1cf7f19,Curated Pathways to Innovation: Personalized CS Education to Promote Diversity,"The lack of diversity in computing is a well-known issue. This poster is a work-in-progress report on Curated Pathways to Innovation (CPI), a web-based tool which gathers existing online resources for computer science (CS) engagement and learning to allow students to learn more about CS careers and content, with a particular focus on improving participation of K-12 girls and under-represented minorities in CS. This project is a collaboration of people from academia in CS and social science, K-12 education, non-profit, and industry. We are about halfway through a 3-year pilot deployment of CPI with all students in a low-income, primarily Latino/a middle school with nearly 500 students, and smaller deployments have been undertaken and are planned for 2018-19. In addition to online content, we have created in-person experiences, including reverse science fairs, summer camps, and a hackathon, which are tracked in the CPI tool. To measure impact, we conduct regular surveys with the students measuring their interest in CS, self-efficacy, and other metrics. Our evaluation of the system based on survey data has helped inform the development of the system and curriculum, but remains preliminary. This poster also discusses the tool itself. It uses gamification in the form of badges to measure student progress. From the beginning, the vision was to use machine learning to customize recommendations based on students' demographics, background, and past performance. This integration is coming to fruition at the same time we are including more interesting visuals in the UI, such as an avatar and animations.",SIGCSE,2019.0,10.1145/3287324.3293845,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
878a8b46c70cbb53728d9c039532a14efb0f1b41,https://www.semanticscholar.org/paper/878a8b46c70cbb53728d9c039532a14efb0f1b41,Using big data analytics to improve HIV medical care utilisation in South Carolina: A study protocol,"Introduction Linkage and retention in HIV medical care remains problematic in the USA. Extensive health utilisation data collection through electronic health records (EHR) and claims data represent new opportunities for scientific discovery. Big data science (BDS) is a powerful tool for investigating HIV care utilisation patterns. The South Carolina (SC) office of Revenue and Fiscal Affairs (RFA) data warehouse captures individual-level longitudinal health utilisation data for persons living with HIV (PLWH). The data warehouse includes EHR, claims and data from private institutions, housing, prisons, mental health, Medicare, Medicaid, State Health Plan and the department of health and human services. The purpose of this study is to describe the process for creating a comprehensive database of all SC PLWH, and plans for using BDS to explore, identify, characterise and explain new predictors of missed opportunities for HIV medical care utilisation. Methods and analysis This project will create person-level profiles guided by the Gelberg-Andersen Behavioral Model and describe new patterns of HIV care utilisation. The population for the comprehensive database comes from statewide HIV surveillance data (2005–2016) for all SC PLWH (N≈18000). Surveillance data are available from the state health department’s enhanced HIV/AIDS Reporting System (e-HARS). Additional data pulls for the e-HARS population will include Ryan White HIV/AIDS Program Service Reports, Health Sciences SC data and Area Health Resource Files. These data will be linked to the RFA data and serve as sources for traditional and vulnerable domain Gelberg-Anderson Behavioral Model variables. The project will use BDS techniques such as machine learning to identify new predictors of HIV care utilisation behaviour among PLWH, and ‘missed opportunities’ for re-engaging them back into care. Ethics and dissemination The study team applied for data from different sources and submitted individual Institutional Review Board (IRB) applications to the University of South Carolina (USC) IRB and other local authorities/agencies/state departments. This study was approved by the USC IRB (#Pro00068124) in 2017. To protect the identity of the persons living with HIV (PLWH), researchers will only receive linked deidentified data from the RFA. Study findings will be disseminated at local community forums, community advisory group meetings, meetings with our state agencies, local partners and other key stakeholders (including PLWH, policy-makers and healthcare providers), presentations at academic conferences and through publication in peer-reviewed articles. Data security and patient confidentiality are the bedrock of this study. Extensive data agreements ensuring data security and patient confidentiality for the deidentified linked data have been established and are stringently adhered to. The RFA is authorised to collect and merge data from these different sources and to ensure the privacy of all PLWH. The legislatively mandated SC data oversight council reviewed the proposed process stringently before approving it. Researchers will get only the encrypted deidentified dataset to prevent any breach of privacy in the data transfer, management and analysis processes. In addition, established secure data governance rules, data encryption and encrypted predictive techniques will be deployed. In addition to the data anonymisation as a part of privacy-preserving analytics, encryption schemes that protect running prediction algorithms on encrypted data will also be deployed. Best practices and lessons learnt about the complex processes involved in negotiating and navigating multiple data sharing agreements between different entities are being documented for dissemination.",BMJ Open,2019.0,10.1136/bmjopen-2018-027688,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f0181da3c6618177bce90173afdd6eb352f4e22e,https://www.semanticscholar.org/paper/f0181da3c6618177bce90173afdd6eb352f4e22e,Guest Editorial: Computer Vision for Animal Biometrics,"Biometric Computer Vision that detects, tracks, identifies, describes, and classifies animal life from captured image and video data, is an emerging subject in machine vision. It is an exciting moment for this field of study. For the first time a myriad of realworld systems and applications are becoming integrated into the practice of the biological sciences. Indeed, Computer Vision systems have also started to assist work in a variety of allied scientific areas including field ecology, agricultural research, animal welfare, conservation, public health and the behavioural sciences. This Special Issue brings together a selection of eight timely papers in this field, submitted by researchers of institutions from across four continents. The works presented here include the analysis of animals ranging from insects and fish, to birds and mammals, but also life during embryonic development. The presented papers showcase some of the current diversity in this research domain: methods span the whole spectrum from traditional feature-based approaches to Deep Learning solutions. Psota et al. in their paper “Tracking of group-housed pigs using multi-ellipsoid expectation maximization” describe a system that utilises depth images accurately to estimate the position and orientation of individual pigs in a group-housed environment over significant periods of time. By applying expectation maximization as a policy for ellipse fitting, their method is able to exploit consistent shape and fixed target numbers to aid tracking. Practical results demonstrate that the system can track 15 group-housed pigs for an average of 19.7 minutes between failure events. Xie et al. in their paper “A novel open snake model based on global guidance field for embryo vessel location” present a framework for blood vessel region extraction and accurate snakebased localisation in imagery of animal embryos. Their open snake model utilises a global guidance field and is initialised by a deformation template. Experimental results on a specific embryo vessel database demonstrate that the proposed algorithm can robustly locate the embryo's blood vessels and obtain orientations of the vessel branches. Comparisons with traditional methods illustrate the effectiveness and competitiveness of their proposed model. Bakkay et al. in their paper “Automatic detection of individual and touching insects from trap images by combining contour-based and region-based segmentation” introduce a method for the detection of insects from camera trap images in difficult conditions by employing an innovative region merging algorithm and an adaptive k-means clustering approach, operating on the object contour's convex hull. Quantitative evaluations show that the proposed method can detect insects with higher accuracy than most widely used approaches. Eerola et al. in their paper “Automatic individual identification of Saimaa ringed seals” describe a method for the automatic image-based individual identification of endangered Saimaa ringed seals (Phoca hispida saimensis) that exploits the species’ permanent and individually unique visual pelage patterns. The proposed framework performs segmentation of the seals from the background, as well as post-processing and classification steps required for identification. Two existing individual identification methods are compared to the presented work using a challenging data set of Saimaa ringed seal images. The results show that the proposed segmentation and post-processing steps are effective and can provide increased identification performance against a generic baseline. Akkaya et al. in their paper “Mouse face tracking using a convolutional neural network” present a convolutional neural network (CNN) tracker called MFTN for following a mouse's face in video footage. Notably, in the proposed architecture, target information is extracted from a combination of lowand high-level features by a particular sub-network to achieve a more robust and accurate tracker. Experiments show that the particular MFTN/c tracker achieved an accuracy of 0.8, a robustness of 0.67, and a throughput of 213 fps on the GPU-powered testing workstation. Beyan et al. in their paper “Extracting statistically significant behaviour from noisy fish tracking data” describe an approach to the cleaning of a large and noisy visual tracking dataset to allow for the extraction of statistically sound results from the underlying image data. In particular, the paper presents an analysis of a dataset of 3.6 million underwater trajectories of a species of fish, which are also labelled with the water temperature at the time of acquisition. By a combination of data binning and robust estimation methods, the authors demonstrate reliable evidence for an increase in fish speed as water temperature increases. Several statistical tests applied to the data confirm that results are statistically significant. Ardo et al. in their paper “A CNN-based cow interaction watchdog” introduce an automated video analysis system for the processing of cow footage that can select or discard recorded video material based on user-defined criteria commonly required in behavioural research to reduce the amount of time experts have to spend on watching video. A CNN architecture is proposed and then evaluated in a pilot study. It is shown that 38% (50% with additional filter parameters) of the recordings in the test dataset could be correctly and successfully removed, while only losing 1% (4%) of the potentially interesting video frames. Finally, Silla Junior et al. in their paper “Bird and whale identification using sound images” describe a novel approach for the automated identification of birds and whales from calls. The visual features of audio used are constructed from different spectrograms and from harmonic and percussion images. These images are then divided into sub-windows from which sets of texture descriptors are extracted for classification. The experiments reported in this paper use a dataset of bird vocalizations targeted for species recognition and a dataset of right whale calls targeted for whale detection, as well as three well-known benchmarks for music genre classification. The authors demonstrate that the fusion of different texture features, as well as texture and audio features together can enhance performance. As is clear from the above content, this Special Issue highlights the great breadth of research in Visual Animal Biometrics today, and the even greater potential for this area of Computer Vision in the future. One may argue that the field is indeed on its way to realising another facet of Jim Gray's 4th scientific paradigm, ever more intricately binding together biological research questions with Computer Vision engineering. In any case, we hope that readers will find the papers put forward here inspiring and informative; and we would like to extend our sincere thanks to all authors and reviewers of the works before us.",IET Comput. Vis.,2018.0,10.1049/iet-cvi.2018.0019,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
eb775c55bfd58e242180870a56adfd9d923d248d,https://www.semanticscholar.org/paper/eb775c55bfd58e242180870a56adfd9d923d248d,Data Science in Action: Key to Cybersecurity,"The coming of Big Data has completely shaken the computing world. With the risein voluminous data, the amountof data at times denotes to the predictive capability of a particular data model and other times to the discovery of hidden insights that appear when rigorous analytical methods are applied to the data itself. Data Science is the application of advanced analytics to activity and access data to uncover unknown risks. It is the practice of deriving valuable insights from data. From a cybersecurity point of view, the value of data refers first to the nature of the data itself. The malicious network traffic data from malware and cyberattacks have much more value than some other data science problems. With respect to security, the valuable insight leads to reduced risk. Data Science is an emerging field that is proving well to deal with the challenges of handling alargeset of data and the outbreakof new data produced from theweb, mobile devicesand social media. The IT sector that deals with the information security and prevention of fraudshas been continuously advancing, using data science techniques to deal with the challenges of handling and achievinginsights from vast amount of log data, uncoverinsider threats and attacks thereby preventing the frauds. The data science methods used for cybersecurity applications is a moderately newconcept. This comprises deployment of statistics, machine learning, mathematics and big data analytics for anomaly detection, network modelling, risk management and more. This paper, thus illustrates how the data science tools and practices can act as a boon in combating with the cybersecurity, what role the machine learning plays in securing the web and the benefits and application of using data science for cybersecurity. Keywordsdata science; cybersecurity; machine learning; big data; statistics; big data analytics",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
673a59d39fe311027b342ff23804708b2eab9400,https://www.semanticscholar.org/paper/673a59d39fe311027b342ff23804708b2eab9400,PIXEL-WISE CLASSIFICATION AND AUTONOMOUS IMAGE ANALYSIS IN A REAL-TIME ROVER OPERATIONS SCENARIO : LESSONS LEARNED FROM THE CANMOON ANALOGUE MISSION,"Introduction: The CanMoon lunar sample return analogue mission was a part of the Canadian Space Agency’s (CSA) Lunar Exploration Analogue Deployment (LEAD) initiative, which aims to develop technologies and processes, as well as to train students and young professionals for future space missions. This mission was carried out by both the University of Western Ontario (Western), and the University of Winnipeg. The analogue mission was a simulation of a real-time operations scenario where a lunar-based scientific rover is operated and controlled from an Earth-based mission control center. For a full overview of the 2019 CanMoon analogue mission see Marion et al. [1]. Three teams were formulated for the mission: planning, science, and field teams. While the science team is in charge of deciding which scientific measurements to take and analyzing the returned data, the planning team commands the rover, keeping in mind the limitations imposed by the rover and the environment. Finally, the field team performs the activities of the rover in the exploration site. As part of the mission, we explored the use and implementation of machine learning and deep learning models to aid in the decision making process of both the science and the planning teams. Leveraging the amount of imagery that the rover sends back to the ground, machine learning models could identify and localize the presence of different objects within the rover’s surroundings. In addition, by performing the inferences within a short amount of time, these methods could provide quick insights and help guide the operators in making their decisions. It should be noted however, that the aim is not to replace the scientists and operators in making decisions, as this was a real-time mission with a human in the loop at all times. Of the 4 science goals laid out by the science team [2], one was finding xenoliths to determine if a rover would be able to identify and sample pieces of lunar mantle material. Keeping in line with this goal, it quickly became apparent that this same task could potentially be accomplished by the models. Thus, the models were designed to perform pixel-wise classification on imagery from the rover. Even though the team had produced geological maps of the landing site based on satellite imagery [2], there were no images on the ground that would have allowed for pretraining a classifier prior to the mission. This also follows the scenario of a real mission where there will be no prior imagery of the ground from the rover’s perspective. It came as a surprise to the team when the first panorama image from the rover contained something that did not appear on the remote sensing data: lichen. Since lichen was protected at this field site, a limitation was imposed on the rover that it could not traverse across lichen. Here then was another opportunity for the use of autonomous image classification, i.e., finding where the lichen was within an image. Methodology: Two algorithms were implemented for this task: random forests [3], and artificial neural networks [4]. To build the training data, all the images from the first few days where curated by the team. Zoom images and panorama images containing known xenoliths and lichen were used to label xenolith pixels and non-xenolith pixels. A sample panorama image is shown in Figure 1 and a sample zoom image is shown in Figure 2. Notice that xenoliths and lichen are visually very similar to one another. And where humans cannot differentiate between one and the other, machine learning models might be able to do better.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d91b1b996678425418321d9de5b251778eb506d7,https://www.semanticscholar.org/paper/d91b1b996678425418321d9de5b251778eb506d7,Knowledge engineering with semantic web technologies for decision support systems based on psychological models of expertise,"Machines that provide decision support have traditionally used either a representation of human expertise or used mathematical algorithms. Each approach has its own limitations. This study helps to combine both types of decision support system for a single system. However, the focus is on how the machines can formalise and manipulate the human representation of expertise rather than on data processing or machine learning algorithms. It will be based on a system that represents human expertise in a psychological format. The particular decision support system for testing the approach is based on a psychological model of classification that is called the Galatean model of classification. The simple classification problems only require one XML structure to represent each class and the objects to be assigned to it. However, when the classification system is implemented as a decision support system within more complex realworld domains, there may be many variations of the class specification for different types of object to be assigned to the class in different circumstances and by different types of user making the classification decision. All these XML structures will be related to each other in formal ways, based on the original class specification, but managing their relationships and evolution becomes very difficult when the specifications for the XML variants are text-based documents. For dealing with these complexities a knowledge representation needs to be in a format that can be easily understood by human users as well as supporting ongoing knowledge engineering, including evolution and consistency of knowledge. The aim is to explore how semantic web technologies can be employed to help the knowledge engineering process for decision support systems based on human expertise, but deployed in complex domains with variable circumstances. The research evaluated OWL as a suitable vehicle for representing psychological expertise. The task was to see how well it can provide a machine formalism for the knowledge without losing its psychological validity or transparency: that is, the ability of end users to understand the knowledge representation intuitively despite its OWL format. The OWL Galatea model is designed in this study to help in automatic knowledge maintenance, reducing the replication of knowledge with variant uncertainties and support in knowledge engineering processes. The OWL-based approaches used in this model also aid in the adaptive knowledge management. An adaptive assessment questionnaire is an example of it, which is dynamically derived using the users age as the seed for creating the alternative questionnaires. The credibility of the OWL Galatea model is tested by applying it on two extremely different assessment domains (i.e. GRiST and ADVANCE). The conclusions are that OWLbased specifications provide the complementary structures for managing complex knowledge based on human expertise without impeding the end users’ understanding of the knowledgebase. The generic classification model is applicable to many domains and the accompanying OWL specification facilitates its implementations.",,2016.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
970c8692ecd2cf3671f3a4c88969e001496235de,https://www.semanticscholar.org/paper/970c8692ecd2cf3671f3a4c88969e001496235de,AMBIVERT: A VIRTUAL PLATFORM FOR NETWORKING LABORATORIES,"Data networks are critical to leverage an increasingly global and connected world. As such, higher education degrees like telecommunications or computer science/engineering, generally include courses addressing networking technologies. Traditionally, laboratory classes in this field make use of real equipment such as personal computers, switches, routers, firewalls, access points and other physical hardware, providing hands-on experience with real equipment and use cases. The main plus of this learning model is therefore to familiarize students with equipment, interfaces, programming languages and operating systems used in production environments. This paradigm, however, has some major drawbacks, namely: Substantial initial investment is required to acquire networking equipment; Scenario setup and configuration can consume a large portion of class time; Scalability is limited as the number of enrolled students grows; Limited laboratory access outside regular class hours (time barrier); Students can’t connect remotely, meaning physical presence is required (geographic barrier); Continuous investment is required to keep up with technology updates; In order to overcome these drawbacks, this paper proposes AMBIVeRT, a platform that replaces the traditional network laboratory with a virtual class environment, emulating typical networking scenarios addressed in computer network related courses. The platform’s architecture is based on several virtual machines (VM) connected through virtual networks, which implement two different domains: an administration domain, for configuration, administration and initial setups, and a student domain, used by scholars to configure and explore different networking scenarios. The platform has been deployed in one single server (Cisco UCS C200 M2 High-Density Rack Server) running VMware vSphere Hypervisor. Each student/group has access to an individual virtual workbench, consisting of three different VMs connected in the same VLAN. A VM houses a GNS3 VM which functions as a back-end headless network emulator. The other two VMs run commonly used operating systems (OS), such as Windows Server and Linux, and act as frontends to the network emulator. Therefore, the platform supports multiple isolated virtual workbenches. Several network scenarios have been tested, from simple network architectures containing a few virtual equipment, to complex CCIE scenarios with 18 virtual appliances, some emulating advanced networking devices. This paper describes the AMBIVeRT platform’s architecture, its functionalities and current limitations. A system benchmark is also presented, with special focus on comparing the performance for different scenario’s complexity and number of active workbenches. Results have shown that AMBIVeRT platform is highly scalable and a serious alternative as a replacement for physical networking laboratories both for local and remote teaching. Moreover, the paper compares the AMBIVeRT platform with traditional physical networking laboratories and with other virtualized network environments, concerning available features, costs and scalability.",EDULEARN19 Proceedings,2019.0,10.21125/EDULEARN.2019.2342,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
06104630aa0518232ff736bce5ba079fbe0506f7,https://www.semanticscholar.org/paper/06104630aa0518232ff736bce5ba079fbe0506f7,Report on the First Working Group Meeting of the “AG Marketing”,"This contribution reports on the ﬁrst meeting of the new formed working group “Data Analysis and Classiﬁcation in Marketing (AG Marketing)” of the data science society (GfKl) held at the KIT, Karlsruhe, November 14th – 15th, 2019. The abstracts of the presentations given reﬂect the ongoing trend to exploit a large variety of digital data sources for marketing purposes and the need for advanced and innovative analysis methods. response model Second, binary proﬁt). Both shortcomings restrict usefulness approach when the return on investment from the direct marketing campaign. In this paper we propose a new approach that is able to overcome the discussed problems. The new approach connects ﬁndings from the ﬁeld of uplift modeling (see, e.g., Radcliﬀe and Surry (1999); Surry and Radcliﬀe (2011); Kane et al (2014)) with ﬁndings from the ﬁeld of sample selection (see, e.g., Heckman (1979)). Using the well-known Hillstrom data set and an own actual online shop direct marketing campaign data set (with data from >270k customers) as examples, we show that the new approach is well suited to correctly select the “right” customers as targets and to improve return on investment from direct marketing campaigns. Customers’ perception of high service quality contributes to customers’ loyalty and, therefore, drives a company’s success and survival within their competitive environment. Drawing on marketing literature, perceived service quality is determined by interaction quality and outcome quality. The latter comprises – among others – customers’ waiting times. Thus, call center managers are expected to provide high service quality by decreasing waiting times and simultaneously to keep operating costs at a minimum by deploying an appropriate number of agents. Hence, this paper conducts a model comparison to predict call arrivals with multiple seasonality. We compare traditional and barely investigated time series models (i.e. ARIMA, Random Walk, TBATS, Innovation State Space, Dynamic Harmonic Regression), regression models (i.e. Generalized Linear Models, Zero Inﬂated Models), and a machine learning approach (i.e. Random Forest). Additionally, we consider a new data processing related approach to enhance forecast accuracy: We investigate whether aggregating sub-daily data to daily values and in turn, disaggregating daily predictions according to averaged call distribution per interval yields more accurate forecasts than predictions of sub-daily data. We analyze call arrivals recorded at a German online retailer’s call center comprising 174.5 weeks of half-hourly data. We calculate forecast accuracy using cross validation in combination with a rolling forecast origin for 52 weeks. Our ﬁndings indicate that a Dynamic Harmonic Regression model has substantial predictive potential for practical use. Random Forest yields comparable results and outperforms traditional approaches. Moreover, we prove that time series models without explanatory variables perform more accurate on ordinary weekdays whereas machine learning and regression models with explanatory variables are more suitable to capture the course of special days, e.g., holidays. For the majority of the models, disaggregated daily predictions generate higher accuracy than predictions of sub-daily data. A good ﬁrst impression is crucial for the success of a sales interaction. Prior research demonstrates that individuals are able to make accurate predictions about one’s personality, skills, traits, or competencies from brief observations, so-called thin slices. Speciﬁcally, studies point on the importance of nonverbal cues (i.e., facial expressions, gestures) in the formation of initial impressions. However, these behaviors are perceived mainly unconsciously, which makes measurement a diﬃcult task. Moreover, existing research is dominated by post-exposure measures and neglects customers’ processing of impressions over time. This research tackles the problems outlined above and introduces continuous measurement of initial impressions in a sales context by a variety of diﬀerent data sources. We provide novel insights by applying high-precision coding of nonverbal behaviors in 22 videotaped sales presentations (elevator pitches) by making use of the body action and posture coding procedure (BAP), which allows the analysis of sales behaviors over the course of time based on over 140 diﬀerent variables with a granularity of 25 observations per second. In addition, respondents (n=663) evaluated these presentations by means of a program analyzer with a granularity of 2 observations per second. Findings show that a substantial percentage of respondents form their impression about the sales representative within the ﬁrst few seconds, whereas negative ﬁrst impressions are formed faster than positive ones. The application of continuous measures (of nonverbal behaviors and customer responses) provides various advantages over existing means of measurement and yields important implications for marketers and future research. It is well known that store-level brand sales may not only depend on contempo-raneous variables like current own and competitive prices or other marketing activities, but also on past prices representing customer response to price changes. It has further been shown that accounting for lagged prices in a sales response model can increase expected brand proﬁts over a static model that ignores price dynamics. On the other hand, non- or semiparametric regression models have been proposed in order to accommodate potential nonlinearities in price response, and related empirical ﬁndings indicate that price eﬀects may show complex nonlinearities, which are diﬃcult to capture with parametric models. Additionally, it is nowadays well established to incorporate store heterogeneity in sales response models, independent whether parametric or nonparametric modeling is used. We combine nonparametric price response modeling, heterogeneity and dynamic pricing. In particular, we model sales response ﬂexibly using a Bayesian semiparametric approach and include the price of the previous period as well as further time-dependent eﬀects. All nonlinear eﬀects are modeled via P-splines, and embedding the semiparametric model into a hierarchical Bayesian framework further enables the estimation of store-speciﬁc (lagged) price response curves. In an empirical study, we demonstrate that our new model provides both more accurate sales forecasts and higher expected proﬁts as compared to competing models that either ignore price dynamics or just include them in a parametric way. Optimal price policies for brands are determined by a discrete dynamic programming algorithm. The Pareto/NBD model is one of the best-known and most used models in customer base analysis. Still, practitioners are confronted with the question of which cohort size and length of calibration period are necessary in order to obtain reliable parameter estimates. In the past years, the usage of Monte Carlo Markov Chain (MCMC) algorithms has increased as these deliver a full posterior distribution rather than just a point estimate for the model parameters. Using MCMC additionally requires hyper parameters whose choice has barely been discussed in the literature yet. We, therefore, perform a broad simulation study on Pareto/NBD distributed data sets to derive minimal requirements for the model´s usage and to outline the choice and inﬂuence of diﬀerent hyper parameters. The results show that the recovery of the purchase process already works well for cohort sizes of 1,000 customers and a calibration period of 52 weeks. Since we are in a non-contractual setting, the dropout process cannot be observed and is therefore much more diﬃcult to estimate from the data. It requires a calibration period of at least two years and 5,000 customers. For all data sets, we generate MCMC estimates using diﬀerent hyper priors as well as the uninformative Jeﬀreys´ prior. The goodness of ﬁt measures tell us that that Jeﬀreys´ prior should be preferred to informative hyper distributions. This especially holds, when we have no preliminary information on our data set.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cad2b66254538e0aed1f737d7409fcb01a5f4ac1,https://www.semanticscholar.org/paper/cad2b66254538e0aed1f737d7409fcb01a5f4ac1,WILDetect: An intelligent platform to perform bird censuses automatically in maritime ecosystem,"The oceans cover two-thirds of the Earth and maritime economy has always been a diverse landscape with abundant resources. With the applications of emerging fields of science and technology in new and existing industries, prominent companies and research organizations have been recently developing and deploying evolving technologies supported by advanced maritime mechatronics systems (AMMSs) to explore and exploit this tough landscape. This massively evolving industry is impacting maritime ecosystem dramatically. The habitats of the marine life, current characteristics of specific types of species and diverse landscape of maritime industry around these habitats are of interest to many researchers, authorities and policymakers. Automatic detection, locating and immediate monitoring of the marine life along with the industry fields around the habitats of this ecosystem may be helpful to both reveal the current and future possible impacts and dictate required policies accordingly. The aim of this study is to develop a novel methodology in order to detect maritime bio ecosystem and perform bio census automatically, particularly birds in regional surveys composed of aerial images. Within this context, a new non-parametric supervised Machine Learning (ML) approach supported by Reinforcement Learning (RL) - WILDetect is built, which employs several hybrid techniques to segment, split and count maritime species, in particular, birds, in order to perform censuses automatically and efficiently. The effectiveness of the proposed approach is demonstrated by a number of experiments performed on 13 surveys composed of gannet species using retrospective data analysis techniques.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8d299258291512b11750cd61746a4f24193f7c07,https://www.semanticscholar.org/paper/8d299258291512b11750cd61746a4f24193f7c07,It’s All About Data: How to Make Good Decisions in a World Awash with Information,"The rise of big and alternative data has created significant new business opportunities in the financial sector. As we start on this journey of fast-moving technology disruption, financial professionals have a rare opportunity to balance the exponential growth of artificial intelligence (AI)/data science with ethics, bias, and privacy to create trusted data-driven decision making. In this article, the authors discuss the nuances of big data sets that are critical when one considers standards, processes, best practices, and modeling algorithms for the deployment of AI systems. In addition, this industry is widely guided by a fiduciary standard that puts the interests of the client above all else. It is therefore critical to have a thorough understanding of the limitations of our knowledge, because there are many known unknowns and unknown unknowns that can have a significant impact on outcomes. The authors emphasize key success factors for the deployment of AI initiatives: talent and bridging the skills gap. To achieve a lasting impact of big data initiatives, multidisciplinary teams with well-defined roles need to be established with continuing training and education. The prize is the finance of the future. TOPICS: Simulations, big data/machine learning Key Findings • The rise of alternative data in finance is creating major opportunities in all areas of the financial industry, including risk management, portfolio construction, investment banking, and insurance. • To build trusted outcomes in AI/ML initiatives, financial professionals’ roles are critical. Given the many nuances in using big data, there is a need for vetted protocols and methods in selecting data sets and algorithms. Best practices and guidelines are effective in reducing the risks of using AI/ML, including overfitting, lack of interpretability, biased inputs, and unethical use of data. • Given the major shortage of talent in AI/data science in finance, practical training of employees and continued education are keys to scale roll out to enable future of finance.",,2020.0,10.3905/jfds.2020.1.025,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5511de2c2c02a67318dcf7944c9180a8ad0f6b8d,https://www.semanticscholar.org/paper/5511de2c2c02a67318dcf7944c9180a8ad0f6b8d,Escalation Prediction using Feature Engineering: Addressing Support Ticket Escalations within IBM's Ecosystem,"Large software organizations handle many customer support issues every day in the form of bug reports, feature requests, and general misunderstandings as submitted by customers. Strategies to gather, analyze, and negotiate requirements are complemented by efforts to manage customer input after products have been deployed. For the latter, support tickets are key in allowing customers to submit their issues, bug reports, and feature requests. Whenever insufficient attention is given to support issues, there is a chance customers will escalate their issues, and escalation to management is time-consuming and expensive, especially for large organizations managing hundreds of customers and thousands of support tickets. This thesis provides a step towards simplifying the job for support analysts and managers, particularly in predicting the risk of escalating support tickets. In a field study at our large industrial partner, IBM, a design science methodology was employed to characterize the support process and data available to IBM analysts in managing escalations. Through iterative cycles of design and evaluation, support analysts' expert knowledge about their customers was translated into features of a support ticket model to be implemented into a Machine Learning model to predict support ticket escalations. The Machine Learning model was trained and evaluated on over 2.5 million support tickets and 10,000 escalations, obtaining a recall of 79.9% and an 80.8% reduction in the workload for support analysts looking to identify support tickets at risk of escalation. The features developed in the Support Ticket Model are designed to serve as a starting place for organizations interested in implementing the model to predict support ticket escalations, and for future researchers to build on to advance research in Escalation Prediction.",ArXiv,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d9ca5fd6af24bba04c56ac6453c4371e532de705,https://www.semanticscholar.org/paper/d9ca5fd6af24bba04c56ac6453c4371e532de705,"Fast, private, and fair","Im ag e co ur te sy o f O U D at a La b you notice of those students admitted in the computer engineering department 80 percent were male and only 20 percent of were female. With a deeper look into those departments’ admission rates and rejection rates, it can be noted the trends are opposite of the initial comparison. The computer science department has a higher rate of female acceptance and a lower rate of female applicants in the entire population. Thus, it can be found female admission corresponds to the subpopulation of applicants. This investigation looks at the association in the whole population that focuses on the reverses within the subpopulations defined by categorical variables. With Simpson’s Paradox, we can find surprising and interesting patterns in the data, which can be useful for companies, universities, and other entities. This is amazing work that can impact the diversity, inclusion, and programs in place at organizations. Next up is Visual Privacy at the OU Data Lab, which explores visual privacy in the realms of IOTs, social media networks, and graph networks. The Visual Privacy project was started in 2017 and has grown into collaborations with various universities. Specifically, my research project, “VIPER,” investigates social media users posting images that contain privacy leaks in regard to themselves or someone else. These private images can include baby faces, credit cards, phone numbers, social security cards, etc. I proposed the use of supervised learning techniques train machine learning models to identify and score private images. To make this research easily integrated, we have been working to deploy a mobile application that will allow users to engage with mitigation techniques to further increase their privacy measures. This concept has been explored in IoT devices within developing smart cities. The need for visual privacy and mitigation strateson’s paradox” is a specific type of occurrence that is a trend in a population. As an example, imagine you are a high-school student deciding which departments to apply to in a university. In your decision process, the acceptance rates at those universities may be an important factor. You start by comparing admission rates for the computer engineering and computer science departments based on last year’s data. From this comparison, you notice the computer engineering department has a better admission rate: 33 percent to 29 percent. Initially you may favor computer engineering. However, with further investigation, A t the University of Oklahoma (OU) ther are several research labs including the OU Data Lab led by Dr. Christan Grant, of which I am a member. The lab focuses on applications of fairness forensics, machine learning, and data management systems. First up, Fairness Forensics. Dr. Kate Crawford coined the term “fairness forensics” during her keynote presentation at the 2017 Conference on Neural Information Processing Systems. At OU, researchers are working on several projects that address potential biases, such as Detecting Simpson’s Paradox. The phrase “SimpLABZ",XRDS,2020.0,10.1145/3433128,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
bd77d7c9e9a541d4851a1dd24ebb0275d476c229,https://www.semanticscholar.org/paper/bd77d7c9e9a541d4851a1dd24ebb0275d476c229,"PODPAC: open-source Python software for enabling harmonized, plug-and-play processing of disparate earth observation data sets and seamless transition onto the serverless cloud by earth scientists",,Earth Science Informatics,2020.0,10.1007/s12145-020-00506-0,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3415a0816ecf56bbeb0ef24d6035b4a23bf90f49,https://www.semanticscholar.org/paper/3415a0816ecf56bbeb0ef24d6035b4a23bf90f49,REAXYS: An excellent tool for cracking chemistry problems,"Medicinal Chemistry is the basis of successful drug discovery and its principal component is effective synthetic organic chemistry. It takes numerous years to become an experienced medicinal chemist who can predict the outcome of various chemical reactions correctly and provide the most efficient synthetic scheme to a pharmaceutically active and safe compound or compound library. Even then, Merck quote, more than half of the cornerstone reactions we attempt are failing (Science 02, 2015), and non-yielding synthesis steps are part of the reasons why the amount of investment into pharmaceutical research and development (R&D) is high (2016, $157 billion). The design of an effective synthesis route to a correctly substituted molecule can become a rate-limiting process and occasionally, molecules are not synthesized, because it simply takes too long to find suitable chemistries. If we want to prepare any molecule of interest quickly and with lower failure rate than today, a disruptive synthesis prediction technology will be required. In line with such a paradigm shift we will need chemists that combine excellent synthesis knowledge as well as competence in machine learning methods and artificial intelligence (AI). 
For the first time computer-aided retro synthesis tools, which can predict reactions correctly, are available and do not rest on the input of rules from researchers. Waller and Segler et al. (Nature, March 2018) in collaboration with Elsevier have developed a ‘deep learning’ computer program that produces blueprints for the sequences of reactions needed to create small organic molecules, such as pharmaceutical active molecules. This novel artificial-intelligence tool has processed nearly every reaction ever published (> 15 million) and has the potential to transform the way synthetic & medicinal chemists work in the future. Segler & Waller et al. tested the synthetic routes that the program generated in a double-blind trial with 45 organic chemists from two institutes in China and Germany and the routes have proven scientifically sound and robust. Increasing the success rate in synthetic chemistry would have a huge benefit in terms of treating diseases more resourcefully, discovering more sustainable chemical solutions and minimizing expenditure in R&D. Reaxys-PAI Predictive Retrosynthetic solution, developed in collaboration between Elsevier and Waller and Segler et al., deploys next generation AI technologies to augment chemical synthesis knowledge, drives innovation and helps to save time and cost.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d6b33882fc593de512e4c89553cf2e3bd16c94d5,https://www.semanticscholar.org/paper/d6b33882fc593de512e4c89553cf2e3bd16c94d5,BIG DATA: FRAMEWORKS AND CHALLENGES,"In today’s world, data is produced and consumed at a fast pace. This has given birth to a domain to collect, store, process and maintain this huge influx of data called big data. Running any form of analysis on such amounts of data is challenging at best and so new and creative techniques were needed to be created. Techniques had to be developed to handle different types of processing like batch processing and stream processing. Each type has its own individual method of function by which it carries out its objectives. In addition to this domain, big data is usually collaborated with other emerging domains like information security, intelligent systems and data science. Keywords— Big data; Big data analytics; Apache Spark; Apache Hadoop; Cloud Computing; IoT. INTRODUCTION Up until 2007, relational databases and data warehouses were used to store the data by an organization. But eventually after the surge of social media, it became harder to store the data in a traditional manner which gave rise to a new concept named “Big Data”. Big Data was introduced for storing large amount of data in an efficient manner. The data being produced due to the rise of technologies can be structured or unstructured depending on the source. It is difficult to process such data as it contains billions of records. The need comes specifically from big companies such as Google, Netflix, Facebook etc. since a huge amount of data gets generated by them on a daily basis. Big Data Analytics is used to process these massive and complex datasets. HISTORY Keeping tracks of data dates back to 7000 years ago when the Mesopotamians introduced accounting for recording crop growth. The earliest implementation of data dates back to 1887 when computing machines used for punching holes in paper cards were used to maintain census data by Herman Hollerith. Data projects and processing machines were used in World War II for deciphering codes, therefore helping in reducing the time taken to decipher the codes. The first data center was implemented by US government for handling tax returns and introduced the concept of electronic storage of data. In 1989, World wide Web was introduced by British scientist Tim Berners-Lee. The 90s were the years when more and more devices started connecting to the internet. The first supercomputer was introduced in 1995 which could do large number of calculations in a second. The term Big Data was coined for the first time in 2005 by O’Reily media, only after a year after the term Web 2.0 was created. Big Data refers to a large amount of data which is impossible to manage with traditional data tools. In 2009, the government of India decided to keep a record of photograph, iris scan and fingerprints of all 1.25 billion citizens. This data is stored in one of the biggest biometric databases in the world. PARAMETERS The data is characterized by 3 V’s in big data. The 3 V’s are: Volume, Variety and velocity. © 2018 JETIR October 2018, Volume 5, Issue 10 www.jetir.org (ISSN-2349-5162) JETIRDQ06076 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 536 A. Volume It is the amount of data which is being generated from various sources. The sources can be social media, online applications, websites etc. YouTube has 1 billion users, Facebook has 2 billion users, Instagram and Twitter have 700 million and 350 million users respectively. These users help in contributing billions of photos, tweets, posts etc. The large amount of data that is being generated every second and every hour is imaginable. B. Velocity In big data, Velocity is the rate at which new data gets generated per second. According to a research, 3.5 billion search results are generated in Google search and over 900 million images are uploaded to Facebook every day. The velocity is very high and big data helps the companies or organizations to hold the explosion of data. C. Variety Variety is the type of data that is being generated. There are usually two types of datastructured and unstructured. Examples of structured data include messages, photos, videos etc. Examples of unstructured data include electronic mails, recordings etc. Variety helps in classifying the data into different categories. Fig 1. Parameters of big data FRAMEWORKS There are different frameworks required to process different kinds of data that is being generated. Data is basically processed in two categoriesbatch processing and stream processing. The frameworks are used to process the dataset according to the condition that needs to be fulfilled. Fig 2. Workflow of big data project © 2018 JETIR October 2018, Volume 5, Issue 10 www.jetir.org (ISSN-2349-5162) JETIRDQ06076 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 537 A. Hadoop It is one of many classic and highly developed frameworks in use today. This framework is almost synonymous with big data. Hadoop is used to do batch processing i.e. the data is put in the form of batches for further processing. It has MapReduce and Hadoop Distributed File System (HDFS) as its primary component in the ecosystem. Hadoop is an important framework in data as it has HDFS and MapReduce which are used for another big data storage for processing. B. Apache Spark Spark is another major and frequently used big data framework. Unlike Hadoop, Spark works on stream processing i.e. data is processed as soon as it is entered in the system. It is faster and more flexible than Hadoop. Spark uses HDFS as it doesn’t contain a storage layer of its own. Spark consists of MLlib, SparkSQL, GraphX and other components. Spark is a real time framework. Fig 3. Architecture of Apache Spark C. Apache Flink Flink is a streaming dataflow engine which facilitates distributed computation. Flink is a combination of both batch and real time processing framework. Flink provides number of application interfaces such as Streaming API, Static data API and SQL-like query Application Programming Interface for Java, Scala and Python respectively. Flink contains an in-house ML and libraries for graph processing. Flink’s features include high performance, low latency, stateful computations, continuous streaming and fault tolerance among many others. D. Apache Storm Storm is a real time, distributed computation system. It is used for processing streaming data and has benchmarked at processing one million tuples per second. It is highly scalable and uses Lisp-like programming languages. Storm is applicable for real time analysis and machine learning, and other cases where higher data velocity is present. Storm can integrate with Hadoop ecosystems and work on YARN, providing a solution for real-time stream processing. Storm is fast, scalable, reliable, fault tolerant and easy to operate once deployed. E. Apache Samza Samza is a real time distributed stream processing framework. It is built on Apache Kafka for messaging and YARN for resource management. Samza has simple API, is fault tolerant, durable, scalable and pluggable. Samza provides processor isolation as it works with YARN and resource isolation with Linux CGroups. Samza also provides restoration and snapshotting of a processor state as it is built to handle large amount of space per partition. © 2018 JETIR October 2018, Volume 5, Issue 10 www.jetir.org (ISSN-2349-5162) JETIRDQ06076 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 538 SNO FRAMEWORK TYPE OF PROCESSING 1 Apache Hadoop Batch Processing 2 Apache Spark Real Time Processing 3 Apache Flink Stream Processing 4 Apache Storm Real Time Processing 5 Apache Samza Real Time Distributed Stream Processing Table 1. Types of Processing for various frameworks SNO FRAMEWORK COMPONENTS 1 Apache Hadoop YARN, HDFS, MapReduce, Resource Manager 2 Apache Spark GraphX, MLlib, SparkSQL, Spark Core, Spark Streaming 3 Apache Flink HDFS, Local-FS, Flink Kernel, DatasetAPI 4 Apache Storm Map RFS, Message Queue 5 Apache Samza Kafka, Samza Task Runner, YARN Client Table 2. Components in various frameworks CHALLENGES IN BIG DATA Web applications face big data frequently due to texts and documents, social computing and search indexing. Big data has been involved in domains like healthcare, medical, scientific researches, biochemistry etc. It provides new opportunities along with some challenges. A. Data Analysis and Storage One of the first challenges is storage medium in speed. Data accessibility should be the highest priority for representation. The data should be easily accessible for analysis. To overcome the disadvantage of slow inputoutput performance by hard-disk, Phase Change memory and SolidState Drive (SSD) were introduced. However, the techniques discussed above do not have the capability to perform these big data processes. Data diversity is another challenge. While handling big datasets, data selection and data reduction is a mandatory, required task. Ensuring consistency is a major issue. Hadoop and MapReduce help in acquiring huge amount of data in an © 2018 JETIR October 2018, Volume 5, Issue 10 www.jetir.org (ISSN-2349-5162) JETIRDQ06076 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 539 unstructured format in significantly short time span. A framework was discussed by Das and Kumar [1] to analyze the data. B. Computational Complexities Representation and knowledge discovery are important issue which requires sub fields like presentation, retrieval, authentication and representation to name a few. A lot of hybrid techniques are used to handle issues and/or queries, but every technique depends on the type of issue and/or queries. As the size keeps increasing in big data, these techniques are not efficient to obtain any meaningful information. Large dataset can be managed by data warehouse and data marts. More computation complexities are required for analyzing large datasets. A mathematical framework is harder to establish when it comes to the domain of big data. Although, t",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3af91b1be29551c1cf19866f47e7035b7e139343,https://www.semanticscholar.org/paper/3af91b1be29551c1cf19866f47e7035b7e139343,Objective Image Quality Assessment: Facing The Real-World Challenges,"There has been a growing interest in recent years in the development of objective image quality assessment (IQA) models, whose roles are not only to monitor image quality degradations and benchmark image processing systems, but also to optimize various image and video processing algorithms and systems. While the past achievement is worth celebrating, a number of major challenges remain when we apply existing IQA models in realworld applications. These include obvious ones such as the challenges to largely reduce the complexity of existing IQA algorithms and to make them easy-to-use and easy-to-understand. There are also challenges regarding the applicability of existing IQA models in many real-world problems where image quality needs to be evaluated and compared across dimensionality, across viewing environment, and across the form of representations − specific examples include quality assessment for image resizing, color-togray image conversion, multi-exposure image fusion, image retargeting, and high dynamic range image tone mapping. Here we will first elaborate these challenges, and then concentrate on a specific one, namely the generalization challenge, which we believe is a more fundamental issue in the development, validation and application of IQA models. Specifically, the challenge is about the generalization capability of existing IQA models, which achieve superior quality prediction performance in lab testing environment using a limited number of subject-rated test images, but the performance may not extend to the real-world where we are working with images of a much greater diversity in terms of content and complexity. We will discuss some principle ideas and related work that might help us meet the challenges in the future. Introduction Over the past decades, a growing number of researchers and engineers in the image processing community have started to realize the importance of image/video quality assessment (IQA/VQA) [40, 29, 4]. This is not surprising because no matter what image/video processing problems we are working on, the same issues repeatedly come up − How should we evaluate the images generated from our algorithms/systems? How do we know our algorithm/system is creating an improvement between the input and output images, and by how much? How can we know one algorithm/system performs better than another, and by how much? What should be the quality criterion for which the design of our algorithms/systems should be optimized? Since the human eyes are the ultimate receivers in most image processing applications, human subjective visual testing would be a reliable solution. However, with the exponential increase of the volume of image/video data being generated daily, it becomes impossible to address these quality issues in a timely manner by subjective visual testing, which is slow, cumbersome and expensive. Instead, only trusted objective IQA models may potentially meet these needs. In academia, objective IQA has been a hot research topic, especially in the past 15 years [35, 4, 29]. First, the commonly used numerical disotrtion/quality measures in the past − the mean squared error (MSE) and the peak signal-to-noise ratio (PSNR) − have been shown to correlate poorly with perceived image quality [28, 30]. Second, a large number of perceptually more meaningful IQA models have been proposed, including full-reference (where a perfect quality reference image is available when evaluating a distorted image) [35, 4, 29], no-reference (where the reference image is not accessible) [34, 24, 31], and reduced-reference (where only partial information about the reference image is available) models [39, 36, 31, 29]. Third, several design principles have been discovered and repeatedly demonstrated to be useful in the design and improvement of IQA models. These include psychophysical and physiological visibility models [35, 4], the structural similarity (SSIM) approaches [28, 32, 33, 20, 49], the natural scene statistics (NSS) and information theoretic approaches [36, 39, 21, 31], the visual saliency based approaches [50], and the machine learning based approaches [6]. Fourth, a number of subject-rated image quality databases have been created and made publicly available [22, 7, 8, 17, 16, 47]. They provide a common benchmark platform for the evaluation and comparison of IQA models, among which several algorithms have achieved high correlations with the subjective mean opinion scores (MOSs) of the test images [23, 38, 33, 49]. In the video delivery industry, perceptual objective IQA methods such as the SSIM algorithm have been incorporated into many practical hardware and software systems to monitor image/video quality degradations and to test/compare image/video encoders and transcoders [27, 25, 26]. The wide use of SSIM has resulted in a Primetime Engineering Emmy Award given by the Academy of Television Arts and Sciences [1]. The remarkable development and successful deployment of modern IQA methods are definitely worth celebrating. Nevertheless, this does not necessarily mean that the existing IQA models have already met the real-world challenges. Otherwise, they should have made a much stronger impact and become a gamechanging factor in the industry. Using the video delivery industry as an example, even now most practitioners are still equating bitrate with quality in the practical design of video delivery architectures. However, using the same bitrate to encode different video content could result in dramatically different visual quality. Clearly, the perceptual quality of the video itself, which is presumably the ultimate evaluation criterion of the whole video delivery system, has not been placed at the driver’s seat. While it is understandable that quality degradation is inevitable at many stages in the video delivery chain due to practical constraints, the real concern here is that there is no existing protocol to monitor and control such quality degradation. As a result, various tricks have been used to manipulate the video content and network resources are allocated in suboptimal ways, leaving the creative intent of the content producers unprotected. While it is certain that the industry needs to be better informed about the great potentials of making the best use of IQA/VQA models, we believe that an equally important aspect that slows down the process is that the existing IQA/VQA models still do not meet many real-world challenges. In the following sections, we will elaborate some of these challenges and then focus on a specific one, namely the generalization challenge. We wish our discussions on some fundamental ideas could provide some useful insights for the future development of IQA models that may meet these real-world challenges. The Real-World Challenges Here we make a list of real-world challenges, many of which are described in more details through examples of practical scenarios. 1. It is highly desirable to reduce the complexity of the IQA/VQA algorithms so that they can be computed in realtime or in an even faster speed. This is especially useful in time-sensitive applications such as live broadcasting and videoconferencing. Many existing models are far from meeting this challenge. 2. It is essential to make the IQA/VQA scores easy-to-use and easy-to-understand. For example, the raw SSIM score does not have an explicit perceptual meaning, making it difficult to determine what level of SSIM index can warrant an excellent video quality and how much improvement in the SSIM index is sufficient to create visible quality improvement. Mapping the raw scores into a perceptually linear domain that is easily linked to human expressions about image quality is desirable. 3. The same video stream shown on different display devices could result in very different perceptual quality. For example, a strongly compressed video that exhibits very annoying artifacts on a large TV could appear to have fine quality when viewed on the screen of a smartphone. The quality may also change significantly when the video is watched on the same TV but at two different viewing distances, one at the default distance and the other at a very close distance. However, existing IQA/VQA models give the same score based on the video stream only, completely ignorant of the viewing device and viewing condition. 4. In a video-on-demand application, a high-quality highresolution (e.g., 4K) source video may be encoded into multiple video streams of different resolutions (e.g., 1080p, 720p, 360p, 240p, etc.) and different bit rates, aiming for satisfying a variety of user needs. In order to measure the quality of the encoded videos, most existing VQA models cannot be computed because the source (reference) and test videos have different spatial resolutions. 5. An image or video may need to be displayed on a screen that has a spatial resolution higher than that of the image resolution. As a result, spatial interpolation is performed. Again, most existing VQA models are not applicable because the reference and test images have different spatial resolutions. 6. An image or video of imperfect quality (e.g., being compressed at an earlier stage) is received and then transcoded to multiple images or videos with different bitrates and resolutions. Most existing IQA/VQA models are not applicable not only because they do not allow for cross-resolution quality assessment, but also because they assume the original reference image/video to have perfect quality, which is not the case here. How to carry out “degraded reference” IQA/VQA is a major challenge. 7. A high dynamic range (HDR) image (e.g., the pixels are in 10 or more bit depths) is tone mapped to a standard dynamic range (SDR) image (8 bits per pixel) in order to be visualized on an SDR display. There is certainly information loss that we would like to capture. However, most existing IQA models do not apply because they cannot compare images/videos with different dynamic ranges.",IQSP,2016.0,10.2352/ISSN.2470-1173.2016.13.IQSP-205,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3f3f00fc84c5ea2f6fba2dea24d139dfcc2278bb,https://www.semanticscholar.org/paper/3f3f00fc84c5ea2f6fba2dea24d139dfcc2278bb,Visual Model Interpretation for Epidemiological Cohort Studies,"Epidemiological cohort studies investigate the cause and development of diseases in human populations. Conventional analyses are challenged by recently increasing study sizes, which is why the incorporation of machine learning gains popularity. State-of-the-art classifiers are however often hard to interpret – an important requirement in medical applications. This thesis addresses the gap between predictive power and interpretability in the context of cohort study analysis. Main contribution is the development of an interactive visual interface for the interpretation and comparison of probabilistic classifiers. It supports the analysis of important features at both global and individual level, computation of partial dependence, and iterative construction of meaningful feature groups. To analyse the longitudinal influence of features, the user can modify the feature set by removing a feature or replacing its value by a previous examination record. The developed visual interface is evaluated in two case studies in order to test its effectiveness for the generation and validation of research hypotheses. The case studies include a realworld epidemiological cohort study and synthetic data. The results indicate the interface’s usefulness for epidemiological research, but also reveal necessary further work for the deployment into a productive environment.",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
505195a1db461b083ee30141923b1610ec2cb8ee,https://www.semanticscholar.org/paper/505195a1db461b083ee30141923b1610ec2cb8ee,Artificial intelligence and the dreaded 's.,"Numerous industries are being disrupted by growth in new technologies, especially information technologies, and healthcare is no exception. Advances in robotics, wireless sensor networks, 5D printing, and cloud technologies are reshaping countless industries. I am intrigued by the increasing importance of automation, machine learning, and artifi cial intelligence (AI) in healthcare. Let us explore three questions together: • Where are common applications of AI and automation in healthcare? • What implications for physician assistants (PAs) arise from increased automation and AI in caring for patients? • Did AI bring back the ’s that causes any self-respecting PA to cringe? I nearly panicked recently when I caught sight of the following two headlines from online articles about new healthcare technologies, which might lead a person to think the PAs of the future are not people at all. At the very least, I was ready to e-mail the AAPA communications team to combat those pesky apostrophes. The articles actually detailed advances in automation and AI within healthcare. Bright.MD raises another $8M for “virtual physician’s assistant” SmartExam (www.mobihealthnews.com/content/ brightmd-raises-another-8m-virtual-physicians-assistantsmartexam) Healthcare Chatbots: The Physician’s Assistant of the Future? (http://blog.kantarhealth.com/blog/brian-mondry/ 2016/11/28/healthcare-chatbots-the-physician’s-assistantof-the-future) Next, let us sort out AI and automation. According to Merriam Webster, artifi cial intelligence is the capability of a machine to imitate intelligent human behavior. Automation, on the other hand, is the automatically controlled operation of an apparatus, process, or system by mechanical or electronic devices that take the place of human labor. COMMON APPLICATIONS A widely adopted automation in healthcare is appointment reminder software that automatically reminds patients of their upcoming scheduled appointments, with options to customize the message and/or time it is delivered for patient preference. Similarly, missed appointment notifi cation systems can alert a PA to a potentially worrisome pattern of missed appointments for a patient identifi ed as high-risk. Robotics, commonly deployed in areas such as pharmacy and surgery, are automations proven to increase effi ciency and safety. According to CB Insights, about 86% of healthcare provider organizations, life science companies, and healthcare technology vendors are using AI technology. The most common applications seem to fall into one of ten categories: managing medical records and other data; doing repetitive jobs such as analyzing tests, interpreting radiologic studies, and data entry; helping design treatment plans; digital consultation (such as the Babylon app); virtual nurses (such as the Molly app), medication management (such as the AiCure app); drug development; precision medicine; health monitoring; and healthcare system analysis.1 Numerous tech giants are investing heavily in AI applications for healthcare as well, such as Microsoft’s Healthcare NExT initiative and Google’s Deepmind Health.",JAAPA : official journal of the American Academy of Physician Assistants,2018.0,10.1097/01.JAA.0000530302.23280.25,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
411b6d4d72be8f21b8bb2ddfbb0fadc7a053b8d0,https://www.semanticscholar.org/paper/411b6d4d72be8f21b8bb2ddfbb0fadc7a053b8d0,A tail of two PanCancer projects: Somatic variant identification and driver gene discovery using TCGA,"OF THE DISSERTATION A Tail of Two PanCancer Projects: Somatic Variant Identification and Driver Gene Discovery Using TCGA. by Matthew H. Bailey Doctor of Philosophy in Biology and Biomedical Sciences Human and Statistical Genetics Washington University in St. Louis, 2018 Professor Li Ding, Chair The implementation of next-generation genomic sequencing has exploded over the past dozen years. Large consortia, such as The Cancer Genome Atlas (TCGA); the International Cancer Genetics Consortium (ICGC); and the Pediatric Cancer Genome Projects (PCGP), made great strides in democratizing big data for the scientific community. These data sets provide a rich resource to build tools for somatic variant discovery and exploratory analysis. Public repositories hold the answer to many novel biological and clinical revelations i.e., the discovery of complex indels, splice creating mutations, alternative super enhancer binding sites, machine learning models to predict mutation impact, and cancer subtype classification and identification. At the end of 2014, seven additional cancer types and 11 different pediatric tumor cohorts were publicly available when compared to the Ding lab’s first PanCancer effort [Kandoth et al., 2013]. Motivated by the possibility of novel cancer driver gene discovery, we launched a new PanCan2 effort. We assembled sequence data from 8,018 cancer cases representing a combined 30 pediatric and adult cancer types from 8 organ systems. Analysis of the resulting data corpus identified 270 cancer-associated genes, 107 of which have not been previously reported in PanCancer studies. Pediatric-enriched mutant genes (e.g., IL7R, PAX5, and H3F3A) were found in tumors from the hematopoietic and central nervous systems, consistent with their roles in early development. Distinctive mutational architectures were identified for each of the 8 organ systems, reflecting the tissue of origin and likely exposure to similar environmental factors. xv TP53 mutant vs. TP53 wild-type tumors had largely distinct patterns of co-occurring mutations, suggesting a pivotal role of TP53 in shaping the mutational network. Cis-activation of receptor tyrosine kinases at mutational, expression, and phosphorylation levels, as well as transactivation of hormone-related transcription factors, were identified through the integration of multiple data types. In the end, this effort did not result in a publication because we did not perform uniform variant calling across all samples and relied primarily on publicly available data sets. Armed with the knowledge that reviewers would require a complete reboot of the TCGA variant calls before another PanCancer paper would be considered, Dr. Li Ding thoughtfully submitted a proposal to acquire funding necessary for the recalling of all TCGA exome sequencing bams using many different calls. This effort is referred to as the Multi-center Mutation Calling in Multiple Cancers (MC3). TCGA cancer genomics data set includes over 10,000 tumor-normal exome pairs across 33 different cancer types, in total >400 TB of raw data files required reanalysis. A comprehensive encyclopedia of somatic mutation calls for the TCGA data was created to enable robust cross-tumor-type analyses. Our approach accounts for variance and batch effects introduced by the rapid advancement of DNA extraction, hybridization-capture, sequencing, and analysis methods over time. We present best practices for applying an ensemble of seven mutation-calling algorithms with scoring and artifact filtering. The data set created by this analysis includes 3.5 million somatic variants and forms the basis for PanCancer Atlas papers. The results have been made available to the research community along with the methods used to generate them. This project is the result of collaboration from a number of institutes and demonstrates how team science drives large genomics projects. Having a complete overhaul of all somatic mutations available in the TCGA, we sought to use these data for a complete TCGA PanCancer analysis. However, instead of relying wholly on in-house algorithms we also performed PanSoftware analysis spanning 26 computational tools from multiple institutions to catalog driver genes and mutations. In total, 9,423 tumor exomes (comprising all 33 of TCGA projects) we identified 299 driver genes with implications regarding their anatomical sites and cancer/cell types. Sequenceand structure-based analyses identified >3,400 putative missense driver mutations supported by multiple lines of evidence. Experimental validation confirmed 60%-85% of predicted mutations as likely drivers. We found xvi that >300 MSI tumors are associated with high PD-1/PD-L1, and 57% of tumors analyzed harbor putative clinically actionable events. Our study represents the most comprehensive discovery of cancer genes and mutations to date and will serve as a blueprint for future biological and clinical endeavors. One of many new waves in the genomics era will be the cohesive integration of multi-omics data. At present, our current understanding of molecular processes in oncogenesis is governed by known-knowns. This is clearly illustrated in our marker paper that displays insights into cancer through the synthesis of findings from TCGA PanCancer Atlas [Ding et al., 2018]. In closing the final chapters of TCGA, we addressed three facets of oncogenesis: (1) somatic driver mutations, germline pathogenic variants, and their interactions in the tumor; (2) the influence of the tumor genome and epigenome on transcriptome and proteome; and (3) the relationship between tumor and the micro-environment, including implications for drugs targeting driver events and immunotherapies. These results will anchor future characterization of rare and common tumor types, primary and relapsed tumors, and cancers across ancestry groups and will guide the deployment of clinical genomic sequencing. In quick succession, both The Cancer Genome Atlas and the International Cancer Genetics Consortium provided the cancer research community with consensus somatic mutation calls for captured exome sequencing created by the Multi-center Mutations Calling in Multiple Cancers effort (MC3) and whole genome sequence provided by the PanCancer (PCAWG). 746 of the samples underwent sequencing by MC3 and PCAWG. We found that that ∼80% of possible mutations in covered exomic regions matched using the two technologies. Using a statistical model we estimated that 15-30% of the unique mutations are attributable to noise caused by variant allele fraction and clonal heterogeneity. We also observed that ∼30% of the mutations uniquely identified by PCAWG could be traced to mutations made by a single caller by MC3 and are not reported in the publicly available MC3 data set. Due to the numerous modes of comparison, we built MAFit an online tool to facilitate engagement with these data. Finally, we highlight the advantages of using whole genome technologies in regions of high and low GC content and perform significantly mutated gene analysis, thus, increasing the targeted/captured exomic space by ∼50% to discover additional genes that could only be found using whole genome sequencing approach. xvii Preface In this document, I identify events of evolutionary pressures by applying innovative study design and improved methodology to address both identify driver genes and mutations in cancer. This work augments our current understanding of significantly mutated genes with respect to age and tissue of origin. Additionally, findings from this work will provide one of the last landscapes in TCGA exomic data, along with a lasting resource for future generations of genomicists. This body of work also reflects a hinge project connecting TCGA exomic with ICGC whole genome somatic mutation calls. As deadlines loom and interesting ideas get pushed to the sideline use the final chapters to I reflect on the opposite vantage point of driver gene discovery and explore negative selection. Protected/Essential genes and gene regions in somatic biology hold great potential as drug targets and intriguing components for modern translational medicine using molecular techniques. Outlined below are seven chapters that encompass my studies as a graduate student at Washington University in St. Louis. The magnitude of this work could not have been done alone, and there are many people who remain thankless that made these data sets widely available and accessible to the cancer community. Much of this work has paved the path to illuminate novel cancer discoveries: Chapter 1: Introduction An introduction to somatic theory of cancer. Chapter 2: Identification and implications of driver genes in adult and pediatric tumorigenesis A first attempt at PanCancer gene discovery of driver genes and their associated effects in adult and pediatric cancers.",,2018.0,10.7936/TVB4-AH61,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3d5958cc1b4090d2d703f555b9485894a4b71e04,https://www.semanticscholar.org/paper/3d5958cc1b4090d2d703f555b9485894a4b71e04,Citygram One: One Year Later,"Citygram is a multidisciplinary project that seeks to measure, stream, archive, analyze, and visualize spatiotemporal soundscapes. The infrastructure is built on a cyber-physical system that captures spatio-acoustic data via deployment of a flexible and scalable sensor network. This paper outlines recent project developments which includes updates on our sensor network comprised of crowd-sourced remote sensing, as well as inexpensive and high quality outdoor remote sensing solutions; development of a number of software tools for analysis, visualization, and development of machine learning; and an updated web-based exploration portal with real-time animation overlays for Google Maps. This paper also includes a summary of technologies and strategies that engage citizen scientist initiatives to measure New York City’s spatio-acoustic noise pollution in collaboration with the Center for Urban Science and Progress (CUSP).",ICMC,2014.0,10.5281/ZENODO.850590,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d853fbaee7d7a0e2e3155044ec9c3e9d8e85d0c4,https://www.semanticscholar.org/paper/d853fbaee7d7a0e2e3155044ec9c3e9d8e85d0c4,TPCx-HS v2: Transforming with Technology Changes,,TPCTC,2017.0,10.1007/978-3-319-72401-0_9,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7d1e7776c037a2e8783ec6198250f009ac1af616,https://www.semanticscholar.org/paper/7d1e7776c037a2e8783ec6198250f009ac1af616,Facilitative moderation for online participation in eRulemaking,"This paper describes the use of facilitative moderation strategies in an online rulemaking public participation system. Rulemaking is one of the U. S. government's most important policymaking methods. Although broad transparency and participation rights are part of its legal structure, significant barriers prevent effective engagement by many groups of interested citizens. Regulation Room, an experimental open-government partnership between academic researchers and government agencies, is a socio-technical participation system that uses multiple methods to lower potential barriers to broader participation. To encourage effective individual comments and productive group discussion in Regulation Room, we adapt strategies for facilitative human moderation originating from social science research in deliberative democracy and alternative dispute resolution [24, 1, 18, 14] for use in the demanding online participation setting of eRulemaking. We develop a moderation protocol, deploy it in ""live"" Department of Transportation (DOT) rulemakings, and provide an initial analysis of its use through a manual coding of all moderator interventions with respect to the protocol. We then investigate the feasibility of automating the moderation protocol: we employ annotated data from the coding project to train machine learning-based classifiers to identify places in the online discussion where human moderator intervention is required. Though the trained classifiers only marginally outperform the baseline, the improvement is statistically significant in spite of limited data and a very basic feature set, which is a promising result.",dg.o '12,2012.0,10.1145/2307729.2307757,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6cd905e2552e94a8994e52cda5dd1e45fd7f1a4c,https://www.semanticscholar.org/paper/6cd905e2552e94a8994e52cda5dd1e45fd7f1a4c,Let the machines out. towards hybrid social systems,"When Alan Turing proposed the imitation game as a method to investigate the question if machines can think, he described a social system. However, the various disciplines that have pursued this seminal enquiry rarely touch base with sociological concepts. Cybernetics developed into various interdisciplinary fields, yet it was mainly rooted in physiological models. In the meantime, the mainstream of AI focused on cognitive problem solving, predominately from a topdown approach. Traditional cognitive science rests on the concept of organisms as information processing systems so does Artificial Life, but from a biological simulation perspective. The recently revitalised branch of machine learning has been successful in deploying bottom-up models combined with large amounts of data. Large scale simulations of the brain are expected to deliver new knowledge about the human brain. ”Second-generation” cognitive science and developmental robotics are embodied and apply neural computation. One might be tempted to say that progress has been made on brains, bodies and on models of minds. I claim that there is something largely missing in this picture, which is the social aspect. There is Social AI, and it embraces a wide variety of topics and concerns from Stafford Beers cybernetic vision of society to simulations of interacting agents, complex systems theory, language, imitation and social learning, social network analysis and social bots, enactment, human-machine interaction, augmented and virtual environments, robot assisted therapy and behavioural game theory, to name a few. I also would like to include autonomous weapons, computer worms and viruses, in particular crypto-ransomware, into this context of social systems. From the other side, an interdisciplinary bridge is constructed under the label of digital sociology. The process of mutual approximation is accompanied by prolific discourses around machine ethics and emerging legal issues. A recently introduced topic of discussion is if robots should pay taxes. So AI observes sociology, and sociology observes AI yet they do not share a coherent theoretical program and fundamental ontological questions are still left to the philosophers. To propose an alternative route, I consider Niklas Luhmann’s theory of social systems as a suitable foundation for guiding the development of hybrid social systems. A hybrid social system is understood as a social assemblage in which minds and machines mingle: humans, machines, certain things, cyborgs. Some animals are welcome, too. To this end, I present a few selected features of Luhmann’s theory and briefly visit some of their theoretical foundations: distinctions, in particular the distinction between system and environment, autopoietic systems, radical constructivism, and (second order) cybernetics.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
71ab10046c1b9adff6a44598f81f0d040b72b4e3,https://www.semanticscholar.org/paper/71ab10046c1b9adff6a44598f81f0d040b72b4e3,Middleware strategies for clouds and grids in e‐Science,"This special issue focuses on middleware strategies for the use of Clouds and Grids in e-Science applications, and is based on selected papers from the Seventh International Workshop on Middleware for Grid, Clouds and e-Science (MGC 2009) and on the Third Latin American Grid workshop (LAGrid 2009). The authors were invited to provide extended versions of their original papers taking into account comments and suggestions raised during the peer review process and comments from the audience during the workshops. Relevant contributions have been provided by McEvoy et al. [1], Eyers et al. [2], Kim et al. [3], Pinheiro et al. [4], Gomes and Costa [5], Futrelle et al. [6], and Ferro et al. [7]. These contributions focus on: • performance and deployment evaluation of a parallel application on a private Cloud; • configuring large-scale storage using a middleware applying machine learning (ML); • power-aware provisioning of virtual machines for real-time Cloud services; • an adaptive fault tolerance mechanism for opportunistic environments with a mobile agent approach; • an approach to enhance the efficiency of opportunistic grids; • use of semantic content management to create knowledge spaces for e-Science; and • a proposal to apply inductive logic programming (ILP) to self-healing problem in grids.",Concurr. Comput. Pract. Exp.,2011.0,10.1002/cpe.1711,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f0df9e8c9cf7b18d6923c3e67b4fed5be17e00c8,https://www.semanticscholar.org/paper/f0df9e8c9cf7b18d6923c3e67b4fed5be17e00c8,A SciDB-based Framework for Efficient Satellite Data Storage and Query based on Dynamic Atmospheric Event Trajectory,"Current research in climate informatics focuses mainly on the development of novel (machine learning, data mining, or statistical) techniques to analyze climate data (e.g. model, in-situ, or satellite) or to make prediction based on these climate data. One important component missing from this analysis workflow is data management that allows efficient and flexible data retrieval, (ease of) reproducibility, and the (ease of) techniques reuse on user-defined data subsets or other data. In this paper, we describe our preliminary investigation on the utilization of the distributed array-based database management system, SciDB, to support data-driven climate science research. We focus on modeling and generating indices that allow effective execution of various spatiotemporal queries on satellite data. Moreover, we demonstrate fast and accurate data retrieval based on user-specified trajectories from the SciDB database containing tropical cyclone trajectories and the complete ten-year QuikSCAT ocean surface wind fields satellite data. Our preliminary work indicates the feasibility of the array-based technology for multiple satellite data storage, query, and analysis. Towards this end, a successful deployment of SciDB-based data storage can facilitate the use of data from multiple satellites for climate and weather research.",BigSpatial@SIGSPATIAL,2015.0,10.1145/2835185.2835190,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
11ec4a1719a330c3251bf56a43f14fb80a238bba,https://www.semanticscholar.org/paper/11ec4a1719a330c3251bf56a43f14fb80a238bba,Addressing Concerns about Responsibility,"Date of publication: 1 December 2014 set of technologies, loosely referred to as “artificial agents,” is becoming more pervasive and more powerful in the current computing landscape. All artificial agents are built on a computational foundation. Some are purely computational, e.g., Internet bots, search engines, and others are physically embodied entities with computational decision-making components, e.g., robots, unmanned aerial vehicles (UAVs), and autonomous cars. The noteworthy feature of artificial agents – the feature that leads to the artificial agent label – is their capacity to operate autonomously. At some level or to some degree, artificial agents operate independently from the humans who design and deploy them. They are agents in the sense that we deploy them to perform tasks on our behalf and often these tasks involve learning and decision-making. Since humans previously performed many of these tasks, we mark the difference, that is, the machine performance of these tasks, by referring to them as “artificial.” Responsibility issues are prominent in the discourse on artificial agents. Much attention has been given to the possibility that artificial agents might develop in ways that will make it impossible to hold humans responsible for their behavior. We believe that this concern misconstrues the situation and distracts attention from more important and more urgent issues. Rather than lamenting the possibility of artificial agents for which no one can be responsible, attention should be focused on how to develop artificial agents so as to ensure that humans can be responsible for their behavior. This involves attending to the optimal distribution of tasks among human and non-human (machine) components of artificial agent systems; appropriate designations of responsibilities to the humans operating in the system; and development and implementation of responsibility practices to support the assignments of tasks and responsibilities. As part of a National Science Foundation funded project (“Ethics for Developing Technologies: An Analysis of Artificial Agent Technology”), we developed a set of recommendations for the future development of artificial agents. An early version of the recommendations was presented to a small group of experts in the fields of robotics, computer science, philosophy, ethics, law, and policy. Based on the feedback received from these experts, the recommendations were revised. We present the final Recommendations for Future Development of Artificial Agents MEREL NOORMAN DEBORAH G. JOHNSON",,2014.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8f2a47cd8a04aa7985d36c1106b01f35290be30f,https://www.semanticscholar.org/paper/8f2a47cd8a04aa7985d36c1106b01f35290be30f,Development of 80- and 100- Mile Work Day Cycles Representative of Commercial Pickup and Delivery Operation,"When developing and designing new technology for integrated vehicle systems deployment, standard cycles have long existed for chassis dynamometer testing and tuning of the powertrain. However, to this day with recent developments and advancements in plug-in hybrid and battery electric vehicle technology, no true “work day” cycles exist with which to tune and measure energy storage control and thermal management systems. To address these issues and in support of development of a range-extended pickup and delivery Class 6 commercial vehicle, researchers at the National Renewable Energy Laboratory in collaboration with Cummins analyzed 78,000 days of operational data captured from more than 260 vehicles operating across the United States to characterize the typical daily performance requirements associated with Class 6 commercial pickup and delivery operation. In total, over 2.5 million miles of realworld vehicle operation were condensed into a pair of duty cycles, an 80-mile cycle and a 100-mile cycle representative of the daily operation of U.S. class 3-6 commercial pickup and delivery trucks. Using novel machine learning clustering methods combined with mileage-based weighting, these composite representative cycles correspond to 90th and 95th percentiles for daily vehicle miles traveled by the vehicles observed. In addition to including vehicle speed vs time drive cycles, in an effort to better represent the environmental factors encountered by pickup and delivery vehicles operating across the United States, a nationally representative grade profile and key status information were also appended to the speed vs. time profiles to produce a “work day” cycle that captures the effects of vehicle dynamics, geography, and driver behavior which can be used for future design, development, and validation of technology. Introduction Under DOE-FOA-0001349 FY15 Award for Mediumand Heavy-Duty Vehicle Powertrain Electrification, Cummins and PACCAR jointly proposed the development of a range-extending plug-in hybrid electric Class 6 pickup and delivery truck. The goal of this project is to demonstrate an electrified vehicle that would deliver a minimum of 50% reduction in fuel consumption across a range of representative drive cycles. In addition to achieving the 50% fuel reduction target, the vehicle also needs to demonstrate as good or better drivability and performance while still meeting emissions requirements when compared to existing conventionally fueled baseline vehicles. Most existing duty cycles used to test conventional internal combustion powered vehicles are of a limited time duration. For example, the Hybrid Truck Utility Forum Class 6 Pickup and Delivery cycle is slightly more than one hour. When testing a system using only fuel as its energy source, this is acceptable; a onehour duty cycle can be used to represent the vehicle operation for the entire work day (e.g., fuel consumption in the middle of the day is very similar to fuel consumption at the end of the day). However, with plug-in electric vehicles, the system (battery characteristics and thermal management systems) may operate differently throughout the work day (especially near the end of the day). For example, the available battery energy may be completely spent prior to the completion of the route. A short duty cycle cannot simply be extrapolated. Evaluating the vehicle over the entire work day also provides the ability to interject appropriate stops that are typical of the Class 6-7 pickup and delivery application. These stops can range from several minutes to much longer and can have significant thermal effect on the vehicle and powertrain systems. These stops may also have a large impact on overall duty cycle mileage (and other duty cycle characteristics such as average speed) as the stops may account for roughly half of the work day. As part of the research and development team, the National Renewable Energy Laboratory (NREL) was been NREL/CP-5400-70943. Posted with permission. Presented at WCX 18: SAE World Congress Experience, 10-12 April 2018, Detroit, Michigan.",,2018.0,10.4271/2018-01-1192,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ec4ce9470af03e607d88b5480c32b972516c45ea,https://www.semanticscholar.org/paper/ec4ce9470af03e607d88b5480c32b972516c45ea,"Woodside Energy Ltd: pioneer in cognitive computing, artificial intelligence and robotics","Cognitive computing is a new disruptive technology with the potential to reshape the oil and gas industry across the entire value chain. For Woodside Energy Ltd (Woodside), embracing this technology is an opportunity to save time, drive efficiency and reduce costs. In 2015, Woodside collaborated with IBM and deployed a cognitive computing system (IBM’s Watson) into its business. The system focuses on capturing the vast proprietary database of knowledge on Woodside’s major capital projects. Today, the Watson proof-of-concept has been successfully deployed by the science function into the business and is now under the care of the projects function. Moreover, it is undergoing continuous advances through further machine learning, additional ingestion of documentation and features linking the cognitive computer system with existing subject matter experts. Given the success of the first pilot program, Woodside is continuing to rapidly leverage cognitive technologies in other areas of the business. In mid-2016, Woodside deployed Watson for drilling events using IBM’s Watson Explorer – Advanced Addition. This program identifies and classifies a wide variety of geological drilling events allowing Woodside’s geoscience team to provide more timely and accurate assessment of potential risks for well design. Woodside continues to develop several other business solutions using these platforms in areas as diverse as continuous improvement, business management, maintenance campaigns, legal advice, general management and robotics. This presentation shares Woodside’s lessons and insights derived from its journey across multiple forms of cognitive technology and provides insights as to the state-of-the-art and adaptation of these systems to achieve specific goals.",,2017.0,10.1071/AJ16142,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
558019e61eb4d446a0571396892c4e7962bd7d26,https://www.semanticscholar.org/paper/558019e61eb4d446a0571396892c4e7962bd7d26,Making Roads Safer by Making Drivers Better,"The world's roads see over 50 million injuries and 1.25 million fatalities every year; road accidents are the leading cause of death among people between the ages of 15 to 30. This talk will describe how mobile sensing (especially using smartphones), signal processing, machine learning, and behavioral science can improve road safety by making people better drivers. I'll discuss several challenges in achieving this goal, as well as learnings from successful deployments in multiple countries. Interesting problems include inferring vehicular dynamics from noisy sensor data; accurate drive detection; detecting and discouraging distracted driving; designing good incentives for safe-driving; and the design of new sensing platforms to augment smartphone sensors.",MobiCom,2017.0,10.1145/3117811.3117847,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a7a408272788bf406919d4c93f8c39b5ee411bad,https://www.semanticscholar.org/paper/a7a408272788bf406919d4c93f8c39b5ee411bad,Templet Web : The Experimental Use of Volunteer Computing Approach in Scientific Platform-asa-Service Implementation,"This article presents the Templet Web cloud service [a1]. The service is designed for high-performance scientific computing automation. The use of high-performance technology is specifically required by new fields of computational science such as data mining, artificial intelligence, machine learning, and others. Cloud technologies provide a significant cost reduction for high-performance scientific applications. The main objectives to achieve this cost reduction in the Templet Web service design are: (1) the implementation of “on-demand” access; (2) source code deployment management; (3) high-performance computing programs development automation. The distinctive feature of the service is the approach mainly used in the field of volunteer computing, when a person who has access to the computer system delegates his access rights to the requesting user. We developed an access procedure, algorithms, and software for utilization of free computational resources of the academic cluster system in line with the methods of volunteer computing. The Templet Web service has been in operation for five years. It has been successfully used for conducting laboratory workshops and solving research problems, some of which are considered in this article. The article also provides an overview of research directions related to the service development.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f186f9f1400211ef326be5b8d68fe2ccd4f257e4,https://www.semanticscholar.org/paper/f186f9f1400211ef326be5b8d68fe2ccd4f257e4,"Hope, Hype, and Fear: The Promise and Potential Pitfalls of the Big Data Era in Criminal Justice","Over the past decade, algorithmic decision systems (ADSs) — applications of statistical or computational techniques designed to assist human-decision making processes — have moved from an obscure domain of statistics and computer science into the mainstream. Advocates of these “intelligence-led” or “evidence-based” policy approaches assume big data tools will allow government agencies to use objective data to overcome historical inequalities to better serve underrepresented groups. 
However, the assumption of objective data is flawed. All human behavior or social phenomenon that machine learning algorithms attempt to predict come from a data-generation process (DGP) which is comprised of trillions of complex interactions between the roughly seven billion people that inhabit our planet. If a statistical model — an abstraction of the DGP — assumes incorrectly about the underlying dynamics, the predictions and conclusions generated will be inaccurate and biased. 
In this paper, I highlight examples from a variety of fields to discuss why this is particularly true for algorithmic decision systems in criminal justice, then examine a specific application of predictive policing in Oakland, California, and then conclude with what police departments should consider in the deployment of policing technologies in the era of artificial intelligence and big data.",,2017.0,10.2139/SSRN.3145308,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6e1ce805a8ced4c2234960cda6e938a1d39b9a0d,https://www.semanticscholar.org/paper/6e1ce805a8ced4c2234960cda6e938a1d39b9a0d,VIM: A Big Data Analytics Tool for Data Visualization and Knowledge Mining,"With the advancement of Information technologies and applications, a copious amount of data is generated, which attracts both the research community to utilize this information for extracting knowledge and the industry for developing the knowledge-based system. Visualization of data, pattern mining from datasets and analyzing data drift for the different features are three highly used applications of machine learning and data science fields. A generic web-based tool integrated with such features will provide prodigious support for preprocessing the dataset and thus extracting accurate information. In this work, we propose such a data visualization tool, named VIM, which is a web-based comprehensive tool for generic data visualization, data preprocessing and mining suitable knowledge with drift analysis of data. Given a dataset, it can envisage the distribution of data with convenient statistical diagrams for different selected features. Moreover, users can employ VIM to generate association rules by selecting multiple features. We have developed VIM using Python Django framework and GraphLab library. We have deployed this tool to make this publicly usable, which can be accessed at http://210.4.73.237:9999/",2017 IEEE International WIE Conference on Electrical and Computer Engineering (WIECON-ECE),2017.0,10.1109/WIECON-ECE.2017.8468939,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6e76b0d359fd8a6a373ce96fd5458deec5252d9a,https://www.semanticscholar.org/paper/6e76b0d359fd8a6a373ce96fd5458deec5252d9a,An Engineering Toolbox to Build Situation Aware Ambient Assisted Living Systems,"Due to increasing anticipated average life and health expenditure ambient assisted living (AAL) systems attract the attention of researchers. To successfully build and deploy AAL systems knowledge from different fields of computer science is needed: pervasive computing to gain the raw data, machine learning and pattern recognition to interpret these data and HCI knowledge to allow implicit interaction with the system.In this paper we propose a reference architecture for building AAL systems. Based on this reference architecture we introduce a toolbox that simplifies the development of AAL systems. The toolbox consists of a meta-model for pipeline systems, a low-level context model, high-level context ontologies, customizable components and tool support.","2008 Third International Conference on Broadband Communications, Information Technology & Biomedical Applications",2008.0,10.1109/BROADCOM.2008.36,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1e211c9568a2472d5a905fab27c51579fd9e8ad0,https://www.semanticscholar.org/paper/1e211c9568a2472d5a905fab27c51579fd9e8ad0,Discovering human descriptions for ubiquitous visual identification,"Identifying suspects in surveillance footage is paramount in ensuring public safety, preventing crime, policing and forensic investigation. At present, finding an individual in real-world CCTV footage given only an eye-witness description is near impossible. The vast majority of contemporary research assumes coarse, expertly-defined categories to describe subjects, ineffective in dealing with unconstrained, low quality and obscured images. Such brittle representations hamper semantic image discrimination and the ability to learn robust predictors from challenging subject matter. This thesis explores human and machine centric techniques for representing and learning semantic human descriptions for suspect identification. By investigating the duality of human-machine communication, we enhance the capabilities of traditional attributes and soft biometric descriptors, expanding their versatility and applicability towards challenging images and large-scale surveillance datasets. We experiment with crowdsourcing human annotations using ordered and similarity comparisons, and estimating attributes from images employing a variety of state-of-the-art machine learning techniques. Our focus is on utilising a lean lexicon of global and body characteristics that are most pertinent when estimated from stand-alone surveillance footage. Significant improvements in suspect retrieval and identification performance are achieved by discovering enhanced soft biometric descriptions which represent visual trait characteristics with more precision and relevance. This work evolves the areas of soft biometrics and identity science, drawing ideas from contemporary image attribute recognition, semantic attribute discovery, pedestrian reidentification and perceptual psychology. Our findings indicate that increasing not only the volume, but the complexity of information conveyed between humans and machines is key in deploying soft biometrics ubiquitously.",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
847f02d2daca7bde8ea88a167dc70a36c1430db5,https://www.semanticscholar.org/paper/847f02d2daca7bde8ea88a167dc70a36c1430db5,The technical hashtag in Twitter data: A hadoop experience,"The continuously growing wealth of data has radically changed the data science landscape. At the same time, Big Data tools have known important progress in terms of optimising performance and scalability. However, applying them into practical deployment settings is still a challenging task that is highly dependent on the particularities of the data. In this paper, we present our experiences with implementing a Big Data analytics pipeline with the purpose of extracting value from Twitter data. We acquire and process nearly 60 million tweets that capture the recent outbreaks of the Ebola and Zika viruses. Our processing pipeline first extracts useful information from tweets and then applies a topic modelling technique, provided by Mahout, a Hadoop-based machine learning library. We further extend our Twitter analysis with the study of temporal evolution of daily sentiment toward an important topic, as expressed through the social platform. We highlight at each level, the technical challenges originating from the specific nature of Twitter data and the lessons drawn from our work.",2016 IEEE International Conference on Big Data (Big Data),2016.0,10.1109/BigData.2016.7841015,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2cda350042dc348cdea5aaab1f0dd5354a849156,https://www.semanticscholar.org/paper/2cda350042dc348cdea5aaab1f0dd5354a849156,A Prediction Model of Privacy Control for Online Social Networking Users,,DESRIST,2018.0,10.1007/978-3-319-91800-6_20,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e34e86ac08b9b7845aaca27c1cb486e38c676949,https://www.semanticscholar.org/paper/e34e86ac08b9b7845aaca27c1cb486e38c676949,Real-time Inferential Analytics Based on Online Databases of Trends: A Breakthrough Within the Discipline of Digital Epidemiology of Dentistry and Oral-Maxillofacial Surgery,"Background: Epidemiological sciences have been evolving at an exponential rate paralleled only by the comparable growth within the discipline of data science. Digital epidemiological studies are playing a vital role in medical science analytics for the past few decades. To date, there are no published attempts at deploying the use of real-time analytics in connection with the disciplines of Dentistry or Medicine. Aims and Objectives: We deployed a real-time statistical analysis in connection with topics in Dental Anatomy and Dental Pathology represented by the maxillary sinus, posterior maxillary teeth, related oral pathology. The purpose is to infer the digital epidemiology based on a continuous stream of raw data retrieved from Google Trends database. Materials and Methods: Statistical analysis was carried out via Microsoft Excel 2016 and SPSS version 24. Google Trends database was used to retrieve data for digital epidemiology. Real-time analysis and the statistical inference were based on encoding a programming script using Python high-level programming language. A systematic review of the literature was carried out via PubMed-NCBI, the Cochrane Library, and Elsevier databases. Results: The comprehensive review of the literature, based on specific keywords search, yielded 491813 published studies. These were distributed as 488884 (PubMed-NCBI), 1611 (the Cochrane Library), and 1318 (Elsevier). However, there was no single study attempting real-time analytics. Nevertheless, we succeeded in achieving an automated real-time stream of data accompanied by a statistical inference based on data extrapolated from Google Trends. Conclusion: Real-time analytics are of considerable impact when implemented in biological and life sciences as they will tremendously reduce the required resources for research. Predictive analytics, based on artificial neural networks and machine learning algorithms, can be the next step to be deployed in continuation of the real-time systems to prognosticate changes in the temporal trends and the digital epidemiology of phenomena of interest.",Modern Applied Science,2019.0,10.5539/MAS.V13N2P81,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
066894fb19cbee73495b0eded1f33fa3c61c1d96,https://www.semanticscholar.org/paper/066894fb19cbee73495b0eded1f33fa3c61c1d96,Mobile sensing and social computing,"With the rapid development of social networks and social environments, mobile sensing has increasingly emerged as one of the most important technologies to develop social computing solutions. Social computing is a general term for an area of computer science that is concerned with the intersection of social behavior and computational systems, providing a programmable combination of contributions from both humans and computers. A key factor for social computing is how social information is collected from the ubiquitous environments and can be widely used to provide social services in mobile environments. Mobile sensing is increasingly becoming part of everyday life, as smartphones are becoming the central personal computational device in people’s lives. Mobile sensing presents several challenges related to wireless sensor networks, machine learning, human–computer interaction, and mobile systems. Sensor-equipped mobile phones can be combined with wireless sensor networks installed in the environment to develop social machines in many sectors of our economy, including business, healthcare, social networks, environmental monitoring, and transportation. Some research efforts on social computing and mobile sensing have been in progress, including mobile sensing algorithms, applications and systems, and methods and techniques to develop virtual societies. This IJDSN Special Issue is an opportunity to bring multi-disciplinary experts, academics, and practitioners together to exchange their experience in the development and deployment of mobile sensing and social computing systems. This Special Issue brings together researchers and developers from industry and academy to report on the latest scientific and technical advances on the application of mobile sensing and social computing and to showcase the latest systems using these technologies. Filipe et al. compile and compare technologies and protocols published in the most recent researches, seeking Wireless Body Area Network (WBAN) issues for medical monitoring purposes to select the most useful solutions for this area of networking. The most important features under consideration in our analysis include wireless communication protocols, frequency bands, data bandwidth, transmission distance, encryption, authentication methods, power consumption, and mobility. WBAN supporting healthcare applications are in early development stage, but offer valuable contributions at monitoring, diagnostic, or therapeutic levels. They cover real-time medical information gathering obtained from different sensors with secure data communication and low power consumption. Filipe et al. demonstrate that some characteristics of surveyed protocols are very useful to medical appliances and patients in a WBAN domain. Marcelino et al. present a solution to overcome barriers between elderlies and their information and communication technology (ICT) usage in order to potentiate all the benefits provided from mobile sensing and social computing. They present a survey on guidelines, standards, and advices regarding usability and accessibility issues when developing solutions for elderly people made having in mind that senior population have singular requirements due to age-related changes and also frequently technological illiteracy. The authors have identified and applied the most important guidelines to their own solution. A prototype was made using responsive design in order to be adaptable to any type of devices. Zong and Wen propose a new approach to calculate the smartphone orientation by detecting the vehicle starting action and then establish the coordinate system relationship between vehicle and smartphone. Furthermore, they trained the classified model offline to match the acceleration characteristics with traveling speed. In the model training process we compared different classification algorithms. Due to enclosed areas and intensive energy consumption, GPS or WiFi sometime are invalid. In this paper, Zong and Wen propose a new approach to estimate the traveling speed after analyzing the acceleration characteristics in time domain and frequency domain. Shuyun et al. propose a method used to calculate the link importance degree index, and the index is used to evaluate the link’s information. Besides, a multiobjective optimization model is proposed, its aim is to minimize the total cruise time under detecting as many important links as possible and minimize the information value undetected by unmanned aerial vehicles (UAVs), and the fuzzy operator is introduced to the constraint conditions. Finally, a case study is used to",Int. J. Distributed Sens. Networks,2016.0,10.1177/1550147716665512,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5f6fcf18593be853e2d891907c8c7f96b51aa401,https://www.semanticscholar.org/paper/5f6fcf18593be853e2d891907c8c7f96b51aa401,Algorithm Optimization Using Features In SVD & Classification In Eigenspace,"Singular Value Decomposition (SVD) is ubiquitous in a range of applications including computer science, economics, engineering, geology, oceanography, psychology, social networking etc. It is an unsupervised modeling technique that creates latent vectors for a subspace that reduces the dimensionality of observed data from n to k (k<<n) dimensions. Latent variables are uncorrelated variation of attribute values that are correlated in the original space. Moreover, SVD can be used to detect/remove noise/outliers, cluster similar entities and make predictions. On the other hand, classification tree is a supervised technique that accomplishes the similar tasks. It models decision trees from training data in order to make intelligent predictions. There is a close connection between SVD and decision trees, but differ in purpose, algorithm design and error analysis techniques. We present a hybrid algorithm bridges the gap between these standalone algorithms and adaptively supersedes their outcomes. For experimental analysis, we use realworld benchmark data, wines, publicly available from UCI machine learning repository. The algorithm is implemented in Matlab, supported by decision trees in Weka software, on MacOS Seirra Version 10.12.3 8GB 160MHZ.",POLIBITS,2017.0,10.17562/PB-56-1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ce3bacabb34cace073de7e9c9875b822339c87f3,https://www.semanticscholar.org/paper/ce3bacabb34cace073de7e9c9875b822339c87f3,Affective Intelligence: The Human Face of AI,,Artificial Intelligence: An International Perspective,2009.0,10.1007/978-3-642-03226-4_4,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5909cd09c89eb363dde9426bdbb1f93e5ff81000,https://www.semanticscholar.org/paper/5909cd09c89eb363dde9426bdbb1f93e5ff81000,Learner Modeling for Integration Skills in Programming,"Mastery development requires not only acquiring component skills, but also practicing their integration into more complex skills. When learning programming, an example is to first learn += and loops, then learn how to combine them into a loop that sums a sequence of numbers. The existence of integration skills has been supported by cognitive science research, yet it has rarely been considered in learner modeling, the key component for adaptive assistance in an intelligent tutoring system (ITS). Without this, early assertions of mastery in ITSs after only basic component skill practice or practice in limited contexts may be merely indicating shallow learning. 
 
My dissertation introduces integration skills, widely acknowledged by cognitive science research, into learner modeling. To demonstrate this, I chose program comprehension with a complex integrative nature. To provide grounds for skill modeling, I applied a Difficulty Factors Assessment (DFA) approach (from cognitive science) and identified integration skills along with generalizable integration difficulty factors in common basic programming patterns. I used the DFA data to inform the construction of the learner model, CKM-HI, which incorporates integration skills in a hierarchical structure in a Bayesian network (BN). Compared with other machine learning approaches, BN naturally utilizes domain knowledge and maintains interpretable knowledge states for adaptation decisions. To address the limitation of prediction metrics to evaluate such multi-skill learner models, I proposed and applied a multifaceted evaluation framework. Data-driven evaluations on a real-world dataset show that CKM-HI is superior to two popular multi-skill learner models, CKM and WKT, regarding predictive performance, parameter plausibility, and expected instructional effectiveness. To evaluate its real-world impact, I built a program comprehension ITS driven by learner modeling and a classroom study deploying this system suggests that CKM-HI could lead to better learning than the CKM model. 
 
My dissertation work is the first to systematically demonstrate the value of integration skill modeling, and offers novel integration-level learner modeling and multifaceted evaluation approaches applicable to a broader context. Further, my work contributes recent ITS infrastructure and techniques to programming education, and also contributes an example of taking an interdisciplinary approach to ITS research.",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
135bb1b1fb1f8f8db2077052079a46e7234bd3a2,https://www.semanticscholar.org/paper/135bb1b1fb1f8f8db2077052079a46e7234bd3a2,Leveraging Legal Stringency on Artificial Intelligence Applications - A 'Copyright Law on Artificial Intelligence' Debate,"Safe Artificial Intelligence (AI) has been a debate since the induction of general intelligence in the Machine Intelligence and Research Institute. The very recent report by the White House of Office of Science and Technology addresses this issue with a great emphasis on the philosophy of developing safe AI. 
Fundamentally, such issues, for a general audience, demands a more stereotypical definition of ownership of such software. Recursively, it also requires a more banal definition of AI itself, devoid of the jargons of Mathematics and Computer Science. In general, a set of codes capable of learning on its own an environment in which it has been deployed is termed as artificial intelligence. Concurrently, such capabilities make these pieces of software to be equated to the human mind, subsequently rendering it the capacity to demonstrate what is scientifically called General Intelligence. It is not difficult to realize the potential of such software and hence its capability to hurt the society when deployed in an uncontrolled fashion in an open environment. 
The Machine Intelligence and Research Institute set up to develop guidelines to build futuristic AI has hence worked on building the ethics of developing safe code in this regard. But despite the regulations, understanding and benchmarking the ownership criteria of this software is a legal decision on its own and as would be more understandable in the following section, is not a trivial issue to judge. Ownership of software that is as smart or close to the human mind gives the owner enormous power. Giving the human being ownership of a product that is an output of an AI seems unreasonable, while at the same time somewhat logical since the machine in itself does not possess a physical brain. Consequently, such issues have been raised in the past and have continued to the present, and as AI has grown as an intelligent entity, so has the tendency to understand it as a physical being rather than lines of codes. The proposal debates on such topics and argues on the ownership rights to products that have been developed by AI.",,2017.0,10.2139/SSRN.2982626,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
70c4c8f4ad8c0bb89f3d774d0e7a12cfb87f4010,https://www.semanticscholar.org/paper/70c4c8f4ad8c0bb89f3d774d0e7a12cfb87f4010,Modeling Semantic Web Services with OPM/S A Human and Machine-Interpretable Language,"The World-Wide-Web is now a ubiquitous, global tool, used for finding information, communicating ideas, carrying out distributed computation, and conducting business, learning and science. Web services and the Semantic Web are emerging as a powerful infrastructure for distributed computing. However, even though standard methods that define semantics of Web services, such as OWL-S, may aid in the development and deployment of these services, they are hardly designed to be easily understandable and usable by developers. Complexity and lack of accessibility of Web services and the Semantic Web hinders their usage by the information industry. OPM/S, which is based on ObjectProcess Methodology (OPM), offers a bi-modal visuallingual representation that is both intuitive for humans and formal for machines. Utilization of ontologies and interoperability are two issues addressed by the OPM/S modeling environment. Ontologies are expressed as meta-libraries, which are specified in OPM or OWL, and can be dynamically linked to semantic Web services in a distributed environment. Interoperability is achieved using a transparent reuse method that enables dynamic development of Web services and their integration into more complex Web services. Using a running example, the paper presents OPM/S and its mapping to OWL-S. The benefits and shortcomings are discusses and compared with other OWL-S modeling methods.",WebDyn@WWW,2004.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2ba48a39ffff2a1d45f7ac6614a3f5137e028d60,https://www.semanticscholar.org/paper/2ba48a39ffff2a1d45f7ac6614a3f5137e028d60,IEEE Access Special Session Editorial: Big Data Services and Computational Intelligence for Industrial Systems,"The pervasive nature of big data technologies as witnessed in industry services and everyday life has given rise to an emergent, data-focused economy stemming from many aspects of industrial applications. The richness and vastness of these services are creating unprecedented research opportunities in a number of industrial fields including public health, urban studies, economics, finance, social science, and geography. We are moving towards the era of Big Data Services , which are deployed in a multi-scale complex distributed architecture. These services can be formed a high-level computational intelligence based on emerging analytical techniques such as big data analytics and web analytics. In this context, computational intelligence employs software tools from advanced analytics disciplines such as data mining, predictive analytics, and machine learning. At the same time, it becomes increasingly important to anticipate technical and practical challenges and to identify best practices learned through experience. This special session has included nine papers, and a brief summary about each paper is presented as follows.",IEEE Access,2015.0,10.1109/ACCESS.2016.2516178,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d07e91bc1c6be69f4e142174b06afe7387cc42c9,https://www.semanticscholar.org/paper/d07e91bc1c6be69f4e142174b06afe7387cc42c9,Analytics and AI for Industry- Specific Applications Minitrack,"The purpose of this minitrack is to explore case studies of applications of data analytics and artificial intelligence based smart services and digital solutions across industries. We will discuss reports that improve our understanding of how analytics and AI are currently used across industries influencing digital transformation of economies. We are interested in getting answers to the question “Where can AI be applied in an industry specific manner (a task with open access data and code) to benchmark and to improve industry standard performance, and grow more opportunities for value creation?” We are also interested in open tech AI applications for manufacturing, agriculture, healthcare as well as for other industry-specific applications. We will emphasize research on the design, analysis, implementation, adoption, and evaluation of real-life cases that provide opportunities to design, develop, and deploy these capabilities as micro-services that solve customer needs, especially those with startup potential. Opening presentation “Business Analytics for Sales Pipeline Management in the Software Industry: A Machine Learning Perspective” proposes a model designed to help sales representatives in the software industry manage the complex sales pipeline. By integrating business analytics in the form of machine learning into lead and opportunity management, datadriven qualification support reduces the high degree of arbitrariness caused by professional expertise and experiences. Through the case study of a software provider, authors developed three models to map the end-to-end sales pipeline process using real business data from the company’s CRM system. The results show a superiority of the CATBoost and Random Forest algorithm over other supervised classifiers such as Support Vector Machine and XGBoost. The study also reveals that the probability of either winning or losing a sales deal in the early lead stage is more difficult to predict than analyzing the lead and opportunity phases separately. In the paper “FraudMemory: Explainable Memory-Enhanced Sequential Neural Networks for Financial Fraud Detection” authors propose a novel fraud detection algorithm. It adopts state-of-art feature representation methods to better depict users and multimodal logs in financial systems. The proposed method uses a sequential model to capture the sequential patterns of each transaction and leverage memory networks to improve both the performance and interpretability. Also, with the incorporation of memory components, new algorithm called “FraudMemory” possesses high adaptability to the existence of concept drift. The paper “Deep Learning for Improved Agricultural Risk Management” authors investigate potential of deep learning in predicting agricultural yield in time and space under weather/climate uncertainty. They evaluate the predictive power of deep learning, benchmarking its performance against more conventional approaches. The findings reveal that deep learning offers the highest predictive accuracy, outperforming all the other approaches. Authors infer that it also has great potential to reduce underwriting inefficiencies and insurance coverage costs associated with using more imprecise yield-based metrics of real risk exposure. In the last paper “Holistic System-Analytics as an Alternative to Isolated Sensor Technology: A Condition Monitoring Use Case” authors propose a system-oriented concept of how to monitor individual components of a complex technical system without including additional sensor technology. By using already existing sensors from the environment combined with machine learning techniques, authors can infer the condition of a system component, without actually observing it. In consequence condition monitoring or additional services based on the component's behavior can be developed without overcoming the challenges of sensor implementation. We hope you will enjoy the papers and their presentation at the conference and we thank the authors for submitting excellent results of their work to make this minitrack successful. Proceedings of the 52nd Hawaii International Conference on System Sciences | 2019",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
64c04084854122fd149b03c28d7c8bc1c70e53c3,https://www.semanticscholar.org/paper/64c04084854122fd149b03c28d7c8bc1c70e53c3,Artificial Intelligence in Customs Risk Management for e-Commerce: Design of a Web-crawling Architecture for the Dutch Customs Administration,"The last decade saw the rise of e-commerce trade and the shift of the manufacturing industry to the emerging economies, China first of all. In this context, the European Customs Authorities experienced an explosion of small parcels coming from e-commerce websites, often from China, and faced difficulties to detect fiscal frauds and security threats using their conventional risk management systems. To address this problem, the European project PROFILE brings together the customs administrations of Netherlands, Belgium, Sweden, Norway, and Estonia, aiming to provide the EU with a shared platform for: (1) accurately assessing customs risks; (2) optimizing operation and logistics by integrating multiple sources of information; (3) developing a shared data platform to share customs risk management (CRM) practices. As part of this project, the Dutch Customs Administration (DCA) and International Business Machines (IBM) Corporation are collaborating to deploy the cutting-edge technologies of artificial intelligence to automatically cross-check the customs declarations coming from Chinese e- commerce against online information. Through a Design Science approach, I carried out this research for the Delft University of Technology, written in collaboration with IBM Netherlands, aiming to deliver a preparatory study for the developing team before the PROFILE project begins. This includes knowledge brokering between the Dutch Customs Administration and IBM Netherlands so that a more precise problem scope can be defined, and the requirements elicited. In particular, this research focuses on the first part of the project: the development of an adaptive web-crawler for e-commerce, able to compare the declarations documents against online information. According to the Dutch Customs Administration, the web-crawling system should gather the description of the goods from declarations, search the product on the web, find its price of sale on the e-commerce platforms, compare it with the value declared in the declaration, and return a risk indicator of green/red flag to the targeting officer. The design process of this system follows approaches coming from the systems engineering discipline, starting with the requirement analysis, addressing them with the state-of-the-art big data analytics, and finally deriving the logical components of the system, whose design is presented through a logical architecture. First, the application domain is investigated. When goods entry the Netherlands need an entry declaration. These goods arrive at the harbor of Rotterdam or airport of Schiphol, where some of these are imported into the country and become import/export, and others stop temporarily as transit waiting to be shipped somewhere else. The Dutch Customs Administration monitors these processes through risk management systems aiming to stop non-compliant goods. This research describes these practices, with a higher focus on the e-commerce risk targeting. About the e- commerce world, a study of the e-commerce processes behind an online purchase is also carried out through a real purchase on Chinese e-commerce. This was used to observe how the Chinese sender described the item, and how the Dutch Customs assessed the risk and decided on the duties to be paid. This led to reflect on the possible frauds scenarios and how to address them. Finally, the Dutch Customs also reported that the products descriptions are often vague and ambiguous, and a more accurate formulation of the problem is described. Secondly, an in-depth literature on the fields of web-crawling and big data analytics techniques is carried out. The possible technologies that could be useful to address the requirements and the problem formulation are investigated. Starting with an analysis of the existing literature on the field of big data analytics, this research also covers the recent trends of machine learning and artificial intelligence. To avoid reporting a too big literature, the topics reported have been accurately chosen, for instance describing only the techniques for web analytics and text analytics. This literature on big data analytics is further broken in two sub-topics, one more theoretical, which classifies the types of analytics methods and defines the technology of machine learning and natural language processing, including the last paradigms of deep learning and reinforcement learning, and one more practical, where guidelines for the design, development, and implementation of machine learning techniques are proposed. It is here that a theoretical framework to systematically reflect on the challenges of the field of big data analytics has been identified. This framework is then used to systematically collect the main technological challenges of the use case under analysis and translate them into non-functional requirements. Finally, the last part of the literature describes what a web-crawler is and what web- crawling/web-craping means. This later extends to the concepts of focused web-crawling and smart, intelligent, adaptive web-crawling, where machine learning techniques are deployed to improve performance. The literature concludes by providing related works of machine learning techniques implemented in smart web-crawling of the e-commerce websites and stating the knowledge gap that needs to be bridged to address the use case under analysis. After the application domain and the literature review, the knowledge from these previous phases combines in a continuous iterative process according to the design science methodology (Hevner, 2014). Through unstructured interviews with the DCA and IBM experts, the requirements elicitation is carried out. The approach by Armstrong and Sage (2000) deriving from the field of systems engineering is used. The main objective of the system to be developed is broken down into a series of sub-activities that must be carefully structured to formulate the requirements. About the non-functional requirements, instead of reflecting on the different domains – technological, environment, law compliance, etc. – as it is proposed by the same systems engineering approach mentioned earlier, this research uses the framework identified in the literature review about the main challenges of big data project (Sivarajah, 2016). To derive the components of the architecture from the requirements and customer needs, the methodology proposed by Suh (1998) called Axiomatic Design has been used, mapping the requirements into architectural components in a rigorous manner. In this way, the design domains proposed by this methodology – customer, functional, physical and process domains – are taken as the reference point for the design process: first, the business needs are identified, then these are translated into requirements, which are mapped into design features. The process domain is left out of this research and will be addressed by the IBM development team in Ireland. The design cycle leads to the design of a web-crawling system represented through a service- oriented architecture (SOA). Its block diagram and black-box description of each application service are provided. Furthermore, the architecture functionality is described with an architecture walk-through and a sequence diagram in the unified modeling language (UML). The result is an innovative real-time web-crawling system to identify the value of a given product on the e-commerce websites. It deploys natural language process models to filter the non-relevant search results, and other machine learning models to best matching the remaining relevant results with a given item description. The design and architecture description of this innovative web-crawling system is the main artifact of this research, while the mixed methodology of systems engineering methodologies and big data frameworks is another important scientific contribution.",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f58fb2a0c35eb3715bc906f5ea4559fdcd186461,https://www.semanticscholar.org/paper/f58fb2a0c35eb3715bc906f5ea4559fdcd186461,Learning Engineering by Product Dissection,"new multi-disciplinary course in Product Dissection has been developed, distributed electronically, and implemented at Penn State, the University of Washington and the University of Puerto RicoMayaguez. The course examines the way in which products and machines work: their physical operation, the manner in which they are constructed, and the design and societal considerations that determine the difference between success and failure in the marketplace. The primary objectives of this course are to develop a basic aptitude for engineering and engineering design, and to develop mental visualization skills; by examination of the design and manufacture of consumer and industrial products. This course is intended to complement engineering science and mathematics courses and to show freshman or sophomore level students how these fundamentals relate to engineering practice. The course is modular and consists of self-standing dissection modules on: bicycle, electric drill, four stroke engine, Funsaver disposable camera, and telephone. This paper describes the philosophy and content of this course and presents results from two years of development and deployment. Acknowledgement: This project was funded by TRP Project #3018, NSF Award #DMI-9413880.",,1996.0,10.18260/1-2--6162,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c00e9a112bdc23742a557bb812005c34b87526b7,https://www.semanticscholar.org/paper/c00e9a112bdc23742a557bb812005c34b87526b7,New Features [Editor's Column],"In being committed to bringing excellence in ITS to our readership, we decided to extend the current content of the Magazine. Thus, this edition includes a new section on “Artificial Intelligence Technologies for ITS” which is aimed at attracting short (up to 4-pages) manuscript submissions on either recently developed or recently implemented AI & Machine Learning methods, especially big data analysis, cloud computing, crowdsourcing, deep learning, virtual reality, augmented reality, virtual sensing, augmented sensing, artificial cognition, knowledge automation, IoT, etc., in transportation systems & services. I have therefore asked our distinguished colleague, Fei-Yue Wang of the Chinese Academy of Sciences and the Chinese National University of Defense Technology, who is also the Immediate Past EiC of the Society’s flagship periodical, Transactions on ITS, to share his enthusiasm and keen research interest into the AI field, in the context of ITS, and encourage you all to consider submitting your contributions to this new, peer-reviewed, section of the Magazine. An additional novelty introduced in this edition is the outcome of our dedication to provide the ITS community with an opportunity to easier access and share research data. Thus, this edition brings a new column on “PhD & MPhil Theses’ Abstracts”. I would like to invite you all to keep submitting needed information by contacting the Column Editors, Fernando García Fernández of Universidad Carlos III de Madrid, Spain, and Zhixiong Li of the University of Central Florida, U.S.A. I am sure that we will all benefit from having this information at hand from now on. A number of aspects of relevance to ITS deployment are addressed by papers selected for this edition of ITS Magazine including interconnected vehicle scenarios, co-operative vehicle-to-pedestrian situations and bus-priority measures. Among these topics is the aspect of two-way interactions between human drivers and their partially automated vehicles—the topic which was also selected for profiling the cover page of this edition of the Magazine. It prompted me to invite you to consider submitting a special issue proposal on partially automated and fully automated vehicles as further research into these interactions will bring a step change, not only to transportation system services in years to come but, to our contemporary driving habits as well. Nobody knows the direction this change will take. I greatly wish for it to be a positive change for the betterment of humanity. At this transition stage the automated vehicle developers have full freedom in envisaging a level of automation they wish to assign to their vehicles and, moreover, freedom in envisaging a way of deploying automated driving functions and, thus, shaping interactions between partially automated vehicles and their human drivers. I therefore wish to see our community take this transition stage as an opportunity towards shaping an ITS technology–enabled Future of the Transportation before it is New Features editorial board",IEEE Intell. Transp. Syst. Mag.,2017.0,10.1109/MITS.2017.2746405,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cce600d54ef32b3f6312b9166da097e0f9eded8a,https://www.semanticscholar.org/paper/cce600d54ef32b3f6312b9166da097e0f9eded8a,Designing a Visual Analytics System for Industry-Scale Deep Neural Network Models,"The complexity of industry-scale deep learning models and datasets pose unique design, visualization, and system challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have designed, developed, and deployed ACTIVIS, a visual analytics system for interpreting industry-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both instanceand subset-level. ACTIVIS has been deployed on Facebook’s machine learning platform. This article is a summary for the VAST’17 paper (TVCG track) ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models [2]. 1 DESIGNING FOR INDUSTRY-SCALE MODELS Despite the increasing interest in developing visualization tools for deep learning interpretation [5–7], the complexity of large-scale models and datasets used in industry pose unique design challenges that are inadequately addressed by existing work. For example, while most existing visualization tools target image datasets, deep learning tasks in industry often involve different types of data, including text and numerical data. Furthermore, in designing tools for realworld deployment, it is a high priority that the tools be flexible and scalable, adapting to the wide variety of models and datasets used. These observations motivate us to design and develop ACTIVIS [2], a visual analytics system for deep neural network models, now deployed on Facebook’s machine learning platform. Since the ACTIVIS project started in April 2016, we have conducted participatory design sessions with over 15 Facebook engineers, researchers, and data scientists across multiple teams to learn about their visual analytics needs. We identified six key design challenges — for data, model, and analytics — that have not been adequately addressed by existing deep learning visualization tools. The challenges include the need to support: (1) diverse input data sources, (2) high data volume, (3) complex model architecture, (4) a great variety of models, (5) diverse subset definitions for analytics, and (6) both instanceand subset-level analyses. These challenges shape the main design goals of ACTIVIS. 2 ACTIVIS CONTRIBUTIONS ACTIVIS’s main contributions include: • A novel visual representation that unifies instanceand subsetlevel inspections of neuron activation, facilitating comparison of activation patterns for multiple instances. *e-mail: kahng@gatech.edu †e-mail: mortimer@fb.com ‡e-mail: adityakalro@fb.com §e-mail: polo@gatech.edu • An interface that tightly integrates an overview of graph-structured complex models and local inspection of neuron activations, allowing users to explore the model at different levels of abstraction. • A deployed system scaling to large datasets and models. • Case studies with Facebook engineers and data scientists that highlight how ACTIVIS helps them with their work. ACTIVIS’s multiple coordinated views help users get a high-level overview of the model from which the user can drill down to perform localized inspection of activations. ACTIVIS visualizes how neurons are activated by user-specified instances or instance subsets, to help users understand how a model derives its predictions. The subsets can be flexibly defined using data attributes, features, or output results, enabling model inspection from multiple angles. While many existing deep learning visualization tools support instancelevel exploration [6, 7], ACTIVIS is the first tool that simultaneously supports instanceand subset-level exploration. Both exploration strategies are common and effective, and they offer complementary analytics benefits. Instance-based analysis instructs how individual instances contribute to a model’s accuracy, but it is tedious to inspect many instances one by one. Subset-based analysis leverages input features or instance subsets to help reveal relationships between data attributes and machine learning algorithms’ outputs [3]. It is especially beneficial when dealing with huge datasets in industry, which may consist of millions or billions of data points. By exploring instance subsets and enabling their comparison with individual instances, users can learn how them models respond to many different slices of the data. We refer our readers to the longer version of our ACTIVIS [2] VAST’17 paper published in IEEE Transactions on Visualization and Computer Graphics.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8246bcab39de6760227a02e306a0823d86bcb1ad,https://www.semanticscholar.org/paper/8246bcab39de6760227a02e306a0823d86bcb1ad,Introduction to the Creation and Appropriation of Knowledge Systems Minitrack,"The objective of this minitrack is to contribute to the body of knowledge that helps scholars and practitioners increase their collective understanding of (1) how knowledge systems are planned, designed, constructed, implemented, used, evaluated, supported, upgraded, and evolved; (2) how knowledge systems impact the context in which they are embedded; and (3) the human behaviors reflected within and induced through both (1) and (2). By knowledge system, we mean a system in which human participants and/or machines perform work (processes and activities) related to the creation, retention, transfer and/or application of knowledge using information, technology, and other resources to produce informational products and/or services for internal or external customers. Such systems may include, but are not limited to, knowledge management systems, decision systems, social media, expert systems, machine learning systems, and other AI systems as well as any other IT-enabled knowledge management processes. It is the 6th year of the minitrack. We received eleven papers this year and after a rigorous review process, we accepted five papers for publication in the proceedings and presentation at the conference. The first paper, co-authored by Nico Wunderlich and Roman Beck, looks at how the concept of a digital business strategy leads to increased organizational innovativeness and firm performance. The research results reveal that IT knowledge of business employees has a higher positive impact on organizational innovativeness in organizations giving the digital business strategy high importance, whereas the top management team IT knowledge plays a greater role when digital business strategy is given low priority. The second paper, co-authored by Hans-Georg Fill and Felix Haerer, looks at how blockchain technologies can be applied to the domain of knowledge management. To validate the technical feasibility of the approach a first technical implementation is described and applied to a fictitious use case. The third paper, co-authored by Dimitris Karagiannis and Robert Andrei Buchmann, proposes a deployment approach for hybrid knowledge bases using agile diagrammatic means. The proposal is based on the RDF-semantics variant of OWL and leads to a particular type of hybrid knowledge bases hosted by the GraphDB system. The approach aims for complementarity and integration, providing means of creating semantic networks that are amenable to ontology-based reasoning. The fourth paper, co-authored by Kevin Lumbard, Ammar Abid, Christine Toh, and Matt Germonprez, focuses on understanding the types of information utilized in industry with the aim of enabling the development and sharing of methodologies that support the design of competitive products. As such, this paper proposes a framework of information archetypes utilized by designers in industry. The research results reveal two archetypes of information utilized by decision-makers within these companies during the development of new products and services. The fifth paper, co-authored by Olivia Hornung and Stefan Smolnik, presents a systematic review of existing evidence on the role of emotions in KM research. The review shows that despite KM’s long tradition, there is only limited evidence as to how emotions are related to KM, most of which mention emotions as motivation for KM. As a study’s result, the authors identify four research opportunities to further examine certain aspects of the role of emotions in KM. We wish to thank all of the authors who submitted work for consideration in this minitrack. We also thank the dedicated reviewers for the time and effort they invested in reviewing the papers. We believe that the accepted papers contribute to furthering our understanding on the creation and appropriation of knowledge systems. We look forward to discuss these further during our sessions in January 2018. Proceedings of the 51 Hawaii International Conference on System Sciences | 2018",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c133bc791556b6945cc45cb191f8c5afe1025821,https://www.semanticscholar.org/paper/c133bc791556b6945cc45cb191f8c5afe1025821,Year : 2018 Of Cyborg Developers and Big Brother Programming,"The main reason modern machine learning mechanisms outperform hand-crafted solutions is the availability of high-quality data in large quantities. We observe that although many day-to-day activities in software engineering (such as bug triaging, reverting regressions, or even implementing code for properly scoped problems) could possibly be automated, we lack the necessary monitoring tools to capture all relevant information. Bug trackers and version control rely mostly on plain text, and specifications are informal or at best semi-structured. After setting the stage via a short excursion to the year 2047, we discuss how a ubiquitous AI, which can learn from every interaction a human developer has with a machine, could take over more and more of the mundane responsabilities in software engineering. We outline how this change will affect software engineering practice as well as education. Posted at the Zurich Open Repository and Archive, University of Zurich ZORA URL: https://doi.org/10.5167/uzh-141364 Originally published at: Alexandru, Carol V; Gall, Harald (2018). Of Cyborg Developers and Big Brother Programming AI. In: The Hawaii International Conference on System Sciences, Hawaii, USA, 3 January 2018 6 January 2018. Of Cyborg Developers and Big Brother Programming AI Carol V. Alexandru Software Evolution and Architecture Lab University of Zurich, Switzerland alexandru@ifi.uzh.ch Harald C. Gall Software Evolution and Architecture Lab University of Zurich, Switzerland gall@ifi.uzh.ch 1. Wednesday, 13. November 2047 – A day in the life of a software analyst Meet Ana, AI data integrator for INGSOFT. The time: 07:32 am. Ana’s personal assistant, Lucio, has been monitoring her heart rate and breathing throughout the night using just his microphone and notices that now is the ideal time to wake up: outside of REM sleep and after roughly 8 hours of sleep – a duration which has been determined from the experience of countless previous nights and consisting of pre-sleep activity, detected mood in the morning, performance throughout the following day and over a dozen other biometric and behavioral indicators. The assistant sounds a gentle alarm at the appropriate volume. After a brief shower, Ana reads her edition of PG (the Personal Gazette). Given the work cycles of her teammates, Lucio suggests to work from home this morning. INGSOFT’s main product is ‘Metadapt’, an AI that dynamically adapts running AI agents for new situations. For the past few weeks, she’s been immersed in deploying Metadapt on the cities traffic system. The 2048 Summer Olympics are coming up, and the AI routing self-driving cars and their vintage counterparts is projected to react poorly to the unexpected change in traffic volume and travel patterns. Ana sits down at her workstation and is assigned the first task for today. ‘Gone are the days of issue trackers and SCRUM meetings’ she thinks, briefly reminiscing her days as an intern at one of the derelict ‘coding zoos’ as they’ve come to call the old-fashioned, cubicle-compartmentalized office buildings of the past. The biocam, installed in her workstation and unobtrusively monitoring Ana’s heart rate, skin temperature, eye movements and pupil dilation, notices her mind wandering and turns on the coffee machine in her kitchen. Ana’s been tasked with talking to the people at Publitrans, who run the cities personal public transport system. They have some vital information regarding anticipated changes in cab traffic during the Olympics. Ana frequently gets these assignments, as she has excellent communication skills and also knows how to speak to people such that Metadapt can follow the discussion. As the smell of her favourite brew permeates the apartment and Ana moves to the Kitchen, Lucio takes the opportunity to tell her that the meeting will take place at 9am. INGSOFT has recently subscribed to WaveVR, that new business communication tool consisting of a few tiny laser projectors and a set of comfy gloves. 3D images of her two contacts at Publitrans, captured using their biocams, is projected into Ana’s eyes as she moves around her office. The gloves help to facilitate personal contact by inducing haptic feedback where necessary and help the biocam to accurately track movements such that participants can easily manipulate other objects projected into the virtual 3D scene. VR has been around for over 30 years, and the experience can still be flaky at times, but it beats wasting hours on daily commutes, plus it saves a lot of energy – with the biosphere going to hell and whatnot. Ana’s job as a data integrator consists of figuring out which pieces of additional information can help Metadapt make the right choices in governing AI, and also implementing the interfaces between different data sources. Sure, there are ongoing efforts to facilitate the automatic, need-driven data exchange between the countless AI systems governing every aspect of the computerized world – like they’ve implemented between Canada, the US and Mexico, but a global solution is still impeded by slow-moving political and social structures. Plus a fairly unique event such the Olympics still requires significant human intervention. The meeting starts, and as one participant struggles with the automatic volume adjustment of his mic (‘how have they still not solved this problem!’, Ana thinks), the other two start discussing the topic at hand. Metadapt is listening in on the conversation to capture those details, which it recognizes as containing new information. This allows Ana to later formalize the conversation and guide Metadapt in synthesizing the necessary interfaces. As the meeting ends, Lucio preemptively checks if any of Ana’s friends happen to be in the area for lunch today. He prepares a few suggestions for the time and location for lunch, based on the daily offers publicized by nearby eateries, how busy they are, as well as Ana’s current dietary needs and preferences. Ana finalizes the formal interface specification connecting Metadapt to Publitrans’ data endpoints by applying necessary corrective measures and as she gets ready to leave, Metadapt is already scanning Publitrans’ historical data relevant to improving traffic flow in the coming year.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7cea771ffead60c81ee41b5f0713dc369f480dde,https://www.semanticscholar.org/paper/7cea771ffead60c81ee41b5f0713dc369f480dde,SAIDE: Scaling Analytics for Image-based Data from Experiments,"Research across science domains is increasingly reliant on image-based data from experiments. The challenge is to analyze the data torrent generated by advanced instruments in a timely manner and provide insights such as measurements for decision-making. Software tools are in high demand for scientists to uncover relevant, but hidden, information in digital images, such as those coming from new materials. A group of computational and material scientists have embraced the multi-disciplinary work of designing software applications, coordinating research efforts connecting (1) emerging algorithms for dealing with complex and large datasets; (2) data analysis methods with basis on pattern recognition and machine learning; and (3) advances in evolving computer architectures. These new trends will accelerate the analyses of image-based recordings, scaling scientific procedures by reducing time between experiments, increasing efficiency, and opening more opportunities for more users of the imaging facilities. This paper aims to provide an overview of our algorithms and deployed software tools, showing results across image scales and how each tool component plays a role in improving image understanding within DOE national laboratories.",,2016.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fb346259efc2d8e9db4af98c084a058df9a784c4,https://www.semanticscholar.org/paper/fb346259efc2d8e9db4af98c084a058df9a784c4,Technical Perspective: If I could only design one circuit …,"a processor architecture for a neural net accelerator and puts a particularly strong focus on efficiently supporting the memory access patterns of neu-ral net computations. This includes minimizing both on-chip and off-chip memory transfers. Other members of the DianNao family include DaDian-Nao, ShiDianNao, and PuDianNao. DaDianNao (big computer) focuses on the challenges of efficiently computing neural nets with one billion or more model parameters. ShiDianNao (vision computer) is further specialized to reduce memory access requirements of Convolutional Neural Nets, a neural net family that is used for computer vision problems. While the number of problems solved by neural nets grows every week, some might wonder: Is this a fundamental change in the field, or will the pendulum swing back to favor a broader range of machine learning approaches? With the PuDianNao (general computer) architecture, the architects hedge their bets on this question by providing an accelerator for more traditional machine learning algorithms. Despite, or perhaps because of, Di-anNao's two Best Paper Awards, some readers may think that building a neu-ral network accelerator is just an academic enterprise. These doubts should be allayed by Google's announcement of the Tensor Processing Unit, a novel neural network accelerator deployed in their datacenters. These processors were recently used to help AlphaGo win at Go. It may be quite some time before we learn of TPU's architecture, but details on the DianNao family are only a page away.E L S have captured our imagination from the very beginning of computer science; however, victories of this approach were modest until 2012 when AlexNet, a "" deep "" neural net of eight layers, achieved a dramatic improvement on the image classification problem. One key to AlexNet's success was its use of the increased computational power offered by graphics processing units (GPUs), and it's natural to ask: Just how far can we push the efficient computing of neural nets? Computing capability has advanced with Moore's Law over these last three decades, but integrated circuit design costs have grown nearly as fast. Thus, any discussion of novel circuit archi-tectures must be met with a sobering discussion of design costs. That said, a neural net accelerator has two big things going for it. First, it is a special-purpose accelerator. Since the end of single-thread performance scaling due to power density issues, integrated circuit architects have searched for clever ways to exploit the increasing transistor counts afforded by Moore's Law without …",,2016.0,10.1145/2996862,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
33000cfad01184a29c7ff6fab57299c6f4d4fd9f,https://www.semanticscholar.org/paper/33000cfad01184a29c7ff6fab57299c6f4d4fd9f,Intelligent drying systems,"The era of Artificial Intelligence (AI) has become a reality. Computer technologies have reached a level where they are not only for information processing, but also as substitute for human being on routine tasks, which can be described as a set of programable operations. In such cases, computer is clearly superior to humans, because it is not affected by emotions and human errors amongst its multifarious advantages. Moreover, computers can do some tasks beyond the scope of human intelligence, e.g., goal-oriented control and optimization. Drying with its inherent transient nature, uncertainty and still poorly understood coupling of transport processes and materials science is an ideal playground for AI. High variability in thermo-physical properties of materials and relatively low energy efficiency of drying make this game even more exciting. Currently, each drying application requires much preliminary research due to unknown process-product interactions, very specific to each product and drying technology. We believe that future development of drying technologies will focus on the need in embedding AI into drying systems. So far, the progress is still very limited. Most published research deals with “black-box” computing, such as artificial neural network (ANN), fuzzy logic (FL) or evolutionary algorithms (EA). These black-box approaches clearly demonstrate ability to manage hidden uncertainty and non-linearity of process/product parameters, but they do not contribute to the improvement of our knowledge about drying process and optimal control strategies. Recent advances in computer vision and machine learning have opened up new avenues for future development of drying systems. Unfortunately, their applications in drying technologies are still very limited. One possible reason is that they still have not demonstrated their unique advantages in simplifying research and development of new knowledge. We also should realize that this shift in paradigm would require critical revision of the philosophy of drying R&D. Drying community needs to become aware of advances in the AI space and applications to realworld systems. So far, control strategies in drying are limited to the control of process parameters. Our knowledge of product quality transformation and critical control points in the process of drying are still not included in dryer control strategy. AI in the form of computer vision and soft sensors will fill this gap, delivering real-time information about the product quality attributes. It is similar to intelligence of experienced cook, who controls the process of food preparation by observing visible changes in food. Unfortunately, our drying technologies are still an art of blind processing. Hence, the concept of intelligent observer is the first step in the development of intelligent drying systems. Sharing of digital data through IoT could provide the basis for the development of unified methodology. The next goal is the development of digital twins of commonly used drying systems. Machine learning could be used as an excellent tool to validate drying models. Eventually it will make possible model predictive control (MPC) and multiobjective optimization (MOO). The first step in this direction is already made. We believe that in the coming decades AI will find important industrial applications in various drying systems, allowing cost-effective production of consistently high quality products. This will require both academics and industry to keep up with the rapidly developing field of AI and technology-push innovations in industrial drying. References below provide a valuable introduction and exposure to the status and future of AI in drying technologies. We",Drying Technology,2020.0,10.1080/07373937.2019.1650452,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e2a330edc8693516e330e479e9e485c4de5010e5,https://www.semanticscholar.org/paper/e2a330edc8693516e330e479e9e485c4de5010e5,La révolution de l'assurance par la donnée : défis scientifiques de l'extraction à la gestion de connaissances,"Marcin Detyniecki is senior R&D officer at the Data Innovation Lab recently created by AXA. Additionally, he is professor at the Polish Academy of Science (IBS PAN) and associate researcher at the computer science laboratory LIP6 of the University Pierre and Marie Curie (UPMC). Currently, his research focuses on the emerging challenge popularly named big data. In the past, he has worked on the usage of new media, with challenges ranging from multimedia information retrieval to image understanding. Several of the developed applications have not only been deployed in the market, but they have also been singled out in international competitions such as TrecVid, ImageClef, MediaEval. This applicative success is the results of a dialogue with more theoretical works on topics such as new challenges in approximate reasoning, information aggregation and fusion, and machine learning from a computational intelligence perspective. Marcin Detyniecki studied mathematics, physics and computer science at the University Pierre and Marie Curie (UPMC) in Paris. In 2000 he obtained his Ph.D. in Artificial Intelligence from the same university. Between 2001 and 2014, he was a research scientist of the French National Center for Scientific Research (CNRS). He has has been researcher at the University of California at Berkeley and at Carnegie Mellon University (CMU). He has been visiting researcher at the University of Florence and at British Telecom Research labs. Today he is member of the research and academic council of UPMC University, member of the executive board of laboratory SMART, elected member of the LIP6 laboratory council, and member of the editorial board of the International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems (IJFUKS). He also funded and animated until 2014 the UPMC – Sorbonne Universités Computer Science Colloquium. Dr. Detyniecki has over 90 publications in journals and conference proceedings, including 6 keynotes.",EGC,2016.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2ce6dc6f3489defb39d9f2f139ec48bd36db5678,https://www.semanticscholar.org/paper/2ce6dc6f3489defb39d9f2f139ec48bd36db5678,An Autonomic Methodology for Embedding Self-tuning Competence in Online Traffic Control Systems,,Autonomic Road Transport Support Systems,2016.0,10.1007/978-3-319-25808-9_13,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ceefe076d1e96b065f60075d086d2c7bf99b8319,https://www.semanticscholar.org/paper/ceefe076d1e96b065f60075d086d2c7bf99b8319,"Taking a biologically inspired approach to the design of autonomous , adaptive machines","I l l u s t r a t I o n b y J u s t I n M e t z The auTomaTed design, construction, and deployment of autonomous and adaptive machines is an open problem. Industrial robots are an example of autonomous yet nonadaptive machines: they execute the same sequence of actions repeatedly. Conversely, unmanned drones are an example of adaptive yet non-autonomous machines: they exhibit the adaptive capabilities of their remote human operators. To date, the only force known to be capable of producing fully autonomous as well as adaptive machines is biological evolution. In the field of evolutionary robotics, one class of population-based metaheuristics—evolutionary algorithms—are used to optimize some or all aspects of an autonomous robot. The use of metaheuristics sets this subfield of robotics apart from the mainstream of robotics research, in which machine learning algorithms are used to optimize the control policy of a robot. As in other branches of computer science the use of a metaheuristic algorithm has a cost and a benefit. The cost is that it is not possible to guarantee if (or when) an optimal control policy will be found for a given robot. The benefit is few assumptions must be made",,2013.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
59da04a47e98fbbd5fcf43d573392d54abd5ed05,https://www.semanticscholar.org/paper/59da04a47e98fbbd5fcf43d573392d54abd5ed05,InfoNames : An Information-Based Naming Scheme for Multimedia Content,"RESEARCH INTERESTS Data management and its intersection with machine learning (an area popularly known as advanced analytics or data science), especially devising data managementinspired abstractions, systems, frameworks, and algorithms to make the end-to-end process of building and using machine learning algorithms for data analytics easier (improving the productivity of data scientists and developers) and faster (improving runtime performance and introducing accuracy trade-offs). My work spans the whole gamut of building systems, algorithm design, theoretical analysis, empirical analysis, and working with practitioners to deploy my research.",,2010.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1a6218b781324f0ad49d82916a0d4b8e2ab1b3e1,https://www.semanticscholar.org/paper/1a6218b781324f0ad49d82916a0d4b8e2ab1b3e1,A Survey on Artificial Intelligence,"Artificial intelligence (AI) is a field of computer science that explores computational models of problem solving, where the problems to be solved are of the complexity of problems solved by human beings. Artificial Intelligence is the study of how to make computers do things which, at the moment, people do better. It is the intelligence of machines and the branch of computer science that aims to create it. The study and design of intelligent agents is also called as Artificial Intelligence. The central problems of AI include such traits as reasoning, knowledge, planning, learning, communication, perception and the ability to move and manipulate objects. This paper intends to study the techniques developed in artificial intelligence (AI) from the standpoint of their applications in all fields related to engineering. In particular, it focuses on techniques developed (or that are being developed) in artificial intelligence that can be deployed in solving problems associated with distinct processes. This paper highlights a comparative study between approaches and its applications. Keywords— Computerscience ;perception; reasoning; manipukated objects;",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7a5c26fc189700b1974a216a6ced89089b713b0d,https://www.semanticscholar.org/paper/7a5c26fc189700b1974a216a6ced89089b713b0d,Transforming Science and Engineering Classrooms with Online Collaborative Tools,"Researchers from the University of Massachusetts Lowell and a non­profit partner, Machine Science Inc., are investigating classroom implementations of a web platform that aids teachers in engaging their students in collaborative inquiry. This technology—the Internet System for Networked Sensor Experimentation (iSENSE)—allows for the contribution of individual student­collected science data to a shared, online repository. iSENSE enables educators and students to initiate their own experiments, contribute data, and then view the data using multiple web­based visualization tools. This paper provides an overview of the iSENSE technology and its use in middle and high school physics, chemistry, and engineering classrooms. The authors worked closely with teachers to integrate data­collection hardware and the iSENSE web software into into their existing curricular designs. Teacher interviews and classroom observations were used to study their adoption. Our work indicates that there are multiple, viable paths for integrating iSENSE into classroom science and engineering instruction, and that the technology gives students new, transformative ways of sharing data with one another and learning from that experience. Perspective/Theoretical Framework The iSENSE team employs a design­based research paradigm, which Wang and Hannafin (2005) define as "" a systematic but flexible methodology aimed to improve educational practices through iterative analysis, design, development, and implementation, based on collaboration among researchers and practitioners in real­world settings, and leading to contextually­sensitive design principles and theories "" (p. 6). The design­based approach is well­suited for the iSENSE research because it reflects the practical limitations and advantages of evaluating an educational intervention in real­world classroom settings. Because of its iterative and flexible nature, this type of research benefits from a mixed­methods approach. Together with teacher and student learning and participation patterns, our research methodology has evolved over the past several years. As the technology and our classroom enactments evolved, we looked for changes in learning as evidenced by data from assessment instruments administered directly to students. Based on this research, the iSENSE investigators have developed detailed case studies of selected classrooms and teachers to generate a textured portrait of individual classrooms, illustrating what aspects of iSENSE are working, and why.",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d03c6612a86ee2ece69a8448827b8eff644be540,https://www.semanticscholar.org/paper/d03c6612a86ee2ece69a8448827b8eff644be540,Artificial Intelligence in Oncology: Potential and Pitfalls,"volumes of data, which confer it the ability to “learn” how to perform the assigned task, under human supervision. In contrast, DL algorithms can be largely automatic once set in motion, learning intricate patterns from even raw data with minimal human intervention, and improving continuously.3 The difference between the two is highlighted in ►Fig. 2. The potential and pitfalls of applying AI in oncology is exemplified by IBM’s Watson for Oncology (WFO) program, operated from Memorial Sloan Kettering Cancer Center (MSKCC) in New York and deployed in more than 230 hospitals across the world.4 Initially, envisioned as a cloud based super-computer embedded with artificial intelligence, AI could shift through massive volume of data (from doctor’s notes to medical studies and clinical guidelines) to generate insights and identify, provocatively, new approaches to cancer care. However, till date, the only evidence, citing its capabilities, has focused on concordance analyses and demonstrated that it is competent (at best) in applying existing standard of care, not that it can improve them.5,6 Further shortcomings pertain to the training dataset being composed of synthetic cases rather than real world patients, resulting in treatment recommendations based on the preferences of Artificial intelligence (AI) has occupied the consciousness of successive generations of computer scientists, science fiction fans, and medical researchers alike, since the inception of the term in 1956.1 The concept of AI as envisioned in popular culture is that of intelligent machines that can interpret the world as humans do, understand language, and learn from real world examples. While this specific vision of machines being able to replicate human thoughts, emotions, and reasons, remains for now in the realm of science fiction, narrower applications of AI that can perform specific tasks as good as humans are poised to transform medicine at the basic, clinical, management, and financial levels. Terminology surrounding these technologies continues to evolve and can often be a source of confusion for noncomputer scientists. ►Fig. 1 provides a broad overview. Data science is the broad field within which the subfields of AI, machine learning (ML), and deep learning (DL) reside, with each successive layer adding more complexity than the former. ML employs algorithms which can learn complex relationships or patterns from empirical data and make accurate decisions without coding specific instructions to accomplish the assigned task.2 The algorithm is “trained” using large",Asian Journal of Oncology,2019.0,10.1055/S-0039-1693233,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d3a340e3d61ea3d80309909839479b757fe8a37c,https://www.semanticscholar.org/paper/d3a340e3d61ea3d80309909839479b757fe8a37c,DPWeka: Achieving Differential Privacy in WEKA A thesis submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science by,"Organizations belonging to the government, commercial, and non-profit industries collect and store large amounts of sensitive data, which include medical, financial, and personal information. They use data mining methods to formulate business strategies that yield high longterm and short-term financial benefits. While analyzing such data, the private information of the individuals present in the data must be protected for moral and legal reasons. Current practices such as redacting sensitive attributes, releasing only the aggregate values, and query auditing do not provide sufficient protection against an adversary armed with auxiliary information. In the presence of additional background information, the privacy protection framework, differential privacy, provides mathematical guarantees against adversarial attacks. Existing platforms for differential privacy employ specific mechanisms for limited applications of data mining. Additionally, widely used data mining tools do not contain differentially private data mining algorithms. As a result, for analyzing sensitive data, the cognizance of differentially private methods is currently limited outside the research community. This thesis examines various mechanisms to realize differential privacy in practice and investigates methods to integrate them with a popular machine learning toolkit, WEKA. We present DPWeka, a package that provides differential privacy capabilities to WEKA, for practical data mining. DPWeka includes a suite of differential privacy preserving algorithms which support a variety of data mining tasks including attribute selection and regression analysis. It has provisions for users to control privacy and model parameters, such as privacy mechanism, privacy budget, and other algorithm specific variables. We evaluate private algorithms on realworld datasets, such as genetic data and census data, to demonstrate the practical applicability of DPWeka.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ec788f197ca6377cccf922d2bb0bfda90409ff3e,https://www.semanticscholar.org/paper/ec788f197ca6377cccf922d2bb0bfda90409ff3e,The Semantic Web. Latest Advances and New Domains,,Lecture Notes in Computer Science,2015.0,10.1007/978-3-319-18818-8,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
12fbe946fbc6c3273335d8680c0fd3a9e78e5775,https://www.semanticscholar.org/paper/12fbe946fbc6c3273335d8680c0fd3a9e78e5775,Cloud computing and virtual machines in LIS education: options and resources,"Purpose 
 
 
 
 
The purpose of this paper is to consider how and why virtual machines (VMs) and cloud computing and related development environments built on cloud-based resources may be used to support and enhance the technological elements of library and information science (LIS) education. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
It is based on analysis of available technologies and relevant applications. 
 
 
 
 
Findings 
 
 
 
 
Cloud computing and virtualization offer a basis for creating a robust computing infrastructure for LIS education. 
 
 
 
 
Practical implications 
 
 
 
 
In the context of LIS education, cloud computing is relevant in two respects. First, many important library and archival services already rely heavily on cloud-based infrastructures, and in the near future, cloud computing is likely to define a much larger part of the computing environment on which libraries and archives rely. Second, cloud computing affords a highly flexible and efficient environment that is ideal for learning about VMs, operating systems and a wide variety of applications. What is more important, it constitutes an environment for teaching and learning that is vastly superior to the ones that currently support most LIS degree programs. From a pedagogical perspective, the key aspect of teaching and learning in the cloud environment is the VM. So, the article focuses a significant portion of its attentions on questions related to the deployment and use of VMs and Linux Containers, within and without cloud-based infrastructures, as means of learning about computer systems, applications and networking and achieving an understanding of essential aspects of both cloud computing and VM environments. 
 
 
 
 
Originality/value 
 
 
 
 
Based on a search of available literature in computer science and library and information science, the paper has no counterparts.",Digit. Libr. Perspect.,2017.0,10.1108/DLP-02-2016-0008,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f09f49207333101368ce1ded5888b2f583b11868,https://www.semanticscholar.org/paper/f09f49207333101368ce1ded5888b2f583b11868,Vision : A comparison between Convolutional Neural Networks and Hierarchical Temporal Memories on object recognition tasks Candidate : dott,"School of Science Master Degree in Computer Science Deep Learning for Computer Vision: A comparison between Convolutional Neural Networks and Hierarchical Temporal Memories on object recognition tasks dott. Vincenzo Lomonaco In recent years, Deep Learning techniques have shown to perform well on a large variety of problems both in Computer Vision and Natural Language Processing, reaching and often surpassing the state of the art on many tasks [1] [2] [3]. The rise of deep learning is also revolutionizing the entire field of Machine Learning and Pattern Recognition pushing forward the concepts of automatic feature extraction and unsupervised learning in general [4]. However, despite the strong success both in science and business, deep learning has its own limitations. It is often questioned if such techniques are only some kind of bruteforce statistical approaches and if they can only work in the context of High Performance Computing with tons of data [5] [6]. Another important question is whether they are really biologically inspired, as claimed in certain cases, and if they can scale well in terms of “intelligence”. The dissertation is focused on trying to answer these key questions in the context of Computer Vision and, in particular, Object Recognition, a task that has been heavily revolutionized by recent advances in the field. Practically speaking, these answers are based on an exhaustive comparison between two, very different, deep learning techniques on the aforementioned task: Convolutional Neural Network (CNN) [7] and Hierarchical Temporal memory (HTM) [8]. They stand for two different approaches and points of view within the big hat of deep learning and are the best choices to understand and point out strengths and weaknesses of each of them. CNN is considered one of the most classic and powerful supervised methods used today in machine learning and pattern recognition, especially in object recognition. CNNs are well received and accepted by the scientific community and are already deployed in large corporation like Google and Facebook for solving face recognition [9] and image auto-tagging problems [10]. HTM, on the other hand, is known as a new emerging paradigm and a new meanlyunsupervised method, that is more biologically inspired. It tries to gain more insights from the computational neuroscience community in order to incorporate concepts like time, context and attention during the learning process which are typical of the human brain. In the end, the thesis is supposed to prove that in certain cases, with a lower quantity of data, HTM can outperform CNN [11].",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5e4a451faf2e47486a5dbeca8a5109b53e22d95a,https://www.semanticscholar.org/paper/5e4a451faf2e47486a5dbeca8a5109b53e22d95a,Research Statement Arun Kumar,"Large-scale data analytics using machine learning (ML), popularly known as advanced analytics or “Big Data” analytics, is transforming almost every data-powered application in the enterprise, Web, science, government, and other domains. However, there are still many barriers to broad and successful adoption of advanced analytics. Designing new ML algorithms and faster ML implementations are important issues that have been studied by researchers for a long time, but for most data-powered applications, the real showstopper is a different issue that is often glossed over in research: the end-to-end process of building ML models given raw data is often too painful even for professional analysts, while developers skilled in both general-purpose programming and the latest ML are rare. The goal of my research is to improve the productivity of the users and developers of advanced analytics systems to enable data-powered applications to realize the full potential of advanced analytics. To this end, my work focuses on fundamental research questions at the intersection of data management and ML that address usability, developability, performance, and scalability issues. My approach to solving a problem involves the whole spectrum of algorithm design, theoretical analysis, empirical analysis, building prototype systems, and deploying them in practice.",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8a5b6be15e969a0aa05febcfd2ba3a6ef2523f74,https://www.semanticscholar.org/paper/8a5b6be15e969a0aa05febcfd2ba3a6ef2523f74,"Biometric System and Data Analysis: Design, Evaluation, and Data Mining","Biometric systems are being used in more places and on a larger scale than ever before. As these systems mature, it is vital to ensure the practitioners responsible for development and deployment, have a strong understanding of the fundamentals of tuning biometric systems. The focus of biometric research over the past four decades has typically been on the bottom line: driving down system-wide error rates. In doing so, powerful recognition algorithms have been developed for a number of biometric modalities. These algorithms operate exceedingly well under test conditions. Books on biometrics tend to focus on biometric systems and their components, and differentiate between the various biometric modalities. Biometric System and Data Analysis: Design, Evaluation, and Data Mining brings together aspects of statistics and machine learning to provide a comprehensive guide to evaluating, interpreting and understanding biometric data. This professional book naturally leads to topics including data mining and prediction, which have been widely applied to other fields but not rigorously to biometrics, to be examined in detail. Biometric System and Data Analysis: Design, Evaluation, and Data Mining places an emphasis on the various performance measures available for biometric systems, what they mean, and when they should and should not be applied. The evaluation techniques are presented rigorously, however are always accompanied by intuitive explanations that can be used to convey the essence of the statistical concepts to a general audience. This last point is an important one for the increased acceptance of biometrics among non-technical decision makers, and ultimately the general public. Biometric System and Data Analysis: Design, Evaluation, and Data Mining is designed for a professional audience composed of practitioners and researchers in industry. This book is also suitable as a reference for advanced-level students in computer science and engineering.",,2010.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
920b236f530422313ebccf5d43f7d3dd9103beba,https://www.semanticscholar.org/paper/920b236f530422313ebccf5d43f7d3dd9103beba,"International Conference on Computing and Artificial Intelligence ( ICCAI 2019 ) April 19-22 , 2019 Bali , Indonesia","Recently, deep learning (DL) plays important roles in many academic and industrial areas especially in computer vision and image recognition. Deep learning uses a neural network with deep structure to build a high-level feature space. It learns data-driven, highly representative, hierarchical image features, which have proven to be superior to conventional hand-crafted low-level features and mid-level features. In ILSVRC2015 (an Annual competition of image classification at large scale), higher recognition accuracy by deep learning than human has been achieved. Deep learning (DL) has also been applied to medical image analysis. Compared with DL-based natural image analysis, there are several challenges in DL-based medical image analysis due to their high dimensionality and limited number of labeled training samples. We proposed several deep learning techniques for medical image analysis including medical image segmentation, medical image detection and medical image recognition. In this keynote talk, I will talk about current progress and futures of medical image analysis with deep learning. ICCAI 2019 CONFERENCE ABSTRACT 12 Keynote Speaker IV Prof. Qijun Zhao Sichuan University, China Prof. Qijun Zhao is currently a professor in the College of Computer Science at Sichuan University. He obtained his B.Sc. and M.Sc. degrees in computer science both from Shanghai Jiao Tong University, and his Ph.D. degree in computer science from the Hong Kong Polytechnic University. He worked as a post-doc research fellow in the Pattern Recognition and Image Processing Lab at Michigan State University from 2010 to 2012. His recent research interests lie in 3D face modeling and recognition, with applications to forensics, intelligent video surveillance, mobile security, healthcare, and human-computer interactions. Dr. Zhao has published more than 60 papers in academic conferences and journals, including CVPR, ECCV, AAAI, ICB, IEEE Trans., and PR. He is the principal investigator for two projects funded by NSFC, one project funded by the National Key Research and Development Program of China, and many projects funded by companies. Dr. Zhao is a reviewer for many renowned field journals and conferences, such as IEEE TPAMI, IEEE TIFS, IJCV, PR, PRL, ICCV, CVPR, ECCV, and FG. He served as a program committee co-chair in organizing the 11th Chinese Conference on Biometric Recognition (CCBR 2016), the 2018 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA), and the 2018 6th International Conference on Bioinformatics and Computational Biology (ICBCB 2018), and as a face recognition area co-chair for the 9th IEEE International Conference on Biometrics: Theory, Applications, and Systems (BTAS 2018). Topic: ""3D Face Reconstruction in Recognition Perspective"" Abstract—The face reveals a lot of information of humans, for example, identity, race, gender, age, emotion, intention, and health. 3D face models are thus widely studied in many disciplines. Yet, acquisition of 3D faces is still much more expensive and less convenient than acquisition of 2D face images, making it unaffordable to deploy 3D face technology in many real-world applications. Our research aims to reconstruct 3D face shapes from either single or multiple uncalibrated 2D face images from a perspective of identity recognition. This talk will introduce our recent progress along this direction. The methods we propose enable not only efficient generation of 3D face models when only 2D imaging devices are available, but also effective exploration of 3D face information for improving face recognition accuracy. We believe that 3D faces will play increasingly important roles in many applications with the rapid development of both 3D face acquisition techniques and 3D face modeling methods.The face reveals a lot of information of humans, for example, identity, race, gender, age, emotion, intention, and health. 3D face models are thus widely studied in many disciplines. Yet, acquisition of 3D faces is still much more expensive and less convenient than acquisition of 2D face images, making it unaffordable to deploy 3D face technology in many real-world applications. Our research aims to reconstruct 3D face shapes from either single or multiple uncalibrated 2D face images from a perspective of identity recognition. This talk will introduce our recent progress along this direction. The methods we propose enable not only efficient generation of 3D face models when only 2D imaging devices are available, but also effective exploration of 3D face information for improving face recognition accuracy. We believe that 3D faces will play increasingly important roles in many applications with the rapid development of both 3D face acquisition techniques and 3D face modeling methods. ICCAI 2019 CONFERENCE ABSTRACT 13 Keynote Speaker V Assoc. Prof. Ken‘ichi Morooka Kyushu University, Japan Assoc. Prof. Ken’ichi Morooka received his M.S. and Ph.D. degrees from Kyushu University, in 1997 and 2000, respectively. He was a visiting researcher with Institute of Systems & Information Technologies/KYUSHU. From 2000 to 2006, he was an associate professor in Graduate School of Science and Engineering, Tokyo Institute of Technology. He was an associate professor in Digital Medicine Initiative (2006-2010) and Department of Medical Sciences, Kyushu University (2010). Currently, he is an associate professor in Graduate School of Information Science and Electrical Engineering, Kyushu University. Also he was a visiting researcher, Illinois Institute of Technology, U.S. (2016). He has published more than 100 journal and conference articles. He has served as a member of organizing and program committees at numerous conferences, e.g. he has been program committes of MLMI 2018 and 2017, IFMIA 2017, CARS 2014 and EMBC 2013. His research interests cover computer-aided support system for therapy and surgery by image information processing and machine learning. Topic: ""Computer Aided System for Minimally Invasive Surgery Using Deep Learning"" Abstract—Recently, deep neural networks (DNNs) have been paid attention by various research fields including vision, audio and natural language. Of course, there are many DNN-based systems for therapy and diagnosis. Our research group has been doing research about computer-aided support systems for safe and accurate minimally invasive surgeries. Especially, to provide useful information for surgeons, our support systems use stereo endoscopic images, DNNs and 3D shapes and deformations of organs. I will present the fundamental techniques of our support system.Recently, deep neural networks (DNNs) have been paid attention by various research fields including vision, audio and natural language. Of course, there are many DNN-based systems for therapy and diagnosis. Our research group has been doing research about computer-aided support systems for safe and accurate minimally invasive surgeries. Especially, to provide useful information for surgeons, our support systems use stereo endoscopic images, DNNs and 3D shapes and deformations of organs. I will present the fundamental techniques of our support system. ICCAI 2019 CONFERENCE ABSTRACT 14 Invited Speaker Assoc. Prof. Sugiono Sugiono Brawijaya University, Indonesia Sugiono, Ph.D was born in Blitar, Indonesia, in 1978. He finished Bachelor degree in Mechanical Engineering Department at Brawijaya University in 2001, received Master Degree in Industrial Engineering at Sepuluh Nopember Institute of Technology, Surabaya in 2004, and graduated Ph.D. degree of Art, Design and Technology from University of Derby, UK, in 2012. Title of his thesis (PhD) is: Investigating an Intelligent Concept Design Tool for Automotive Car Body Design. His research interests lie in bioengineering ergonomics and intelligent product design. He worked as project analyser in investigating of fuel distribution for industry at PT. Surveyor Indonesia from 2001 to 2002. He also worked as purchasing vice leader at PT. Mitra Saruta (Textile) from 2004 to 2005. Currently, he is working as a lecturer at Department of Industrial Engineering, Brawijaya University start from 2005. He is a head of Work Design and Ergonomics Laboratory and head of Research Committee at Brawijaya University. He is an international reviewer of research, certificated by ISO 17024. He is also working as editor in chief of the Indonesian Journal of Disability Studies (IJDS). He is a senior member of Hong Kong Chemical, Biological and Environmental Engineering Society (HKCBEES), member of Indonesian Ergonomics Society (Perhimpunan Ergonomi Indonesia – PEI) and Member of International Association of Engineers (IAENG). Topic: ""The Importance of Open Innovation Concept to Improve Health and Safety Factors in Transportation"" Abstract—Controlling driver stress level is going to be popular research and put it a very important factor to reduce the risk of a road accident. Understanding the role of road complexity and information technology in transportation issues and their relationship with humans psychophysiological is a good challenge and profitable prospect for the future. Images from the Electrocardiograph (ECG) and Electroencephalography (EEG) are the important tools to identify the driver stress as part of a safety alert system. The Electrocardiograph (ECG) is to monitor every heart rate change and Electroencephalography (EEG) is to record brain signal change correlated with brain functions (thinking, visual, decision, etc.) from three different road types (city road, rural road, and motorway). In this speech, I will deliver a potential open innovation of health and safety factors in transportation (car, train) from the perspective of interaction among human, car, and environment.Controlling driver stress level is going to be popular research and put it a very important factor to reduce the risk of a road accident. Understanding the role of road complexity and information t",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b50d5ecc3e581cf6e0cfbc4c313e0cff78dd2a63,https://www.semanticscholar.org/paper/b50d5ecc3e581cf6e0cfbc4c313e0cff78dd2a63,Developing an Autonomous Swarm of Small Helicopters: Controlling Cooperative Team Behaviour for Search and Surveillance,"This work describes the components for controlling an autonomous swarm of small helicopters to execute search and surveillance tasks. The proposed system will be designed to support and be deployed by land forces. The aim is that the swarm can operate fully autonomously and use cooperative team strategies based on decentralized multiagent system control. The different components of the system are investigated by an interdisciplinary team in the Newcastle Robotics Lab comprising experts from computer engineering, computer science, computer vision, control, electrical engineering, machine learning, mechatronics, optimisation, wireless communications, signal processing and software engineering. The proposed system can help to reduce risk in land force missions that require search and exploration of dangerous or unknown environments.",,2013.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a8db0a284e7ece81f20c30a2a473399ea07f682c,https://www.semanticscholar.org/paper/a8db0a284e7ece81f20c30a2a473399ea07f682c,Investigation and Analysis of Efficient Pattern Discovery Method for Text Mining,"the concept of text mining is nothing but the mechanism of extracting non-trivial and interesting data from the unstructured text dataset. Text mining is consisting of many computer science disciplines with highly oriented towards the artificial intelligence in general such as the applications like information retrieval, pattern recognition, machine learning, natural language processing, and neural networks. The main difference between the search and text mining is that, search needs users attentions means based users requirement search action will perform whereas text mining is the internal process which attempts to find out information in the pattern which is not known before. To do the text mining, there are many methods presented still to the date those are having their own advantages and disadvantages. The major problems related to such techniques are efficient use and update of discovered patterns, problems related to the synonymy and polysemy etc. In this paper we are investigating the one such method which is presented to overcome above said problems related to the text mining's. The method presented here is based on innovative as well as effective pattern discovery technique and this consisting of processes like pattern deploying and pattern evolving in order to improve the effectiveness of using and updating discovered patterns for finding relevant and interesting information.",,2013.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
81c37e0a9828a547dd18a102969881b49af070d8,https://www.semanticscholar.org/paper/81c37e0a9828a547dd18a102969881b49af070d8,Oswald and Urwin: Written evidence to Commons Science and Technology Committee inquiry into algorithms in decision-making (April 2017),"Executive summary 
 
- In the UK policing context, the use of algorithmic decision-making tools could be described as 
being in a developmental stage with implementation on a force by force basis; 
- Such tools may be used in a number of different contexts, including decision-making or risk- 
assessments relating to individuals; 
- 
‘Algorithms in Policing –Take ALGO-CARE™’ is a proposed decision-making framework for the 
deployment of algorithmic assessment tools in the policing context; 
- 
Algo-care aims to translate key public law and human rights principles into practical 
considerations and guidance that can be addressed by public sector bodies; 
- Concerns around transparency and accountability cannot be addressed by a one-size-fits-all 
approach; 
- The factors identified by Algo-care necessitate the careful drafting of procurement contracts 
with third party software suppliers to require disclosure of algorithmic workings in a way 
that would facilitate investigation; 
- 
A number of challenges remain to the satisfactory audit and validation of machine learning 
algorithmic tools.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
229fc8a087adad290460117ba621b3cf5d4ed9c3,https://www.semanticscholar.org/paper/229fc8a087adad290460117ba621b3cf5d4ed9c3,Audio computing in the wild: frameworks for big data and small computers,"This talk introduces some machine learning algorithms that are designed to process as much data as needed while spending the least possible amount of resources, such as time, energy, and memory. Examples of those applications, but not limited to, can be a large-scale multimedia information retrieval system where both queries and the database items are noisy signals; collaborative audio enhancement from hundreds of user-created Youtube clips of a music concert; an event detection system running in a small device that has to process various sensor signals in real time; a lightweight custom chipset for speech enhancement on handheld devices; instant music analysis engine running on smartphone apps. In all those applications, efficient machine learning algorithms are supposed to achieve not only a good performance, but also a great resourceefficiency. To meet these contradicting requirements at the same time, I have developed various matrix factorization algorithms (or topic models): a topic model that takes sparse landmark representations as input, a latent component sharing technique to analyze a set of crowdsourced audio recordings, and a hashing-based speed-up technique for faster sparse coding in topic modeling. Finally, to describe an extremely optimized deep learning deployment system, Bitwise Neural Networks (BNN) will be also discussed. In BNNs, all the inputs, outputs, and operations are defined with Boolean algebra (e.g. in BNNs a multiplication between floating-points is reduced down to a single XNOR gate for the two binary inputs). Some preliminary results on the MNIST dataset and speech denoising demonstrate that a straightforward extension of backpropagation can successfully train BNNs whose performance is comparable while necessitating vastly fewer computational resources. Biography: Minje Kim is a PhD candidate in the Department of Computer Science at the University of Illinois at Urbana-Champaign. Before joining UIUC, he worked as a researcher in ETRI, a national lab in Korea, from 2006 to 2011. He received his Bachelor’s and Master’s degrees in the Division of Information and Computer Engineering at Ajou University (with honor) and in the Department of Computer Science and Engineering at POSTECH (Summa Cum Laude) in 2004 and 2006, respectively. During his PhD study, he interned at Creative Technologies Lab in Adobe Research four times from 2012 to 2015. His research focuses on developing machine learning algorithms applied to audio processing, stressing out the computational efficiency in the resource-constrained environments or in the applications involving large unorganized datasets. He received Richard T. Cheng Endowed Fellowship from UIUC in 2011. Google and Starkey grants also honored his ICASSP papers as the outstanding student papers in 2013 and 2014, respectively. Audio Computing in the Wild: Frameworks for Big Data and Small Computers",,2016.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
88d7163b49626faf661e1f9ac3cc4af1c5b3ca83,https://www.semanticscholar.org/paper/88d7163b49626faf661e1f9ac3cc4af1c5b3ca83,SSC 19-WKII-02 On-orbit Calibration of Magnetometer Using Stochastic Gradient Descent,"Magnetometers are a key component of small satellite attitude determination and control systems (ADCS). They are typically calibrated on the ground during the spacecraft Assembly, Integration, and Testing (AIT) program. UNSW Canberra Space have observed magnetometer calibration changes during the AIT program, with the most likely cause being spacecraft component magnetisation. Vibration testing has been noted to result in significant changes to magneto-inductive magnetic sensor calibration biases, and to a lesser extent, scale gains. Ground-based calibration can be time consuming, manpower intensive and susceptible to human error. For larger spacecraft production runs, it is desirable to reduce or eliminate the time required to conduct calibration. This paper outlines the use of stochastic gradient descent as a way of calibrating magnetometers on-orbit. The method was tested using the Buccaneer Risk Mitigation Mission satellite developed and operated jointly by Australia’s Defence Science and Technology Group and University of New South Wales Canberra Space. The newly tuned calibration parameters were successfully tested on-orbit. The results are compared with the values generated during ground-based calibration. The method reduced error by approximately 20% during a six-orbit test period. The updated parameters result in an angular change in the indicated magnetic direction of up to 7.2 deg. INTRODUCTION Magnetometers are a key component of small satellite attitude determination and control systems (ADCS). They are typically calibrated on the ground during the spacecraft Assembly, Integration, and Testing (AIT) program. UNSW Canberra Space have observed magnetometer calibration changes during the AIT program, with the most likely cause being spacecraft component magnetisation. Vibration testing has been noted to result in significant changes to magneto-inductive magnetic sensor calibration biases, and to a lesser extent, scale gains. Both the mechanical vibration and the high magnetic field from the vibration table are expected to induce changes in the calibration offsets. Ground-based calibration can be time consuming, manpower intensive and susceptible to human error. For larger spacecraft production runs, it is desirable to reduce or eliminate the time required to conduct calibration. On-orbit calibration of magnetometers is of interest to UNSW Canberra Space (UCS) to reduce AIT workload and to avoid issues that occur due to changes in magnetometer calibration. Typically, on-orbit calibration is conducted using non-linear least-squares or other numerical error minimization methods. There is an increasing interest in the application of machine learning methods to spacecraft attitude determination and control solutions. The application of simple machine learning methods offers a starting point for the development of more complex applications of machine learning to spacecraft attitude determination and control. Additionally, for spacecraft with deployable hardware, it can be infeasible to calibrate magnetometers in flight configuration; requiring on-orbit calibration methods. Primary magnetometers are commonly mounted on a deployable boom arm. Launch Services Provider (LSP) do not always provide authorisation to deploy and restow these boom arm after acceptance vibration testing. This paper outlines UCS application of stochastic gradient descent to an in-orbit magnetometer to improve magnetometer calibration, and UCS intention to develop these methods for future missions. ON-ORBIT CALIBRATION METHODS Numerical methods are commonly used for the calibration of magnetometers and other attitude determination sensors on spacecraft in-orbit. Such methods include the TWOSTEP algorithm, the differential value approach and Kalman Filters. Gradient Descent (GD) is a machine learning technique via which neural networks learn. The Buccaneer Risk",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0b0bb8652877edd9d363213bcc98e8e3605ea669,https://www.semanticscholar.org/paper/0b0bb8652877edd9d363213bcc98e8e3605ea669,Special issue XSEDE16 & PEARC17 – Practice and experience in advanced research computing,"The conference on Practice and Experience in Advanced Research Computing (PEARC) provides a forum for discussing challenges, opportunities, and solutions among the broad range of participants in the research computing community. This community-driven effort builds on successes of the past and aims to grow and be more inclusive by involving additional local, regional, national, and international cyberinfrastructure and research computing partners spanning academia, government, and industry. The PEARC conference series is working to integrate and meet the collective interests of our growing community. PEARC originated from the XSEDE conference series, which showcased the discoveries, innovations, challenges, and achievements of those who use and support NSF Extreme Science and Engineering Discovery Environment (XSEDE) resources and services, as well as other digital resources and services throughout the world. The following papers represent the best papers from the transitionary conferences of this research community: XSEDE16, the final XSEDE conference, and PEARC17, the inaugural PEARC conference. These papers capture both important research contributions to the advanced computing community and best practices for advanced computing systems and remote user interfaces. These three papers cover a diverse set of topics: the impact of science gateways, innovative hardware impacting cyberinfrastructure, and improving computational frameworks with workflows, machine learning, and visualizations. The paper by Hu et al1 discusses how geospatial data have exploded to massive volume and diversity and subsequently cause serious usability issues for researchers in various scientific areas. This paper describes TopoLens, a cyberGIS community data service framework to facilitate geospatial big data access, processing, and sharing based on a hybrid supercomputer architecture. TopoLens delivers community data services developed for easy and efficient access to high-resolution topographic data. It supports on-demand data and map services, powered by hybrid cyberinfrastructure with cloud and HPC support, to efficiently produce datasets that are customized based on a user's request. The usability of TopoLens has been acknowledged in the topographic user community evaluation. The paper by Hancock et al2 dives into Jetstream, the NSF's first distributed production cloud resource. Jetstream offers a unique capability within the XSEDE-supported US national cyberinfrastructure, delivering interactive virtual machines (VMs) via the atmosphere interface. As a multi-region deployment that operates as an integrated system, Jetstream is proving effective in supporting modes and disciplines of research traditionally underrepresented on larger XSEDE-supported clusters and supercomputers. Jetstream has been used to perform research and education in biology, biochemistry, atmospheric science, earth science, and computer science. Lastly, the paper written by Li and Song3 discusses the challenges of large-scale simulations generating huge amounts of data with potentially critical information that is saved in intermediate files and is not instantly visible until advanced data analytics techniques are applied after reading all simulation. In this paper, the authors build a new computational framework to couple scientific simulations with multi-step machine learning processes and in situ data visualizations. This computational framework is built upon different software components and provides plug-in data analysis and visualization functions over complex scientific workflows. With this advanced framework, users can monitor and get real-time notifications of special patterns or anomalies from ongoing extreme-scale turbulent flow simulations. These three papers demonstrate the wide range of topics addressed by the PEARC community, which spans academic researchers, resource providers, industry partners, and other affiliates. The work done by this community and the insights shared at the annual PEARC conference are intended for broad application for those active in advanced computing research and practice. If you are already part of the PEARC community, we look forward to seeing you soon; if you have not yet attended a PEARC conference, we hope these papers will provide motivation and justification to invest your time and travel to join us. The editors would like to acknowledge the hard work of the program committees, both for XSEDE16 and for PEARC17, whose combined efforts resulted in this special issue. Sincerely, Paul Navrátil Maytal Dahan Technical Program Chair Technical Program Chair XSEDE16 PEARC17",Concurr. Comput. Pract. Exp.,2019.0,10.1002/CPE.5325,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
76747636a3682ea8ae657a236901f2f12f71c7f5,https://www.semanticscholar.org/paper/76747636a3682ea8ae657a236901f2f12f71c7f5,"devoted to Concept Theory , Classification , Indexing and Knowledge Representation Contents","Knowledge organization (KO) is considered a distinctive disciplinary focus of information science, with strong connections to other intellectual domains such as philosophy, computer science, psychology, sociology, and more. Given its inherent interdisciplinarity, we ask what might a map of the physical, cultural, and intellectual geography of the KO community look like? Who is participating in this discipline’s scholarly discussion, and from what locations, both geographically and intellectually? Using the unit of authorship in the journal Knowledge Organization, where is the nexus of KO activity and what patterns of authorship can be identified? Cultural characteristics were applied as a lens to explore who is and is not participating in the international conversation about KO. World Bank GNI per capita estimates were used to compare relative wealth of countries and Hofstede’s Individualism dimension was identified as a way of understanding attributes of countries whose scholars are participating in this dialog. Descriptive statistics were generated through Excel, and data visualizations were rendered through Tableau Public and TagCrowd. The current project offers one method for examining an international and interdisciplinary field of study but also suggests potential for analyzing other interdisciplinary areas within the larger discipline of information science. Hauser, Elliott and Joseph T. Tennis. 2019. “Episemantics: Aboutness as Aroundness.” Knowledge Organization 46 (8): 590595. 16 references. DOI:10.5771/0943-7444-2019-8-590. Abstract: Aboutness ranks amongst our field’s greatest bugbears. What is a work about? How can this be known? This mirrors debates within the philosophy of language, where the concept of representation has similarly evaded satisfactory definition. This paper proposes that we abandon the strong sense of the word aboutness, which seems to promise some inherent relationship between work and subject, or, in philosophical terms, between word and world. Instead, we seek an etymological reset to the older sense of aboutness as “in the vicinity, nearby; in some place or various places nearby; all over a surface.” To distinguish this sense in the context of information studies, we introduce the term episemantics. The authors have each independently applied this term in slightly different contexts and scales (Hauser 2018a; Tennis 2016), and this article presents a unified definition of the term and guidelines for applying it at the scale of both words and works. The resulting weak concept of aboutness is pragmatic, in Star’s sense of a focus on consequences over antecedents, while reserving space for the critique and improvement of aboutness determinations within various contexts and research programs. The paper finishes with a discussion of the implication of the concept of episemantics and methodological possibilities it offers for knowledge organization research and practice. We draw inspiration from Melvil Dewey’s use of physical aroundness in his first classification system and ask how aroundness might be more effectively operationalized in digital environments. Aboutness ranks amongst our field’s greatest bugbears. What is a work about? How can this be known? This mirrors debates within the philosophy of language, where the concept of representation has similarly evaded satisfactory definition. This paper proposes that we abandon the strong sense of the word aboutness, which seems to promise some inherent relationship between work and subject, or, in philosophical terms, between word and world. Instead, we seek an etymological reset to the older sense of aboutness as “in the vicinity, nearby; in some place or various places nearby; all over a surface.” To distinguish this sense in the context of information studies, we introduce the term episemantics. The authors have each independently applied this term in slightly different contexts and scales (Hauser 2018a; Tennis 2016), and this article presents a unified definition of the term and guidelines for applying it at the scale of both words and works. The resulting weak concept of aboutness is pragmatic, in Star’s sense of a focus on consequences over antecedents, while reserving space for the critique and improvement of aboutness determinations within various contexts and research programs. The paper finishes with a discussion of the implication of the concept of episemantics and methodological possibilities it offers for knowledge organization research and practice. We draw inspiration from Melvil Dewey’s use of physical aroundness in his first classification system and ask how aroundness might be more effectively operationalized in digital environments. Broughton, Vanda. 2019. “The Respective Roles of Intellectual Creativity and Automation in Representing Diversity: Human and Machine Generated Bias.” Knowledge Organization 46(8): 596606. 82 references. DOI:10.5771/0943-7444-2019-8-596. Abstract: The paper traces the development of the discussion around ethical issues in artificial intelligence, and considers the way in which humans have affected the knowledge bases used in machine learning. The phenomenon of bias or discrimination in machine ethics is seen as inherited from humans, either through the use of biased data or through the semantics inherent in intellectually-built tools sourced by intelligent agents. The kind of biases observed in AI are compared with those identified in the field of knowledge organization, using religious adherents as an example of a community potentially marginalized by bias. A practical demonstration is given of apparent religious prejudice inherited from source material in a large database deployed widely in computational linguistics and automatic indexing. Methods to address the problem of bias are discussed, including the modelling of the moral process on neuroscientific understanding of brain function. The question is posed whether it is possible to model religious belief in a similar way, so that robots of the future may have both an ethical and a religious sense and themselves address the problem of prejudice. The paper traces the development of the discussion around ethical issues in artificial intelligence, and considers the way in which humans have affected the knowledge bases used in machine learning. The phenomenon of bias or discrimination in machine ethics is seen as inherited from humans, either through the use of biased data or through the semantics inherent in intellectually-built tools sourced by intelligent agents. The kind of biases observed in AI are compared with those identified in the field of knowledge organization, using religious adherents as an example of a community potentially marginalized by bias. A practical demonstration is given of apparent religious prejudice inherited from source material in a large database deployed widely in computational linguistics and automatic indexing. Methods to address the problem of bias are discussed, including the modelling of the moral process on neuroscientific understanding of brain function. The question is posed whether it is possible to model religious belief in a similar way, so that robots of the future may have both an ethical and a religious sense and themselves address the problem of prejudice. Chen, Shu-Jiun. 2019. “Semantic Enrichment of Linked Personal Authority Data: A Case Study of Elites in Late Imperial China.” Knowledge Organization 46(8): 607-614. 13 references. DOI:10. 5771/0943-7444-2019-8-607. Abstract: The study uses the Database of Names and Biographies (DNB) as an example to explore how in the transformation of original data into linked data, semantic enrichment can enhance engagement in digital humanities. In the preliminary results, we have defined instance-based and schema-based categories of semantic enrichment. In the instance-based category, in which enrichment occurs by enhancing the content of entities, we further determined three types, including: 1) enriching the entities by linking to diverse external resources in order to provide additional data of multiple perspectives; 2) enriching the entities with missing data, which is needed to satisfy the semantic queries; and, 3) providing the entities with access to an extended knowledge base. In the schema-based categories that enrichment occurs by enhancing the relations between the properties, we The study uses the Database of Names and Biographies (DNB) as an example to explore how in the transformation of original data into linked data, semantic enrichment can enhance engagement in digital humanities. In the preliminary results, we have defined instance-based and schema-based categories of semantic enrichment. In the instance-based category, in which enrichment occurs by enhancing the content of entities, we further determined three types, including: 1) enriching the entities by linking to diverse external resources in order to provide additional data of multiple perspectives; 2) enriching the entities with missing data, which is needed to satisfy the semantic queries; and, 3) providing the entities with access to an extended knowledge base. In the schema-based categories that enrichment occurs by enhancing the relations between the properties, we Knowl. Org. 46(2019)No.8 KO KNOWLEDGE ORGANIZATION Official Journal of the International Society for Knowledge Organization ISSN 0943 – 7444 International Journal devoted to Concept Theory, Classification, Indexing and Knowledge Representation have identified two types, including: 1) enriching the properties by defining the hierarchical relations between properties; and, 2) specifying properties’ domain and range for data reasoning. In addition, the study implements the LOD dataset in a digital humanities platform to demonstrate how instances and entities can be applied in the full texts where the relationship between entities are highlighted in order to bring scholars more semantic details of the texts. Clavi",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6560c4ff2b5b08074d71c4a67aea1bf87062d7b9,https://www.semanticscholar.org/paper/6560c4ff2b5b08074d71c4a67aea1bf87062d7b9,Software Run-time Entropy: A Novel Type of Indicators for Software Failure Prediction,"With the development of computer science and software engineering, software becomes more and more complex. Traditional software reliability assurance techniques including software testing and evaluation can't ensure software reliable execution after being deployed. Software failure prediction techniques based on failure indicators can predict software failures according to abnormal indicator values. The latter can be collected using runtime monitoring techniques. An essential part of this method is finding proper indicators which have strong correlation with software failures. We propose a novel type of indicators in this work named software runtime entropy, which takes both software module execution time and call times into consideration. Three common open source software, grep, flex and gzip are used as study cases for finding the relationships between the indicators and software failures. Firstly, a series of fault injection experiments are conducted on those three software respectively. The decision tree algorithm is used to train those data to build the correlation models between software runtime entropy and software failures. Several common measures in machine learning domains such as accuracy, recall rates, and F-measure are used to evaluate the models. The decision tree models can be used as failure mechanisms to assist the failure prediction work. One can examine the value of runtime entropy and make a warning report when it ranges from the normal interval to abnormal one.",,2019.0,10.20944/preprints201909.0238.v1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cc79c0fa6e39ea58910fe54bb9b966d333d76cb3,https://www.semanticscholar.org/paper/cc79c0fa6e39ea58910fe54bb9b966d333d76cb3,Data Engineering Project (Educating for the Future PhUSE Working Group),"With an expected 100% increase, over the next 3 years, of data from non-EDC sources (such as smartphones, wearables and custom apps) the traditional methods of managing data for clinical trials presents executives with a resourcing headache. As such, many companies are looking for lower cost strategies to mitigate this resourcing challenge. However, citing case studies from other industries, there are new methodologies/technologies in data engineering which could enable automation of much of the “heavy-lifting” currently practiced in clinical data management and statistical programming. This paper discusses the Data Engineering Project within the PhUSE Computational Science (CS) Working Group, Educating for the Future, with a view to educate clinical data managers in data engineering principles so that they can be prepared, equipped and effective in dealing with the coming “data tsunami” heading to the shores of clinical research. INTRODUCTION Did you realise we are living in the age of the Fourth Industrial Revolution? Perhaps you have been busy downloading a myriad of “apps” designed to make your life easier or connecting on social media, uncovering relationships and associations you didn’t even know you had. Perhaps you have been shopping a global marketplace, comparing prices, quality and availability, all at your fingertips and in a minutes’ time. While this has been happening, the Fourth Industry Revolution has been evolving at exponential proportions . Just ask Siri! The term “Industrie 4.0”, was originated in Germany, as a government-led initiative, to transform manufacturing through advanced digital capability. Thus creating the concept of a “smart factory”, based on four key design principles : 1. Interconnection of machines, devices, sensors and people 2. Vast amounts of useful information (data) to drive decision making 3. Technical assistance to aid humans, for example to visualise data or to perform tasks that may be a safety risk for a human. 4. The use of cyber-physical systems to make decisions on their own and to perform tasks as autonomously as possible. Emerging from the premise of “Industrie 4.0” is the advent of the term “The Fourth Industrial Revolution” (also referred to as “4IR” or “I4.0”). This term originated in 2016 when described by Klaus Schwab (Founder and Executive Chairman of the World Economic Forum), as a “technological revolution that will fundamentally alter the way we live, work, and relate to one another”. Klaus goes on to describe it as a digital revolution with innovative uses of a combination of technologies that build upon the premise of the third revolution (i.e. electronics and information technology to automate production). As a result, emerging technologies have brought forth advancements in fields such as artificial intelligence, robotics, the Internet of Things, autonomous vehicles, 3D printing, nanotechnology, biotechnology, materials science, energy storage, and quantum computing. This rapid evolution will undoubtedly affect industries world-wide, already disrupting many industries, such as travel agencies, video rentals and bookstores . The pharmaceutical industry is also experiencing the impacts of I4.0. Digital and mobile technologies have brought on significant advancements in data acquisition and accessibility as it relates to health care and patient data. A recent study, conducted in 2017, by Tufts University in collaboration with Veeva Systems, shows that close to 100% 2 of companies surveyed, estimate utilization of various electronic data sources in clinical studies. As shown in Figure 1. below, companies utilizing e-sources such as, smartphones, custom applications, and mobile health, will more than double in the next 3 years . Therefore, requiring greater capabilities in handling large volumes of data, data from multiple sources and data of varying formats. As with other industries, data will become a critical asset and the effective utilisation of this data can play a key role in driving growth in the business while bringing novel therapies to the patients who need them. source: Tufts – Veeva 2017 EClinical Landscape Study. Tufts University, 2018, pp. 11–13, Tufts – Veeva 2017 EClinical Landscape Study. In this paper, we will focus on the works of the Data Engineering Project within the Educating for the Future Working Group. With the formation of the Working Group in early 2018, the team had taken on the mission to explore how data engineering techniques, successfully deployed in other industries, could be utilised in the pharmaceutical industry, with a goal to facilitate the education of the pharmaceutical industry on these techniques. We will share with you some introductory information about Data Engineering and Data Science and explore how embracing new data engineering techniques may affect the industry culture. You will learn about use cases of Data Engineering in other industries and how advances in digital capability have affected their business model. We will also share some of the many software packages and tools available to enable automation, commonly used in Data Engineering and Data Science. Finally we will reflect on the benefits that data standardisation has brought to the pharmaceutical industry and share our vision for disseminating information to facilitate your learning going forward. DATA ENGINEERING To start this learning journey, exploring the term “Data Engineering” opens the door to the vast opportunities and roles available today with the overarching goal to optimise the use of data in day to day business operations. In doing a simple search on the internet, “what is data engineering?”, one will find many posts expressing their understanding of Data Engineering, with some variation. However, what is clear is that Data Engineering encompasses the many considerations that need to be taken into account to optimally curate, transform, secure and disseminate data suitable for analysis. As technology and tools have become more advanced, building such a platform and infrastructure requires engineers and architects of both general and specific expertise. The Data Engineer combines knowledge in areas such as software development, infrastructure, data architecture, data warehousing, cloud technology and data cleaning in order to design, build and 3 test solutions that define the pipelines of data throughout the enterprise, making the data accessible to the organisation. [5] [27] [31] Optimised Data Engineering appropriately balances the efficiency of an automated process against the cost of development and maintenance of that process, ensuring repetitive processes that require humans to write code, press keys, cut-and-paste and update documents are minimised or eliminated. DATA SCIENCE Often paired with the term “Data Engineering” is also the term “Data Science”. According to Kelle O’Neal and Charles Roe: “Data Science allows enterprises the ability to turn their data assets into a narrative. Data Science allows that narrative to be expanded across timelines, in different data spaces that trace from the past into the future, with much more involved questions and answers about an enterprise, different potential outcomes, and repercussions based on recommendations. Data Science employs a range of mathematical, business, and scientific techniques to solve complex problems about an organisation’s data assets.” [26] In contrast, the focus of the Data Engineer is on the process from data curation to dissemination and the focus of the Data Scientist is on the analytics of the data, thus extracting knowledge from the data. To achieve quality data capture, near-real-time accessibility and meaningful analytics, one cannot function without the other, and effective teamwork optimises the value of each role. As such, an analytics team would be composed of distinct roles/capabilities : ● Data Engineers (in areas such as database architecture, database development, machine learning architecture, ETL scripting, etc.) ● Data Scientists ● Business Analysts Data Engineering brings together the broad expertise, of these roles, to ensure the data are curated and accessible to the Data Scientist, and in our environment today, this process is becoming more and more complex. Therefore, expertise in curating big-data and data of varying formats (structured and unstructured) is a critical core competency to optimise the potential impact of these digital assets (i.e. the data). The Data Scientist works deep in the data, utilizing various tools and techniques to discover patterns in the data that may drive decision making for the business. Optimising utilisation of the data to enable accurate conclusions can bear greater value to the organisation. As an example, per Tom Eunice’s post, “a fraud-detection algorithm may be very accurate when based on many months of historical data. However, months of historical data may not always be available. Designing a fraud-detection model that is still accurate using historical data from only a few days would be of more use and more practical to implement.” [17] The Business Analyst helps the Data Scientist understand the meaning of the data and the relevance of any discovered relationships. Initially, uncovering relationships in the data and upon further investigation, identifies meaningful patterns that may reveal information that otherwise may not have been known. [17] As you will see in the sections to follow, the full complement of the roles in an analytics team is what drives the business value. One discipline without the other (e.g. data engineering without data science) will result in missed opportunities. In the sections to follow, we often refer to Data Engineering, however, due to the close ties to Data Science, some examples elude to both Data Engineering and Data Science. USE CASES FROM OTHER INDUSTRIES In this section, we present three use cases from the transportation, retail, and agricultural industries. T",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a7ccf4a08be3847dac5c13928583b1665cf77c81,https://www.semanticscholar.org/paper/a7ccf4a08be3847dac5c13928583b1665cf77c81,Dynamic Data Management In A Data Grid Environment,"A data grid is a geographically distributed set of resources providing a facility for com-putationally intensive analysis of large datasets to a large number of geographically distributed users. In the scientific community, data grids have become increasingly popular as scientific research is driven by large datasets. Until recently, developments in data management for data grids have focused on management of data at lower layers in the data grid architecture. With dataset sizes expected to approach ex-abyte scale in coming years, data management in data grids are facing a new set of challenges. In particularly, the problem of automatically placing and deleting data replicas to optimally use grid resources. This thesis describes a dynamic data management framework to handle automatic replica creation and deletion in a data grid environment. The dynamic data manager uses machine learning to predict data popularity and balance the system for improved end-user performance. We implement the dynamic data manager for CMS, one of the largest high-energy physics experiments in the world, and evaluate the performance of the deployed system. iii ACKNOWLEDGMENTS I would like to thank my advisor, David Swanson. Every time I felt like I was in over my head, David was there to calm me down and keep me moving in the right direction. Also, I would like to thank him for hiring me as a graduate research assistant, giving me the chance to enter the world of distributed computing. I want to thank my unofficial advisor, Brian Bockelman. I appreciate his guidance and excellent technical support during my time in HCC. He is a bottomless well of knowledge and I could never have done this without him. I want to thank Ying Lu. Ying took me in and showed me the world of research when I was still new to Computer Science. Her kindness and support will forever inspire me to become a better person and help others like she has helped me. I also want to thank all the people in the CMS experiment that have helped me during this project. Without their dedication to curing my lack of knowledge in anything CMS related I would never have made it this far. I would especially like to thank Christoph Paus and Maxim Goncharov for giving the opportunity to work on this interesting project and letting me create my own solution. Also, I would like to thank Nicolo Magini for his prompt replies …",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6af5028a8944432e0ddc1ce6203783ae91587525,https://www.semanticscholar.org/paper/6af5028a8944432e0ddc1ce6203783ae91587525,The Interdisciplinary Nature Of Knowledge Discovery Databases And Data Mining,": Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. In the last few years, knowledge discovery and data mining tools have been used mainly in experimental and research environments, business user etc. A large degree of the current interest in KDD is the result of the media interest surrounding successful KDD applications, for example, the focus articles within the last two years in Business Week, Newsweek, Byte, PC Week, and other large-circulation periodicals. Unfortunately, it is not always easy to separate fact from media hype. Nonetheless, several well documented examples of successful systems can rightly be referred to as KDD applications and have been deployed in operational use on large-scale real-world problems in science and in business.",,2016.0,10.18535/IJECS/V4I11.16,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
07c9be06c353c0cea043b335ef39ed464d13aff1,https://www.semanticscholar.org/paper/07c9be06c353c0cea043b335ef39ed464d13aff1,"Eye Data , Big Data , Games , and Extreme Expertise","Although there are many machine-learning programs that can acquire new problem-solving strategies, it is not known exactly how their processes will manifest themselves in human behavior, if at all. In order to find out, a line-by-line protocol analysis was conducted of a subject discovering problemsolving strategies. A model was developed that could explain 96% of the lines in the protocol. On this analysis, the subject’s learning was confined to 11 rule acquisition events, wherein she temporarily abandoned her normal problem solving and focused on improving her strategic knowledge. Further analysis showed that: (1) Not all rule acquisition events are triggered by impasses; (2) Rules are acquired gradually, both because of competition between new and old rules, and because of the subject’s apparently deliberate policy of gradual generalization. (3) This subject took a scientific approach to strategy discovery, even planning and conducting small experiments. 3.4 Learning Problem-Solving, Perceptual Learning, and Decision Making – recent research we need to know about! 3.4.1 Week 5 – Learning Problem-Solving Lee, H. S., Betts, S., & Anderson, J. R. (2015). Learning problem-solving rules as search through a hypothesis space. Cognitive Science, 1–44. doi:10.1111/cogs.12275. Abstract: Learning to solve a class of problems can be characterized as a search through a space of hypotheses about the rules for solving these problems. A series of four experiments studied how different learning conditions affected the search among hypotheses about the solution rule for a simple computational problem. Experiment 1 showed that a problem property such as computational difficulty of the rules biased the search process and so affected learning. Experiment 2 examined the impact of examples as instructional tools and found that their effectiveness was determined by whether they uniquely pointed to the correct rule. Experiment 3 compared verbal directions with examples and found that both could guide search. The final experiment tried to improve learning by using more explicit verbal directions or by adding scaffolding to the example. While both manipulations improved learning, learning still took the form of a search through a hypothesis space of possible rules. We describe a model that embodies two assumptions: (1) the instruction can bias the rules participants hypothesize rather than directly be encoded into a rule; (2) participants do not have memory for past wrong hypotheses and are likely to retry them. These assumptions are realized in a Markov model that fits all the data by estimating two sets of probabilities. First, the learning condition induced one set of Start probabilities of trying various rules. Second, should this first hypothesis prove wrong, the learning condition induced a second set of Choice probabilities of considering various rules. These findings broaden our understanding of effective instruction and provide implications for instructional design. 3.4.2 Week 6 – Perceptual Learning Goldstone, R. L., Landy, D. H., & Son, J. Y. (2010, APR). The Education of Perception. Topics in Cognitive Science, 2(2), 265–284. doi:{10.1111/j.1756-8765.2009.01055.x}. Abstract: Although the field of perceptual learning has mostly been concerned with lowto middlelevel changes to perceptual systems due to experience, we consider high-level perceptual changes that accompany learning in science and mathematics. In science, we explore the transfer of a scientific principle (competitive specialization) across superficially dissimilar pedagogical simulations. We argue that transfer occurs when students develop perceptual interpretations of an initial simulation and simply continue to use the same interpretational bias when interacting with a second simulation. In arithmetic and algebraic reasoning, we find that proficiency in mathematics involves executing spatially explicit transformations to notational elements. People learn to attend mathematical operations in the order in which they should be executed, and the extent to which students employ their perceptual attention in EDBDG&EE Spring 2016 Page 7 this manner is positively correlated with their mathematical experience. For both science and mathematics, relatively sophisticated performance is achieved not by ignoring perceptual features in favor of deep conceptual features, but rather by adapting perceptual processing so as to conform with and support formally sanctioned responses. These “rigged-up perceptual systems” offer a promising approach to educational reform. Goldstone, R. L., de Leeuw, J. R., & Landy, D. H. (2015, FEB). Fitting perception in and to cognition. Cognition, 135(SI), 24–29. doi:{10.1016/j.cognition.2014.11.027}. Abstract: Perceptual modules adapt at evolutionary, lifelong, and moment-to-moment temporal scales to better serve the informational needs of cognizers. Perceptual learning is a powerful way for an individual to become tuned to frequently recurring patterns in its specific local environment that are pertinent to its goals without requiring costly executive control resources to be deployed. Mechanisms like predictive coding, categorical perception, and action-informed vision allow our perceptual systems to interface well with cognition by generating perceptual outputs that are systematically guided by how they will be used. In classic conceptions of perceptual modules, people have access to the modules’ outputs but no ability to adjust their internal workings. However, humans routinely and strategically alter their perceptual systems via training regimes that have predictable and specific outcomes. In fact, employing a combination of strategic and automatic devices for adapting perception is one of the most promising approaches to improving cognition. 3.4.3 Not Read or Discussed Dunn, T. L. & Risko, E. F. (2015). Toward a metacognitive account of cognitive offloading. Cognitive Science, n/a–n/a. doi:10.1111/cogs.12273. Abstract: Individuals frequently make use of the body and environment when engaged in a cognitive task. For example, individuals will often spontaneously physically rotate when faced with rotated objects, such as an array of words, to putatively offload the performance costs associated with stimulus rotation. We looked to further examine this idea by independently manipulating the costs associated with both word rotation and array frame rotation. Surprisingly, we found that individuals’ patterns of spontaneous physical rotations did not follow patterns of performance costs or benefits associated with being physically rotated, findings difficult to reconcile with existing theories of strategy selection involving external resources. Individuals’ subjective ratings of perceived benefits, rather, provided an excellent match to the patterns of physical rotations, suggesting that the critical variable when deciding on-the-fly whether to incorporate an external resource is the participant’s metacognitive beliefs regarding expected performance or the effort required for each approach (i.e., internal vs. internal + external). Implications for metacognition’s future in theories of cognitive offloading are discussed. Howes, A., Duggan, G. B., Kalidindi, K., Tseng, Y.-C., & Lewis, R. L. (2015). Predicting shortterm remembering as boundedly optimal strategy choice. Cognitive Science, n/a–n/a. doi:10.1111/ cogs.12271. Abstract: It is known that, on average, people adapt their choice of memory strategy to the subjective utility of interaction. What is not known is whether an individual’s choices are boundedly optimal. Two experiments are reported that test the hypothesis that an individual’s decisions about the distribution of remembering between internal and external resources are boundedly optimal where optimality is defined relative to experience, cognitive constraints, and reward. The theory makes predictions that are tested against data, not fitted to it. The experiments use a no-choice/choice utility learning paradigm where the no-choice phase is used to elicit a profile of each participant’s performance across the strategy space and the choice phase is used to test predicted choices within this space. They show that the majority of individuals select strategies that are boundedly optimal. Further, individual differences in what people choose to do are successfully predicted by the analysis. Two issues are discussed: (a) the EDBDG&EE Spring 2016 Page 8 performance of the minority of participants who did not find boundedly optimal adaptations, and (b) the possibility that individuals anticipate what, with practice, will become a bounded optimal strategy, rather than what is boundedly optimal during training. 3.5 Games 3.5.1 Week 6 – Overviews Rebetez, C. & Betrancourt, M. (2007). Video game research in cognitive and educational sciences. Cognition, Brain, Behavior, 11(1), 131–142. Retrieved from http://search.ebscohost.com.libproxy.rpi. edu/login.aspx?direct=true&db=psyh&AN=2007-05198-007&site=ehost-live&scope=site. Abstract: This work reviews several aspects of the growing research field interested in video games. First, the evolution of this media in the educational field is discussed. Three different fields interested in the cognitive impact playing of video games are reviewed: abilities and skills, attitudes and motivation, knowledge and content learning. However, most studies used video games as new experimental materials and tasks to contribute to their specific field (i.e., attention and perception), and not as a scientific object of interest per se. We claim that the research on video games is in need of a conceptual and methodological framework in which results and effects could be compared, interpreted and generalized. We argue that video games can have multiple effects on players and that these effects can be used as educational potentials. An empirically-based classification of games, depending on their potential effects for an educ",,2016.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a8f59ac02f845e7932528a28d013cbf25bdcf324,https://www.semanticscholar.org/paper/a8f59ac02f845e7932528a28d013cbf25bdcf324,"Advances in Data Analysis, Data Handling and Business Intelligence - Proceedings of the 32nd Annual Conference of the Gesellschaft für Klassifikation e.V., Joint Conference with the British Classification Society (BCS) and the Dutch/Flemish Classification Society (VOC), Helmut-Schmidt-University, Ha",,GfKl,2010.0,10.1007/978-3-642-01044-6,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a1ff48ac0f13e0a76c7c91fdeac7e89840364413,https://www.semanticscholar.org/paper/a1ff48ac0f13e0a76c7c91fdeac7e89840364413,Robotic Antarctic meteorite search: outcomes,"Automation of the search for and classification of Antarctic meteorites offers a unique case for early demonstration of robotics in a scenario analogous to geological exploratory missions to other planets and to the Earth's extremes. Moreover, the discovery of new meteorite samples is of great value because meteorites are the only significant source of extraterrestrial material available to scientists. In this paper we focus on the primary outcomes and technical lessons learned from the first field demonstration of autonomous search and in situ classification of Antarctic meteorites by a robot. Using a novel autonomous control architecture, specialized science sensing, combined manipulation and visual servoing, and Bayesian classification, the Nomad robot classified five indigenous meteorites during an expedition to the remote site of Elephant Moraine in January 2000. Nomad's expedition proved the rudiments of science autonomy and exemplified the merits of machine learning techniques for autonomous geological classification in real-world settings. On the other hand, the expedition showcased the difficulty in executing reliable robotic deployment of science sensors and a limited performance in the speed and coverage of autonomous search.",Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),2001.0,10.1109/ROBOT.2001.933270,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ead9b2621e17dddcfe87a81da4a7f01f284b6637,https://www.semanticscholar.org/paper/ead9b2621e17dddcfe87a81da4a7f01f284b6637,Predicting Customer Preferences in Non-experimental Retail Settings,"This thesis investigates the application of computational statistics and Machine Learning in consumer preference prediction, with specific reference to the challenges imposed by real world operational retail environments. Some retailers base their competitiveness on Machine Learning. For instance, Dunnhumby analyzes more than 400 million online consumer records for retailers, such as Tesco, to optimise business decisions. The experiments in this thesis investigate three main challenges commonly presented in operational scenarios that hinder the application of Machine Learning in retail environments: 1. The measurement of correlation of feature factors for Machine Learning in a noisy setting; 2. The exploration and exploitation balance for predicting purchase preferences on new products; 3. The model adaptability to the changing dynamics over time. A design of a distributed Machine Learning framework for building practical applications of consumer preference prediction is also presented. Experiment 1: Correlation between Contextual Information and Purchase Behaviours under a Non-experimental Retail Environment The first experiment applies statistical methodologies, namely odds ratio and Mantel-Haenszel method, to analyze contextual information in a retail business. More specifically, it investigates the correlation between customers’ recent online browsing behaviours on Boots.com and their in-store purchase behaviours at Boots’ retail stores nationally in a non-experimental noisy setting. Methodologies such as stratified analysis with K-means clustering are proposed to detect and eliminate confounding factors that affect the evaluation of the correlation. The dataset for this experiment, provided by Boots UK, is the first year of a 2-year anonymised real in-store and online purchase records data. It contains profiles of 10,217,972 unique consumers who are Advantage Card holders and 2,939 unique selected products under 10 different brands. Experiment 2: Resources Allocation of Exploration and Exploitation for New Products under Retail Constraints The second experiment provides a two-stage batch solution based on matrix factorization and binary integer programming to optimise the customer response rate to new products of a simulated group buying system. This experiment investigates how the balance between the exploration of new products and the exploitation of existing known model affects overall business gains through purchase prediction and recommendation. In this experiment, the products are new with no prior profile and the number of new products a retailer can recommend to each customer is limited. The effectiveness of one of the traditional experimental design techniques in improving the learning efficiency during the exploration process is evaluated. Experiment 3: Continuous Model Selection for a Changing Retail Environment The third experiment investigates, using root-mean-square error and mean average precision measures, the adaptability of data model for consumer purchase prediction in a non-static retail environment. In particular, it analyzes the prediction accuracy of data models with static parameters over time. A continuous model selection approach by using an automatic hyperparameter tuning technique, namely random search, is proposed and is evaluated. The results challenge the traditional assumption that a one-off initial model selection is sufficient. The dataset for this experiment is a 2-year anonymised real in-store and online purchase records data provided by Boots UK. System Design: A Distributed Machine Learning Framework with Automated Modeling This system design outlines the concept and system architecture. It also demonstrates scenarios of a distributed Machine Learning framework for (i) evaluating, comparing and deploying scalable learning algorithms, (ii) tuning hyperparameters of algorithms manually or automatically and (iii) evaluating model training status. The design has become the foundation of a popular open-source software project - PredictionIO. The project is followed by over 5000 data scientists and practitioners on Github. Contributions to Science The major contribution of this thesis is to offer robust research-based methodologies to handle prediction challenges in real world operational environment for retail businesses. Computational statistics and Machine Learning methodologies are proposed to 1) identify contextual factors that are relevant to consumer preference in noisy non-experimental setting; 2) determine the importance of exploration and exploitation for new products under real-world constraints; 3) adjust data model continuously to adapt to changes in retail environments. This thesis contributes to the existing literature in a number of ways. First, this research proposes a novel statistical method to isolates the influence of confounding factors in correlation analysis for consumer preference prediction. It is a topic that received little attention in empirical literature. Second, this research proves the existence of the correlation between consumer online browsing and in-store purchase behaviours in a real retail dataset. This is a significant finding for the retail industry to improve prediction accuracy in the future. Third, this research examines the influence of the balance between exploration and exploitation of new product profiles on maximising business gains. Forth, this research proves that random selection surprisingly outperforms D-optimal experimental design in some retail cases. Fifth, this research challenges the existing assumption that model selection is needed only once at the initial stage. It proves that prediction accuracy can be improved significantly by continuous model selection. Sixth, this thesis presents the implementation of a continuous model selection approach by using automatic hyperparameter tuning techniques. Finally, this thesis presents a design of a distributed system that can be used for building predictive retail applications.",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7f99a822d07eb0b809e4b1b9b456925eb9098a1b,https://www.semanticscholar.org/paper/7f99a822d07eb0b809e4b1b9b456925eb9098a1b,The best of both worlds: Highlighting the synergies of combining manual and automatic knowledge organization methods to improve information search and discovery in oil and gas enterprises,"Research suggests organizations across all sectors waste a significant amount of time looking for information and often fail to leverage the information they have. In response, many organizations have deployed some form of enterprise search to improve the ‘findability’ of information. Debates persist as to whether thesauri and manual indexing or automated machine learning techniques should be used to enhance discovery of information. In addition, the extent to which a Knowledge Organization System (KOS) enhances discoveries or indeed blinds us to new ones remains a moot point. The oil and gas industry is used as a case study using a representative organization. Drawing on prior research, a theoretical model is presented which aims to overcome the shortcomings of each approach. This synergistic model could help re-conceptualize the ‘manual’ versus ‘automatic’ debate in many enterprises, accommodating a broader range of information needs. This may enable enterprises to develop more effective information and knowledge management strategies and ease the tension between what is often perceived as mutually exclusive competing approaches. Certain aspects of the theoretical model may be transferable to other industries which is an area for further research. Introduction Business justification Oil and gas exploration seeks to identify and model hydrocarbon resources through geoscientific methods. Exploration wells can cost over $100Million in deep water (Blackman 2012) and typically have a 30% chance of success (Oil and Gas UK 2011). It is therefore critical that all relevant information is included. A review of surveys across all business sectors indicates 24% of a business professional’s time is spent looking for information (Chui et al. 2012, Doane 2010, Outsell 2005, Feldman et al. 2005, Lowe et al. 2004, Adkins 2003, Delphi 2002, Feldman and Sherman 2001). Much lower figures (9-14%) have been reported from observational studies in organizations (Robinson 2010, Majid et al. 2000) and much higher figures (40%) reported in the oil and gas industry (Hills 2014, Chum et al. 2011). A review of surveys indicates that 48% of organizations felt search was unsatisfactory (Norling & Boye 2013, Mindmeter 2011, Doane 2010, Microsoft & Accenture 2010, Feldman 2009, AIIM 2008, Feldman et al. 2005, Tonstad & Bjorge 2003). Executives indicate missed opportunities caused by failing to leverage information effectively in the oil and gas enterprise could represent as much as 22% of annual revenue (Oracle 2012). Acknowledging this significant opportunity cost, Rasmus (2013) proposes the Serendipity Economy, where discovery of information can produce major leaps in value that cannot be predicted. Exploiting and using information to make better decisions and improve performance are the goals for Knowledge Management (KM). Causal factors for enterprise search performance are numerous, including information silos, search expertise, governance and technology issues (White 2012, DeLone and McLean 2002). Data from search logs (Dale 2013, Romero 2013) and from practitioners (Andersen 2012, White 2012), indicate issues exist with enterprise search. One issue is the vocabulary problem where two people will not choose the same name for the same concept 80% of the time (Furnas et al. 1987), causing a mismatch between the search terms used and the information sought. This leads to challenges for enterprise search in finding precise information and recalling all relevant information. Another issue is the minimal use of faceted search and categories which rarely stimulate serendipitous encounters (Cleverley and Burnett 2015a). Despite major investments, dissatisfaction with enterprise search is widespread (White 2014, Norling and Boye 2013). The role and concomitant benefits of thesauri and manual indexing as well as automated machine learning techniques in information discovery is a source of ongoing debate. While this topic is well developed within the literature, it is far from being addressed conclusively. Collins and Porras (1997, pg. 10) describe the decision making process of visionary companies in terms of “the tyranny of the OR, the genius of the AND” when coping with contradictory forces. Is this a philosophy to apply to enterprise information (knowledge organization) with respect to manual and automated methods? Furthermore, Knowledge Organization Systems (KOS) themselves may act to reveal, or conversely obscure information discoveries. Given these issues, there is a need to assess how manual and automated Knowledge Organization (KO) techniques might support information search and discovery. This research therefore reconsiders these issues within the context of the oil and gas industry, with the explicit intention of developing a synergistic model which encompasses the main benefits of each approach into a ‘best of both worlds’ scenario. The following research questions were identified to fulfill the aim of the research, the rationale for their inclusion is presented in the literature review: Q1. To what extent can a thesaurus be enhanced through automated techniques? Q2. What is the value of auto-categorizing content that is already manually classified? Q3. To what extent can manual and automated KOS techniques be combined in a search user interface to stimulate serendipity? The next section reviews the literature with a focus on oil and gas, followed by the methodology. The results are presented with discussion to help the reader better understand the findings and limitations. The paper concludes with the presentation of a theoretical model, areas for further research and implications for theory and practice. Literature Review This section presents a critical review of the academic and practitioners literatures relevant to the research, guided by a conceptual model (Figure 1). It provides a background to the areas under research from both academic and practitioner standpoints, identifies how the literature has informed the research questions, presents gaps in the existing literature and emphasizes how this research addresses those gaps, and highlights areas of input into the theoretical model. Figure 1 – Conceptual model to guide the reader through the literature review. Knowledge Organization (KO) Knowledge Organization (KO) expresses and imposes a particular structure of knowledge (a ‘view of reality’) behind collections of information (Ohly 2012). This reality is socially constructed: what is reality for one group may not be for another (Berger and Luckman 1966). Hjorland (2008, pg. 86) offers a holistic definition of KO, encompassing the broader social division of mental labour, to the narrower intellectual activities, “..such as document description, indexing and classification performed in libraries, databases, archives etc. These activities are done by librarians, archivists, subject specialists as well as by computer algorithms”. Hjorland continues, “Library and Information Science (LIS) is the central discipline of KO in this narrow sense (although seriously challenged by, among other fields, computer science)”. This alludes to the tension that exists between Library and Computer Science. Recent evidence from organizations (Quaadgras and Beath 2011) contradicts the definition made by Hjorland that KO is the preserve of information specialists. Corporate library or information center functions have traditionally focused on the centralized manual indexing of information using KOS, with indexes under their stewardship (Heye 2003). The growth in digital information creation has led to the breakup of these gatekeeping services and the centralized manual indexing model to a more federated model of KO by the masses. Zeeman et al. (2011) found government libraries plan to deploy, “high-end thesaurus and ontology tools.. to work with structured and unstructured data for decision-making research”. This provides evidence of how some corporate librarian skills and services are changing. Classification and categorization can be achieved manually (by creator or mediator) or automatically through supervised/semi-supervised machine learning. The use of the terms classification and categorization have been (and continue to be) used interchangeably by practitioners and can cause conceptual misunderstandings. Simplifying, classification organizes information to mutually exclusive non overlapping classes, whilst categorization is more flexible, recognizing similarities across entities enabling information to be organized into one or more categories (Jacob 2004). Applying this to a ‘typical’ oil and gas document, classification may involve assigning an item to a single Document Type it is a ‘Well Proposal’. Whilst categorization may include assigning the document to be about oil and gas well ‘33/4b5’ and ‘light tight oil’. Classification and categorization typically need an existing set of classes/categories like a taxonomy or authority list, whilst ‘tagging’ is also used to refer to the process of adding terms which may include those from outside controlled vocabularies to emphasize prominence (Hedden 2013). Knowledge Organization Systems (KOS) Hodge (2000, pg.1) defines Knowledge Organization Systems (KOS) as including, “classification and categorization schemes that organize materials at a general level, subject headings... and authority files that control variant versions of key information such as geographic names and personal names. Knowledge organization systems also include highly structured vocabularies, such as thesauri, and less traditional schemes, such as semantic networks and ontologies.” This definition is adopted for the research study, including automatically generated associative thesauri that involve no manual input. Zeng (2008) arranges KOS types in order of increasing sophistication, by both structure and use cases (eliminating ambiguity, controlling synonyms, establishing relationships and presenting properties). A corporate ",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1bd64a7541e8d34f0d7782f0e786b2ffd78e55b7,https://www.semanticscholar.org/paper/1bd64a7541e8d34f0d7782f0e786b2ffd78e55b7,Interacting in various application domains,"eLearning and Education.- Arab Children's Reading Preference for Different Online Fonts.- Adaptation Decisions and Profiles Exchange among Open Learning Management Systems Based on Agent Negotiations and Machine Learning Techniques.- Accessing e-Learning Systems via Screen Reader: An Example.- Using Tablet PCs and Pen-Based Technologies to Support Engineering Education.- Optimal Affective Conditions for Subconscious Learning in a 3D Intelligent Tutoring System.- Computer-Based Learning to Improve Breast Cancer Detection Skills.- Virtual Classroom and Communicability: Empathy and Interaction for All.- Communicability for Virtual Learning: Evaluation.- Attention and Motivation in Hypermedia Systems.- A Web-Based, Interactive Annotation Editor for the eCampus Development Environment for SCORM Compliant E-Learning Modules.- An Innovative Way of Understanding Learning Processes: Eye Tracking.- A Set of Rules and Strategies for UNSAM Virtual Campus.- HCI Professional Involvement in k-12 Education: On Target or Missing the Mark?.- A Language Learning System Utilizing RFID Technology for Total Physical Response Activities.- Promoting Metacognition in Immersive Cultural Learning Environments.- The Application of the Flexilevel Approach for the Assessment of Computer Science Undergraduates.- Development of Ubiquitous On-Demand Study Support Environment for Nursing Students.- The Effects of Prior Knowledge on the Use of Adaptive Hypermedia Learning Systems.- Supporting Learners in Adaptive Learning Environments through the Enhancement of the Student Model.- The Concept of IMPRESSION: An Interactive Instruction System and Its Practice for Real-Time Distance Lessons between U.S. and Japan.- Improving Children's Writing Ability.- From Paper to Module - An Integrated Environment for Generating SCORM Compliant Moodle Courses Out of Text and Multimedia Elements.- Development of a Simulator of Abacus: Ancient Analog Calculator on a Mobile Phone as a Teaching Material.- A Proposal for a Framework for an e-Alumni Program Using SNS.- Supporting End-User Development of Personalized Mobile Learning Tools.- Didactic Models as Design Representations.- Interactive Learning Panels.- WebELS: A Content-Centered E-Learning Platform for Postgraduate Education in Engineering.- A Pen-Based Teaching System for Children and Its Usability Evaluation.- Development of a Visualised Sound Simulation Environment: An e-Approach to a Constructivist Way of Learning.- Games and Entertainment.- Causal Links of Presence.- Games Design Principles for Improving Social Web Applications.- A Multiple-Level 3D-LEGO Game in Augmented Reality for Improving Spatial Ability.- An Online Survey System on Computer Game Enjoyment and Personality.- Playability Testing of Web-Based Sport Games with Older Children and Teenagers.- Exploring the Elements and Design Criteria of Massively-Multiplayer Online Role-Playing Game (MMORPG) Interfaces.- Healthcare Game Design: Behavioral Modeling of Serious Gaming Design for Children with Chronic Diseases.- Analyzing Human Behaviors in an Interactive Art Installation.- The Effects of Quest Types and Gaming Motivations on Players' Knowledge Acquisitions in an Online Role-Playing Game Environment.- Self-movement Feeling Generation in Sports Watching with Screen Movement via Pan-Tilt Steerable Projector.- Design of Interactive Emotional Sound Edutainment System.- Understanding Online Game Addiction: Connection between Presence and Flow.- The Experience of Presence in 3D Web Environment: An Analysis of Korean Second Life.- Influence of Real-World Ten-Pin Bowling Experience on Performance during First-Time Nintendo Wii Bowling Practice.- Emotionally Adapted Games - An Example of a First Person Shooter.- DiamondTheater: A System for Reproducing Theater and Supporting Creative Activities.- Work, Collaboration and Business.- New Health Information Systems (HIS) Quality-in-Use Model Based on the GQM Approach and HCI Principles.- An Information Visualization Approach to Hospital Shifts Scheduling.- Designed to Fit: Challenges of Interaction Design for Clothes Fitting Room Technologies.- Usability for Poll Workers: A Voting System Usability Test Protocol.- CAD and Communicability: A System That Improves the Human-Computer Interaction.- A Novel Visualization Tool for Evaluating Medication Side-Effects in Multi-drug Regimens.- Design of a Web Intervention to Change Youth Smoking Habits.- Smart Makeup Mirror: Computer-Augmented Mirror to Aid Makeup Application.- Studying Reactive, Risky, Complex, Long-Spanning, and Collaborative Work: The Case of IT Service Delivery.- Human Computer Interaction in Virtual Standardized Patient Systems.- Towards Standardized Pen-Based Annotation of Breast Cancer Findings.- ImproV: A System for Improvisational Construction of Video Processing Flow.- E-Assessment: A Suitable Alternative for Measuring Competences?.- Green Advocate in E-Commerce.- Gesture-Based Sharing of Documents in Face-to-Face Meetings.- Developing, Deploying and Assessing Usage of a Movie Archive System among Students of Film Studies.- Using Activity Descriptions to Generate User Interfaces for ERP Software.- Developing a Nomenclature for EMR Errors.- Mapping for Multi-source Visualization: Scientific Information Retrieval Service (SIRS).- Client-Side Visualization of Internet Forums for Information Retrieval.- Social-Technical Tools for Collaborative Sensemaking and Sketching.- Developing Some User Interfaces of TV under Enormous Channels Environment.- Electronic Glassboard - Conception and Implementation of an Interactive Tele-presence Application.- A New Automatic Teller Machine (ATM) Proposal through the Analysis of ATMs of Three Banks.- Advanced Applications.- Designing Usable Bio-information Architectures.- Run-Time Adaptation of a Universal User Interface for Ambient Intelligent Production Environments.- Heuristic Evaluation of Mission-Critical Software Using a Large Team.- Interface Development for Early Notification Warning System: Full Windshield Head-Up Display Case Study.- Reflections on the Interdisciplinary Collaborative Design of Mapping the Universe.- Distilling Support Opportunities to Improve Urban Search and Rescue Missions.- A New Approach to Design an Interactive System for Molecular Analysis.- The Differences of Aviation Human Factors between Individualism and Collectivism Culture.- Web-Based Training System for Improving Aviation Maintenance Performance.- Allocating Human-System Interfaces Functions by Levels of Automation in an Advanced Control Room.- Development of an Expert System as a User Interface for an RFID Application.- Developing a Validation Methodology for Educational Driving Simulators and a Case Study.- Developing a Usable Mobile Flight Case Learning System in Air Traffic Control Miscommunications.",,2009.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e2319566d80c45d903bc194140cc6354c677fea9,https://www.semanticscholar.org/paper/e2319566d80c45d903bc194140cc6354c677fea9,Attentional modulation of spread of activation,,,2012.0,10.1007/springerreference_226271,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
eae25a870a641a25951609ba3e3a57244b96cf9b,https://www.semanticscholar.org/paper/eae25a870a641a25951609ba3e3a57244b96cf9b,Teaching and Learning High School Physics in Kenyan Classrooms Using Analogies,"This study provides insights into the nature of the analogies deployed by Kenyan physics teachers and generated by students in class. The analogies looked at (both teacher- and student-generated) were largely environmental (drawn from students’ socio-cultural environment), anthropomorphic (life and human characteristics ascribed to analogues), and to a limited extent, scientific (analogue and target are science concepts). In some cases, anthropomorphic analogies proved problematic for students, sometimes resulting in serious misconceptions. Good analogy use is based on clear identification of matching and non-matching features of the analogue-target structure. Several models, including the General Model for Analogy Teaching (GMAT), Teaching with Analogy (TWA) and Working with Analogies (WWA) are discussed in this paper, with a view to providing a lens through which analogies can be understood. To transform students’ understanding from cultural belief systems to the science system of thinking, while respecting their socio-cultural backgrounds, can be a daunting task. Where it proves problematic for the students to ‘decamp’ from indigenous ways of reasoning, then collateral learning may be considered.RésuméUne analogie comprend deux composantes : l’analogue, c’est-à-dire la notion familière ou connue, et la cible, c’est-à-dire le concept nouveau, non encore familier. Elle est un outil fort utilisé en enseignement des matières scientifiques. Nécessairement, les analogues et les cibles sont tirés de domaines différents; seules les similarités sont exploitées pour entraîner une compréhension de la cible grâce au repérage, dans l’analogue, des caractéristiques ou attributs ayant des correspondants dans la cible. En d’autres termes, on se sert de la compréhension de l’analogue pour expliquer la cible.Les analogies ont des caractéristiques différentes selon la personne qui les a formulé et le contexte dans lequel elles sont utilisées. Le cas illustré dans cet article vise à éclairer la nature des analogies que les enseignants de physique kényens utilisent pour expliquer les concepts de la physique à leurs élèves de niveau deux (2) (≪ Form 2 ≫, équivalent de la troisième secondaire). Il vaut la peine de souligner que les écoles du Kenya suivent un curriculum commun centralisé et qu’elles ont l’anglais comme langue d’enseignement. La langue d’instruction est un facteur clé qui influence la nature des analogies utilisées par les enseignants et les étudiants.Les données ont été recueillies grâce à l’observation de trois classes de physique de niveau deux (2) sur une période de 14 semaines. De plus, les manuels, les programmes et les annotations des enseignants ont été recueillis et analysés. On a également réalisé des entrevues informelles avec les enseignants et avec certains étudiants ou groupes d’étudiants choisis au hasard. Au cours de la période d’observation, trois sujets principaux ont été enseignés : l’électricité, la cinématique et les machines. La plupart des analogies enregistrées concernaient l’électricité. Onze d’entre elles sont analysées dans cet article selon les modèles suivants : le GMAT de Zeitoun (Model for Analogical Teaching) (1984), le TWA de Glynn (Teaching with Analogies) (1991) et le WWA de Nashon (Working with Analogies) (2000).Les analogies (générées soit par les enseignants, soit par les étudiants) étaient en grande partie environnementales (dérivées du milieu socioculturel de l’étudiant), anthropomorphiques (attribuant à l’analogue des caractéristiques humaines ou biólogiques) et, dans une faible mesure, scientifiques (où l’analogue et la cible sont tous deux des concepts scientifiques). Dans certains cas, les analogies anthropomorphiques se sont avérées difficiles pour les étudiants et ont parfois conduit à de graves erreurs conceptuelles. Dans cet article, les analogies anthropomorphiques sont considérées comme environnementales, car elles restent liées aux contextes locaux et culturels. De telles analogies ne devraient être utilisées que dans le cas où les étudiants n’ont guère une connaissance scientifique préalable suffisante pour qu’on puisse en dériver des analogues. Si les étudiants possèdent déjà cette connaissance, les enseignants devraient s’efforcer d’utiliser ou d’exploiter des analogies scientifiques. On note un manque de systématicité évident dans la construction et l’exposition des analogies chez les enseignants en raison de leur incapacité d’exploiter les modèles structurels théoriques comme le GMAT, le TWA ou le WWA. Peu d’enseignants se sont servis d’une stratégie adéquate pour effectuer le passage de l’anthropomorphique au scientifique. L’auteur recommande que l’analogue et la cible soient dé-liés et que la signification de la cible serve à consolider la solution du problème. Une telle dissociation évitera que les étudiants ne confondent le sens de l’analogue et celui de la cible.",,2003.0,10.1080/14926150309556572,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
81be6cbe3c26a2e0070591fd591bcb9b4b4a4b1f,https://www.semanticscholar.org/paper/81be6cbe3c26a2e0070591fd591bcb9b4b4a4b1f,Bayesian Modeling for Optimization and Control in Robotics,"Robotics has the potential to be one of the most revolutionary technologies in human history. The impact of cheap and 
potentially limitless manpower could have a profound influence on our everyday life and overall onto our society. As 
envisioned by Iain M. Banks, Asimov and many other science fictions writers, the effects of robotics on our society might 
lead to the disappearance of physical labor and a generalized increase of the quality of life. However, the large-scale 
deployment of robots in our society is still far from reality, except perhaps in a few niche markets such as manufacturing. 
One reason for this limited deployment of robots is that, despite the tremendous advances in the capabilities of the 
robotic hardware, a similar advance on the control software is still lacking. The use of robots in our everyday life is still 
hindered by the necessary complexity to manually design and tune the controllers used to execute tasks. As a result, 
the deployment of robots often requires lengthy and extensive validations based on human expert knowledge, which 
limit their adaptation capabilities and their widespread diffusion. In the future, in order to truly achieve an ubiquitous 
robotization of our society, it is necessary to reduce the complexity of deploying new robots in new environments and 
tasks. 
The goal of this dissertation is to provide automatic tools based on Machine Learning techniques to simplify and 
streamline the design of controllers for new tasks. In particular, we here argue that Bayesian modeling is an important tool 
for automatically learning models from raw data and properly capture the uncertainty of the such models. Automatically 
learning models however requires the definition of appropriate features used as input for the model. Hence, we present 
an approach that extend traditional Gaussian process models by jointly learning an appropriate feature representation 
and the subsequent model. By doing so, we can strongly guide the features representation to be useful for the subsequent 
prediction task. 
A first robotics application where the use of Bayesian modeling is beneficial is the accurate learning of complex dynamics models. For highly non-linear robotic systems, such as in presence of contacts, the use of analytical system 
identification techniques can be challenging and time-consuming, or even intractable. We introduce a new approach for 
learning inverse dynamics models exploiting artificial tactile sensors. This approach allows to recognize and compensate 
for the presence of unknown contacts, without requiring a spatial calibration of the tactile sensors. We demonstrate 
on the humanoid robot iCub that our approach outperforms state-of-the-art analytical models, and when employed in 
control tasks significantly improves the tracking accuracy. 
A second robotics application of Bayesian modeling is automatic black-box optimization of the parameters of a controller. When the dynamics of a system cannot be modeled (either out of complexity or due to the lack of a full state 
representation), it is still possible to solve a task by adapting an existing controller. The approach used in this thesis is 
Bayesian optimization, which allows to automatically optimize the parameters of the controller for a specific task. We 
evaluate and compare the performance of Bayesian optimization on a gait optimization task on the dynamic bipedal 
walker Fox. Our experiments highlight the benefit of this approach by reducing the parameters tuning time from weeks 
to a single day. 
In many robotic application, it is however not possible to always define a single straightforward desired objective. 
More often, multiple conflicting objectives are desirable at the same time, and thus the designer needs to take a decision 
about the desired trade-off between such objectives (e.g., velocity vs. energy consumption). One framework that is 
useful to assist in this decision making is the multi-objective optimization framework, and in particular the definition of 
Pareto optimality. We propose a novel framework that leverages the use of Bayesian modeling to improve the quality 
of traditional multi-objective optimization approaches, even in low-data regimes. By removing the misleading effects 
of stochastic noise, the designer is presented with an accurate and continuous Pareto front from which to choose the 
desired trade-off. Additionally, our framework allows the seamless introduction of multiple robustness metrics which can 
be considered during the design phase. These contributions allow an unprecedented support to the design process of 
complex robotic systems in presence of multiple objective, and in particular with regards to robustness. 
The overall work in this thesis successfully demonstrates on real robots that the complexity of deploying robots to solve 
new tasks can be greatly reduced trough automatic learning techniques. We believe this is a first step towards a future 
where robots can be used outside of closely supervised environments, and where a newly deployed robot could quickly 
and automatically adapt to accomplish the desired tasks.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b70174998d8a0e49c8cb7b6bb9665e8139c3e853,https://www.semanticscholar.org/paper/b70174998d8a0e49c8cb7b6bb9665e8139c3e853,What do Support Analysts Know About Their Customers? On the Study and Prediction of Support Ticket Escalations in Large Software Organizations,"Understanding and keeping the customer happy is a central tenet of requirements engineering. Strategies to gather, analyze, and negotiate requirements are complemented by efforts to manage customer input after products have been deployed. For the latter, support tickets are key in allowing customers to submit their issues, bug reports, and feature requests. Whenever insufficient attention is given to support issues, however, their escalation to management is time-consuming and expensive, especially for large organizations managing hundreds of customers and thousands of support tickets. Our work provides a step towards simplifying the job of support analysts and managers, particularly in predicting the risk of escalating support tickets. In a field study at our large industrial partner, IBM, we used a design science methodology to characterize the support process and data available to IBM analysts in managing escalations. Through iterative cycles of design and evaluation, we translated our understanding of support analysts' expert knowledge of their customers into features of a support ticket model to be implemented into a Machine Learning model to predict support ticket escalations. We trained and evaluated our Machine Learning model on over 2.5 million support tickets and 10,000 escalations, obtaining a recall of 79.9% and an 80.8% reduction in the workload for support analysts looking to identify support tickets at risk of escalation. Further on-site evaluations, through a prototype tool we developed to implement our Machine Learning techniques in practice, showed more efficient weekly support-ticket-management meetings. The features we developed in the Support Ticket Model are designed to serve as a starting place for organizations interested in implementing our model to predict support ticket escalations, and for future researchers to build on to advance research in escalation prediction.",2017 IEEE 25th International Requirements Engineering Conference (RE),2017.0,10.1109/RE.2017.61,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fc7b130ae9280808b3f11615782482d570d77874,https://www.semanticscholar.org/paper/fc7b130ae9280808b3f11615782482d570d77874,From Texts to Networks,"This work is part of the Dynamic Networks project in the center for Computational Analysis of Social and Organizational Systems (CASOS) of the School of Computer Science (SCS) at Carnegie Mellon University (CMU). Support was provided, in part, by the National Science Foundation (NSF) Integrative Graduate Education and Research Traineeship (IGERT) program, 9972762, the Army Research Lab, and the Army Research Institute .The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Researc Lab, the Army Research Institute, the National Science Foundation, or the U.S. government. Development of Computational Solutions  Utilize machinery from Machine Learning and Artificial Intelligence  Deploy and develop supervised and semisupervised sequential stochastic learning techniques in order to train classifiers and build models that generalize to new data  Construct a classifier h that for every sequence of (x, y) (joint probability) (where x = words per sequence and y = corresponding category) or (x|y) (conditional probability) predicts a sequence y = h (x) for any sequence of x, incl. new and unseen data  We work with Generative (aka discriminative) models: P(x,y), such as Hidden Markov Model (HMM), and Conditional models: P(y|x), such as Maximum Entropy Markov Models (MEMM) and Conditional Random Fields (CRF)",,2008.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ea89d5ab69627a9361a340d43866cc0950a50fa1,https://www.semanticscholar.org/paper/ea89d5ab69627a9361a340d43866cc0950a50fa1,Distributed Learning Algorithms for Sensor Networks,"Wireless sensor networks have received significant attention in the last decade owing to their widespread use not only in monitoring the physical world but also in surveillance. The energy and communication constraints of sensor nodes, coupled with distributed processing of sensed signals, lead to challenges in developing effective methods to perform desired inference tasks such as object detection or classification. Further, the lack of well-calibrated sensors is a major obstacle for the rapid deployment of sensor networks. This dissertation develops gossip-based learning algorithms for distributed signal processing in sensor networks. In gossip-based algorithms, sensor nodes share information with local neighbors to converge upon common knowledge about the sensed environment. Gossip-based methods allow for manageable communication among energy-constrained nodes and also accommodate changing network communication topologies. We consider three related problems and develop gossip-based processing solutions. We first consider the problem of joint signature estimation and node calibration using distributed measurements over a large-scale sensor network. We develop a new Distributed Signature Learning and Node Calibration algorithm, called D-SLANC, which estimates the signature of a commonly-sensed source signal and simultaneously estimates calibration parameters local to each sensor node. The approach we take is ii to model the sensor network as a connected graph and make use of the gossip-based distributed consensus to update the estimates at each iteration of the algorithm. We prove convergence of the algorithm to the centralized data pooling solution. We also compare its performance with the Cramér-Rao bound (CRB), and study the scaling performance of both the CRB and the D-SLANC algorithm. Secondly, we develop a gossip-based algorithm for distributed `1-optimization in a large-scale sensor network setting. Specifically, we consider sensor nodes which can measure only a part of the entire measurement vector. We formulate the `1optimization problem as quadratic optimization and develop a distributed, gossipbased algorithm using the projected-gradient approach. We analyze the performance of the proposed algorithm using synthetic data and compare it with a standard `1 solver. Third, we consider the problem of distributed classifier learning in a large-scale sensor network setting. We adopt a machine learning approach to the problem and develop a distributed, gossip-based algorithm that learns the optimal (large-margin) hyperplane separating the two classes, using the projected-gradient approach. We illustrate the performance of the proposed algorithm using both synthetic and realworld datasets.",,2010.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
667d1da01344286cf6f0f8104d1d6ddc2a61eb31,https://www.semanticscholar.org/paper/667d1da01344286cf6f0f8104d1d6ddc2a61eb31,Handbook of sustainable engineering,,,2013.0,10.1007/978-1-4020-8939-8,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
aeaf6b914f082be5b6a153bfa244bc852a8f267d,https://www.semanticscholar.org/paper/aeaf6b914f082be5b6a153bfa244bc852a8f267d,Ushahidi: a crisis mapping system,"Computing plays a significant role in our communities from the local to the global. Computing greatly benefits social causes such as community organization, education, humanitarian relief and information access. However, many students proceed through an undergraduate degree learning of the impact that computing has in business and science, but are not made aware of the positive social impact that computing has and will continue to have.
 Students are taught about the impact of computing for social good as part of the author's senior Software Engineering course at MacEwan University in Edmonton, AB, Canada. In particular, the humanitarian open-source software project called Ushahidi [1] (http://www.ushahidi.com) as the platform for group projects since 2011.
 Ushahidi is a crisis-mapping software project that was initially developed during the 2008 Kenyan elections to allow citizens to communicate issues (e.g., violence, intimidation, and voting irregularities) that the state media was not reporting. Ushahidi has continued as an actively developed open-source project and currently has thousands of deployments worldwide including over 3000 in the U.S. alone. It has been used to help communities during numerous crises such as the Haitian earthquake, Snowmaggedon and in Syria and other ""Arab Spring"" countries to report violence.
 Last year the IPython notebook (http://www.ipython.org) as a second option for my students. Categorizing IPython as a social good project is debatable, it has broadened the experience beyond a single open-source application.
 Using these projects to teach about computing's social impact over the last three years is of interest for three reasons:
 • Open-source - Use of popular open-source humanitarian software is a compelling option for student projects so that the students' work can have a broader impact and to expose students to real-world development communities. Open-source communities also vary in their tools and collaboration style, experiences in working with the IPython and Ushahidi communities are shared.
 • Breadth of projects -- Several example student projects and the feedback students have given as to what they enjoyed most and least about their projects are discussed.
 • Effective Practices -- Use of a common virtual machine to scaffold the projects so students can get software installed and running quickly, within an hour or two, and avoid configuration problems.
 In each year of the last three years the author tried different approaches in having the students work on projects. He also used different pedagogical approaches, incorporating both individual and group projects, allowed students to select their own projects versus assigning them and tried different project scopes, from very specific tasks (i.e., fixing bugs) to broader, open problems such as adding new features. Finally, he mentored the students himself as well as worked with external mentors.
 In 2013, the author presented a poster titled ""Teaching Software Engineering with a Humanitarian Open-Source Software Project"" at SIGCSE. Following that poster he collaborated with the Foss2Serve project (http://foss2serve.org) and mentored faculty members at other universities in incorporating humanitarian FOSS projects in their curriculum.",CSOC,2015.0,10.1145/2809957.2809969,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9b98e6ed05c2a32bbaad1b6c3245c2e7d7a38b1d,https://www.semanticscholar.org/paper/9b98e6ed05c2a32bbaad1b6c3245c2e7d7a38b1d,A Methodology for Developing Environmental Information Systems with Software Agents,,,2009.0,10.1007/978-3-7643-8900-0_6,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5caa35987b5df145a5bdc8dd4766dbeff952ebe2,https://www.semanticscholar.org/paper/5caa35987b5df145a5bdc8dd4766dbeff952ebe2,Research on Application and Development of Financial Big Data,"This paper discuss the basic thinking, methods and tools of software engineering, masters the financial business knowledge, the analysis and design theory and methods of financial information systems, and has the ability to analyze, design, implement and maintain financial information systems, and can be used in financial applications. IT companies such as development companies and financial information system providers engage in the analysis, design and implementation of software, or engage in the analysis, design, implementation, maintenance and management of financial information systems in the IT departments of various financial institutions such as banks, securities and insurance. A financial information-based composite software talent with a solid professional foundation, broad knowledge, and the ability to adapt to the future development of information technology. Specifically, it is reflected in four aspects: knowledge system, professional skills, project experience and comprehensive quality: Introduction On July 1, 2017, the General Office of the State Council issued the “Opinions on Strengthening the Service and Supervision of Market Subjects by Using Big Data”; on July 4, the State Council issued the “Guiding Opinions on Actively Promoting the “Internet+” Action” On September 5, the State Council issued the ""Outline for the Promotion of Big Data Development."" The intensive introduction of these heavy documents marks the official establishment of China's big data strategic deployment and top-level design. Benefiting from the rapid expansion of the big data market, the demand for related IT support has exploded. Among them, enterprises that provide big data infrastructure, big data software technology services, and industry big data content consulting services have brought unprecedented Customer group. IDC predicts that by 2020, the company's expenditure based on big data computing and analysis platform will exceed 500 billion US dollars, and the compound growth rate will reach 34.1% in the next 5 years; in the next 3 to 5 years, China needs 1.8 million data talents, but currently only About 300,000 people. At the same time, China's colleges and universities in cloud computing, data science and other majors are still in their infancy, and the talents cultivated each year are far from meeting the needs of the industry. Therefore, it is imperative to open a big data major and accelerate the cultivation of talents. The financial industry is the industry that relies most on data and is the easiest to realize data. In recent years, emerging financial institutions such as consumer loans and P2P are the products of the combination of big data technology and finance. At present, the demand for big data talents in finance is extremely strong in China. Only Internet finance is one year, and the growth rate is 3-5 times per year. It is generally believed that there will be a gap of 1 million talents in Internet finance, and the most lacking is big data risk control talents, including data mining and statistical modeling talents from primary to advanced. Core and Featured Courses Cloud Computing and Introduction to Big Data As an introductory course in the direction of this major, this course introduces students to the concepts, technologies and applications related to cloud computing and big data, and enables students to establish a preliminary understanding of the relevant knowledge, technology and development prospects of the profession, as a guide for the follow-up course. Distributed Computing Framework Foundation As the foundation and core technology course of this major, this course introduces students to the basic concepts, installation and configuration of the Hadoop distributed computing system, distributed programming model (Map/Reduce), distributed file system (HDFS), and related scheduling. , monitoring and maintenance tools enable students to build a basic understanding of distributed computing systems, master the primary distributed application design and implementation methods, and lay the theoretical and practical foundation for subsequent in-depth courses. Distributed Database Management and Development As a core technical course in this major, this course introduces students to the basic concepts of distributed databases, installation and configuration, management and maintenance, data access and development. The course focuses on NoSQL databases such as HBase, MongoDB, Redis, etc., and describes their use and development in a distributed environment. To enable students to establish a basic understanding of distributed databases, master the primary distributed database application system design and development methods, and lay the theoretical and practical foundation for the subsequent in-depth courses. Distributed Computing Framework Component Technology As a core advanced course in this major, this course introduces students to mainstream components on the Hadoop distributed computing platform, including Hive, Pig, Sqoop, Flume, Kafka, Zookeeper and more. Enable students to have a complete Hadoop ecosystem-based design and implementation of big data applications. Real-time Calculation and Memory Calculation As a core advanced course in this major, this course introduces students to high-performance distributed computing frameworks, including Storm and Spark, as a more powerful alternative to the Hadoop framework. Data Visualization Technology As an elective course in this major, this course introduces students to the basics of data visualization and the design and use of platforms and development tools, including Excel, Reporting Services, Chart.js, D3.js, Tableau, etc. Through this course, students will be able to present the results of big data processing in an efficient, flexible and friendly manner. Data Statistics and Analysis As a core advanced course in this major, this course introduces students to statistical analysis techniques based on Python and R. Including data file editing and finishing, basic statistical analysis, parameter estimation and hypothesis testing, non-parametric testing, analysis of variance, correlation analysis, regression analysis, cluster analysis, discriminant analysis, factor analysis, correspondence analysis, reliability analysis, survival Analysis, time series analysis, and the drawing of statistical graphs enable students to master the processing and analysis methods of typical industry business data. Knowledge System Mathematical basis: Including calculus, linear algebra, probability statistics, numerical analysis, etc. IT foundation: Including operating systems, networks, databases, software engineering, programming techniques, data structures and algorithms, etc. Knowledge base in the financial sector. Including international finance, marketing, insurance, securities investment, etc. Professional Skills Database system management and development: MySQL, MongoDB, Redis, HBase, etc. Big Data Application Development Language: Java as the core, supplemented by Python, Scala, R, etc. Construction, configuration, development and deployment of big data processing frameworks: Hadoop, Storm, Spark, etc. Use of data analysis and presentation tools: reporting tools, D3.js, etc. Project Experience Familiar with enterprise software project life cycle, development process, specification, etc. Understand and implement software quality requirements: performance, security, scalability, maintainability, reliability, etc. Understand the financial industry: industry background, business model, market characteristics, and how data and IT systems are used in the financial industry Comprehensive Quality Good professional basic qualities: document writing, presentation reporting, business communication, etc. Strong learning ability and study habits, has a certain degree of microinnovation, data awareness Course Settings Table The professional competence-course structure derivation process mainly includes two stages: “computation ability theme” and “capability-curriculum structure transformation”. The main process of the first phase of the ""computational power theme"" is as follows: Figure 1. The data analysis process of ""ability topic calculation. Hadoop Big Data Integrated Experiment System This experimental system is designed to provide students with a complete set of Hadoop and its environment, design, development, monitoring, maintenance tools, software and services. With this experimental system, the experimental and training environment requirements of the core technology courses of this major can be met. This experimental system is divided into two major components: A virtual lab environment for students to learn big data. The environment is carried out by means of the aforementioned virtualized desktop teaching system, and the network administrator configures the big data learning virtual machine in advance for the students to use. The real environment for research or large-scale case presentations. This environment is carried out through several servers. Main Function A. Basic platform: The basic platform for big data storage and processing, which can realize the storage and management of massive data, support common components of platforms such as Hive, Impala, Pig, Spark, and Yarn, and provide support for data analysis services on the platform. These common components increase the ease of use of platform data, making data manipulation and data analysis easier to use, saving labor and reducing labor time. B. Data integration: support the unified storage of massive structured data, semi-structured data, and unstructured data, deepen the expansion of enterprise intelligence and service capabilities, and improve the decision-making level of enterprises. We can use enterprise-level data ETL tools or open source ETL tools. For example, Flume, Sqoop, Kafka, etc., integrate externally structured, semi-structured and unstructured data into big data platforms. Through this pla","DEStech Transactions on Social Science, Education and Human Science",2019.0,10.12783/dtssehs/aems2018/28012,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5ca9446ca8a6dc59b682d0dca76e1861ca241df9,https://www.semanticscholar.org/paper/5ca9446ca8a6dc59b682d0dca76e1861ca241df9,Diseño e implementación de una herramienta de análisis de datos y automatización de procesos para mercados financieros,"In this final degree project, a research about algorithmic trading as well as the learning of the data management is presented. A tool has been developed for that purpose and a data base has been deployed. Both are going to work together in the study of the financial markets. 
To begin with, the planning of the architecture and structure for the data base has to be developed and the choice of what kind of data base fit better for the project has to be made. Furthermore, we needed to establish a connection between the financial markets data and our tool. 
Secondly, the manage and the store of the information. The first assignment was the obtaining of the information. The creation of algorithms whereby takes the markets information in order to save them in our data base. In addition, the use of this information to model and calculate financial ratios to keep also in our data base. 
Finally, the data science. With this amount of data is possible create algorithms which can find out patterns to save them in the data base. When the live data get into our tool, the implement create the pattern of this data, store it and compare with historical patterns. If there is a coincidence, the tool knows what happened in the past and we try to forecast the fluctuate. 
To sum up, the object of this project is to study one of the many ways of the research of the financial market. The machine learning, the data science and automatic trading is for sure the present and the future of the investment.---ABSTRACT---En este proyecto de fin de grado, se realizara una investigacion sobre el trading algoritmico, asi como de la gestion de la informacion. Se desarrollara una herramienta para ese proposito y se desplegara una base de datos. Ambos trabajaran conjuntamente en el estudio de los mercados financieros. 
En primer lugar, se describira la planificacion de la arquitectura y estructura de la base de datos, previo estudio del tipo de base de datos que mejor se ajusta al proyecto. Ademas, se establecera una conexion entre los datos de los mercados financieros y nuestra herramienta. 
En segundo lugar, la gestion y almacenamiento de la informacion. Se desarrollaran implementaciones que recopilan toda esta informacion de los mercados y la modelan para ademas calcular ratios financieros. 
Por ultimo, la mineria de datos y el aprendizaje automatico. Con esta cantidad de datos es posible crear algoritmos que puedan encontrar patrones para posteriormente almacenarlos. Cuando los datos en vivo entran en nuestra herramienta, el aplicativo crea el patron de estos datos, lo almacena y compara con patrones historicos. Si existe una coincidencia, la herramienta consulta lo sucedido y tratamos de predecir la fluctuacion. 
En resumen, el objetivo de este proyecto es estudiar una de las muchas formas de investigacion en el sector. El aprendizaje automatico, la ciencia de los datos y el trading algoritmico es el presente y el futuro de la inversion.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0ef6ef0cdd88bee0dbfad69f0632f165427fea3f,https://www.semanticscholar.org/paper/0ef6ef0cdd88bee0dbfad69f0632f165427fea3f,Submission in Response to NSF CI 2030 Request for Information PAGE 1 DATE AND TIME : 2017-04-05 09 : 19 : 23 REFERENCE NO : 224,"maximum ~200 words). Education and the Learning Sciences are now at the point of becoming dataand algorithm-intensive in the methodologies used to conduct research. In fact, education research is transitioning into a “Fourth Paradigm” discipline (Hey et al. 2009). The intersection of education with data science techniques has been a cornerstone of growth in research on learning over the last decade, and has translated into an unprecedented amount of data that needs to be securely collected, manipulated, and made available for analysis. Areas such as Multimodal Learning Analytics are producing dense streams of audio, video, psychophysiological and environmental data, and clickstreams that need to be managed by teams of stakeholders through infrastructure that guarantees its security and reliability, as well as student privacy. The application of analytic techniques to learning infrastructures has profound implications for how we both understand (at a theoretical level) and operationalize (at an institutional level) learning progression on all educational in professional learning. Therefore, Learning Analytics researchers require scalable infrastructure to pose the new kinds of questions that until recently were either unimaginable or impractical to investigate. Question 1 Research Challenge(s) (maximum ~1200 words): Describe current or emerging science or engineering research challenge(s), providing context in terms of recent research activities and standing questions in the field. Education and the Learning Sciences are now at the point of becoming dataand algorithm-intensive in the methodologies used to conduct Submission in Response to NSF CI 2030 Request for Information PAGE 2 DATE AND TIME: 2017-04-05 09:19:23 REFERENCE NO: 224 research. Education research is transitioning into a “Fourth Paradigm” discipline (Hey et al. 2009), and the intersection of education with data science techniques has been a cornerstone of growth over the last decade. The application of such techniques to learning infrastructures has profound implications for how we both understand (at a theoretical level) and operationalize (at an institutional level) learning progression on all educational in professional learning. This can be seen through the rapid growth of student success modeling systems, now a common feature in learning platforms (e.g. Blackboard, Canvas, Moodle) and educational administrative settings (e.g. Ellucian Signals, Civitas). Further, this has led to rapid scholarly investigation of data-driven approaches to understanding learning and the formation of new educational data science communities (including Educational Data Mining and Learning Analytics) with associated professional societies, conferences, journals, summer training institutes, and degree programs. The explosion of interest in “educational cyberinfrastructure” is reflected in the range of government educational reports (including U.S., European and Australasian) mapping the state of the art and future roadmaps (REFS). RESEARCH QUESTIONS Like many other communities entering this computing-intensive paradigm, Learning Analytics researchers require scalable infrastructure to pose questions that until recently were either unimaginable or impractical to investigate. Examples include: • How do we provide personalized, real time feedback to learners at massive scale, based on the analysis of millions of permutations of activity traces from the use of digital tools such as simulations, design toolkits, and experimental apparatus? Who benefits from this feedback and how does it affect students’ behaviors? • Given the capacity to aggregate data streams from myriad platforms, personal devices and environmental sensors, how can we design coherent visualizations to help educators and learners interpret and act productively? • What are the most effective data models to aggregate different data streams and sources to provide meaningful and theoretically valid insights into learning? • What are the ways in which human experts (e.g. teachers, instructors, mentors) will interpret and act on predictive models of student success which can be highly multidimensional and probabilistic in nature? • The newest methods of text analytics can in principle codify a large corpus of student writing or online discourse automatically, performing in seconds what might take humans weeks of painstaking qualitative analysis. To develop such tools, researchers need to experiment systematically, with many iterations. How robust is the performance of automated analysis compared to human coding? Do analytic models transfer between learning contexts and institutions? What size training corpus is required for a return in performance? Can data be de­ identified at scale in order to meet ethical requirements? • What kind of data models most effectively represent the temporal nature of learning to assess progression and to provide contextualized feedback to learners and instructors? Question 2 Cyberinfrastructure Needed to Address the Research Challenge(s) (maximum ~1200 words): Describe any limitations or absence of existing cyberinfrastructure, and/or specific technical advancements in cyberinfrastructure (e.g. advanced computing, data infrastructure, software infrastructure, applications, networking, cybersecurity), that must be addressed to accomplish the identified research challenge(s). The increasing technology mediation in learning and education has translated into an unprecedented amount of data that needs to be securely collected, manipulated, and made available for analysis. The overall procedure requires a flexible and yet extremely secure data infrastructure accompanied by a software layer to make the data available for applications. Areas such as Multimodal Learning Analytics are producing dense streams of audio, video, psychophysiological and environmental data, and clickstreams that need to be managed by teams of stakeholders through infrastructure that guarantees its security and reliability, as well as student privacy. The current approaches rely on off-the-shelf tools that have not been conceived for these experimental settings. In fact, such tools are suitable for scenarios that are significantly distant from those emerging in the context of educational research. The infrastructure needed to address these challenges comprises: • Secure, scalable platform with high communication bandwidth and storage capacity to store streams of data captured by systems such as video, audio, physiological and environmental data, and clickstreams. • High performance data management procedures to execute analytical procedures (machine learning algorithms, computation of visualizations, etc.) in near real-time fashion (e.g. suitable for interventions in a classroom and at the individual student level). • High availability services for data queries from external agents. In the current emerging ecosystems, the produced data streams need to be made available through high availability mechanisms (APIs, Web services) to other stakeholders to facilitate distributed analysis and translation into actionable knowledge, often joining data across modalities (e.g. clickstream data with student record data and in-classroom Submission in Response to NSF CI 2030 Request for Information PAGE 3 DATE AND TIME: 2017-04-05 09:19:23 REFERENCE NO: 224 audio data). • A sociotechnical layer to the infrastructure which enables the fluid sharing and use of sensitive data (e.g. FERPA and COPPA protected) among legal entities (academic institutions, researchers, vendor partners) with ease. Question 3 Other considerations (maximum ~1200 words, optional): Any other relevant aspects, such as organization, process, learning and workforce development, access, and sustainability, that need to be addressed; or any other issues that NSF should consider. The upskilling of the user community is an often-neglected part of technology innovation programs, which typically results in new digital tools being used in rudimentary ways because it enables, or even requires, new ways of working that users are not ready for. In short, people change far more slowly than technology. Habits can be hard to change, especially among more senior educators and researchers with established ways of collaborating and conducting research. Thus, a coherent strategy for upskilling the workforce is required, arguably, focusing on the doctoral students and early career researchers who are strategically placed to introduce new practices into their research groups. We propose that “educational cyberinfrastructure” can be harnessed to upskill the workforce needed to leverage cyberinfrastructure initiative. By “eating our own dog food”, therefore, we should deploy data-intensive tracking, analysis and feedback techniques to build the skills people will need to use those very same techniques in their own work. We need to address questions such as: • Can we design training platforms that can track the usage patterns of cyberinfrastructure tools, in order to understand usage and coach users to take them to the next level of expertise? • Can we address the urgent educational data science skills shortage, and accelerate the development of the next generation of Learning Analytics researchers and educators, in order to instrument cyberinfrastructure tools, and to provide useful analytics in science and engineering educational platforms? Consent Statement “I hereby agree to give the National Science Foundation (NSF) the right to use this information for the purposes stated above and to display it on a publically available website, consistent with the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode).”",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6f070c05dbd342d3906ee2fbdd2c3529ed558851,https://www.semanticscholar.org/paper/6f070c05dbd342d3906ee2fbdd2c3529ed558851,Designing and evaluating techniques to mitigate misinformation spread on microblogging web services,"Online social media is a powerful platform for dissemination of information during important realworld events. Beyond the challenges of volume, variety and velocity of content generated on online social media, veracity poses a much greater challenge for effective utilization of this content by citizens, organizations, and authorities. Veracity of information refers to the trustworthiness / credibility / accuracy / completeness of the content. Over last few years social media has also been used to disseminate misinformation in the form of rumors, hoaxes, fake images, and videos. We aim to address this challenge of veracity or trustworthiness of content posted on social media. The spread of such untrustworthy content online has caused the loss of money, infrastructure and threat to human lives in the offline world. We focus our work on Twitter, which is one of the most popular microblogging web service today. We provide an in-depth analysis of misinformation spread on Twitter during real-world events. We propose and evaluate automated techniques to mitigate misinformation spread in real-time. The main contributions of this work are: (i) we analyzed how true versus false content is propagated through the Twitter network, with the purpose of assessing the reliability of Twitter as an information source during real-world events; (ii) we showed the effectiveness of automated techniques to detect misinformation on Twitter using a combination of content, meta-data, network, user profile and temporal features; (iii) we developed and deployed a novel framework for providing indication of trustworthiness / credibility of tweets posted during events. We evaluated the effectiveness of this real-time system with a live deployment used by real Twitter users. First, we analyzed Twitter data for 25+ global events from 2011-2014 for the spread of fake images, rumors, and untrustworthy content. Some of the prominent events analyzed by us are: Mumbai blasts (2011), England Riots (2011), Hurricane Sandy (2012), Boston Marathon Blasts (2013), Polar Vortex (2014). We identified tens of thousands of tweets containing fake images, rumors, fake websites, and by malicious user profiles for these events. We performed an in-depth characterization study of how this false versus the true data is introduced and disseminated in the Twitter network. Second, we showed how features of meta-data, network, event and temporat from user-generated content can be used effectively to detect misinformation and predict its propagation during realworld events. Third, we proposed and evaluated an automated methodology for assessing credibility of information in tweets using supervised machine learning and relevance feedback approach. We developed and deployed a real-time version in TweetCred, a system that assigns a credibility score to tweets. TweetCred, available as a browser plug-in, has been installed and used by 1,808 real Twitter users. During ten months of its deployment, the credibility score for about 12 million tweets was computed, allowing us to evaluate TweetCred in terms of accuracy, performance, effectiveness and usability. The system TweetCred built as part of this thesis work is used effectively by emergency responders, firefighters, journalists and general users to obtain credible content from Twitter. This thesis work has shown that measuring credibility of the Twitter content is possible using semi-automated techniques, and the results can be valuable to the real-world users. The insights obtained from this research and deployment provide a basis for building more sophisticated technology to tackle similar problems on different social media.",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d15db8c76050e50bcac5cb4cb0b45deb85574606,https://www.semanticscholar.org/paper/d15db8c76050e50bcac5cb4cb0b45deb85574606,Automated Scalability of Cloud Services and Jobs,"Many scientific and commercial applications require access to computation, data or networking resources based on dynamically changing requirements. Users and providers both require these applications or services to dynamically adjust to fluctuations in demand and serve end-users at required quality of service (performance, reliability, security, etc.) and at optimized cost. This may require resources of these applications or services to automatically scale up or down. 
The European funded COLA (Cloud Orchestration at the Level of Application) project aims to design and develop a generic framework that supports automated scalability of a large variety of applications. Learning from previous similar efforts and with the aim of reusing existing open source technologies wherever possible, COLA elaborated a modular architecture called MiCADO (Microservices-based Cloud Application-level Dynamic Orchestrator) [1] that provides optimized deployment and run-time orchestration for cloud applications. 
 
MiCADO is built from well-defined building blocks implemented as microservices. This modular design supports various implementations where components can be replaced relatively easily with alternative technologies. The generic, technology independent architecture diagram of MiCADO is represented in Figure 1. Building blocks, both on the MiCADO Master and also on the MiCADO Worker Nodes are implemented as microservices. The current implementation uses widely applied technologies, such as Docker Swarm as Container Orchestrator [2], Occopus as Cloud Orchestrator [3], and Prometheus [4] as the Monitoring System. 
 
The user facing interface of MiCADO is a TOSCA (Topology and Orchestration Specification for Cloud Applications, an OASIS standard) [5] based description of the desired topology and its associated scalability and security policies. This interface can then be embedded to existing GUIs, custom web interfaces or science gateways. 
 
The first prototype implementations of MiCADO show promising results on various application types. The two main targeted application categories are cloud-based services where scalability is achieved by scaling up or down the number of containers and virtual machines based on load, performance and cost, and the execution of a large number of (typically parameter sweep style) jobs where a certain number of these jobs need to be executed by a set deadline. 
Direct involvement of industry partners assures that the results of COLA are prototyped on real application scenarios. Three near production quality demonstrators and twenty further proof of concept case studies are being implemented using MiCADO and demonstrating its applicability in case of both service and job type scalability. Some of the applications prototyped are directly related to services utilized in science gateways, such as the Data Avenue service of WS-PGRADE [6].",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
dbc0764f1c752ac6dc8dc2b468201fe362a2f037,https://www.semanticscholar.org/paper/dbc0764f1c752ac6dc8dc2b468201fe362a2f037,The moral imperative of artificial intelligence,"of a five-game match in Seoul, Korea. (AlphaGo is a program developed by DeepMind, a British AI company acquired by Google two years ago.) After Deep Blue’s victory against chess world champion Gary Kasparov in 1997, the game of Go was the next grand challenge for game-playing artificial intelligence. Go has defied the brute-force methods in game-tree search that worked so successfully in chess. In 2012, Communications published a Research Highlight article by Sylvain Gelly et al. on computer Go, which reported that “Programs based on Monte-Carlo tree search now play at human-master levels and are beginning to challenge top professional players.” AlphaGo combines tree-search techniques with searchspace reduction techniques that use deep learning. Its victory is a stunning achievement and another milestone in the inexorable march of AI research. By using deep–learning techniques to prune the search tree, AlphaGo can be said to augment bruteforce search with “intuition,” which it has developed by playing numerous games against itself. (Google said AlphaGo does not use Lee’s games as its training data.) By relying on learned “intuition,” AlphaGo is able to overcome the so-called Polanyi’s Paradox. The philosopher Michael Polanyi observed in 1966, “We can know more than we can tell ... The skill of a driver cannot be replaced by a thorough schooling in the theory of the motorcar.” Some labor economists have viewed Polanyi’s Paradox as a major barrier for AI, arguing it implies a limit on its potential to automate human jobs. AlphaGo’s victory demonstrates machine learning provides a path around that barrier. Indeed, the automation of driving has been a major challenge for AI research over the past decade. In 2004, economists argued driving was unlikely to be automated in the near future due to Polanyi’s Paradox. A year later, a Stanford autonomous vehicle won a DARPA Grand Challenge by driving over 100 miles along an unrehearsed desert trail. Now, more than a decade later, both technology companies and car companies vigorously pursue the automation of driving. I expect the technical challenges to be resolved in the coming decade. In 1843, Ada Lovelace, well known for her work on Charles Babbage’s Analytical Engine, wrote to Babbage that she wished to see computing technology developed for “the most effective use of mankind.” It is difficult for me to think of any computing technology other than automated driving that can be deployed in a decade or two and with such benefit for humanity. About 1.25 million people worldwide die from car accidents every year. Over 90% of these accidents are caused by human error. By automating driving, we could save over a million lives a year, as well as avoid countless injuries. At the same time, the automation of driving would have a huge disruptive effect on the global economy. Existing industries will shrivel, and whole new industries will rise. In the U.S., close to 10% of all jobs involve operating a vehicle and we can expect to see the majority of these jobs disappear. The human cost of such a profound change cannot be underestimated. As a precedent we can see what happened to U.S. manufacturing over the past 35 years. While manufacturing output in constant dollars is at an all-time high, manufacturing employment peaked around 1980 and is today lower than it was in 1947. The disappearance of millions of jobs due to automation may explain a recent, rather shocking, finding by economists Angus Deaton, winner of the 2015 Nobel Memorial Prize in Economic Science, and Anne Case, that mortality for white middle-aged Americans has been increasing over the past 25 years, due to an epidemic of suicides and afflictions stemming from substance abuse. Thus, the automation of driving would be hugely beneficial, saving lives and preventing injuries on a massive scale. At the same time, it would have a profoundly adverse impact on the labor market. In the balance, life saving and injury prevention must take precedence, and we have a moral imperative to develop and deploy automated driving. The solution to the labor problem will not be technical, but sociopolitical. As computing professionals, we also have a moral imperative to acknowledge the adverse societal consequences of the technology we develop and to engage with social scientists to find ways to address these consequences. Follow me on Facebook, Google+, and Twitter.",Commun. ACM,2016.0,10.1145/2903530,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d5b9ece53e3fbe06ebc703850d94b796b892b1b1,https://www.semanticscholar.org/paper/d5b9ece53e3fbe06ebc703850d94b796b892b1b1,Real-Time Inferential Analytics Based on Online Databases of Trends: A Breakthrough within the Discipline of Digital Epidemiology in Dentistry and Dental Anatomy,"BACKGROUND Epidemiological sciences have been evolving at an exponential rate paralleled only by the comparable growth within the discipline of data science. Digital epidemiological studies are playing a vital role in medical science analytics for the past few decades. To date, there are no published attempts at deploying the use of real-time analytics in connection with the disciplines of Dentistry or Medicine. AIMS AND OBJECTIVES We deployed a real-time statistical analysis in connection with topics in Dental Anatomy and Dental Pathology represented by the maxillary sinus, posterior maxillary teeth, related oral pathology. The purpose is to infer the digital epidemiology based on a continuous stream of raw data retrieved from Google Trends database. MATERIALS AND METHODS Statistical analysis was carried out via Microsoft Excel 2016 and SPSS version 24. Google Trends database was used to retrieve data for digital epidemiology. Real-time analytics and the statistical inference were based on encoding a programming script using Python high-level programming language. A systematic review of the literature was carried out via PubMed-NCBI, the Cochrane Library, and Elsevier databases. RESULTS The comprehensive review of databases of the literature, based on specific keywords search, yielded 491813 published studies. These were distributed as 488884 (PubMed-NCBI), 1611 (the Cochrane Library), and 1318 (Elsevier). However, there was no single study attempting real-time analytics. Nevertheless, we succeeded in achieving an automated real-time stream of data accompanied by a statistical inference based on data extrapolated from Google Trends. CONCLUSION Real-time analytics are of considerable impact when implemented in biological and life sciences as they will tremendously reduce the required resources for research. Predictive analytics, based on artificial neural networks and machine learning algorithms, can be the next step to be deployed in continuation of the real-time systems to prognosticate changes in the temporal trends and the digital epidemiology of phenomena of interest.",,2018.0,10.20944/PREPRINTS201811.0260.V2,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
01dd665737875faf046c2ef972806bd01f71c5d3,https://www.semanticscholar.org/paper/01dd665737875faf046c2ef972806bd01f71c5d3,"Precision Dairy Edge, Albeit Analytics Driven: A Framework to Incorporate Prognostics and Auto Correction Capabilities for Dairy IoT Sensors",,Advances in Intelligent Systems and Computing,2018.0,10.1007/978-3-030-03405-4_35,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3a4ed766d6f9378bbda96072d1eaf88c84c15131,https://www.semanticscholar.org/paper/3a4ed766d6f9378bbda96072d1eaf88c84c15131,A Context-Driven Data Visualization Engine for Improved Citizen Service and Government Performance,"Every day, the US government creates and consumes significant amounts of data. Federal agencies are finally riding the wave of Big Data Analytics for solving problems such as improving citizen service. Domestic tranquility, customer satisfaction, transparency, and providing quality service are among the many critical goals that most government agencies aim to achieve through citizen service. Federal, State, County, and City governments are constantly challenged by the overwhelming number of service requests from citizens, organizations, scholars, the media, and many other entities. Governments, however, are constantly facing accountability issues that can call into question their role and efficiency. This paper, through the use of data science and context, introduces a novel engine that could be used at any federal agency to improve citizen service, evaluate performance metrics, and provide pointers to enhance satisfaction rates. The model used in the engine (called iGPS) is deployed through a number of real-world citizen's complaints datasets. Insightful and actionable data visualizations are introduced to federal employees depending on their context. - The goal is to aid them in decision-making. The model is tested for the first time (through six governmental datasets from 46 agencies), machine learning models are developed, visualizations are built, the system is deployed at the US government, and experimental results are recorded and presented.",,2018.0,10.21494/ISTE.OP.2018.0303,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
23e2eebc58dcd5ccf21a7940480a299c5a591f28,https://www.semanticscholar.org/paper/23e2eebc58dcd5ccf21a7940480a299c5a591f28,Customer support ticket escalation prediction using feature engineering,,Requirements Engineering,2018.0,10.1007/s00766-018-0292-3,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3516e17605a0e18918f898a2283c4e94a70d4e72,https://www.semanticscholar.org/paper/3516e17605a0e18918f898a2283c4e94a70d4e72,Opportunities and Challenges in Association and Episode Discovery from Electronic Health Records,"83 guidance; machine understanding of human behavior, emotional, and physiological states; and the need to respond appropriately to unintended stimuli. Numerous machine perception , cognition, and communication challenges also remain such as image-guided intervention, speech and language understanding, two-hand-like manipulative dexterity, and learning systems that adapt to an individual's long-term change of state. Solutions likely lie at the intersection of new and ongoing research in computer science, materials, psychology, and neuroscience. The societal pressure to mitigate the healthcare crisis presents an unprecedented opportunity for computing , information science, and engineering. Whereas the pursuit of understanding the pathogenesis of disease will be accelerated with new algorithms and increasingly powerful computation and data architec-tures, we look to other computation-enabled means to provide additional avenues to the pursuit of quality of life. Multidisciplinary approaches are required to engineer a privacy-maintaining information infrastructure with secure, real-time access to unprecedented amounts of heterogeneous health, medical, and treatment data. New generations of algorithms must be developed to utilize the resulting global resource of population-based evidence for assisted discovery, knowledge creation, and even individual point-of-care decisions. Ana-lytics based on modeling phenomena ranging from the physiology of humans to their social interactions are required to optimize therapies ranging from molecular medicine to be-havioral interventions. Such advances in human-centered computing in combination with standardization and commercialization of unobtrusive sensing and robotics will trigger a disruptive change in health-care and wellbeing by empowering individuals to more directly participate. Finally, partnerships among academic , industrial, and governmental bodies are required to enable these computer science innovations and realize their deployment in order to help transform healthcare. Will Barkis is a AAAS science and technology policy fellow at the American Association for the Advancement of Science. Contact him at wbarkis@gmail.com. As healthcare practices, both small and large, move from traditional paper-based patient charts to electronic health records (EHRs), new opportunities are emerging for secondary uses of data captured as part of routine care. Such opportunities include not only traditional research meth-odologies involving relatively small cohorts of selected patients, but also large-scale data mining analyses encompassing hundreds of thousands or even millions of patients at once. Performing these nontraditional analyses has required novel computational approaches, sometimes borrowing from techniques originally developed in other elds such as genomics and network theory. Additionally, to interpret such large volumes of data in a meaningful",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4867a63e5edd7ee10f28067d6dd5b07d69321de7,https://www.semanticscholar.org/paper/4867a63e5edd7ee10f28067d6dd5b07d69321de7,Thesis Title: A Framework for Improving the Performance of Signature-based Network Intrusion Detection Systems,"Network Intrusion detection systems (NIDSs) have been widely deployed in different network environments (e.g., banks, schools) to defend against a variety of network attacks (e.g., Trojans, worms). Generally, a network intrusion detection system can be classified into two categories: signature-based NIDS and anomaly-based NIDS. In realworld applications, the signature-based NIDS is more prevalent than the anomaly-based detection as the false alarm rate of the former is much lower than the latter. However, we identify three major issues that can greatly affect the performance of a signature-based NIDS. Expensive signature matching. The traditional signature matching in a signature-based NIDS is too expensive that the computing burden is at least linear to the size of an incoming string. Therefore, the operational burden of a signature-based NIDS could be significantly increased in a large-scale network environment. Overhead network packets. In a large-scale network environment, a signature-based NIDS usually has to drop lots of network packets since the number of incoming packets exceeds its maximum processing capability. Massive false alarms. Although the false alarm rate of a signature-based NIDS is much smaller than that of an anomaly-based NIDS. The number of false alarms generated by a signature-based NIDS can still increase the difficulty in analyzing true alarms and adversely affect the analysis results. To mitigate the above issues, in this thesis, we propose several approaches in improving the performance of a signature-based NIDS such as Snort in the following three aspects: Signature matching improvement.We design an exclusive signature matching scheme to help perform a more efficient signature matching with the purpose of enhancing the performance of signature matching in a heavy traffic environment. Network packet filtration and reduction. To mitigate this issue, we advocate the method of constructing a packet filter such as blacklist-based packet filter, list-based packet filter and trust-based packet filter to help filter out target network packets for a signature-based NIDS such as Snort in terms of IP reputation. This packet filter can be deployed in front of a signature-based NIDS and reduce its workload in an intensive traffic network. False alarm reduction. To resolve this issue, we design several false alarm filters such as machine-learning based false alarm filters, alarm filters using knowledge-based alert verification and context-based alarm filters to help reduce false alarms (or non-critical alarms) that are generated by a signature-based NIDS. A Framework. In addition, we further propose a framework by combining the above work to overall improve the performance of a signature-based NIDS such as Snort. As a case study of the framework, we implement an enhanced filter mechanism (shortly EFM) that consists of three major components: a context-aware blacklist-based packet filter, an exclusive signature matching component and a KNN-based false alarm filter. In particular, the component of context-aware blacklist-based packet filter is responsible for filtering out network packets in terms of IP reputation. The exclusive signature matching component is implemented in the context-aware blacklist-based packet filter and aims to speed up the signature matching. At last, the component of KNN-based false alarm filter is responsible for filtering out false alarms which are produced by the context-aware blacklist-based packet filter and the NIDS. In the evaluation, the experimental results demonstrate that our framework is promising and by deploying with the EFM, the performance of a signature-based NIDS such as Snort can be improved in the aspects of network packet filtration, signature matching improvement and false alarm reduction.",,2013.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
036cc3918942bf59f2d3ea6364e7c2a5f0ecbe94,https://www.semanticscholar.org/paper/036cc3918942bf59f2d3ea6364e7c2a5f0ecbe94,"Frontiers of WWW Research and Development - APWeb 2006, 8th Asia-Pacific Web Conference, Harbin, China, January 16-18, 2006, Proceedings",,APWeb,2006.0,10.1007/11610113,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8e1f7b351893a37457209ec192dc125a2b227aa3,https://www.semanticscholar.org/paper/8e1f7b351893a37457209ec192dc125a2b227aa3,Bounded Rationality in the Iterated Prisoner ’ s Dilemma,"In diverse fields such as computer science, economics and psychology, bounded rationality has emerged as an important research topic. Models which assume the existence of perfectly rational agents seem inadequate for many realworld problems where agents often lack perfect rationality. Classes of imperfect rationality include the conditions of incomplete knowledge, memory, information or computational ability. In this paper, bounded rationality is investigated in the context of the Iterated Prisoner’s Dilemma (IPD). Six papers are surveyed which address issues of optimality and cooperation in repeated games with applications to IPD, using either a machine learning or game theory approach. Each paper imposes some bound on players’ rationality; these approaches to bounding are classified and compared based on their results and applicability.",,2005.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
57683b102a2f59e246900e48d310236c152946b7,https://www.semanticscholar.org/paper/57683b102a2f59e246900e48d310236c152946b7,Cost-effective sensors placement and leak localization – the Neptun pilot of the ICeWater project,"This paper extends previous research to analytically identify leaks within a water distribution network (WDN), by combining hydraulic simulation and network science based data analysis techniques. The WDN model is used to run several ‘leakage scenarios’, by varying leak location (pipe) and severity, and to build a dataset with corresponding variations in pressure and flow, induced by the leak. All junctions and pipes are considered for potential pressure and flow sensors deployment; a clustering procedure on these locations identifies the most relevant nodes and pipes, and cost-effectiveness was considered. A graph is then generated from the dataset, having scenarios as nodes and edges weighted by the similarity between each pair of nodes (scenarios), in terms of pressure and flow variation due to the leak. Spectral clustering groups together similar scenarios in the eigen-space spanned by the most relevant eigen-vectors of the Laplacian matrix of the graph. This method uses superior traditional techniques. Finally, support vector machines classification learning is used to learn the relation between variations in pressure and flow at the deployed meters and the most probable set of pipes affected by the leak.",,2015.0,10.2166/AQUA.2015.037,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
583b889e75b5766800e99215ae782f7d1834c47e,https://www.semanticscholar.org/paper/583b889e75b5766800e99215ae782f7d1834c47e,Data Mining on Grids,,IC3,2011.0,10.1007/978-3-642-22606-9_36,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
349e889830c94a87ae601fdbc849ee72053d4ef0,https://www.semanticscholar.org/paper/349e889830c94a87ae601fdbc849ee72053d4ef0,Editorial: Introduction to the Special Issue with Papers from the Fifth International Conference on Development and Learning (ICDL),"In June 2006, 168 researchers from 11 countries converged on the campus of Indiana University Bloomington to participate in the Fifth International Conference on Development and Learning (ICDL 05). Ever since its inaugural meeting at Michigan State University in 2000, ICDL has had a unique thematic focus. The conference brings together empirical scientists working in a broad range of natural and social sciences, such as developmental psychologists and neuroscientists, with researchers who are more focused on theoretical models and engineering, such as neural network or robotic systems. The goal is to identify principles of learning and development that are held in common between natural and artificial systems (Weng et al., 2001). The payoff is twofold: capturing these principles will pave the way to a better understanding of how development unfolds in animals and humans, and it will also lead to new kinds of technological systems that are capable of autonomous development. Learning and development are traditional core topics for the journal Adaptive Behavior. This special issue presents a cross-section of cutting edge topics and approaches exemplified by five papers presented at ICDL 05. Eighteen out of a total of 93 papers that were presented at ICDL 05 received nominations by reviewers and members of the program committee to be included in this special issue. The five papers presented here were among those that received the highest overall rankings. After the conference, incorporating discussion and feedback, these papers were extensively revised and expanded, and they underwent a second round of peer review. The papers cover a range of topics, including reinforcement learning, the development of visual attention, modeling gaze following, the construction of interaction histories for autonomous agents, and the concept of distributed intelligence. Together they provide a snapshot of the breadth and sophistication of this emerging interdisciplinary effort into fundamental principles of autonomous development. Reinforcement learning, and more specifically temporal difference (TD) learning, has been a strong focus in machine learning as well as in human and animal experimentation. While it has been quite successful as a framework for specific forms of biological learning, relatively little work has been done to investigate its interactions with other components of a cognitive architecture, or in the context of autonomous behavior. William Alexander’s article attempts to integrate TD learning with models of attentional shifting in an autonomous agent. The proposed model incorporates effects of learning on the attentional representation of stimuli, and it does so in real time. The results point to a clear performance gain and better match to empirical data when attentional effects are combined with TD learning. This illustrates the importance of building models that integrate different functionalities and operate within a common cognitive architecture. Matthew Schlesinger, Dima Amso, and Scott Johnson have studied the role of visual attention in infant development and argued that active deployment of attentional resources is crucial for the development of a visual object perception, specifically visual completion. Building on this empirical work, the current article presents a detailed model of the developing visual system in an attempt to use the model to gain insight into",Adapt. Behav.,2007.0,10.1177/1059712307078641,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
45dc2fedaf830f59d05113f958b45b65f5360ffc,https://www.semanticscholar.org/paper/45dc2fedaf830f59d05113f958b45b65f5360ffc,"Introduction to the Special Issue on Representation, Analysis, and Recognition of 3D Humans","Modeling, processing, recognizing, searching, and retrieving 3D human data (shapes, gestures, interactions) is a well-established research area in Multimedia. In the last decade, there has been a tremendous increase in opportunities for using 3D human data in medicine, security, and human computer interaction, largely driven by the development of effective devices and algorithms for recovering 3D data (e.g., Microsoft Kinect, Intel RealSense, Google Project Tango, and Apple Prime-Sense). Such rich information opens the way to new modes of experiential computing, interactive environments, as well as new multimedia content. This special issue is of interest to an interdisciplinary target audience as well interdisciplinary teams of contributors spanning: applied math, multimedia experiential computing, computational science and engineering, and application domain experts. Several fundamental research problems within the scope include: Representations for 3D static and dynamic human data Representations for non-rigid 3D objects (face, body) Temporal modeling of 3D face/body sequences Machine learning techniques for 3D human representations Computationally efficient strategies for resource constrained deployments Fusing multiple cues: shape, color, texture, motion etc. This special issue aims to bring together researchers interested in defining new and innovative solutions that advance the way 3D human data are used in multimedia computing, communications and applications such as human behavior understanding from 3D sensors, animation and entertainment, sports analytics, natural interaction, virtual and augmented reality. Application areas of interest include, but are not limited to:",ACM Trans. Multim. Comput. Commun. Appl.,2018.0,10.1145/3181709,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
57b672bbd41fa4fae7b4307191978599513107f4,https://www.semanticscholar.org/paper/57b672bbd41fa4fae7b4307191978599513107f4,Data Engineering Project (Educating For The Future PhUSE WG),"With an expected 100% increase, over the next 3 years, of data from non-EDC sources (such as smartphones, wearables and custom apps) the traditional methods of managing data for clinical trials presents executives with a resourcing headache. As such, many companies are looking for lower cost strategies to sure up this shortfall in resourcing. However, citing case studies from other industries, there are new methodologies/technologies in data engineering which could enable automation of much of the “heavy-lifting” currently practiced in clinical data management and statistical programming. This paper discusses the Data Engineering Project within the PhUSE Computational Science (CS) Working Group, Educating For The Future, with a view to educate clinical data managers in data engineering principles so that they can be prepared, equipped and effective in dealing with the coming “data tsunami” heading to the shores of clinical research. INTRODUCTION Did you realise we are living in the age of the Fourth Industrial Revolution? Perhaps you have been busy downloading a myriad of “apps” designed to make your life easier or connecting on social media, uncovering relationships and associations you didn’t even know you had. Perhaps you have been shopping a global marketplace, comparing prices, quality and availability, all at your fingertips and in a minutes’ time. While this has been happening, the Fourth Industry Revolution has been evolving at exponential proportions ​. Just ask Siri! The term “Industrie 4.0”, was originated in Germany, as a government-led initiative, to transform manufacturing through advanced digital capability. Thus creating the concept of a “smart factory”, based on four key design principles ​: 1. Interconnection of machines, devices, sensor and people 2. Vast amounts of useful information (data) to drive decision making 3. Technical assistance to aid humans, for example to visualise data or to perform tasks that may be of safety concern for a human. 4. The use of cyber-physical systems to make decisions on their own and to perform tasks as autonomously as possible. Emerging from the premise of “Industrie 4.0” is the advent of the term “The Fourth Industrial Revolution” (also referred to as “4IR” or “I4.0”). This term originated in 2016 when described by Klaus Schwab (Founder and Executive Chairman of the World Economic Forum), as a “technological revolution that will fundamentally alter the way we live, work, and relate to one another”. Klaus goes on to describe it as a digital revolution with innovative uses of a combination of technologies that build upon the premise of the third revolution (i.e. electronics and information technology to automate production). As a result, emerging technologies have brought forth advancements in fields such as ​artificial intelligence, robotics, the Internet of Things, autonomous vehicles, 3D printing, nanotechnology, biotechnology, materials science, energy storage, and quantum computing. This rapid evolution will undoubtedly affect industries world-wide, already disrupting many industries, such as travel agencies, video rentals and bookstores​ . The pharmaceutical industry is also experiencing the impacts of I4.0. Digital and mobile technologies has brought on significant advancements in data acquisition and accessibility as it relates to health care and patient data. As reported in the Tufts-Veeva eClinical Landscape study in 2017, data coming from sources such as, smartphones, custom applications, and mobile health are expected to double in the next 3 years ​. Therefore requiring greater capabilities in handling large volumes of data, as well as data from coming in through various data streams and 1 PhUSE EU Connect 2018 formatting. As with other industries, data will become a critical asset to their business and the effective utilisation of this data can play a critical role in driving growth in the business and bringing novel therapies to the patients who need them. In this paper, we will focus on the works of the Data Engineering Project within the Educating For The Future Working Group. With the formation of the Working Group in early 2018, the team had taken on the mission to explore how data engineering techniques, successfully deployed in other industries, could be utilised in the pharmaceutical industry, with a goal to ​facilitate the education ​of the pharmaceutical industry on these techniques. We will share with you some introductory information about Data Engineering and Data Science and explore how embracing new data engineering techniques may affect the industry culture. You will learn about use cases of Data Engineering in other industries and how advances in digital capability have affected their business model. We will also share some of the many software packages and tools available to enable automation, commonly used in Data Engineering and Data Science. Finally we will reflect on the benefits that data standardisation has brought to the pharmaceutical industry and share our vision for disseminating information to facilitate your learning going forward. DATA ENGINEERING To start this learning journey, exploring the term “Data Engineering” opens the door to the vast opportunities and roles available today centered around data. In doing a simple search on the internet, ​“what is data engineering?​”, one will find many posts expressing their understanding of Data Engineering with some variation but also some similarity. However, what is clear is that Data Engineering encompasses the many considerations that need to be taken into account to optimally curate, transform, secure and disseminate data suitable for analysis. As technology and tools have become more advanced, building such a platform and infrastructure requires engineers and architects of both general and specific expertise. The Data Engineer combines knowledge in areas such as software development, infrastructure, data architecture, data warehousing, cloud technology and data cleaning in order to design, build and test solutions that define the pipelines of data throughout the enterprise, making the data accessible to the organisation.​ [5] [27] [31] Optimised Data Engineering appropriately balances the efficiency of an automated process against the cost of development and maintenance of that process, ensuring repetitive processes that require humans to write code, press keys, cut-and-paste and update documents are minimised or eliminated. DATA SCIENCE Often paired with the term “Data Engineering” is also the term “Data Science”. According to Kelle O’Neal and Charles Roe: “Data Science allows enterprises the ability to turn their data assets into a narrative. Data Science allows that narrative to be expanded across timelines, in different data spaces that trace from the past into the future, with much more involved questions and answers about an enterprise, different potential outcomes, and repercussions based on recommendations. Data Science employs a range of mathematical, business, and scientific techniques to solve complex problems about an organisation’s data assets.” ​ In contrast, the focus of the Data Engineer is on the process from data curation to dissemination and the focus of the Data Scientist is on the analytics of the data, thus extracting knowledge from the data. To achieve quality data capture, near-real-time accessibility and meaningful analytics, one cannot function without the other, and effective teamwork optimises the value of each role. As such, an analytics team would be composed of distinct roles/capabilities​ : ● Data Engineers (in areas such as database architecture, database development, machine learning architecture, ETL scripting , etc.) ● Data Scientists ● Business Analysts Data Engineering brings together the broad expertise, of these roles, to ensure the data are curated and accessible to the Data Scientist, and in our environment today, this process is becoming more and more complex. Therefore, 2 PhUSE EU Connect 2018 expertise in curating big-data and data of varying formats (structured and unstructured) is a critical core competency to optimise the potential impact of these digital assets (i.e. the data). The Data Scientist works deep in the data, utilizing various tools and techniques to discover patterns in the data that may drive decision making for the business. Optimising utilisation of the data to enable accurate conclusions can bear greater value to the organisation. As an example, per Tom Eunice’s post, “a fraud-detection algorithm may be very accurate when based on many months of historical data. However, months of historical data may not always be available. Designing a fraud-detection model that is still accurate using historical data from only a few days would be of more use and more practical to implement.” ​ The Business Analyst helps the Data Scientist understand the meaning of the data and the relevance of any discovered relationships. Initially, uncovering relationships in the data and upon further investigation, identifies meaningful patterns that may reveal information that otherwise may not have been known. ​ As you will see in the sections to follow, the full complement of the roles in an analytics team is what drives the business value. One discipline without the other (e.g. data engineering without data science) will result in missed opportunities. In the sections to follow, we often refer to Data Engineering, however, due to the close ties to Data Science, some examples elude to both Data Engineering and Data Science. USE CASES FROM OTHER INDUSTRIES In this section, we present three use cases from the transportation, retail, and agricultural industry. The use cases illustrate the importance and usage of Data Engineering. In each example the data collected, the consumer of the data, and the value of the organisation is reviewed. Similarities and potential applications to the pharmaceutical industry are discussed. UBER When",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7561dd4e255c372e170f44ab08d645e837265d28,https://www.semanticscholar.org/paper/7561dd4e255c372e170f44ab08d645e837265d28,"Small Data, Big Data, Good Data: Toward Data-Centric and Trustworthy Internet of Things","While it has been two decades since its conceptualization, the Internet of Things (IoT) remains a holy grail by and large, with only limited and small-scale deployments in reality. Among the many challenges it faces, two major ones are (1) large-scale data acquisition which cumulates “small data” into “big data” to enable more insightful IoT analytics, and (2) data trustworthiness which ensures “good data” to be consumed by IoT applications. These represent the quantity and quality aspects and underpin a wide range of IoT developments. The first part of this talk introduces Participatory IoT which takes a grassroots approach to large-scale data acquisition, where humans are leveraged as sensors to contribute data. A crucial issue arises there is incentives, i.e., how to motivate large participation with each participant exerting the best effort. I will discuss mechanism design which is an interdisciplinary research field between economics and computer science. The second part of this talk focuses on data trustworthiness. I will discuss trust and reputation systems and machine learning techniques (e.g., deep learning) for improving data quality in both participatory and orthodox (i.e., conventional sensor-based) IoT contexts. I will also describe some real deployments of my research work. The talk ends with a number of directions that constitute my future research plans.",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5fdc8c4d8a5bef3320e185c59bbb80e8e15805c8,https://www.semanticscholar.org/paper/5fdc8c4d8a5bef3320e185c59bbb80e8e15805c8,A new modelling framework over temporal graphs for collaborative mobility recommendation systems,"Over the years, collaborative mobility proved to be an important but challenging component of the smart cities paradigm. One of the biggest challenges in the smart mobility domain is the use of data science as an enabler for the implementation of large scale transportation sharing solutions. In particular, the next generation of Intelligent Transportation Systems (ITS) requires the combination of artificial intelligence and discrete simulations when exploring the effects of what-if decisions in complex scenarios with millions of users. In this paper, we address this challenge by presenting an innovative data modelling framework that can be used for ITS related problems. We demonstrate that the use of graphs and time series in multi-dimensional data models can satisfy the requirements of descriptive and predictive analytics in real-world case studies with massive amounts of continuously changing data. The features of the framework are explained in a case study of a complex collaborative mobility system that combines carpooling, carsharing and shared parking. The performance of the framework is tested with a large-scale dataset, performing machine learning tasks and interactive realtime data visualization. The outcome is a fast, efficient and complete architecture that can be easily deployed, tested and used for research as well in an industrial environment.",2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC),2017.0,10.1109/ITSC.2017.8317664,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
eb417bd65e7c3d95e209e5310433cbfa704869e1,https://www.semanticscholar.org/paper/eb417bd65e7c3d95e209e5310433cbfa704869e1,AC 2012-4161: A WIRELESS SENSOR NODE POWERED BY SOLAR HAR- VESTER FOR MARINE ENVIRONMENT MONITORING AS A SENIOR DESIGN PROJECT,"Improving the design component in undergraduate engineering education has been an immediate and pressing concern for educators, professional societies, industrial employers and agencies concerned with national productivity and competitiveness. The projects are a valuable component of the science and engineering education. The design experience develops the students’ lifelong learning skills, self-evaluations, self-discovery, and peer instruction in the design’s creation, critique, and justification. Students learn to understand and make use of the manufacturer data sheets, application notes, and technical manuals when developing their design projects. The experience, which would be difficult to complete individually, gives the students a sense of satisfaction and the accomplishment that is often lacking in many engineering courses, using traditional teaching approaches. Furthermore, the design experience motivates student learning and develops skills required in industry. This paper discusses the development of a student project involving a number of senior undergraduate students at our engineering technology program. A simple, low cost solar power system to power a wireless sensor network (WSN) was developed during this project. The proposed WSN may be used for monitoring a coastal shallow water marine environment. It is composed of several sensor nodes or buoys. The description of this harvester, the system characteristics and performances are presented in details. Various aspects of the educational experience are examined such as the educational goals of the project, project organization, and outcomes. Innovative educational approaches are described such as brainstorming session and discussion with students of high-level choices described by a decision tree, component selections, simulations and system performance and characteristics computation. In the second part of the paper the design solution that was adopted is described in details. The adopted design solution includes: power electronics circuitry (DC-DC converter design and test), maximum power point tracking (MPPT) algorithms, control strategies, battery and super-capacitor selection as energy buffers, and overall system performances. Different MPPT and charging algorithms were analyzed and evaluated for their effectiveness in solar energy conversion system, as well as the control algorithms and implementation to maximize the power output. The project is a good example of multi-disciplinary cooperation as well as providing valuable hands-on experience. In addition to providing useful lessons in teamwork and project management, the project will provide a working demonstration wireless sensor network and solar energy system. The goal of the design project is to explore and enhance students understanding of the fundamental engineering principles, power circuit simulation capability and hands-on demonstration of system prototyping. Introduction and Project Rationale The wireless sensor networks (WSNs) is an autonomous network system which consists of large number of micro sensor nodes and has the characteristics of capability of sensing, calculation communication and low cost, and low power. It is a “smart” system that can accomplish various monitoring tasks, according to different environment conditions. Monitoring of water environment is one of its typical applications. Compared with existing real-time automatic water P ge 25120.2 environment monitoring systems, WSNs-based water environment monitoring system has strongpoint as follows : 1) Less effect of the system on ecological environment: nodes transmit water environment parameters to base station by low power and low radiation wireless channel and multi-hop communication protocol. Marine wireless sensor networks offer an unmatched option to a wide range of different domains. The significance of the aforementioned research lies in the fact that it opens the door for a variety of applications as well as new areas of relevant research in wireless networks. The possibility of having hundreds of thousands of sensor nodes diving in the ocean collecting data about the different inhabitants offers a unique opportunity for ocean studies and researchers in the field. The ability to seed wireless sensors that can dive deep in the ocean taking real-time pictures and reporting relevant data about the oceanic life can play a major role in bringing ocean research to new levels. In the following we present the development and designed of a solar energy harvester that can be used to provide power to a WSN for marine environment monitoring system. Coastal marine systems are particularly vulnerable to the effects of human activity attendant on industrial, tourist and urban development. Information and communications technologies offer new solutions for monitoring such ecosystems in real time. Therefore, during the past decade various initiatives emerged, from small-scale networks to complex coastal observation systems. Among small-scale networks, WSNs are a highly attractive solution for its easiness in deployment, operating and dismantling. WSNs are also relatively inexpensive. Energy harvesting or the process of acquiring energy from the surrounding environment has been a continuous human endeavor throughout history, e.g. the use of watermills in ancient Greece, and of sailboats by Phoenicians and Egyptians, circa 4000 B.C. Unlike the conventional electric power generation systems, in energy harvesting concept, fossil fuels are not used and the generation units might be decentralized. There are many sources for harvesting energy. Solar, wind, ocean, hydro, electromagnetic, electrostatic, thermal, vibration, and human body motion are renewable sources of energy. Even the energy of radio frequency waves, propagated due to television and radio broadcasting in the environment, can be harvested. Economic, environmental, and geopolitical constraints on global conventional energy resources started forcing the nations to accelerate energy harvesting from renewable energy sources. Thus, advanced technical methods should be developed to increase the efficiency of devices in harvesting energy from various environmentally friendly resources and converting them into electrical energy. These developments have sparked the interest in engineering community as well as in the engineering education community to develop more energy harvesting applications and new curriculums for renewable energy and energy harvesting topics. Nowadays, there is an increasing interest to harvest energy at a much smaller scale, for applications such as the ones found in many embedded systems where the power requirements are often small (less than 100 mW). Sustaining the power requirement for autonomous wireless and portable devices is an important issue. However, this progress has not been able to keep up with the development of microprocessors, memory storage, and wireless technology applications. For example, in wireless sensor networks, battery-powered sensors and modules are expected to last for a long period of time. However, conducting battery maintenance for a large-scale network consisting of hundreds or even thousands of sensor nodes may be difficult, if not impossible. Ambient power sources, as a replacement for batteries, come into consideration to minimize the maintenance and the cost of operation. Power scavenging may enable wireless and portable electronic devices to be completely self-sustaining, so that battery maintenance can be eventually removed. When P ge 25120.3 compared with energy stored in common storage elements, such as batteries, capacitors, and the like, the environment represents a relatively infinite source of available energy. Systems continue to become smaller, yet less energy is available on board, leading to a short runtime for a device or battery life. Researchers continue to build high-energy density batteries, but the amount of energy available in the batteries is not only finite but also low, which limits the life time of the systems. Extended life of the electronic devices is very important; it also has more advantages in systems with limited accessibility, such as those used in monitoring a machine or an instrument in a manufacturing plant used to organize a chemical process in a hazardous environment. The critical long-term solution should therefore be independent of the limited energy available during the functioning or operating of such devices. 1.1. Objectives of Project Work In response to these demands, universities offering baccalaureate and graduate degrees in electrical engineering or engineering technology must develop curricula to educate a workforce that is well equipped to meet these challenges. Unfortunately, US universities have not kept pace with the growing fields of power electronics, wireless networks or renewable energy, and they are not educating enough students in these recent technology applications, as needed by our industries. Rather this growth has been principally research and industry oriented. Little progress is being reported, with very few notable exceptions on the role of educational institutions in either keeping pace with this growth or addressing the importance of introducing new emerging engineering technologies, applications, and effective classroom and laboratory instruction. Engineering and engineering technology programs must offer a relevant and validated curriculum that prepares students for post-graduation success. Courses that cover traditional subject matter in mathematics, the sciences, engineering economics and other related topics provide the foundation of knowledge upon which specific skill sets are added. However, it is critical for engineering/technology to transition from theoretical work in the classroom towards experiential learning with applications of technology and design. The main objective of senior design courses in engineering and engineering technology",,2012.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e469e466fbf3eaf69edb57fa33c9da4aaebf3a27,https://www.semanticscholar.org/paper/e469e466fbf3eaf69edb57fa33c9da4aaebf3a27,Trends in Computer Vision: An overview of vision-based data acquisition and processing technology and its potential for the transportation sector,"This report foresees imaging technology rapidly advancing in applications that require simple object detection, such as vehicle collision avoidance and advanced traffic management systems. The paper speculates that future advances, such as improved object detection algorithms, machine learning and “augmented reality” may influence a number of transportation applications in the automotive, industrial, and infrastructure sectors. This report is part of the Connected Vehicle Technology Scan and Assessment project. This two year scanning series of Connected Vehicle Insight reports will assess emerging, converging and enabling technologies outside the domain of mainstream transportation research. ITS America seeks technologies that will potentially impact state-of-the-art or state-of-the-practice in Intelligent Transportation Systems (ITS) deployment over the next decade, with an emphasis on the ""connected vehicle."" The Technology Scan Series notes trends, technologies, and innovations that could influence, or be leveraged as part of, next-generation intelligent transportation systems within the next five to seven years. The series’ focus is on developments in applied science and engineering and innovation in data acquisition, dissemination, processing, and management technologies and techniques that can potentially support transportation.",,2011.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b42f4e46a6af7aa9dd98a76c39de417ce52ae461,https://www.semanticscholar.org/paper/b42f4e46a6af7aa9dd98a76c39de417ce52ae461,Language Resources as by-Product of Evaluation: The MULTITAG Example,"In this paper, we show how the paradigm of evaluation can function as language resource producer for high quality and low cost validated language resources. First the paradigm of evaluation is presented, the main points of its history are recalled, from the first deployment that took place in the USA during the DARPA/NIST evaluation campaigns, up to latest efforts in Europe (SENSEVAL2/ROMANSEVAL2, CLEF, CLASS etc.). Then the principle behind the method used to produce high-quality validated language at low cost from the byproducts of an evaluation campaign is exposed. It was inspired by the experiments (Recognizer Output Voting Error Recognition) performed during speech recognition evaluation campaigns in the USA and consists of combining the outputs of the participating systems with a simple voting strategy to obtain higher performance results. Here we make a link with the existing strategies for system combination studied in machine learning. As an illustration we describe how the MULTITAG project funded by CNRS has built from the by-products of the GRACE evaluation campaign (French Part-Of-Speech tagging system evaluation campaign) a corpus of around 1 million words, annotated with a fine grained tagset derived from the EAGLES and MULTEXT projects. A brief presentation of the state of the art in Part-Of-Speech (POS) tagging and of the problem posed by its evaluation is given at the beginning, then the corpus itself is presented along with the procedure used to produce and validate it. In particular, the cost reduction brought by using this method instead of more classical methods is presented and its generalization to other control task is discussed in the conclusion. 1. The paradigm of Evaluation Comparative evaluation in language engineering has been used as a basic paradigm in the USA DARPA program on human language technology since 1984. Activities similar in kind, have been pursued in Europe, both at national and at European level, but on a smaller scale and over a limited time (Mariani and Paroubek, 1999). The latest efforts concerning evaluation in Europe are CLEF (Cross Language Text Retrieval System Evaluation in collaboration with NIST and TREC conference), SENSEVAL2/ROMANSEVAL-2 (Kilgarriff, 1998) and CLASS (evaluation across FP5 project clusters). Comparative evaluation is a paradigm in which a set of participants compare the results of their systems using the same or similar control tasks (Bernsen et al., 1999) and related data with metrics that are agreed upon. More precisely, Comparative evaluation consists in (1) choosing or creating a control task, (2) in gathering system or component developers and integrators who are interested in testing their systems against those of others, (3) in organizing an evaluation campaign which necessarily involves distributing linguistic data for training and testing the systems, and (4) in defining the protocol and the metrics which will be used in the results assessment. A control task is the function that the participating systems have to perform during an evaluation together with the conditions under which this function must be performed (e.g. for parser evaluation, a control task can be the bracketing of the constituents). Every deployment of the paradigm of evaluation in the field of Language Engineering entails the production of linguistic data: the organizers build the reference and test data sets and the participants apply their systems on test data to produce evaluation data. When a quantitative black-box (Sparck-Jones and Galliers, 1995) evaluation methodology is applied the data produced is generally abundant and could easily be re-used as training material if the cost of filtering out the errors was not so high. 2. Combining to Improve In machine learning, it is well known that ensemble methods or committees of learning machines can often improve the performance of a system in comparison to a single learning machine. A very promising algorithm based on this principle now under investigation is Ada boost (Schwenk, 1999). In the same field, people have applied for a long time “winner take all” strategies to combine, inside the same system, the output of several basic processing units (Simpson, 1990). In the course of its evaluation program on speech recognition (S., 1998), NIST developed the ROVER (Recognizer Output Voting Voting Error Reduction) (Fiscus, 1997), to produce a composite Automatic Speech Recognition system output from the output of several ASR systems. Such composite system has an error rate inferior to the one of any of its components. In the ROVER, the output of several ASR systems is first combined into a single transition network using a modified version of the dynamic programming alignment technique used by NIST to score ASR systems1. This network is then explored and a simple voting strategy (highest number of votes) is used to select the best scoring word at each decision point. In (Fiscus, 1997), NIST reports a incremental 5.6% Word Error Rate reduction (12.5% relative) using voting by frequency of occurrence and maximum confidence (the output of the ASR systems was annotated with confidence measures (Chase, 1997)). Concerning, POS tagging, the principle of combination has been used in the past by (Marquez and Padro, 1998) who combines two taggers to annotate a corpus and by (Tufis, 1999) who uses several versions of same tagger but trained on different data. The SCLITE tool is freely available from http://www.itl.nist.gov/iaui/894.01/software.htm 3. The GRACE POS tagging evaluation campaign and its data. GRACE(Adda et al., 1999) was the first large scale evaluation campaign for Part-Of-Speech tagging for the French language. It was part of the French program CCIIL (Cognition, Intelligent Communication and Language Engineering), jointlypromoted by the Engineering Sciences and Human Sciences departments of the CNRS. The call for tenders was published in November 1995 and the first year has been devoted to bootstrapping the program by defining and installing the different organization committees. From the participants point of view, GRACE was made of 3 phases: training, dry-run and test. The first one was used by the participant to calibrate their systems on untagged data, the two others were complete runs of the evaluation protocol were the participants had to tag a large amount of text and to provide a mapping between their tagset and the reference tagset which had been derived with their collaboration from the EAGLES format (Leech and Wilson, 1995). The training corpus was distributed globally to all the participants in January 1996, while the dry run corpus was distributed individually to each participant in an encrypted form during the fall of 1996. The results were discussed during a workshop restricted to the participants, a satellite event to the Journees Scientifiques et Techniques du Reseau FRANCIL, in April 1997 (Adda. et al., 1997). The test corpus was distributed in the same manner as for the dry run, at the end of December 1997. The preliminary results of the tests were discussed with the participants in a workshop in May 1998. The final results were disclosed on the WEB2 during fall of 1998 as soon as they had been validated by the organizers (cross validation with two different processing chains based on different algorithms and developed at two different sites) and the participants. At the beginning there were 18 participants from 5 different countries (CA, USA, D, CH, FR), from both public research and industry, and 3 evaluators EPFL, INaLF-CNRS and Limsi-CNRS.The 2 corpus providers were Limsi and INaLF. Out of the 21 initial participants, 17 only took part in the dry run and only 13 completed the tests. The size of the training corpus was around 10 million words of untagged texts, evenly distributed between literary works and newspaper articles. For the dry run, the participants tagged a corpus of roughly 450,000 words with a similar genre distributionand the performance measure was computed over 20,000 words to which a reference description had been manually assigned. For the tests, the participants had to mark a corpus of 650,000 words and the measure was taken over 40,000 words. GRACE used the quantitative black box metrics: Decision and Precision, which were derived especially for GRACE from the metrics used in Information Retrieval (Precision and Recall). Precision measures the ability of a POS tagger to assign a correct tag to a given word form, and Decision measures the capacity of a POS tagger to restrict for a given word form the number of candidate tags with respect to a given tagset. One of the lessons to draw from the GRACE experience, is that ideally, results should be cross-validated with two different processing chains, based on different algorithms http://www.limsi.fr/TLP/grace (when this is possible) and developed at two different sites in order to ensure their accuracy and quality. The evaluation toolkit of GRACE has been packaged as a demonstration by the ELSE project and is freely available3. GRACE proved to be a success; its results are: a better knowledge of the existing systems in each domain and of their state of development; precise evaluation metrics defined in collaboration with the participants; an evaluation toolkit freely available, a new product on the market (one participant decided to add a tagger to his catalogue as a result of his participation); the creation of a community of actors interested in evaluation; and last of all, the initial data to build the new linguistic resource described here.",LREC,2000.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
232d11a7a3181f071c4d82a3578930eff9af8d93,https://www.semanticscholar.org/paper/232d11a7a3181f071c4d82a3578930eff9af8d93,"Advances in Grid Computing - EGC 2005, European Grid Conference, Amsterdam, The Netherlands, February 14-16, 2005, Revised Selected Papers",,EGC,2005.0,10.1007/b137919,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a77cdf9be23720dacda79caf2fd2c342a383a3e4,https://www.semanticscholar.org/paper/a77cdf9be23720dacda79caf2fd2c342a383a3e4,"Future Generation Information Technology, First International Conference, FGIT 2009, Jeju Island, Korea, December 10-12, 2009. Proceedings",,FGIT,2009.0,10.1007/978-3-642-10509-8,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
eb29ce0e5b35246ddeae37bef671a4e793c5d70c,https://www.semanticscholar.org/paper/eb29ce0e5b35246ddeae37bef671a4e793c5d70c,Guest Editorial - Special Issue on biometric security and privacy,"Biometrics is the science of recognising individuals based on their behavioural and biological characteristics such as face, fingerprints, iris, voice, gait and signature. A typical biometric system may be viewed as a pattern classification system which utilises advanced signal processing schemes to compare and match biometric data. The past decade has witnessed a rapid increase in biometrics research in addition to the deployment of large-scale biometrics solutions in both civilian and law enforcement applications. Example applications that incorporate biometric recognition include: logical and physical access systems; surveillance operations to fight against fraud and organised crime; immigration control and border security systems; national identity programs; identity management systems, and determination of friend or foe in military installations. Since an individual’s biometric data is personal and sensitive, issues related to biometric security and privacy have been raised. These include (a) spoofing, where an adversary presents a falsified biometric trait to the system with the intention of masquerading as another person; (b) evasion, where a person attempts to obfuscate or modify a biometric trait in order to avoid being detected by the system; (c) database alteration, where the templates stored in a database are modified in order to undermine system integrity; and (d) template compromise, where the stored biometric data is perused or stolen and exploited for illegitimate means. The advent of cloud computing technology and personal mobile devices has broadened the application domain of biometrics; however, at the same time, it has brought to the forefront the need for dedicated security technologies to protect biometric data from being misappropriated and used for purposes beyond those intended. Similarly, the use of surveillance systems in public areas presents new challenges with respect to privacy. The research community has responded to these concerns with new security and privacy enhancement and protection technologies. There are numerous indicators of the increasing interest, e.g. a number of special sessions in conferences, evaluation campaigns, tutorials, large-scale collaborative projects and ongoing efforts towards standardisation. A number of signal processing methods have been developed to analyse the vulnerability of biometric systems and design solutions to mitigate the impact of these vulnerabilities. At the same time, privacy-preserving constructs have been developed by signal processing researchers in order to ensure that stored and/or transmitted biometric data is adequately protected from misuse. This special section was conceived to champion recent developments in the rapidly evolving field and also to encourage research in new signal processing solutions to security and privacy protection. After a rigorous pre-selection and peerreview process, eight articles were selected for inclusion in this special section. Brief summaries of each follow. The first contribution from Hadid, Evans, Marcel and Fierrez focuses on the security side of biometrics, providing a gentle introduction to spoofing and countermeasures and a methodology for their assessment. The paper also provides a case study in face recognition. The next contribution discusses how adversarial machine learning techniques can be harnessed to protect biometric systems from sophisticated attacks. Biggio, Fumera, Russu, Didaci and Roli argue that security is best delivered with adaptive, security-by-design solutions. Itkis, Chandar, Fuller, Campbell and Cunningham report the challenges in designing effective cryptosystems for iris recognition systems. Their work also illustrates the shortcoming of the more traditional performance metrics used in biometrics and promotes the use of a new entropy metric. Patel, Ratha and Chellappa’s contribution reviews different approaches to cancelable biometric schemes for template protection. The aim of such techniques is to preserve privacy by preventing the theft of biometric templates through the application of non-invertible transforms. Barni, Droandi and Lazzeretti describe a different approach to template protection based on cryptographic technology. They illustrate how secure, two-party computation and signal processing in the encrypted domain can be combined to enhance security and protect privacy. Still on the theme of template protection, Lim, Teoh and Kim describe their work on biometric feature-type transformation. Such transformations are typically used as a pre-cursor to many forms of biometric cryptosystems which demand specific input formats such as point-set or binary features. The final paper on template protection discusses the practical implications of biometric security and offers a fresh perspective to the problem. Nandakumar and Jain argue that improvements to security and privacy seldom come without degradations to recognition performance and that, consequently, there remains a significant gap between theory and practice. The special section rounds off with an article by Bustard on the privacy and legal concerns surrounding the collection, storage and use of personal biometric data. In particular, the article discusses recent European legislation on this issue and its potential impact on the adoption of biometrics technology.",,2015.0,10.1109/MSP.2015.2443271,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b2e829db145c7d4b1b6eb9ef7a4c2434a05e5d5e,https://www.semanticscholar.org/paper/b2e829db145c7d4b1b6eb9ef7a4c2434a05e5d5e,A Novel Approach To Distorted English Character Recognition Using Bank Propagation Neural Network,"A person’s learning of a new language starts with learning alphabets of the language. The validation of a learning process is its recognition under any circumstances. The application developed in this paper, is aimed at making the “recognition process of alphabets” after different distortions on the original structure and frame of the alphabet. The system creates the distorted alphabet with graphical transformations and recognition is done by a back propagation neural network. The graphical transformations include scaling, rotation and translation functions written in Open GL. The distorted character is recognized by the neural network coded in C#. The system can be deployed for any distorted image recognition application. The system has shown satisfactory performance for distorted character recognition. © 2013 Elixir All rights reserved. Elixir Comp. Sci. & Engg. 58 (2013) 14992-14996 Computer Science and Engineering Available online at www.elixirpublishers.com (Elixir International Journal) Nachamai M et al./ Elixir Comp. Sci. & Engg. 58 (2013) 14992-14996 14993 input value and second, the gradient of the activation function. The output of a BPNN is compared with the target output and an error is calculated for a training iteration. This error is then back propagated to the neural network and utilized to adjust the weights, thereby minimizing the mean squared error between the network’s prediction output and the target output. Consequently, the BPNN model yields predictive output that is similar to the target output. 2.. Methods And Materials Character recognition systems aim at attaining a system that can recognize a character when an exact match is found. Distorted character recognition happens to be a great challenge, as the character might completely be of a different structure which may not relate to the original form. The work in the paper is come with a new method for identifying distorted characters, the general flow is depicted in figure 2.1. Treating the characters as graphical elements on screen, the distortions are introduced through various graphical transformations. Then the neural network is deployed to identify the distorted character. Figure 2.1 General Flow of the System 2.1 Graphical Transformations The first phase of the work includes selecting a particular character on screen and applying various graphics primitives. The implementation of phase 1 is done using OpenGL [6]. The interface consists of over 250 different function calls which can be used to draw complex three-dimensional scenes from simple primitives. OpenGL's basic operation is to accept primitives such as points, lines and polygons, and convert them into pixels. This is done by a graphics pipeline known as the OpenGL state machine. Most OpenGL commands either issue primitives to the graphics pipeline or configure how the pipeline processes these primitives. The graphical transformations defined in the system are: • Translation • Scaling • Rotation on xaxis • Rotation on y-axis • Shearing • Reflection • Orthogonal projection The GUI of the system is depicted in figure 2.1.1. The Left panel has object propertieswhich include the color change option for the selected alphabet. In continuation left panel has the alphabets and rotation menu for X axis and Yaxis with directions – clock wise and anticlockwise. The Center region is the character display region. The right panel has the remaining alphabets. Figure 2.1.1 GUI of the system The alphabet “A” is clicked on the left panel and it appears on the character display region as shown in the figure 2.1.2. Figure 2.1.2 Alphabet Display The figure 2.1.3 shows the color menu that can be applied on the character selected. The system is not restricted to balck and white or gray sclae, it is vulnerable to color characters too. Figure 2.1.3 Color Menu. The color application of “blue” is shown in figure 2.1.4. Figure 2.1.4 Color Change on the Alphabet A sample demonstration of the transformations such as the anti clockwise rotation of the character in Y-axis, translation of the alphabet and scaling of the alphabet are depicted in figure 2.1.5, 2.1.6 and 2.1.7 respectively. 2.2 Character Recognition The recognition of character is done using the BPNN. The BPNN architecture comprises one input layer, many hidden layers and one output layer. The BPNN parameters include a number of hidden layers, a number of hidden neurons, an activation function, learning rate, momentum, etc. All of these parameters have significant impact on neural network performance. In most cases, one hidden layer is sufficient for Nachamai M et al./ Elixir Comp. Sci. & Engg. 58 (2013) 14992-14996 14994 computing arbitrary decision boundaries that can approximate any continuous function. Therefore, the number of hidden layers is usually set at 1. After performing some trials the final number of neurons in the hidden layer can be determined. Figure 2.1.5 Anti clock Wise Rotation of the Alphabet Figure 2.1.6 Scaling of the Alphabet Figure 2.1.7 Translation of the Alphabet Back propagation algorithms are generally termed to be slow in learning as there are many back-propagated values into the hidden layer. The back propagation algorithm used in this system uses a Resilient Propagation (RPROP) algorithm [7]. The RPROP adaptation of the weight-step is not “blurred” by gradient behavior; instead, each weight has an individual evolving update-value. [8] The weight-step is only determined by its update-value and the sign of the gradient. [9] RPROP requires no parameter tuning and learning. Adaptation is only affected by the sign of the partial derivative, learning is equally spread over the network and so RPROP is a fast learner. [10] The number of learning steps is significantly reduced due to a local learning scheme. The System GUI for the neural network is shown in figure 2.2.1. The learning step and recognition process are illustrated in the figure 2.2.2. Figure 2.2.1 BPNN GUI Figure 2.2.2 Recognized Alphabet 3. Experimental Result The system was tested on English alphabets grouped into two categories; the test set 1 was framed with the entire set of alphabets – all the 26 alphabets, the test set 2 was a random pick of any 10 alphabets. Each test set was tested in three different variations applied to. The levels of transformations were taken as standard transformation, medium transformation and high transformation. The standard transformation would be up to 30% transformations applied on the chosen character, all the transformations were applied one by one and only once to the chosen alphabet. The medium transformation was 60% and high transformation was up to 90%. The table 3.1 shows the results of recognition after application of each transformation on the alphabet and the total recognition score. The tables 3.2 and 3.3 show the recognition scores for the medium and high transformations applied. The tabulation result allows making a clear conclusion that the transformations such as scaling and translation, does not affect the recognition process. The column data for translation and scaling proves evidential for the above conclusion. The table 3.3 above shows the invariance for transformation lesser than 90%. The result analysis has identified that the shearing and projection of the alphabet has not proven to be a good take for the recognition. When an alphabet is put forth for a transformation of shearing or projection the structural frame of the alphabet changes, it might be the possible reason for the lesser recognition accuracy when compared to the other transformations applied. The graphical results for the different levels are shown in figure 3.1. The figure 3.1 depicts the transformations applied at the standard level, i.e. When translation is applied its maximum value is 30%, it’s evident from the graph the shearing recognition score is the lowest of all. The figure 3.2 shows the graph for medium level of transformations. Nachamai M et al./ Elixir Comp. Sci. & Engg. 58 (2013) 14992-14996 14995 Figure 3.1 Standard Transformations Figure 3.2 Medium Transformation The graph elucidates lucidly the recognition scores for standard and high is better for the Random dataset (RDS). The All alphabet dataset happens to be greater than RDS for the medium dataset. The better recognition in RDS might be possibly due to the reason that the dataset comprises of only lesser data in process, and it is random in nature. The advantage would also be the same; randomness in data pick is the way a real time system would operate. Figure 3.3 High Transformation The figure 3.3 shows the high transformation with the maximum value being 90%. The comparison of the recognition scores with the three levels of transformations is shown in the figure 3.4. Figure 3.4 Transformation Comparison Amongst Data Sets 4. Conclusion and Future Work The interface of the system is user friendly. The recognition process of the system is efficient with the steps involved in developing it. The system has explored the possibility of training on synthetically generated font data and the performance was good. The system can recognize distorted characters. The generation and manipulation of characters is two dimensional. Rotation of certain characters had to be blocked for visibility Table 3.1 Level standard -transformations applied lesser than 30% Translation Scaling Rotation X –axis Rotation Y-axis Shearing Projection Total Recognition score All 26 alphabets 100 100 99 99 87 86 95.17 Radom data pick(10 at a time) 100 100 100 100 88 89 96.17 Table 3.2 Level Medium transformations applied lesser than 60% Translation Scaling Rotation X -axis Rotation Yaxis Shearing Projection Total Recognition score All 26 alphabets 100 100 97 95 80 82 92.33 Radom data pick(10 at a time) 100 100 95 90 77 80 90.33 Table 3.3 Level High transformations applied lesser than 90% Translation Scaling Rotation X –axis Rotation Y-axis Shearing Projection Total Recognition score All 26 alphabets ",,2013.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fa240ca00c5ce39af54062b5385577dfea9bcc8d,https://www.semanticscholar.org/paper/fa240ca00c5ce39af54062b5385577dfea9bcc8d,Service science – the trend and the future core,"With the increasing emphasis on the service sector in global business, the rise of the new scholarly domain of service science became inevitable. This innovative research domain appears to be a promising discipline in the modern business environment. Although, the term service science was issued by IBM a mere 6 years ago, it rapidly took off as the Council on Competitiveness published the Palmisano Report in that same year, forecasting that services would become the motor of economic development in the years to come. Service science applies a wealth of scientific methods from various disciplines, including management science, cognitive science, social science, computer science, economics, and engineering. As the global service-based economy and its research fields expand, the services, the vehicle of future business growth, will and should be the focus of business-related fields. This special issue comprises papers from the INFORMS 2010 Service Science Conference, organized by the National Taiwan University of Science and Technology, and held on 7–10 July 2010, in Taipei, Taiwan. From 78 submissions, 19 papers having high evaluation scores were selected, and the authors were invited to submit an extended version of the conference paper for further review. After review and revision, seven papers were accepted for publication in this special issue. These papers describe a cross-section of practical application experiences and state-of-the-art research work using service science methodologies across a variety of industrial domains in several countries. Accordingly, outstanding and insightful results are reported in these papers. Recently, commerce-related subjects have stimulated the broader use of computer science along with other state-of-the-art technologies. Any method for analysis derived from the principles gains superiority over others. Accordingly, Mehravaran and Logendran proposed mathematical programming models for the bicriteria unrelated parallel machine scheduling problem with sequence-dependent setup times in a supply chain. Moreover, the authors also built a search algorithm to find optimal and near optimal solutions for the problem, which has been shown to be NP-hard. Their work indicates that their algorithm is capable of producing high quality solutions within a reasonable computation time. Owing to the dramatic expansion of computer science and Internet-based technologies, the management of renewable resources such as human resources may be estimated by IT systems as part of an IT infrastructure library. Consequently, the twin objectives of better fulfilling customer needs and maximizing the efficiency of resources can be achieved. Grabarnik and Shwartz’s paper describes an IT system-based approach for scheduling of requests for services in groups, which minimizes the duration of requests that use partially overlapping sets of resources. This article validates its empirical results by comparing them to existing open source schedulers, illustrating that their approach is superior. The preliminary integrated approach used by Grabarnik and Shwartz has thus created new insights for the discipline. With the proliferation of cutting-edge devices such as computers and laptops, e-learning, also known as online learning, has drawn global interest. Now a major social trend, the rapid evolution in e-learning has led to the use of portable communication devices, such as cell phones, in a new learning style, called mobile learning (m-learning). In their paper concerning m-learning, Crescente and Lee collaborate to examine this up-to-date learning style. The authors establish a theoretical framework for m-learning, and suggest that this field has great promise as an academic discipline. Their research in this rising discipline has laid an innovative foundation for future researchers in associated fields. Another excellent case of applying methods in the service sciences to better examine the service-oriented issues of the local cultural industry (LCI) in Taiwan is the work of Chen. His study in this volume not only emphasizes service quality improvement for LCIs, but also adopts an integrated method of the Malcolm Baldrige National Quality Award, the quality function deployment, along with the balanced scorecard, to meet the needs of creative, systematic, and customeroriented solutions. Since globalization and localization have flourished worldwide in recent years, distinct cultural and historical resources of certain regions have been exploited by a variety of manufacturing and service industries. The tourist industry, often an LCI, is a key industry in many nations. In Chen’s study, I-lan County was used as a reference for LCI in Taiwan. They concluded that",,2011.0,10.1080/10170669.2011.548922,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
68fdd90a6abc57d2fe62364574ba5642921f9b86,https://www.semanticscholar.org/paper/68fdd90a6abc57d2fe62364574ba5642921f9b86,Books in brief,,Nature,2012.0,10.1038/481143a,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ad0d71673f44c34067b23d0c8d130b19150dcce4,https://www.semanticscholar.org/paper/ad0d71673f44c34067b23d0c8d130b19150dcce4,"The Invention Bootcamp, a Four-Week Summer Course For High School Underrepresented Students in a University Setting","The Invention Bootcamp is a four-week interdisciplinary program where twenty-five high school students underrepresented in STEM (Science, Technology, Engineering and Math) are invited to discover and experience the worlds of engineering, innovation, and entrepreneurship in a college setting. The course creates, deploys and tests in the field a new educational approach to inspire future inventors. In addition to teaching STEM skills in a hands-on and collaborative manner, the course presents high school students with role models in the form of undergraduate mentors, instructors, researchers, and guest speakers in class and during field trips. The course thus helps empower them, helps them gain confidence in the classroom, but also experience a foretaste of being a college student. By the end of the pilot course in Summer 2016, we asked students if they felt they could be engineers or inventors in the future. A strong majority (91%) agreed they could. Several aspects of the bootcamp are unique, and we would like to share the key learnings. They include: 1) The application process, which was based on non-cognitive variables. No grades were required. Applicants needed to deliver a 2-min video showing their motivation and how they would improve their school cafeteria. Students needed to have a curiosity towards STEM fields and the invention process. A recommendation letter was also needed. 2) The population targeted, which is underrepresented students in STEM such as minorities, women, and low income students. 3) The hiring and training of eight undergraduate mentors and a mentor coordinator. We had one mentor per group of three high school students. The mentor program created a supportive environment to provide students with the emotional, academic and technical support they needed to be successful in this course. By offering close, near-peer support, we enhanced student learning, classroom effectiveness, and retention of students. The majority of mentors was in the classroom with students for the entire program. They all are engineering students with a strong engineering background, and a good attitude under stress and in groups. 5) The hands-on curriculum, that meshed engineering tools (soldering iron, milling machine, hand tools, laser cutter, 3D printer), visit of guest lecturers (local entrepreneurs and innovators), and work on group projects using a human-center design thinking approach.",,2017.0,10.18260/1-2--28983,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
38b9c7cf66607431ac6555a6381a4bfd82e82f0c,https://www.semanticscholar.org/paper/38b9c7cf66607431ac6555a6381a4bfd82e82f0c,Motivation and Context,"Published by the IEEE Computer Society 0018-9162/10/$26.00 © 2010 IEEE  Pay-as-you-go utility computing services by companies such as Amazon and new initiatives by Google, IBM, Microsoft, and the National Science Foundation (NSF) have begun to provide applications researchers in areas such as machine learning and scientific computing with access to large-scale cluster resources. However, system researchers, who are developing the techniques and software infrastructure to support cloud computing, still have trouble obtaining low-level access to such resources. Open Cirrus (http://opencirrus.org) aims to address this problem by providing a single testbed of heterogeneous distributed data centers for systems, applications, services, and open source development research. The project is a joint initiative sponsored by Hewlett-Packard (HP), Intel, and Yahoo! in collaboration with the NSF, the University of Illinois at Urbana-Champaign (UIUC), the Karlsruhe Institute of Technology (KIT), the Infocomm Development Authority (IDA) of Singapore, the Russian Academy of T here is growing interest in cloud computing within the systems and applications research communities. However, systems researchers often find it difficult to do credible work without access to large-scale distributed data centers. Application researchers could also benefit from being able to control the deployment and consumption of hosted services across a distributed cloud computing testbed. Open Cirrus is a cloud computing testbed that, unlike existing alternatives, federates distributed data centers. It aims to spur innovation in systems and applications research and catalyze development of an open source service stack for the cloud. Arutyun I. Avetisyan, Institute for System Programming of the Russian Academy of Sciences",,2010.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5f94aa14635c9b1ae49ffbc55dd33c8adc955f48,https://www.semanticscholar.org/paper/5f94aa14635c9b1ae49ffbc55dd33c8adc955f48,Design Specifications for the Scenario-Based Training Automated Collection and Evaluation System (SBT-ACES),"Evaluation is a powerful mechanism for realizing engaging, effective and efficient training. The data distilled from summative and formative evaluations can be used to iteratively improve synthetic learning environments, scenario-based content, and assessment tools, thereby enabling more personally meaningful learning experiences. In this paper, we leverage the science of training, psychometric theory, and artificial intelligence to inform the design of the Scenario-Based Training - Automated Collection and Evaluation System (SBT-ACES). SBT-ACES provides instructors with an integrated analytics toolset to facilitate the collection, manipulation, analysis and visualization of evaluative data from a federation of systems, titled the Instructional Support System (ISS). The ISS is a constellation of applications, databases and devices supporting Marine Air Ground Task Force (MAGTF) training via the Digital Virtual Training Environment (DVTE), a laptop-based simulation suite deployed worldwide. A machine learning component within the ISS can leverage post-synthesized SBT-ACES data to continuously tune training such that, over time, accumulated evidence enables content to be adaptively tailored to optimize trainees' learning experiences.",,2010.0,10.1177/154193121005402705,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
544884e39649edfe2b04fdde3cd107f52ba9a961,https://www.semanticscholar.org/paper/544884e39649edfe2b04fdde3cd107f52ba9a961,A GlObAl ClOud COmpuTinG TesTbed,"Published by the IEEE Computer Society 0018-9162/10/$26.00 © 2010 IEEE Pay-as-you-go utility computing services by companies such as Amazon and new initiatives by Google, IBM, Microsoft, and the National Science Foundation (NSF) have begun to provide applications researchers in areas such as machine learning and scientific computing with access to large-scale cluster resources. However, system researchers, who are developing the techniques and software infrastructure to support cloud computing, still have trouble obtaining low-level access to such resources. Open Cirrus (http://opencirrus.org) aims to address this problem by providing a single testbed of heterogeneous distributed data centers for systems, applications, services, and open source development research. The project is a joint initiative sponsored by Hewlett-Packard (HP), Intel, and Yahoo! in collaboration with the NSF, the University of Illinois at Urbana-Champaign (UIUC), the Karlsruhe Institute of Technology (KIT), the Infocomm Development Authority (IDA) of Singapore, the Russian Academy of T here is growing interest in cloud computing within the systems and applications research communities. However, systems researchers often find it difficult to do credible work without access to large-scale distributed data centers. Application researchers could also benefit from being able to control the deployment and consumption of hosted services across a distributed cloud computing testbed. Open Cirrus is a cloud computing testbed that, unlike existing alternatives, federates distributed data centers. It aims to spur innovation in systems and applications research and catalyze development of an open source service stack for the cloud. Arutyun I. Avetisyan, Institute for System Programming of the Russian Academy of Sciences",,2010.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f1a6bee3db1e8655f01e463a3344386da198e327,https://www.semanticscholar.org/paper/f1a6bee3db1e8655f01e463a3344386da198e327,C Ov Er F E at U Re Open Cirrus: a Global Cloud Computing Testbed C Ov Er F E at U Re,"Pay-as-you-go utility computing services by companies such as Amazon and new initiatives by Google, IBM, Microsoft, and the National Science Foundation (NSF) have begun to provide applications researchers in areas such as machine learning and scientific computing with access to large-scale cluster resources. However, system researchers, who are developing the techniques and software infrastructure to support cloud computing, still have trouble obtaining low-level access to such resources. Open Cirrus (http://opencirrus.org) aims to address this problem by providing a single testbed of heterogeneous distributed data centers for systems, applications, services, and open source development research. The project is a joint initiative sponsored by Hewlett-Packard (HP), Intel, and Yahoo! in collaboration with the NSF, the University of Illinois at Urbana-Champaign (UIUC), the Karlsruhe Institute of Technology (KIT), the Infocomm Development Authority (IDA) of Singapore, the Russian Academy of T here is growing interest in cloud computing within the systems and applications research communities. However, systems researchers often find it difficult to do credible work without access to large-scale distributed data centers. Application researchers could also benefit from being able to control the deployment and consumption of hosted services across a distributed cloud computing testbed. Open Cirrus is a cloud computing testbed that, unlike existing alternatives, federates distributed data centers. It aims to spur innovation in systems and applications research and catalyze development of an open source service stack for the cloud. Arutyun I. Avetisyan, Institute for System Programming of the Russian Academy of Sciences",,2010.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
88ea92d8b618525ff2a7815c08f0fc2c68d8daa2,https://www.semanticscholar.org/paper/88ea92d8b618525ff2a7815c08f0fc2c68d8daa2,Human Activity Inference via physical sensing in support of Industrial Equipment Maintenance,"The paper describes an active research project at Intel’s High Volume Manufacturing (HVM) facility located at Leixlip, Co. Kildare, Ireland. The project explores the practical aspects of deploying RFID transponders, subtle sensing platforms and machine learning based inferencing in a harsh, realworld environment. The key features of the sensing platform, the data collection process and the translation of data into information using visualization and inferencing techniques are described.",,2006.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
48953c5a0246187e7fcea94271acd9761ed17116,https://www.semanticscholar.org/paper/48953c5a0246187e7fcea94271acd9761ed17116,FOUR FACTOR AUTHENTICATION SYSTEM TEXT-BASED GRAPHICAL PASSWORD SCHEME,"Captcha is a security mechanism designed to differ-entiate between computers and humans, and is used to defend against malicious bot programs. Text-based Captchas are the most widely deployed differentiation mechanism, and almost all text-based Captchas are single-layered. Numerous successful at-tacks on the single-layer text-based Captchas deployed by Google, Yahoo! and Amazon have been reported. In new two-layer Captcha scheme was deployed in 2015 by microsoft. This appears to be the first application of two-layer Captchas. It is therefore natural to ask a fundamental question: is the two-layer Captcha as secure as its designers expected? Intrigued by this question, we have for the first time systematically analyzed the security of the two-layer Captcha in this paper. INTRODUCTION APTCHA(Completely Automated Public Turing Test to CTell Computers and Humans Apart) is used to prevent automated registration, spam or malicious bot programs [1], It automatically generates and evaluates a test, difficult for computers to solve, but easy for humans. If the success rate of solving a Captcha for humans reaches 90% or higher, and computer programs only achieve a success rate of less than 1%, the Captcha can be considered secure [3]. Current Captchas can be divided into three categories: text-based, image-based and audio-based. Text-based Captcha is usually based on English letters and Arabic numerals, and uses sophisticated distortion, rotation or noise interference to prevent the recognition of a machine. Compared to the latter two Captcha categories, text-based Captcha is the most widely used scheme This wide-spread usage is due to its obvious advantages [5], [6]: users’ task is a text recognition problem which is intuitive to users worldwide; globally, people can recognize English letters and Arabic numerals, thus text-based Captcha has few localization issues; at last, text-based Captcha Manuscript received May 11, 2016; revised October 11, 2016; December 13, 2016 and February 7, 2017; accepted March 1, 2017. Date of publication March 1, 2017; date of current version March 1, 2017. the National Natural Science Foundation of China (61472311) supported this work and the Fundamental Research Funds for the Central Universities. The authors are with the Institute of Software Engineering, Xidian Univer-sity, Xi’an, Shaanxi, 710071, P.R.China. ( e-mail: hchgao@xidian.edu.cn). Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/TIFS.2017. was the earliest form of Captchas and people are more willing to undertake this challenge compared to other Captcha forms. Security is the most significant concern for text-based Captchas. There have been numerous examples of Captcha failure, and many attacks have been proposed, noted in [7]= Researchers have recently claimed that their simple generic attacks have broken a wide range of text-based Captchas in a single step [10], [11]. Although many text-based Captchas have been proven insecure, the most recent studies [12], [13] suggest that Captcha is still a useful security mechanism, and the security of text-based Captchas is currently a hot topic in the academic field. With each failed Captcha scheme, Captcha designers accu-mulated experience. Then they designed better schemes, which aimed to improve both usability and security. These prior attempts also promoted a scientific understanding of Captcha’s robustness. As [6] suggested, the robustness of text-based Captchas should rely on the difficulty of finding where each character is (segmentation), rather than what each character is (recognition). Current machine learning algorithms, such as Neural Networks or K-Nearest Neighbors (KNN), can easily recognize distorted or rotated single characters correctly Therefore, text-based Captchas should be segmentation-resistant, and this principle has become the basis for designing text-based Captchas. Based on these findings, Microsoft deployed a two-layer Captcha in 2015. It connected the sides of each character, across both top-tobottom and right-to-left, in order to increase the difficulty of detecting where each character is. This was the first application of two-layer Captchas, and it appeared more secure than previous schemes. All attacks proposed were unsuccessful, including these powerful generic methods pro-posed in [10], [11]. It is therefore natural to ask a fundamental question: is the two-layer Captcha as secure as its designers expected? This open question precipitated our study. In this paper, we examine the security of the two-layer Captcha. We attack this scheme by a series of processing steps. First, a novel © 2018 JETIR March 2018, Volume 5, Issue 3 www.jetir.org (ISSN-2349-5162) JETIR1803118 Journal of Emerging Technologies and Innovative Research (JETIR) www.jetir.org 660 two-dimensional segmentation approach is proposed to separate a Captcha image along both vertical and horizontal directions, which helps create many single characters and is unlike traditional segmentation techniques. Second, we improved LeNet-5 [14], a radical Convolutional Neural Network (CNN) model, as our recognition engine. Our CNN model, as compared with the traditional template-based KNN, shows improved performance and effectiveness. Third, a Captcha generating imitator was designed which is able to automatically produce Captchas similar to the real-world ones deployed by Microsoft. We adopt it to generate a significant amount of data for our training sets to train the CNN. Then, the trained CNN is used to recognize the Captchas deployed in the wild. Our attack has achieved a success rate of 44.6% with an average speed of 9.05 seconds on a standard desktop computer (with a 3.3 GHz Intel Core i3 CPU and 2GB RAM). Judging by both criteria commonly used in the Captcha community [5], [9], we have successfully broken the twolayer Captcha deployed by Microsoft. This appears to be the first systematic analysis of two-layer Captchas. Our two-dimensional segmentation technique is innovative; it can be applied as a basis for other successful attacks. Additionally, our data preparation method, imitating the Captcha generation process to build training sets, and using it to train the recognition engine to recognize the objects in the real world, is the first attempt in the field of analyzing Captchas’ robustness. Our experiments demonstrate that it decreases time spent on data preparation and reduces manual labor. Also, this approach can be adapted for other projects requiring a large amount of data to train machine learning systems, and for people working in Captcha design, our work provides guidelines for designing Captchas with better security and usability. The remainder of the paper is organized into the following sections. A review of related work is provided in Section 2. Then, Section 3 introduces and analyzes Microsoft’s two-layer Captcha. Following that, the whole attack process is presented in Section 4, and Section 5 details the experiment process and presents attack results. In Section 6, we talk about other design choices, the applicability and novelty of this work, and also provide guidelines for designing Captchas with better security and usability. Finally, Section 7 concludes the paper with a discussion of the implications of our work EXISTING SYSTEM: security primitives hard was based on mathematical problems . emerging and exciting security was used in hard AI problems for new paradigm, but has been underexplored creating cryptographic primitives is Fundamental task in security which is based on hard mathematical problems that are computationally intractable.  limited success has been achieved by this pradigm as compared with the cryptographic primitives based on hard math problems and their wide applications.  Hard AI (Artificial Intelligence) problems used for security, initially proposed is an exciting new paradigm. Inviting puzzle is the most notable primitive which distinguishes human users from computers by presenting a challenge. OUR WORK New security primitive was presented by us which was based on hard AI problems, namely, a novel family of graphical password systems built on top of Puzzle technology, which we call Puzzle as graphical passwords (CaPRP). CaPRP is both a Puzzle and a graphical password scheme. security problems number was addressed by CaPRP combined altogether, such as online guessing attacks, relay attacks, and, if combined withdual-view technologies, shoulder-surfing attacks. Notably, a CaPRP password can be found only probabilistically by automatic online guessing attacks even if the password is in the search set. A novel approach was offered by CaPRP to address the well-known problem in image popular graphical password systems in hotspot image, such as PassPoints, that often leads to weak password choices. Panacea is not a CaPRP, but it also gifted a reasonable security and usability which was appeared to be suit well with few practical applications for developing online security.We present exemplary CaPRPs built on both text Puzzle and image-recognition Puzzle. One of them is a text CaPRP where in a password is a characters sequence like a text password, but entered by clicking the right sequence of character on CaPRP images. CaPRP offers protection against online dictionary attacks on passwords, which have been for long time a major security threat for various online services. This threat was floodedmore and considered as a top cyber security risk. Defense against online dictionary attacks is a more subtle problem than it might appear. It offers reasonable security and usability and appears to fit well with some practical applications for improving online security. This threat is widespread and considered as a top cyber security risk. Defense against online dictionary attacks is a more subtle problem thanit might appear. Puzzle Login(top of Puzzle technology Using m",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
71af6878cf6528e2df24a856809106059484e259,https://www.semanticscholar.org/paper/71af6878cf6528e2df24a856809106059484e259,Developments in Artificial Intelligence – Opportunities and Challenges for Military Modeling and Simulation,"One of the principal themes the NATO Science and Technology Organization (STO) is fostering in 2017 is ""Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)"". Simulation might play a significant role to play in these developments as it can act as a testbed for such concepts and support the military decision makers in future operations that are enhanced by AI. Simulation is already making a significant impact in the development of AI outside of the defence sector. Companies such as DeepMind and Nvidia are using computer games and simulations to “train” AI and autonomous systems, analogous to humans training in simulations. The rate of progress is high, driven by increases in computing power, availability of data and improved algorithms. AI can now “beat” humans at many computer and board games and is moving towards tackling more strategic games that have parallels with military C2. If such developments translate into the defence sphere then we could foresee humans and autonomous systems training in the same simulation systems, both separately and together, and the AI in the autonomous system being the same as that in the simulation. As autonomous systems proliferate across the nations, M&S technology and techniques might be used to improve the interoperability of autonomous systems. To maximise such synergies, it will be essential that NATO embraces all communities that have an interest in AI. Assessing the risks of potential adversary’s use of AI and commercial autonomous systems is also necessary. Despite recent advances, AI development still faces significant technological and ethical challenges and these must be monitored and addressed as necessary. 1.0 CONTEXT Artificial Intelligence or AI is a technology or concept that has developed over many decades and periodically becomes mainstream news. In the latter half of the 2010s this is still very much the case with regular forecasts of its impact on society, jobs and the world economy. Some of these predictions appear to be nearing reality and AI devices are even entering the home. AI also has the potential to influence and sometimes disrupt the ways that companies and organisations operate and AI-based technology revolutions are anticipated. AI also has enduring impact on media and culture and like much technology it can be considered to have beneficial and harmful uses and its impact has and will have political and ethical implications. Over the decades many AI predictions have come true to some degree but in some cases not at all or only partially. Good examples of this are to be found in transport where passenger aircraft have considerable levels of automation but society remains some way off from accepting pilotless passenger aircraft. For STO-MP-MSG-149 11 1 Developments in Artificial Intelligence – Opportunities and Challenges for Military Modeling and Simulation railways, some are now fully automated whilst others continue to put high reliance on the human. Cars can now park themselves and have high levels of automation but are yet to be fully autonomous in all environments and applications. There is no doubt however, that there is a trend towards greater use of autonomous systems and AI. This is being driven by ever greater processing power together with the ability for very large data sets (“big data”) to be captured and used to help build more capable AI. Such resources can also be accessed online in the cloud, driving down the cost of developing and distributing AI programs. The military have developed and deployed autonomous systems for a very long time, for example in the use of land and sea mines. In the 20th century proximity fuzes came into service that were semi-intelligent, sensing and exploding at the most appropriate time for the target. Analogue computers also assisted operators as part of fire control calculations and missiles and rockets in World War 2 become remotely piloted or fully autonomous. With the advent of digital computing, autonomy in military systems is commonplace, reducing the manpower requirement or in assisting the human, but there remains a significant ethical dimension in the use of fully autonomous systems. Developments in AI and autonomous systems outside of defence are of significant interest as they may provide answers how to better manage and interpret data within military command and control systems but also because they may enhance potential adversary’s capabilities. This was recognised in 2017 by the NATO Science and Technology Organization’s (STO) which made ""Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)"" one of its principal themes. The modelling and simulation (M&S) community has strived itself to develop AI, reducing or eliminating the need for human input. Sometimes termed Semi-Automated Forces (SAF) or Computer-Generated Forces (CGF) this has benefits in analysis and training, reducing the number of role players and improving consistency. Simulation itself is now being used to “train” AI/autonomous systems, as such environments are repeatable and controllable and can generate highly tailorable data output. However, reproducing credible and realistic behaviours in simulation remains a significant challenge and the M&S community continues to strive to enhance its AI. The computer games industry also sees AI as a challenge as games can easily lose their entertainment value if their AI is poorly implemented. 2.0 WHAT IS ARTIFICIAL INTELLIGENCE? Artificial intelligence (AI) is a broad topic area as it depends on what nature of human intelligence are being replicated and that AI technology can take many different forms. The Oxford Dictionary definition is “The theory and development of computer systems able to perform tasks normally requiring human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.” The nature of the intelligence can range from “narrow” intelligence which is highly tailored or specialised through to artificial “general” intelligence which is flexible, adaptive and inventive, much like the human brain. There are many approaches to AI for example, decision tress, fuzzy logic and neural nets with some approaches becoming synonymous with AI. For example, machine learning is an approach that gives ""computers the ability to learn without being explicitly programmed” by learning from and making predictions from data. In broad terms AI is the ‘what’, machine learning is an approach to the ‘how’, and self-driving cars might be the ‘why’. Machine learning methods are based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, partially supervised or unsupervised. Neural nets or networks are computer systems modelled on the human brain and nervous system with an interconnected group of nodes, akin to the vast network of neurons in a brain. Deep or reinforcement learning, which is inspired by the way animals seem to learn, has taken the neural nets approach and added layers of nodes taking advantage of current day higher processing power and making significant advances in image recognition for example and is generally seen as at the current forefront of AI technology. Developments in Artificial Intelligence – Opportunities and Challenges for Military Modeling and Simulation 11 2 STO-MP-MSG-149 An autonomous system builds on the use of AI and extends it into the physical world, in for example a robot or vehicle, requiring an awareness of the world through sensors, a task(s) and minimal human intervention. Some argue that some AI algorithms are not really showing intelligence but are a predetermined and limited set of responses to a predetermined and limited set of inputs. Professor Isbell of Georgia Tech suggests that systems should have two features before they can be considered AI. Firstly, they must learn over time as their environment changes. Secondly, their challenge must be demanding too for humans to learn, so a machine programmed to automate repetitive work would not be considered an AI system. Another example would be the AI in many computer video games; it may appear to represent human behaviour but this is preprogrammed and there is little or no learning over time. 3.0 HISTORICAL CONTEXT An overview of the general history of AI including work on AI by the M&S community will be provided to give context to the progress currently being made and the possible trajectory of the field into the future.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6f39f4c0a093bdd61b21e6007c7c70e097e8f6eb,https://www.semanticscholar.org/paper/6f39f4c0a093bdd61b21e6007c7c70e097e8f6eb,A Tale of Two Convergences : Applications and Computing Platforms,"There are two important types of convergence that will shape the near term future of computing sciences. The first is the convergence between HPC and Cloud platforms for science. The second is the integration between Simulations and Big Data applications. We believe understanding these trends is not just a matter of ideal speculation but is important in particular to conceptualize and design future computing platforms for Science. This paper presents our analysis of the convergence between simulations and big-data applications as well as selected research about managing the convergence between HPC and Cloud platforms. I. APPLICATIONS AND PLATFORMS: STATUS QUO On the one hand, there is a march towards the exascale computing platforms. On the other hand, there is a proliferation of software systems to support dataintensive applications with an accompanying cloud infrastructure of well publicized dramatic and increasing size and sophistication. Traditional simulations involve applications of differential equation based models that need very fine space and time steps and this leads to numerical formulations that need the memory and compute power of traditional HPC resources to solve individual problems (capability computing). A big data application does not typically need a full HPC system for However there are different types of parallelism that can/need to be exploited in order for data-intensive analytics to scale. Ref. [1] was an initial attempt to try to understand the convergence in high-performance computing and data-intensive computing. It was by necessity both high-level and broad reaching. In this paper we build upon initial work in Ref. [1] and we revisit the convergence problem by decomposing along two different trends: platforms (high-performance and cloud platforms) and applications (simulations and bigdata). Understanding these trends is important: (i) to engineer the platforms of the future, that might support both HPC and data-intensive problems, (ii) allow efficient sharing of large scale resources running simulations and data analytics; (iii) the need for higher performance Big Data algorithms; (iv) a richer software environment for research community building on many ”big data” tools, and (v) Facilitate a sustainability model for HPC, as it does not have resources to build and maintain a full software stack. II. UNDERSTANDING APPLICATIONS Needless to say there are many similarities between between data-intensive and simulation applications. Some high-level differences worth a brief mention are: • Classic Non-iterative MapReduce is major paradigm in data-intensive sciences, but it is not a common simulation paradigm except where ”reduce” summarizes pleasingly parallel execution as in some Monte Carlo simulations • Data intensive applications often have large collective communication, whereas classic simulation has a lot of smallish point-to-point messages which motivates the MapCollective model • Simulations tend to need high precision and very accurate results (partly because of differential operators), however, dataintensive problems often don’t need high accuracy as seen in trend to low precision (16 or 32 bit) deep learning networks, as there are no derivatives and the data has inevitable errors. In order to understand and analyze systematically these differences we examined extensively the landscape of applications across the HPC and dataintensive spectrum. For example in Ref. [?], [2] on examining applications with common characteristics, we introduced the concept of Ogres, and 64 Convergence Diamonds (features). Ogres provide a means of understanding and characterizing the most common application characteristics found across the two paradigms. Ogres provide a classification and structure including, (i) classic MPI-based simulations, (ii) pleasingly parallel and workflow systems, and (iii) data-intensive applications epitomized by deep learning. Full details of Ogres and their facets can be found in Ref. [2]. We introduce four Ogres views — classification dimensions or features. These views are: 1) Problem Architecture: Related to the machine architecture needed to support application and describes properties of problem such as Pleasing Parallel or Uses Collective Communication. 2) Execution View: Describes issues such as I/O versus compute rates, iterative nature and regularity of computation and the classic Vs of Big Data defining problem size, rate of change, etc. Execution facets allow the separation of ”Data” and ”Model” for both simulations and data-intensive applications. 3) Data Source and Style views include specifying how the data is collected, stored and accessed. For example: Streaming, files versus objects, HDFS vs. Lustre 4) Processing view describe types of processing steps including nature of algorithms and kernels used by model e.g. Linear Programming, Learning, Maximum Likelihood, Spectral methods, Mesh type. It incoporates aspects of key simulation kernels and in particular includes facets seen in NAS Parallel Benchmarks and Berkeley Dwarfs Of course there are other ways of looking at the Ogres and our work should be treated as an initial suggestion for further discussion. Comparison between Data Intensive and Simulation Problems It is useful to understand the aspects of data-intensive applications that are unique and those that are similar to traditional compute-intensive simulations. In general, data-intensive applications are generally more heterogeneous than computeintensive simulation problems. Typically a data pipeline (or workflows) comprises of multiple steps: data ingest, transfer, pre-processing, several rounds of processing (e. g. for cleaning, fusing, computation of summary statistics) and advanced analytics. Each step of the pipeline can be characterized according to computational characteristics facet: (i) by the size of the input, intermediate and output data, (ii) data access pattern (sequential, random) and (iii) computational characteristics (e.g. the parallelisms deployed). The analytics part of the pipeline is compute-intensive and thus, resemble many characteristics of traditional simulations problems. For example, many analytics and machine learning problems can be formulated with linear algebra or n-body (see seven giants [3]). Thus, analytical kernels (e. g. linear algebra libraries, such as BLAS, SCALAPACK) provide the basis for data analytics. For example, machine learning algorithm, such as SVM or principal component analysis (PCA) rely on dense and sparse linear algebra. Often these analytical kernels are implemented using low-level libraries using finegrained, tightly coupled parallelism often implemented with MPI, which yield into better performance than shoehorning the problem into a rigid MapReduce programming model. However, there is also a lack of scalable analytics algorithms that are able to operate on high-dimensional, sparse datasets. We use Ogres (facets) to facilitate this comparision. There are some clear similarities: Embarassingly parallel, BSP and SPMD are common in both arenas. However, the Classic MapReduce architecture is a major Big Data paradigm, but has much less common in simulations with one example between the execution of multiple simulations (as in Quantum Monte Carlo) followed by a reduce operation to collect the results of different simulations. The Iterative Map-Collective architecture is common in Big Data analytics, such as in clustering where there is no local graph structure and the parallel algorithms involve large-scale collectives but no point to point communication. The same structure is seen in N-body (long range force) or other “all-pairs” simulations without the locality typical from discretizing differential operators. Many simulation problems have the Map-Communication architecture with numerous small point-to-point messages coming from local interactions between points defining system to be simulated. The importance of sparse data structures and algorithms is well understood in simulations and is seen in some Big Data problems such as PageRank, which calculates the leading eigenvector of the sparse matrix formed by internet site links. Other Big Data sparse data structures are seen in user-item ratings and bags of words problems. Most items are rated by few users and many documents contain a small fraction of the word vocabulary. However important data analytics involve full matrix algorithms; for example recent papers [4], [5] on a new MultiDimensional Scaling method use conjugate gradient solvers with full matrices. Note that there are similarities between some Big Data graph problems and particle simulations with an unusual potential defined by the graph node connectivity. Both use the Map-Communication architecture and the links in a Big Data graph are equivalent to strength of force between the graph nodes considered as particles. In this analogy, many Big Data problems are “long range force” corresponding to a graph where all nodes are linked to each other. As in simulation cases, these O(N2) problems are typically very compute intense but straightforward to parallelize efficiently. It is interesting to consider the analogue of the “fast multipole” methods for the fully connected Big Data problems which can dramatically improve the performance to O(N) or O(N log N). Finally note the network connections used in deep learning are sparse but in recent image interpretation studies [?], the network weights are block sparse (corresponding to links to pixel blocks) and can be formulated as full matrix operations with GPUs and MPI running efficiently with these blocks. The above discussion focuses on a qualitative comparison of Big Data applications with traditional simulation (HPC) applications visualization, comparing the structure. As shown here there are similarities as well as points of distinction. It is likely however, that there will be significant differences in the “computational feature” fa",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fa48c911b1a390c058d9c39536aa0529c576c2ff,https://www.semanticscholar.org/paper/fa48c911b1a390c058d9c39536aa0529c576c2ff,TextureCam: Autonomous Image Analysis for Astrobiology Survey,"Introduction: The TextureCam concept is a “smart camera” that aims to improve scientific return by increasing science autonomy and observation capabilities both when the spacecraft is stationary and when it is traversing. This basic onboard science understanding can summarize terrain encountered during travel [1] and direct autonomous instrument deployment to targets of opportunity [2]. Such technology would assist Mars sample caching or return missions where the spacecraft must survey vast areas to identify potential sampling sites and habitat indicators. The instrument will use texture channels to differentiate and map habitat-relevant surfaces. Here we use the term texture not in the geologic sense of formal physical properties, but rather in the computer vision sense to mean statistical patterns of image pixels. These numerical signatures can in turn distinguish geologically relevant elements such as roughness, pavement coatings, regolith characteristics, sedimentary fabrics and differential outcrop weathering. Similar algorithms can perform microstructure analysis and sedimentology using microscopic images. On the scale of meters, surfaces and features can be recognized to summarize rover traverse and draft surficial maps for downlink. Reliable image classification in field conditions is difficult due to factors like variable lighting, surface conditions, and sediment deposition. Future development aims for reliable recognition of basic geological elements. Rad-hardened FPGA processing will instantiate these general-purpose texture analysis algorithms as part of an integrated, flight-relevant prototype. Experimental setup: An initial test demonstrates automatic recognition of stromatoform structures in outcrop. These surfaces have distinctive laminar textures that may indicate previous biogenic activity [3] and would be valuable targets for followup investigation and human review. While it would be unlikely to find such features exposed on the Mars surface, they provide a controlled test to guide initial development and evaluation. We use a machine learning approach, training the system on example pixel labels supplied by the designer. This yields a statistical model of pixel relationships that can classify new surfaces (Figure 1). Stromatolite images were acquired from the Pilbara formation [3]. We consider three outcrops of laminar structures with conical and wavy morphologies. We also include a scene containing much finer-scale localized egg carton laminations. These all show a common lay100 200 300 400 500 600 50",,2012.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c72de50fa5150eb6669bd81c3526e851ee0e9a1c,https://www.semanticscholar.org/paper/c72de50fa5150eb6669bd81c3526e851ee0e9a1c,OFA Workshop 2022 Abstracts,"The FSDP office hours will have knowledgeable and experienced people on hand to guide people through the various steps necessary to make use of the FSDP cluster. This includes account creation, vpn setup, access to the builder machine, access to the beaker web interface, installation of the beaker command line tool, creation of simple automated tests, running of those tests, running of manual tests, and setting up CI pipelines for upstream repositories. high- performance CPU/GPU architectures to efficiently support large-scale distributed training. We also highlight some of our co-design efforts to utilize MPI for large-scale DNN training on cutting-edge CPU/GPU architectures available on modern HPC clusters. The tutorial covers training traditional ML models including—K-Means, nearest neighbours—using the cuML framework accelerated using MVAPICH2-GDR. Also, the tutorial presents accelerating GPU-based Data Science applications using MPI4Dask, which is an MPI-based backend for Dask. Throughout the tutorial, we include hands-on exercises to enable attendees to gain first-hand experience of running distributed ML/DL training and Dask on a modern GPU cluster. (DPU) programmable ARM cores in the network adapter. This technology allows part of a middleware to be executed on the DPU. Such flexibility allows overlap of computation and communication across host and DPU cores. This talk will focus on how to offload MPI non-blocking collectives into DPU to accelerate MPI applications. Examples from the MVAPICH2-DPU library together with performance evaluation results for a set of non-blocking collectives and applications (such as P3DFFT) on a 1,024 process environment with Bluefield-2 will be presented. Schemes to offload different parts of Deep Learning Frameworks with the DPU technology will also be highlighted. Performance evaluation results for offloading ResNet-50 training with CIFAR-10 dataset on Bluefield-2 adapters will be presented. on the other hand, fall into the realm of communication APIs use to access NICs and other network hardware. This talk will discuss the impact making the libfabric communication API aware of heterogenous compute devices, such as CPUs and GPUs, collectively referred to as XPUs. It will discuss the features needed by both the communication and computation APIs in order to support high-performance applications needing both scale-up and scale-out support in a device agnostic fashion. configurations, ensuring NVMe and NVMe-oF environments can be represented entirely in Swordfish and Redfish environments. Our work discovers vulnerabilities in the InfiniBand architecture. We show that any system that tries to make use of RDMA opens an attack surface allowing local users to bypass the security mechanisms of the operating system and its kernel. Importantly, we demonstrate how any unprivileged user can inject packets into RDMA connections created on a local network controller, even if they are created in kernel space. Therefore, any kernel application that makes use of RDMA opens an attack surface allowing the attacker to manipulate the kernel-level applications from user space by injecting RDMA requests into their connections. As an example, we show how the adversary can bypass security mechanisms of operating and file systems to directly manipulate NVMe disks at the block level without administrative privileges by manipulating NVMe-oF kernel modules. functions. The proposed hint design is composed of service-granularity and function-granularity hints for achieving varied optimization goals and reducing design space for further optimizing the underneath RDMA communication engine. We co-design a key-value store called HatKV with HatRPC and LMDB. The effectiveness and efficiency of HatRPC are validated and evaluated with our proposed Apache Thrift Benchmarks (ATB) and YCSB workloads. Performance evaluations show that the proposed HatRPC approach can deliver up to 55% performance improvement for ATB benchmarks compared with other state-of-the-art RDMA protocols. In addition, the co-designed HatKV can achieve up to 85.5% improvement for YCSB workloads. by PCIe 6.0 intelligence/machine and high-performance on the adoption of PCIe 4.0 technology in data center and the PCIe 5.0 Compliance Program. The deployment of the HPE Slingshot interconnect technology on upcoming exascale systems such as Frontier at OLCF and El-Capitan at LLNL drives a need for a thorough analysis of the Slingshot networking ecosystem. MPI libraries have been extensively optimized over the years for an underlying Infiniband networking environment. The interconnect between nodes heavily influences performance and scalability of communication, motivating a need for a thorough evaluation of MPI-level performance on a system connected by slingshot interconnect technology. In this work, we present a detailed analysis of the performance of various MPI and communication libraries on the early access Spock system at OLCF with nodes connected by Slingshot-10. We evaluate point-to-point and collective performance using MVAPICH2, OpenMPI + UCX, Cray MPICH, and RCCL on AMD MI100 GPUs and AMD Epyc Rome CPUs. operations. We present our early experience and proof-of-concept framework that provides a vendor-agnostic smartNIC storage API, to present block storage to bare metal hosts, virtual machine hosts, and kubernetes containers. of and control of network speed when to CPU frequencies. In-network compute alleviates the host CPU load by running tasks directly in the network, additional computation/communication overlap and potentially improving overall application performance. It a processing process We investigate the specialties that a sPIN NIC high-performance, low-power, and flexible packet We PsPIN, a first open-source sPIN implementation, based on a multi-cluster RISC-V architecture and designed according to the identified architectural specialties. We investigate the performance of PsPIN with cycle-accurate simulations, showing that it can process data at full network speed for several use cases, introducing minimal latencies (26 ns for 64 B packets) and occupying a total area of mm 2 (22",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6d1a2b1d238dcd410e746f8be2dcc109a62b6347,https://www.semanticscholar.org/paper/6d1a2b1d238dcd410e746f8be2dcc109a62b6347,"Adaptive Sky: A Feature Correspondence Toolbox for a Multi-Instrument, Multi-Platform Distributed Cloud Monitoring Sensor Web","The current suite of spaceborne and in-situ assets, including those deployed by NASA, NOAA, and other groups, provides distributed sensing of the Earth's atmosphere, oceans, and land masses. As part of an activity supported through NASA's Earth Science Technology Office (ESTO), we have developed techniques that enable such assets to be dynamically combined to form sensor webs that can respond quickly to short-lived events and provide rich multi-modal observations of objects, such as clouds, that are evolving in space and time. A key focus of this work involves relating the observations made by one instrument to the observations made by another instrument. We have applied approaches derived from data mining, computer vision, and machine learning to automatically establish correspondence between different sets of observations. We will describe a number of Earth science scenarios that were used to direct this development and which have benefited from the approach.",2008 IEEE Aerospace Conference,2008.0,10.1109/AERO.2008.4526451,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
598e392add0580da30ae50559e8361df27e34d0d,https://www.semanticscholar.org/paper/598e392add0580da30ae50559e8361df27e34d0d,Emotional System for Military Target Identification Prof,"The thought of man-made systems and machines having emotions sounds like science fiction, however, few decades ago the idea of machines with intelligence seemed also like fiction, but today we are developing intelligent machines and using them successfully in different applications. Would we accept the idea of machines that could “feel”? What role would emotions play in machine learning and decision making? How can we model artificial emotions within intelligent systems? Can a machine’s decision capability be improved if it had emotions? These are questions one may ask when hearing that machines may have emotions, albeit artificial emotions. In this paper, we discuss these questions, and review briefly some of the latest works on modeling emotions within intelligent systems; including our own model that is based on an emotional neural network (EmNN). The EmNN has emotional neurons, weights, and two embedded emotional responses; anxiety and confidence. These emotional parameters are updated during task learning, and used during decision making. The paper will also present an application of the EmNN to military target identification, in addition to discussing the potential of using the emotional system to improve information exploitation. 1.0 INTRODUCTION In our daily lives, the amount of information we receive, perceive and then react to is tremendous. Much of our reaction to input information is formed into decisions that we make. What makes us act in a certain way, or decide for, or against an act has its roots back in our previous experiences. Whether we choose anchovies or pineapple on our pizza is a decision we make based on a previous experience with these flavors that leads us to decide what to have. Such a simple process of deciding on our favorite topping is as important and complicated as the more critical decisions we need to make throughout our lives. All decisions involve a learning process, resulting in association, classification and then decision making. During the learning process information which can take different forms, is exchanged between our natural sensors and the main processor; the brain. This sounds very technical and parallel to describing a computing system. However, computing systems lack one aspect of a human processing system; emotions. Over the past decades machines and systems have been developed and deployed to aid us in decision making and taking action; spanning application areas from simple electronic toys, industry and manufacturing, intelligence and security, to more complicated tasks in medicine, military applications, and space navigation. As the creators (designers) of these machines, we aim to assure that the action or the decision taken by the machine is correct and complies with our way of making a decision. In simplistic terms, we tend to create machines that would make the decisions on our behalf, and as information age progresses, and powerful high-tech systems grow even faster, our expectations of the machines are increasing. Most of the systems that we develop, and use to make decisions on our behalf, do not go through the learning process and the experiences that we possess. With the exception of some artificial intelligent systems that could interact with their input stimuli, and adapt their output or decision accordingly, systems Emotional System for Military Target Identification 18 2 RTO-MP-IST-087 UNCLASSIFIED/UNLIMITED UNCLASSIFIED/UNLIMITED and machines rely entirely on a set of commands that we provide to govern their actions, and this has been fine until our demands started requiring more complicated decisions by machines. Therefore, more and more intelligent systems are being developed, based in particular on neural networks which form the brain of a machine. These systems imitate our learning process and decision making by repeatedly exposing a neural network to examples of input information and its corresponding output, response, action or decision; this process models the previous experience process in humans. The neural network-based systems have been popularly used, and have shown success in various application areas, where association, classification and decision making can be obtained based on accumulated memory of past experiences. Despite the success of such intelligent systems, there has been a major and vital difference between a human decision-maker and a machine decision-maker, namely emotionwe have it, and machines do not. The idea of machines with affection or feelings is controversial, and some works expressed doubt about this idea [1,2], however, the concept of machines with emotions has lately attracted the attention of many researchers, and is currently gaining momentum with novel architectures emerging to artificially model emotions in one way or another. Recent definitions of emotion have either emphasized the external stimuli that trigger emotion, or the internal responses involved in the emotional state, when in fact emotion includes both of those things and much more [3]. The effective role of emotions on cognitive processing, learning and decision making in animals and humans has been emphasized by several researchers [4-8]. Emotions play an important role in human decision-making process, and thus they should be embedded within the reasoning process when we try to model human reactions [9]. Although computers and machines do not have physiologies like humans, information signals and regulatory signals travel within them; there will be functions in an intelligent complex adaptive system, that have to respond to unpredictable, complex information that play the role that emotions play in people [1]. Such computers will have the same emotional functionality, but not the same emotional mechanisms as human emotions. We may think of machine emotions as machine intelligence; we do not expect machines to “feel” the way we feel, but we could simulate machine emotions just as we do machine intelligence [9]. There have been examples of research works that attempted to incorporate emotions in machines in one way or another [9-20]. It was concluded from these works that if emotions such as anxiety, fear, and stress are included in systems that aim to simulate the human behaviour in certain circumstances, the system will be more user-friendly and its responses will be more similar to human behaviour. Other recent research works suggested the use of emotional components within neural models and control systems. For example, Abu Maria and Abu Zitar [21] proposed and implemented a regular and an emotional agent architecture which is supposed to resemble some of the human behaviours. They noticed that artificial emotions can be used in different ways to influence decision-making. Gobbini and Haxby [22] proposed a model for distributed neural systems that participate in the recognition of familiar faces, highlighting that this spatially distributed process involves not only visual areas but also areas that primarily have cognitive and social functions such as person knowledge and emotional responses. Coutinho and Cangelosi [7] suggested the use of modelling techniques to tack into the emotion/cognition paradigm, and presented two possible frameworks that could account for their investigation, one of which explored the emergence of emotion mechanisms. Most of these previous attempts on incorporating emotions in to machine learning have shown successful results, and provided a positive trend to developing machines with emotions, albeit simulated. Lately, we proposed an emotional neural network (EmNN) which was based on the novel emotional back propagation (EmBP) learning algorithm [23], and used it to solve a facial recognition problem. In other works [24,25], we explored the potential of using emotional neural networks in different tasks, such as Emotional System for Military Target Identification RTO-MP-IST-087 18 3 UNCLASSIFIED/UNLIMITED UNCLASSIFIED/UNLIMITED more complicated face recognition tasks and in blood cell identification. The difference between an emotional system and the more traditional approaches; including intelligent systems, is the simulated artificial emotions of the system. These additions have several advantages over traditional approaches. The embedded artificial emotions narrow the gap between humans and systems; thus instead of “human and machine interaction” we have “human and human-like machine interaction”. The closer and more coherent communication of information between humans and emotional systems has the advantage of faster communication, since both systems (human and machine) have emotions. This is not the case with traditional systems, where often a human operator perceives information and makes decisions which could differ due to his/her emotions. In this paper, we present the emotional neural network (EmNN) and describe its emotional parameters. The EmNN will also be applied to identify potential military targets, such as navy ships, helicopters, jetfighters, tanks and other assorted military vehicles. One of the aims of this work is to mimic the way a human would recognize these targets, by: firstly, using different images of targets with random orientations, angles, and backgrounds, secondly, avoiding complicated image pre-processing phases, and using only global image pattern averaging to simulate a human’s “glance” or quick look at a target image, and finally, using the EmNN to perform the target identification, by repeatedly exposing random target images to the network during its training phase; this process simulates the human “getting familiar” with the objects, without the need to look into edges, local features, angles or colors of a potential target. The paper is organized as follows: Section 2 presents the EmNN and describes the differences between conventional neural networks and emotional neural networks. Section 3 presents the application of the EmNN to military target identification, describin",,2009.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3de2f24844da58c0d94ee7c021e3e7d4ea2fb7f2,https://www.semanticscholar.org/paper/3de2f24844da58c0d94ee7c021e3e7d4ea2fb7f2,NERSC “Visualization Greenbook” Future Visualization Needs of the Doe Computational Science Community Hosted at NERSC,"In this paper we present the findings and recommendations that emerged from a one-day workshop held at Lawrence Berkeley National Laboratory (LBNL) on June 5, 2002, in conjunction with the National Energy Research Scientific Computing (NERSC) User Group (NUG) Meeting. The motivation for this workshop was to solicit direct input from the application science community on the subject of visualization. The workshop speakers and participants included computational scientists from a cross-section of disciplines that use the NERSC facility, as well as visualization researchers from across the country. We asked the workshop contributors how they currently visualize their results, and how they would like to do visualization in the future. We were especially interested in each individual's view of how visualization tools and services could be improved in order to better meet the needs of future computational science projects. The outcome of this workshop is a set of findings and recommendations that are presented in more detail later in this paper, and are briefly summarized here. Scientific visualization is a crucial technological capability that plays an important role in understanding data created by computational science projects as well as experiments. In order to be effective, visualization technology should be easy to use for a non-expert. The term “easy to use” encompasses a number of different categories, including a short learning curve, tight integration with computational frameworks, availability on the desktop as well as the fixed visualization facility, tools that are tailored for each specific application domain, and low cost. Current visualization tools fall short in several key areas of capability. Few visualization tools are capable of processing large datasets, such as those commonly generated at NERSC. Better support for parallel visualization tools may prove useful in leveraging large parallel machines as visualization resources. Multivariate visualization - multiple grids, many species, and many dimensions - is needed in order to quickly gain insight into large datasets. Related “drill-down” capabilities, such as the ability to quickly move from macro to micro views (used in “data mining”), would be extremely helpful in understanding data but are missing from most visualization tools. Many application scientists perceive a conundrum when it comes to visualization support. Support for visualization within each individual program level is often inadequate or nonexistent due to funding constraints, yet support for visualization at the institutional level is also often inadequate or nonexistent. Better solutions are needed for remote visualization. Current approaches are further constrained by network bandwidth and access to resources. The proliferation of visualization tools and data formats poses challenges. Researchers must often master many different tools in order to achieve the desired results. Data format conversion is often required when moving between tools. Common data formats and frameworks for visualization tools are needed to reduce duplication of effort and better promote sharing of resources and results. Better communication is needed between the visualization and computational science communities. The computational scientists are often unaware of current trends and practices in the visualization community. By being more aware of the needs of the computational science community, the visualization research programs can be crafted so as to be more responsive to their needs. As a result of the workshop, we have developed a set of recommendations that can be summarized as follows: • Establish a coherent program that focuses on remote visualization. A remote visualization program should provide tools and infrastructure that can be used by multiple “virtual teams”. • Establish mechanisms whereby generally-applicable visualization technology is developed and deployed in a centralized fashion. • Develop a research program in interactive visualization with running codes that stresses the integrated design and development of coupled simulation-visualization methods. • Establish a research program in the areas of multi-field visualization and multi-dimensional data visualization. • Establish a research program in the area of automated data exploration for next generation petascale datasets.",Int. J. High Perform. Comput. Appl.,2002.0,10.1177/1094342003017002001,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
78ed081186954487f7803ef3a863b4c324ddf054,https://www.semanticscholar.org/paper/78ed081186954487f7803ef3a863b4c324ddf054,Exploring the Activity Fabric of an Intelligent Environment with Hierarchical Hidden Markov Theory,"The Internet of Things (IoT) was designed for widespread convenience. With the smart tag and the sensing network, a large quantity of dynamic information is immediately presented in the IoT. Through the internal communication and interaction, meaningful objects provide real-time services for users. Therefore, the service with appropriate decision-making has become an essential issue. Based on the science of human behavior, this study employed the environment model to record the time sequences and locations of different behaviors and adopted the probability module of the hierarchical Hidden Markov Model for the inference. The statistical analysis was conducted to achieve the following objectives: First, define user behaviors and predict the user behavior routes with the environment model to analyze user purposes. Second, construct the hierarchical Hidden Markov Model according to the logic framework, and establish the sequential intensity among behaviors to get acquainted with the use and activity fabric of the intelligent environment. Third, establish the intensity of the relation between the probability of objects’ being used and the objects. The indicator can describe the possible limitations of the mechanism. As the process is recorded in the information of the system created in this study, these data can be reused to adjust the procedure of intelligent design services. Keywords—Behavior, big data, hierarchical Hidden Markov Model, intelligent object. I. RESEARCH OBJECTIVES AND BACKGROUND HE ways in which an Intelligent Environment offers its service have varied from the traditional ones, which require users to request services in various ways. With advancement in network technologies, various convenient services are being rolled out. The traditional, passive intelligent environments have many devices of their own with particular ways of use; therefore, those who do not know how to use them cannot benefit from the services offered. An excellent intelligent environment can improve this situation by using context-aware devices and various inference systems to determine the service a user may need and provide it to him/her automatically. In the past, the user requested a service while the environment was in a passive mode; things have changed with an intelligent environment that turns itself from a passive environment to an active one. Services are provided anywhere automatically at proper times upon the decision, made by the system, of what a user may need. Therefore, the intelligent environment is the road map to the future. The system has controls over the Chiung-Hui Chen is with the Department of Visual Communication Design, Asia University, Taichung 41354, Taiwan (e-mail: 7451616@gmail.com). information about the user and environment through context-aware devices. With this information, a user can benefit from the best services optimized for the environment. An intelligent environment usually includes furniture, electrical appliance, materials and spaces provided for human activities, and deployed sensors that detect user behaviors and environmental changes, and activate specific services in response to corresponding events and things that are triggered. An intelligent environment requires sufficient user information to provide proper services; however, perceiving a user’s intention in a behavior from the user’s status information is the most important part of the job. In other words, as long as the intention of the user is confirmed, it will make it simpler for an inference system to search for the service that suits the user’s status. The better-known human behavior identification is mostly performed with image processing techniques, plus rule-based inference system: the system identifies according to user’s current behavior and then creates a semantic model based on logic model of the behavior. The solution offered by this type of research often contains few simple analyses for actions [13]. User behavior must be analyzed and his/her intent understood in order to provide optimum user information to an inference system. However, as the sensors deployed in an intelligent environment increase dramatically, various sorts of disorganized big data gather. Depending merely on specification logic built upon experts’ knowledge is not good enough to infer an event. Instead, Machine Learning and big data analysis should be adopted to explore the connection between data and event. Furthermore, IoT began mainly as an idea of providing the benefits of ubiquitousness and convenience. Massive dynamic messages are presented in the IoT environment in real time while the objects of content provide the real time service to the user through internal communication and interaction. Thus, providing services based on informed decision has become an important issue. Collection of user’s status information by the environment is mandatory for both rule-based and probabilistic inference systems; therefore, such a collection is the key to a successful inference of the system for a proper service. In consideration of the above factors, this study employs Praxeology as the basis, combined with environment model in reference to the probability module of Hierarchical Hidden Markov Chain Model. It aims at making inference through the process of data analysis and documenting when and where various human behaviors occur, to achieve the following objectives: 1. Define user behavior, predict user behavior path Exploring the Activity Fabric of an Intelligent Environment with Hierarchical Hidden Markov Theory",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7b590cad93653aee70915e105612492d70c591de,https://www.semanticscholar.org/paper/7b590cad93653aee70915e105612492d70c591de,Fall 2010 Colloquium,"Siemens is a global powerhouse in electronics and electrical engineering, employing more than 400,000 people world-wide and operating in industry, energy and healthcare sectors – driving innovation in key areas such as smart grid, high-speed rail, personalized medicine and many more. With the rise of sensing and storage technologies, data analytics in general, and data processing and statistical machine learning methods in particular, are becoming crucial for managing and optimizing large scale systems . Siemens Corporate Research is one of several Siemens Corporate Technology research and development centers committed to transforming research into practical, innovative solutions and services that support Siemens’ broad range of businesses. In this talk I will discuss several solutions developed by SCR that exemplify the transition of emerging technologies into mature software systems deployed worldwide to monitor and control processes and equipment. Bio: Mathaeus Dejori holds a degree in Electrical Engineering from the Technical University in Graz, Austria and received his PhD in Computer Science from the Technical University in Munich, Germany. He joined Siemens as a research scientist in his last year of his PhD continuing his research in computational biology, machine learning and text mining. In 2008 he joined Siemens Corporate Research (SCR) in Princeton NJ where he is now leading a global team of researchers developing new data driven applications and services.",,2010.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
680fc7e5572e64be83d10a299b828e99a612ad5b,https://www.semanticscholar.org/paper/680fc7e5572e64be83d10a299b828e99a612ad5b,Science Magazine,"SCIENCE sciencemag.org T echnological innovations are penetrating all areas of science, making predominantly human activities a principal bottleneck in scientific progress while also making scientific advancement more subject to error and harder to reproduce. This is an area where a new generation of artificial intelligence (AI) systems can radically transform the practice of scientific discovery. Such systems are showing an increasing ability to automate scientific data analysis and discovery processes, can search systematically and correctly through hypothesis spaces to ensure best results, can autonomously discover complex patterns in data, and can reliably apply small-scale scientific processes consistently and transparently so that they can be easily reproduced. We discuss these advances and the steps that could help promote their development and deployment. Applying AI to the practice of science is not new. AI pioneer and Nobel laureate Herbert Simon hypothesized that cognitive mechanisms involved in scientific discovery are a special case of general human capabilities for problem-solving and, with colleagues, developed systems in the 1970s and 1980s that demonstrated reasoning capabilities for analyzing scientific data ( 1). Also in the 1970s, Joshua Lederberg (another Nobel winner) and colleagues developed the DENDRAL system for analyzing mass spectrometry data in order to hypothesize molecular structures ( 2). More recent breakthroughs, such as robot scientists and software that formulates laws for complex dynamical systems, demonstrate broader applicability of AI techniques for scientific discovery ( 3). Over the past two decades, AI has seen accelerating scientific advances and concomitant commercial-sector successes because of advances on three fronts: steady scholarly advances, especially as success has increased the numbers of interested participants; Moore’s law and steady exponential increases in computing power; and exponential increases in, and broad availability of, relevant data in volumes never previously seen. Those scientific efforts that have leveraged AI advances have largely harnessed sophisticated machine-learning techniques to create correlative predictions from large sets of “big data.” Such work aligns well with the current needs of petaand exascale science. However, AI has far broader capacity to ac-",,2009.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1d5c56d41068bdbadd319a3f62bd71d2d1cfb6f8,https://www.semanticscholar.org/paper/1d5c56d41068bdbadd319a3f62bd71d2d1cfb6f8,Artificial neural network technologies to identify biomarkers for therapeutic intervention.,"High-throughput technologies such as DNA/RNA microarrays, mass spectrometry and protein chips are creating unprecedented opportunities to accelerate towards the understanding of living systems and the identification of target genes and pathways for drug development and therapeutic intervention. However, the increasing volumes of data generated by molecular profiling experiments pose formidable challenges to investigate an overwhelming mass of information and turn it into predictive, deployable markers. Advanced biostatistics and machine learning methods from computer science have been applied to analyze and correlate numerical values of profiling intensities to physiological states. This article reviews the application of artificial neural networks, an information-processing tool, to the identification of sets of diagnostic/prognostic biomarkers from high-throughput profiling data.",Current opinion in molecular therapeutics,2004.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
af8db38ac764d82ea1e492174858771b147e211e,https://www.semanticscholar.org/paper/af8db38ac764d82ea1e492174858771b147e211e,Towards a Multidisciplinary Approach to Peer Production : Questions and an Application,"The past decade has seen explosive growth in user-generated, user-mediated content and the web as a fundamentally social space. More recently, however, the web has increasingly become a site of “production” – cultural, social, and economic – in ways few could have predicted. The rise and shifting nature of novel types of peer production and the composition of the resulting artifacts are a largely unexplored phenomenon. This paper argues that a new cross-disciplinary research agenda is vital to understanding how people get things done together online and optimizing peer production. iLink, developed under the DARPA CALO program, is a machine learning engine designed to explore these issues and to help improve peer production in a real military context. The iLink project made clear that we stand at edge of a virtual green field; the right lines of inquiry and exploration could yield fundamental new breakthroughs in social and computer sciences, and provide the foundations for a new generation of intelligent, peer-production enabled applications. Challenges and Opportunities Much ink has been spilled over peer production mechanisms as essentially an Information Retrieval problem. More recently, computer scientists and some social scientists have approached the problem from the perspective of social network analysis. While both of these approaches can yield extremely interesting insights, isolated from one another, each misses important elements of how and peer production happens over time. Both approaches have only rarely taken into account the highly dynamic, temporally-specific nature of communities and that this dynamism is reflective of the nature of humankind itself. Graph analysis, alone, tends to consider the aggregate statistics of human interaction and assign overly general, undifferentiated values to both individuals and the groups to which they belong. IR, alone, ignores the social context in which information – and more importantly knowledge and meaning – is generated and in constant interpretive motion as it traverses networks. In other alone, these disciplines lack the analytic tools and methodological frameworks necessary to understanding the bigger questions around peer production, collective intelligence, and cooperative sensemaking that they seek to answer. More cross-domain inquiry that combines the best of a wide variety of disciplines to ask new kinds of questions is necessary to understanding peer production and technology-mediation social participation. The importance of effective peer-to-peer production mechanisms seems obvious in the aftermath of the Sept 11 and NASA catastrophes. And yet team members in these contexts are essentially using the same tools and passive web applications that they were a decade ago. Email predominates as the default peer production mechanism among teams working to accomplish tasks. While “Web 2.0” applications like Twitter and Facebook increase the weak-link like ties in networks and the chances for serendipitous discovery ( to say nothing of “social prioperception” – in the words of one well-known and over-eager blogger), they also can hurt as much as help. Some common challenges to effective peer production for which we have inadequate integrated theoretical frameworks and few compelling application solutions include: The User Generated Web tends to be extremely “noisy”. It can be shrill. It is most often personally irrelevant, even in a purely social sense. Closed “work” communities are often not much better. Noise increases the “costs” of usage and engagement. Transaction cost theory and allied insights from behavioral economics, not to mention cognitive psychology, have rich models that could be extended to add insights here. Beyond the noise communities tend to generate, the sheer volume of information can quickly overload users. Community members often are overwhelmed with potentially relevant information, unable to effectively keep up with and filter for the most personally meaningful and relevant information. Sensemaking is hard alone, and can be even harder together. Working cooperatively requires cognitive alignment and effort that can be hard to achieve/maintain, and the benefits of working with others do not necessarily outweigh the costs. Online communities are subcultures, and ever-changing ones at that. There is little appreciation of online communities as both similar to and yet different from offline communities. Economic sociology, behavioral network economics, and anthropology have important insights here, but have been largely ignored by computer scientists seeking to understand change in human societies over time. For example, how do communities define themselves? What signaling do they use to communicate to one another? What are the means of social reproduction and how to members “teach” one another shared value and symbolic systems? What insights from behavioral signaling, linguistics and NLP, could be combined with learning theories and pattern detection to explore these dynamics? In terms of collective problem-solving, what structural traits (if any) makes some communities more or less successful? Meaning – and by extension, “relevance” -is understood, mediated, and manipulated in a dynamic social context. As documents or questions are passed around a human network, the original artifact changes subtly as it comes into contact with and is manipulated by that network. Members of the community may edit the document and pass it along, or ask questions about the document. Members of the community may make clarifying points that alter and focus the nature of the original question. Artifacts may be rated explicitly, or implicitly through the velocity at which they traverse the community, the discussions that happen about the artifact, or by the fact that the artifact is roundly ignored. The meaning attached to artifacts is a function of dynamic social processes that are difficult to characterize. There are a remarkable variety of meta-signals attached to peer-produced artifacts, but we lack coherent ways of detecting, assessing and measuring these signals as they wax and wane. Trust and reputation are difficult and costly to assess. Yet they remain essential elements of effective cooperation. While these concepts are often subsumed under the term “social capital”, they in fact point to somewhat different ideas. How is trust defined and gained in different kinds of communities? How important is reputation? When does it matter (most)? And how would we measure such abstract, subjective value judgments? Do these terms take on different meaning online than offline? Diversity of interpretation, opinion and contributions is likely key to the quality of peer-produced artifacts, but how can that be enforced – let alone accurately assessed? Recent work suggests that diversity is likely more important than sheer size of community in, for instance, collective prediction. Further, how do we represent and measure diversity over time? Incentives matter. How can insights from mechanism design be combined with HCI design, cognitive psychology, dynamic graph analysis and behavioral economics to generate novel choice architectures that best motivate collective behavior? How can machine learning and other artificial intelligence approaches be deployed to optimize a group’s collective output? Today’s web applications are passive and put all of the burden of collaborating and sensemaking on the individual. Simply exposing users to communities and the artifacts that they generate does not collective intelligence make. Recent studies in fact highlight the quite the opposite – that effective, prolific minorities dominate and bias so called “peer-production”. How can we create software that is more “aware” and “adaptive” to a user’s context, technology that can learn to improve over time and that is fault-tolerant? Individuals play different roles in different communities. Further, those roles change frequently as individuals themselves change. How and why do these roles change over time? What do social pressures (positive and negative) have to do with these dynamics? Are there discernable characteristic signals that indicate such shifts? Are those patterns structural, correspondent to a bounded community? The remainder of this paper describes work under DARPA’s CALO (Cognitive Assistant that Learns and Organizes) program that attempted to address a subset of the questions raised above in a real community of military users. While our experience only scratches the surface of these issues, it also points up both the opportunities for further developing the line of inquiry as well as the real importance of creating effective peer production mechanisms. Background: The iLink Project under CALO The CALO program was focused on building a cognitive assistant. Initially, this goal was understood as assisting the user with computational resources, either local to the desktop or web-based. Real assistants certainly provide this kind of help (e.g., assisting with the use of on-line scheduling systems). Good assistants also provide help, however, with human resources, e.g., human expertise identification. In general, social networks are an essential resource for knowledge workers. In fact, social networks may be the single most important resource for a user both in terms of depth and frequency of use. The CALO program incorporated this notion of assistance as the iLink sub-project. A canonical problem for social network assistance is expertise and stakeholder identification who knows what? The idea is that important knowledge is 'somewhere' in the network. The idea is that if there is a catalog of this knowledge then a user could access this catalog and find the right small set of targets for a query. Many companies particularly consulting companies that need the ability to rapidly form pools of expertise for new projects have tried data-entry style knowledge catalogs. For the",,2009.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
230a36e5f82ddc925e0381137fb62fec4f5f4289,https://www.semanticscholar.org/paper/230a36e5f82ddc925e0381137fb62fec4f5f4289,Human-Centered Computing: Challenges and Perspectives,"Computing is at one of its most exciting moments in history, playing an essential role in supporting many important human activities. The explosion in the availability of information in various media forms and through multiple sensors and devices means, on one hand, that the amount of data we can collect will continue to increase dramatically, and, on the other hand, that we need to develop new paradigms to search, organize, and integrate such information to support all human activities. Human centered computing (HCC) is an emerging field that aims at bridging the existing gaps between the various disciplines involved with the design and implementation of computing systems that support people's activities. HCC aims at tightly integrating human sciences (e.g. social and cognitive) and computer science (e.g. human-computer interaction (HO), signal processing, machine learning, and ubiquitous computing) for the design of computing systems with a human focus from beginning to end. This focus should consider the personal, social, and cultural contexts in which such systems are deployed. Beyond being a meeting place for existing disciplines, HCC also aims at radically changing computing with new methodologies to design and build systems that support and enrich people's lives. In this presentation, we discuss the existing challenges in HCC and describe what we consider to be the three main areas of interest: media production, analysis, and interaction. In addition, we identify the core characteristics of HCC, describe example applications.and propose a research agenda for HCC.",2007 IEEE 23rd International Conference on Data Engineering Workshop,2007.0,10.1109/ICDEW.2007.4400966,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1d8e6b172411c53e967503b90d70bfecf8063cc4,https://www.semanticscholar.org/paper/1d8e6b172411c53e967503b90d70bfecf8063cc4,Special issue: societal aspects of synthetic biology,,Systems and Synthetic Biology,2009.0,10.1007/s11693-009-9043-6,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
21607c547d820b0d29df0e2cb5b1aa461e83f858,https://www.semanticscholar.org/paper/21607c547d820b0d29df0e2cb5b1aa461e83f858,Characterizing Distributed Stream Processing Systems for IoT Applications,"The growing ability to collect observations on the physical and natural world has introduced data-driven science as a first class paradigm. However, the types of data driving scientific and data-management challenges have themselves evolved in the last decade. From efforts to batch-analyze large datasets from a few massive instruments like the Large Synoptic Survey Telescope (LSST)1, that takes hours if not days to process, we have made our way to high throughput gene sequencing and adaptive weather analysis [1]that process large data volumes in minutes or hours for clinical diagnostics and steering weather instruments. This evolution continues on with the deployment of sensors and mobile devices generating stream of events to be examined in seconds or faster to drive decisions.This highlights the need for middleware for decision support applications that process streams, at large numbers and fast rates, and need to respond rapidly to control costly and critical systems in a closed loop. Two popular terms that capture these application domains and their middleware are the broad areas of Internet of Things (IoT), and Big Data platforms, particularly in the velocity dimension. Many applications within IoT exemplify Dynamic Data Driven Applications Systems (DDDAS) [2] such as Smart Power and Water Grids [3], Smart Transportation, and urban monitoring. Such application exhibit the need for an Observe Orient Decide Act (OODA) feedback cycle, where observations on the system are used to decide when and how to optimize the system, enact these decisions, and continue their observations to ensure the goal, such as reliability or efficiency, is met. These decision logic themselves may use simple time-series forecasting models, rule-based systems or sophisticated machine learning algorithms, depending on the latency available for decision making.Scientific workflows were popular for orchestrating data-intensive applications on Grids and Clouds, and some dynamic workflows, including our own work, have been used to control DDDAS applications [1]. However, workflow systems are not crafted for low-latency applications. There has been a growth in Distributed Stream Processing Systems (DSPS) over the past few years to support streaming applications. Platforms like Apache Storm??and Apache Spark Streaming allow composition of streaming dataflows from user logic, and are executed incrementally with low latency over hundreds of streams at 1000's of events per second, on commodity clusters.DSPS appear to be an intrinsic platform needed by IoT middleware but need better examination based on the emerging needs of this novel domain. In this paper, we characterize the role, capabilities and the performance requirements from DSPS to leverage them for large-scale IoT applications. Specifically, we make the following contributions:1) We offer IoT application scenarios which exemplify the role of DSPS, and other information flow processing systems, within a closed-loop data-driven decision making and control system.2) We characterize IoT data streams based on real-world, public datasets from Smart Grids, Smart Transportation, and environmental monitoring, to understand their throughput and message size distributions.3) We identify several micro-patterns used in composition of DSPS and applications dataflow patterns for IoT scenarios, such as data-preprocessing, temporal aggregation and forecasting, that are representative of workloads in this area. This, along with the stream characterization, help define IoT benchmark workloads and associated performance characteristics to evaluate DSPS.4) We validate this workload by offering a comparative evaluation of Storm and Spark Streaming, two popular open-source DSPS, and discuss the results.5) Lastly, we identify open problems in DSPS that are essential to be addressed to enable their effective use in IoT middleware, including support for elasticity on Clouds, use of edge devices for distributed analytics, deadline-driven stream processing, and integrating DSMS with DSPS for event and stream analytics.",HiPC 2015,2015.0,10.1109/HIPCW.2015.22,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
886103ac067499364fd7af8a9ef1a7c0999f20cb,https://www.semanticscholar.org/paper/886103ac067499364fd7af8a9ef1a7c0999f20cb,A methodology of error detection: improving speech recognition in radiology,"Automated speech recognition (ASR) in radiology report dictation demands highly accurate and robust recognition software. Despite vendor claims, current implementations are sub-optimal, leading to poor accuracy, and time and money wasted on proofreading. Thus, other methods must be considered for increasing the reliability and performance of ASR before it is a viable alternative to human transcription. One such method is post-ASR error detection, used to recover from the inaccuracy of speech recognition. This thesis proposes that detecting and highlighting errors, or areas of low confidence, in a machine-transcribed report allows the radiologist to proofread more efficiently. This, in turn, restores the benefits of ASR in radiology, including efficient report handling and resource utilization. 
To this end, an objective classification of error-detection methods for ASR is established. Under this classification, a new theory of error detection in ASR is derived from the hybrid application of multiple error-detection heuristics. This theory is contingent upon the type of recognition errors and the complementary coverage of the heuristics. Inspired by these principles, a hybrid error-detection application is developed as proof of concept. The algorithm relies on four separate artificial-intelligence heuristics together covering semantic, syntactic; and structural error types, and developed with the help of 2700 anonymised reports obtained from a local radiology clinic. Two heuristics involve statistical modeling: pointwise mutual information and co-occurrence analysis. The remaining two are non-statistical techniques: a property-based, constraint-handling-rules grammar, and a conceptual distance metric relying on the ontological knowledge in the Unified Medical Language System. When the hybrid algorithm is applied to thirty real-world radiology reports, the results are encouraging: up to a 24% increase in the recall performance and an 8% increase in the precision performance over the best single technique. In addition, the resulting algorithm is efficient and modular. 
Also investigated is the development necessary to turn the hybrid algorithm into a real-world application suitable for clinical deployment. Finally, as part of an investigation of future directions for this research, the greater context of these contributions is demonstrated, including two applications of the hybrid method in cognitive science and machine learning. 
Keywords. medical informatics, automatic speech recognition, natural language processing, hybrid error detection, computer-assisted editing, radiology reporting",,2006.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c2ce4727c7c082507bd41dc3188c20235d6aeec3,https://www.semanticscholar.org/paper/c2ce4727c7c082507bd41dc3188c20235d6aeec3,Biometric Analytics Cost Estimating,"This paper examines the interplay between biometric technologies and advanced analytics, referred to as biometric analytics, as a way to detect fraudulent entries in a biometric system. We follow a systematic approach, based on cost estimating standards, to ascertain whether deploying such a capability is worthwhile. A simple case study is presented that illustrates the key aspects of our analysis. In addition, there are a series of cost elements that quantify the impact biometric vulnerabilities have on individuals, companies, and countries. Introduction As biometric technologies and systems find their way into more and more facets of daily life, the need for such systems to be reliable and secure has become that much more important. Concurrently, fields such as data science are growing at a dynamic rate and are providing organizations new and powerful ways in which to leverage large quantities of data to drive improvements and transformation. This paper aims to explore the confluence of these two fields, what we refer to as biometric analytics, and begin to estimate the benefits and costs of implementing a biometric analytics capability that looks to address the problem of fraudulent records in a biometric data store. This paper is organized as follows. We begin by presenting a focused background on biometrics and advanced analytics as a way to orient the reader for what is to come and introduce the motivation behind our study. A brief summary of our estimation approach follows. With our approach established, we present an example case study that illustrates the key aspects of the problem. We conclude with some salient points that emerged during our analysis and thoughts for future work. Background In this section, we provide the reader with some relevant background information on the topics of biometric and advanced analytics. Biometrics Biometric technologies measure and analyze human physiological and behavioral characteristics. Identifying an individual’s physiological characteristics is based on direct measurement of a part of the body, e.g., fingertips, hands, face, and eye retinas and irises. Identifying behavioral characteristics is based on information derived from actions, such as speech and how one signs his/her name. Because the characteristics they measure are thought to be distinct to each person, biometrics can be very effective personal identifiers. Unlike more traditional identification methods that rely on something one has, such as an identification card for building access, or something one knows, such as a PIN to access an ATM, biometrics are integral to something about the individual. Being inherently linked to the individual, they are more reliable, cannot be forgotten, and are less easily lost, stolen, or spoofed. While biometric technologies vary in complexity, capabilities, and performance, all share several elements in common. At a fundamental level, all biometric identification systems reduce to pattern recognition systems. They use sensors such as cameras and scanning devices to capture images, recordings, or measurements of an individual’s characteristics along with computer hardware and software to extract, encode, store, and compare these characteristics. Because the process is almost always automated, biometric decision-making is typically very fast, and in some cases, real-time. Depending on the application, biometric systems can be used in one of two modes: verification or identification. Verification, or authentication, is used to verify a person’s identity; i.e., to authenticate that an individual’s reported identity is their true identity. Identification, on the other hand, is used to establish a person’s identity; i.e., to determine who a person is. Although biometric technologies measure different characteristics in substantially different ways, all biometric systems involve similar processes that can be divided into two distinct stages: (1) enrollment and (2) verification or identification. In enrollment, a biometric system is populated with the information needed to identify a specific person. The person first provides an identifier, such as an identification card. He or she then presents the biometric (e.g., fingertips, hand, iris) to a suitable acquisition device, the distinctive features are located, and one or more samples are extracted, encoded, and stored as a reference template for future comparisons. Finally, this biometric is linked to the identity specified on the identifier. In verification systems, the objective is to verify that a person is who he or she claims to be (i.e., the person who enrolled). After the individual provides the identifier that was used during enrollment, the specific biometric is presented. The system captures the biometric and generates a trial template. The system then compares the trial biometric template with this person’s reference template to determine whether the individual’s trial and stored templates match. Verification is often referred to as 1:1 (one-to-one) matching. Verification systems can contain databases ranging from dozens to millions of enrolled templates but are always predicated on matching an individual’s presented biometric against his or her reference template. In identification systems, the objective is to identify who a person is. Unlike verification systems, an identifier is not necessary. To find a match, instead of locating and comparing the person’s reference template against his or her presented biometric, the trial template is compared against the stored reference templates of all individuals enrolled in the system. Identification systems are referred to as 1:N (one-to-N, or one-to-many) matching because an individual’s biometric is compared against multiple biometric templates in the system’s database. Advanced Analytics The field of analytics is as broad as that of biometrics, arguably broader. At its core, analytics is the discovery and communication of meaningful patterns in data, relying on the simultaneous application of statistics and mathematics, computer programming, and data manipulation to extract valuable knowledge from data. While analytics can be as austere as fitting a line to a set of data points, it can also be as complex as developing an artificial neural network to perform speech recognition. In the context of our analysis, we will focus on more complex analytics, rooted in the field of machine learning. Machine learning, as defined rather formally by Mitchell [1], is a framework where, “a computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.” More intuitively, machine learning is most commonly used to mean the application of induction algorithms and other algorithms that can be said to “learn.” An algorithm is said to be inductive if it takes as input specific instances and produces a model that generalizes beyond these instances. The learning aspect is typically realized through a process called supervised learning, wherein the algorithm is presented with a training data set from which to learn. This training data set consists of example inputs and their desired outputs or “labels.” For instance, the inputs could be physical characteristics of a person, such as height, weight, hair color, and so on; the corresponding labels could then be male or female. The algorithm would use these inputs and the corresponding labels to “learn” a model that mapped inputs to output. In cases where a training data set is not available, one turns to unsupervised learning. Here, no labels are given to the algorithm, leaving it on its own to find structure in the input data. Clustering, or grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other group, is a classical unsupervised machine learning task. Motivation Biometric systems have long been used in law enforcement and forensics, and more recently, has gained prominence as a reliable, cost-effective means of personal authentication. Government and commercial applications include immigration, border control, airport security, physical access control, ATM authentication, and mobile device security [2]. To quickly perform verification of a subject or to perform identification against a watch list, government agencies and other users of these systems maintain large databases of digital biometric records. For example, today, the FBI’s Integrated Automated fingerprint Identification System (IAFIS) contains fingerprint records for over 64 million individuals [3]. Biometric systems are vulnerable to attacks at various stages in the biometric recognition process, including attacks on the database in which enrolled entries are stored. Large biometric databases pose challenges to testing and protecting the integrity of collected data. For example, fingerprint databases may be vulnerable to cyberattacks aimed at impersonating or concealing an individual’s identity through the use of synthetically generated fingerprint images (spoofs). These images could allow the attacker to replace their own fingerprints in the database so that the attacker is not recognized during subsequent identification attempts. Additionally, an attacker can perform a “masquerade attack,” in which they impersonate another individual by injecting a synthetic image that has been reconstructed from the desired individual’s feature set. A number of advanced analytics techniques (e.g., machine learning approaches) have been proposed to address the problem of spoofed biometric detection [4, 5]. The belief is that with the rapid growth of fields such as data science and our ability to mine and exploit massive data stores (such as those associated with biometric records), identification of spoofed records in large databases should now be possible. Further",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
34da0580c4f9f564efd93e5dce1100f30cd03915,https://www.semanticscholar.org/paper/34da0580c4f9f564efd93e5dce1100f30cd03915,Many human activities are a bottleneck in progress,"SCIENCE sciencemag.org T echnological innovations are penetrating all areas of science, making predominantly human activities a principal bottleneck in scientific progress while also making scientific advancement more subject to error and harder to reproduce. This is an area where a new generation of artificial intelligence (AI) systems can radically transform the practice of scientific discovery. Such systems are showing an increasing ability to automate scientific data analysis and discovery processes, can search systematically and correctly through hypothesis spaces to ensure best results, can autonomously discover complex patterns in data, and can reliably apply small-scale scientific processes consistently and transparently so that they can be easily reproduced. We discuss these advances and the steps that could help promote their development and deployment. Applying AI to the practice of science is not new. AI pioneer and Nobel laureate Herbert Simon hypothesized that cognitive mechanisms involved in scientific discovery are a special case of general human capabilities for problem-solving and, with colleagues, developed systems in the 1970s and 1980s that demonstrated reasoning capabilities for analyzing scientific data ( 1). Also in the 1970s, Joshua Lederberg (another Nobel winner) and colleagues developed the DENDRAL system for analyzing mass spectrometry data in order to hypothesize molecular structures ( 2). More recent breakthroughs, such as robot scientists and software that formulates laws for complex dynamical systems, demonstrate broader applicability of AI techniques for scientific discovery ( 3). Over the past two decades, AI has seen accelerating scientific advances and concomitant commercial-sector successes because of advances on three fronts: steady scholarly advances, especially as success has increased the numbers of interested participants; Moore’s law and steady exponential increases in computing power; and exponential increases in, and broad availability of, relevant data in volumes never previously seen. Those scientific efforts that have leveraged AI advances have largely harnessed sophisticated machine-learning techniques to create correlative predictions from large sets of “big data.” Such work aligns well with the current needs of petaand exascale science. However, AI has far broader capacity to ac-",,2014.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
001ef56b588ff4ca79f0d194140c5b69f9f80a8e,https://www.semanticscholar.org/paper/001ef56b588ff4ca79f0d194140c5b69f9f80a8e,Path Integral Quantum Mechanics: from the basics to the latest developments,"This work is part of a joint doctoral program in condensed matter physics and theoretical chemistry that aims at simulating the quantum dynamics of light nuclei in materials and molecular systems. The general goal of the project is to develop a mathematical and simulation framework to address quantum reaction rate calculations and quantum-driven diffusion of light nuclei, such as hydrogen. H atoms show an intrinsic quantum delocalization which can be of the order of chemical bond lengths, so that nuclear quantum effects (NQEs) can have a strong impact. Zero-point energy and tunneling effects allow the exploration of regions of space that would be classically forbidden, with relevant consequences on H diffusion. The efficient simulation of molecules and materials from first principles is a long-standing challenge in the physical sciences. Machine learned force fields promise to speed up these simulations by several orders of magnitudes whilst being as accurate as high-level quantum mechanics. In the past 3 years several different approaches were proposed to fulfill this promise built on Gaussian Process Regression and Neural Networks. In this poster we demonstrate that highly accurate molecular force fields can be built using the Atomic Cluster Expansion framework and linear least squares regression. Our model is built from body ordered symmetric polynomials which is a natural extension of the traditional molecular mechanics force fields. We show that these relatively simple models are able to achieve state of the art accuracy on the MD17 benchmark dataset of small organic molecules. Furthermore, we also train several other machine learning models like sGDML, ANI and GAP, as well as a classical force field and compare them on tasks such as normal mode prediction and extrapolation to high temperature data. Finally, we fit the potential energy surface of a large flexible organic molecule and compare how well the models reproduce the dihedral torsional energy landscape form as little as 500 reference calculation. The STFC Rutherford Appleton Laboratory houses the ISIS Neutron and Muon Sources, which produce beams of neutrons and muons that can be used to study materials at the atomic level. Muons are subatomic particles -produced by bombarding a graphite target with pulses of high-energy protons that originate in a synchrotron- which are 100% spin-polarised and have approximately 1/10 of the mass of a proton. In a μSR experiment, spin-polarized positive muons are implanted in a sample and can be used, among other things, to study hydrogen defects, the magnetic structure of the sample or the organic radicals that may result from adding the muon to an organic sample. However, the μSR technique We introduce vibrational dynamical mean-field theory (VDMFT) as a non-perturbative and systematically improvable method for the simulation of anharmonic lattice dynamics. Inspired by its origin in electronic structure theory, VDMFT is a real-space embedding approach that maps the anharmonic dynamics of an extended, periodic lattice onto an impurity problem where the spectral density is self-consistently tailored. We develop VDMFT and its cluster extension with classical and quantum impurity solvers for one-dimensional models. When compared to classical exact molecular dynamics, VDMFT produces spectral function and density of states that precisely captures the frequency shifts, phonon lifetimes, and temperature dependence induced by anharmonicity. With much fewer degrees of freedom in the impurity model than in the full supercell, the approach is expected to converge to accurate results at affordable computational costs. Understanding the underlying mechanism of proton transport in hydrogen-bonded systems is crucial to a wide variety of applications ranging from voltage-gated proton channels in biological systems to proton exchange membrane fuel cells. Imidazole and 1,2,3-triazole are two promising hydrogen-bonded organic heterocycles that conduct protons via a structural transport mechanism involving intermolecular proton hops. The theoretical study of proton transport in these systems has proved challenging so far because ab initio simulations, which model the bond breaking and forming involved in structural diffusion, impose a significant computational cost given the system sizes and timescales needed to converge diffusion properties and hydrogen bond dynamics. Here, we leverage ab initio multiple time-stepping, an algorithmic advance that can be used to speed up molecular dynamics simulations, to accumulate ab initio trajectories in excess of a nanosecond for imidazole and each tautomer of 1,2,3-triazole. By using correlation function analysis, we decompose the mechanism of proton transport into a series of first-order processes and show that the proton transport mechanism occurs over three distinct time and length scales. We demonstrate that the linearity of hydrogen bond chains formed in imidazole and 1,2,3-triazole is positively correlated with the rate of proton diffusion. We also uncover evidence of a ‘blocking’ mechanism in both tautomers of 1,2,3-triazole, where hydrogen bonds formed by the middle nitrogen atom create a trap that limits the mobility of protons across the hydrogen bond network. Our simulations thus provide insights into the origins of the experimentally observed 10-fold difference in conductivity between imidazole and 1,2,3-triazole. [1] Z. Long, A. Atsango, J. Napoli, T. Markland, M. Tuckerman, J. Phys. Chem. Lett., 11 , 6156-6163 (2020) Establishing a comprehensive and quantitative understanding of mechanical instabilities in single-molecule junctions is a prerequisite for possible applications in nanoelectronic devices. Recent experimental and theoretical studies have revealed a variety of different processes triggering mechanical instabilities, including current-induced heating and nonconservative forces, however, the underlying mechanisms remain largely elusive. In this contribution, we present a fully quantum mechanical investigation of current-induced bond rupture in molecular junctions, employing the numerically exact hierarchical quantum master equation approach [1]. Based on a generic model for molecular junctions, our systematic study identifies three dissociation mechanisms: (1) ultrafast dissociation induced by the population of anti-bonding electronic states, (2) incoherent stepwise vibrational ladder climbing, (3) coherent multilevel vibrational excitations induced by multiple electronic transitions. Considering a broad range of different regimes and processes, comprising weak to strong electronic-vibrational and molecule-lead coupling as well as vibrational relaxation, we analyze the different mechanisms in detail. Furthermore, strategies for improving the stability of molecular junctions are discussed. Hybrid lead halide perovskites are a class of materials that have unique photophysical properties due to their anharmonic lattices and predominately ionic bonding. High quantum yield, a tunable band gap, high defect tolerance and low binding energy all make perovskites ideal for photovoltaic devices. Lead halide perovskites have exceptionally low rate of electron-hole recombination rates, which is implicated in their high-power conversion efficiencies. However, since both electrons and holes are diffusive and strongly couple to an anharmonicity lattice, elucidating the nature of this phenomena is theoretically difficult and little is known about the mechanism causing low recombination of charge carriers. In this work, we aim to explain the effects of anharmonicity on recombination phenomena and study how photogenerated electron and hole bind, dissociate and recombine by using molecular dynamics simulations. Using an effective mass model of the photoexcited charge carriers, we develop and deploy a quasiparticle based path integral molecular dynamics framework to study recombination. Using an atomistic model for perovskite lattice allows us to capture all orders of anharmonicity, reducing the computational complexity associated with studying this system, which would be intractable from standard solid-state methods. Equilibrium constant of isotope fractionation of boron between its two main aqueous species namely boric acid and borate is the main proxy for reconstruction of seawater pH and atmospheric pCO 2 in ancient era. The theoretically evaluated value of 1.0194 reported by Kakihana and coworkers [1] for this equilibrium constant which has been in use for some decades has now been found to underestimate A purpose of the current study is to find an alternative approach to the computation of multi-loop Feynman diagrams. Recently, deeper properties of Feynman amplitudes emerged through the study of differential forms. Feynman integrals are rewritten through the Baikov representation, from which it emerges that they form a vector space equipped with a scalar product defined by ‘intersection numbers’ of differential forms. The integral of interest is then projected onto a basis of ‘Master Integrals’ of said vector space; the basis was proven to be finite dimensional: its dimension corresponds to the one of the homology group associated to the space of integration, (or of the cohomology group equivalently).The poster includes the following:- the form of a Feynman integral in Baikov representation along with its implications (i.e., the identification of a vector space)- the determination of the dimension of said space from a geometric point of view (along with some explanatory figures)- the topological constructions underlying (along with some explanatory figures)- How the topological constructions lead to some observations related to the identification of a preferred basis of MIs.An extended treatment on these topics will be found in the editorial ‘Co-homology of Differential Forms and Feynman Diagrams’ (authors: S.L. Cacciatori, M. Conti, P. Mastrolia, S. Trevisan), which has yet to be pub",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
f2afef04b2ace0a5604c66e33f1982088f548e04,https://www.semanticscholar.org/paper/f2afef04b2ace0a5604c66e33f1982088f548e04,PANTHER. Pattern ANalytics To support High-performance Exploitation and Reasoning.,"Sandia has approached the analysis of big datasets with an integrated methodology that uses computer science, image processing, and human factors to exploit critical patterns and relationships in large datasets despite the variety and rapidity of information. The work is part of a three-year LDRD Grand Challenge called PANTHER (Pattern ANalytics To support High-performance Exploitation and Reasoning). To maximize data analysis capability, Sandia pursued scientific advances across three key technical domains: (1) geospatial-temporal feature extraction via image segmentation and classification; (2) geospatial-temporal analysis capabilities tailored to identify and process new signatures more efficiently; and (3) domain- relevant models of human perception and cognition informing the design of analytic systems. Our integrated results include advances in geographical information systems (GIS) in which we discover activity patterns in noisy, spatial-temporal datasets using geospatial-temporal semantic graphs. We employed computational geometry and machine learning to allow us to extract and predict spatial-temporal patterns and outliers from large aircraft and maritime trajectory datasets. We automatically extracted static and ephemeral features from real, noisy synthetic aperture radar imagery for ingestion into a geospatial-temporal semantic graph. We worked with analysts and investigated analytic workflows to (1) determine how experiential knowledge evolves and is deployed in high-demand, high-throughput more » visual search workflows, and (2) better understand visual search performance and attention. Through PANTHER, Sandia's fundamental rethinking of key aspects of geospatial data analysis permits the extraction of much richer information from large amounts of data. The project results enable analysts to examine mountains of historical and current data that would otherwise go untouched, while also gaining meaningful, measurable, and defensible insights into overlooked relationships and patterns. The capability is directly relevant to the nation's nonproliferation remote-sensing activities and has broad national security applications for military and intelligence- gathering organizations. « less",,2015.0,10.2172/1228822,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6ba71a02fef4cfea5040cf2d8f2c8d1246ea5317,https://www.semanticscholar.org/paper/6ba71a02fef4cfea5040cf2d8f2c8d1246ea5317,Key body pose detection and movement assessment of fitness performances,"Motion segmentation plays an important role in human motion analysis. Understanding the intrinsic features of human activities represents a challenge for modern science. Current solutions usually involve computationally demanding processing and achieve the best results using expensive, intrusive motion capture devices. In this thesis, research has been carried out to develop a series of methods for affordable and effective human motion assessment in the context of stand-up physical exercises. 

The objective of the research was to tackle the needs for an autonomous system that could be deployed in nursing homes or elderly people's houses, as well as rehabilitation of high profile sport performers. Firstly, it has to be designed so that instructions on physical exercises, especially in the case of elderly people, can be delivered in an understandable way. Secondly, it has to deal with the problem that some individuals may find it difficult to keep up with the programme due to physical impediments. They may also be discouraged because the activities are not stimulating or the instructions are hard to follow.

In this thesis, a series of methods for automatic assessment production, as a combination of worded feedback and motion visualisation, is presented. The methods comprise two major steps. First, a series of key body poses are identified upon a model built by a multi-class classifier from a set of frame-wise features extracted from the motion data. Second, motion alignment (or synchronisation) with a reference performance (the tutor) is established in order to produce a second assessment model. Numerical assessment, first, and textual feedback, after, are delivered to the user along with a 3D skeletal animation to enrich the assessment experience. This animation is produced after the demonstration of the expert is transformed to the current level of performance of the user, in order to help encourage them to engage with the programme.

The key body pose identification stage follows a two-step approach: first, the principal components of the input motion data are calculated in order to reduce the dimensionality of the input. Then, candidates of key body poses are inferred using multi-class, supervised machine learning techniques from a set of training samples. Finally, cluster analysis is used to refine the result. Key body pose identification is guaranteed to be invariant to the repetitiveness and symmetry of the performance. Results show the effectiveness of the proposed approach by comparing it against Dynamic Time Warping and Hierarchical Aligned Cluster Analysis.

The synchronisation sub-system takes advantage of the cyclic nature of the stretches that are part of the stand-up exercises subject to study in order to remove out-of-sequence identified key body poses (i.e., false positives). Two approaches are considered for performing cycle analysis: a sequential, trivial algorithm and a proposed Genetic Algorithm, with and without prior knowledge on cyclic sequence patterns. These two approaches are compared and the Genetic Algorithm with prior knowledge shows a lower rate of false positives, but also a higher false negative rate. The GAs are also evaluated with randomly generated periodic string sequences.

The automatic assessment follows a similar approach to that of key body pose identification. A multi-class, multi-target machine learning classifier is trained with features extracted from previous motion alignment. The inferred numerical assessment levels (one per identified key body pose and involved body joint) are translated into human-understandable language via a highly-customisable, context-free grammar.

Finally, visual feedback is produced in the form of a synchronised skeletal animation of both the user's performance and the tutor's. If the user's performance is well below a standard then an affine offset transformation of the skeletal motion data series to an in-between performance is performed, in order to prevent dis-encouragement from the user and still provide a reference for improvement.

At the end of this thesis, a study of the limitations of the methods in real circumstances is explored. Issues like the gimbal lock in the angular motion data, lack of accuracy of the motion capture system and the escalation of the training set are discussed. Finally, some conclusions are drawn and future work is discussed.",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
500f355ea68a410a55e5eedfa661cd9a75b07f18,https://www.semanticscholar.org/paper/500f355ea68a410a55e5eedfa661cd9a75b07f18,Text Mining for News and Blogs Analysis,,Encyclopedia of Machine Learning,2010.0,10.1007/978-0-387-30164-8_827,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7c80e9a8ba692e46c7058cae5c27c9902dc40cf2,https://www.semanticscholar.org/paper/7c80e9a8ba692e46c7058cae5c27c9902dc40cf2,Information Forensic Application using Soft Computing Techniques,"In all these forensic applications, soft computing techniques such as neural networks, fuzzy logic, evolutionary computing, and rough set, play an important role in learning complex data structures and patterns, and classifying them to make intelligent decisions. Image forensic techniques use natural properties of image to determine forgery or locate tampering. This special issue aims to highlight state-of-the-art research and novel solutions in information forensic applications using emerging soft computing techniques .The aim of this paper is that with the rise of digital crime (signature forgery, image forgery, illegal transaction, etc.) and the pressing need for methods to combat these forms of criminal activities, there is an increasing awareness of the importance of information forensics for security applications. The emergence and evolution of new digital technologies are dramatically changing how information is captured, processed, analyzed, interpreted, transmitted, and stored. While digital technology has greatly improved the collection and analysis of evidences, the underlying research challenges primarily focus on the integrity and the reliability of validating the resulting forensic decisions accurately. Furthermore, digital evidences can be easily tampered, altered, or forged to commit fraud, identity theft, or impersonate someone else, to remain elusive from law enforcement. Using image processing techniques, it is easy to tamper the original image by replacing an individual’s face, and making the change difficult to detect. The main objective, and major contribution, of the research is two-fold: conducting research in soft computing methodologies to improve their computational powers, and use these methodologies as means to provide a new approach for solving the problems of quality of service communications as an example of the class of complex, dynamical, multi-variable, multi-body, uncertain systems, for which, the author believes, soft computing is not only preferred but actually inevitable. INTRODUCTION Soft Computing is a multi-disciplinary field. Soft Computing is a new multidisciplinary field that goal was to construct new generation Artificial Intelligence, known as Computational Intelligence. Soft Computing in its latest incarnation as the fusion of the fields of Fuzzy Logic, Neuro-computing, Evolutionary and Genetic Computing, and Probabilistic Computing into one multidisciplinary system. The main goal of Soft Computing is to develop intelligent machines and to solve nonlinear and mathematically system problems, the applications of Soft Computing have proved two main advantages. First, it made solving nonlinear problems, in which mathematical models are not available, possible. Second, it introduced the human knowledge such as cognition, recognition, understanding, learning, and others into the fields of computing. This resulted in the possibility of constructing intelligent systems such as autonomous selftuning systems, and automated designed systems. Different techniques have been employed to solve problems occurring in various dynamic segments of supply chain. As more soft computing applications are introduced and used, a growing body of papers has been established that can guide the future design and deployment of supply chain solutions. This research aims at reviewing the common soft computing techniques applied to supply chain management, the main issues to address include: what are the main problems within supply chain that have been investigated using soft computing techniques? What techniques have been employed? What are the main findings and achievements up to date? SOFT COMPUTING Soft Computing is a group of unique methodologies, contributed mainly by Expert System (ES), Fuzzy Logic (FL), Neural Networks (NN), and Evolutionary Algorithms (EA), which provide flexible information processing capabilities to solve real-life problems. The advantages of employing soft computing is its capability to tolerate imprecision, uncertainty, and partial truth to achieve tractability and robustness on simulating human decisionmaking behavior with low cost . In other words, soft computing provides the opportunity to represent ambiguity in human thinking with the uncertainty in real life. The major soft computing techniques are following. (a) Fuzzy logic As the basic theory of soft computing, fuzzy logic supplies mathematical power for the emulation of the thought and perception processes. Fuzzy systems are very useful not only in situations involving highly complex systems but also in situations where an approximate solution is warranted. To deal with qualitative, inexact, uncertain and complicated processes, Fuzzy control is one prominent example. In fuzzy control, data is characterized by linguistic variables and expert knowledge (IF-Then-rules) using these variables are mapped into rule bases. In fuzzy control these bases can be used for logical inferences for controlling purposes. One of the reasons for the success of fuzzy logic is that the linguistic variables, values and rules enable the engineer to translate Dharmendra Kelde et al, / (IJCSIT) International Journal of Computer Science and Information Technologies, Vol. 4 (1) , 2013, 69 72",,2013.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
76e5d249aab8bb581365122ddfcb06ca959b105b,https://www.semanticscholar.org/paper/76e5d249aab8bb581365122ddfcb06ca959b105b,SURVEY ON HETEROGENEOUS NETWORK TRAFFIC ANALYSIS WITH SUPERVISED AND UNSUPERVISED DATA MINING TECHNIQUES,"Network Traffic Analysis (NTA) in heterogeneous networks is one of the emerging research areas receiving substantial attention from both the research community and traffic analyzers. Many tasks in NTA can be naturally cast in a supervised and unsupervised learning model. Many supervised classification models and unsupervised clustering learning models in data mining have been proposed for heterogeneous network. Due to the importance of network traffic analysis in data mining research with the rapid development of new models, To provide a comprehensive review on supervised classification and unsupervised clustering model on heterogeneous type of network in this paper and systematically give a summarization of the state-of-the-art techniques for network traffic analysis. It addresses the problem of network management such as traffic load, quality of service, and trend analysis. This survey covers real time supervised classification and unsupervised clustering algorithms and analyze techniques for heterogeneous networks. It provides taxonomy of the different supervised classification algorithms and unsupervised clustering algorithms and evaluates the various performance metrics that are significantly used for the purpose of comparison. A detailed review is provided covering fuzzy relational clustering algorithm, classification learning algorithms, global voting algorithm and hybrid algorithms. The survey evolve certain open issues, key research challenges for network traffic analysis using supervised classification and unsupervised clustering model in heterogeneous networks, and likely to provide productive research directions. Key Words--Supervised and Unsupervised Mining, Traffic Data Analysis, Heterogeneous Network D.Jayachitra et al, International Journal of Computer Science and Mobile Computing, Vol.3 Issue.7, July2014, pg. 47-59 © 2014, IJCSMC All Rights Reserved 48 I. STATE OF ART The growing population of the aged and the disable is leading to expansion of autonomous service systems. In data mining the data appear in limitless stream for classification of data stream. The problem of data stream classification, where the data enter in an unreal unlimited stream and the probability to evaluate each record is briefed. The problem is solved with the existence of stream classification algorithm. Sparse coding as demonstrated in [14] which fundamentally challenged to find an embedding for the data by assigning feature values based on subspace cluster membership. A direct application of sparse coding resulted in a collapse of knowledge relocate to sparse coding, by incorporating distribution distance approximate for the embedded data. Bayesian learning and expectation-maximization (EM) techniques were developed under the proposed generative model as shown in [17] for recognizing new training data for learning new unseen sites. Previously unseen attributes combined with their semantic labels were also exposed through another EMbased on the generative model. Segmentation algorithm is applied on this signal that automatically estimates the number of partitions and the partition borders as presented here [2] fails in holding each subtrajectory of the sampling set by different subtrajectories of the MOD (cluster), under the minimization of objective. Space-efficient algorithms maintain duplicate-insensitive order sketches so that rank-based queries are roughly processed with relative rank error that guarantees in the presence of data duplicates. Besides the space efficiency, the algorithm is time-efficient and highly accurate in [9]. Moreover, one scan algorithm is practical to the heavy hitter problem using distinct elements when compared to the existing fault-tolerant distributed communication techniques. For the discovery of a significant arrangement of data generated by human behavior, a clustering technique capable of detecting outliers is often employed. To be specific, Possibilistic c-Means (PCM), Fuzzy Possibilistic c-Means (FPCM) and Possibilistic Fuzzy c-Means (PFCM) are robust against outliers. However, they suffer from the local optimum problem and need to find a suitable means of combining and adjusting several free parameters to achieve optimal performance. Anomaly detection aims to recognize a minute group of instances which deviate remarkably from the accessible data. A well-known definition of outlier is that given an observation which deviates so much from other observations, as to arouse the uncertainties behavior generated by different mechanism, it gives the universal idea of an outlier and encourages many anomaly detection methods. Detecting anomalous insiders in collaborative information systems as shown in [1] intend to analyze the impact of such information in the future. The goal of the current work was to determine the basic information in the access logs and Meta information for the subjects in anomaly detection. On line alert aggregation based on a active, probabilistic model in [18] essentially are regarded as a data stream version of a maximum likelihood approach for the estimation of the model parameters. An online oversampling principal component analysis (OSPCA) illustrated in [8] aims in detecting the occurrence of outliers from a great amount of information via online update procedure. Fuzzy-state Q-learning (FSQL) process is incorporated, which is capable of learning human behavior patterns in a non-supervised manner and predicting subsequent human actions. In the latter case, interaction between certain users as shown in [7] often affects their choice of actions, and thus the situation of action learning is quite complicated. Error terms augment the standard sum of squared error computational experiments as shown in [16] that the modified learning method helps to extract fewer rules without increasing individual rule complexity and without decreasing classification accuracy. Ontology-based fuzzy video semantic content model uses spatial/temporal relations in event and conception definitions supply a wide domain pertinent rule construction average. D.Jayachitra et al, International Journal of Computer Science and Mobile Computing, Vol.3 Issue.7, July2014, pg. 47-59 © 2014, IJCSMC All Rights Reserved 49 Fuzzy video semantic content as shown in [10] allows the user to construct ontology for a given domain. In addition to domain ontology additional rule definitions are used to lower spatial relation computation cost and to identify some complex situations more effectively. Fascinatingly, the idea of fuzzy partitioning based on relational data is not novel, and can be traced. The purity of a cluster is defined as the fraction of the cluster size that the largest class of objects assigned to that cluster in [3] fails to extend these ideas to the development of a hierarchical fuzzy relational clustering algorithm. To derive a novel method for measuring similarity between the data objects in sparse and high dimensional field as shown in [15], same principle can be made use of. But alternative forms are defined for the relative similarity and do not use average but have other methods to combine the relative similarities according to the different viewpoints. More specially, show that hubness, i.e., the tendency of high-dimensional data to enclose points that frequently occur in k-nearest neighbor lists of other points in [20]. The hubness was successfully exploited in clustering. The cluster-adaptive distance bound based on separating hyper plane boundaries of Voronoi clusters in [5] enables well-organized spatial filtering, with a comparatively small preprocessing storage space overhead and is applicable to euclidean and Mahalanobis similarity measures. The large organizations will not retrieve and process petabytes of data, for various purposes such as data mining and decision support. Thus, there exist numerous applications that access large multimedia databases, which fail in efficient support. Fast smallest amount spanning tree-inspired clustering algorithm uses an efficient implementation of the cut and the rotation property of the minimum spanning trees. The rich properties of the MST algorithms fails in adapting MST inspired clustering algorithm [12] to more general and larger data sets, primarily when the whole data set cannot fit into the main memory. MAximal Resemblance Data Labeling (MARDL) allocate each unlabeled data point into the matching suitable cluster based on the narrative categorical clustering representative. To detect the drifting concepts at different sliding windows, DCD contrast the cluster distributions in the middle of the last clustering consequence and the temporal current clustering result [11]. However, since only an inadequate amount of labeled data are available in the above real world applications, how to establish anomaly of unseen data (or events) draws attention from the researchers in data mining and machine learning communities. Deploying the semantics embedded in web services request and present semantic web services, but the sharing of knowledge is not addressed [4]. Multivariate Reconstructed Phase Space (MRPS) for recognizing multivariate temporal patterns as shown in [13] is characterized by identifying the anomalies or events in a dynamic data system. Supervised model is based on training a data sample from data source with accurate clustering. Network traffic clustering is an important and challenging problem. The objective is to conclude the applications that produce a certain group of packets, such as video, peer-to-peer, gaming, email etc. However approach is no longer effective as there are now many different kinds of network applications, some of which deliberately change their behavior in order not to be detected. Another complexity is that due to isolation requirements and computational problem, it is envisage that classification algorithms are allowed to use only partial information present in the network data and avoid deep packet ",,2014.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c02a596c78e8e669dd7912771ea5f51d57668126,https://www.semanticscholar.org/paper/c02a596c78e8e669dd7912771ea5f51d57668126,Agent Based Pattern Recognition,"data types (ADTs) [WB01] are used in software applications to model real world entities from the application domain. An ADT can be implemented using different data structures. The study of data structures and the algorithms that manipulate them is among the most fundamental topics in computer science [Mou01]. Most of what computer systems spend their time doing is storing, accessing, and manipulating data in one form or another. There are numerous examples from all areas of computer science where a relatively simple application of good data structure techniques resulted in massive savings in computation time and, hence, money. Let us consider that in a software application a Collection ADT (also known as Bag) is used. The main operations supported by a collection of elements are: insertion of an element into the collection, deletion of an element from the collection and searching an element in the collection. In order to better motivate our approach, we performed an experiment considering the List ADT and three data structures for implementing a List: vector (dynamic array), linked list and balanced search tree. The main operations supported by a list of elements are: insertion of an element into the list (at the beginning, at the end, at a certain position), deletion of an element from the list (a given element or from a given position), searching an element in the list, iterating through the list, accessing an element from the list at a certain position and updating an element from a certain position. 4.2 Automatic selection of data representations using ANN Data structures [WB01] provide means to customize an abstract data type according to a given usage scenario. The volume of the processed data and the data access flow in the software application influence the selection of the most appropriate data structure for implementing a certain abstract data type. During the execution of the software application, the data flow and volume is fluctuating due to external factors (such as user interaction), that is why the data structure selection has to be dynamically adapted to the software system’s execution context. This adaptation has to be made during the execution of the software application and it is hard or even impossible to predict by the software developer. Consequently, in our opinion, machine learning techniques would provide a better selection at runtime of the appropriate data structure for implementing a certain abstract data type. CHAPTER 4. SUPERVISED LEARNING IN SOFTWARE DEVELOPMENT 21 Artificial neural networks are emerging as the technology of choice for many applications, such as pattern recognition, speech recognition [SH07], prediction [LB05], system identification and control. We will use a feedforward neural network that will be trained using the backpropagation-momentum learning technique [RN02]. 4.3 Experimental evaluation In this section we aim at evaluating the accuracy of the technique proposed in Section 4.2, i.e. the ANN model’s prediction accuracy. As there is no publicly available case study for the problem of automatic selection of data representations, nor a case study in the related literature that can be reproduced, we consider our own case study. We describe in this section simulation results of applying our learning based approach to a selection problem that will be described below. Starting from the data set given at [For10], we have simulated an experiment for selecting the most appropriate data structure for implementing the List ADT. The considered data set consists of the results of a chemical analysis of wines grown in the same region in Italy but derived from different cultivars. The analysis determined the quantities of 13 constituents found in each types of wines [Win91]. The data set for evaluating the ANN classification model presented in Section 4.2 consists of (input, output) samples collected and pre-processed as we have described in Subsection 4.2.2. An input represents an execution context and the target output is the most suitable implementation for the List ADT (1, 2 or 3 according to the selected implementation). In our case study, as the instantiation of the List ADT occurs in the Wine class, an execution context will contain the values of the attributes of this class (13 attributes corresponding to the wine constituents described at [Win91]). The collected data set consists of 178 input-output samples and will be denoted by D. Considering the experimental results presented above, we can conclude that our approach provides optimized data structure selection and reduces the computational time by selecting the data structure implementation which provides a minimum overall complexity for the operations performed on a certain abstract data type on a given execution scenario. 4.4 Comparison to related work In this section we aim at providing a brief comparison of our approach with several existing approaches for the problem of automatic selection of data representations. To our knowledge, so far, there are no existing machine learning approaches for the considered problem, and, moreover, there are no publicly available case studies for it. 4.5 Automatic selection of data representations using SVM The design and implementation of efficient abstract data types are important issues for software developers. Selecting and creating the appropriate data structure for implementing an abstract data type is not a trivial problem for a software developer, as it is hard to anticipate all the use scenarios of the deployed application. Moreover, it is not clear how to select a good implementation for an abstract data type when access patterns to it are highly variant, or even unpredictable. The problem of automatic data structure selection is a complex one because each particular data structure is usually more efficient for some operations and less efficient for others, that is why a static analysis for choosing the best representation can be inappropriate, as the performed operations can not be statically predicted. Therefore, we propose a predictive model in which the software system learns to choose the appropriate data representation, at runtime, based on the effective data usage pattern. This paper describes a new attempt to use a Support Vector Machine model in order to dynamically select the most suitable representation for an aggregate according to the software system’s execution context. Computational experiments confirm a good performance of the proposed model and indicates the potential of our proposal. The advantages of our approach in comparison with similar approaches are also emphasized. CHAPTER 4. SUPERVISED LEARNING IN SOFTWARE DEVELOPMENT 22 The study of data structures and the algorithms that manipulate them is among the most fundamental topics in computer science [Mou01]. Most of what computer systems spend their time doing is storing, accessing, and manipulating data in one form or another. There are numerous examples from all areas of computer science where a relatively simple application of good data structure techniques resulted in massive savings in computation time and, hence, money. Software applications use abstract data types (ADTs) [WB01] to model real world entities from the application domain. An ADT can be implemented using different data structures. Let us consider that in a software application a Collection ADT (also known as Bag) is used. The main operations supported by a collection of elements are: insertion of an element into the collection, deletion of an element from the collection and searching an element in the collection. In order to better motivate our approach, we performed an experiment considering the List ADT and three data structures for implementing a List: vector (dynamic array), linked list and balanced search tree. The main operations supported by a list of elements are: insertion of an element into the list (at the beginning, at the end, at a certain position), deletion of an element from the list (a given element or from a given position), searching an element in the list, iterating through the list, accessing an element from the list at a certain position and updating an element from a certain position. In this section we present several existing approaches for the problem of automatic selection of data repesentations. To our knowledge, so far, there are no existing machine learning approaches for the considered problem, and, moreover, there are no publicly available case studies for it. Data structures [WB01] provide means to customize an abstract data type according to a given usage scenario. The volume of the processed data and the data access flow in the software application influence the selection of the most appropriate data structure for implementing a certain abstract data type. During the execution of the software application, the data flow and volume is fluctuating due to external factors (such as user interaction), that is why the data structure selection has to be dynamically adapted to the software system’s execution context. This adaptation has to be made during the execution of the software application and it is hard or even impossible to predict by the software developer. Consequently, in our opinion, machine learning techniques would provide a better selection at runtime of the appropriate data structure for implementing a certain abstract data type. First, the software system S is monitored during the execution of a set of scenarios that include the instantiation of the abstract data type T . The result of this supervision performed by a software developer is a set of execution contexts, as well as the type and the number of operations from O performed on T saved in a log file. The software developer will analyze the resulted log file and will decide, for each execution context (input) , the most suitable implementation for T given the execution context (output). This decision will be based on computin",,2012.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ed3f7d88bfa1bf12e08c00bfd455f6f6475e3018,https://www.semanticscholar.org/paper/ed3f7d88bfa1bf12e08c00bfd455f6f6475e3018,Report on the Workshop on Advancing Assisted Cognition Technology for Persons with Traumatic Brain Injury (TBI),"1. Purpose of the Workshop Traumatic Brain Injury (TBI) can impair a variety of cognitive functions, including memory, way finding, and performing multi-step tasks. Today's assistive technology for people with cognitive impairments has limited functionality (for example, simple timer-based reminder systems) and/or requires extensive manual customization (for example, wander-alert systems that sound an alarm if a user moves out of a pre-designated neighborhood). Recently, researchers in computer science, rehabilitation engineering, and rehabilitation medicine have begun to come together to create a new generation of more powerful and flexible technologies for assisting cognition. The envisioned systems will use a variety of wearable and/or environmental sensors to infer a user's context and cognitive state, so that prompts, reminders, and other forms of automatic intervention are more appropriate and less distracting. Machine learning machines will be used to create customized user profiles with less manual input, and to update those profiles as users’ conditions change. Tasks being addressed include navigation, remediation of memory impairments, behavioral self-regulation, and monitoring and guidance in the performance of activities of daily living. In general, the research seeks to leverage advances in artificial intelligence, hardware and sensors, and human-computer interaction, in order to create systems that are more robust, easier to deploy, and less obtrusive to the end user. Research in assisted cognition technology, also called ""cognitive orthotics"", requires interdisciplinary work, with the involvement of computer scientists, of engineers, and of medical researchers and professionals such as psychologists, neuropsychologists, neurologists, and rehabilitation specialists.",,2005.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6a5a25b541d82b72df440d5c9587398d77937937,https://www.semanticscholar.org/paper/6a5a25b541d82b72df440d5c9587398d77937937,Physical properties of marine sediments and their application toward climate change studies,"The Integrated Ocean Drilling Program (IODP) is the world’s largest collaborative research initiative for the study of the ocean seafloor, subseafloor, and their application to earth system science. The proc essing of recovered cores from the subseafloor by IODP has been standardized to mainta in constant, reproducible results and to make the measurements available to the scien tifi community using a worldwide-web based database. The study of physical prop erties is a basic requirement for the study of marine sediments as they form a founda tion data set for most of earth system dynamic studies. This thesis focuses on solving four different scien tific problems using analyses and modeling of marine sediments based on physical properties among other IODP data. The four manuscripts presented here are appli ed cases that use marine sediment physical properties to understand various aspects o f past and future climate change. The main objectives of this dissertation are: valid ation of the design and development steps of a new seafloor observatory that will measu re physical properties over time in IODP boreholes; dimensioning of this system based o n the physical properties previously measured at the deployment site; develop ment of a classification system for the major types of carbonate sediments used to reco nstruct past sea level rise; and reconstructing past Pacific Ocean maximum bottom wa ter salinities during the Last Glaciation Maximum (LGM) based on physical properti es of marine sediments. These objectives were addressed by using multivariate sta tistics, machine learning techniques and a one dimensional diffusion model. A new modular borehole observatory, named SCIMPI (S imple Cabled Instrument for Measuring Parameters In situ) was successfully developed and tested for deployment. This system is equipped to take time se ri measurements of temperature, pressure and electrical resistivity in the geologic al formation where it is emplaced. The characteristics of the system and the testing proce ss are presented in Chapter 1. In Chapter 2, the modular design of the first SCIMP prototype is dimensioned using a clustering approach based on physical prope rties from the deployment site. The multivariate statistics applied show direct rel ation with the geological formation and gas hydrate dynamics at the site. In the third Chapter of this dissertation, physical properties are related to characteristics of marine carbonates that, in turn, a e indicative of their lithological type. Three different models were tested: Linear Di scriminant Analysis, Random Forest and, Support Vector Machines. This study dem onstrates a strong nonlinear relationship between physical properties and carbon ate lithotypes. The results show that machine learning models can help identify lith ologies, thus aiding the selection of sample locations, core-log correlations, and proces sing of these rare core materials. Finally, pore fluid chloride concentration profiles from six Pacific Ocean deep sea sites were used to reconstruct past salinities. Thi s study expands the spatial coverage of salinity reconstructions during the Last Glacial Maximum (LGM) to the equatorial Pacific and North and South Pacific Gyres. These re constructed LGM chloride concentrations of deep Pacific bottom water are ~4% greater than today’s values. This is consistent with the view that the deep ocean den sity structure was primarily controlled by salinity variations during the LGM.",,2013.0,10.23860/diss-lado-insua-tania-2013,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
183094b5dceb37bdbde9805f1960503aad72ff8f,https://www.semanticscholar.org/paper/183094b5dceb37bdbde9805f1960503aad72ff8f,From pixels to semantics: visual concept detection and its applications; Från pixlar till semantik: detektion av visuella koncept samt tillämpningar,"Aalto University, P.O. Box 11000, FI-00076 Aalto www.aalto.fi Author Mats Sjöberg Name of the doctoral dissertation From pixels to semantics: visual concept detection and its applications Publisher School of Science Unit Department of Information and Computer Science Series Aalto University publication series DOCTORAL DISSERTATIONS 156/2014 Field of research Computer and Information Science Manuscript submitted 11 June 2014 Date of the defence 25 November 2014 Permission to publish granted (date) 12 August 2014 Language English Monograph Article dissertation (summary + original articles) Abstract The amount of digital visual information available in the world today is enormous, and the rate at which more is continuously generated is simply unbelievable. For example YouTube gets 100 hours of new video every minute, and Facebook more than 350 million new photos every day. At best, this represents the creativity and knowledge of millions or even billions of people, made available to the entire world thanks to the Internet. The problem is of course: how do we find the ""needle"" that is relevant to us in this enormous ""haystack""? Web search engines such as Google and Bing are decent solutions to find textual content, but finding relevant visual content is as yet an unsolved problem. The core issue is the semantic gap between the raw visual data processed by computers, and the abstract concepts and ideas humans use to communicate. This thesis studies one approach to this problem, namely using mid-level concepts to bridge the semantic gap. These semantic concepts are e.g. objects, locations, persons or events which are relatively concrete and thus comparatively easy to associate with the raw visual data. These can then be used to formulate more abstract queries, or used to index and further organise an image or video database. An overview of semantic concept detection using machine learning techniques is presented here, together with some applications. A central issue is keeping the computational speed and efficiency at a practical level for huge amounts of visual data, while still producing accurate and relevant results. To this end, this thesis studies several fast approximative versions of the popular Support Vector Machine (SVM) algorithm, and proposes some improvements to the fast Self-Organising Map (SOM) algorithm to improve its accuracy. Several large-scale realworld experimental applications are presented including image retrieval using social network tags, video search, indoor location recognition, and semantic visualisation of large image and video databases. The empirical evidence presented in this thesis shows that while the semantic gap problem is still not solved, the semantic concept approach produces concrete improvements to realworld applications. The improvements proposed and evaluated contribute to making the machine learning algorithms faster and thus more practically useful for processing huge amounts of visual data.The amount of digital visual information available in the world today is enormous, and the rate at which more is continuously generated is simply unbelievable. For example YouTube gets 100 hours of new video every minute, and Facebook more than 350 million new photos every day. At best, this represents the creativity and knowledge of millions or even billions of people, made available to the entire world thanks to the Internet. The problem is of course: how do we find the ""needle"" that is relevant to us in this enormous ""haystack""? Web search engines such as Google and Bing are decent solutions to find textual content, but finding relevant visual content is as yet an unsolved problem. The core issue is the semantic gap between the raw visual data processed by computers, and the abstract concepts and ideas humans use to communicate. This thesis studies one approach to this problem, namely using mid-level concepts to bridge the semantic gap. These semantic concepts are e.g. objects, locations, persons or events which are relatively concrete and thus comparatively easy to associate with the raw visual data. These can then be used to formulate more abstract queries, or used to index and further organise an image or video database. An overview of semantic concept detection using machine learning techniques is presented here, together with some applications. A central issue is keeping the computational speed and efficiency at a practical level for huge amounts of visual data, while still producing accurate and relevant results. To this end, this thesis studies several fast approximative versions of the popular Support Vector Machine (SVM) algorithm, and proposes some improvements to the fast Self-Organising Map (SOM) algorithm to improve its accuracy. Several large-scale realworld experimental applications are presented including image retrieval using social network tags, video search, indoor location recognition, and semantic visualisation of large image and video databases. The empirical evidence presented in this thesis shows that while the semantic gap problem is still not solved, the semantic concept approach produces concrete improvements to realworld applications. The improvements proposed and evaluated contribute to making the machine learning algorithms faster and thus more practically useful for processing huge amounts of visual data.",,2014.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
85471a36add4d178d97db3f051f9193c1664d9d4,https://www.semanticscholar.org/paper/85471a36add4d178d97db3f051f9193c1664d9d4,Addressing human bottlenecks in big data,"We live in an era when compute is cheap, data is plentiful, and system software is being given away for free. Today, the critical bottlenecks in data-driven organizations are human bottlenecks, measured in the costs of software developers, IT professionals, and data analysts. How can computer science remain relevant in this context? The Big Data ecosystem presents two archetypal settings for answering this question: NoSQL distributed databases, and analytics on Hadoop. In the case of NoSQL, developers are being asked to build parallel programs for global-scale systems that cannot even guarantee the consistency of a single register of memory. How can this possibly be made to work? I'll talk about what we have seen in the wild in user deployments, and what we've learned from developers and their design patterns. Then I'll present theoretical results — the CALM Theorem — that shed light on what's possible here, and what requires more expensive tools for coordination on top of the typical NoSQL offerings. Finally, I will highlight some new approaches to writing and testing software — exemplified by the Bloom language — that can help developers of distributed software avoid expensive coordination when possible, and have the coordination logic synthesized for them automatically when necessary. In the Hadoop context, the key bottlenecks lie with data analysts and data engineers, who are routinely asked to work with data that cannot possibly be loaded into tools for statistical analytics or visualization. Instead, they have to engage in time-consuming data “wrangling” — to try and figure out what's in their data, whip it into a rectangular shape for analysis, and figure out how to clean and integrate it for use. I'll discuss what we heard talking with data analysts in both academic interviews and commercial engagements. Then I'll talk about how techniques from human-computer interaction, machine learning, and database systems can be brought together to address this human bottleneck, as exemplified by our work on various systems including the Data Wrangler project and Trifacta's platform for data transformation.",Big Data 2014,2014.0,10.1109/BIGDATA.2014.7004205,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
06fafe3ce4b1ac6e6a0fbf718e5acf8fc785e3f4,https://www.semanticscholar.org/paper/06fafe3ce4b1ac6e6a0fbf718e5acf8fc785e3f4,"Governing the Ungovernable: Algorithms, Bots, and Threats to Our Information Comfort-Zones","Laws, norms, policies, and institutions have failed to keep up with advances in artificial intelligence. Popularly, we still think of governance of these systems using quotes and metaphors from science fiction authors. The public awareness of the sophistication and capabilities of current systems are also skewed, often in extremes: predicting robot warfare and mind control or suffering complete naivete. Furthermore, the public often equates artificial intelligence with physical animatronics or other macro-embodiments like self-driving cars. 
 
The reality is one in which intelligent systems are embedded in more and more everyday products and services. The so-called “internet of things” represents a kind of ubiquitous computing that anticipates our needs and provides us information like time on daily commute without asking, in the case of Google Now, or adjusts the room temperature based on usage patterns, in the case of the Nest. More basically though, we see smarter algorithms powering seemingly neutral services like Google’s search engine or Facebook’s news feed. 
 
We are entering an era of personalization and technological service by means of machine learning. The concomitant effects of widespread intelligent systems are still unknown for the long-term, especially given that laws, norms, policies, and institutions will eventually develop. However in the near-term, we are already seeing the positive outcomes of convenience and superior service delivery overshadowed by problematic outcomes like censorship and privacy invasion. And beyond the consumer application of this technology, there exist weaponized versions in use by activists such as those under the banner of Anonymous, and states or terrorists pursuing toolkits for cyberwarfare. 
 
This panel will discuss the current capabilities and emerging trends in the deployment and use of intelligent systems, as well as the unique ethical, legal, and political challenges posed by them. We will explore the gaps that already exist in policies around issues like privacy and cyberwarfare, and how they are exacerbated by new intelligent systems and degrees of automation. Panelists will offer suggestions for policymakers, technology creators, and average users, as well as future research needs to advance the cause of effective governance around intelligent systems. 
 
 
SPEAKERS 
Erhardt Graeff (moderator) is a graduate student at the MIT Media Lab and MIT Center for Civic Media. He has studied and spoken about the civic potential of bots, as well as their privacy issues surrounding their commercial use. Erhardt holds an MPhil in Modern Society and Global Transformations from the University of Cambridge. 
 
Tim Hwang is a researcher of web communities, intelligent systems, and the economics of the Internet. He currently is principal investigator of the Social Architecture of Intelligent Systems project at the Data & Society Research Institute in New York. He has worked with the Mozilla Foundation, the Berkman Center for Internet and Society, the Electronic Frontier Foundation, Creative Commons, Google, Tumblr, and Imgur. He is also the co-founder of ROFLCon, a series of conferences on memes and internet culture. 
 
Kate Darling is a Research Specialist at the MIT Media Lab, and a Fellow at the Harvard Berkman Center for Internet & Society and the Yale Information Society Project. Her passion for technology and robots has led her to interdisciplinary fields: After co-teaching a robot ethics course at Harvard Law School, she now increasingly writes and lectures at the intersection of law and robotics, with focus on the legal impact of social issues. Kate holds a PhD in Intellectual Property at the ETH Zurich. 
 
Paulo Shakarian is a Major in the U.S. Army and an Assistant Professor in the Department of Electrical Engineering and Computer Science, U.S. Military Academy, West Point. Additionally, he is a Research Fellow with the West Point Network Science Center, as well as an Affiliate Scholar with the West Point Cyber Research Center. Paulo is also the primary investigator for the Algorithmic Network Science Group. He is the lead author of Introduction to Cyber-Warfare: A Multidisciplinary Approach. Paulo holds a Ph.D. in Computer Science from the University of Maryland, College Park. 
 
Kashmir Hill is a senior online editor at Forbes writing about privacy, technology and the law at The Not-So Private Parts. She has previously worked for the International Herald Tribune in Hong Kong; The Washington Examiner; the National Press Foundation; and Covington & Burling in Washington, D.C.",,2014.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5e5b5b2f31278953171cdb9b45756308a6dbcef3,https://www.semanticscholar.org/paper/5e5b5b2f31278953171cdb9b45756308a6dbcef3,Theory and practice of coordination algorithms exploiting the generalised distributive law,"A key challenge for modern computer science is the development of technologies that allow interacting computer systems, typically referred as agents, to coordinate their decisions whilst operating in an environment with minimal human intervention. By so doing, the decision making capabilities of each of these agents should be improved by making decisions that take into account what the remaining agents intend to do. Against this background, the focus of this thesis is to study and design new coordination algorithms capable of achieving this improved performance. In this line of work, there are two key research challenges that need to be addressed. First, the current state-of-the-art coordination algorithms have only been tested in simulation. This means that their practical performance still needs to be demonstrated in the real world. Second, none of the existing algorithms are capable of solving problems where the agents need to coordinate over complex decisions which typically require to trade off several parameters such as multiple objectives, the parameters of a sufficient statistic and the sample value and the bounds of an estimator. However, such parameters typically characterise the agents’ interactions within many real world domains. For this reason, deriving algorithms capable of addressing such complex interactions is a key challenge to bring research in coordination algorithms one step closer to successful deployment. The aim of this thesis is to address these two challenges. To achieve this, we make two types of contribution. First, we develop a set practical contributions to address the challenge of testing the performance of state-of-the-art coordination algorithms in the real world. More specifically, we perform a case study on the deployment of the max-sum algorithm, a well known coordination algorithm, on a system that is couched in terms of allowing the first responders at the scene of a disaster to request imagery collection tasks of some of the most relevant areas to a team of unmanned aerial vehicles (UAVs). These agents then coordinate to complete the largest number of tasks. In more detail, max-sum is based on the generalised distributive law (GDL), a well known algebraic framework that has been used in disciplines such as artificial intelligence, machine learning and statistical physics, to derive effective algorithms to solve optimisation problems. Our iv contribution is the deployment of max-sum on real hardware and the evaluation of its performance in a real world setting. More specifically, we deploy max-sum on two UAVs (hexacopters) and test it a number of different settings. These tests show that max-sum does indeed perform well when confronted with the complexity and the unpredictability of the real world. The second category of contributions are theoretical in nature. More specifically, we propose a new framework and a set of solution techniques to address the complex interactions requirement. To achieve this, we move back to theory and tackle a new class of problem involving agents engaged in complex interactions defined by multiple parameters. We name this class partially ordered distributed constraint optimisation problems (PO-DCOPs). Essentially, this generalises the well known distributed constraint optimisation problem (DCOP) framework to settings in which agents make decisions over multiple parameters such as multiple objectives, the parameters of a sufficient statistic and the sample value and the bounds of an estimator. To measure the quality of these decisions, it becomes necessary to strike a balance between these parameters and to achieve this, the outcome of these decisions is represented using partially ordered constraint functions. Given this framework, we present three sub-classes of PO-DCOPs, each focusing on a different type of complex interaction. More specifically, we study (i) multi-objective DCOPs (MO-DCOPs) in which the agents’ decisions are defined over multiple objectives, (ii) risk-aware DCOPs (RA-DCOPs) in which the outcome of the agents’ decisions is not known with certainty and thus, where the agents need to carefully weigh the risk of making decisions that might lead to poor and unexpected outcomes and, (iii) multiarm bandit DCOPs (MAB-DCOPs) where the agents need to learn the outcome of their decisions online. To solve these problems, we again exploit the GDL framework. In particular, we employ the flexibility of the GDL to obtain either optimal or bounded approximate algorithms to solve PO-DCOPs. The key insight is to use the algebraic properties of the GDL to instantiate well known DCOP algorithms such as DPOP, Action GDL or bounded max-sum to solve PO-DCOPs. Given the properties of these algorithms, we derive a new set of solution techniques. To demonstrate their effectiveness, we study the properties of these algorithms empirically on various instances of MO-DCOPs, RA-DCOPs and MAB-DCOPs. Our experiments emphasize two key traits of the algorithms. First, bounded approximate algorithms perform well in terms of our requirements. Second, optimal algorithms incur an increase in both the computation and communication load necessary to solve PO-DCOPs because they are trying to optimally solve a problem which is potentially more complex than canonical DCOPs.",,2012.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5418000890eeef3cf4f813bb5ec747344dfed9e5,https://www.semanticscholar.org/paper/5418000890eeef3cf4f813bb5ec747344dfed9e5,Dynamic Adaptive Remote Health Monitoring for Patients with Chronic Disease,"Chronic diseases are the leading causes of death and disability in the United States. More than 70% of deaths among Americans are caused by chronic diseases and more than 133 million Americans have at least one chronic disease. Due to the prevalence of chronic disease-related issues, it is prudent to seek out methodologies that would facilitate the prevention, monitoring, and feedback for patients with chronic diseases.This dissertation describes WANDA (Weight and Activity with Other Vital Signs Monitoring System); a system that leverages sensor technologies and wireless communications to monitor the health-related measurements of patients with chronic diseases. The system was developed and validated in conjunction with the Computer Science Department, the School of Nursing and Ronald Regan Medical Center at the University of California, Los Angeles to enable real-time patient monitoring, user task optimization, missing data imputation and key clinical symptom prediction. The main contributions of designing and developing the WANDA system are 1) data abstraction and integration of the server side; 2) development of the smartphone and web applications; 3) data backup and recovery; 4) algorithm design and development of missing data imputation; 5) algorithm design and development of task optimization and early adaptive alarm; and 6) system deployment for clinical trials.The WANDA system is a three-tier architecture consisting of wireless sensors, web servers, and back-end data analytics engines. The first tier comprises sensors that measure patients' vital signals and transmit data to the web server tier. The second tier consists of web servers that receive data from the first tier and maintains data integrity. The third tier is a back-end database server that performs data backup and recovery and various data analysis including dynamic task optimization, missing data imputation and adverse event prediction.The WANDA dynamic task optimization function applies data analytics in real-time to discretize continuous features and apply data clustering and association rule mining techniques to manage a sliding window size dynamically and to prioritize required user tasks. The developed algorithm minimizes the number of daily action items required by patients using association rules that satisfy a minimum support, confidence, confirmation and conditional probability thresholds. Each of these tasks maximizes information gain, thereby improving the overall level of patient adherence and satisfaction. Experimental results from applying EM-based clustering and Apriori and confirmation-based rule mining algorithms show that the developed algorithm can reduce the number of user tasks by up to 76.19% with higher confidence levels.Although missing data is highly undesirable as automated alarms may fail to notify healthcare professionals of potentially dangerous patient conditions, many studies reported high missing data rates in remote health monitoring. In this dissertation, I exploit machine learning techniques including projection adjustment by contribution estimation regression (PACE), Bayesian methods, and voting feature interval (VFI) algorithms to predict both non-binomial and binomial data. The experimental results show that the aforementioned algorithms are superior to other methods with high accuracy and recall. This approach also shows an improved ability to predict missing data when training on entire populations, as opposed to training unique classifiers for each individual.The WANDA early adaptive alarm function discretize continuous features, applying Expectation Maximization (EM) clustering and association rule mining techniques for predicting future sensor readings and system non-use dynamically. The experiment results shows that developed algorithm can predict upto 27.08% of sensor readings and system non-use within the next three days.Additionally, the results of performed clinical trials shows that patients monitored by WANDA are less likely to have readings fall outside a healthy raange and have higher adherence rate and more communications with health care professionals.",,2012.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d0390e23cd9b720f634e08b03a660fe8406b378d,https://www.semanticscholar.org/paper/d0390e23cd9b720f634e08b03a660fe8406b378d,Robust Real-Time Recognition of Action Sequences Using a Multi-Camera Network,"Robust Real-Time Recognition of Action Sequences Using a Multi-Camera Network by Rahul Ratnakar Kavi Master of Science in Computer Science West Virginia University Vinod K. Kulathumani, Ph.D., Chair Real-time identification of human activities in urban environments is increasingly becoming important in the context of public safety and national security. Distributed camera networks that provide multiple views of a scene are ideally suited for real-time action recognition. However, deployments of multi-camera based real-time action recognition systems have thus far been inhibited because of several practical issues and restrictive assumptions that are typically made such as the knowledge of a subjects orientation with respect to the cameras, the duration of each action and the conformation of a network deployment during the testing phase to that of a training deployment. In reality, action recognition involves classification of continuously streaming data from multiple views which consists of an interleaved sequence of various human actions. While there has been extensive research on machine learning techniques for action recognition from a single view, the issues arising in the fusion of data from multiple views for reliable action recognition have not received as much attention. In this thesis, I have developed a fusion framework for human action recognition using a multi-camera network that addresses these practical issues of unknown subject orientation, unknown view configurations, action interleaving and variable duration actions. The proposed framework consists of two components: (1) a score-fusion technique that utilizes underlying view-specific supervised learning classifiers to classify an action within a given set of frames and (2) a sliding window technique that is used to parse a sequence of frames into multiple actions. The use of a score-fusion technique as opposed to a feature-level fusion of data from multiple views allows us to robustly classify actions even when camera configurations are arbitrary and different from training phase and at the same time reduces the required network bandwidth for data transmission permitting wireless deployments. Moreover, the proposed framework is independent of the underlying classifier that is used to generate scores for each action snippet and thus offers more flexibility compared to sequential approaches like Hidden Markov Models. The amount of training and parameterization is also significantly lower compared to HMM-based approaches. This Real-Time recognition system has been tested on 4 classifiers which are Linear Discriminant Analysis, Multinomial Naive Bayes, Logistic Regression and Support Vector Machines. Over 90% accuracy has been achieved by this system in Real-Time recognizing variable duration actions performed by the subject. The performance of the system is also shown to be robust to camera failures.",,2012.0,10.33915/etd.151,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b389c75e1382fbc88cccfd8f660e216255bb6302,https://www.semanticscholar.org/paper/b389c75e1382fbc88cccfd8f660e216255bb6302,Modeling the impact of human mobility: mobile devices as sensors and content vectors,"Cellular technology has had a profound impact on modern computing. As an expanding computing paradigm, mobile computing presents many challenges and opportunities. Cell phones give researchers an unprecedented glimpse into the way that humans move, but records of a phone's movement are precise neither in time, nor space. The ease of deploying the devices offers connectivity solutions in areas that previously had little hope of receiving Internet access, but the mobility of the devices means users may be difficult to locate. 
The ubiquity of mobile devices gives researchers a chance to understand how and when people move. By analyzing regional mobility patterns, researchers can understand the impacts of human mobility on communications networks. This information can also be used in a wide range of other fields: social science, urban planning, and biology, among others. Understanding movement can help model human mobility, which may provide better ways to test new communications protocols or experiment with planned changes to an urban area. However, challenges come from the variance in how many calls different users make, and in cell tower density between urban and rural areas. Additionally, the datasets containing records of people's movements are often proprietary and thus unavailable. 
Using an unprecedentedly large dataset of over 3 billion cellphone data records for hundreds of thousands of users in Los Angeles and New York, this dissertation demonstrates techniques to quantitatively characterize and model movement patterns within the cities. By selecting appropriate metrics and validating against a set of volunteers, I show that differences between regions can be accurately observed (e.g., Angelenos, in general, take day-to-day trips that are 34%-53% longer than those of New Yorkers). Further, this thesis presents algorithms based on logistic regressions that allow the discovery of home and work locations of users. Such estimates can be used to calculate average commute distances for a region that are within 1 mile of census estimates. Further, this thesis shows how to combine the macroscopic properties of cities with user profiles (generated via machine learning techniques) to create a fully synthetic city with user behavior that mimics the behavior of actual users. These synthetic cities can be used to model user behaviors in simulations of mobile systems. 
The second half of my dissertation explores using mobile devices such as laptops and cell phones to connect developing regions to the Internet. Access to the Internet is an increasingly powerful way to make available tools for healthcare, education, and more. However, the world has a ""digital divide""; parts of the world have 80% of their population connected to the Internet while other parts have less than 15% Internet penetration. Fortunately, although adoption of traditional computing platforms is slow, cell phone adoption in these areas is very high. Thus, using these and other mobile devices to deliver content is a promising avenue of research into narrowing the divide. 
Despite the promise of mobile devices, many challenges exist. The mobility of the devices means that it is hard to know where a user will be when she wants to access Internet content. Further, the devices may be connected to the Internet through a poor quality Internet connection. Satellite or cellular Internet connections are costly and multi-hop, delay-tolerant connections may have hours of latency. 
To address the challenges to connectivity in developing regions, my dissertation proposes and evaluates collaborative caching techniques. In these techniques, a group of peer-nodes (e.g., laptops or mobile phones) cooperate to store and retrieve data, minimizing how often users need to use the expensive or slow Internet connection. However, because data are shared by all members of the cache, user mobility plays a large role in the efficacy of such a cache. I show that even with user mobility, collaborative caches reduce latencies by over 35%. As a demonstration of the efficacy of the techniques, this thesis showcases a real-world deployment of a system called C-LINK which uses collaborative caching. C-LINK was deployed in rural Nicaragua on top of a multi-hour, delay-tolerant network. 
Overall, this thesis opens up an interdisciplinary body of work exploring the mobile phone as the next generation of computing. Specifically, how mobile devices can help answer questions on topics as far-ranging as carbon footprints and Internet connectivity.",,2012.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e93da65941be2d70b75b02524bdaedce78ae332b,https://www.semanticscholar.org/paper/e93da65941be2d70b75b02524bdaedce78ae332b,INTANGIBLE ASSETS: HOW TO MANAGE THEM?,"This study articulated issues of learning, knowledge creation, circulation of information and practice in the workplace, in research for PhD in Information Science. Data collection was done during the year 2009, in a medium size company providing IT services business to business, located on the outskirts of Belo Horizonte, which has presence in several states. Its clients are private companies, government agencies and police forces. Due to its business model, he needs to follow ""the state of the art"" in IT, keeping your employees constantly updated, considering the intangible assets his greatest asset, as machines are replaced routinely. The research used the analytical perspective of communities of practice and focused on the relationship between information technology use, learning, knowledge creation, focusing on how such relationships took place. The ethnographic methodology was used, with approximately 500 hours of participant observation, at the Service Desk of the company team, that directly interfaces with his clients: IT managers, technicians and users. They operate remote or in field. It presents the collected data, with analysis focused on the partial disclosure of the intangible aspects of workflow management team of partners, chosen because it had the largest number of interfaces within and outside the company. In conclusion show the elements implied in the interactions between socio-technical observed, revealing its similarities to situations and contexts highly specialized in the search for knowledge creation. As a recommendation it presents the principles for cultivating communities of practice, as stated in the previous work of the authors, and can be deployed in the management of technology and information systems provided by the company to its clients.",ICISTM 2010,2010.0,10.4301/CONTECSI9969320101478,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b13bb2275adf2a283a2860747be0b58a4c4f3c70,https://www.semanticscholar.org/paper/b13bb2275adf2a283a2860747be0b58a4c4f3c70,Book Review,"In The Voice in the Machine, Roberto Pieraccini takes the reader on a journey through the history of speech technologies, from the speaking machine of von Kempelen in 1804 through the current uses of statistical machine learning that make possible speech recognition, artificial speech production, speaker recognition, and dialog management. Although this is more of a popular science book about the history of speech technologies than a book dedicated to voice user interface (VUI) design, HCI researchers and VUI designers will very likely find the information accessible and valuable. The fundamental goal of human factors engineering is to strive for optimal allocation of tasks to machines and humans, and you can’t do that if you don’t have a good understanding of machine and human capability. Without getting into the complex mathematics of machine learning, Pieraccini does a superb job of communicating how machines work with speech and provides enough background about human speech to aid an understanding of the capabilities and limitations of current systems. The first chapter provides background in the operating characteristics of human speech and communication. Part of the reason that no one has yet built a machine that has the same speech capabilities as humans is that there are aspects of human language that no one completely understands. Despite this, there is much about human language that we do know, and this chapter does an excellent job of summarizing that research and knowledge base. The next six chapters of the book cover the major speech technology topics, including different approaches to speech recognition, artificial speech production, statistical language models, and dialog management. There are already books and seminal papers that provide the mathematics behind modern speech engineering for technical audiences (e.g., Jelinek, 1997; Manning & Schütze, 1999; Rabiner, 1989). For the more general audience of a popular science book, Pieraccini provides figures, graphs, and accessible analogies to guide readers though complex topics such as endpoint (silence) detection; formantbased template matching; feature vectors and dynamic time warping for isolated-word recognition; and their extensions to continuous speech recognition, artificial speech production, and dialog management. The vast majority of commercial speech systems have components based on training hidden Markov models for statistical acoustic and language modeling (and for more advanced natural language understanding, dialog management). This may seem counterintuitive, but throughout the book Pieraccini shows consistent historical patterns of researchers, starting with attempts to directly model speech input and output with rules, but with continued research eventually establishing the superiority of statistical methods. The tests sponsored by the Defense Advanced Research Project Agency with evaluations by the National Institute of Standards and Technology played a key role in standardizing the comparison of different approaches to speech recognition and natural language processing, primarily through coordinating the collection of a common set of audio (a corpus) to use to train and evaluate the systems under test. From 1988 through 2006 the National Institute of Standards and Technology conducted evaluations of speech read in noise-free environments, spontaneous interactive speech, conversational speech, broadcast speech, and meeting speech (in which the system not only transcribes the speech but must recognize who is speaking). Through 2001, large corporate research centers and universities made much of the investment in developing speech technologies with the goal of eventually creating systems fluent in human speech. In Chapter 8 (“Becoming Real”), Pieraccini moves away from technology to a history of how speech recognition technologies became real programs serving real customers of real companies. The commercialization of speech technologies followed the realization that the technologies did not have to perfectly match human capabilities to be useful. With appropriate design and a constrained domain, it is possible to build applications that are actually useful. Beginning in the 1990s, a number of products became available that would let users dictate to their personal computers, initially one word at a time (e.g., Dragon Dictate, IBM VoiceType), and later allowing continuous speech with interspersed commands (e.g., Dragon NaturallySpeaking, IBM ViaVoice). During the same decade, conversational systems started to appear, beginning with Wildfire, an automated personal telephone assistant. The secret to Wildfire’s success was the way its prompts constrained what callers said in response, limiting the required capability of the speech recognizer—an early example of VUI design. In the first decade of the 21st century, the number of deployed speech applications exploded. Pieraccini notes several key drivers of this explosion. One was the past proliferation of poorly designed interactive voice response applications using",,2004.0,10.1081/jfp-120022982,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
64bf06684e2ce6841815692c8fffe93fad258576,https://www.semanticscholar.org/paper/64bf06684e2ce6841815692c8fffe93fad258576,Transactions on Edutainment II,,Trans. Edutainment,2009.0,10.1007/978-3-642-03270-7,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b216d74b2f54ebbc675060b80cbe3b0c8c7c0fb3,https://www.semanticscholar.org/paper/b216d74b2f54ebbc675060b80cbe3b0c8c7c0fb3,"FUZZY LOGIC AND PROBABILITY APPLICATIONS: Bridging the Gap, edited by Timothy J. Ross, Jane M. Booker and W. Jerry Parkinson. Society for Industrial and Applied Mathematics, Philadelphia, and American Statistical Association, Alexandria, Virginia, 2002, xxiii + 409 pages, ISBN 0-89871-525-3.","LEARNING BAYESIAN NETWORKS, by Richard E. Neapolitan. Prentice Hall, Upper Saddle River, NJ, 2004, xv þ 674 pages, ISBN 0-13-012534-2. This book is the first unified text as well as a useful reference on the theory and applications of Bayesian networks. It deals with probabilistic modeling, especially in the areas of computer science, computer engineering, and electrical engineering. It is also a valuable resource for courses on expert systems, machine learning, and artificial intelligence. Appropriate for classroom teaching or self-instruction, the text is organized to provide fundamental concepts in an accessible, practical format. Beginning with a basic theoretical introduction, the author then provides a comprehensive discussion of inference, methods of learning, and applications based on Bayesian networks. The book contains hundreds of examples and problems which make learning of rather complex concepts easy. ENTROPY MEASURES, MAXIMUM ENTROPY PRINCIPLE AND EMERGING APPLICATIONS, edited by Karmeshu. Springer-Verlag, Heidelberg and New York, 2002, x þ 297 pages, ISBN 3-540-00242-1. This book is dedicated to the memory of Professor J. N. Kapur for his significant contributions to the field of entropy measures and maximum entropy applications. BOOK REVIEWS AND ABSTRACTS 638 D ow nl oa de d by [ Q ue en sl an d U ni ve rs ity o f T ec hn ol og y] a t 1 3: 57 3 1 O ct ob er 2 01 4 Eminent scholars from various fields of applied information theory have contributed to this festschrift volume which was planned on the occasion of his 75th birthday. The articles cover topics in the areas of physical, biological, engineering and social sciences such as information technology, soft computing, nonlinear systems and molecular biology with a thematic coherence. The volume will be useful to researchers working in these fields enabling them to see the underlying unity and power of entropy optimization frameworks. MULTI-ROBOT SYSTEMS: From Swarms to Intelligent Automata, Volume III, edited by Alan C. Schultz, Lynne E. Parker, and Frank E. Schneider. Kluwer Academic Publishers, Dordrecht, Boston and London, 2003, ix þ 307 pages, ISBN 1-4020-1185-7. This proceedings volume documents recent cutting-edge developments in multi-robot systems research and is the result of the Second International Workshop on Multi-Robot Systems that was held in March 2003 at the Naval Research Laboratory in Washington, D.C. This workshop brought together top researchers working in areas relevant to designing teams of autonomous vehicles, including robots and unmanned ground, air, surface, and undersea vehicles. The workshop focused on the challenging issues of team architectures, vehicles learning adaptation, heterogeneous group control and cooperation, task selection, dynamic autonomy, mixed initiative, and human and robot team interaction. A broad range of applications of this technology are presented in this volume including UCAVs (Unmanned Combat Air Vehicles), planetary exploration, assembly in space, cleanup, and urban search and rescue. This proceedings volume represents the contributions of the top researchers in this field and serves as a valuable tool for professionals in this interdisciplinary field. NUMERICAL AND ANALYTICAL METHODS FOR SCIENTISTS AND ENGINEERS USING MATHEMATICA, by Daniel Dubin. Wiley-Interscience, Hoboken, NJ, 2003, xvi þ 633 pages, ISBN 0-471-26610-8. Written from the perspective of a physicist rather than a mathematician, this book focuses on modern practical applications in the physical and engineering sciences, attacking these problems with a range of numerical and analytical methods, both elementary and advanced. Incorporating the widely used and highly praised Mathematicaw software package, the author offers solution techniques for the partial differential equations of mathematical physics such as Poisson’s equations, the wave equation, and Schrödinger’s equation, including Fourier series and transforms, Green’s functions, the method of characteristics, grids, Galerkin and simulation methods, elementary probability theory, and statistical methods. The book is designed for both advanced undergraduate and graduate students in the physical and engineering sciences, as well as professionals who want to learn these methods. The full text of the book is also provided electronically on an accompanying CD, along with animotions, user-modifiable source code, and links to related Web material. BOOK REVIEWS AND ABSTRACTS 639 D ow nl oa de d by [ Q ue en sl an d U ni ve rs ity o f T ec hn ol og y] a t 1 3: 57 3 1 O ct ob er 2 01 4 COMPUTATIONAL INTELLIGENT SYSTEMS FOR APPLIED RESEARCH: Proceedings of the Fifth International FLINS Conference, edited by Da Ruan, Pierre D’hondt, and Etienne E. Kerre. World Scientific, Singapore, 2002, xii þ 591 pages, ISBN 981-238-066-3. FLINS is an acronym for Fuzzy Logic and Intelligent Technologies in Nuclear Science. FLINS 2002 is the fifth in a series of FLINS conferences and covers state-of-the-art research and development in computational intelligence for applied research in general and for unclear science and engineering in particular. This book outlines the trends in computational intelligence in control, decision-making, and nuclear engineering, and presents the latest developments of computational intelligent systems in applied research and nuclear applications. ENGINEERING AND SCIENTIFIC COMPUTATIONS USING MATLAB, by Sergey E. Lyshevski. Wiley-Interscience, Hoboken, NJ., 2003, x þ 227 pages, ISBN 0-471-46200-4. Going beyond the traditional MATLAB user manuals and college texts, this book guides you through the most important aspects and basics of MATLAB programming and problem solving from fundamentals to practice. Augmenting its discussion with a wealth of practical worked-out examples and qualitative illustrations, the book demonstrates MATLAB’s capabilities and offers step-by-step instructions on how to apply the theory to a practical realworld problem. The book is user-friendly and comprehensive in scope. NON-LOCALITY AND MODALITY, edited by Tomasz Placek and Jeremy Butterfield. Kluwer Academic Publishers, Dordrecht, Boston and London, 2002, x þ 352 pages, ISBN 1-4020-0661-6. This book collects together the invited contributions to the NATOAdvanced Research Workshop on Modality, Probability, and Bell’s Theorems, which was held in the Kolegium Polonijne in Cracow, Poland, on August 19–23, 2001. It contains 20 contributions that are divided into several groups. The first group consists of three papers that give a philosophical re-examination of the views of Bohr and von Neumann on quantum theory, especially concerning non-locality. It is followed by a group of papers regarding the various interpretations of quantum theory and a group focusing on entanglement and non-locality. The next group of papers applies unsharp quantum theory to the Bell and Kochen-Specker theorems. Finally, there are two groups of papers about more philosophical aspects of quantum theory, one focusing on philosophical doctrines about causation and one on doctrines about modality. THE NATURE OF TIME: Geometry, Physics, and Perception, edited by Rosolino Buccheri, Metod Saniga, and William Mark Stuckey. Kluwer Academic Publishers, Dordrecht, Boston and London, 2003, xvii þ 446 pages, ISBN 1-4020-1200-4. The book is a product of the NATO Advanced Research Workshop on the Nature of Time that was held in Tatranská Lomnica, Slovakia, on May 21-24, 2002. It consists of BOOK REVIEWS AND ABSTRACTS 640 D ow nl oa de d by [ Q ue en sl an d U ni ve rs ity o f T ec hn ol og y] a t 1 3: 57 3 1 O ct ob er 2 01 4 40 contributions that are grouped into four chapters: 1. Internal Times and Consciousness; 2. Mathematical Approaches to the Concept of Time; 3. The Physicist’s View of Time; and 4. Integrative Science’s View of Time. Each chapter also contains an overview of issues covered in that chapter. As these chapter titles suggest, the concept of time is examined from several distinct perspectives. The contributions taken as a whole represent the most up-todate scholarly research on the enigmatic nature of time. The book should be of interest to readers of this journal since it contains a lot of material pertaining to systems of various kinds. SYSTEMATIC ORGANISATION OF INFORMATION IN FUZZY SYSTEMS, edited by Pedro Melo-Pinto, Horia-Nicolai Teodorescu, and Toshio Fukuda. IOS Press, Amsterdam, 2003, viii þ 399 pages, ISBN 1-58603-295-X. This volume is based on contributions presented at the NATO Advanced Workshop (devoted to the subject area described by the title of the volume), which took place in Vila Real, Portugal, on October 24–26, 2001. It consists of 24 contributions that discuss recent developments in the area of fuzzy systems, especially those pertaining to their role in information processing and knowledge discovery. The book is an important resource for researchers and graduate students in computer, information and systems sciences. NONLINEAR DIFFERENCE EQUATIONS: Theory with Applications to Social Science Models, by Hassan Sedaghat. Kluwer Academic Publishers, Dordrecht, Boston, and London, xv þ 388 pages, ISBN 1-4020-1116-4. This book presents a rare mix of the latest mathematical theory and procedures in the area of nonlinear difference equations and discrete dynamical systems, together with applications of this theory to models in economics and other social sciences. The theoretical results include not only familiar topics on chaos, bifurcation, stability and instability of cycles and equilibria, but also some recently published and some as yet unpublished results on these and related topics (e.g. the theory of semiconjugates). In addition to rigorous mathematical analysis, the book discusses several social science models and analyzes some of them in substantial detail. This book is of potential interest to professionals and graduate students in mathematics and applied mathematics, as well as researchers in",Int. J. Gen. Syst.,2003.0,10.1080/03081070310001618272,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
da1feaf9b8d64cd18ae0d08334c8b987fd89afc9,https://www.semanticscholar.org/paper/da1feaf9b8d64cd18ae0d08334c8b987fd89afc9,"Artificial intelligence in economics and management : an edited proceedings on the Fourth International Workshop : AIEM4, Tel-Aviv, Israel, January 8-10, 1996","Foreword. Part I: Artificial Intelligence Techniques. Using Machine Learning, Neural Networks And Statistics to Predict Corporate Bankruptcy: A Comparative Study P.P.M. Pompe, A.J. Feelder. Prolog Business Objects in a Three-Tier Architecture D.G. Schwartz. The Effect of Training Data Set Size and the Complexity of the Separation Function on Neural Network Classification Capability: The Two-Group Case M. Leshno, Y. Spector. Imaginal Agents D.G. Schwartz, D. Te'eni. Part II: Financial Applications. Financial Product Representation and Development Using a Rule-Based System A. Lange, et al. Applications of Artificial Intelligence and Cognitive Science Techniques in Banking P. Lenca. Part III: Business Applications. AI-Supported Quality Function Deployment Y. Reich. Knowledge Reuse in Mass Customization of Knowledge-Intensive Services M. Benaroch. Harvest Optimization of Citrus Crop Using Genetic Algorithms N. Levin, J. Zahavi. 'Corpus', An Approach to Capitalizing Company Knowledge M. Grundstein. Part IV: Economic Applications. Fuzzy Approach in Economic Modelling of Economics of Growth V. Deinichenko, et al. Computer Based Analysis of an Economy in Transition to Steady State Equilibrium K. Cichocki, T. Szapiro. A Multistrategy Conceptual Analysis of Economic Data K.A. Kaufman, R.S. Michalski. The Credible Modeling of Economic Agents with Limited Rationality B. Edmonds, S. Moss. Reasoning and A Programming Language for Simulating Economic and Business Processes with Artificially Intelligent Agents B. Edmonds, et al. Part V: Qualitative and Cognitive Research. Information Processing, Motivation and Decision Making L.M. Botelho, H. Coelho. A PracticalTool for Explanation of Quantitative Model Behavior R. Berndsen. Practical Application of Artificial Intelligence in Education and Training L. Dannhauser.",,1996.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
707f495c06e84d9485652ba619e908dc69bfe405,https://www.semanticscholar.org/paper/707f495c06e84d9485652ba619e908dc69bfe405,Representation and decision making in the immune system,"The immune system has long been attributed cognitive capacities such as 
""recognition"" of pathogenic agents; ""memory"" of 
previous infections; ""regulation"" of a cavalry of detector and 
effector cells; and ""adaptation"" to a changing environment and 
evolving threats. Ostensibly, in preventing disease the immune system must be capable of discriminating states of pathology in the organism; identifying causal agents or ``pathogens''; and correctly deploying lethal effector mechanisms. What is 
more, these behaviours must be learnt insomuch as the paternal genes cannot 
encode the pathogenic environment of the child. 
Insights into the mechanisms underlying these phenomena are of 
interest, not only to immunologists, but to computer scientists pushing the 
envelope of machine autonomy. 
 
This thesis approaches these phenomena from the perspective that immunological 
processes are inherently inferential processes. By considering the immune system 
as a statistical decision maker, we attempt to build a bridge between 
the traditionally distinct fields of biological modelling and statistical 
modelling. Through a mixture of novel theoretical and empirical analysis we 
assert the efficacy of competitive exclusion as a general principle 
that benefits both. For the immunologist, the statistical modelling perspective 
allows us to better determine that which is phenomenologically sufficient from 
the mass of observational data, providing quantitative insight that may offer 
relief from existing dichotomies. For the computer scientist, the biological 
modelling perspective results in a theoretically transparent and empirically 
effective numerical method that is able to finesse the trade-off between myopic 
greediness and intractability in domains such as sparse approximation, continuous learning and boosting weak heuristics. Together, we offer this as a modern reformulation of the interface between computer science and immunology, established in the seminal work of Perelson and collaborators, over 20 years ago.",,2010.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5ce53e4276a89af12c3a4201fdf9e63817b06108,https://www.semanticscholar.org/paper/5ce53e4276a89af12c3a4201fdf9e63817b06108,Keynote Speakers,"1.Prof. Ee-Peng Lim Title: Multimodal Sensemaking using Social Media Data Abstract: As social media becomes an integral part of daily lives, it captures interesting user generated content and behaviour data that can be sensed and analysed. While social media companies use the insights learnt from such data to improve their user interface and experience, there are many other interesting insights that help us improve urban environment and public services. Social media data also offers a cheap and scalable approach to perform sensemaking on the urban environment. In this talk, we will showcase a few ongoing research projects in the Living Analytics Research Centre (LARC) which focus on multimodal sensemaking using social media data. The talk will share some new machine learning methods and systems to profile users, locations, and public transportation services. The reasonably good accuracy of these methods also allow them to be deployed in urban application solutions. Biography: Ee-Peng Lim is a professor at the School of Information Systems of Singapore Management University (SMU). He received Ph.D. from the University of Minnesota, Minneapolis. His research interests include social network and web mining, information integration, and digital libraries. He is the co-Director of the Living Analytics Research Center (LARC) jointly established by SMU and Carnegie Mellon University. He is currently an Associate Editor of the ACM Transactions on Information Systems (TOIS), ACM Transactions on the Web (TWeb), IEEE Transactions on Knowledge and Data Engineering (TKDE), Information Processing and Management (IPM), Social Network Analysis and Mining, Journal of Web Engineering (JWE), IEEE Intelligent Systems, International Journal of Digital Libraries (IJDL) and International Journal of Data Warehousing and Mining (IJDWM). He was a member of the ACM Publications Board until December 2012. He serves on the Steering Committee of the International Conference on Asian Digital Libraries (ICADL), Pacific Asia Conference on Knowledge Discovery and Data Mining (PAKDD), and International Conference on Social Informatics (Socinfo). 2.Prof. T.C. Chang Title: Critical Tourism Studies in Asia: Issues and Questions Abstract: When King and Porananond (2014: 6) first used the phrase “[to] Asianise the field” in their edited book Rethinking Asian Tourism: Cultures, Encounters and Local Response, they were encouraging greater indigenous scholarship in Asian tourism studies. It is their belief that Asian scholars working in and on Asia bring useful insights and unique perspectives that might differ from non-Asian research, thereby creating more holistic understandings of the world. Such an Asian-centric scholarly inclination has bloomed particularly in the 2000s following increasing numbers of Asians on tour, greater intra- and domestic-tourism within Asia, and the opening of new travel destinations throughout the continent. Around the same time as this surge in Asian tourists and indigenous tourism scholarship is an increasing attention paid to ‘Critical Tourism Studies’ (CTS). The basic premise of CTS is the application of social-cultural theory in tourism analyses, the acknowledgement of researcher bias, and a challenge to mainstream knowledge through a focus on the ‘Other’ (which might include the marginalised, oppressed and the indigenous among other interest groups). In this presentation, I will consider the ‘global’ scope of CTS and the ‘local’ field of Asian tourism knowledge in three ways. Firstly, we will examine what is critical about CTS and how it has evolved over time. Secondly the implications of CTS for Asian tourism research will be explored, and finally issues and questions surrounding critical Asian tourism scholarship will be surfaced. It is the aim of this talk to envision the full potential of CTS if the breadth and depth of ‘local/Asian’ tourism knowledge is brought to bear on its ‘global/lofty’ research agenda. Biography: T.C. Chang is an Associate Professor at the Department of Geography, National University of Singapore (NUS). He was also Assistant and Vice Dean of the Faculty of Arts and Social Sciences (NUS) from 2008 till 2015. He is the co-editor of two books on Asian tourism: Interconnected Worlds: Tourism in Southeast Asia (with Peggy Teo and K.C. Ho; Elsevier, Oxford) and Asia On Tour: Exploring the Rise of Asian Tourism (with Tim Winter and Peggy Teo; Routledge, London and New York). His research interests include Asian tourism knowledge; urban tourism and urban development issues; arts, culture and creativity. 3.Dr Kao Cai Title: An Innovative Model for Municipal Solid Waste Collection Routing in Singapore Abstract: To collect and dispose growing amounts of municipal solid waste (MSW) changed to be a hot topic along with the rapid urbanization in past decades. Cities are more and more dependent on the incineration instead of landfilling due to the cost-efficiency and environmental concerns. Considering the limited number of incineration plants and complicated situation of transportation in both spatial and temporal dimensions in different cities, the optimal routing for waste collection turns to be meaningful research topic. In this research, the ant colony optimization (ACO)-based multi-objective routing model coupled with min-max model and Dijkstra's algorithm is proposed to address the question of which route to take from these waste-generating points to the target incineration plant(s) considering travel time, accident probability (black spots), and population exposure, so as to support the routing decision-making. The model is successfully implemented in Singapore and the effectiveness of the model has also been justified. Besides, few limitations of this research have also been discussed, some of which would also be the future directions of our research, especially the design and integration of a web-based routing decision-making support system. Short Bio: Dr. Kai Cao holds a Ph.D. degree in Geography from the Chinese University of Hong Kong in Hong Kong and obtained his B.S. and M.Phil. degrees in Geography from Nanjing University in China. Prior to joining Department of Geography at NUS, he had also worked in Center for Geographic Analysis at Harvard University, Department of Geography at University of Illinois at Urbana-Champaign, and World History Center at University of Pittsburgh respectively. His current research interests center on GIScience and its applications, especially on the topics of spatial simulation and optimization, cyberinfrastructure and geocomputation, land use planning support and urban mobility. 4.Prof. Ida Ayu Dwi Giriantari Title: Renewable Energy Deployment in Indonesia; opportunity and challenge Short Bio: Ida Ayu Dwi Giriantari is Professor in Electrical Engineering Udayana University. She hold PhD from The University of New South Wales in 2003. Her research areas are in Power Plant, Renewable Energy and Power Transformers. She is Head of Magister Program of Electrical Engineering, Udayana University.",1994 Internatonal Conference on Parallel Processing Vol. 2,2018.0,10.1088/1755-1315/38/1/011002,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1529f58b6f883888b798aca8590fac0d85b96375,https://www.semanticscholar.org/paper/1529f58b6f883888b798aca8590fac0d85b96375,A component-model approach to determining what to learn *,"Research in machine learning has typically addressed the problem of how and when to learn, and ignored the problem of formulating learning tasks in the first place. This paper addresses this issue in the context of the CASTLE system,1 that dynamically formulates learning tasks for a given situation. Our approach utilizes an explicit model of the decision-making process to pinpoint which system component should be improved. CASTLE can then focus the learning process on the issues involved in improving the performance of the particular component. 1. Determining what to learn A theory of learning must ultimately address three issues: when to learn, what to learn, and how to learn. The overwhelming majority of research in machine learning has been concerned exclusively with the last of these questions, how to learn. This work ranges from work in purely inductive category formation to more knowledge-based approaches. The aim of this work has generally been to develop and explore algorithms for generalizing or specializing category definitions. The nature of the categories being defined---i.e., what is being learned---is rarely a consideration in the development of these algorithms. For purely inductive approaches, this is entirely a matter of the empirical data that serves as input to the learner. In explanation-based approaches (EBL), it is a matter of the user-defined ``goal concept''---in other words, input of another sort. In neither case is the question of what is being learned taken to be within the purview of the model under development.2 Some work---in particular that in which learning has been addressed within the context of performing a task---has addressed the first question above, namely, when to learn. A * The research presented in this paper was carried out at the Institute for the Learning Sciences at Northwestern University, and is discussed in full in the author’s Ph.D. thesis [Krulwich, 1993]. 1CASTLE stands for C oncocting A bstract S trategies T hrough L earning from E xpectation-failures. 2 Although the need to address this question in the actual deployment of EBL algorithms has been explicitly recognized [Mitchell et. al., 1986, p. 72]. common approach to this issue, known as failure-driven learning, is based on the idea that a system should learn in response to performance failures. The direct connection this establishes between learning and task performance has made this approach among the most widespread in learning to plan. For the most part, however, even these models do not address the second question above, what to learn. In many cases, this is because the models are only capable of learning one type of lesson. What to learn is thus predetermined. For example, many systems that learn exclusively from planner success always learn the same thing, namely a generalized form of the plan that was created (e.g., [Mitchell, 1990]). Similarly, many systems that learn from plan outcomes always learn the same type of planning knowledge--e.g., when it is feasible to make certain simplifying assumptions or to defer planning---from each situation (e.g., [Chien, 1990; DeJong et. al., 1993]). Even systems that can learn more than one thing generally do so in a predetermined and inflexible fashion (e.g., [Minton, 1988; Hammond, 1989]). While this type of solution can often be effective for any individual application setting, it fails to provide an account of how a learning system could determine for itself what to learn, and do so in a manner that is flexible enough to take account of the internal and external context of learning. For a system that is capable of learning a wide variety of types of concepts, in a wide variety of settings, the number of hardwired mapping rules required to do this would be very large, and the rules themselves would get very difficult to manage or reason about, and may even be impossible to formulate. More importantly, however, is the fact that hard-wired rules of this type do not provide a theory of determining what to learn. Just as a set of rules for actions can result in intelligent behavior without providing a foundation for the actions, hard-wired rules for determining what to learn can be effective but nonetheless do not necessarily provide a theory underlying these decisions. The point is that just as complex decisions about actions are very difficult to formulate using hard-wired rules, and thus require inference, so too complex decisions about what to learn require inference. 2. An everyday example Consider the case of a person cooking rice pilaf for the first time. The last step in the directions says to ``cover the pot From: AAAI Technical Report SS-94-02. Compilation copyright © 1994, AAAI (www.aaai.org). All rights reserved. and cook for 25-30 minutes.'' Suppose the person starts the rice cooking and then goes off to do something else---say, clean up the house. In the interim, the pot boils over. When the person returns to the kitchen a half-hour later, the rice pilaf is ruined. What should be learned from this sequence of events? Intuitively we can imagine a number of lessons that might be learned: • Whenever a covered pot containing liquid is on the stove, keep an ear peeled for the sound of the lid bouncing or the sound of the water bubbling. • Do not put a covered pot with liquid in it over a high flame, because it will boil over. The flame should be turned down. • When cooking over a high flame, leave the pot uncovered or the lid ajar. • Don't do loud things while cooking on the stove. • When cooking liquid in a covered pot, stay in the kitchen, because it's hard to hear a pot boiling over from the other rooms. • Don't cook over high flame when busy. While all of these lessons are sensible, they are very disparate, in that they address very different issues. The lessons concern different aspects of behavior, refer to different portions of the agent's plan, and are expressed in different vocabularies. It is difficult to imagine how any learning process that did not distinguish among these alternatives in some way would be capable of such diverse behavior. Rather, it seems more likely that before the agent can undertake the task of learning from the mistake, he must select a lesson (or set of lessons) to learn. In other words, given that the agent has decided to learn from the mistake, and given that he is capable of carrying out the learning task, he still has to first determine what to learn. We see, then, that the agent could learn several things in response to the rice pilaf boiling over. Which of the lessons the agent should learn, whether changes to the cooking methods, the idea of staying in the kitchen, or of tuning his perceptual apparatus, depend on the agent's perceptual and planning abilities, and on his knowledge of the domain. The key point is that many different lessons are possible. Any approach to determining what to learn must be flexible enough to account for this diversity. 3. Modeling cognitive tasks What would an appropriate theory of determining what to learn look like? Imagine the thought processes going on in the agent's head (consciously or subconsciously) in viewing his situation and considering what lesson to learn: • Why was the rice ruined? The rice boiled over. • Could I have done something differently at the time I started the rice cooking to prevent the problem? Yes, I could have lowered the flame or uncovered the pot. • Without doing this, could I have prevented it from boiling over? Yes, if I had heard it. • Could I have heard it boiling over? Maybe I could have, if I'd paid more attention. • Why couldn't I hear it boiling over? I was using the vacuum in the living room. • Could I have planned things differently to enable me to hear? Yes, I could have delayed vacuuming or stopped every few minutes to check the rice.",,1999.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
21be8fe004dcfb56692fc632dd6ca070021ee23f,https://www.semanticscholar.org/paper/21be8fe004dcfb56692fc632dd6ca070021ee23f,"From Revolutionary to Evolutionary: 10 Years of 1-to-1 Computing: Laptop Initiatives Are Now a Decade Old. Once a Point of Controversy, They Have Become the Cornerstone of Every District's Technology Hopes","ONE-TO-ONE COMPUTING is aging quite nicely, thank you. It's getting on 10 years since school districts began experimenting with laptop initiatives, and their efforts--controversial at first, with school leaders and parents questioning the allocation of resources--are now paying enormous dividends. Today large 1-to-1 deployments in schools are welcomed for the transforming educational benefits they are anticipated to bring. The anticipation is warranted. Studies on the effects of 1-to-1 computing, from sources such as the Center for Research in Educational Policy (www.cre.mem his.edu) and NetDay (www.netday.org), all support the premise that student access to computers in the classroom improves student engagement and achievement, and helps students acquire critical 21st-century skills. Whether the computers are desktops, laptops, or tablets, in a lab, on a wireless cart, or on a kid's bedroom desk, students want 1-to-1 access and see computers as learning tools, as essential as a pencil or calculator. Educators agree: What makes the difference is not individual possession of a computer, but rather the availability of computers for classroom instruction. But what will ultimately determine the degree of success a 1-to-1 initiative has is the quality of professional and curriculum development a school provides to support teachers in integrating computers into instruction. Tablets Take Hold Laptops were long the machine of choice for 1-to-1 programs, but tablets are gaining popularity. On the outskirts of Chicago lies Hinsdale Township High School District 86, which numbers about 4,600 students in its two high schools. In 2005, Hinsdale deployed tablet PCs on wireless carts to its teachers and students. The district trained teachers in the use of the tablets and developed curriculum for math, science, and humanities courses that helped them include the computers in classroom lessons. Hinsdale's use of wireless carts is a widely used solution for what can be a daunting appropriation of resources in both staff time and dollars. Carts are popular because they offer plug-and-play computers for all students. Teachers find this minimizes classroom management and tech-support issues. ""One-to-one computing and tablet PCs are not revolutionary,"" says Tim Hohman, Hinsdale's director of technology. ""They are evolutionary. We have been building toward this for 10 years, starting with one computer in a classroom, to minilabs, to wireless carts. One-to-one is the next logical step in student computing and learning."" Adam Fischer, director of information services and technology at Kent School in Connecticut, says his school has also moved its 1-to-1 initiative from notebooks to tablet PCs. The reason for the move can be seen in the classroom of veteran Kent physics teacher Peter Goodwin. Goodwin's students solve physics equations on their tablets and e-mail their work to him. By reviewing their work step-by-step, Goodwin can isolate exactly where students make mistakes. He then works the problems out in class using the tablet and an LCD projector, posting both the problem and the annotated solution on the class website. Goodwin, after more than 25 years of teaching, has found that the tablets enable him to cover far more material with a higher rate of student mastery. The transition from laptops to tablets was made possible, according to Fischer, by Kent's diligent teacher training efforts. ""Instead of dry tutorial sessions, we want teachers in 1-to-1 programs to experiment with the machines,"" he says. ""In addition to regular training sessions, we let our teachers take the tablets home on weekends and over the summer. …",,2006.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fea46a73703538ddcc5d7e80e787ee189b6a76e5,https://www.semanticscholar.org/paper/fea46a73703538ddcc5d7e80e787ee189b6a76e5,The Composite Materials Manufacturaing HUB - Crowd Sourcing as the Norm,"The Composites Manufacturing HUB puts composites manufacturing simulations in the hands of those who need them to invent new and innovative ways to capture the extraordinary benefits of these high performance products at an acceptable manufactured cost. The HUB provides the user simple browser access to powerful tools that simulate the actual steps and outcome conditions of a complex manufacturing process without the need to download and maintain software in the conventional manner. Learning use of the manufacturing simulation tools will also be accomplished on the HUB in order to allow for continuous learning and growth of the human talent required in composites manufacturing. Need for manufacturing simulation ! ! Simulation in the design of composite structure has developed during the past four decades to a level of sophistication that allows for the successful design of complex integrated structural geometries consisting of multiaxial composite laminates of curvilinear geometry, sandwich construction, adhesive and mechanical joints, as well as, monocot constructions that possess significantly less sub-assemblies over their metallic counterparts. The Boeing 787 Dreamliner is one example of the success that this simulation capability has achieved to date (1). Here the forward fuselage shown in Figure 1 (40-ft. in length and 20-ft. in diameter) is designed and constructed as a single assembly. Simulation of the complex geometry and performance characteristics of this composite structure were enabled through geometric modeling, multi-axial laminate analysis of the material architecture and structural analysis of the forward fuselage structure. Sophisticated computer simulation codes now offer simulation tool sets that address these design issues. Simulation of the manufacturing of composite structure is not at the same level of development as that of design simulation. VISTAGY Inc. (Waltham, Mass.) recently announced the results of its composites engineering benchmarking survey entitled, ""How do your Composite Design Processes Compare to Industry Best Practices?""(2) The results of the study revealed that only 56 percent of the composite design companies surveyed considered themselves knowledgeable in composites manufacturing practices and were able to apply that knowledge during design. This suggests that 44 percent of companies need to enhance their knowledge of the manufacturing process if they are to improve their competitiveness. The process for developing new manufacturing simulation tools remains in its infancy. Unlike design simulation software, the manufacturing of polymer composite materials and structures involves multi-physics phenomena such as the curing reactions of thermoset polymers, melting and solidification of thermoplastic polymers, flow and impregnation of viscous polymers in fibrous preforms and tows, consolidation of fiber preforms, conduction and convective heat transfer, geometric conformation of fiber preforms to curvilinear surfaces, residual deformations due to anisotropy in thermal expansion and tooling-composite thermal interactions. These phenomena span the disciplines of polymer science, rheology, reaction kinetics, fluid mechanics of nonNewtonian liquids, heat and mass transfer, mathematical topology, anisotropic thermoelasticity and viscoelasticity. While multi-physics analysis tools have recently been introduced, their use in composites manufacturing simulation is still quite early. There are commercial tools offer a broad range of physical modeling capabilities to model flow, turbulence, heat transfer, and reactions for industrial applications. There is a strong economic driving force from the automotive industry to accelerate the development of manufacturing design tools and to discover lower cost manufacturing techniques. More recently, specialized simulation tools have been developed to address specific aspects of composites manufacturing. There is a commercial suite of software tools that supports multi-axial laminate definition and generation of flat patterns for sharing design data with the manufacturing floor. This tool creates ply geometry by defining transitions with sequence, drop-off and stagger profiles that automatically populate the CAD model. It can determine variable offset surfaces and solids, including mock-up surfaces for interference checking, mating surfaces that model where two parts join together and tooling surfaces for manufacturing. The tool provides manufacturing details such as splices, darts and tape courses and can develop data such as flat patterns and data to drive automated cutting machines, laser projection systems, fiber placement machines and tape laying machines. Another type of simulation tool, uses finite element (FE) software to simulate large deformations of highly anisotropic materials in the sheet forming process. There is also a commercial tool that simulates the curing and thermal deformations of thermoset polymer composites. Its foundation is a coupled thermochemical-stress-flow model with a dynamic autoclave controller simulation. It is, in essence, a virtual autoclave, equipped with capabilities enabling one to consider the following process parameters: heat transfer/autoclave characteristics, resin cure kinetics, multidirectional laminates/fabrics, honeycomb panels, thermal expansion/resin cure shrinkage and tool-part interaction. These examples illustrate the growing competencies in composites manufacturing simulation, but to provide the most value for the composites industry it is essential that these simulation tools be linked in a manner that provides for the modeling of the complete manufacturing process. Only then can the true economic benefits of composites simulation be realized. Further, access to the current suite of simulation tools is limited to individuals who have access to large scale computing and to organizations who have purchased expensive licenses for the simulation tools. Entrepreneurs who will significantly accelerate the innovation and development of this powerful set of tools, as well as, the composites manufacturing field, are at a severe disadvantage, because the overhead of just one set of commercially available simulation tools is substantial. Composite Materials Manufacturing HUB characteristics and functionality The Composites Manufacturing HUB is a cloudbased cooperative platform that hosts composites manufacturing simulation tools that may be accessed with a web browser from the Internet. The National Science Foundation provided the funding to develop the original HUB concept. There are currently 20 types of HUB organizations using the platform and software. The most successful HUB involves the subject of nanoparticles. To date that HUB boasts 10,000 users worldwide. It has over 350,000 simulations with over 210 engineering tools to simulate important nano phenomena important in nanoelectronics, materials science, thermal science, physics and chemistry. Over 2,500 content items such as tutorials seminars and full classes drive the overall community to over 175,000 users annually. The user community connects students at all levels, research professionals, faculty and industrial users. Tools range from molecular modeling and simulation to photonics. The Composites Manufacturing HUB has adopted the same platform functionality, which allows users to access tools on a server via web browser. Tools hosted on the Composites Manufacturing HUB can range from simple tools that require only small amounts to computational cycles and those that require the power super computing systems. The HUB provides access to the appropriate level of computing power for each tool and user problem. Further, the platform hosts learning tools that teach the underlying principles upon which the tool is based and demonstrate the correct use and limitations of the tool. Examples of tools include simple engineering mechanics formulations and models of heat and mass transfer essential to simulate composites manufacturing processes. Molecular modeling simulation and uncertainty quantification will inform all the manufacturing process simulations to provide guidance from first principles and to account for process variability. The HUB will also provide a forum for evaluation of tool performance by the user community though hosted discussions and rating systems. The HUB community can post “Wish lists” on the HUB for discussion. Tool developers are rewarded for both tool use levels and the development of new tools through funding developed by the HUB. Tools developed and placed on the HUB are subjected to a financial analysis to determine their worth to the HUB and the developer is rewarded accordingly. Specific composites manufacturing processes is the focus of the Composites Manufacturing HUB. While the choice of manufacturing processes is initially limited by the tools currently available, the number of process simulations will be expanded by new tool development during the program. Indeed, it is likely that the available tools on the HUB will be continuously changing as tools are invented, developed and matured. Over time mature tools will likely be migrated to commercial support enterprises. As such, the HUB embraces technology readiness levels (TRL) of TRL 2 through TRL 6 and fosters rapid deployment of manufacturing processes poised for commercialization. The HUB simultaneously embraces technology readiness levels of TRL 2 through TRL 6. At the TRL 6 level, existing simulation software is provided to the user community with the goals of education the user community in tool use and establishing gaps in functionality required for complex composites manufacturing process simulations. The TRL 2 level work is the research necessary to address the scientific foundation of the simulation tools identified to fill the missing gaps. In this way, the Composites Manufacturing HUB provides a “food chain” for development of the comprehensiv",,2011.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8ec56c3b91e96f59a91293e7b6d1522d347fad18,https://www.semanticscholar.org/paper/8ec56c3b91e96f59a91293e7b6d1522d347fad18,Cross-Language Interfacing and Gesture Detection with Microsoft Kinect,"Stage provides an immersive visualization environment for mission operations, in which we integrate Google Streetview and the Kinect Natural User Interface (NUI). The Kinect is controlled through a C++ dynamic link library, where it is interfaced to other environments: Java, Unity, and Teamcenter Visualization. Through the Kinect API, we retrieve the positions of various skeleton points, and use them to perform gesture processing and identification. The goal of the project is expand the uses of Stage to more than mission operations, and to explore the degree of seamlessness that we can achieve by using the Kinect NUI. This will highly depend on the types of gestures that are used to perform commands as well as the accuracy of the detection algorithms. We found that the learning curve of our system to be low since the number and complexity of the commands are low. However, other factors such as fatigue limit the feasibility of the NUI as a primary input method. 1. Background Stage is an experimental visualization environment for mission operations. For it to serve this purpose, it should handle three functions: input, display, and control. We define the following terms as follows: Input—Collect and process data from device (robot, spacecraft, etc.) Display—Display data from device to user in a usable format Control—Process user’s input and converts to commands recognized by device As of now, the display function is fairly complete, and we have fabricated input by using Mars’ ground data and Google Streetview. Control functions are purely client-side for the time being. The distinguishing factor of Stage is that the display is 12 ft. high, 270 degree cylindrical shaped. Figure 1 illustrates the physical configuration of Stage. Three projectors are required to illuminate the whole screen. The advantage this set-up has compared to traditional display methods is that it maintains the relative position of objects. The code-base of Stage is written in Java. The 3D environment is rendered and managed by the Ardor3D library. In 3D rendering, the maximum view angle a camera is allowed is about 179. A piecewise rendering trick is used to achieve the full 270 degree view angle. Figure 1. Stage Configuration 2. Google Streetview Undocumented API We have found that the raw Google Streetview image data is not available through their public API. Therefore we access them through an undocumented API described by Jamie Thompson. It describes the sequence and methods on retrieving the raw image data of a given latitude and longitude pair. Note: There is a problem regarding the assumption of image dimensions (refer to Appendix A) in Thompson’s article. The result is a spherical projected image of the location desired. This spherical image can be displayed correctly by creating a sphere and mapping the Streetview image to the sphere as a texture within the 3D scene. 3. Google Streetview Integration We have added support for Google Streetview in Stage. It takes a pair of latitude and longitude coordinates, and retrieves the image data from Google’s servers. The image is processed and rendered with the 270 degree view geometry. Basically, the behavior of the program is similar to Google Earth, but with a wider view angle. This includes the ability to walk around various locations on earth, and the program will appropriately load and transition between images. 4. Cross-Language Kinect Interface Stage supports keyboard, mouse, and Nintendo Wii Remote inputs. Now we would like to include Microsoft Kinect support. The challenge associated with this is that Stage is written in Java, and the Kinect API supports C/C++ and C#. We solved this problem by writing interface code on each end: Java and C++. Our C++ interface is a dynamic link library (DLL) which uses the Kinect API and exports the Kinect functions. The memory mapping of the data structures is known, so any program that can call functions from this DLL should be able to retrieve data coming from the Kinect. We call this the wrapper DLL. The Java side of the interface accesses the wrapper DLL through the Java Native Access (JNA) library. The JNA library allows us to load the DLL and call functions through the DLL and store the output to a memory block. The raw data is remapped into a proper Java-native data structure that we can use to perform gesture processing. We use a similar architecture set-up to interface the Kinect to Unity, a 3D game development tool, and Teamcenter Visualization, a CAD design tool. We will focus more on the implementation details used for Stage, however. Refer to the figures found in Appendix B. 5. Gesture Detection The method used to detect gestures in Stage is similar to the operation of a deterministic finite automaton (DFA). It is a clearly defined DFA in which at each state the system evaluates a condition, which can return either -1, 0, or 1. A value of -1 signals the system to return to the beginning state, 0 will result in no state change, and 1 will cause the system to advance a state. All states are ordered and there is no skipping of states or backtracking except for returning to the beginning state. A gesture is detected when the final state is reached. Figure 2 is a DFA of a four state gesture. We generalize that a gesture with more states are more complex. For example, our implementation of a wave gesture consists of four states. The gestures used to control Stage are probably the simplest type of gestures possible, which are composed of two states. Figure 2. DFA of Four State Gesture 6. Challenges Ardor3D was quite difficult to work with. The documentation was difficult to find that we thought it did not exist. It required inspection of the source code to understand the behavior when we needn’t involve ourselves with the implementation details. The gestures we used in Stage are simple. If we wish to implement more complex gestures, the deconstruction of the gesture will be difficult. The wave gesture we have implemented as a test did not have a good detection rate. It resulted in primarily false negatives. There is also the possibility of multiple gestures interfering with each other. 7. Discussion In developing and testing the gestures, we have found that they can allow a user to finely control the camera and perform movements. However, we have had issues with unintended control, which is inherent with this type of input. It requires the user to focus only on controlling Stage. We have also found that the behavior of the Kinect is not consistent across different machines. Another issue with using the Kinect is user fatigue. The average user will become tired after about one hour of use, whereas traditional input methods can be used for several hours before fatigue sets in. While this disqualifies the Kinect as a primary input method, it may be a valuable tool as a secondary input device. 8. Future Work Stage is currently in a dismantled state. For future work, we would like to deploy the software and use it on the actual Stage platform instead of performing tests on standard display devices. This would allow us to test factors such as whether one Kinect sensor is sufficient for the whole environment. In terms of using Stage for a mission operation, there is lots of work needed to be done. To begin, a stitching algorithm for constructing either spherical or cylindrical projected images from realtime camera data can be written. Another important function for Stage to have is the ability to issue RAPID commands, which is a communications protocol for controlling robots. Future performance requirements will necessitate that Stage be ported over to a faster programming language, such as C/C++ or C#. We are currently having performance difficulties with processing and rendering Streetview images with the Java environment. It seems unlikely that Stage will be able to process and 30-60 frames per second of panoramic images without significant performance enhancements. Acknowledgments The work described in this paper was supported by the National Science Foundation under grant number 0852088 to California State University, Los Angeles. It was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration. We would also like to acknowledge David Torosyan for collaborating with the work done on Stage, the CURE Program, and Section 317F. References Ardor3D. Retrieved 15 Aug 2011. . Thompson, Jamie. Google Streetview Static API. 15 May 2010. . UNITY: Game Development Tool. Retrieved 15 Aug 2011. . Vrsinsky, Jan. 360Cities – Panoramic Photography Blog. 10 June 2010. . Appendix A. Google Streetview Undocumented API Given the latitude/longitude or panoId of a location, retrieve the metadata. The panoId is a unique identifier string for a location, which is used internally by Google. The metadata can be retrieved at the following URLs: http://cbk0.google.com/cbk?output=json&ll=[latitude],[longitude] http://cbk0.google.com/cbk?output=json&panoid=[panoId] The output type can be changed to ‘xml’ if xml output is desired. The metadata is used to retrieve the panoId of the location. With the panoId, it is possible to retrieve the image tiles and construct the spherical projected image. Several tiles put together form the entire image. The dimension of each tile is 512x512 pixels. Tiles are accessed at the following URL: http://cbk0.google.com/cbk?output=tile&panoid=[panoId]&zoom=[zoo m]&x=[x]&y=[y] Possible values for vary from 1-5, but values of 4 and 5 may not be available at all areas. The and parameters are used to indicate the coordinates of the tile piece; these values are always positive and start at 0,0 . The maximum values of these coordinates vary, even for identical zoom levels. In the case of an invalid , , or parameter, it will return a single channel image which will be all black. This makes the detection of out of bound values simple. Retrieve the tiles by ",,2011.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3d3dcf34c2a204c4dabebbe3d1dafd991369b7dc,https://www.semanticscholar.org/paper/3d3dcf34c2a204c4dabebbe3d1dafd991369b7dc,Configurable accelerators for video analytics,"Video analytics is the science of analyzing image sequences and video with the aim to gain a cognitive understanding of a scene. The applications which can take advantage of video analytics are diverse, ranging from media measurement systems and surveillance, to medical imaging and traffic systems. Unfortunately, many of these algorithms can still not be deployed in embedded environments, or achieve real-time performance, because of the computational and size, weight, and power (SWaP) constraints of such systems. In particular, performing complex imaging tasks in real time are still beyond the capabilities of general CPUs and embedded microcontrollers alone. Alternatively, systems that have the ability to perform video analytics in real-time usually require high SWaPs that forbid their use within an embedded system. The goal of this dissertation is to explore several areas which have the potential for enabling low SWaP accelerators to meet the performance goals of real-time systems. These areas include low-cost field programmable gate arrays (FPGAs), graphics processing units (GPUs), three-dimensional (3D) integrated circuits (ICs), and flexible, high-performance FPGA systems which enable algorithm exploration. 
FPGAs have become a highly competitive platform for implementing low-power systems aimed at real-time applications. This dissertation describes the implementation of two popular machine learning algorithms, the artificial neural network (ANN) and support vector machine (SVM), targeting embedded FPGA systems. These algorithms were chosen because they can have direct impacts on commercial applications where these algorithms are used extensively. Both implementations demonstrate the ability to perform at the 30 frames-per-second necessary to support real-time operation and can be configured to meet the resource constraints of the system. The second class of accelerator, the GPU, consists of tens to hundreds of functional units with an underlying hardware architecture that has been fixed. This dissertation examines a key algorithm towards recognizing salient features within an image, known as center-surround distribution distance. Through the use of a GPU platform, the algorithm was able to be accelerated by up to 30 times over an optimized CPU implementation, enabling the algorithms use for real-time applications. The third area, the 3D IC, targets an application specific integrated circuit (ASIC) design that has historically been the most efficient choice for accelerating custom applications, as they provide the highest performance at the lowest SWaP. This dissertation demonstrates the design and implementation of a custom accelerator chip using 3D technology targeted towards a complete embedded camera accelerator platform. The chip implements a popular pre-processing algorithm which extracts skin regions from an image and can operate at 312 frames-per-second (10X real-time performance). Lastly, this dissertation explores the Falcon framework, which allows high-performance FPGA systems to automatically be composed from an algorithmic specification. In particular, this dissertation addresses the difficulties which arise when trying to compose multi-FPGA based systems that are made up of several different types of IP cores. This level of configurability is enabled through the development of several key components including an underlying hardware infrastructure, intelligent mapping tool, and a graphical user interface (GUI), allowing a systems designer to build video analytics systems quickly and easily. The use of the Falcon framework is then demonstrated on a state-of-the art, biologically inspired algorithm, used for multi-class image recognition tasks, known as Hierarchical Model and X (HMAX). Compared to a CPU implementation, the FPGA system generated through the use of the tool garnered a speedup of 16X. 
These studies demonstrate the benefits and disadvantages of several prevailing technologies valuablefor creating real-time systems which are capable of analyzing images and video. Understanding these tradeoffs is important as the use of video and images is gaining importance in many commercial applications. The architectures, methods, and tools described within this dissertation are intended to enable image analytics to be used in wider variety of applications.",,2011.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
16d38d2d0ceeb3f909fea66e514fb397232888a5,https://www.semanticscholar.org/paper/16d38d2d0ceeb3f909fea66e514fb397232888a5,"Proceedings of the Eleventh International Joint Conference on Measurement and Modeling of Computer Systems, SIGMETRICS/Performance 2009, Seattle, WA, USA, June 15-19, 2009","It is our great pleasure to welcome you to SIGMETRICS/Performance 2009. SIGMETRICS is the flagship conference of the ACM special interest group for the computer systems performance evaluation community. Performance is the flagship conference of the IFIP working group on performance modeling and analysis. Every three years, the two conferences are held jointly, and this is the eleventh joint conference. 
 
This year, we will continue with several of the innovations introduced at last year's SIGMETRICS program. The main conference will again be a full three days, featuring 27 papers, 21 posters, and three invited talks from computer science luminaries, both academic and industrial. We will also reprise the demo competition and student thesis panel that were so well received last year. We are introducing an industrial information seminar, to provide an opportunity for students and academics to hear from representatives in industrial research about performance-related projects and their impact on deployed products and services. 
 
Supplementing the main conference are four interesting workshops, ranging from the venerable to the avant-garde. The workshop on MAthematical performance Modeling and Analysis (MAMA) continues its eleventh year as a forum for talks on early research in the more mathematical areas of computer performance analysis. The workshop on Hot Topics in Metrics (HotMetrics), which had a stellar inauguration last year, will reprise its role in helping to identify ""big"" and ""hard"" problems in performance evaluation and to develop innovative approaches to solving them. We also introduce two new workshops this year: GreenMetrics will explore how improvements to or new uses of Information and Communication Technology (ICT) can contribute towards efforts to minimize global climate change, a problem of increasing importance in modern society. The Learning for Networking workshop will investigate the use of machine learning techniques to tackle the increasingly complex architecture and control features in telecommunications and computer networks. 
 
This year also features a splendid series of tutorials, on topics ranging from social networks to data-center networks, from data-flow programming to packet-flow configuration, and from internet measurement to internet service construction.",SIGMETRICS/Performance,2009.0,10.1145/2492101,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
07cade71a8c2f14f4048c07234b6686260543edf,https://www.semanticscholar.org/paper/07cade71a8c2f14f4048c07234b6686260543edf,The Academic Study of the History of Technology,"THE central problem confronting the student of the history of technology is that of bewildering diversity. The range of human wants, from food and shelter to television and space travel, is so vast and the variety of human ingenuity that has been deployed to satisfy those wants is so great that it is difficult to perceive even the vague outlines of general principles of the sort that must surely characterise an established branch of history. Even if technology is divided into a few major divisions such as, for example, building and civil engineering, textiles, power, transport, communications and hand and machine tools, it is at once apparent that each division includes a huge number of individual inventions and developments all of which have, apparently, claims to be included in the scope of the history of technology. However the problem is not quite so intractable as it appears at first sight"": Selection, based on value judgments, is an essential feature of all acknowledged branches of learning and if the history of technology appears to be an exception this must be because (despite the immense importance of modem technology) it is of very recent origin, considered as a part of academic history. Curiously enough there was, about a hundred years ago, considerable interest in the history of technology; witness the writings of John Farey, Samuel Smiles and Thomas Ewbank. But this was followed by a period, the quarter-century before the first World War, when interest in the history of technology-and indeed in the history of science-waned. This was the age which Whitehead was later to stigmatise as one of the dullest in the history of human thought. It follows that the field is tolerably open and it is my purpose to suggest an approach to the history of technology which imparts a degree of coherence to this at present ill-ordered subject and may therefore be of some use to teachers and to research workers.t The desideratum is selection and selection must be based on value judgments. The problem, then, is on what can we base our value judgments? I suggest that the basic criterion to apply in considering any invention or technological development is the extent to which its study will contribute to the establish ment of the needed general principles. For unless we can establish",,1968.0,10.1177/007327536800700104,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
cdb6b5919cb42b4e6c1ccff3da0df867fde8ac70,https://www.semanticscholar.org/paper/cdb6b5919cb42b4e6c1ccff3da0df867fde8ac70,NATURAL LANGUAGE PROCESSING IN VIRTUAL REALITY TRAINING ENVIRONMENTS,"Technological advances in areas such as transportation, communications, and science are rapidly changing our world--the rate of change will only increase in the 21st century. Innovations in training will be needed to meet these new requirements. Not only must soldiers and workers become proficient in using these new technologies, but shrinking manpower requires more crosstraining, self-paced training, and distance learning. Two key technologies that can help reduce the burden on instructors and increase the efficiency and independence of trainees are virtual reality simulators and natural language processing. This paper focuses on the design of a virtual reality trainer that uses a spoken natural language interface with the trainee. RTI has developed the Advanced Maintenance Assistant and Trainer (AMAT) with ACT II funding for the Army Combat Service Support (CSS) Battlelab. AMAT integrates spoken language processing, virtual reality, multimedia and instructional technologies to train and assist the turret mechanic in diagnosing and maintenance on the M1A1 Abrams Tank in a hands-busy, eyes-busy environment. AMAT is a technology concept demonstration and an extension to RTI’s Virtual Maintenance Trainer (VMAT) which was developed for training National Guard organizational mechanics. VMAT is currently deployed in a number of National Guard training facilities. The AMAT project demonstrates the integration of spoken human-machine dialogue with visual virtual reality in implementing intelligent assistant and training systems. To accomplish this goal, RTI researchers have implemented the following features: • Speech recognition on a Pentium-based PC, • Error correcting parsers that can correctly handle utterances that are outside of the grammar, • Dynamic natural language grammars that change as the situation context changes, • Spoken message interpretation that can resolve pronoun usage and incomplete sentences, • Spoken message reliability processing that allows AMAT to compute the likelihood that it properly understood the trainee (This score can be used to ask for repeats or confirmations.), • Goal-driven dialogue behavior so that the computer is directing the conversation to satisfy either the user-defined or computer-defined objectives, • Voice-activated movement in the virtual environment, and • Voice synthesis on a Pentium-based PC.",,1998.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ad5d928e3b8da09a8433539f4cdf2fb8c3d202b3,https://www.semanticscholar.org/paper/ad5d928e3b8da09a8433539f4cdf2fb8c3d202b3,Rolling locomotion of a deformable soft robot with built-in power source,"In this paper we present a neural network architecture for learning of robotic grasping tasks. The neural network model allows acquisition of different neural representations of the grasping task through a successive learning over two stages in a strategy that uses already learned representations for the acquisition of the subsequent knowledge. Systematic computer simulations have been carried out in order to test learning and generalization capabilities of the system. The proposed model can be used as a high level controller for a robotic dexterous hand during learning and execution of grasping tasks. Adaptive stair-climbing behaviour with a hybrid legged-wheeled robot M. Eich, F. Grimminger and F. Kirchner Robotics Group German Research Center for Artificial Intelligence Bremen, Germany markus.eich@dfki.de Abstract Inspired by quadruped animals we developed the hybrid legged-wheeled robot ASGUARD. We showed already that this robot is able to cope with a variety of stairs, very rough terrain, and is able to move very fast on fat ground. We will describe a versatile adaptive control approach for such a system which is based only on proprioceptive data. An additional inclination and roll feedback is used to make the same controller more robust in terms of stair climbing capabilities. At the same time, high velocities can be reached on fat ground without changing the configuration of the system. By using twenty compliant legs, which are mounted around four individually rotating hip-shafts, we abstract from the biological system. For the locomotion control we use an abstract model of bio-inspired Central Pattern Generators (CPG), which can be found in biological systems from humans to insects. In contrast to existing work, ASGUARD uses the sensed feedback of the environment to adapt the walking pattern in real time.Inspired by quadruped animals we developed the hybrid legged-wheeled robot ASGUARD. We showed already that this robot is able to cope with a variety of stairs, very rough terrain, and is able to move very fast on fat ground. We will describe a versatile adaptive control approach for such a system which is based only on proprioceptive data. An additional inclination and roll feedback is used to make the same controller more robust in terms of stair climbing capabilities. At the same time, high velocities can be reached on fat ground without changing the configuration of the system. By using twenty compliant legs, which are mounted around four individually rotating hip-shafts, we abstract from the biological system. For the locomotion control we use an abstract model of bio-inspired Central Pattern Generators (CPG), which can be found in biological systems from humans to insects. In contrast to existing work, ASGUARD uses the sensed feedback of the environment to adapt the walking pattern in real time. On the mechanized inspection of glass fiber plastic pipes and pipe joints P. Chatzakos, A. Lagonikas, D. Psarros, V. Spais, K. Hrissagis and A. Khalid Zenon SA, 5 Kanari str., Glyka Nera, Athens, Greece. TWI Ltd, Granta Park, Great Abington, Cambridge, United Kingdom. pchatzak@zenon.gr Abstract In this paper the development of a robotic multi-axis crawler to accurately deploy inspection equipment on glass fiber plastic pipe and pipe joints is presented. The developed crawler is able to carry a range of sensors to automatically inspect complex geometry components, making traditional scanners redundant. First results show that the dexterity of the mechanized system is such that component coverage and positional accuracy are maximized using only a minimum number of degrees of freedom.In this paper the development of a robotic multi-axis crawler to accurately deploy inspection equipment on glass fiber plastic pipe and pipe joints is presented. The developed crawler is able to carry a range of sensors to automatically inspect complex geometry components, making traditional scanners redundant. First results show that the dexterity of the mechanized system is such that component coverage and positional accuracy are maximized using only a minimum number of degrees of freedom. Development and gait generation of the biped robot stepper-senior Y. Liu, M. Zhao, J. Zhang, L. Li, X. Su and H. Dong Department of Automation Tsinghua University Beijing, China liuyu@mails.tsinghua.edu.cn Abstract This paper presents the development and gait generation of biped robot Stepper-Senior. Parallel double crank mechanism and elastic materials are introduced in the 10 DOF lower limbs to mechanically restrict the sole to contact flat with the ground for stable fast walking. Virtual Slope Walking is used for biped gait generation, which is a simple method with strongly intuitive parameters for real-time utilization. In walking experiment, Stepper-Senior reaches the speed of 0.65 m/s and accomplishes omnidirectional walking.This paper presents the development and gait generation of biped robot Stepper-Senior. Parallel double crank mechanism and elastic materials are introduced in the 10 DOF lower limbs to mechanically restrict the sole to contact flat with the ground for stable fast walking. Virtual Slope Walking is used for biped gait generation, which is a simple method with strongly intuitive parameters for real-time utilization. In walking experiment, Stepper-Senior reaches the speed of 0.65 m/s and accomplishes omnidirectional walking. Development of an underground explorer robot based on an earthworm’s peristaltic crawling H. Omori, T. Nakamura and T. Yada Department of Precision Mechanics, Faculty of Science and Engineering Chuo University Tokyo, Japan h_omori@bio.mech.chuo-u.ac.jp Abstract Although shield tunnel construction and tunnel boring machines have developed a great deal, the machines are still large in size and consume large amounts of energy. A robot small enough to investigate under the ground by itself would extend the range of underground investigation both under the Earth‟s surface and, in the future, on the moon. This study focuses on the peristaltic crawling of an earthworm as a locomotion mechanism for an underground explorer robot. In peristaltic crawling, extension and contraction waves are propagated in the anteroposterior direction by varying the thickness and length ofAlthough shield tunnel construction and tunnel boring machines have developed a great deal, the machines are still large in size and consume large amounts of energy. A robot small enough to investigate under the ground by itself would extend the range of underground investigation both under the Earth‟s surface and, in the future, on the moon. This study focuses on the peristaltic crawling of an earthworm as a locomotion mechanism for an underground explorer robot. In peristaltic crawling, extension and contraction waves are propagated in the anteroposterior direction by varying the thickness and length of the earthworm‟s segments, and a large surface area is brought into contact during motion. Furthermore, it requires no more space than that of an excavation part on the anterior of the robot. The proposed robot consists of several parallel link mechanisms. We confirmed that the robot could move on a plane surface, also climb a tube, and in perforated dirt. Good performance was observed in the experiments. Rolling locomotion of a deformable soft robot with built-in power source Y. Matsumoto, H. Nakanishi and S. Hirai Department of Robotics Ritsumeikan University Shiga, Japan rr008033@se.ritsumei.ac.jp Abstract Locomotion over rough terrain has been achieved mainly by rigid body systems, including crawlers and leg mechanisms. We have proposed an alternative method, which uses deformation of a robot body, and developed a prototype of this deformable robot. But, electric power was externally supplied, such that power supply cables hindered the locomotion of the robot. We describe here the rolling locomotion of a deformable soft robot with an internally supplied power source. We applied dynamic simulation with particle-based modelling to analyze the rolling motion of this robot and found that increased weight had little influence on the kinematics performance of this robot on flat surfaces. Increased weight, however, was effective in providing greater stability on slopes.Locomotion over rough terrain has been achieved mainly by rigid body systems, including crawlers and leg mechanisms. We have proposed an alternative method, which uses deformation of a robot body, and developed a prototype of this deformable robot. But, electric power was externally supplied, such that power supply cables hindered the locomotion of the robot. We describe here the rolling locomotion of a deformable soft robot with an internally supplied power source. We applied dynamic simulation with particle-based modelling to analyze the rolling motion of this robot and found that increased weight had little influence on the kinematics performance of this robot on flat surfaces. Increased weight, however, was effective in providing greater stability on slopes. GRACE – Generic robotic architecture to create emotions T. H. H. Dang, S. L. Zarshenas and D. Duhaut IFI Francophone Institute for Informatics South Brittany University Vannes, France dthha@ifi.edu.vn Abstract In this paper, we present a model of emotions that we proposed in EmotiRob project. We make a comparison of recent models of emotions and show that our model is generic in basing on the theories of emotions of Ortony et al., Lazarus, Scherer and then the personality theory of Meyers-Brigg and Meyers.In this paper, we present a model of emotions that we proposed in EmotiRob project. We make a comparison of recent models of emotions and show that our model is generic in basing on the theories of emotions of Ortony et al., Lazarus, Scherer and then the personality theory of Meyers-Brigg and Meyers. Motion design for an insectomorphic robot on unstable obstacles Y. F. Golubev and V. V. Korianov Mech.-Math. dept., Lomono",,2008.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c4a7e1cff529ec23a83f7556a987a8688ed7aa0c,https://www.semanticscholar.org/paper/c4a7e1cff529ec23a83f7556a987a8688ed7aa0c,Computer Aided Materials Selection For Design And Manufacturing,"Machine design as taught in most machine design courses tends to focus mainly on considerations of geometry and stress analysis, with the selection of material and manufacturing processes and the specification of manufacturing tolerances becoming almost an afterthought. Although most engineering students are taught materials science as a fundamental science course, the curriculum downstream does not foster the incorporation of these principles into the systematic selection of the most appropriate material for a certain shape and function, or the criterion-based selection of the optimal manufacturing process. Tighter integration between the introductory materials courses and the downstream design and manufacturing courses is just part of the solution. With the advances in materials and manufacturing technology, a plethora of materials and processes has evolved. Undergraduate courses in design and manufacturing cannot provide detailed coverage of all materials and processes, and thus one has to harness the knowledge archiving and retrieval capabilities possible with today’s information technology. The authors describe their experiences with a popular materials and process selection program (Cambridge Engineering Selector) that has been deployed in a junior level manufacturing processes class as well as a senior/graduate level aluminum design class. Students experience different aspects of the software, with the usage of its vast capabilities getting more sophisticated as they progress along the curriculum. Background The process of design necessitates a good understanding of the properties of materials as well as the manufacturing processes necessary to create a product out of these materials. Fundamentals of material behavior and of manufacturing processes are, in most engineering curricula, typically imparted in engineering science type courses early in the curriculum. The actual usage of this information in the design process is left to the capstone machine design projects and engineering design courses. There is a disconnect between the learning of the early years and the real-world product design work that follows. This is natural, because fundamental classes talk in generalities – materials, for example, are classified broadly as metals, ceramics, polymers and composites and their general structure, mechanical and physical properties are discussed. Likewise, manufacturing processes are broadly classified into forming, solidification, removal and joining. It is always somewhat of a shock for students to learn that the number of real-world materials available1 to them for engineering design range between 40,000 – 80,000 instead of the idealized four or five categories taught in class. Similarly, there are thousands of P ge 812.1 Proceedings of the 2003 American Society for Engineering Education Annual Conference & Exposition Copyright © 2003, American Society for Engineering Education viable manufacturing processes; not just the four or five broad categories. Secondly, engineering science classes rarely discuss materials or process costs. Machine design and economics lay increasing pressure on exacting materials with more precise properties and lower factor of safety. For example, a shaft currently made of medium carbon chromium molybdenum steel, can economically be substituted with manganese chromium steel (with no loss in mechanical properties) if the price of molybdenum goes up. As another example2, a titanium nitride coat on an HSS drill-bit can extend its life ten-fold between sharpenings, paying many times over for the added cost of the coating. This trend and the diversity of materials, both proprietary and generic, have made materials selection not just an art, but a complex web of intertwined property matches to meet the design requirements. There is no way to go but to use information technology for materials selection. Manufacturing engineers in the field, too, are constantly being pressed to reduce costs – it is a good idea for students to be armed with some kind of knowledge of relative costs of materials and processes. The designer is now forced to become better aware of the manufacturing choices and their systematic selection3. Additionally, new process technologies have made obsolete the processes that they replace – for example, the increasing use of laser cutting for sheet metal part cutting. Further, manufacturing of discrete micro and smaller level products are being enabled by technologies such as nanotechnology and nanofabrication4. Approach While Computer-Aided Design and Manufacturing have existed for many years, there were few widely-available computer-based methods of material/process selection. Limited computing power in the past meant that the sheer number of materials and processes available could only be stored on mainframe computer. The desktop and laptop computers of the present decade easily match the computing power of supercomputers of the early 1980s. This has enabled materials selection to move from its status of being considered only at the start of the design process and then the selection being frozen into place. It is now possible to constantly change process and material based on economics. The emergence of the Internet and Web technology enable the use of live data to enable the lowest cost material to be chosen to provide comparable performance. The first author obtained orientation training at an earlier ASEE Conference in the use of a commercially available package called Cambridge Engineering Selector5 – CES4. This was procured in Fall 2002 by the authors’ university. The school license allows distribution of the software (which fits on 1 CD and can be copied) to a specified number of students and faculty for academic use on their office or home computers for the duration of the license. The authors worked with two classes that require the specification of materials and/or processes – Manufacturing Processes (MEEN 446, taught to juniors) and Aluminum-Based Product Design and Manufacture (MEEN 645, taught to seniors and graduate students). The manufacturing processes course, owing to time limitations, focuses principally on manufacturing with traditional materials such as metals and alloys. Other important materials like polymers, composites, ceramics and natural materials are covered only in brief reviews, and lead to the risk that a student will not be able to assimilate the importance of these other materials. The P ge 812.2 Proceedings of the 2003 American Society for Engineering Education Annual Conference & Exposition Copyright © 2003, American Society for Engineering Education aluminum course requires students to obtain a better understanding of wrought and cast aluminum alloys – the use of the software proves to be a welcome supplement by adding depth regarding the specific applications and properties of numbered alloys. CES4 divides the “universe” of materials into six major groups metals, polymers, ceramics, composites, foams and natural materials. It provides physical properties like density, mechanical properties like tensile strength and yield strength, thermal properties like conductivity and coefficient of expansion, electrical properties like conductivity and resistivity, optical and corrosion properties and economic properties for the various groups of materials. It similarly divides the “universe” of processes into major groups. The information is hierarchically arranged and cross-linked to each other in relational database format. CES 4 software for selection of materials is in several levels. Level 1 was introduced to the MEEN 446 students and Level 2 and 3 to MEEN 645 students – the latter introduces a professional – level materials selection system for senior/graduate students. Level 1 contains data for eighty widely-used materials. Students worked out material property attribute comparison charts in terms of modulusdensity to look at low specific weight material and yield strength – cost to determine the strongest material at lowest cost. Further, the students were able to easily generate bar and bubble charts based on specified properties for the designed machine component and provides a ranking of the materials. Once the materials are chosen, the manufacturing processes appropriate for the material are indicated. The students also did another project6 in which the material was already specified and more emphasis laid on selecting the right process, given the tolerances and cost constraints. Student evaluation of CES4 introduction module The students from the MEEN 446 worked on this module in groups of 3-4. A total of 19 students responded. The responses are summarized in Figures 1 – 3. Information received is relevant to the Manufacturing Processe s Course",,2003.0,10.18260/1-2--11842,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
aa5ee51f7477c1464f2b23548bda37e7cad63aa9,https://www.semanticscholar.org/paper/aa5ee51f7477c1464f2b23548bda37e7cad63aa9,Teaching artificial intelligence as the year 2000 approaches,"In the August 1992, Communications of the ACM, Maurice Wilkes discusses how researchers in artificial intelligence have not as yet been able to write programs enabling a machine to pass Turing’s test. While not wishing to give the impression that Turing’s dream could in fact ever come true or that AI can progress much beyond expert system design, Wilkes suggests that researchers would have a better chance of reaching Turing’s goal with analogue rather than digital machines. But are digital machines really to blame for the lack of progress in machine intelligence us dreamed of by Turing? A question for educators in particular is: Why, at this point in time, in the face of so little progress and so much pessimism, should a course whose goal is seeking artificial intelligence in Turing’s sense be taught? If it should be taught, how does one teach it? In this paper I offer answers to these questions with the aim in mind of showing the importance of AI to a computer science curriculum. L SHOULD ARTIFICIAL INTELLIGENCE BE TAUGHT? In his article “Artificial Intelligence as the Year 2000 Approaches” published in the August, 1992, issue of Communications of the ACM, Maurice Wilkes reasons “it is difficult to escape the conclusion that, in the 40 years that have elapsed since 1950, no tangible progress has been made towards realizing machine intelligence in the sense that Turing had envisaged. Perhaps, the time has come to face the possibility that it never will be realized with a digital computer.” [10] Wilkes points out that digital computers work within a purely logical system. Within such a system, however, it is well known that there are things that simply cannot be done. He mentions Godel’s proof and Desergues’s theorem as examples of the limitations such systems have been shown to face, as well as the inability of modern digital computers to solve differential equations directly. Furthermore, thinking and intelligence seem to be intimately connected with learning, but unresrricfed learning programs that would make machines intelligent in Turing’s sense don’ t exist. The reason they don’ t exist is not because the AI community hasn’t tried hard enough. Indeed, Wilkes cites some admirable Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. ACM-24thCSE-2/93 -lN, USA 01993 ACM 0-89791 -566-6193 /000210038 . ..$1 .50 attempts such aa Samuel’s checker player, which still falls short of the mark of being an unrestricted learning program in the sense that Samuel “found that he could devise ways in which the program could be automatically adjusted to optimize performance, but no way in which the program could invent an entirely new measure and add it to the set it already possessed. In other words, he could write a program to deal with mathematical forms, but not with intellectual concepts.” If the learning progmms produced so far always (as Wilkes claims they have) turn out to be programs that optimize their performance by modifying their internal state, either by adjusting parameters or by updating data structures, then we really don’t have anything as yet coming close to mirroring human intelligence. Wilkes concludes by suggesting that “we take as a working hypothesis that intelligent behavior in Turing’s sense is outside the range of the digital computer.” Such a negative hypothesis, according to WWces, still has merit for guiding research in artificial intelligence in that “a recognition that Turing’s dream is not going to be realized with a digital computer would perhaps help students avoid unpromising lines of research.” Many agree with Wilkes’ view that traditional AI with its emphasis on symbol manipulation and fiist-order logic has not lived up to its expectations. For instance, AI has been widely criticized as not contributing significantly to the solution of realworld problems in machine vision and robotics, or language understanding and translation. As a result, Al has not been seen by many aa P means by which we may gain a better understanding of human cognition, thought processes, and concept formation. Is the digital computer to blame for this lack of progress? As Wilkes points out, the digital computer is an abstraction+ne which a human designer finds useful as a way of organizing his thoughts-that perforce works within a logical system. Is an analogue computer then the answer? Do analogue devices process information in a way that comes closer to the way in which human brains function to process information? Are they more “naturally” suited to achieve Turing’s dream of machine intelligence? WNces refrains from making any predictions concerning analogue computers and the future of AI cautioning us that the analogue devices we are able to construct may themselves be subject to limitations which may or may not parallel those of digital machines. As yet, anyway, no one knows what the future prospects hold for analogue devices turning Turing’s dream into a reality. What are the prospects for neural networlm, fuzzy logic systems, or object-oriented prograrrum “rig? Again there is much PMsimism as to the success such alternatives will provide for constructing machine intelligence as long as we are still primarily working with digital computers, simulations on digital computers,",SIGCSE '93,1993.0,10.1145/169070.169100,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b362fb80387c3777c35733e6c68ba84c174af440,https://www.semanticscholar.org/paper/b362fb80387c3777c35733e6c68ba84c174af440,"The Secrets of Their Success: Foremost Leaders in Education Technology Provide Five Lessons on Using Today's Tools to Engage Students, Recharge Teachers, and in Some Cases, Change the World","THIS DOESN'T SEEM to be the place to come to discover the secrets of teaching with technology. Between drab warehouses to the left and lonesome train tracks to the right, Alan Kay's office is in a nondescript building set in the concrete fringes of Los Angeles. But its interior puts the exterior to shame. It is swank and cavernous--a good place to hide as Kay, nowhere to be seen. appears to be doing. ""Can I get you a drink?"" asks Kim Rose, a cognitive scientist who works with teachers and students in various schools and community learning centers, applying Kay's ideas. Alan Kay, one of the earliest pioneers in educational technology, is--as he should be--busy. Busy not like the rest of us are busy. Busy plotting-there-invention-of-the-world busy. Over in the corner, a British-style red phone booth adds to the room's allure and catches the eye. ""When that phone rings."" Rose says, ""pick it up and say the password."" Just then, the black receiver begins to vibrate with a faint jingle. With that. Rose is gone. Speaking the magic word into the receiver appears to activate a hydraulic mechanism. The booth's back begins to fall away. A short corridor off to the right leads to a threshold. What happens next is not transmissible. Certainly not right now. A smiling receptionist had made sure of it with a float of legal papers just minutes before. What can be said is that the mouse arrow you move on your screen and those little adjustable boxes that open and close, the icons that you click, the very idea of a laptop computer--for all of this you can, in part, give thanks to Alan Kay. And without knowing it, in the midst of a search to uncover ways to teach with technology that can transform classroom instruction, I had just stepped into his office. But more on that later. Here are the revelations my quest turned up: SECRET #1: Turn On to tablets. A year into their initial deployment, ThinkPad X41 Tablet PCs from Lenovo (www.lenovo.com) continue to elicit raves at all-girls Saint Mary's School in Raleigh, NC. According to Head of School Theo W. Coonrod, ""Classroom instruction and student learning have been revolutionized with this one cool machine."" Saint Mary's teachers say they find the tablet much better than laptops. ""There's no screen barrier between the student and the teacher, so [communication] is much clearer,"" says computer science teacher Jessica Sepke. Plus, ""at the end of each lesson, I have a copy of everything I wrote."" This comes in handy. Recently, a student in Sepke's class had been hospitalized for 25 days. Each of those days, Sepke posted her notes online exactly as they appeared in class, and the student didn't miss a thing. To weed out distractions, teachers will soon be using a piece of software to control instant messaging and Web site use. ""We'll say, 'OK, today we're going to the NASA Web site,' and push it out to the class computers, and that's the only site they can go to until a new site is pushed,"" says Sepke. When school officials decided to provide faculty with tablets, Sepke wasn't sure how the teachers would react. Turns out, ""every single teacher who has gotten a tablet has had some light-bulb moment about what they could do with it in their classroom that they couldn't do before. Our dance teacher looked at it and said, 'You know, my kids can do blocking and costume design on this, and we can hand it back and forth and walk through the ideas.' Music teachers use them to handwrite individualized lessons for students in musical notation and to instantly e-mail the lessons as homework assignments. In biology, a teacher creates and distributes a precise molecular diagram to the class with an effortless click. And our coaches take them out onto the field. All of our teachers have had some amazing 'Oh, look! I can do this! This is so cool!' moment, and then they take them into the classroom and do it."" SECRET #2: KMAC is a four-letter word for student achievement. …",,2006.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0eb1aa6dc0cbe0cc4e6cdef58007f843e7198121,https://www.semanticscholar.org/paper/0eb1aa6dc0cbe0cc4e6cdef58007f843e7198121,Semantic information processing of spoken language: how may I help you?,"The next generation of voice-based user interface technology will enable easy-to-use automation of new and existing communication services, achieving a more natural human-machine interaction. By natural, we mean that the machine understands what people actually say, in contrast to what a system designer expects them to say. This approach is in contrast with menu-driven or strongly-prompted systems, where many users are unable or unwilling to navigate such highly structured interactions. AT&Ts How May I Help You? (HMIHY)(sm) technology shifts the burden from human to machine wherein the system adapts to peoples language, as contrasted with forcing users to learn the machins jargon. We have developed algorithms which learn to extract meaning from fluent speech via automatic acquisition and exploitation of salient words, phrases and grammar fragments from a corpus. In this talk I will describe the speech, language and dialog technology underlying HMIHY, plus experimental evaluation on live customer traffic from AT&T's national deployment for customer careAllen Gorin is the Head of the Speech Interface Research Department at AT&T Laboratories, with long-term research interests focusing on machine learning methods for spoken language understanding. In recent years, he has led a research team in applying speech, language and dialog technology to AT&Ts ""How May I Help You?"" (HMIHY) (sm) service, which has been deployed nationally for long distance customer care. He was awarded the 2002 AT&T Science and Technology Medal for his research contributions to spoken language understanding for HMIHYHe received the B.S. and M.A. degrees in Mathematics from SUNY at Stony Brook, and the Ph.D. in Mathematics from the CUNY Graduate Center in 1980. From 1980-83 he worked at Lockheed investigating algorithms for target recognition from time-varying imagery. In 1983 he joined AT&T Bell Labs where he was the Principal Investigator for AT&T's ASPEN project within the DARPA Strategic Computing Program, investigating parallel architectures and algorithms for pattern recognition. In 1987, he was appointed a Distinguished Member of the Technical Staff. In 1988, he joined the Speech Research Department at Bell Labs. He has served as a guest editor for the IEEE Transactions on Speech and Audio, and was a visiting researcher at the ATR Interpreting Telecommunications Research Laboratory in Japan. He is a member of the Acoustical Society of America, Association for Computational Linguistics and an IEEE Senior MemberHome page for Allen Gorin: http://www.research.att.com/info/algor.",IUI '03,2003.0,10.1145/604045.604047,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10636510a1f28acfeb4d0823a44c843baf3c78c9,https://www.semanticscholar.org/paper/10636510a1f28acfeb4d0823a44c843baf3c78c9,A Comparative Evaluation of .net Remoting and JAVA RMI,"Distributed application technologies such as Micrososoft.NEJ Remoting, and Java Remote Method Invocation (RMI) have evolved over many years to keep up with the constantly increasing requirements of the enterprise. In the broadest sense, a distributed application is one in which the application processing is divided among two or more machines. Distributed middleware technologies have made significant progress over the last decade. Although Remoting and RMI are the two of most popular contemporary middleware technologies, little literature exists that compares them. In this paper, we study the issues involved in designing a distributed system using Java RMI and Microsoft.NET Remoting. In order to perform the comparisons, we designed a distributed distance learning application in both technologies. In this paper, we show both similarities and differences between these two competing technologies. Remoting and RMI both have similar serialization process and let objects serialization to be customized according to the needs. They both provide support to be able to connect to interface definition language such as Common Object Request Broker Architecture (CORBA). They both contain distributed garbage collection support. Our research shows that programs coded using Remoting execute faster than programs coded using RMI. They both have strong support for security although implemented in different ways. In addition, RMI also has additional security mechanisms provided via security policy files. RMI requires a naming service to be able to locate the server address and connection port. This is a big advantage since the clients do not need to know the server location or port number, RMI registry locates it automatically. On the other hand, Remoting does not require a naming service; it requires that the port to connect must be pre-specified and all services must be well-known. RMI applications can be run on any operating system whereas Remoting targets Windows as the primary platform. We found it was easier to design the distance learning application in Remoting than in RMI. Remoting also provides greater flexibility in regard to configuration by providing support for external configuration files. In conclusion, we recommend that before deciding which application to choose careful considerations should be given to the type of application, platform, and resources available to program the application. Introduction: A distributed system is a collection of loosely coupled processors interconnected by a communication network [8]. From tbe point view of a specific processor in a distributed system, the rest of the processors and their respective resources are remote, whereas its own resources are local. Generally, one host at one site or machine, the server, has a resource that another host at another site or machine, the client (or the user), would like to use [8]. The purpose of the distributed system is to provide an efficient and convenient environment for such sharing of resources. A distributed application is one in which the application processing is divided among two or more machines [8]. This division of processing also implies that the data involved is also distributed. Distributed application technologies such as Micrsosoft.NET Remoting, Distributed Component Object Model (DCOM), Java Remote Method Invocation (RMI), and Common Object Request Broker Architecture (CORBA) have evolvedovermanyyearstokeepupwitbtheconstantlyincreasing requirements of the enterprise. They all are based on objects that have identity and they either have or can have state [ 1]. Developers can use remote objects witb virtually the same semantics as local objects. This simplifies distributed programming by providing a single, unified programming model. They are also associated with a component model. A component is a separate, binarydeployable unit of functionality [3]. Using components in a distributed application increases its deployment flexibility. .In this paper, we focus on Microsoft.NET Remoting and Java RMI. These two are by far the most popular distributed technology at present. Java RMI, acronym for Remote Method Invocation, is designed by Sun Microsystems which targets working on distributed objects on Java virtual machines [2]. RMI allows Java developers to make calls to objects in different Java Virtual Machines, whether they are in different processes or on different hosts. .NET Remoting is designed by l\ficrosoft Corporation as a successor to DCOM .. NET Remoting is the manner in which .NET makes objects callable over a network. In contrast to RMI's emphasis on Java-only development, .NET Remoting supports multi-language interoperability. Both of these technologies share similarities and differences. For 1 Ibrahim: A Comparative Evaluation of .net Remoting and JAVA RMI Published by ScholarWorks@UARK, 2004 COMPUTER SCIENCE AND COMPUTER ENGINEERING: Taneem Ibrahim .. NET Remoting and Java RMI 87 developing distributed application, a lot of times developers are confronted with the question of which technology to choose. This is often a daunting task. This paper attempts to make a comparative evaluation between these two very popular distributed technologies in various aspects of designing a distributed application. In order to perform these comparisons, we have designed a simple distance learning application in both technologies. In the rest of the paper, we give an architectural background of Remoting and RMI, describe the distributed distance learning application, and present the experimental results of the evaluation. In conclusion, we show our observations in regard to which application to choose, and recommend any future research work that can be done in this area. Overview of Architectures: Microsoft. NET Remoting Architecture .NET Remoting enables client programs to call methods of remote objects. When the client creates a connection to the server object, the .NET Framework creates a proxy object on the client [3]. The proxy object provides the client with the same view of the server object that it would have if the server objects were in its application space. It can call the server object's methods through the proxy object. Figure 1 depictsthis process. The figure shows a client method that calls a method on the remote object through the proxy object created at run time. Any data passed by the client method to the proxy is packaged by a formatter so that it can be sent across the network. The process of packaging the data for transaction is called marshalling [3]. Client Process Server Process I CientMethocl I ! ! I ll Jz Channel",,2004.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
985bb6a0e0fcb61943fab51e94887389f51b5b99,https://www.semanticscholar.org/paper/985bb6a0e0fcb61943fab51e94887389f51b5b99,Deploying computer-aided cross-training of technicians and engineers for semiconductor manufacturing,"A consortium consisting of three universities and three community college systems, in three contiguous states, each with semiconductor manufacturing as an economic backdrop, is implementing “cross-training” of technicians and engineers for semiconductor manufacturing. The expectation is that “cross-training” technicians and engineers, such that they better understand the roles and skill sets of the other, will enhance their effectiveness as team members in real factory settings. The modules cover basic semiconductor unit processes (e.g., lithography, metalization, etch) and their facility demands, design of experiments, and factorylevel dynamics, from both technician and engineering perspectives. The modules include interactive, schematic-based simulator panels for selected manufacturing machines, to support a need-based, top-down learning paradigm. In addition, the modules have structured exercises that require interactive roles between technicians and engineers. The “side-by-side” presentation of text, graphics, animations, videos, simulations and exercises will give technicians enhanced exposure to math and science, and it will give engineers enhanced exposure to machine (tool)",,2001.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a407364d8d1c6cd44bffb2fa7918b4733c246a64,https://www.semanticscholar.org/paper/a407364d8d1c6cd44bffb2fa7918b4733c246a64,"Digital libraries : people, knowledge, and technology : 5th International Conference on Asian Digital Libraries, ICADL 2002, Singapore, December 11-14, 2002 : proceedings","Keynote and Invited Papers.- Challenges in Building Digital Libraries for the 21st Century.- Building Digital Libraries Made Easy: Toward Open Digital Libraries.- Dublin Core: Process and Principles.- From Digital Library to Digital Government: A Case Study in Crime Data Mapping and Mining.- Progress on Educational Digital Libraries: Current Developments in the National Science Foundation's (NSF) National Science, Technology, Engineering, and Mathematics Education Digital Library (NSDL) Program.- Examples of Practical Digital Libraries: Collections Built Internationally Using Greenstone.- Data Mining Technologies for Digital Libraries and Web Information Systems.- Papers.- Chinese Text Summarization Using a Trainable Summarizer and Latent Semantic Analysis.- A Linear Text Classification Algorithm Based on Category Relevance Factors.- A Hierarchical Framework for Multi-document Summarization of Dissertation Abstracts.- Generality of Texts.- The Effiectiveness of a Graph-Based Algorithm for Stemming.- Searching Digital Music Libraries.- The NUS Digital Media Gallery - A Dynamic Architecture for Audio, Image, Clipart, and Video Repository Accessible via the Campus Learning Management System and idtvukcan the Digital Library.- A Schema Language for MPEG-7.- Bitmap-Based Indexing for Multi-dimensional Multimedia XML Documents.- What People Do When They Look for Music: Implications for Design of a Music Digital Library.- Distributing Relevance Feedback in Content Based Image Retrieval Systems.- Multistrategy Learning of Rules for Automated Classification of Cultural Heritage Material.- VideoCube: A Novel Tool for Video Mining and Classification.- Developing Tsinghua University Architecture Digital Library for Chinese Architecture Study and University Education.- Retrieving News Stories from a News Integration Archive.- A Data Mining Approach to New Library Book Recommendations.- Grouping Web Pages about Persons and Organizations for Information Extraction.- Personalized Services for Digital Library.- Automatic References: Active Support for Scientists in Digital Libraries.- Organizing and Maintaining Dynamic Digital Collections.- Navigation, Organization, and Retrieval in Personal Digital Libraries of Email.- A Work Environment for a Digital Library of Historical Resources.- A Personalized Collaborative Digital Library Environment.- Virtual Tutor: A System for Deploying Digital Libraries in Classrooms.- Resource Annotation Framework in a Georeferenced and Geospatial Digital Library.- Building Policy, Building Community: An Example from the US National Science, Technology, Engineering, and Mathematics Education Library (NSDL).- Building a Digital Library from the Ground Up: An Examination of Emergent Information Resources in the Machine Learning Community.- Subscription Clubs for E-journals: Indian Initiatives.- A Multilingual Multi-script Database of Indian Theses: Implementation of Unicode at Vidyanidhi.- A Workbench for Acquiring Semantic Information and Constructing Dictionary for Compound Noun Analysis.- Building Parallel Corpora by Automatic Title Alignment.- Offline Isolated Handwritten Thai OCR Using Island-Based Projection with N-Gram Models and Hidden Markov Models.- A Cache-Based Distributed Terabyte Text Retrieval System in CADAL.- Rural Digital Library: Connecting Rural Communities in Nepal.- Collection Development for the Digital Age: The Case of Malaysia.- Digital Divide: How Can Digital Libraries Bridge the Gap?.- Digital Libraries in Academia: Challenges and Changes.- Building Digital Libraries for Children: Reviewing Information Literacy of Students and Teachers.- The Use and Functionality of the Environmental Data Registry: An Evaluation of User Feedback.- Adding Semantics to 3D Digital Libraries.- INEXP: Information Exchange Protocol for Interoperability.- Study on Data Placement and Access Path Selection in an FC-SAN Virtual Storage Environment.- Building an OAI-Based Union Catalog for the National Digital Archives Program in Taiwan.- Intergenerational Partnerships in the Design of a Digital Library of Geography Examination Resources.- Pie Charts for Visualizing Query Term Frequency in Search Results.- Information Therapy in Digital Libraries.- Evaluation of Task Based Digital Work Environment.- A Framework for Flexible Information Presentation in Digital Collections.- Electronic Journal of the University of Malaya (EJUM): An Attempt to Provide a Truly Electronic Environment.- Patenting the Processes for Content-Based Retrieval in Digital Libraries.- Secure Content Distribution for Digital Libraries.- A Strategic Level for Scientific Digital Libraries.- Bridging the Gap between Information Resource Design and Enterprise Content Management.- A Digital Content Management Model for Making Profits in Digital Content Sites.- Posters.- The Idea of a Digital Library: Issues of Today.- US-Korea Collaboration on Digital Libraries: An Overview and Generalization for Pacific Rim Collaboration.- ETDs at HKU: A Spearhead for Digital Library Growth.- MiMedicalLibrary: A Digital Health Library for Michigan.- Reference Services in a Digital Library of Historical Artifacts.- An Integrative User-Centered Purchase Request Service in the Age of Digital Library Development.- Vitalising Library and Information Science Education: A Challenge in the Digital Information Environment.- Developing a Dialogue Library System.- WebClipper: A Personal/Community Link Library Builder Based on Web Link Management Technique.- Hiding a Logo Watermark in an Image for Its Copyright Protection.- Searching Video Segments through Transcript, Metadata, and SVG Objects.- Similar Sub-trajectory Retrieval Based on k-Warping Distance Algorithm for Moving Objects in Video Databases.- A Keyword Spotting System of Korean Document Images.- An Efficient Strategy for Adding Bulky Data into B+-Tree Indices in Information Retrieval Systems.",,2002.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a132844518e757c8c34d356c8f1f05a724d0aeab,https://www.semanticscholar.org/paper/a132844518e757c8c34d356c8f1f05a724d0aeab,Wideband Beamforming www.modernh.com for to for,"Real-time Narrowband and Wideband Beamforming Techniques Fully-digital RF ArraysApplication of Frequency Invariant Constraints Wideband BeamformingLow-cost Smart AntennasGeolocation of RF SignalsUltra-Wideband, Short-Pulse Electromagnetics 5Statistical Signal Processing in EngineeringMIMO Antennas Wireless CommunicationAdaptive Array SystemsWireless Communication SystemsUltra Wideband Wireless CommunicationVLSIConvex Optimization in Signal Processing and CommunicationsSignal Processing Algorithms for Communication and Radar SystemsAn Analysis of Wideband Beamforming Techniques and Hardware Requirements for Analog and Digital Radar ArchitecturesUltra-Wideband Antennas and PropagationPervasive Mobile and Ambient Wireless CommunicationsIssues in Electronic Circuits, Devices, and Materials: 2013 EditionAdvances in Neural Networks - ISNN 2005Academic Press Library in Signal ProcessingUWB Communication SystemsCooperative and Cognitive Satellite SystemsAll-Optical Signal ProcessingRobust Adaptive BeamformingNeural information processing [electronic resource]Multiple Access CommunicationsUltra Wideband Signals and Systems in Communication EngineeringAdvanced Wireless CommunicationsTrue-Time-Delay Beamforming für ultrabreitbandige Systeme hoher LeistungIssues in Electronic Circuits, Devices, and Materials: 2012 EditionSimplified Robust Adaptive Detection and Beamforming for Wireless CommunicationsImaging: Sensors and TechnologiesWideband Direction of Arrival Estimation and Wideband Beamforming for Smart Antenna SystemsAdvances in Acoustic Emission TechnologyWideband BeamformingSound Visualization and ManipulationDistributed Sensor NetworksCognitive Radio Communication and NetworkingAdvanced Antenna Systems for 5G Network DeploymentsSound Capture for Human / Machine InterfacesMicrowave and Millimeter Wave Circuits and Systems multi-path signals. Key Features: Unique book focusing on wideband beamforming Discusses a hot topic coinciding with the increasing bandwidth in wireless communications and the development of UWB technology Addresses the general concept of beamforming including fixed beamformers and adaptive beamformers Covers advanced topics including sub-band adaptive beamforming, frequency invariant beamforming, blind wideband beamforming, beamforming without temporal processing, and beamforming for multi-path signals Includes various design examples and corresponding complexity analyses This book provides a reference for engineers and researchers in wireless communications and signal processing fields. Postgraduate students studying signal processing will also find this book of interest.This practically-oriented, all-inclusive guide covers all the major enabling techniques for current and next-generation cellular communications and wireless networking systems. Technologies covered include CDMA, OFDM, UWB, turbo and LDPC coding, smart antennas, wireless ad hoc and sensor networks, MIMO, and cognitive radios, providing readers with everything they need to master wireless systems design in a single volume. Uniquely, a detailed introduction to the properties, design, and selection of RF subsystems and antennas is provided, giving readers a clear overview of the whole wireless system. It is also the first textbook to include a complete introduction to speech coders and video coders used in wireless systems. Richly illustrated with over 400 figures, and with a unique emphasis on practical and state-of-the-art techniques in system design, rather than on the mathematical foundations, this book is ideal for graduate students and researchers in wireless communications, as well as for wireless and telecom engineers.This volume collects the papers from the 2013 World Conference on Acoustic Emission in Shanghai. The latest research and applications of Acoustic Emission (AE) are explored, with particular emphasis on detecting and processing of AE signals, development of AE instrument and testing standards, AE of materials, engineering structures and systems, including the processing of collected data and analytical techniques as well as experimental case studies.This book presents an alternative and simplified approaches for the robust adaptive detection and beamforming in wireless communications. It adopts several systems models including DS/CDMA, OFDM/MIMO with antenna array, and general antenna arrays beamforming model. It presents and analyzes recently developed detection and beamforming algorithms with an emphasis on robustness. In addition, simplified and efficient robust adaptive detection and beamforming techniques are presented and compared with exiting Spectrum Sensing: Basic Techniques; Cooperative Sensing Wideband Transmission Orthogonal Frequency Division Multiplexing Multiple Input Multiple Output for Cognitive Radio; Convex Optimization for Cognitive Radio; Cognitive Core (I): Algorithms for Reasoning and Learning; Cognitive Core (II): Game Theory; Cognitive Radio Network IEEE The First Cognitive Radio Wireless Regional Area Network Standard, and Testbeds.This and of research ultra-wideband radar systems; ultra-wideband and transient antennas; pulsed power generation and propagation; ultra-wideband polarimetry; ultra-wideband and transient metrology; detection and identification studies; RF interactions and chaotic effects; Multiple (UWB) the transmission sequence, the combining s cheme and ML decision rule for two-branch transmit diversity scheme with one and M receivers. Ultra Wide Band Radio, UWB multiple access in Gaussian channels, the UWB channel, UWB system with M-ary modulation, M-ary PPM UWB multiple access, coded UWB schemes, multi-user detection in UWB radio, UWB with space time processing and beam forming for UWB radio. Adaptive Beamforming the and work ofleading researchers investigating various approaches in onecomprehensive uncertainty set of thearray steering vector. their the standard Capon beamformers with a spherical orellipsoidal uncertainty set of the array steering Diagonal loading for finite sample size beamforming * Mean-squared error beamforming for signal estimation * Constant modulus beamforming * Robust wideband beamforming using a steered adaptive beamformerto adapt the weight vector within a generalized sidelobe cancellerformulation Adaptive Beamforming up-to-date reference for engineers, researchers, and inthis rapidly expanding field.ULTRA including part of massive MIMO to provide the important aspects of emerging technology. Aimed at researchers, professionals and graduate students in electrical engineering, electromagnetics, communications and signal processing including antenna theory and design, smart antennas, communication systems, this book: Investigates real time MIMO antenna designs for WLAN/WiMAX/LTE applications. Covers effects of ECC, MEG, TARC, and equivalent circuit. Addresses the coupling and diversity aspects of antenna design problem for MIMO systems. Focus on the MIMO antenna designs for the real time applications. Exclusive chapter on 5G Massive MIMO along with case studies throughout the book.Geolocation of RF Signals—Principles and Simulations offers an overview of the best practices and innovative techniques in the art and science of geolocation over the last twenty years. It covers all research and development aspects including theoretical analysis, RF signals, geolocation techniques, key block diagrams, and practical principle simulation examples in the frequency band from 100 MHz to 18 GHz or even 60 GHz. Starting with RF signals, the book progressively examines various signal bands – such as VLF, LF, MF, HF, VHF, UHF, L, S, C, X, Ku, and, K and the corresponding geolocation requirements per band and per application – to achieve required performance objectives of up to 0o precision. Part II follows a step-by-step approach of RF geolocation techniques and concludes with notes on state-of-the-art geolocation designs as well as advanced features found in signal generator instruments. Drawing upon years of practical experience and using numerous examples and illustrative applications, Ilir Progri provides a comprehensive introduction to Geolocation of RF Signals, and includes hands-on real world labs and applications using MATLAB in the areas of: RF signals specifications, RF geolocation distributed wireless communications networks and RF geolocation. Geolocation of RF Signals—Principles and Simulations will be of interest to government agency program managers industry professionals and engineers, academic researchers, faculty and graduate students who are interested in or currently designing, developing and deploying innovative geolocation of RF Signal systems.Advanced Antenna Systems for 5G Network Deployments: Bridging the Gap between Theory and Practice provides a comprehensive understanding of the field of advanced antenna systems (AAS) and how they can be deployed in 5G networks. The book gives a thorough understanding of the basic technology components, the state-of-the-art multi-antenna solutions, what support 3GPP has standardized together with the reasoning, AAS performance in real networks, and how AAS can be used to enhance network deployments. Explains how AAS features impact network performance and how AAS can be effectively used in a 5G network, based on either NR and/or LTE Shows what AAS configurations and features to use in different network deployment scenarios, focusing on mobile broadband, but also including fixed wireless access Presents the latest developments in multi-antenna technologies, including Beamforming, MIMO and cell shaping, along with the potential of different technologies in a commercial network context Provides a deep understanding of the differences between mid-band and mm-Wave solutionsWith a continuously increasing desire for natural and comfortable human/machine interaction, the acoustic interface of any terminal for multimedia or telecommunication services is challenged to allow seamless and hands-free audio commun",,2022.0,10.1002/9780470661178,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4df2175c0daadf630623a505f623fe41a386853d,https://www.semanticscholar.org/paper/4df2175c0daadf630623a505f623fe41a386853d,Selfish Sparse RNN Training,"Sparse neural networks have been widely applied to reduce the computational demands of training and deploying over-parameterized deep neural networks. For inference acceleration, methods that discover a sparse network from a pretrained dense network (dense-to-sparse training) work effectively. Recently, dynamic sparse training (DST) has been proposed to train sparse neural networks without pre-training a dense model (sparse-to-sparse training), so that the training process can also be accelerated. However, previous sparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs) and Convolutional Neural Networks (CNNs), failing to match the performance of dense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting. In this paper, we propose an approach to train intrinsically sparse RNNs with a fixed parameter count in one single run, without compromising performance. During training, we allow RNN layers to have a non-uniform redistribution across cell gates for better regularization. Further, we propose SNT-ASGD, a novel variant of the averaged stochastic gradient optimizer, which significantly improves the performance of all sparse training methods for RNNs. Using these strategies, we achieve state-of-the-art sparse training results, better than the dense-to-sparse methods, with various types of RNNs on Penn TreeBank and Wikitext-2 datasets. Our codes are available at https://github.com/ Shiweiliuiiiiiii/Selfish-RNN. Department of Computer Science, Eindhoven University of Technology, the Netherlands Faculty of Electrical Engineering, Mathematics, and Computer Science at University of Twente, the Netherlands . Correspondence to: Shiwei Liu <s.liu3@tue.nl>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).",ICML,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a95cd552d81a668f333d30b1c28f840b554d51fe,https://www.semanticscholar.org/paper/a95cd552d81a668f333d30b1c28f840b554d51fe,Six-Sigma Quality Management of Additive Manufacturing,"Quality is a key determinant in deploying new processes, products, or services and influences the adoption of emerging manufacturing technologies. The advent of additive manufacturing (AM) as a manufacturing process has the potential to revolutionize a host of enterprise-related functions from production to the supply chain. The unprecedented level of design flexibility and expanded functionality offered by AM, coupled with greatly reduced lead times, can potentially pave the way for mass customization. However, widespread application of AM is currently hampered by technical challenges in process repeatability and quality management. The breakthrough effect of six sigma (6S) has been demonstrated in traditional manufacturing industries (e.g., semiconductor and automotive industries) in the context of quality planning, control, and improvement through the intensive use of data, statistics, and optimization. 6S entails a data-driven DMAIC methodology of five steps—define, measure, analyze, improve, and control. Notwithstanding the sustained successes of the 6S knowledge body in a variety of established industries ranging from manufacturing, healthcare, logistics, and beyond, there is a dearth of concentrated application of 6S quality management approaches in the context of AM. In this article, we propose to design, develop, and implement the new DMAIC methodology for the 6S quality management of AM. First, we define the specific quality challenges arising from AM layerwise fabrication and mass customization (even one-of-a-kind production). Second, we present a review of AM metrology and sensing techniques, from materials through design, process, and environment, to postbuild inspection. Third, we contextualize a framework for realizing the full potential of data from AM systems and emphasize the need for analytical methods and tools. We propose and delineate the utility of new data-driven analytical methods, including deep learning, machine learning, and network science, to characterize and model the interrelationships between engineering design, machine setting, process variability, and final build quality. Fourth, we present the methodologies of ontology analytics, design of experiments (DOE), and simulation analysis for AM system improvements. In closing, new process control approaches are discussed to optimize the action plans, once an anomaly is detected, with specific consideration of lead time and energy consumption. We posit that this work will catalyze more in-depth investigations and multidisciplinary research efforts to accelerate the application of 6S quality management in AM.",Proceedings of the IEEE,2021.0,10.1109/JPROC.2020.3034519,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4451eb93f34442a3b02ba4125e2263cdc390da9a,https://www.semanticscholar.org/paper/4451eb93f34442a3b02ba4125e2263cdc390da9a,Collective knowledge: organizing research projects as a database of reusable components and portable workflows with common interfaces,"This article provides the motivation and overview of the Collective Knowledge Framework (CK or cKnowledge). The CK concept is to decompose research projects into reusable components that encapsulate research artifacts and provide unified application programming interfaces (APIs), command-line interfaces (CLIs), meta descriptions and common automation actions for related artifacts. The CK framework is used to organize and manage research projects as a database of such components. Inspired by the USB ‘plug and play’ approach for hardware, CK also helps to assemble portable workflows that can automatically plug in compatible components from different users and vendors (models, datasets, frameworks, compilers, tools). Such workflows can build and run algorithms on different platforms and environments in a unified way using the customizable CK program pipeline with software detection plugins and the automatic installation of missing packages. This article presents a number of industrial projects in which the modular CK approach was successfully validated in order to automate benchmarking, auto-tuning and co-design of efficient software and hardware for machine learning and artificial intelligence in terms of speed, accuracy, energy, size and various costs. The CK framework also helped to automate the artifact evaluation process at several computer science conferences as well as to make it easier to reproduce, compare and reuse research techniques from published papers, deploy them in production, and automatically adapt them to continuously changing datasets, models and systems. The long-term goal is to accelerate innovation by connecting researchers and practitioners to share and reuse all their knowledge, best practices, artifacts, workflows and experimental results in a common, portable and reproducible format at https://cKnowledge.io/. This article is part of the theme issue ‘Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico’.",Philosophical Transactions of the Royal Society A,2021.0,10.1098/rsta.2020.0211,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6e0cdea54f6ef68adb7870b0777eaa1bc44de42b,https://www.semanticscholar.org/paper/6e0cdea54f6ef68adb7870b0777eaa1bc44de42b,2021 ACTIVITY REPORT Project-Team DIVERSE Diversity-centric Software Engineering,"modify the behavior or non-functional properties of a software. Deep software variability calls to investigate how to systematically handle cross-layer conﬁguration. The diversiﬁcation of the different layers is also an opportunity to test the robustness and resilience of the software layer in multiple environments. Another interesting challenge is to tune the software for one speciﬁc executing environment. In essence, deep software variability questions the generalization of the conﬁguration knowledge. Functional Description: Familiar is an environment for large-scale product customisation. From a model of product features (options, parameters, etc.), Familiar can automatically generate several million variants. These variants can take many forms: software, a graphical interface, a video sequence or even a manufactured product (3D printing). Familiar is particularly well suited for developing web conﬁgurators (for ordering customised products online), for providing online comparison tools and also for engineering any family of embedded or software-based products. Functional Description: PIT and Descartes are mutation testing systems for Java applications, which allows you to verify if your test suites can detect possible bugs, and so to evaluate the quality of your test suites. They evaluate the capability of your test suite to detect bugs using mutation testing (PIT) or extreme mutation testing (Descartes). Mutation testing does it by introducing small changes or faults into the original program. These modiﬁed versions are called mutants. A good test suite should able to kill or detect a mutant. Traditional mutation testing works at the instruction level, e.g., replacing "">"" by ""<="", so the number of generated mutants is huge, as the time required to check the entire test suite. That’s why Extreme Mutation strategy appeared. In Extreme Mutation testing, the whole body of a method under test is removed. Descartes is a mutation engine plugin for PIT which implements extreme mutation operators. Both provide reports combining, line coverage, mutation score and list of weaknesses in the source. reasoning, and (iii) the adaptations and the predictive model of their impact on the trade-off. We use this framework to build three languages with self-adaptive virtual machines and discuss the relevance of the abstractions, effectiveness of correctness envelopes, and compare their code size and performance results to their manually implemented counterparts. We show that the framework provides suitable abstractions for the implementation of self-adaptive operational semantics while introducing little performance overhead compared to a manual implementation. change propagation. This work leverages classical inconsistency repair mechanisms to explore the vast search space of change propagation. Our approach not only suggests changes to repair a given inconsistency but also changes to repair inconsistencies caused by the aforementioned repair. In doing so,our approach follows the developer’s intent where subsequent changes may not contradict or backtrack earlier changes. We argue that consistent change propagation is essential for effective model driven engineering. Our approach and its tool implementation were empirically assessed on 18 case studies from industry, academia, and GitHub to demonstrate its feasibility and scalability. A comparison with two versioned models shows that our approach identiﬁes actual repair sequences that developers that categorize experiments belonging to either safe or failure-prone states. We apply ChaT to a video streaming application use case. The simulation results show the effectiveness of ChaT to achieve our goals: identifying execution classes and detecting failure-prone experiments based on metamorphic relationships with high level of statistical scores. two standards and we provide TOSCA Studio, a model-driven tool chain for TOSCA that conforms to OCCI. TOSCA Studio allows to graphically design cloud applications as well as to deploy and manage them at runtime using a fully model-driven cloud orchestrator based on the two standards. Our contribution is validated by successfully transforming and deploying three cloud applications: WordPress, Node Cellar and Multi-Tier. These achievements constitute basic blocks upon which we will continue building our research and systems, extending for example the applicability to secure supply chains. • Abstract: The aim of the Falcon project is to investigate how to improve the resale of available resources in private clouds to third parties. In this context, the collaboration with DiverSE mainly aims at working on efﬁcient techniques for the design of consumption models and resource consumption forecasting models. These models are then used as a knowledge base in a classical autonomous loop. • Abstract: The GLOSE project develops new techniques for heterogeneous modeling and simulation in the context of systems engineering. It aims to provide formal and operational tools and methods to formalize the behavioral semantics of the various modeling languages used at system-level. These semantics will be used to extract behavioral language interfaces supporting the deﬁnition of coordination patterns. These patterns, in turn, can systematically be used to drive the coordination of any model conforming to these languages. The project is structured according to the following tasks: concurrent xDSML engineering, coordination of discrete models, and coordination of discrete/continuous models. The project is funded in the context of the network DESIR, and supported by the GEMOC initiative. The use case chosen for the demonstrator is the high-level description of a remote control drone system, whose the main objective is to illustrate the design and simulation of the main functional chains, the possible interactivity with the model in order to raise the level of understanding over the models built, and possibly the exploration of the design space. • Abstract: Debug4Science aims to propose a disciplined approach to develop domain-speciﬁc debugging facilities for Domain-Speciﬁc Languages within the context of scientiﬁc computing and numerical analysis. Debug4Science is a bilateral collaboration (2020-2022), between the CEA DAM/DIF and the DiverSE team Summary: The goal of the RESIST associate team is to explore the science of resilient software by foundational work on advanced a priori testing methods such as metamorphic testing and a posteriori continuous improvements through digital twins. resist web site • Abstract: Most modern software systems (operating systems such as Linux, Web browsers such as Firefox or Chrome, video encoders such as x264 or ffmpeg, servers, mobile applications, etc.) are subject to variation or come in many variants. Hundreds of conﬁguration options, features, or plugins can be combined, each potentially with distinct functionality and effects on execution time, memory footprint, etc. Among conﬁgurations, some of them are chosen and do not compile, crash at run time, do not pass a test suite, or do not reach a certain performance quality (e.g., energy consumption, security). In this JCJC ANR project, we follow a thought-provocative and unexplored direction: we consider that the variability boundary of a software system can be specialized and should vary when needs be. The goal of this project is to provide theories, methods and techniques to make variability vary. Speciﬁcally, we consider machine learning and software engineering techniques for narrowing the space of possible conﬁgurations to a good approximation of those satisfying the needs of users. Based on an oracle (e.g., a runtime test) that tells us whether a given conﬁguration meets the requirements (e.g., speed or memory footprint), we leverage machine learning to retroﬁt the acquired constraints into a variability that can be used to automatically specialize the conﬁgurable system. Based on a relative small number of conﬁguration samples, we expect to reach high accuracy for many different kinds of oracles and subject systems. Our preliminary experiments suggest that varying variability can be practically useful and effective. However, much more work is needed to investigate sampling, testing, and learning techniques within a variety of cases and application scenarios. We plan to further collect large experimental data and apply our techniques on popular, open-source, conﬁgurable software (like Linux, Firefox, ffmpeg, VLC, Apache or JHipster) and generators for media content (like videos, models for 3D printing, or technical papers written in LaTeX). • Abstract: in the context of this project, DGA-MI and the INRIA team DiverSE explore the existing approaches to ease the development of formal speciﬁcations of domain-Speciﬁc Languages (DSLs) dedicated to packet ﬁltering, while guaranteeing expressiveness, precision and safety. In the long term, this work is part of the trend to provide to DGA-MI and its partners a tooling to design and develop formal DSLs which ease the use while ensuring a high level of reasoning. • Abstract: The ONEWAY project aims at maturing digital functional bricks for the following capacities: 1) Digitalization, MBSE modeling and synthetic analysis by substitution model, of all the information and under all the points of view necessary for the design and validation across an extended enterprise of the complete aircraft system and at all its levels of decomposition, 2) Generic and instantiable conﬁguration management throughout the life cycle, on products and their support systems, in product lines or on aircraft programs, interactively in the context of an extended enterprise, 3) Decision support for launching, then controlling and steering a Product Development • Abstract: The IPSCO (Intelligent Support Processes and Communities) project aims to develop a new customer support platform for digital companies and public services. Both by implementing intell",,2022.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5c51a7ca91a4c0b261fac9b0fd3628c55c55b03e,https://www.semanticscholar.org/paper/5c51a7ca91a4c0b261fac9b0fd3628c55c55b03e,Trends and Directions of Financial Technology (Fintech) in Society and Environment: A Bibliometric Study,"The contemporary innovations in financial technology (fintech) serve society with an environmentally friendly atmosphere. Fintech covers an enormous range of activities from data security to financial service deliverables that enable the companies to automate their existing business structure and introduce innovative products and services. Therefore, there is an increasing demand for scholars and professionals to identify the future trends and directions of the topic. This is why the present study conducted a bibliometric analysis in social, environmental, and computer sciences fields to analyse the implementation of environment-friendly computer applications to benefit societal growth and well-being. We have used the ‘bibliometrix 3.0’ package of the r-program to analyse the core aspects of fintech systematically. The study suggests that ‘ACM International Conference Proceedings’ is the core source of published fintech literature. China leads in both multiple and single country production of fintech publications. Bina Nusantara University is the most relevant affiliation. Arner and Buckley provide impactful fintech literature. In the conceptual framework, we analyse relationships between different topics of fintech and address dynamic research streams and themes. These research streams and themes highlight the future directions and core topics of fintech. The study deploys a co-occurrence network to differentiate the entire fintech literature into three research streams. These research streams are related to ‘cryptocurrencies, smart contracts, financial technology’, ‘financial industry stability, service, innovation, regulatory technology (regtech)’, and ‘machine learning and deep learning innovations’. The study deploys a thematic map to identify basic, emerging, dropping, isolated, and motor themes based on centrality and density. These various themes and streams are designed to lead the researchers, academicians, policymakers, and practitioners to narrow, distinctive, and significant topics.",Applied Sciences,2021.0,10.3390/app112110353,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
849d470245aed80394124dc54f1ebaf30e854a81,https://www.semanticscholar.org/paper/849d470245aed80394124dc54f1ebaf30e854a81,Security as a Solution: An Intrusion Detection System Using a Neural Network for IoT Enabled Healthcare Ecosystem,"Aim/Purpose The primary purpose of this study is to provide a cost-effective and artificial intelligence enabled security solution for IoT enabled healthcare ecosystem. It helps to implement, improve, and add new attributes to healthcare services. The paper aims to develop a method based on an artificial neural network technique to predict suspicious devices based on bandwidth usage. Background COVID has made it mandatory to make medical services available online to every remote place. However, services in the healthcare ecosystem require fast, uninterrupted facilities while securing the data flowing through them. The solution in this paper addresses both the security and uninterrupted services issue. This paper proposes a neural network based solution to detect and disable suspicious devices without interrupting critical and life-saving services. Methodology This paper is an advancement on our previous research, where we performed manual knowledge-based intrusion detection. In this research, all the experiments were executed in the healthcare domain. The mobility pattern of the devices was divided into six parts, and each one is assigned a dedicated slice. The security module regularly monitored all the clients connected to slices, and machine learning was used to detect and disable the problematic or suspicious devices. We have used MATLAB’s neural network to train the dataset and automatically detect and disable suspicious devices. The different network architectures and different training algorithms (Levenberg–Marquardt and Bayesian Framework) in MATLAB software have attempted to achieve more precise values with different properties. Five iterations of training were executed and compared to get the best result of R=99971. We configured the application to handle the four most applicable use cases. We also performed an experimental application simulation for the assessment and validation of predictions. Contribution This paper provides a security solution for the IoT enabled healthcare system. The architectures discussed suggest an end-to-end solution on the sliced network. Efficient use of artificial neural networks detects and block suspicious devices. Moreover, the solution can be modified, configured and deployed in many other ecosystems like home automation. Findings This simulation is a subset of the more extensive simulation previously performed on the sliced network to enhance its security. This paper trained the data using a neural network to make the application intelligent and robust. This enhancement helps detect suspicious devices and isolate them before any harm is caused on the network. The solution works both for an intrusion detection and prevention system by detecting and blocking them from using network resources. The result concludes that using multiple hidden layers and a non-linear transfer function, logsig improved the learning and results. Recommendations Everything from offices, schools, colleges, and e-consultation is currently for Practitioners happening remotely. It has caused extensive pressure on the network where the data flowing through it has increased multifold. Therefore, it becomes our joint responsibility to provide a cost-effective and sustainable security solution for IoT enabled healthcare services. Practitioners can efficiently use this affordable solution compared to the expensive security options available in the commercial market and deploy it over a sliced network. The solution can be implemented by NGOs and federal governments to provide secure and affordable healthcare monitoring services to patients in remote locations. Recommendations Research can take this solution to the next level by integrating artificial intellifor Researchers gence into all the modules. They can augment this solution by making it compatible with the federal government’s data privacy laws. Authentication and encryption modules can be integrated to enhance it further. Impact on Society COVID has given massive exposure to the healthcare sector since last year. With everything online, data ecurity and privacy is the next most significant concern. This research can be of great support to those working for the security of health care services. This paper provides “Security as a Solution”, which can enhance the security of an otherwise less secure ecosystem. The healthcare use cases discussed in this paper address the most common security issues in the IoT enabled healthcare ecosystem. Future Research We can enhance this application by including data privacy modules like authentication and authorisation, data encryption and help to abide by the federal privacy laws. In addition, machine learning and artificial intelligence can be extended to other modules of this application. Moreover, this experiment can be easily applicable to many other domains like e-homes, e-offices and many others. For example, e-homes can have devices like kitchen equipment, rooms, dining, cars, bicycles, and smartwatches. Therefore, one can use this application to monitor these devices and detect any suspicious activity. © 2021 Informing Science Institute. All rights reserved.",,2021.0,10.28945/4838,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
75cbdcb6a867ec2a2919f1e4734fa9ac8bec85c5,https://www.semanticscholar.org/paper/75cbdcb6a867ec2a2919f1e4734fa9ac8bec85c5,Development of Social Media Analytics System for Emergency Event Detection and Crisis Management,"Social media platforms have proven to be effective for information gathering during emergency events caused by natural or human-made disasters. Emergency response authorities, law enforcement agencies, and the public can use this information to gain situational awareness and improve disaster response. In case of emergencies, rapid responses are needed to address victims' requests for help. The research community has developed many social media platforms and used them effectively for emergency response and coordination in the past. However, most of the present deployments of platforms in crisis management are not automated, and their operational success largely depends on experts who analyze the information manually and coordinate with relevant humanitarian agencies or law enforcement authorities to initiate emergency response operations. The seamless integration of automatically identifying types of urgent needs from millions of posts and delivery of relevant information to the appropriate agency for timely response has become essential. This research project aims to develop a generalized Information Technology (IT) solution for emergency response and disaster management by integrating social media data as its core component. In this paper, we focused on text analysis techniques which can help the emergency response authorities to filter through the sheer amount of information gathered automatically for supporting their relief efforts. More specifically, we applied state-of-the-art Natural Language Processing (NLP), Machine Learning (ML), and Deep Learning (DL) techniques ranging from unsupervised to supervised learning for an in-depth analysis of social media data for the purpose of extracting real-time information on a critical event to facilitate emergency response in a crisis. As a proof of concept, a case study on the COVID-19 pandemic on the data collected from Twitter is presented, providing evidence that the scientific and operational goals have been achieved. © 2021 Tech Science Press. All rights reserved.","Computers, Materials & Continua",2021.0,10.32604/CMC.2021.017371,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
241be5393dba38bbc634d9147618fe11c161eb09,https://www.semanticscholar.org/paper/241be5393dba38bbc634d9147618fe11c161eb09,Rapidly-exploring Random Forest: Adaptively Exploits Local Structure with Generalised Multi-Trees Motion Planning,"Sampling-based motion planners perform exceptionally well in robotic applications that operate in highdimensional space. However, most works often constrain the planning workspace rooted at some fixed locations, do not adaptively reason on strategy in narrow passages, and ignore valuable local structure information. In this paper, we propose Rapidly-exploring Random Forest (R R F*)—a generalised multitrees motion planner that combines the rapid exploring property of tree-based methods and adaptively learns to deploys a Bayesian local sampling strategy in regions that are deemed to be bottlenecks. Local sampling exploits the local-connectivity of spaces via Markov Chain random sampling, which is updated sequentially with a Bayesian proposal distribution to learns the local structure from past observations. The trees selection problem is formulated as a multi-armed bandit problem, which efficiently allocates resources on the most promising tree to accelerate planning runtime. R R F* learns the region that is difficult to perform tree extensions and adaptively deploys local sampling in those regions to maximise the benefit of exploiting local structure. We provide rigorous proofs of completeness and optimal convergence guarantees, and we experimentally demonstrate that the effectiveness of R R F*’s adaptive multi-trees approach allows it to performs well in a wide range of problems. I . I N T R O D U C T I O N Motion planning is one of the fundamental methods for robots to navigate and integrate with the real-world. Obstacles and physical constraints are ubiquitous regardless of the type of robotic applications, and we wish to safely and efficiently navigate the robot from some initial state to a target state. Sampling-based motion planners (SBPs) are of a class of robust methods to perform motion planning. SBPs do no need to explicitly construct the often intractable high-dimensional Configuration Space (C-space). SBP samples C-space randomly for valid connections and iteratively builds a roadmap of connectivity. SBPs are guaranteed to find a solution if one exists [1]. Further developments on SBPs had granted asymptotic optimality [2]—a guarantee that the solution will converge, in the limit, to the optimal solution. One of the fundamental issues with SBPs lies in SBP’s approach—the exceedingly low likelihood of sampling within narrow passages. Intuitively, regions with low visibility will have a low probability to be randomly sampled. Therefore, when SBPs perform roadmap-based planning by random sampling and creating connections in C-space, highly constrained regions become problematic as they limit the connectivity of free space [3], [4]. Narrow passages severely restrict the performance of SBPs because SBPs throw away the unlikely samples that do fall within narrow passages if Correspondence to tin.lai@sydney.edu.au †School of Computer Science, The University of Sydney, Australia. Fig. 1. Overview illustration of R R F* using multiple trees to adaptively plans on the current most promising tree, which rapidly explores visible space and exploits local structures with Bayesian local sampling. Green and red nodes refers to the initial qinit and target qtarget configuration; teal and purple trees are the rooted Tinit, Ttarget and local trees respectively. Orange hatched circles are the current states of local samplers sampling for their respective local trees. R R F* learns to creates new local trees in highly constrained regions that are hard to extend from root trees, of which will be benefited from proposing informed samples through Bayesian local sampling. the tree failed to expand. Consequently, narrow passages will bottleneck the tree’s growth until a series of tree expansions had successfully created connections within the restricting narrow passages. Our contribution is an incremental multi-trees SBP— Rapidly-exploring Random Forest (R R F*)—that learns from sampling information and adjust planning strategy accordingly. We formulate R R F* to learns regions that are likely to be bottlenecked and adaptively deploys Bayesian local sampling to exploits C-space’s local structure. Unlike previous local SBPs that deploy local trees everywhere regardless of the nearby region’s complexity, R R F* utilises the rapid growth of the rooted trees approach for open spaces and adaptively uses local trees within bottlenecks. Bayesian local sampling tackles the narrow passage problem by performing sequential Markov chain Monte Carlo (MCMC) random walks within the passage, and at the same time, updates its proposal distribution from sampled outcomes. In additions, R R F* plans with multiple trees and allocates planning resources on the most promising tree by the reward signal from our multi-armed bandit formulation. We provide rigorous proofs on completeness and optimal convergence guarantees. ar X iv :2 10 3. 04 48 7v 1 [ cs .R O ] 7 M ar 2 02 1 Experimentally, we show that R R F* achieves superior results in a wide range of scenarios, especially, R R F* yields high sample efficiency in highly constrained C-space. I I . R E L AT E D W O R K One of the most influential works on SBPs is Probabilistic Roadmap (P R M), which creates a random roadmap of connectivity that can be reused [5]. Rapidly-exploring Random Tree (R R T) [6] follows a similar idea but instead uses a tree structure to obtain a more rapid single-query solution. Most SBPs minimises some cost, e.g., distance metric or control cost. Therefore, P R M∗ [7] and R R T* [8] are introduced that exhibit asymptotic optimal guarantee. The runtime performance of SBPs had been one of the main focus in existing works. For example, to address the narrow passage problem, some planners focus the random sampling to specific regions in C-space [9]. Such an approach often requires some technique to discover narrow passages, e.g., using bridge test [10], space projection [11], heuristic measures of obstacles boundary [12], [13], and densely samples at discovered narrow regions [14]–[16]. There are also sampling techniques that improve sampling efficiency by using a restricted or learned sampling distribution [17]–[20], which either formulate some regions for generating samples or deploy a machine learning approach to learn from experience. However, while those approaches improved sampling efficiency, they do not directly address the limited visibility issue within narrow passages. Some planners had employed a multi-tree approach to explore C-space more efficiently. For example, growing bidirectional trees can speed up the exploration process because tree extensions at different origin are subject to a different degree of difficulty [21]. Potential tree locations can be searched with the bridge test, followed by a learning technique to model the probability of tree selection [22]. Local trees had been employed for growing multiple trees in parallel [4], [23]. However, current approaches utilise local trees regardless of the complexity of the nearby regions. While those approaches had improved sampling efficiency, it would be more beneficial to deploy local planners dependent on the C-space complexity adaptively. Several SBPs had utilised Markov Chain Monte Carlo (MCMC) for local planning, as it allows utilisation of information observed from previous samples [24]. Monte Carlo random walk planner searches free space by constructing a Markov Chain to propose spaces with high contributions [25]. The roadmap of a PRM can be formulated as the result of simultaneously running a set of MCMC explorations [26]. Therefore, the connectives between feasible states can be modelled as a chain of samples walking within the free space. Our proposing R R F* exploits this property by utilising a Bayesian approach in proposing chained samples by sequentially updating our belief on the space that are deemed to be bottlenecks. I I I . R A P I D LYE X P L O R I N G R A N D O M F O R E S T Motion planning’s objective is to construct a feasible trajectory from an initial configuration qinit to a target configuration qtarget, where q ∈ C ⊆ R denotes a state the C-space and d ≥ 2 is the dimensionality. The obstacle spaces Cobs denotes the set of invalid states, and the set of free space Cfree is defined as the closure set cl(C \ Cobs). In motion planning, there is often some cost function that the planner wants to optimise. Problem 1 (Asymptotic optimal planning): Given C,Cobs, a pair of initial qinit and target qtarget configurations, a cost function Lc : σ → [0,∞), and let Γ(Cfree) denotes the set of all possible trajectories in cl(Cfree). Find a solution trajectory σ∗ that exhibits the minimal cost. That is, find σ∗ such that σ∗(0) = qinit, σ∗(1) = qtarget, and Lc(σ∗) = minσ∈Γ(Cfree) Lc(σ). A. High-level description Fig. 2. Illustration of R R F* performing Bayesian local sampling for a local tree. The spherical proposal distribution is overlaid on the local sampler as a blue curve, which is sequentially learned after 10 previous failed samples (purple arrows). The transparent grey arrows illustrates the likelihood of sampling 20 times from the probability distribution shown. R R F* uses multiple trees to adaptively explores and exploits different regions of C-space. Traditional approaches use only a single tree rooted at qinitial to construct a roadmap of connected configurations. The tree grows outwards by sampling random configurations that create a new connection to the closest node. Since the tree expansion is limited to a local scope bounded by the neighbourhood visibility of the frontier tree nodes, such an approach will often reject updates from valid samples when there exists no free route from the closest existing node towards the sampled configuration. R R F* overcomes this limitation by addressing the sampling-based motion planning problem with a divide-and-conquer approach. R R F* follows the approach in [27] which uses local trees for Bayesian local sampling. However, instead of purely",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
68aff2731ad4ee1e83309d3b2f7ff240d7987208,https://www.semanticscholar.org/paper/68aff2731ad4ee1e83309d3b2f7ff240d7987208,Design Development And Fabrication Of Sugarcane Bud,"Design, Development and Fabrication of a Precision Autocollimating Solar Sensor /PASS/DESIGN, DEVELOPMENT, AND FABRICATION OF PROTOTYPE LOW-ENRICHED SUPERHEATER FUEL ELEMENTS (LESH).Design, Development and Fabrication of a Deployable/retractable Truss Beam Model for Large Space Structures ApplicationDesign, Development and Testing of Calspan/Chrysler Research Safety Vehicle Phase II. Volume I. Final Technical ReportDesign, Development, and Fabrication of a Sealed, Brushless Dc Motor Final ReportCompendium of Industrial Research, Design, and Development Facilities Available in Uttar PradeshDesign, Development and Fabrication of an Advanced High-Precision Robotic System for MicrosurgeryDesign, development, fabrication and testing of an operational prototype surge protectiveDesign, Development, and Fabrication of the FMU-26/B and FMU-26A/B Bomb FuzesDesign, Test, and Microfabrication of MEMS and MOEMSDesign and Development of RFID and RFID-Enabled Sensors on Flexible Low Cost SubstratesDesign for ManufacturingDesign and Fabrication of an Internally Insulated Filament Wound Liquid Hydrogen Propellant TankCalifornia. Court of Appeal (2nd Appellate District). Records and BriefsScientific and Technical Aerospace ReportsFurniture DesignDesign, Development and Fabrication of a Solar Experiment Alignment Sensor (SEAS)Axiomatic Design and Fabrication of Composite StructuresAdvanced Technology for Design and Fabrication of Composite Materials and StructuresRobotic Fabrication in Architecture, Art and Design 2014Proceedings of the U.S./U.S.S.R. Seminar on Problems of Design, Development, Fabrication and Test of Breeder Reactor ComponentsNuclear Science AbstractsRobotic Fabrication in Architecture, Art and Design 2018Research, Development, and Mechanization in the United States Post Office DepartmentDistributed Intelligence In DesignAeronautical Engineering ReviewDesign Methodologies for Space Transportation SystemsDesign, Development, and Fabrication of a Sealed, Brushless DC MotorDesign, Fabrication, Properties and Applications of Smart and Advanced MaterialsU.S. Government Research ReportsProcesses and Design for ManufacturingAEC Authorizing Legislation, Fiscal Year 1969Smart Material Systems and MEMSMaterials, Design and Manufacturing for Lightweight VehiclesDesign, Development and Fabrication of a New Generation Semiconductor X-ray DetectorNuclear Regulatory Legislation, 109th Congress, 2nd SessionDigital Design and FabricationDesign, Development, and Fabrication of a Electronic Analog Microminiaturized Electronic Analog Signal to Discrete Time Interval ConverterDesign, Development, Fabrication, and Testing of a Synchronous Condenser for a High-power Three-phase Traction DriveProduct Design for Manufacture and Assembly In response to tremendous growth and new technologies in the semiconductor industry, this volume is organized into five, information-rich sections. Digital Design and Fabrication surveys the latest advances in computer architecture and design as well as the technologies used to manufacture and test them. Featuring contributions from leading experts, the book also includes a new section on memory and storage in addition to a new chapter on nonvolatile memory technologies. Developing advanced concepts, this sharply focused book— Describes new technologies that have become driving factors for the electronic industry Includes new information on semiconductor memory circuits, whose development best illustrates the phenomenal progress encountered by the fabrication and technology sector Contains a section dedicated to issues related to system power consumption Describes reliability and testability of computer systems Pinpoints trends and state-of-theart advances in fabrication and CMOS technologies Describes performance evaluation measures, which are the bottom line from the user’s point of view Discusses design techniques used to create modern computer systems, including high-speed computer arithmetic and high-frequency design, timing and clocking, and PLL and DLL designDesign for Manufacturing assists anyone not familiar with various manufacturing processes in better visualizing and understanding the relationship between part design and the ease or difficulty of producing the part. Decisions made during the early conceptual stages of design have a great effect on subsequent stages. In fact, quite often more than 70% of the manufacturing cost of a product is determined at this conceptual stage, yet manufacturing is not involved. Through this book, designers will gain insight that will allow them to assess the impact of their proposed design on manufacturing difficulty. The vast majority of components found in commercial batch-manufactured products, such as appliances, computers and office automation equipment are either injection molded, stamped, die cast, or (occasionally) forged. This book emphasizes these particular, most commonly implemented processes. In addition to chapters on these processes, the book touches upon material process selection, general guidelines for determining whether several components should be combined into a single component or not, communications, the physical and mechanical properties of materials, tolerances, and inspection and quality control. In developing the DFM methods presented in this book, he has worked with over 30 firms specializing in injection molding, die-casting, forging and stamping. Implements a philosophy which allows for easier and more economic production of designs Educates designers about manufacturing Emphasizes the four major manufacturing processesPresenting unified coverage of the design and modeling of smart microand macrosystems, this book addresses fabrication issues and outlines the challenges faced by engineers working with smart sensors in a variety of applications. Part I deals with the fundamental concepts of a typical smart system and its constituent components. Preliminary fabrication and characterization concepts are introduced before design principles are discussed in detail. Part III presents a comprehensive account of the modeling of smart systems, smart sensors and actuators. Part IV builds upon the fundamental concepts to analyze fabrication techniques for silicon-based MEMS in more detail. Practicing engineers will benefit from the detailed assessment of applications in communications technology, aerospace, biomedical and mechanical engineering. The book provides an essential reference or textbook for graduates following a course in smart sensors, actuators and systems.This book introduces various advanced, smart materials and the strategies for the design and preparation for novel uses from macro to micro or from biological, inorganic, organic to composite materials. Selecting the best material is a challenging task, requiring tradeoffs between material properties and designing functional smart materials. The development of smart, advanced materials and their potential applications is a burgeoning area of research. Exciting breakthroughs are anticipated in the future from the concepts and results reported in this book.The work described in this document was performed in compliance with the scope of work as specified in Contract AF 08(635)-2850 tendered Honeywell Ordnance Division on 13 June 1962. All phases of a complete development program were carried out in order to achieve the goal of developing a safe, highly reliable fuze compatible with available subsonic and supersonic delivery systems. The final result of this development program was a multi-purpose fuze operable in three different modes: impact short-delay, impact medium-delay, and airburst. Fuzes were subjected to every environmental, functional, and safety test for development of fuzes required by the Air Force and by the contract. A program for the development of fuzes incorporating a retard-mode capability into the fuze was conducted, but the mode could not be included without extensive fuze redesign. Several recommendations were made by the contractor to expend additional efforts under the production program to effect the following: loading simplification, battery firing device simplification or integration, safing and arming mechanism simplification, and general safety improvements. (Author).The book presents research from Rob|Arch 2018, the fourth international conference on robotic fabrication in architecture, art, and design. In capturing the myriad of scientific advances in robotics fabrication that are currently underway – such as collaborative design tools, computerised materials, adaptive sensing and actuation, advanced construction, on-site and cooperative robotics, machine-learning, human-machine interaction, large-scale fabrication and networked workflows, to name but a few – this compendium reveals how robotic fabrication is becoming a driver of scientific innovation, cross-disciplinary fertilization and creative capacity of an unprecedented kind.pt.1: Considers S. 2880 and companion H.R. 14905, to authorize appropriations for AEC. Focuses on general budget and reactor development program; pt.2: Continuation of hearings on AEC FY69 authorization. Appendix includes reports. a. ""National Accelerator Laboratory, Design Report 1968, Universities Research Associates, "" prepared by AEC 1968 (p. 1223-1456). b. ""Report of Ad Hoc Panel on Low-Beta Toroidal Plasma Research, "" Sept. 1967 (p. 1459-1583). c. ""Bronco Oil Shale Study, "" prepared by AEC, Interior Dept, CER Geonuclear Corp., and Lawrence Radiation Laboratory, Oct. 13, 1967 (p. 1743-1813).Compiles statutues and materials relating to nuclear regulatory legislation through the 109th Congress, 1st Session.Hailed as a groundbreaking and important textbook upon its initial publication, the latest iteration of Product Design for Manufacture and Assembly does not rest on those laurels. In addition to the expected updating of data in all chapters, this third edition has been revised to provide a top-notch textbook for ",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8440f4751eb5ef87c41f95d969af473ed4149019,https://www.semanticscholar.org/paper/8440f4751eb5ef87c41f95d969af473ed4149019,Cloud Computing Based Real Time Vehicle Tracking And Speed Epdf Download,"This book addresses the emerging area of cloud computing, providing a comprehensive overview of the research areas, recent work and open research problems. The move to cloud computing is no longer merely a topic of discussion; it has become a core competency that every modern business needs to embrace and excel at. It has changed the way enterprise and internet computing is viewed, and this success story is the result of the long-term efforts of computing research community around the globe. It is predicted that by 2026 more than two-thirds of all enterprises across the globe will be entirely run in cloud. These predictions have led to huge levels of funding for research and development in cloud computing and related technologies. Accordingly, universities across the globe have incorporated cloud computing and its related technologies in their curriculum, and information technology (IT) organizations are accelerating their skill-set evolution in order to be better prepared to manage emerging technologies and public expectations of the cloud, such as new services. A critical part of ensuring that systems are advancing alongside technology without complications is problem solving. Practical applications of problem-solving theories can model conflict and cooperation and aid in creating solutions to real-world problems. Soft-Computing-Based Nonlinear Control Systems Design is a critical scholarly publication that examines the practical applications of control theory and its applications in problem solving to fields including economics, environmental management, and financial modelling. Featuring a wide range of topics, such as fuzzy logic, nature-inspired algorithms, and cloud computing, this book is geared toward academicians, researchers, and students seeking relevant research on control theory and its practical applications. This book explores the significant role of granular computing in advancing machine learning towards in-depth processing of big data. It begins by introducing the main characteristics of big data, i.e., the five Vs—Volume, Velocity, Variety, Veracity and Variability. The book explores granular computing as a response to the fact that learning tasks have become increasingly more complex due to the vast and rapid increase in the size of data, and that traditional machine learning has proven too shallow to adequately deal with big data. Some popular types of traditional machine learning are presented in terms of their key features and limitations in the context of big data. Further, the book discusses why granular-computing-based machine learning is called for, and demonstrates how granular computing concepts can be used in different ways to advance machine learning for big data processing. Several case studies involving big data are presented by using biomedical data and sentiment data, in order to show the advances in big data processing through the shift from traditional machine learning to granularcomputing-based machine learning. Finally, the book stresses the theoretical significance, practical importance, methodological impact and philosophical aspects of granular-computing-based machine learning, and suggests several further directions for advancing machine learning to fit the needs of modern industries. This book is aimed at PhD students, postdoctoral researchers and academics who are actively involved in fundamental research on machine learning or applied research on data mining and knowledge discovery, sentiment analysis, pattern recognition, image processing, computer vision and big data analytics. It will also benefit a broader audience of researchers and practitioners who are actively engaged in the research and development of intelligent systems. This book is a compilation of research work in the interdisciplinary areas of electronics, communication, and computing. This book is specifically targeted at students, research scholars and academicians. The book covers the different approaches and techniques for specific applications, such as particle-swarm optimization, Otsu’s function and harmony search optimization algorithm, triple gate silicon on insulator (SOI)MOSFET, micro-Raman and Fourier Transform Infrared Spectroscopy (FTIR) analysis, high-k dielectric gate oxide, spectrum sensing in cognitive radio, microstrip antenna, Ground-penetrating radar (GPR) with conducting surfaces, and digital image forgery detection. The contents of the book will be useful to academic and professional researchers alike. This volume contains the technical papers presented in the workshops associated with the European Conference on Service-Oriented and Cloud Computing, ESOCC 2016, held in Vienna, Austria, in September 2016: 4th International Workshop on Cloud for IoT, CLloT 2016, Second International Workshop on Cloud Adoption and Migration, CloudWays 2016, First International Workshop on Patterns and Pattern Languages for SOCC: Use and Discovery, PATTWORLD 2016, combined with the First International Workshop on Performance and Conformance of Workflow Engines, PEaCE 2016, IFIP WG SOS Workshop 2016 Rethinking Services ResearCH, ReSeRCH 2016. Furthermore, there is a topical section presenting the results of the PhD Symposium. The abstracts of the presentations held at the European Projects Forum, EU Projects 2016, are included in the back-matter of the volume. The 15 full papers included in this volume were carefully reviewed and selected from 49 submissions. They focus on specific topics in service-oriented and cloud computing domains such as limits and/or advantages of existing cloud solutions, future internet technologies, efficient and adaptive deployment and management of servicebased applications across multiple clouds, novel cloud service migration practices and solutions, digitization of enterprises in the cloud computing era, federated cloud networking services. Im Zeitalter des Internet of Things (IoT) erzeugen Edge-Geräte in jedem Sekundenbruchteil gigantische Datenmengen. Dabei besteht das Hauptziel dieser Netzwerke darin, aus den gesammelten Daten sinnvolle Informationen abzuleiten. Gleichzeitig werden gewaltige Datenmengen in die Cloud übertragen, was extrem teuer und zeitaufwändig ist. Es ist somit notwendig, effiziente Mechanismen für die Verarbeitung dieser gewaltigen Datenmengen zu entwickeln, und dafür sind effiziente Datenverarbeitungstechniken erforderlich. Nachhaltige Paradigmen wie Cloud Computing und Fog Computing tragen zu einem geschickten Umgang mit Themen wie Leistung, Speicherund Verarbeitungskapazitäten, Wartung, Sicherheit, Effizienz, Integration, Kosten, Energieverbrauch und Latenzzeiten bei. Allerdings werden ausgefeilte Analysetools benötigt, um die Anfragen in einer optimalen Zeit zu bearbeiten. Daher wird derzeit eifrig an der Entwicklung eines effektiven und effizienten Rahmens geforscht, um den größtmöglichen Nutzen zu erhalten. Bei der Verarbeitung der gewaltigen Datenmengen steht das maschinelle Lernen besonders hoch im Kurs und wird in zahlreichen Disziplinen angewandt, auch in den sozialen Medien. In Machine Learning Approach for Cloud Data Analytics in IoT werden sämtliche Aspekte des IoT, des Cloud Computing und der Datenanalyse ausführlich erläutert und aus verschiedenen Perspektiven betrachtet. Das Buch präsentiert den neuesten Stand der Forschung und fortschrittliche Themen. So erhalten die Leserinnen und Leser aktuelle Informationen und können das gesamte Spektrum der Anwendungen von IoT, Cloud Computing und Datenanalyse erfassen. International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA) is one of the flagship conferences on BioComputing, bringing together the world’s leading scientists from different areas of Natural Computing. Since 2006, the conferences have taken place at Wuhan (2006), Zhengzhou (2007), Adelaide (2008), Beijing (2009), Liverpool & Changsha (2010), Malaysia (2011) and India (2012). Following the successes of previous events, the 8th conference is organized and hosted by Anhui University of Science and Technology in China. This conference aims to provide a high-level international forum that researchers with different backgrounds and who are working in the related areas can use to present their latest results and exchange ideas. Additionally, the growing trend in Emergent Systems has resulted in the inclusion of two other closely related fields in the BIC-TA 2013 event, namely Complex Systems and Computational Neuroscience. These proceedings are intended for researchers in the fields of Membrane Computing, Evolutionary Computing and Genetic Algorithms, DNA and Molecular Computing, Biological Computing, Swarm Intelligence, Autonomy-Oriented Computing, Cellular and Molecular Automata, Complex Systems, etc. Professor Zhixiang Yin is the Dean of the School of Science, Anhui University of Science & Technology, China. Professor Linqiang Pan is the head of the research group of Natural Computing at Huazhong University of Science and Technology, Wuhan, China. Professor Xianwen Fang also works at the Anhui University of Science & Technology. Distributed systems intertwine with our everyday lives. The benefits and current shortcomings of the underpinning technologies are",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
76e78bc520bdc550e762a5d82bd6aca84bd8c5bb,https://www.semanticscholar.org/paper/76e78bc520bdc550e762a5d82bd6aca84bd8c5bb,Data Driven Methods For Fault Detection And Diagnosis In Chemical Processes Advances In Industrial Control Epdf File,"Guaranteeing a high system performance over a wide operating range is an important issue surrounding the design of automatic control systems with successively increasing complexity. As a key technology in the search for a solution, advanced fault detection and identification (FDI) is receiving considerable attention. This book introduces basic model-based FDI schemes, advanced analysis and design algorithms, and mathematical and control-theoretic tools. This second edition of Model-Based Fault Diagnosis Techniques contains: • new material on fault isolation and identification and alarm management; • extended and revised treatment of systematic threshold determination for systems with both deterministic unknown inputs and stochastic noises; • addition of the continuously-stirred tank heater as a representative process-industrial benchmark; and • enhanced discussion of residual evaluation which now deals with stochastic processes. Model-based Fault Diagnosis Techniques will interest academic researchers working in fault identification and diagnosis and as a text it is suitable for graduate students in a formal university-based course or as a self-study aid for practising engineers working with automatic control or mechatronic systems from backgrounds as diverse as chemical process and power engineering. With pressure increasing to utilise wastes and residues effectively and sustainably, the production of biogas represents one of the most important routes towards reaching national and international renewable energy targets. The biogas handbook: Science, production and applications provides a comprehensive and systematic guide to the development and deployment of biogas supply chains and technology. Following a concise overview of biogas as an energy option, part one explores biomass resources and fundamental science and engineering of biogas production, including feedstock characterisation, storage and pre-treatment, and yield optimisation. Plant design, engineering, process optimisation and digestate utilisation are the focus of part two. Topics considered include the engineering and process control of biogas plants, methane emissions in biogas production, and biogas digestate quality, utilisation and land application. Finally, part three discusses international experience and best practice in biogas utilisation. Biogas cleaning and upgrading to biomethane, biomethane use as transport fuel and the generation of heat and power from biogas for stationery applications are all discussed. The book concludes with a review of market development and biomethane certification schemes. With its distinguished editors and international team of expert contributors, The biogas handbook: Science, production and applications is a practical reference to biogas technology for process engineers, manufacturers, industrial chemists and biochemists, scientists, researchers and academics working in this field. Provides a concise overview of biogas as an energy option Explores biomass resources for production Examines plant design and engineering and process optimisation This book provides a complete picture of several decision support tools for predictive maintenance. These include embedding early anomaly/fault detection, diagnosis and reasoning, remaining useful life prediction (fault prognostics), quality prediction and self-reaction, as well as optimization, control and self-healing techniques. It shows recent applications of these techniques within various types of industrial (production/utilities/equipment/plants/smart devices, etc.) systems addressing several challenges in Industry 4.0 and different tasks dealing with Big Data Streams, Internet of Things, specific infrastructures and tools, high system dynamics and non-stationary environments . Applications discussed include production and manufacturing systems, renewable energy production and management, maritime systems, power plants and turbines, conditioning systems, compressor valves, induction motors, flight simulators, railway infrastructures, mobile robots, cyber security and Internet of Things. The contributors go beyond state of the art by placing a specific focus on dynamic systems, where it is of utmost importance to update system and maintenance models on the fly to maintain their predictive power. In many industrial applications early detection and diagnosis of abnormal behavior of the plant is of great importance. During the last decades, the complexity of process plants has been drastically increased, which imposes great challenges in development of model-based monitoring approaches and it sometimes becomes unrealistic for modern largescale processes. The main objective of Adel Haghani Abandan Sari is to study efficient fault diagnosis techniques for complex industrial systems using process historical data and considering the nonlinear behavior of the process. To this end, different methods are presented to solve the fault diagnosis problem based on the overall behavior of the process and its dynamics. Moreover, a novel technique is proposed for fault isolation and determination of the root-cause of the faults in the system, based on the fault impacts on the process measurements. Reliability Analysis and Asset Management of Engineering Systems explains methods that can be used to evaluate reliability and availability of complex systems, including simulation-based methods. The increasing digitization of mechanical processes driven by Industry 4.0 increases the interaction between machines and monitoring and control systems, leading to increases in system complexity. For those systems the reliability and availability analyses are increasingly challenging, as the interaction between machines has become more complex, and the analysis of the flexibility of the production systems to respond to machinery failure may require advanced simulation techniques. This book fills a gap on how to deal with such complex systems by linking the concepts of systems reliability and asset management, and then making these solutions more accessible to industry by explaining the availability analysis of complex systems based on simulation methods that emphasise Petri nets. Explains how to use a monitoring database to perform important tasks including an update of complex systems reliability Shows how to diagnose probable machinery-based causes of system performance degradation by using a monitoring database and reliability estimates in an integrated way Describes practical techniques for the application of AI and machine learning methods to fault detection and diagnosis problems",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
64d3f534c2cfa83110f82d092b266167d450c95d,https://www.semanticscholar.org/paper/64d3f534c2cfa83110f82d092b266167d450c95d,Biomedical and health informatics approaches remain essential for addressing the COVID-19 pandemic,"As this March issue of the Journal of the American Medical Informatics Association (JAMIA) is published, we have experienced a year of sheltering-in-place, wearing masks, frequent handwashing, and COVID-19 testing. For some of us, this year also included COVID-19 infection and loss of family, friends, and colleagues— and more recently, COVID-19 vaccination. I accepted JAMIA’s first COVID-related paper on March 19, 2020 less than 24 hours after its submission, and the accepted version was available online within days. To date, JAMIA has received almost 400 COVIDrelated submissions and published 69 contributions in issues since June 2020. The March issue includes 7 papers and correspondence related to COVID-19 including a call from World Health Organization authors to strengthen data in response to COVID-19 and beyond, a report on virtual care expansion in the Veterans Health Administration, and correspondence about telemedicine, privacy, and information security in the age of COVID-19. In this editorial, I highlight 5 papers that reflect the breadth of biomedical and health informatics approaches to addressing the COVID-19 pandemic. Haendel and colleagues provide an overview of the rationale, design, infrastructure, and deployment of the National COVID Cohort Collaborative (N3C). N3C (covid.cd2h.org), an open science community focused on analyzing individual-level data from many centers, was developed by the Clinical and Translational Award Program, the National Center for Translational Sciences, and the scientific community to enable rapid collaboration among clinicians, researchers, and data scientists to identify treatments and specialized care and subsequently reduce the immediate and long-term consequences of COVID-19. To overcome technical, regulatory, policy, and governance barriers to sharing and harmonizing individual-level clinical data, N3C developed (a) legal agreements and governance for organizations and researchers; (b) data extraction scripts to identify and ingest positive, negative, and possible COVID-19 cases; (c) a data quality assurance and harmonization pipeline to create a single harmonized dataset; (d) a secure data enclave with data, machine learning, and statistical analytics tools; (e) dissemination mechanisms; and (f) a synthetic data pilot to democratize data access. There are 3 datasets for analysis: synthetic, deidentified, and limited. The Attribution and Publication Policy encompasses all N3C contributions as reflected in the author contribution statement for this paper. Analyses posted within the N3C enclave leverage the contributor attribution model to track the transitive credit of all upstream contributors. The N3C infrastructure is designed to be scalable and extensible to other topics. Sun and colleagues designed COVID-19 Trial Finder, an opensource semantic search engine, to facilitate patient-centered search of COVID-19 trials. It is powered by a machine-readable dataset for all COVID-19 trials in the United States. COVID-19 Trial Finder also includes a web-based visualization of the geographic distribution of COVID-19 trials. The initial search is by location and radius distance from trial sites; this is refined through a set of dynamically generated medical questions to assess patient eligibility for nearby COVID-19 trials. They assessed the precision of COVID-19 Trial Finder for COVID19 using 20 case reports from LitCOVID. Overall precision across the 20 cases was 79.76%, although it varied widely across cases. The major factor contributing to imprecision was the inability to generate relevant questions in some instances which prevented filtering out irrelevant trials. The system (https://covidtrialx.dbmi.columbia.edu) and its source code (https://github.com/WengLab-InformaticsResearch/COVID19-TrialFinder) are accessible online. Hassandoust, Akhaghpour, and Johnson examine individual privacy concerns and intention to adopt contact tracing mobile applications through a situational privacy calculus model. Using structural equation modeling and a national sample (N1⁄4853) of survey",J. Am. Medical Informatics Assoc.,2021.0,10.1093/jamia/ocab007,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a9f9cff401095d7a95a7cfc98c5f457ee43bd471,https://www.semanticscholar.org/paper/a9f9cff401095d7a95a7cfc98c5f457ee43bd471,QAC: Quantum-computing Aided Composition,"In this chapter I will discuss the role of quantum computing in computer music and how it can be integrated to better serve the creative artists. I will start by considering different approaches in current computer music and quantum computing tools, as well as reviewing some previous attempts to integrate them. Then, I will reflect on the meaning of this integration and present what I coined as QAC (Quantum-computing Aided Composition) as well as an early attempt at realizing it. This chapter will also introduce The QAC Toolkit Max package,1 analyze its performance, and explore some examples of what it can offer to realtime creative practice. Lastly, I will present a real case scenario of QAC in the creative work Disklavier Prelude #3. Computer Music and Quantum Computing tools Recent literature exploring the intersection of Quantum Computing (QC) with creative music practice, of which this book is a prime example, have welcomed the potential increase in speed and computational power offered by future fault tolerant QC machines. In the context of Computer Music (CM), a need for more powerful machines is evident in the limitations that classical machines still present for the realtime (or near realtime) control of compositional processes [2]. Several research projects have already proposed proof-of-concept implementations that integrate QC with music practice, in simulation or with current hardware. However, there is still no consistency between the tools, and approaches undertaken in these explorations, and current CM practice. More importantly, a disconnect between scientific research and creative practice, may only serve to maintain a gap between scientists and artists. My proposed line of inquiry here intends to bridge that gap by focusing on the tools used and how they articulate with realtime creative music practices. 1 The QAC Toolkit is available via the Max Package Manager [1]. Pre-publication draft, to appear in the book Quantum Computer Music, E. R. Miranda (Ed.) Omar Costa Hamido, PhD QAC: Quantum-computing Aided Composition Figure 1 Computer Music Tools and Quantum Computing Tools Modern Computer Music tools and Quantum Computing tools have been shaped by their main users in their practice in such a way that each might currently be regarded as capable of drawing their own self-contained world of practice and practitioners (see figure 1). CM tools include score engravers, Digital Audio Workstations (DAW), and visual programming environments, like Musescore [3], Ableton Live [4], and Max/MSP2 [5], respectively. The use of these 3 categories of tools is deeply embedded in the creative practice of writing musical scores, recording and producing a track, and developing new interactive instruments as well as enabling realtime control of compositional processes. On the other hand, current QC tools seem to inherit greatly from code-based programming practices, where the majority of its user base is found. These include the different QC programming frameworks3 like Qiskit [6], Cirq [7], and pyQuil [8], that are accessed using a terminal or a Jupyter notebook, as well as some web apps that allow the design of circuits online like Strangeworks [9], IBM Quantum Experience [10], and QPS [11].4 These QC tools, based on a more traditional computer programming paradigm, can still be regarded as inheriting the punch card computation paradigm. In it, the user is faced with the clearly delineated step sequence of writing the code, submitting the code to be executed, and waiting for the results to come back. On the other hand, within the CM tools sphere, it is often seen the predominance of a realtime computation paradigm, where the program is being changed as it is being executed. It is worth noting that, while I offer these simple categories here, painting the landscape with large brushstrokes, there are examples of practices that challenge these broad boundaries. Such is the case with live coding [14] where performer-composers can be found programming music live using code-based languages, and often projecting their code on a screen on stage alongside them. 4 For a more complete list of current Quantum Computing tools see [12], [13]. 3 Most of them are based in the Python programming language. 2 From now on, in this chapter, simply referred to as Max. Omar Costa Hamido, PhD QAC: Quantum-computing Aided Composition Computer Music practice, in its broadest sense, is strongly informed by this realtime computation paradigm. From listening to notes as they are being dropped on a score, to tuning audio effects while a song is playing, and algorithmically generating tones and melodies that respond to a live input on the fly. Previous attempts for an integration Given that only recently QC has become more available to the larger community of researchers and enthusiasts worldwide, in both tools and learning references, the first initiatives to integrate QC with Music Composition mostly came from researchers with a Computer Science background. Unsurprisingly, these attempts have relied heavily on QC code-based tools, meant for non-artistic practice. Such is the case with Hendrik Weimer's quantenblog, where he presents some musical examples that were built with his C library for QC simulation, libquantum [15]. As early as 2014, I myself attempted to integrate the (then very obscure) QCL programing language5 with my compositional practice and electroacoustic setup, with no practical success. A second generation can be found expressed in the work published by researchers with stronger artistic considerations. The integration strategies present in these works are mostly characterized by more complex systems that include higher software stack requirements, or simply the proposal of a new CM dedicated application altogether. The first generation of the Quantum Synthesizer [17], a Max-based synthesizer making use of QC, can illustrate this. In this first generation of the Quantum Synthesizer, a 48 hour hackathon project at the Qiskit Camp Europe, in September 2019 [18], Max is used as a frontend where the user changes selected parameters that are passed to a backend Python environment via OSC.6 In turn, this Python environment, that can be running on the same machine or somewhere else in the local area network, is configured with Qiskit and running several Python scripts that account for the network information exchange and to programmatically build quantum circuits based on the parameters received from Max. These circuits are then simulated locally (with or without a noise model) or sent to real quantum computer hardware in the cloud. After retrieving the execution results, these are returned to Max, via OSC, which changes the state of the synthesizer accordingly (see figure 2). 6 Open sound control, a mostly music related, udp-based, networking protocol. 5 The first quantum computing programming language by Bernhard Ömer [16] Omar Costa Hamido, PhD QAC: Quantum-computing Aided Composition Figure 2 Architecture of Quantum Synth. From [17] A similar strategy is explored by Eduardo Reck Miranda in his interactive quantum vocal system architecture (see figure 3). In it, there is also a CM system that is connected to a Python environment, within a networked software architecture. However, Miranda’s approach relies more heavily on the direct use of Python and Jupyter notebooks, with Csound scripts being triggered from the Python environment [19], [20, p. 17]. The musical output, in this case, is managed through a more code-based interface, which was intended to work more seamlessly with the QC framework. This is at the cost of a higher learning curve, and a less realtime CM environment. Figure 3 The interactive quantum vocal system architecture. Reproduced by permission from Eduardo Reck Miranda [20, Fig. 15] Omar Costa Hamido, PhD QAC: Quantum-computing Aided Composition Another approach, taken by James Weaver, has been to create an entirely new CM application environment from scratch. In his web application, Quantum Music Composer, Weaver creates a new interface that allows the user to generate 3rd species counterpoint melodies, based on melody and harmony matrices [21]. The app generates output as Lilypond code, that can be rendered as a readable musical score using the Lilypond music score engraver [22]. Though it has a more clean interface, its use is also more restricted than the previous examples. It is clear from most of these examples that more visual user interfaces, that aren’t simply just a terminal window, are more inviting to musicians and creative users. However, it is also clear that most of these implementations still relied on rather complex software infrastructures that are not easy to set up and modify during the creative process. Weaver’s system requires considerable non-CM and non-QC skills to modify it and use it to achieve a different compositional goal. Miranda’s system requires more knowledge of code-based practices. And my own initial version of the Quantum Synthesizer, even in the more inviting user interface that was explored shortly after its hackathon conception (see figure 3), requires different software pieces to be running at the same time and be launched in the correct order. Figure 4 Second GUI for the Quantum Synth presented in [23] Omar Costa Hamido, PhD QAC: Quantum-computing Aided Composition At this point, there is both a need for more musician friendly interfaces as well as to rethink what is to be expected of this integration and what it should look like. On the one hand it seems that reducing code-based language barriers is one avenue, on the other hand, simplifying the deployment/setup/configuration process of these systems is equally important to make it a more practical tool. For the rest of this chapter, I will give an account of my own work to this effect. A new Quantum-computing Aided Composition When exploring the integration of QC with Music, it is important to have a clear idea of w",ArXiv,2022.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e23e0c6529f205cdeb65f226a59fd996b8a25e8b,https://www.semanticscholar.org/paper/e23e0c6529f205cdeb65f226a59fd996b8a25e8b,A proposal to systematize introducing DevOps into the software development process,"The software development industry has been evolving with new development standards and service delivery models. Agile methodologies have reached their completion with DevOps, thereby increasing the quality of the software and creating greater speed in delivery. However, a gap regarding the formalization of its adoption and implementation doubts became relevant. My hypothesis is that, by systematizing the introduction of DevOps into the software development process and defining the function of the members of the DevOps team members, may well make it quicker to implement this process, thus reducing conflicts between the teams. As part of the investigation of this hypothesis, the result of the research will be applied in practical development environments i.e. in a Technology Agency of the State of the Brazilian Government and also at the Brazilian Company Neurotech in order to evaluate its effectiveness from metrics appropriate for DevOps environments. Keywords—DevOps, software development, maturity model I. PROBLEM DevOps is an emerging practice that has been adopted in the software development cycle. It focuses on the convergence of standards between the Development teams and the Operations teams and it seeks to improve cooperation between both teams, hence the origin of the term [1]. However, there is no consensus on the definition of what DevOps is. Wiedemann et al. [2] emphasize that one of the biggest challenges in the industry is the lack of a formal concept for DevOps. This conceptual flaw in what DevOps is, directly impacts the understanding of the objects and actions needed to overcome this flaw [3]. The adoption of Agile and DevOps Methodologies continues to grow, driven by the “need for speed”, agility and flexibility, evidenced in the World Quality Report of 2019 [4], in which 99% of the interviewees said they were using DevOps in at least some of their business. Implementing DevOps has become more difficult due to the lack of formalizing the concept and adoption processes. Although there is an abundance of information, practices and tools related to DevOps, it is still unclear how anyone could take advantage of this rich, yet diffuse information in an organized and structured way to properly adopt DevOps [6]. The study by Zulfahmi [7] provides evidence of this lack of standardization. The author also states that other major challenges are the lack of process and guidelines for implementing the practice of DevOps in Continuous Delivery. In the Systematic Literature Review conducted by Gasparaite; Naudziunaite and Ragaisis [8], 24 DevOps models were identified, but only 04 were considered applicable for practical use: the Focus Area, Bucena-Kirikova, Mohamed, and Radstaak models. However, in the Radstaak model, not all the steps necessary for its practical use have been described; the Mohamed model did not present the evaluation process, only how to apply the model; and the Focus Area and Bucena-Kirikova Models can be applied in practice, since their authors document the evaluation methods adopted in academic publications. Another aspect to be highlighted concerns what must be done or would be appropriate prior to an organization adopting DevOps. Leite et al. [9] ponder this aspect by considering a difficult question that the literature has not yet fully answered. They further point out that it is incomplete and even contradictory in relation to the subject. At the same time, the variety of DevOps tools seems to challenge the idea of there being a single person with the role of administering the entire process. Even mature teams, who have both knowledge of development and infrastructure operations, may find it difficult to be familiar with all these tools, thus making it necessary to define the roles of those involved in this process. The need to systematize the introduction of DevOps in the software development process is therefore necessary. Thus, the need for more research and empirical work is essential to put into practice and validate a proposal to systematize the introduction of DevOps. In view of the above, the following problems are in evidence: a) the lack of a conceptual definition and, consequently, the need for a proposal to systematize the introduction of DevOps in the software development process, that presents systematic improvements, evidenced by its adoption and the results of its effectiveness; and b) the lack of definition of the roles of those involved in this cycle. The present thesis should conduct a study that sets out to identify the sets of good practices already used in software development processes that use DevOps, and based on these to conduct an analysis, validation and standardization of their use, with a view to systematizing the introduction and execution of DevOps in this process. II. RESEARCH HYPOTHESIS A software production line that uses DevOps, has a welldefined automation cycle, starting with the developers' source code commits for the code version-control system. When the CI server identifies the completion of the commits, it performs the necessary tests and, if necessary, provides feedback to the developers [10]. In this thesis we will propose systematic practices for introducing DevOps and improving the efficiency of the software production process that uses DevOps. As a way of conducting the research and delimiting the scope of the study, the following hypothesis is proposed: Having adopted a set of systematized practices for introducing DevOps, and for formalizing and delimiting the role of the members of the DevOps team in the Software Production Process, Software Factories become faster in their implementation, thereby reducing conflicts between teams and providing quality deliverables. To validate the hypothesis, the following questions were constructed: RQ1: What are the gaps in the software development process that use DevOps practices? In order to propose solutions to the existing problems when introducing DevOps, one must first identify the gaps in the process. The purpose of RQ1 is to locate these gaps and the actions that must be taken to resolve such issues. RQ2: What are the practices that support the systematic introduction of DevOps in the software development process? The objective of RQ2 is to identify all the good practices used by the development and academic industry that support introducing it into the software development process. III. EXPECTED CONTRIBUTION The objective of my doctoral thesis is to improve the software development process using DevOps and to propose new ways of introducing it. The expected contributions of this project are summarized below: 1) to systematize the introduction of DevOps in the software development process; and 2) to improve the adoption of DevOps in the software development process. IV. METHODS AND PRELIMINARY FINDINGS As a preliminary study, a Systematic Mapping (SM) of Literature [11] was undertaken, in May 2020, with a view to analyzing the topic, and hence seeking to identify existing gaps in the area of DevOps and difficulties in the implementation process. The SM [11] will serve to underpin the initial conduct of the research, and thus provide a diagnosis of the software production process that uses DevOps, also providing initial data that can be used to build a set of systematic practices for adopting DevOps. The SM [11] showed how DevOps is highlighting the following issues: pipeline security, effective adoption of a cloud environment, adoption of microservices, infrastructure as code, use of container solutions, and tools to automate the development pipeline. Regarding best practices, DevOps was found to have: Infrastructure as Code, Continuous Integration, Continuous Delivery, Continuous Deployment. One of the questions raised in the MS [11] was about the existence of Maturity Models, in which 11 were found. However, only two were validated by companies, and one of them is applied only in IBM solutions. The roles of the actors involved in the process were found in 02 studies, with different names, but performing the same function. In a second phase, a Multi-Vocal Literature Review will be carried out to look for relevant information on emerging industry topics, which were not achieved by using SM. This type of Review has both academic publications and gray literature as input, its main objective being to close the gap between academic research and professional practice [12]. The methodology adopted for practical research will be Design Science Research (DSR), which is a method that establishes and operationalizes research when the desired objective is an artifact or a recommendation [13]. Given the above, there will be a need for an iterative cycle and the production of an artifact, which may be a set of systematic practices for adopting DevOps, which will be validated and improved throughout the research. The research will be applied in a Technology Agency of the State of the Brazilian Government, of which the author is Technical Director and has an Information Systems Coordination Unit with several applications under development using DevOps practices, and in the Brazilian company Neurotech, which develops advanced solutions of Artificial Intelligence, Machine Learning and Big Data, and has a portfolio of more than 100 customers. V. EVALUATION PROCEDURES I am starting my second year of doctorate and by its end, RQ1 and RQ2 will have been answered. The next step will be to propose a handbook that will systematize the introduction of DevOps, during which it will be applied in practice as previously mentioned in a State Technology Agency and Neurotech. The instrument for evaluating the results of the research proposed in the DevOps development process will be conducted by means of specific DevOps metrics, collected throughout the development process. Some studies have presented a set of metrics that can be used to assess the applicability of DevOps in the Software Production Process: Delivery ti",ICSE,2021.0,10.1109/ICSE-Companion52605.2021.00124,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
785d454eab4020c736fe96f91f8ff59800823308,https://www.semanticscholar.org/paper/785d454eab4020c736fe96f91f8ff59800823308,A Study on the Manifestation of Trust in Speech,"Research has shown that trust is an essential aspect of human-computer interaction directly determining the degree to which the person is willing to use a system. An automatic prediction of the level of trust that a user has on a certain system could be used to attempt to correct potential distrust by having the system take relevant actions like, for example, apologizing or explaining its decisions. In this work, we explore the feasibility of automatically detecting the level of trust that a user has on a virtual assistant (VA) based on their speech. Since, to our knowledge, no public databases were available to study the effect of trust in speech, we developed a novel protocol for collecting speech data from subjects induced to have different degrees of trust in the skills of a VA. The protocol consists of an interactive session where the subject is asked to respond to a series of factual questions with the help of a virtual assistant. In order to induce subjects to either trust or distrust the VA’s skills, they are first informed that the VA was previously rated by other users as being either good or bad; subsequently, the VA answers the Lara and Leonardo contributed equally to the paper. Lara focused on the design, implementation and deployment of the data collection protocol as well as data curation. Pablo and Lara worked on the analysis of the collected database. Leonardo worked on the machine learning approaches, experimentation and analysis of results. Silvina helped design the data collection protocol. Jazmin helped with data curation. Agust́ın and Luciana directed the work, with hands-on contributions in the design of the protocol, the code, the machine learning approaches and the experiments. Preprint submitted to Computer Science and Language February 19, 2021 ar X iv :2 10 2. 09 37 0v 1 [ cs .H C ] 9 F eb 2 02 1 subjects’ questions consistently to its alleged abilities. All interactions are speech-based, with subjects and VAs communicating verbally, which allows the recording of speech produced under different trust conditions. Using this protocol, we collected a speech corpus in Argentine Spanish. We show clear evidence that the protocol effectively succeeded in influencing subjects into the desired mental state of either trusting or distrusting the agent’s skills, and present results of a perceptual study of the degree of trust performed by expert listeners. Finally, we found that the subject’s speech can be used to detect which type of VA they were using, which could be considered a proxy for the user’s trust toward the VA’s abilities, with an accuracy up to 76%, compared to a random baseline of 50%. These results are obtained using features that have been previously found useful for detecting speech directed to infants and non-native speakers. The collected speech dataset is publicly available for research use.",ArXiv,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
ad1afa483378c4c8adb55823bc4ba8b0f6529534,https://www.semanticscholar.org/paper/ad1afa483378c4c8adb55823bc4ba8b0f6529534,MAGITS: A Mobile-based Information Sharing Framework for Integrating Intelligent Transport System in Agro-Goods e-Commerce in Developing Countries,"The technological advancement in Intelligent Transport Systems and mobile phones enable massive collaborating devices to collect, process, and share information to support the sales and transportation of agricultural goods (agrogoods) from farmer to market within the Agriculture Supply Chain. Mobile devices, especially smartphones and intelligent Point of Sale (PoS), provide multiple features such as Global Positioning System (GPS) and accelerometer to complement infrastructure requirements. Despite the opportunity, the development and deployment of the innovative platforms integrating Agro-goods transport services with e-commerce and e-payment systems are still challenging in developing countries. Some noted challenges include the high cost of infrastructure, implementation complexities, technology, and policy issues. Therefore, this paper proposes a framework for integrating ITS services in agro-goods e-commerce, taking advantage of mobile device functionalities and their massive usage in developing countries. The framework components identified and discussed are Stakeholders and roles, User Services, Mobile Operations, Computing environment with Machine Learning support, Service goals and Information view, and Enabling Factors. A Design Science Research (DSR) method is applied to produce a framework as an artifact using a six-step model. Also, a case study of potato sales and transportation from the Njombe region to Dar es Salaam city in Tanzania is presented. The framework constructs the ability to improve information quality shared among stakeholders; provide a cost-effective and efficient approach for buying, selling, payment, and transportation of Agriculture goods. Keywords—Intelligent transport system; stakeholders; mobile phone; agro-goods; information sharing; agriculture supply chain; machine learning",,2021.0,10.14569/ijacsa.2021.0120684,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8881dadf57a7515039c978bf3755c336fbb417af,https://www.semanticscholar.org/paper/8881dadf57a7515039c978bf3755c336fbb417af,Customer Relationship Management Customer Satisfaction Ebooks Download,"This book introduces a fuzzy classification approach, which combines relational databases with fuzzy logic for more effective and powerful customer relationship management (CRM). It shows the benefits of a fuzzy classification in contrast to the traditional sharp evaluation of customers for the acquisition, retention and recovery of customers in online shops. The book starts with a presentation of the basic concepts, fuzzy set theory and the combination of relational databases and fuzzy classification. In its second part, it focuses on the customer perspective, detailing the central concepts of CRM, its theoretical constructs and aspects of analytical, operational and collaborative CRM. It juxtaposes fuzzy and sharp customer classes and shows the implications for customer positioning, mass customization, personalization, customer assessment and controlling. Finally, the book presents the application and implementation of the concepts in online shops. A detailed case study presents the application and a separate chapter introduces the fuzzy Classification Query Language (fCQL) toolkit for implementing these concepts. In its appendix the book lists the fuzzy set operators and the query language’s grammar. This work offers a state-of-the art survey of information systems research on electronic customer relationship management (eCRM). It provides important new frameworks derived from current cases and applications in this emerging field. Each chapter takes a collaborative approach to eCRM that goes beyond the analytical and operational perspectives most often taken by researchers in the field. Chapters also stress integration with other enterprise information systems. The book is organized in four parts: Part I presents an overview of the role of CRM and eCRM in marketing and supply chain management; Part II focuses on the organizational success factors behind eCRM implementation; Part III presents cases of eCRM performance enhancement; and Part IV addresses eCRM issues in business-to-consumer commerce. Continuous improvements in digitized practices have created opportunities for businesses to develop more streamlined processes. This not only leads to higher success in day-today production, but it increases the overall success of businesses. Enterprise Information Systems and the Digitalization of Business Functions is a key resource on the latest advances and research for a digital agenda in the business world. Highlighting multidisciplinary studies on data modeling, information systems, and customer relationship management, this publication is an ideal reference source for professionals, researchers, managers, consultants, and university students interested in emerging developments for business process management. Research Paper (postgraduate) from the year 2019 in the subject Business economics Customer Relationship Management, CRM, grade: 1.5, Kwame Nkrumah University of Science and Technology, language: English, abstract: Customer Relationship Management (CRM) practices are business strategies designed to reduce costs and increase profitability by solidifying customer loyalty. With intense competition among insurance companies in Ghana, this study sought to assess Customer Relationship Management practices and Customer Retention in NSIA Insurance. The study was conducted to identify critical factors necessary for customer retention in carrying out customer relationship management practices in the selected insurance company and to develop effective customer relationship management practices to manage customer retention for sustainability within the insurance industry using NSIA Insurance as a case study. Well structured questionnaires and face-to-face interview were the methods adopted for the investigation of the study. A sample size of 40 respondents was considered, they were made up of customers and the staff who are fully involved in customer relationship management of the insurance company. Data collected from the completed questionnaires and the interviews were grouped into frequency tables and expressed in percentages. The researcher relied on the SPSS in interpreting the collected data. The study shows that even though NSIA insurance has policies on customer relationship management practices, these policies are not carried out fully to accomplish the ultimate goal of customer retention. The study recommends that for the insurance company to command an adequate number of loyal customers, NSIA Insurance should consistently improve on its quality of service to address the preference of the customers and consider the five service quality constructs of reliability, assurance, tangibility, empathy and responsiveness. The two-volume set CCIS 143 and CCIS 144 constitutes the refereed proceedings of the International Conference on Electronic Commerce, Web Application, and Communication, ECWAC 2011, held in Guangzhou, China, in April 2011. The 148 revised full papers presented in both volumes were carefully reviewed and selected from a large number of submissions. Providing a forum for engineers, scientists, researchers in electronic commerce, Web application, and communication fields, the conference will put special focus also on aspects such as e-business, e-learning, and e-security, intelligent information applications, database and system security, image and video signal processing, pattern recognition, information science, industrial automation, process control, user/machine systems, security, integrity, and protection, as well as mobile and multimedia communications. Customer Relationship Management, Fourth Edition, is a much-anticipated update of a bestselling textbook, including substantial revisions to bring its coverage up to date with the very latest in CRM practice. The book introduces the concept of CRM, explains its benefits, how and why it can be used, the technologies that are deployed, and how to implement it, providing you with a guide to every aspect of CRM in your business or your studies. Both theoretically sound and managerially relevant, the book draws on academic and independent research from a wide range of disciplines including IS, HR, project management, finance, strategy and more. Buttle and Maklan, clearly and without jargon, explain how CRM can be used throughout the customer life cycle stages of customer acquisition, retention and development. The book is illustrated liberally with screenshots from CRM software applications and case illustrations of CRM in practice. New to this Edition: Updated instructor support materials online Full colour interior Brand",,2021.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a3bbdc81b8438f4339a46f343aac30bfb6ec6128,https://www.semanticscholar.org/paper/a3bbdc81b8438f4339a46f343aac30bfb6ec6128,FABRICATION OF SELF-POWERED DEVICES AND THE APPLICATION IN HUMAN-COMPUTER INTERFACE,"The development of computation resources, sensing technologies
and artificial intelligence has been the critical driving force for the development
of next-generation smart devices in this Internet of Things (IoT) era. The
requirement of sustainable and reliable sensing operation is becoming more and
more important due to the exponential increment in the number of sensors with
different functions deployed everywhere. Such a sensing network provides a more
convenient way for connection and communication, making the boundary among
humans, machines, and the external environment gradually blurred. Self-powered
sensors which can directly utilize the external input energy to drive the
operation of their own are highly desirable compared with the current sensing
technology with a limited life cycle caused by power shortage. The invention of
Triboelectric Nanogenerators (TENG) and Piezoelectric Nanogenerators (PENG) provides
a novel direction for self-powered sensing technology. Both TENG and PENG can
directly convert mechanical energy to an electrical signal, which could be
further processed and understood by computers and humans. 

In this dissertation, the research efforts have led to the design
and fabrication of self-powered devices as well as system integration in the Human-Computer
Interface (HCI). Materials modification was carried out to boost the performance
of TENG output. A Piezotronic device using new semiconductor material,
Tellurium, was fabricated and its fundamental charge transport characteristics
were carefully studied for a new understanding in piezotronics. Beyond materials
science, a system-level demonstration of using TENG for HCI application was
also carried out. With the help of artificial intelligence technology such as
machine learning and deep learning, more in-depth information was successfully
extracted from the general TENG signal. The combination between Finite Element
Analysis (FEA) and deep learning provided a more powerful platform for the
development and verification of TENG-based sensing devices with improved working
reliability. The presented concepts and results in this dissertation show the
potential for the implementation of novel self-powered sensing technology in
the future development for smart sensors, virtual/augmented reality (VR/AR) and
other HCI-related areas.",,2021.0,10.25394/PGS.15016119.V1,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
98c11801766a3475eb3eecb0a26390b592b731b0,https://www.semanticscholar.org/paper/98c11801766a3475eb3eecb0a26390b592b731b0,Spatially Explicit Capture-Recapture Through Camera Trapping: A Review of Benchmark Analyses for Wildlife Density Estimation,"Camera traps have become an important research tool for both conservation biologists and wildlife managers. Recent advances in spatially explicit capture-recapture (SECR) methods have increasingly put camera traps at the forefront of population monitoring programs. These methods allow for benchmark analysis of species density without the need for invasive fieldwork techniques. We conducted a review of SECR studies using camera traps to summarize the current focus of these investigations, as well as provide recommendations for future studies and identify areas in need of future investigation. Our analysis shows a strong bias in species preference, with a large proportion of studies focusing on large felids, many of which provide the only baseline estimates of population density for these species. Furthermore, we found that a majority of studies produced density estimates that may not be precise enough for long-term population monitoring. We recommend simulation and power analysis be conducted before initiating any particular study design and provide examples using readily available software. Furthermore, we show that precision can be increased by including a larger study area that will subsequently increase the number of individuals photo-captured. As many current studies lack the resources or manpower to accomplish such an increase in effort, we recommend that researchers incorporate new technologies such as machine-learning, web-based data entry, and online deployment management into their study design. We also cautiously recommend the potential of citizen science to help address these study design concerns. In addition, modifications in SECR model development to include species that have only a subset of individuals available for individual identification (often called mark-resight models), can extend the process of explicit density estimation through camera trapping to species not individually identifiable.",Frontiers in Ecology and Evolution,2020.0,10.3389/fevo.2020.563477,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
17af969fdf80f2e8d426984a04aeae659bee1dc5,https://www.semanticscholar.org/paper/17af969fdf80f2e8d426984a04aeae659bee1dc5,Four equity considerations for the use of artificial intelligence in public health,"New technologies can either improve or worsen health inequities.1 Innovative technologies involving artificial intelligence are no exception, particularly where they are adopted and implemented in health systems. Indeed, determining whether and how artificial intelligence might contribute to reducing or exacerbating health inequities has been identified as a priority research area by several stakeholders and by numerous ethics and policy guidance documents.2–4 Understanding the connection between health inequities and artificial intelligence should be a priority when deploying these technologies in public health. Since public health activities typically target populations instead of individuals and require collective action instead of individual intervention,5 introducing artificial intelligence technologies to support these activities may influence (either positively or negatively, intentionally or unintentionally) health inequities more than in other areas. As such, identifying the distinctive equity considerations and dimensions that might emerge in the public health context is critical. However, doing so is not a straightforward task. First, we cannot simply look to past technological innovations to determine which health equity considerations or implications might arise with the use of artificial intelligence in public health because technological innovations and their diffusion in health systems each produce or interact with health inequities in novel ways.1 We may not be able to assume that the trends or pathways that create or prevent inequities will be the same when implementing artificial intelligence technologies as they are with other technological innovations. This limitation may be particularly challenging with artificial intelligence technologies given their use of big data and machine learning. Second, artificial intelligence represents a vast and sometimes contested area of study and application. Here we define artificial intelligence as a branch of computer science that explores the ability of computers to imitate aspects of intelligent human behaviour, such as problem-solving, reasoning and recognition.2 Technologies that are supported by artificial intelligence are therefore numerous, and include natural language processing, object recognition and reinforcement learning, among others. The ways in which these technologies might be deployed in public health are equally numerous, including digital disease surveillance, machine learning to predict incidences of noncommunicable diseases, and others. Finally, given that health inequities are often defined as differences in health that are unjust, even what should be counted as health inequities and what it means to achieve health equity may differ according to the nature of the new technology, how it is or has been integrated into health systems and our judgements about its interaction with the public’s health.6 As a result, before research or health system interventions in this area are developed or implemented, we should first seek to conceptually map the unique ways in which inequities might manifest when artificial intelligence is implemented or used in public health. Indeed, important work examining the unique equity dimensions associated with specific artificial intelligence technologies in this area has begun.7 Yet, we posit that there are general equity considerations and dimensions that can be identified and used as starting points for the reflection of equitable artificial intelligence in public health, and that it would be of benefit for the field to have these identified and enumerated. We will briefly describe four key equity considerations and dimensions and conclude by discussing how they can be used as starting points to further understand and enhance the equitable deployment of artificial intelligence in public health.",Bulletin of the World Health Organization,2020.0,10.2471/blt.19.237503,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1461f93c93e7122063a1185d48f1ef3a9e35d40e,https://www.semanticscholar.org/paper/1461f93c93e7122063a1185d48f1ef3a9e35d40e,Collective Knowledge: organizing research projects as a database of reusable components and portable workflows with common APIs,"This article provides the motivation and overview of the Collective Knowledge framework (CK or cKnowledge). The CK concept is to decompose research projects into reusable components that encapsulate research artifacts and provide unified application programming interfaces (APIs), command-line interfaces (CLIs), meta descriptions and common automation actions for related artifacts. The CK framework is used to organize and manage research projects as a database of such components. 
Inspired by the USB ""plug and play"" approach for hardware, CK also helps to assemble portable workflows that can automatically plug in compatible components from different users and vendors (models, datasets, frameworks, compilers, tools). Such workflows can build and run algorithms on different platforms and environments in a unified way using the universal CK program pipeline with software detection plugins and the automatic installation of missing packages. 
This article presents a number of industrial projects in which the modular CK approach was successfully validated in order to automate benchmarking, auto-tuning and co-design of efficient software and hardware for machine learning (ML) and artificial intelligence (AI) in terms of speed, accuracy, energy, size and various costs. The CK framework also helped to automate the artifact evaluation process at several computer science conferences as well as to make it easier to reproduce, compare and reuse research techniques from published papers, deploy them in production, and automatically adapt them to continuously changing datasets, models and systems. 
The long-term goal is to accelerate innovation by connecting researchers and practitioners to share and reuse all their knowledge, best practices, artifacts, workflows and experimental results in a common, portable and reproducible format at this https URL .",ArXiv,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
728742d1886a476b3208619953fe814fa3170184,https://www.semanticscholar.org/paper/728742d1886a476b3208619953fe814fa3170184,"Centering disability perspectives in algorithmic fairness, accountability, & transparency","It is vital to consider the unique risks and impacts of algorithmic decision-making for people with disabilities. The diverse nature of potential disabilities poses unique challenges for approaches to fairness, accountability, and transparency. Many disabled people choose not to disclose their disabilities, making auditing and accountability tools particularly hard to design and operate. Further, the variety inherent in disability poses challenges for collecting representative training data in any quantity sufficient to better train more inclusive and accountable algorithms. This panel highlights areas of concern, present emerging research efforts, and enlist more researchers and advocates to study the potential impacts of algorithmic decision-making on people with disabilities. A key objective is to surface new research projects and collaborations, including by integrating a critical disability perspective into existing research and advocacy efforts focused on identifying sources of bias and advancing equity. In the technology space, discussion topics will include methods to assess the fairness of current AI systems, and strategies to develop new systems and bias mitigation approaches that ensure fairness for people with disabilities. For example, how do today's currently-deployed AI systems impact people with disabilities? If developing inclusive datasets is part of the solution, how can researchers ethically gather such data, and what risks might centralizing data about disability pose? What new privacy solutions must developers create to reduce the risk of deductive disclosure of identities of people with disabilities in ""anonymized"" datasets? How can AI models and bias mitigation techniques be developed that handle the unique challenges of disability, i.e., the ""long tail"" and low incidence of many types of disability - for instance, how do we ensure that data about disability are not treated as outliers? What are the pros and cons of developing custom/personalized AI models for people with disabilities versus ensuring that general models are inclusive? In the law and policy space, the framework for people with disabilities requires specific study. For example, the Americans with Disabilities Act (ADA) requires employers to adopt ""reasonable accommodations"" for qualified individuals with a disability. But what is a ""reasonable accommodation"" in the context of machine learning and AI? How will the ADA's unique standards interact with case law and scholarship about algorithmic bias against other protected groups? When the ADA governs what questions employers can ask about a candidate's disability, and HIPAA and the Genetic Information Privacy Act regulate the sharing of health information, how should we think about inferences from data that approximate such questions? Panelists will bring varied perspectives to this conversation, including backgrounds in computer science, disability studies, legal studies, and activism. In addition to their scholarly expertise, several panelists have direct lived experience with disability. The session format will consist of brief position statements from each panelist, followed by questions from the moderator, and then open questions from and discussion with the audience.",FAT*,2020.0,10.1145/3351095.3375686,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
211f274fcbaddf251c6bedf9af96390c2f2bd7a7,https://www.semanticscholar.org/paper/211f274fcbaddf251c6bedf9af96390c2f2bd7a7,Design and exploration of semiconductors from first principles: A review of recent advances,"Recent first-principles approaches to semiconductors are reviewed, with an emphasis on theoretical insight into emerging materials and in silico exploration of as-yet-unreported materials. As relevant theory and methodologies have developed, along with computer performance, it is now feasible to predict a variety of material properties ab initio at the practical level of accuracy required for detailed understanding and elaborate design of semiconductors; these material properties include (i) fundamental bulk properties such as band gaps, effective masses, dielectric constants, and optical absorption coefficients; (ii) the properties of point defects, including native defects, residual impurities, and dopants, such as donor, acceptor, and deep-trap levels, and formation energies, which determine the carrier type and density; and (iii) absolute and relative band positions, including ionization potentials and electron affinities at semiconductor surfaces, band offsets at heterointerfaces between dissimilar semiconductors, and Schottky barrier heights at metal–semiconductor interfaces, which are often discussed systematically using band alignment or lineup diagrams. These predictions from first principles have made it possible to elucidate the characteristics of semiconductors used in industry, including group III–V compounds such as GaN, GaP, and GaAs and their alloys with related Al and In compounds; amorphous oxides, represented by In–Ga–Zn–O; transparent conductive oxides (TCOs), represented by In2O3, SnO2, and ZnO; and photovoltaic absorber and buffer layer materials such as CdTe and CdS among group II–VI compounds and chalcopyrite CuInSe2, CuGaSe2, and CuIn1−xGaxSe2 (CIGS) alloys, in addition to the prototypical elemental semiconductors Si and Ge. Semiconductors attracting renewed or emerging interest have also been investigated, for instance, divalent tin compounds, including SnO and SnS; wurtzite-derived ternary compounds such as ZnSnN2 and CuGaO2; perovskite oxides such as SrTiO3 and BaSnO3; and organic–inorganic hybrid perovskites, represented by CH3NH3PbI3. Moreover, the deployment of first-principles calculations allows us to predict the crystal structure, stability, and properties of as-yet-unreported materials. Promising materials have been explored via high-throughput screening within either publicly available computational databases or unexplored composition and structure space. Reported examples include the identification of nitride semiconductors, TCOs, solar cell photoabsorber materials, and photocatalysts, some of which have been experimentally verified. Machine learning in combination with first-principles calculations has emerged recently as a technique to accelerate and enhance in silico screening. A blend of computation and experimentation with data science toward the development of materials is often referred to as materials informatics and is currently attracting growing interest.",Applied Physics Express,2018.0,10.7567/APEX.11.060101,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
aabfdea7a2f6c8aef71fa044f9475a4e79e97236,https://www.semanticscholar.org/paper/aabfdea7a2f6c8aef71fa044f9475a4e79e97236,"Managing Tasks Across the Work-Life Boundary: Opportunities, Challenges, and Directions","ABSTRACT With the global shift towards remote work, understanding how people maintain their desired boundary has become critically relevant to HCI research at large. In this paper, we examine how people employed task management tools across the work-life boundary before the emergence of COVID-19. We report findings from a survey deployed to 150 information workers during Summer 2019 that inquired about task management tool usage, contextual task management practice, and preferences for separating work and nonwork. We first characterize and identify trends across tool use, job role, and task management practice. We find that the majority of task management activity occurs during work hours, and that information workers regularly managing work tasks beyond work hours and vice versa. We use the findings to inform new research questions that are pertinent to managing work-life boundaries in the context of the pandemic, its resulting stay-at-home orders, and more broadly, in the new future of work. CCS CONCEPTS • Human-centered computing → Human computer interaction (HCI); Empirical studies in HCI. KEYWORDS task management practices, work-life boundary, online survey. ABOUT THE AUTHOR/S Alex C. Williams University of Tennessee acw@utk.edu Alex C. Williams is a postdoctoral researcher in the School of Information and Computer Sciences at the University of California, Irvine. His research intersects the areas of human-computer interaction, machine learning, cognitive science, and workplace studies. He holds a PhD in Computer Science from the University of Waterloo. His dissertation research focused on designing and studying new systems for aiding people in the work-related transitions experienced in daily life. (https://acw.io) Shamsi Iqbal Microsoft Research shamsi@microsoft.com Shamsi Iqbal is a Principal Researcher in the Information and Data Sciences group at Microsoft Research AI, Redmond. Her primary research expertise is in the area of Attention Management for Multitasking Domains. Her work is motivated by the vision of transforming the field of productivity research in response to the changing technology landscape with an eye towards making people happy and satisfied with the process and the outcome. Currently she is focusing on how productivity is defined in the new era of multitasking and distraction, introducing novel ways of being productive and determining metrics for evaluating productivity. More specifically, she develops experiences and technology that helps people maintain focus when needed, but at the same time introduce new concepts of getting things done in limited focus environments. Julia Kiseleva Microsoft Research julia.kiseleva@microsoft.com Ryen White Microsoft Research ryenw@microsoft.com Ryen White is a Partner Research Manager at Microsoft Research AI, where he leads a large team of world-class scientists and engineers researching and developing intelligent experiences. In recent roles, Ryen led the applied science organization for Cortana and he served as chief scientist for Microsoft Health. Ryen’s research has focused on understanding search interaction and on developing tools to help people search more effectively. New Future of Work 2020, August 3–5, 2020 © 2020 Copyright held by the owner/author(s).",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
253a9d34eeac18be83bcfbd649d17194f63b4334,https://www.semanticscholar.org/paper/253a9d34eeac18be83bcfbd649d17194f63b4334,Using Self-consistency to Determine Uncertainty in Particle Accelerator Diagnostic Measurements,"Control of a particle accelerator relies on interpreting input diagnostics, and tuning the accelerator settings to achieve desired diagnostic values. Particle accelerator diagnostics can fail, slowly or abruptly, and currently a human operator is tasked with evaluating this. For truly autonomous operation of particle accelerators, we must be able to determine the reliability of a diagnostic using nothing more than the set of all diagnostic data. We present a method of quantifying the scale of error in diagnostic measurements based on gaussian process (GP) regression and the intuition that each diagnostic measurement from a beam position monitor (BPM) should be predictable given the set of all other BPM measurements. 1 Autonomous Accelerator Operation Particle accelerator operation currently centers on a team of skilled human operators and some automated routines for optimizing the accelerator’s operation against a set of performance metrics. Accelerators produce a large amount of data, that data is of variable reliability, and the optimization is typically multi-objective and spans a high-dimensional configuration space. This makes particle accelerators an ideal candidate for applying machine learning techniques [2]. Autonomous control is one of the key areas for progress for machine learning applications to particle accelerators [3]. There has been much progress in applying machine learning based optimization to accelerator controls. Scheinker et al. [7] demonstrated the ability for model-independent control to adapt an accelerator to deliver a certain longitudinal phase space in a free-electron laser linac based only on a handful of control knobs and a diagnostic read-out. Duris et al. [1] have demonstrated that Bayesian optimization with Gaussian process models can reach better optima faster than simplex models in computer simulations of free electron lasers. Other recent work by Scheinker et al. [8] demonstrated that a model-independent optimization algorithm can improve the average pulse energy in the LCLS SASE FEL by automatically tuningsettings with no foreknowledge of free electron laser physics. These applications all show the capability of machine learning techniques to tune an accelerator to a desired operating point. However, they all involve human-in-the-loop operation – there are still operators in the control room. For industrial applications such as water treatment – where there might be two operators for ten accelerators, instead of ten operators for one – or for accelerators deployed for space applications – where there would be no operators present – the control systems must be truly autonomous. This means that the control system must be able to evaluate the reliability of each diagnostic measurement using only the set of all diagnostic measurements. Particle accelerator diagnostics can fail for numerous reasons, and either catastrophically or slowly, and any autonomous accelerator control system must be able to detect these and distinguish them. In this paper we present a concept for determining the reliability of a set of beam position monitor (BPM) measurements using diagnostic self-consistency. In an ideal system with no errors, the BPM Third Workshop on Machine Learning and the Physical Sciences (NeurIPS 2020), Vancouver, Canada. measurements should be highly correlated. If the measurements have some noise on top of them, then a predictive model that captures uncertainty should have the scale of that uncertainty be proportional to the level of noise on the measurements. We can therefore use the predictive uncertainty of a model fit to the BPM data to determine the scale of that uncertainty with no outside input. 2 Diagnostic Self-Consistency Consider a set of diagnostic measurements {Xi}. In many cases, there are physical reasons to believe that these measurements are correlated. For our case, where we analyze BPMs, beam dynamics suggests that the BPM measurements are very strongly correlated due to the existence of an underlying transfer map that relates the beam phase space coordinates at each BPM to each other. Given enough BPMs, we should be able to reconstruct that map in a model that allows us to predict the value measured at one BPM given all the values at the other BPMs. We refer to this as the existence of diagnostic self-consistency. Formally, we expect that for each Xα measurement, given the set of inputs {Xi 6=α}, there should be a function Xα = fα({Xi 6=α}) in an ideal case with perfect diagnostic reliability for each i. In the case that fα exists and is perfectly accurate for each α then we have diagnostic self-consistency. In reality, the diagnostics will have some noise in their readings, due to electronic noise, resolution limits, etc. Thus, each Xα will have some best fit model fα which has some uncertainty in the prediction. By quantifying that uncertainty, we can extract the uncertainty in each diagnostic measurement and ascertain the variance in a diagnostic’s measurement using only the set of all diagnostic measurements. Because we are fitting the relation between ∼ 10s of diagnostics, and need to have a variance in those predictions, we will use gaussian process (GP) regression [6] to compute the correlation. GP regression has already been used effectively for optimization [1] as well as in building physics-based models for improved optimization [5]. 3 The ATR Beamline: A Test Case To generate test data, we use MAD-X simulations of the ATR beamline at Brookhaven National Laboratory, fig. (1). The ATR beamline is used to extract beam from the Alternating Gradient Synchrotron and transport it to the Relativistic Heavy Ion Collider. This beamline has a number of vertical and horizontal bends, and a number of correctors and BPMs along the line. Figure 1: Lattice layout and Twiss parameters for the ATR beamline. For what follows, we generated a dataset of 500 different beams with randomly offset initial positions and angles to the ideal axis. To compute the fα for each BPM, we train the GP models using a training set of 400 randomly selected beams, with the additional 100 held out as a test set, which we will use to infer the error in the models.",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0878f21553a04ae10ed1a3e55ddf4091e6238df2,https://www.semanticscholar.org/paper/0878f21553a04ae10ed1a3e55ddf4091e6238df2,ICIN 2020 Program,"For nearly fifty years, beginning with Kleinrock's pioneering work on using queueing theory to model packet flows in communication networks, network modeling has adopted the individual packet as primary level of granularity for network modeling and analysis. With the advent of terabit-switching capabilities, information-centric networking, and data centers with complex workloads and hundreds of thousands of components, the time would seem ripe to raise the level of abstraction beyond the packet. In this talk, we identify higher-level modeling abstractions are already proving useful as well as new needed abstractions. But also we identify cases where packet-level models are still crucial in providing important insights. Wednesday, February 26 10:00 11:00 K2: Keynote 2 Softwarization and IoT evolution Lefteris Mamatas (University of Macedonia, Greece) Room: La Grande Scène Chair: Alex Galis (University College London (UCL), United Kingdom (Great Britain)) Abstract: The Internet of Things (IoT), a main enabler for Industry 4.0, is considered as a system connecting myriads of people, things and services. IoT enables new large-scale applications with diverse constraints (e.g., limited resource availability or mobility) and requirements (e.g., ultra low delays). A main challenge is the evolution beyond large networks of sensing devices to multiple cooperating network deployments that implement context-sensitive communication and cloud processing strategies, through the seamless adoption of Softwarization technologies. The talk includes the following aspects: (i) a motivation of the above vision with two novel use-cases on smart-city and maritime contexts; (ii) a discussion on the evolutionary and clean-slate approaches to the IoT Softwarization; (iii) the missing elements and open issues in Software-Defined IoT and Edge Cloud technologies; and (iv) insights from our practical experience in relevant implementations and real experiments. The Internet of Things (IoT), a main enabler for Industry 4.0, is considered as a system connecting myriads of people, things and services. IoT enables new large-scale applications with diverse constraints (e.g., limited resource availability or mobility) and requirements (e.g., ultra low delays). A main challenge is the evolution beyond large networks of sensing devices to multiple cooperating network deployments that implement context-sensitive communication and cloud processing strategies, through the seamless adoption of Softwarization technologies. The talk includes the following aspects: (i) a motivation of the above vision with two novel use-cases on smart-city and maritime contexts; (ii) a discussion on the evolutionary and clean-slate approaches to the IoT Softwarization; (iii) the missing elements and open issues in Software-Defined IoT and Edge Cloud technologies; and (iv) insights from our practical experience in relevant implementations and real experiments. Wednesday, February 26 11:30 12:30 TS3: Network Slicing Room: La Grande Scène Chair: Prosper Chemouil (Orange Labs (retired), France) TS3.1 A Lightweight Policy-aware Broker for Multi-domain Network Slice Composition Xuan-Thuy Dang (Technische Universität Berlin & DAI Labor, Germany); Fikret Sivrikaya (GT-ARC gGmbH & Technische Universität Berlin, Germany) TS3.2 Enhancing the performance of 5G slicing operations via multi-tier orchestration Miquel Puig Mena (i2cat Foundation, Spain); Apostolos Papageorgiou, Leonardo Ochoa-Aday and Muhammad Shuaib Siddiqui (Fundació i2CAT, Internet i Innovació Digital a Catalunya, Spain); Gabriele Baldoni (ADLINK Technology, France) TS3.3 An Efficient Online Heuristic for Mobile Network Slice Embedding Katja Ludwig (University of Augsburg, Germany); Andrea Fendt (Nokia Bell Labs & University of Augsburg, Germany); Bernhard Bauer (University of Augsburg, Germany) Wednesday, February 26 12:30 13:00 DPS: Demo/Poster ""Elevator Pitch"" Session Room: La Grande Scène Chair: Prosper Chemouil (Orange Labs (retired), France) DPS.1 A QUIC-based proxy architecture for an efficient hybrid backhaul transport Michele Luglio and Mattia Quadrini (University of Rome Tor Vergata Dip. Ing. Elettronica, Italy); Cesare Roseti and Francesco Zampognaro (University of Rome Tor Vergata, Italy); Simon Pietro Romano (University of Napoli Federico II, Italy) DPS.2 A Blockchain-based Brokerage Platform for Fog Computing Resource Federation Marco Savi, Daniele Santoro, Katarzyna Di Meo and Daniele Pizzolli (Fondazione Bruno Kessler, Italy); Miguel Pincheira (OpenIoT Research Area, FBK CREATE-NET & University of Trento, Italy); Raffaele Giaffreda (FBK CREATE-NET, Italy); Silvio Cretti (Fondazione Bruno Kessler, Italy); Seung-woo Kum (Korea Electronics Technology Institute, Korea (South)); Domenico Siracusa (Fondazione Bruno Kessler, Italy) DPS.3 Optimized Network Slicing Proof-of-Concept with Interactive Gaming Use Case José J Alves Esteves, Jr. (Orange Labs & Sorbonne Université, France); Amina Boubendir and Fabrice M. Guillemin (Orange Labs, France); Pierre Sens (Université de Paris 6, France) DPS.4 A Deployable Containerized 5G Core Solution for Time Critical Communication in Smart Grid Van Giang Nguyen, Karl-Johan Grinnemo, Javid Taheri and Anna Brunstrom (Karlstad University, Sweden) DPS.5 FogGuru: a Fog Computing platform based on Apache Flink Davaadorj Battulga (University of Rennes 1 & U-Hopper, Italy); Daniele Miorandi (U-Hopper, Italy); Cedric Tedeschi (University of Rennes I / INRIA, France) DPS.6 5G Experimentation Framework: Architecture Specifications, Design and Deployment Louiza Yala (Orange Labs, France); Sihem Cherrared (University of Rennes 1 & Orange Labs and INRIA, France); Grzegorz Panek (Orange Polska, Poland); Sofiane Imadali and Ayoub Bousselmi (Orange Labs, France) DPS.7 A New Service Management Framework for Vehicular Networks Jose Ramirez, Onyekachukwu Augustine Ezenwigbo, Gayathri Karthick and Ramona Trestian (Middlesex University, United Kingdom (Great Britain)); Glenford E Mapp (MIddlesex University & Cantego Limited, United Kingdom (Great Britain)) DPS.8 Creating trust in automation in intent-based mobile network management Ville Vartiainen (Aalto University, Finland); Dmitry Petrov and Vilho Räisänen (Nokia Bell Labs, Finland) DPS.9 Interoperable and discrete eHealth Data Exchange between Hospital and Patient Andreea Ancuta Corici, Olaf Rode, Ben Kraufmann, Andreas Billig, Jörg Caumanns and Markus Deglmann (Fraunhofer FOKUS, Germany); Viktoria Walter, Janina Rexin and Gunther Nolte (Vivantes Netzwerk für Gesundheit GmbH, Germany) Wednesday, February 26 14:00 15:00 K3: Keynote 3 Network Operations and AI Rafia Inam (Ericsson, Sweden) Room: La Grande Scène Chair: Diego Lopez (Telefonica I+D, Spain) Abstract: The Fifth Generation Mobile Networks (5G) are seen as a key enabler for diverse-natured industry verticals (such as automotive, manufacturing, mining, utility, health, etc.) by providing a platform to support heterogeneous sets of network quality requirements. The presentation will discuss how Artificial Intelligence and automation can support Telecom industry to manage the increased complexity, scalability, and diversity in its use cases. The work presents different aspects of the network operations of the future, done in an automated, proactive, and intent-driven fashion using different AI techniques. The Fifth Generation Mobile Networks (5G) are seen as a key enabler for diverse-natured industry verticals (such as automotive, manufacturing, mining, utility, health, etc.) by providing a platform to support heterogeneous sets of network quality requirements. The presentation will discuss how Artificial Intelligence and automation can support Telecom industry to manage the increased complexity, scalability, and diversity in its use cases. The work presents different aspects of the network operations of the future, done in an automated, proactive, and intent-driven fashion using different AI techniques. Wednesday, February 26 15:00 16:10 TS4: Improving Service Performance Room: La Grande Scène Chair: Amina Boubendir (Orange Labs, France) TS4.1 Multimedia Service Management with Virtualized Cache Migration Reza Shokri Kalan (Ege UniversityTurkey, Turkey); Muge Sayit (Ege University, Turkey); Stuart Clayman (University College London (UCL), United Kingdom (Great Britain)) TS4.2 Proposal of Profile and Event Sharing by Agent Communication Masafumi Katoh (Fujitsu Labotatories Ltd., Japan); Tomonori Kubota and Eiji Yoshida (Fujitsu Laboratories, Japan); Yuji Kojima (Fujitsu Limited, Japan); Yuuichi Yamagishi (FUJITSU LIMITED, Japan) TS4.3 Double Mask: An Efficient Rule Encoding for Software Defined Networking Ahmad Abboud (University of Lorraine, France); Abdelkader Lahmadi (INRIA Nancy Grand Est, France); Michael Rusinowitch (INRIA Nancy-Grand Est, France); Miguel Couceiro (University of Lorraine, France); Adel Bouhoula (Higher School of Communication of Tunis & University of Carthage, Tunisia); Mondher Ayadi (Numeryx, France) Wednesday, February 26 16:35 18:00 TS5: Network Security Room: La Grande Scène Chair: Ved P. Kafle (National Institute of Information and Communications Technology, Japan) TS5.1 Neural network based anomaly detection for SCADA systems Lenhard Reuter (AIT Austrian Institute of Technology, Austria); Oliver Jung (AIT Austrian Institute of Technology GmbH, Austria); Julian Magin (AIT Austrian Institute of Technology, Austria) TS5.2 DDoS Detection System Using Feature Selection and Machine Learning Algorithms in a Distributed System Amjad Alsirhani (Dalhousie University, Faculty of Computer Science & Canada, Canada); Geetanshu Grover and Srinivas Sampalli (Dalhousie University, Canada); Peter Bodorik (Dalhousie University, Faculty of Computer Science, Canada) TS5.3 Configuration of the Detection Function in a Distributed IDS Using Game Theory Clement Weill (Institut Polytechnique de Paris & CEA LIST, France); Alexis Olivereau (CEA, LIST, France); Djamal Zeghlache (Insti","2020 23rd Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN)",2020.0,10.1109/icin48450.2020.9059372,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
485a6fae2f4669fe2f6a8fcb77d613beb6b47da9,https://www.semanticscholar.org/paper/485a6fae2f4669fe2f6a8fcb77d613beb6b47da9,Guest Editorial: Introduction to Special Section on Smart Systems and Intelligent Networking Powered With Big Data Analytics,"SMART systems, including Internet of Things (IoT), have emerged to address contemporary economic, societal, and environmental challenges, such as business and production automation, urban sustainability, climate change, healthcare, and globalization. They encompass different autonomous or collaborative systems with functions of sensing, actuation, and control for describing and analyzing a situation, and make decisions based on the available data in a predictive or adaptive manner. Intelligent networking enables these functions of smart systems by offering a global infrastructure for networked physical devices and everyday objects, which generates a gigantic amount of data, or big data. In addition, big data analytics is also employed in analyzing the big data so as to enable the networking to be intelligent and allow smart systems to perform astute, autonomous or collaborative actions. Nevertheless, the efficient and effective big data management and knowledge discovery of large-scale smart systems, big data analytics for intelligent networking, and networking technologies for big data (e.g., collection, processing, analysis and visualization) needmore explorations. The special section on “Smart Systems and Intelligent Networking Powered With Big Data Analytics” brings a timely research topic. The design of big data enabled smart systems and intelligent networks are still in a preliminary stage. Of the 39 submitted papers, 11 were selected for this issue. The selected articles cover topics including social network systems, cloud systems, network systems, blockchain systems, cyber-physical social systems, virtual network functions systems, smart IoT systems, disaster-resilient communication systems, smart grid systems, and indoor positioning systems. A brief review is provided as follows. In “Fair-Aware Competitive Event Influence Maximization in Social Networks,” Gao et al. address influential users’ selection for social network systems, and they propose a propagation model to describe the information propagation process and a randomized algorithm based on cross entropy. To solve the problem of resource restriction for cloud systems, Yuan et al. in “Minimizing Financial Cost of DDoS Attack Defense in Clouds with Fine-Grained ResourceManagement” study themainstream cloud pricing models and present a birth-death process-based mechanism that monitors the workloads of the customer systems, and adaptively increases the resources in the event of a DDoS attack. In “Decreasing Big Data Application Latency in Satellite Link by Caching and Peer Selection,” Jiang et al. design a social relation exploration method in 6G integrated space-air-ground network systems to reduce the link delay between the satellite and the satellite base station. In “Secure Lending: Blockchain and Prospect Theory-Based Decentralized Credit Scoring Model,” Hassija et al. propose a distributed credit score evaluation in blockchain systems to reduce the handful work in the current system. In “GAN-Driven Personalized Spatial-Temporal Private Data Sharing in Cyber-Physical Social Systems,” Qu et al. investigate the problemof privacy protection in cyber-physical social systems and propose a generative adversarial nets (GAN)-based personalized model to achieve differential privacy and thereby enhance spatial-temporal private data sharing. To investigate the problem of virtual network function deployment and flow scheduling in distributed data centers, Gu et al., in “Service Function Chain Deployment and Network Flow Scheduling in Geo-distributedData Centers,” combine server usagewith the communication cost and present a two-phased algorithmbyfirst balancing resource requirements and then selecting function locations. In “An Intelligent Dynamic Offloading from Cloud to Edge for Smart IoT Systems with Big Data,” Wang et al. address the dynamic offloading challenges for smart IoT systems and propose a dynamic switching algorithm to ensure tasks to be either offloaded on cloud or edge according to the system’s real-time conditions. To solve the problem for the traffic classification of anonymity tools, Bovenzi et al., in “A Big Data-Enabled Hierarchical Framework for Traffic Classification,” leverage machine learning to design a big data-enabled hierarchical framework, which seamless integrates data parallelism with model parallelism. In “Big Data on the Fly: UAV-MountedMobile Edge Computing for Disaster Management,” Xu et al. utilize unmanned aerial vehicles as big data carriers to realize emergency communication enabled by LoRaWAN (Long Range Wide Area Networking) for disaster management. In “A Differentially Private Big Data Nonparametric Bayesian Clustering Algorithm in Smart Grid,” Guan et al. propose a differentially private clustering algorithm based on the infinite gaussian mixture model to Digital Object Identifier 10.1109/TNSE.2020.3037986 2526 IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING, VOL. 7, NO. 4, OCTOBER-DECEMBER 2020",IEEE Trans. Netw. Sci. Eng.,2020.0,10.1109/tnse.2020.3037986,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7dac3510bc3391140aa51fd690bf012290dda4c7,https://www.semanticscholar.org/paper/7dac3510bc3391140aa51fd690bf012290dda4c7,"After postmodernism, technologism","Contemplating what comes after postmodernism we find ourselves like Walter Benjamin’s angel of history, inspired by Klee’s painting Angelus Novus. A violent storm blows from Paradise, irresistibly propelling the angel into the future to which his back is turned, while before him a pile of debris grows skyward; ‘This storm is what we call progress’ (Benjamin, 2003, p. 393). Likewise, the debris of past ‘-isms’ grows skywards before us, piled upon with new post-, trans-, and post-posts that take shape against the background of what has come before them. If Benjamin’s angel is caught by the storm mid-air, we are caught afloat on a tidal surge of technological progress. Interface technologies that enable fast-forwarding and rewinding, skipping, surfing channels and browsing the internet, bring about the collapse of narrative at a prosaic, pervasive and almost imperceptible level. Digital technology is fast becoming our cultural landscape, altering human experience and consequently changing zeitgeist and weltanschauung. Digital technology has begun to affect people’s conception of themselves in relation to others and institutions (Rushkoff, 2013). New intimacies with machines are creating what Turkle (2011) calls ‘tethered’ selves, subjects wired into social existence through technology. Our back is turned to the future, our gaze is fixed on the past, but under our feet technologism swells. Technologism denotes belief in the power of technology to improve society and human lives. As deployed here, it implies also technology-driven practices and contingent circumstances. Institutional technologism imposes new modes of teaching and learning. Willingly or dragged kicking and screaming, educators are coerced into ‘tethered’ teaching. Students are inculcated into ‘tethered’ learning. We become epistemic subjects wired into flipped classrooms, blended learning, and virtual learning environments replete with podcasts, lecture captures, sidebars, and top menus. Students can dip and delve, skip and surf the syllabus. Technologism in education prompts de facto postnarrative curricula and modes of delivery bereft of Socratic midwifery, with implications for the future of knowledge. Theory, says Eagleton (2016) apropos literature, is the point at which a practice is ‘forced into a new form of self-reflectiveness, taking itself as an object of its own inquiry.’ The term technologism may be widened to include a scholarly stance that is fired by the impact of technologies, and underpins fields of inquiry such as science and technology studies, postphenomenology, and posthumanism. Further to paraphrase Eagleton’s characterization of theory, such a stance may enter philosophy of education as a ‘systematic reflection on the assumptions, procedures, and conventions’ that govern technology-driven educational practice. We theorize what comes after postmodernism. But if we glance down at the tidal surge carrying us, we may ponder what is the future of critical inquiry itself—of systematic self-reflection that by necessity articulates itself in narrative—when digital natives, having grown into postnarrativity, become the next generation of scholars and educators.",What Comes After Postmodernism in Educational Theory?,2018.0,10.1080/00131857.2018.1461398,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a93e75b9cdcff21f0cc804d33271b6fff38c7a22,https://www.semanticscholar.org/paper/a93e75b9cdcff21f0cc804d33271b6fff38c7a22,Digitizing the Pharma Neurons - A Technological Operation in Progress!,"BACKGROUND
Digitization and automation is the buzzword in clinical research and pharma companies are investigating heavily here. Right from drug discovery to personalized medicine, digital patients and patient engagement, there is great consideration of technology at each step.


METHODS
The published data and online information available is reviewed to give an overview of digitization in pharma, across the drug development cycle, industry collaborations and innovations. The regulatory guidelines, innovative collaborations across industry, academics and thought leadership are presented. Also included are some ideas, suggestions, way forwards while digitizing the pharma neurons, the regulatory stand, benefits and challenges.


RESULTS
The innovations range from discovering personalized medicine to conducting virtual clinical trials, and maximizing data collection from the real-world experience. To address the increasing demand for the real-world data and the needs of tech-savvy patients, the innovations are shaping up accordingly. Pharma companies are collaborating with academics and they are co-innovating the technology. E.g. Massachusetts Institute of Technology's program. This focuses on the modernization of clinical trials, strategic use of artificial intelligence and machine learning using real-world evidence, assess the risk-benefit ratio of deploying digital analytics in medicine, and proactively identifying the solutions.


CONCLUSIONS
With unfolding data on impact of science and technology amalgamation, we need a need shared mindset between data scientists and medical professionals to maximize the utility of enormous health and medical data. To tackle this efficiently, there is a need of cross-collaboration and education, and align with ethical and regulatory requirements. A perfect blend of industry, regulatory, and academia will ensure a successful digitization of pharma neurons.",Reviews on recent clinical trials,2020.0,10.2174/1574887115666200621183459,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c6833112f857c54b83d144d5c48cee6e4868dc22,https://www.semanticscholar.org/paper/c6833112f857c54b83d144d5c48cee6e4868dc22,Mobile Technology for Clinical Operations: Challenges and Opportunities,"Abstract 
On the ground best practices for the use of mobile technology within e-clinical with a focus on practical applications available now for clinical operations and data management. Hear real life implementations and learnings from the field. As mobile becomes more and more common, dialog must focus on actual experiences during the conduct of trials so that clinical teams can make informed decisions on the deployment, security, and usage of mobile applications at various stages of clinical development. 
 
In this discussion, the audience will hear real stories from clinical professionals and their on the ground experiences with mobile applications. The audience will walk away with a deeper understanding of the benefits and limitations of mobile applications in clinical operations and better enable them to adopt mobile applications with practical expectations. Attendees will be guided on the practical use of these technologies. They will learn to determine the use cases that are most relevant and applicable in Clinical Operations. 
 
 
 
Biography: 
 
Jay Smith is Head of Product for TransPerfect’s Trial Interactive E-Clinical platform. Jay brings 25 years of product management, consulting, and engineering experience across verticals such as life sciences, enterprise software, healthcare, government, entertainment and manufacturing. Previously, Jay has led product efforts for Medidata, Sparta Systems, Cureatr, VenueNext, Apogy, and Liquent. Jay holds an MBA from Villanova University and a degree in Computer Science and Physics from Gettysburg College. 
 
  
 
  
 
  
 
  
 
  
 
  
 
Speaker Publications: 
 
 
  “Analysis of Multiple Samples Using Multiplex Sample NMR: Selective Excitation and Chemical Shift Imaging Approaches”; Analytical Chemistry / 2001 / 73(11):2541-6 
 “Variable Temperature Study of the Cross-Relaxation Dynamics in the Hyperpolarized Xenon-Induced Enhancement of Surface Nuclei”; The Journal of Physical Chemistry B / 2001 / 105(7):1412-1421 
 “Cross-relaxation dynamics between laser-polarized xenon and surface species using a simple three-spin model”; Chemical Physics Letters / 2000 / 317(1-2):165-173 
 
 
  
 
10th International Conference on Clinical Research and Clinical Trials; Amsterdam, Netherlands- March 18-19, 2020. 
 
Abstract Citation: 
 
Practical ClinOps Applications of AI and Machine Learning – Real-World Use Cases, Euro Clinical Trials 2020, 10th International Conference on Clinical Research and Clinical Trials; Amsterdam, Netherlands- March 18-19, 2020 (https://clinicaltrials.pharmaceuticalconferences.com/abstract/2020/mobile-technology-for-clinical-operations-challenges-and-opportunities)",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
20831632f67745592fd1938fd0f15891191d10db,https://www.semanticscholar.org/paper/20831632f67745592fd1938fd0f15891191d10db,Enabling WITSML Stream Analytics Through Open Source Big Data Technologies,"
 This work presents a set of interconnected open source big data technologies through an example case to demonstrate the processes used to generate, process, store, and consume real-time wellsite information transfer standard markup language (WITSML) data from drilling and completions (D&C) operations. The new proposed approach to manage real-time data is based on the use of distributed storage and processing technologies to simultaneously analyze large volumes of information, previously considered only on an individual well basis. This new approach leverages the open source technology available in the market to target the gathering of the data, its secure database storage, and its future processing in a single workflow, to obtain the most value added from the oilfield-generated data during D&C operations. This work presents a set of big data tools and their application.
 For this case study, a dynamic, open, easily configurable, and fully scalable work environment was obtained. Open source big data technologies prove to be different when handling operations data, as compared to traditional technologies. Traditional technologies typically require several manual inputs and configurations to enter the data. Data models for these technologies are usually closed or proprietary, and the provided visualizations are limited either to single well analysis or to basic analytical dashboards. The technologies used for this case fully enable data input and storage, as well as allow the generation and real-time deployment of machine learning and data science algorithms and models.
 As a different approach, this paper introduces the use of big data architectures for the D&C business to obtain better results for gathering, storing, and analyzing real-time information than the business has traditionally achieved.",,2020.0,10.2118/198963-ms,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a94e9463b4ea30cb70b3287f4657fea641ff2d53,https://www.semanticscholar.org/paper/a94e9463b4ea30cb70b3287f4657fea641ff2d53,2. Deployable Predictive Maintenance Strategy Based on Models Developed to Monitor Assets in Nuclear Power Plants,"Identifying and Clustering Activity in Seismic Waveforms In this work we describe a data-driven approach for identifying and clustering activities, such as vehicle movement, in seismic waveforms. We leverage established methods for pre-processing raw seismic signals, such as power-spectral-density analysis, and for extracting discrete seismic events, such as enveloping. Using the extracted events, we explore two methods for clustering the events into groups that represent similar activities. The first approach uses wavelet transforms to identify key features of the waveform and then applies k-means clustering. The second method is novel in the seismic domain and bypasses the need for feature engineering by using the normalized compression distance between events to quantify event similarity. Preliminary results for both clustering methods on real sensor data are presented along with a discussion of the technical challenges and implications for each approach. We conclude with directions for further development. Joint work with Erick Draayer, Nicole McMahon, Dylan Anderson, David Stracuzzi, Sandia National Laboratories. Sandia National Laboratories is a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy’s National Nuclear Security Administration under contract DENA0003525. 27. Augmenting Expert Search for Sources of Bias in Nuclear Data Validation Benchmarks Using Machine Learning Mike Grosskopf, Los Alamos National Laboratory Estimates of cross sections for isotopes in nuclear reaction processes are critical for understanding and modeling nuclear physics in reactors and other scientific applications. These estimates are tested in simulation of validation benchmarks, which integrate many sets of nuclear data into one model of a complex experiment. Differences between the simulation results and experiment feedback into a search for ways to improve the nuclear data estimates. Because the experiments depend on a wide set of nuclear data in complicated, inter-dependent ways, the process is largely driven by expert judgement. Utilizing machine learning tools for prediction and interpretability to learn and communicate complex, high-dimensional relationships can augment this expert search. We present results using random forests to predict the bias in validation benchmarks and using model interpretability metrics for assessing importance in the high-dimensional, highly-correlated feature space. Joint work with Denise Neudecker, Michel Herman, Wim Haeck, Scott Vander Wiel, Mike Rising, Alex Clark, Pavel Grechanuk (Oregon State University). 28. Extreme values of physical processes and dynamical systems with random initialization Charlotte Haley, Argonne National Laboratory Extremes of wind speed can be unpredictable in the presence of complex orography. In this study we examine a model for vertical wind speed that accounts for the non-stationary diurnal fluctuations of wind speed due to convection and gravity wave impacts at the top of the boundary layer and consider hierarchical Bayesian approaches to modeling arbitrary level-crossings based on renewal theory. We additionally address a similar problem of computing extreme excursion probabilities in dynamical system arising from random initialization. These are of interest in power systems, design of critical infrastructure, etc. Accurately computing extreme excursion probabilities help us to balance costs and risks. We have developed novel computational algorithm that leverages Rice's formula to construct an importance biasing distribution that helps us compute the extreme excursion probabilities efficiently. Joint work with Vishwas Rao, Argonne National Laboratory. 29. SimILE (Simulated Implicit Likelihood Estimation): Bayesian Analyses Using Estimated Implicit Likelihoods with Discretized Simulated Data Michael Hamada, Los Alamos National Laboratory We present a Bayesian inferential method where the likelihood for a model is unknown (i.e., implicit) but where data can easily be simulated from the model. We discretize simulated (continuous) data to estimate the implicit likelihood in a Bayesian analysis employing a Markov chain Monte Carlo algorithm. Some examples are presented as well as a small study on some of the method's properties. CoDA 2020 Poster Abstracts | Day 2: Wednesday, February 26, 2020 -29LA-UR-2021392 http://cnls.lanl.gov/coda2020 30. Data Fusion, Modelling, and the Spread of Modern Humans Rachel J. A. Hopkins, University of Oxford (UK) & University of New Mexico (USA) The spread of Anatomically Modern Humans (AMHs) and the coinciding demise of the Neanderthals in Europe (50-40k years ago) has fascinated researchers since the dawn of archaeology and the investigation of human evolution. However, archaeological datasets are inherently messy and sampling is affected by ‘survival bias’. We will therefore demonstrate how data fusion, innovative Bayesian modelling, and Kernel Density Estimation (KDE) can drastically improve the quality of data interpretation by integrating qualitative and quantitative evidence. The resulting spatio-temporal models indicate that AMHs migrated into central Europe 5-7k years earlier than previously estimated. These findings change our understanding of the conditions under which Neanderthals and AMHs may have interacted, and challenge the often-perpetuated dichotomy in their material culture. This work is simultaneously innovating on diagnostic and analyses techniques to constrain interpretation on multiple fronts. We applied recent improvements in radiocarbon dating methodologies (e.g. ultrafiltration and single amino acid dating) and designed targeted sampling strategies, to reduce noise deriving from exogenous carbon contamination and nonhuman activities at archaeological sites. For analysis, we adapted Bayesian modelling and KDE techniques to integrate different qualitative and quantitative data from geophysics, forensics, physical anthropology, genetics, palaeoproteomics and material culture -work that included the development of new analyses and visualisation tools, such as KDE mapping of geolocated radiocarbon information. Our research generated pioneering high-resolution chronologies for major archaeological sites along the Danube fluvial corridor, and the first pan-European spatio-temporal mapping of AMH and Neanderthal activity that includes reliable data from not only western and central European, but also eastern European sites. As a result, we increased the reliability of the AMH dispersal chronology, as well as refined our understanding of changing migration patterns over large geographical areas, and of the regional variability and complexity of the period between 50-30k BP (Before Present). Joint work with Bibaná Hromadová (CNRS, Nanterre, France), András Markó (Hungarian National Museum, Budapest), Jean-Luc Guadelli (CNRS, Bordeaux, France), Nikolay Sirakov (National Institute of Archaeology and Museum, Bulgarian Academy of Sciences), Aleta Guadelli (CNRS, Bordeaux, France), Christopher Bronk Ramsey (University of Oxford, UK), Tom F. Higham (University of Oxford, UK). Acknowledgments: This research was funded by the European Research Council under the European Union's 7th Framework Programme (FP7/2007-2013) / ERC grant agreement no. [324139] “PalaeoChron” awarded to Prof Tom Higham; the National Institute of Archaeology of the Bulgarian Academy of Sciences; and UMR5199 CNRS PACEA (France). Wolfson College Oxford provided travel contributions, and the Hunt Fellowship (Wenner Gren Foundation) supports the publication process. Access to material for radiocarbon dating was granted by: Dr Walpurga Antl-Weiser, Prof Dr Maria Teschler Nicola, Dr Christine Neugebauer-Maresch, Dr L’ubomı́ra Kamı́nska, Dr Matej Ruttkay, Dr Alena Šefčáková, Dr Anna Durisova, Dr Gasparik Mihaly, Dr Klára Paloás, Dr László Makádi, Dr Dušan Borić, and Dr Emanuela Cristiani. 31. Penalized Ensemble Kalman Filters for High Dimensional Non-linear Systems Elizabeth Hou, University of Michigan The ensemble Kalman filter (EnKF) is a data assimilation technique that uses an ensemble of models, updated with data, to track the time evolution of a usually non-linear system. However, its performance can suffer when the ensemble size is smaller than the state space, as is often necessary for computationally burdensome model. To solve this problem in this high dimensional regime, we propose a computationally fast and easy to implement algorithm called the penalized ensemble Kalman filter (PEnKF). Under certain conditions, it can be theoretically proven that the PEnKF will have good performance (the error will converge to zero) despite having fewer ensemble members than state dimensions. Further, the proposed approach makes fewer assumptions about the structure of the system than localization methods. These theoretical results are supported with simulations of several non-linear and high dimensional systems. Joint work with Earl Lawrence, Alfred Hero. 32. Characterizing Dislocation Defects in Dark Field X-ray Microscopy Images of Bulk Diamond Marylesa Howard, Nevada National Security Site Material defects play a large role in material response under shock loading, yet our understanding of how these defects initiate, propagate, and annihilate are not well understood. Using a newly developed diagnostic, dark field X-ray microscopy (DFXM), we can now visualize the behavior of dislocation defects in materials at the mesoscale under varying conditions. Using data from the European Synchrotron Radiation Facility, we apply a variety of image processing techniques to capture relevant features to locate and characterize size and orientation of dislocation defects in bulk diamond DFXM images. Beyond simple visualization, this analysis drives statistical characterization of the defects and their dynamic response to ",,2020.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
50221a897b860ab50a4a73aeb58d3801d92c0e06,https://www.semanticscholar.org/paper/50221a897b860ab50a4a73aeb58d3801d92c0e06,Tracing the Ghosts in Our Ethical Shells,"We proposes a new scientific model that enables the ability to collect evidence, and explain the motivations behind people's cyber malicious/ethical behaviors. Existing models mainly focus on detecting already-committed actions and associated response strategies, which is not proactive. That is the reason why little has been done in order to prevent malicious behaviors early, despite the fact that issues like insider threats cost corporations billions of dollars annually, and its time to detection often lasts for more than a year.We address those problems by our main contributions of:+ A better model for ethical/malicious behavioral analysis with a strong focus on understanding people's motivations. + Research results regarding ethical behaviors of more than 200 participants, during the historic Covid-19 pandemic. + Novel attack and defense strategies based on validated model and survey results. + Strategies for continuous model development and integration, utilizing latest technologies such as natural language processing, and machine learning. We employed mixed-mode research approach of: integrating and combining proven behavioral science models, case studying of hackers, survey research, quantitative analysis, and qualitative analysis. For practical deployments, corporations may utilize our model in: improving HR processes and research, prioritizing plans based on the model's informed human behavioral metrics, better analysis in existing or potential cyber insider threat cases, generating more defense tactics in information warfare and so on.",,2020.0,10.31234/osf.io/awzvs,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5f2836111f7b2e3c9c4df81a7b9b8f183bfb5301,https://www.semanticscholar.org/paper/5f2836111f7b2e3c9c4df81a7b9b8f183bfb5301,Accelerating Artificial Intelligence on the Grid,"In this work, we present an ongoing, three year project funded by the Advanced Research Project Agency (ARPA-E) to develop an infrastructure to accelerate the development and deployment of artificial intelligence solutions for the electric grid. The project addresses the critical issues that we have identified as hampering the deployment of AI on electric grid measurements. These issues have stymied the potential for insight from high resolution grid measurements and generally hindered artificial intelligence innovation in the utility industry. The project consists of three main components. The first is the deployment of a diverse set of grid sensors to capture a variety of grid behaviour, both from the field and in simulation. The second is the deployment of a highly performant, scalable, cloud-based data management and AI platform designed for time series data to enable the easy storage, processing and analysis of grid sensor data. The third is the cultivation of an open research community of experts around the platform and data through useful educational material, code and data sharing, and data science competitions. Overall, the project will accelerate the development of analytics, machine learning, and AI by addressing existing gaps in data, tools, and people, with the aim of improving the electric grid.",2020 Clemson University Power Systems Conference (PSC),2020.0,10.1109/PSC50246.2020.9131317,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d5ab35e5090aa23093b717841742da9cf44861e6,https://www.semanticscholar.org/paper/d5ab35e5090aa23093b717841742da9cf44861e6,Game-Theoretic Safety Assurance for Human-Centered Robotic Systems,"In order for autonomous systems like robots, drones, and self-driving cars to be reliably introduced into our society, they must have the ability to actively account for safety during their operation. While safety analysis has traditionally been conducted offline for controlled environments like cages on factory floors, the much higher complexity of open, human-populated spaces like our homes, cities, and roads makes it unviable to rely on common design-time assumptions, since these may be violated once the system is deployed. Instead, the next generation of robotic technologies will need to reason about safety online, constructing high-confidence assurances informed by ongoing observations of the environment and other agents, in spite of models of them being necessarily fallible.This dissertation aims to lay down the necessary foundations to enable autonomous systems to ensure their own safety in complex, changing, and uncertain environments, by explicitly reasoning about the gap between their models and the real world. It first introduces a suite of novel robust optimal control formulations and algorithmic tools that permit tractable safety analysis in time-varying, multi-agent systems, as well as safe real-time robotic navigation in partially unknown environments; these approaches are demonstrated on large-scale unmanned air traffic simulation and physical quadrotor platforms. After this, it draws on Bayesian machine learning methods to translate model-based guarantees into high-confidence assurances, monitoring the reliability of predictive models in light of changing evidence about the physical system and surrounding agents. This principle is first applied to a general safety framework allowing the use of learning-based control (e.g. reinforcement learning) for safety-critical robotic systems such as drones, and then combined with insights from cognitive science and dynamic game theory to enable safe human-centered navigation and interaction; these techniques are showcased on physical quadrotors—flying in unmodeled wind and among human pedestrians—and simulated highway driving. The dissertation ends with a discussion of challenges and opportunities ahead, including the bridging of safety analysis and reinforcement learning and the need to ``close the loop'' around learning and adaptation in order to deploy increasingly advanced autonomous systems with confidence.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a5d85748055ff954e38c4c1f24138d614d4323fa,https://www.semanticscholar.org/paper/a5d85748055ff954e38c4c1f24138d614d4323fa,SmartDashCam: Automatic Live Calibration for DashCams,"Dashboard camera installations are becoming increasingly common due to various Advanced Driver Assistance Systems (ADAS) based services provided by them. Though deployed primarily for crash recordings, calibrating these cameras can allow them to measure real-world distances, which can enable a broad spectrum of ADAS applications such as lane-detection, safe driving distance estimation, collision prediction, and collision prevention Today, dashboard camera calibration is a tedious manual process that requires a trained professional who needs to use a known pattern (e.g., chessboard-like) at a calibrated distance. In this paper, we propose SmartDash-Cam, a system for automatic and live calibration of dashboard cameras which always ensures highly accurate calibration values. Smart-DashCam leverages collecting images of a large number of vehicles appearing in front of the camera and using their coarse geometric shapes to derive the calibration parameters. In sharp contrast to the manual process we are proposing the use of a large amount of data and machine learning techniques to arrive at calibration accuracies that are comparable to the manual process. SmartDashCam implemented using commodity dashboard cameras estimates realworld distances with mean errors of 5.7 % which closely rivals the 4.1% mean error obtained from traditional manual calibration using known patterns.",2019 18th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN),2019.0,10.1145/3302506.3310397,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b5342b0f8b4a9a07cd479d9996268e24ee343e0f,https://www.semanticscholar.org/paper/b5342b0f8b4a9a07cd479d9996268e24ee343e0f,Intelligent Pothole Detection and Road Condition Assessment,"Poor road conditions are a public nuisance, causing passenger discomfort, damage to vehicles, and accidents. In the U.S., road-related conditions are a factor in 22,000 of the 42,000 traffic fatalities each year. Although we often complain about bad roads, we have no way to detect or report them at scale. To address this issue, we developed a system to detect potholes and assess road conditions in real-time. Our solution is a mobile application that captures data on a car's movement from gyroscope and accelerometer sensors in the phone. To assess roads using this sensor data, we trained SVM models to classify road conditions with 93% accuracy and potholes with 92% accuracy, beating the base rate for both problems. As the user drives, the models use the sensor data to classify whether the road is good or bad, and whether it contains potholes. Then, the classification results are used to create data-rich maps that illustrate road conditions across the city. Our system will empower civic officials to identify and repair damaged roads which inconvenience passengers and cause accidents. This paper details our data science process for collecting training data on real roads, transforming noisy sensor data into useful signals, training and evaluating machine learning models, and deploying those models to production through a real-time classification app. It also highlights how cities can use our system to crowdsource data and deliver road repair resources to areas in need.",ArXiv,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
bcefcd43386dc73b9504de724a11e39f055ca120,https://www.semanticscholar.org/paper/bcefcd43386dc73b9504de724a11e39f055ca120,Mastering Structured Data on the Semantic Web: From HTML5 Microdata to Linked Open Data,"A major limitation of conventional web sites is their unorganized and isolated contents, which is created mainly for human consumption. This limitation can be addressed by organizing and publishing data, using powerful formats that add structure and meaning to the content of web pages and link related data to one another. Computers can ""understand"" such data better, which can be useful for task automation. The web sites that provide semantics (meaning) to software agents form the Semantic Web, the Artificial Intelligence extension of the World Wide Web. In contrast to the conventional Web (the ""Web of Documents""), the Semantic Web includes the ""Web of Data"", which connects ""things"" (representing real-world humans and objects) rather than documents meaningless to computers. Mastering Structured Data on the Semantic Web explains the practical aspects and the theory behind the Semantic Web and how structured data, such as HTML5 Microdata and JSON-LD, can be used to improve your sites performance on next-generation Search Engine Result Pages and be displayed on Google Knowledge Panels. You will learn how to represent arbitrary fields of human knowledge in a machine-interpretable form using the Resource Description Framework (RDF), the cornerstone of the Semantic Web. You will see how to store and manipulate RDF data in purpose-built graph databases such as triplestores and quadstores, that are exploited in Internet marketing, social media, and data mining, in the form of Big Data applications such as the Google Knowledge Graph, Wikidata, or Facebooks Social Graph. With the constantly increasing user expectations in web services and applications, Semantic Web standards gain more popularity. This book will familiarize you with the leading controlled vocabularies and ontologies and explain how to represent your own concepts. After learning the principles of Linked Data, the five-star deployment scheme, and the Open Data concept, you will be able to create and interlink five-star Linked Open Data, and merge your RDF graphs to the LOD Cloud. The book also covers the most important tools for generating, storing, extracting, and visualizing RDF data, including, but not limited to, Protg, TopBraid Composer, Sindice, Apache Marmotta, Callimachus, and Tabulator. You will learn to implement Apache Jena and Sesame in popular IDEs such as Eclipse and NetBeans, and use these APIs for rapid Semantic Web application development. Mastering Structured Data on the Semantic Web demonstrates how to represent and connect structured data to reach a wider audience, encourage data reuse, and provide content that can be automatically processed with full certainty. As a result, your web contents will be integral parts of the next revolution of the Web. What youll learn Extend your markup with machine-readable annotations and get your data to the Google Knowledge Graph Represent real-world objects and persons with machine-interpretable code Develop Semantic Web applications in JavaReuse and interlink structured data and create LOD datasets Who this book is for The book is intended for web developers and SEO experts who want to learn state-of-the-art Search Engine Optimization methods using machine-readable annotations and machine-interpretable Linked Data definitions. The book will also benefit researchers interested in automatic knowledge discovery. As a textbook on Semantic Web standards powered by graph theory and mathematical logic, the book could also be used as a reference work for computer science graduates and Semantic Web researchers.",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
efb247294b66ec334f81f9f53f4baf00e54ccc40,https://www.semanticscholar.org/paper/efb247294b66ec334f81f9f53f4baf00e54ccc40,Digital Transformation in Healthcare – South Africa Context,"Digital transformation is growing at a slow rate in medical schemes or healthcare compared to other industries such as banking and insurance. The healthcare sector needs to embrace the digital transformation and adopt and optimize on use of technology, otherwise, the sector will be left behind. Other sectors have taken advantage of technology, for example in the retail sector, nowadays people shop online, bank, and do travel bookings online. The logistic business has also embraced digital transformation in that most activities are now done through devices at the convenience of one’s office or home. The recent HPCSA conference included topics such as Telemedicine’s where several digital transformation and innovations in the health sector were also presented. What was evident in the discussions was that progress in accelerating digital transformation is pounded by a slow pace of regulation and other relevant guidelines. The topics discussed clearly revealed that the health sector is still far behind compared to other countries. For example, there is a gap in the adoption of digitally enabled tools for diagnosing, providing treatment, and better management of chronic conditions and other conditions. Electronic medical records are still not a part of routine care both from the supply and the funders side except a handful of players. On the funders side, you do find several medical schemes that invest in technology, for example, there are schemes that are already implementing digital application forms for smooth onboarding of new members. This is with the aim of going digital and reduces paper application forms. Similarly, with the submission of claims of which more than 98% are submitted in electronic form has transformed significantly. Strategies such as digital marketing are typically used to reach the target market and communicate more effectively with members. Several schemes have invested a lot in product development such as mobile apps, developing communication channels through online and social media platforms. Social media platforms provide an opportunity for brand repositioning, it also provides an opportunity to reach a new target market and access to a larger pool potential client base. Social media platforms could also be used as a tool to improve service to clients, create convenience, provide instant interaction with clients. However, very few medical schemes optimize on these platforms, particularly small to medium schemes. There is still a need to measure value add of digital transformation to members, chiefly where the quality of care is concerned. *Address correspondence to this author at the General Manager Research and Monitoring, Council for Medical Schemes, South Africa; E-mail: m.willie@medicalschemes.com The Health Professions Council of South Africa is a statutory regulator of healthcare professions in South Africa. Medical schemes are non-profit organisation which are registered with the Registrar of Medical Schemes. Members belonging to a scheme make contributions and in return receive medical cover according to the rules of the scheme. A recent study conducted by Willie (2019) which was an unstructured survey on the use of medical scheme mobile app by members. The survey revealed than more than 75% of the respondents did not have the app installed. Some of the sentiments for not using the app were: • Lack of awareness about the app • The app is complex • No reason to use the app • Does not meet my needs Digital disruption has great potential in healthcare, the main areas of investments are certainly Big Data analytics and AI (Artificial Intelligence). Some of the big data analytics tools are useful for improving efficiencies where some of the tools can be automated, this potentially could yield better utilization of human resources and could potentially have huge cost savings. In the main, Big data and AI tools are used to profile clients, medical service providers and look at healthcare utilization patterns and trends. Some of the techniques such as predictive analytics are important in that they can be used not only to profile member but create a strategy to combat attrition. Insights from the data could be useful for data drive decision-making process that potentially save huge downstream cost for medical schemes. There is also great potential in investing in digital marketing and the optimal use of mobile apps. DIGITAL TRANSFORMATION INNITIATIVES IN THE PUBLIC SECTOR SOUTH AFRICA HEALTHCARE There are several innovations that must take place in the public sector in South Africa as far as digital 2 Global Journal of Immunology and Allergic Diseases, 2019, Vol. 7 Willie and Nkomo transformation is concerned, chiefly these are still at beta phases and their overall impact and outcomes are still to be realized. Furthermore, there are pockets of digital innovations in the public sector dating back to 2014, some are initiatives employed at provincial level whiles others are deployed at the national level. An integrated holistic approach at the national level could ascertain value add and impact in the sector. Box 1 below depicts the Department of Health’s (DoH) digital and eHealth developments and implementation from 2014. USE OF ARTIFICIAL INTELLIGENCE IN HEALTHCARE Artificial Intelligence (AI), Machine Learning (ML) and Big Data Analytics are some of the most talkedabout technologies in recent years. According to Bali, Garg, and Bali (2019), AI aims to mimic human cognitive functions, such as the ability to reason, discover meaning, generalize, or learn from experience. Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data (Jiang, 2017). Machine learning is the foundation of modern AI and is essentially an algorithm that allows computers to learn independently without following any explicit programming (Uzialko, 2019). The use of AI is already at advanced stages in other industries, the adoption in healthcare is growing at a steady rate, however, there is no doubt AI is certainly going to change the face of healthcare delivery. AI is being employed in a numerous setting, for example, Year Digital developments 2014 Aviro launched their innovative eHealth app. North West department of health outlines eHealth plans (RHIS) Cell – Life’s iDART hits the target. Tier. Net, the software application that monitors patients on HIV and TB treatment. The NDoH has issued a tender for a service provider to conduct an evaluation of the use of the Tier.Net NDoH sets out eHealth standards evaluation process 2015 The Mpumalanga DoH issues eHealth tender eHealth rollout high on Gauteng’s agenda Mobenzi has partnered with the Anova Health Institute to support the Limpopo (DoH) with the deployment of the Mobenzi mHealth technology emocha launches TB mHealth platform in South Africa NDoH is working with the CSIR to develop an eHealth system to accompany the rollout of NHI North West DoH announce eHealth pilot 2016 emocha Boosting MDR-TB linkage to care in South Africa emocha’s miLINC for MDR-TB mHealth platform was designed after the NDoH approached John Hopkins University The Human Research Science Council (HSRC) has announced the development of a new mHealth app aimed specifically at pregnant teens NDoH using eHealth to improve health facilities South Africa adopts WHO’s HIV ‘Test and Treat’ guidelines 2017 mHealth aiding in the diagnoses of burn injuries Generic and Biosimilar Medicine of Southern Africa has asked the South African government to accelerate the evaluation and registration of more affordable biosimilar medicines in South Africa. South African medical information-exchange company, Healthbridge, has announced their acquisition of Infosys software solutions’ healthcare division WHO and ITU to use eHealth to strengthen health services in Africa South Africa digital health accelerator attracts top eHealth startups 2018 The National Department of Health (NDoH) has identified IT and health information systems (HIS). The South African Medical Research Council (SAMRC) has partnered with Jembi Health systems NPC Philips and UJ renew MoU to empower healthcare professionals Digital health Cape Town have announced the commencement of their second accelerator programme A new mobile app, called ViaOpta Hello, has been unveiled to help hundreds of thousands of South African living with blindness and severe visual impairment 2019 a subsidiary of CompuGroup Medical SE has developed an e-scripting solution that is helping over 1,000 South African doctors ensure medication adherence among their patients. Aviro Health launches whatsapp channel to support HIV self-testing Box 1: Digital developments in the public sector. Digital Transformation in Healthcare – South Africa Context Global Journal of Immunology and Allergic Diseases, 2019, Vol. 7 3 funders, as well as administrators, use it to adjudicate and process of claims, hospital facilities for assessing bed occupancy. AI is also used to analyses unstructured data such as images, videos, physician notes to enable clinical decision making and information sharing. Other commentators such as Reddy (2018) argues that AI is more prevent in the area of medical diagnosis. AI systems can analyze huge volumes of data faster far more than humans, this improves efficiencies in identifying medical diagnoses than doctors. It should be noted that AI cannot completely replace the medical profession but could be used as a tool to optimize currently process and reach medical conclusions and decision-making factor, thus saving costs and improving quality of life. APPLICATIONS OF ARTIFICIAL INTELLIGENCE Artificial has the potential to change the healthcare industry in South Africa for the better, this is subject to optimal use in both the supply and demand side of the health care ecosystem. AI is deliver",Global Journal of Immunology and Allergic Diseases,2019.0,10.31907/2310-6980.2019.07.01,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c6b50daee0a95fe3e1b1d17ae3875f63ecd22887,https://www.semanticscholar.org/paper/c6b50daee0a95fe3e1b1d17ae3875f63ecd22887,Talking machines : democratizing the design of voice-based agents for the home,"Embodied voice-based agents, such as Amazon Echo, Google Home, and Jibo, are becoming increasingly present in the home environment. For most people, these agents represent their first experience living with artificial intelligence in such private and personal spaces. However, little is known about people's desires, preferences, and boundaries for these technologies. This thesis shares insights, learnings, methods, and tools from ajourney with 69 children, adults, and older adults to help democratize the design of voice-based agents for the home. In the first study, participants interact with and discover various voice-based agents to capture first impressions of the technology. In the second study, participants engage in longterm encounters with agents in their home to experience and reflect upon their preferences, desires and boundaries for these devices. Qualitative and quantitative data from interview transcripts, card sorting, and deployed cultural and technology probes is used to identify agent action preferences, sociotechnical themes, daily usage trends, personality preferences, and future ""wishes"" for agents. This work culminates with participants designing their dream agents for the home through a structured ideation process. Throughout this work, a series of participatory design tools and methods are developed, iterated upon, and implemented to create a language of engagement with participants. These methods and tools are shared as an open-source design kit for others seeking to explore the domain. Cynthia Breazeal, Thesis Supervisor Associate Professor of Media Arts & Sciences",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d36e8cf9da64716d9ee217afac68d8d422128a9a,https://www.semanticscholar.org/paper/d36e8cf9da64716d9ee217afac68d8d422128a9a,Improving Volcanic Data Storage and Management by means of a SQL Database,"In geophysics, and more specifically in volcanology, seismic data are collected continuously through stations equipped with sensors that sample at low frequency. As a result of this data gathering, many millions of samples are obtained in a campaign (after some months). This could be considered as a Big Data issue due to the huge amount of different information obtained and different data storage formats. Nowadays this problem has not been addressed efficiently to date. This work presents a proposal for an optimal storage, maintenance and access to geophysical data. Thus, in order to deal with this huge amount of information, a specialized SQL database scheme has been designed. It has been later optimized in a number of ways, such as: selecting the best data management engine, using the optimal type of search index, or tuning the different configuration parameters. The database consist of three different ’logical’ data blocks: catalogued data, collected data, and processed data. The first type refers to data about volcanic events previously identified by the corresponding geophysical entities and publicly available. The second type is formed by data gathered in different campaigns in volcanoes, considering the sensors deployed in stations, and, from them, the different physical magnitudes they can measure. The third type corresponds to the knowledge extracted from the raw data in previous types obtained through the application of manual and automatic (Data Mining/Machine Learning) techniques for the identification of volcanic events or the prediction of eruptions, for instance. The latter could be seen as a ’service’ that the system would offer to end-users. As a proof of concept, the database has been feed with a part of the data collected in a campaign at Etna volcano (six sensors in different stations during one year). Several experiments and tests have been conducted by geophysicists and computer science researchers to validate its utility.",,2019.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
45cbdace972359f0ae71b35b3714a6f4ec4a1816,https://www.semanticscholar.org/paper/45cbdace972359f0ae71b35b3714a6f4ec4a1816,IEEE INFOCOM 2020 Workshops: IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS) - Program,"In the past decade, applications of Internet of Things (IoT) such as Smart Home, Smart Cities, Smart Healthcare etc. have been deployed where the devices in our surroundings are interconnected to provide better services and comfort to humans. More recently, we witness the emerging applications in industrial internet, supply chains and other areas where the scale of the systems, the number of devices and data being generated continuously increases. It will be at very high cost to send all the data to a centralized server, as done in cloud computing, for processing and decision-making. Therefore, recent trend is to move the computation tasks from centralized cloud to edge devices which are closer to data sources. In this talk, I will describe Edge, an infrastructure for collaborative edge computing, where the intelligence and decision-making are pushed to the edge of and within the network closer to the sources of data, EdgeMesh lets the edge devices share the data and collaborate on the computation tasks. It also facilitates higher scalability and reliability with flexible and dynamic system reconfiguration. Short Bio: Dr. Cao is currently a Chair Professor of Department of Computing at The Hong Kong Polytechnic University, Hong Kong. He is also the director of the Internet and Mobile Computing Lab in the department and the director of University's Research Facility in Big Data Analytics. His research interests include parallel and distributed computing, wireless sensing and networks, pervasive and mobile computing, and big data and cloud computing. He has co-authored 5 books, co-edited 9 books, and published over 600 papers in major international journals and conference proceedings. He received Best Paper Awards from conferences including IEEE DSAA 2017, IEEE SMARTCOMP 2016, IEEE ISPA 2013, IEEE WCNC 2011, etc. Dr. Cao served the Chair of the Technical Committee on Distributed Processing of IEEE Computer Society from 2012 to 2014, a member of IEEE Fellows Evaluation Committee of the Computer Society and the Reliability Society, a member of IEEE Computer Society Education Awards Selection Committee, a member of IEEE Communications Society Awards Committee, and a member of Steering Committee of IEEE Transactions on Mobile Computing. He has also served as chairs and members of organizing and technical committees of many international conferences, and as associate editor and member of the editorial boards of many international journals. Dr. Cao is a fellow of IEEE and ACM distinguished member. In 2017, he received the Overseas Outstanding Contribution Award from China Computer Federation. Sunday, April 28 10:30 11:45 Mobile Edge Computing Room: Forum E Chair: Xu Chen (Sun Yat-sen University, P.R. China) Economics of Investment and Use of Shared Network Infrastructures Iordanis Koutsopoulos (Athens University of Economics and Business, Greece); Angeliki Anastopoulou (Athens University of Economics and Business & Center for Research and Technology Hellas, Greece); Merkourios Karaliopoulos (Athens University of Economics and Business, Greece) PayFlow: Micropayments for Bandwidth Reservations in Software Defined Networks David Chen, Zhiyue Zhang, Ambrish Krishnan and Bhaskar Krishnamachari (University of Southern California, USA) DeepMarket: An Edge Computing Marketplace with Distributed TensorFlow Execution Capability Susham Yerabolu, Soyoung Kim, Samuel Gomena, Xuanzhe Li, Rohan Patel, Shraddha Bhise and Ehsan Aryafar (Portland State University, USA) Sunday, April 28 10:30 11:30 Session 1: Cloud Applications Room: Scene D Chair: Qichao Xu (Shanghai University, P.R. China) Session 1.1 Credible and Economic Multimedia Service Optimization in Cloud Network Tengfei Cao, Changqiao Xu and Han Xiao (Beijing University of Posts and Telecommunications, P.R. China); Lujie Zhong (Capital Normal University, P.R. China) Session 1.2 Reinforcing the Edge: Autonomous Energy Management for Mobile Device Clouds Venkatraman Balasubramanian (Arizona State University, USA); Faisal Zaman (University of Ottawa, Canada); Moayad Aloqaily (Gnowit Inc., Canada); Saed Alrabaee (United Arab Emirates University, United Arab Emirates); Maria Gorlatova (Duke University, USA); Martin Reisslein (Arizona State University, USA) Session 1.3 Context-Aware Service Chaining Framework for Over-the-Top Applications in 5G Networks Chang Ge, David Lake, Ning Wang, Yogaratnam Rahulan and Rahim Tafazolli (University of Surrey, United Kingdom (Great Britain)) Sunday, April 28 11:30 12:30 Session 2: Cloud System Room: Scene D Chair: David Lake (University of Surrey, United Kingdom (Great Britain)) Session 2.1 Design and Evaluation of a Software Defined Passive Optical Intra-Rack Network in Data Centers Yueping Cai, Sen Luo, Li Zhou, Zongchen Yao and Tianchi Li (Chongqing University, P.R. China) Session 2.2 Point Estimator Log Tracker for Cloud Monitoring Tariq Daradkeh and Anjali Agarwal (Concordia University, Canada); Nishith Goel (Cistel, Canada); Andrew J. Kozlowski (Cistech Ltd., Canada) Session 2.3 Cyberspace Surveying and Mapping: Hierarchical Model and Resource Formalization Rui Xu (Cyberspace Security Key Laboratory of Sichuan Province & China Electronic Technology Cyber Security Co., Ltd, P.R. China); Zhiyong Zhang and ZhiHong Rao (China Electronic Technology Cyber Security Co., Ltd & Cyberspace Security Key Laboratory of Sichuan Province, P.R. China); Jianfeng Chen (China Electronic Technology Cyber Security Co. Ltd, P.R. China); Ming LI (China Electronic Technology Cyber Security Co., Ltd, P.R. China); Fang Liu (China Electronic Technology Cyber Security Co., Ltd & Cyberspace Security Key Laboratory of Sichuan Province, P.R. China); Shengli Pan (China University of Geosciences (Wuhan), P.R. China) Sunday, April 28 13:30 14:30 Session 3: Cloud Security Room: Scene D Chair: Zhi Zhou (Sun Yat-sen University, P.R. China) Session 3.1 Policy-based Bigdata Security and QoS Framework for SDN/IoT: An Analytic Approach Keshav Sood (Deakin University, Melbourne Australia, Australia); Shiva Raj Pokhrel (Deakin University & Nepal Telecom, Australia); Shui Yu (University of Technology Sydney, Australia); Mohammad Reza Nosouhi (University of Technology Sydney & CSIRO' Data61, Australia) Session 3.2 Intrusion Detection in Autonomous Vehicular Networks: A Trust Assessment and Q-learning Approach Rui Xing and Zhou Su (Shanghai University, P.R. China); Yuntao Wang (Xi'an Jiaotong University, P.R. China) Session 3.3 Early Online Classification of Encrypted Traffic Streams using Multi-fractal Features Erik Areström and Niklas Carlsson (Linköping University, Sweden) Sunday, April 28 14:30 15:30 Keynote Session 2: Automated Management of Softwarized and Virtualized Network Functions Xiaoming Fu Room: Scene D Chair: Zhi Zhou (Sun Yat-sen University, P.R. China) Abstract: Online management of large-scale cloud and virtualized resources poses many challenges and opportunities. As emerging network appliances, such as firewalls, loadbalancing, QoS and DPI, are more and more implemented as software-defined functions and converging on the cloudized and virtualized network infrastructure, the management of such functions becomes more and more critical. I will introduce some of our recent work towards efficient management of NFV-based service chaining, placement of data and virtual machines (VMs), as well as scheduling of NFV resources. Online management of large-scale cloud and virtualized resources poses many challenges and opportunities. As emerging network appliances, such as firewalls, loadbalancing, QoS and DPI, are more and more implemented as software-defined functions and converging on the cloudized and virtualized network infrastructure, the management of such functions becomes more and more critical. I will introduce some of our recent work towards efficient management of NFV-based service chaining, placement of data and virtual machines (VMs), as well as scheduling of NFV resources. Short Bio: Prof. Xiaoming Fu received his Ph.D. from Tsinghua University, Beijing, China in 2000. He was then a research staff at the Technical University Berlin until joining University of Göttingen as an assistant professor in 2002, where he has been a professor in computer science and heading the Computer Networks Group since 2007, and the founding director of Sino-German Institute of Social Computing since 2015. Prof. Fu's research interests lie in networked systems and applications, including cloud computing, mobile computing, big data and social networks. He has actively participated in national and international projects, including as coordinator of EU FP7 GreenICN, CleanSky and MobileCloud projects as well as H2020 ICN2020 project. He has also served on the program or organization committees of several conferences such as ACM SIGCOMM, CoNEXT, MOBICOM, ICN, IEEE INFOCOM, ICNP and ICDCS. He is a Fellow the IET and the Academia Europaea, an IEEE senior member and Distinguished Lecturer, and has served as secretary (2008-2010) and vice chair (2010-2012) of IEEE Communications Society Technical Committee on Computer Communications (TCCC), as well as as chair (2011-2013) of the Internet Technical Committee (ITC), the joint committee of IEEEE Communications Society and the Internet Society. Sunday, April 28 16:00 18:00 S4: Age of Information in Wireless Networks Room: Scene C Chair: Bo Ji (Temple University, USA) Maintaining Information Freshness under Jamming Andrey Garnaev (WINLAB, Rutgers University, USA); Wuyang Zhang, Jing Zhong and Roy Yates (Rutgers University, USA) Fundamental Bounds on the Age of Information in General Multi-Hop Interference Networks Shahab Farazi (Worcester Polytechnic Institute, USA); Andrew G. Klein (Western Washington University, USA); Donald R. Brown, III (Worcester Polytechnic Institute, USA) Minimizing The Age of Information: NOMA or OMA? Ali Maatouk (CentraleSupélec, France); Mohamad Assaad (CentraleSupelec, France); Anthony Ephremides (University of Marylan",IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),2019.0,10.1109/infcomw.2019.8845227,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10b099d0b80232c76707ca395a17ce92871381ab,https://www.semanticscholar.org/paper/10b099d0b80232c76707ca395a17ce92871381ab,PD11-02 DEVELOPMENT AND APPLICATION OF A DIGITAL IMAGE ANALYSIS CLASSIFIER FOR DIAGNOSTIC PROSTATE NEEDLE BIOPSIES TO PREDICT METASTASES,"INTRODUCTION AND OBJECTIVES: Conventional histological analysis cannot differentiate diagnostic prostate needle biopsies (PNBX) from patients with localized disease (stage M0) versus those with de novo metastases (stage M1). Digital image analysis (DIA) is a new precision medicine platform that shows promise in advancing diagnostic histopathology. Our team has developed and deployed a novel DIA risk of metastasis (RoM) classifier that systematically interrogates standard-of-care PNBX slides to determine M-stage and likelihood of clinical metastatic progression. METHODS: We collected diagnostic PNBX from 163 prostate cancer cases (85 M0 and 78 M1) that contained high grade cancer foci. Pathological annotation was performed by two independent pathologists. Following slide digitization at 40X, image tiles (approximately 30 image tiles per case, n = 6,501) were used to extract 62 handcrafted (HC) and 64 autoencoder (AE) nuclear features, as well as 1,234 tissue-based (TI) features. A Z-score normalization of features was performed. The nuclear and tissue architecture features were combined and used in a Leave One Out cross validation with 3 models: elastic net, gradient boosted decision trees, and a multi-layer perceptron. RoM scores were generated for each tile, and cases were classified into M0, M1, or ""uncertain"" based on the variance of RoM scores. Clinical progression was predicted using the model. RESULTS: After color normalization, 273,103 total nuclei and 6501 tiles were analyzed and values extracted for HC, AE, and TI features. Of the three models tested, the E-net model performed the best with an AUC of 0.885 from a cross-validation experiment. Of the 163 cases, 80% were assigned to M0 or M1 status, while 20% were placed into the ""uncertain"" category. Fifteen from 78 M1 cases were misclassified as M0 cases, while the other 63 cases were correctly classified. The classifier predicted progression to metastatic disease with an area under the curve (AUC) of 0.75. A second validation cohort of 59 more cases is underway. CONCLUSIONS: DIA technology and machine learning software enabled extraction of 1,360 features that differentiate high-grade M0 from M1 prostate cancer. Initial results show promise in using this classifier to prognosticate progression based solely on diagnostic PNBX. DIA technology has the potential to serve as a cost-effective clinical tool for confirming M-stage and predicting metastatic progression. When combined with additional clinical variables, the accuracy of the prediction can be further improved. Source of Funding: PCF Challenge Award, the STOP Cancer Foundation, AUA Summer Medical Student Reserach Fellowship: Herbert Brendler, MD Research Fund, DOD PC131996, PCF-Movember GAP1 Unique TMAs Project, Prostate Cancer Foundation (PCF) Creativity Award, Jean Perkins Foundation, NIH/NCI P01 CA098912-09, NIH R01CA131255 and P50CA092131, Stephen Spielberg Team Science Award",Journal of Urology,2019.0,10.1097/01.JU.0000555358.82417.19,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
74a1c016719ed6d22744f708071538c7e7011767,https://www.semanticscholar.org/paper/74a1c016719ed6d22744f708071538c7e7011767,Issues of Authenticity in Autonomously Creative Systems,"As the value of computer generated artefacts increases, we need to question how creative software will fit into human culture, both as creative collaborators and autonomously creative entities. We hypothesise that, in certain contexts where creative software has increased or total autonomy, lack of authenticity will be a major limiting factor for creative systems in the arts and possibly elsewhere. After explaining and motivating this hypothesis, we study notions of authenticity in the human context and use this to raise issues of computational authenticity. We propose a number of ways to address these issues, including influencing public perception of creative software, practical approaches to software recording and using its life experiences, and employing alternative methodologies for building creative systems. We believe that identifying, exploring and addressing issues of authenticity will help to maximise the beneficial contributions that autonomously creative software can bring to society. Introduction and Motivation As with many other areas of AI research and practice, there has recently been somewhat of a step change in the quality of artefacts generated through the employment of advanced techniques such as deep learning, and in some cases similar advances in the ease of use in deploying generative systems. As an example, looking at artistic/textural style transfer, the process previously involved writing a bespoke program to apply a certain visual texture to an image, or to generate art in the style of a particular artist or movement. Now style transfer merely involves supplying a style image and target image and waiting while a generative artificial neural network is trained and employed, which captures aspects of both style and target in a new image (Gatys, Ecker, and Bethge 2016), with remarkably good results. While pastiche generation is not normally seen as particularly creative, deep learning researchers are beginning to advance the creative autonomy of their generative systems, for instance through so-called Creative Adversarial Networks (Elgammal et al. 2017), and Computational Creativity researchers are employing such techniques in existing autonomous systems such as The Painting Fool (Colton 2012). As the ubiquity of creative systems and the quality of their output increases, we have to consider how it will fit into human society. Like any advance in technology or change in societal values, there will be pros and cons to having truly autonomous, creative, software embedded in human culture. We start from the position that the advantages of having such software will far outweigh any difficulties that it would bring. The Computational Creativity community is somewhat split over the question of whether effort should be spent in advancing software to fully autonomous levels of creativity, or whether it would be better to concentrate on building co-creative tools for mixed initiative usage. We focus here on future scenarios where software can exhibit the same level of creative autonomy as people in the arts and sciences. In this context, we question whether opportunities for creative systems (and attendant advantages to human society) will be missed through a lack of authenticity in software, due to fundamental differences between people and machines, including a lack of life experiences to draw upon. To motivate studying authenticity as an issue, consider the following demonstration, which has been carried out at more than a dozen talks by the first author, known hereafter as the presenter. The poem in figure 1 is presented as a wellknown piece by female poet Maureen Q. Smith. There is then some joint discussion around what the author may have meant with this short poem about childbirth, for instance: the poem may be about her own experience, as per the ‘My boy’ sentence; the ‘begin again’ phrase may have been a reference to a literal beginning (of life), or a re-boot for the family; the ‘joy’, ‘pain’ and ‘tears’ are probably literal; the ‘fears’ may be about the birth process, but equally about the future of the world the baby is born into. The presenter then points out that he has made a mistake: in fact, the author was a man, called Maurice Q. Smith. A re-evaluation is then undertaken, with the ‘pain’ perhaps now being projected, or expressing a painful worry about the Figure 1: Poem about childbirth used in a demonstration addressing authorial intent and authenticity. birth process. The presenter then points out that Maurice Q. Smith was actually a convicted child molester when he wrote the poem, and that it was widely regarded as depicting the process of grooming a child. The following re-evaluation highlights the suddenly much darker interpretation of ‘Born of me, for me’ with aspects such as ‘The joy, the pain’ becoming disturbing if taken literally, and ‘fears’ perhaps portraying worries about being caught in a criminal act. The presenter then says that the poem was actually generated by a computer program using texts about childbirth, and suggest that – with no concerns of an unsettling backstory – another re-evaluation can take place. At this point, the demonstrator attempts to hypothesise what the software meant when it wrote about ‘joy’ and ‘pain’, but realises there was no further valuable meaning to be gleaned from an understanding of the generative process. Similarly, while the ‘tears’ are obviously not literal, it’s seems impossible to project any further meaning onto the usage of this term. The presenter then invites the audience to question whether the poem has now lost some or all of its meaning, whether – now we can’t indulge in projecting thoughts/experiences/intentions onto the author – the value of the poem has decreased or not. There tends to be agreement that people project more value onto the poem when they believed it was written by a person. For full disclosure, the presenter ends by revealing that he wrote the poem for the purposes of the demonstration. Of course, poems can be – and often are – written, and read from third party perspectives. Well-intentioned ideals such as Death of the Author (Barthes 1967) and the intentional fallacy (Wimsatt and Beardsley 1954), advocate taking poetry and literature at face value, without inferring authorial intention and actively ignoring any knowledge of the author. It is not clear how easy such ideals are to implement in practice. The childbirth example shows how knowledge of merely the author’s name naturally affects the reading of the poem, providing much context. Also, poetry is often enjoyed as a performance art, e.g., in poetry slams, where the personality and intentions of the poet shape the performance, and are explicitly included in commentaries. In a more reasonable appreciation of poetry, where at least some people infer authorial backgrounds and intentions – as highlighted by the childbirth poem – there may be an uncanny valley effect (Seyama and Nagayama 2007), e.g., if software autonomously wrote a beautiful ballad about teenage love, audiences would question what the software really knows about this topic, and hence what value the song really has. We believe that many issues around lack of meaning and authorial intention can be understood via the lens of authenticity, or lack thereof, in computational systems. In the next section, we study authenticity from various perspectives in a human context. Following this, we use the study to raise certain issues about software authenticity, and suggest ways to address some of the issues, broadly in three areas: managing public perception of software authenticity; enabling software to use its life experiences in the creative process; and employing alternative methodologies for building creative systems. We conclude by discussing what these issues may mean for the future of Computational Creativity research. Authenticity in the Human Context A Million Little Pieces by James Frey (2003) is an autobiography about a struggle with drug addiction and rehabilitation. Published in twenty-nine languages, it has sold over 5 million copies, was on Oprah Winfrey’s Book Club selection and number one on the New York Times Best Seller list. In 2006, The Smoking Gun published an article (thesmokinggun.com/documents/celebrity/million-littlelies) claiming that many events in (Frey 2003) had not happened and that Frey had fictionalized his life. The public took this hard: Oprah Winfrey said she felt “duped” and publically rebuked him for “fabricating” and “lying” and the public felt “betrayed” (Wyatt 2006). More than 10 class action lawsuits were filed on the grounds of negligent misrepresentation and consumer fraud, with readers asking for compensation for the time they “wasted” reading a book they thought was non-fiction. Publisher Random House withdrew from a deal with Frey and offered full refunds to readers, and some libraries re-catalogued Frey’s book as fiction. Post-2006 editions come with disclaimers by both publisher and author, in which Frey writes: “My mistake, and it is one I deeply regret, is writing about the person I created in my mind to help me cope, and not the person who went through the experience. . . . I believe, and I understand others strongly disagree, that memoir allows the writer to work from memory instead of from a strict journalistic or historical standard. It is about impression and feeling, about individual recollection . . . It is a subjective truth, altered by the mind of a recovering drug addict and alcoholic. Ultimately, it’s a story . . . that I could not have written without having lived the life I’ve lived.” (Frey 2006, p2) The debate and strong feelings about A Million Little Pieces centre on our modern notion of authenticity. This is an ethical characteristic, an ideal which shapes our worldview – “that one should be true to oneself and lead a life that is expressive of what the person takes herself to be” (Varga 2013, p.5). Authenticity is particul",ICCC,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c6f2f23b95cc8d441c5a9d63270f9bb3cee4edeb,https://www.semanticscholar.org/paper/c6f2f23b95cc8d441c5a9d63270f9bb3cee4edeb,The Role of Technology in Health Care Innovation: A Commentary,"Information and communication technologies offer the opportunity for tremendous innovation in healthcare. Technology-based therapeutic and care coordination systems, that embrace web-, mobile-, sensing-, computing, and bioinformatics technologies, offer considerable promise for enabling entirely new models of healthcare both within and outside of formal systems of care and offer the opportunity to have a large public health impact. 
 
In a recent article, Linda Rosenberg highlights three primary changes she sees evolving within U.S. healthcare policy and practice, including increasing: (1) consumer engagement in their own care management, (2) integration of physical care and mental healthcare within care settings, and (3) data-driven care that measures outcomes and is responsive to performance metrics (Rosenberg, 2012). 
 
We propose that technology can play a key role in all three of these evolutions, plus more. First, technology affords a new model for enabling consumers to play a central role in selecting and helping to define the course of their own healthcare. A wide array of tools exist to monitor and capture real-time data about individuals’ behavioral and physiological states (e.g., via user input into mobile applications, wearable sensors and/or smartphone sensors). Additionally, there has been an explosion of research and development activities leading to the creation of self-directed tools that provide on-demand, educational or therapeutic support anytime/anywhere to help individuals manage their own health behavior. These tools may also provide individuals with the option to engage an extended support network in their own healthcare management (e.g., by sharing their health behavior data with family and friends to both empower and support them; by participating in virtual supportive communities, etc.). Further, decision support tools are increasingly being developed to help individuals better understand, access, and make choices about treatment. 
 
Second, although integration of behavioral healthcare into care settings that have largely managed physical health holds great promise for increasing coordination, quality, and impact of care, this evolution also creates a scenario in which already overburdened clinicians may not feel like they have the expertise, time, or resources to effectively address the behavioral health (e.g., substance use, mental health) needs of their clients. Technology offers tremendous opportunity to facilitate integrated care. Technology-based care systems, grounded in science-based approaches to promote health behavior, offer the potential to concurrently address an array of chronic illnesses and behavioral health issues in a manner that is optimally responsive to each patient’s needs. In this way, technology may reduce disease-specific, siloed care and offer countless opportunities for tailoring behavioral monitoring and intervention delivery for each individual and in response to changing health behavior trajectories over time. 
 
Third, embracing technology as part of the fundamental infrastructure for healthcare delivery allows for detailed data capture that can enable a careful examination of outcomes, including at the patient, provider, organizational and system levels. In some cases, this may optimally occur when electronic health records containing data captured within the care setting are integrated with data captured via other electronic media used by patients to monitor their health behavior outside of care settings. And, in other cases, detailed data capture can help individuals identify meaningful patterns in their own behavioral health. 
 
A growing body of research has demonstrated the acceptability, effectiveness, and cost-effectiveness of technology-based therapeutic tools (e.g., Marsch & Ben-Zeev, 2012). The widespread reach and ‘just-in-time’ therapeutic support they provide may help to prevent escalation of symptoms and promote sustained support for health behavior. Additionally, given the ubiquity of technology, including rapidly growing access to the Internet and smartphones among diverse communities, an approach to healthcare that embraces technology may play a key role in reducing healthcare disparities. 
 
Technology may increase our ability to pursue the World Health Organization’s definition of health, which embraces complete mental, physical and social well-being and not merely the absence of disease and infirmity (WHO, 1948). We may not get all the way there, but we may get closer than ever before. Unlike traditional medical care, which is simply a periodic visitor in the lives of people, technology can be there every minute of every day. It can measure, analyze, and intervene at any time and do so in virtually all aspects of life (many of which are outside the traditional purview of healthcare but which affect wellbeing). Technology in cars can prevent one from turning left into an oncoming truck. It can be used to measure gait to determine whether a person’s balance or cognitive well-being has deteriorated. It can offer games to relax, information to guide, or warnings to prepare individuals when weather becomes dangerous. Technology can relieve loneliness and isolation, speed productivity, and drive and park a car. All these things are not dreams; they are operational right now. Of course, the use of technology must be tailored to the needs and assets of individuals, families, and communities and be under their control. But, if offered in this way, technology allows for the potential to touch virtually every part of life that affects wellbeing. 
 
In order to fully realize this potential, however, a number of key challenges need to be tackled. First, although a large number of technology-based therapeutic tools have been developed, a unified, coordinated, and scalable technology-based system has not yet been fully developed. In order for this approach to have a large impact on population health, an understanding of the optimal combination of tools to include in a technology-based care delivery system is needed. Additionally, an understanding of the best model for integrating and deploying such a system, in a manner that brings value to all relevant stakeholders, is critical. 
 
Relatedly, the cultural divide between healthcare specialists and technology specialists needs to be bridged. A number of groups have already launched initiatives to facilitate coordinated efforts among a diverse group of health specialists and technologists, but an ongoing and focused dialogue is needed to achieve the end goal product. 
 
Additionally, current reimbursement models are not yet structured to accommodate many technology-based therapeutic tools in their payer models. The way in which these reimbursement models are structured over time will undoubtedly have a substantive impact on the adoption and sustained use of this approach. 
 
There are reasons to be both excited and cautious about this evolution. Although technology targeting behavioral health can offer valuable tools to behavioral health specialists, it will likely also force a transition in care delivery models. As mentioned above, there are already hundreds of research articles demonstrating that technology-based behavioral interventions, such as those targeting depression, anxiety disorders and substance use disorders, can be as effective as clinicians who treat these disorders. With increasing consumer access and demand for technology-based therapeutic tools, behavioral health specialists will need to embrace this evolution and evolve along with it. Professionals will need to view technology as a powerful partner in improving quality and productivity of behavioral healthcare. 
 
We recognize that there will be resistance by some to this change. But we see evidence of similar trends in virtually every labor-intensive profession, including our own. Faculty members at universities are finding they need to adjust to the prospect that hundreds of thousands of people can (for many topics) learn as effectively from computers as they can in a classroom. Researchers are finding that, by leveraging technology, mega research studies can be conducted in much less time, include more diverse participant representation, and address many more research questions than ever before. Statisticians may find that data mining and machine learning, along with massive volumes of data, can change the need for inferential statistics. 
 
In many ways, it is difficult to envision an evolution in healthcare, including the realization of the goals of the Affordable Care Act of 2010, without centrally embracing technology as a key part of a new model of healthcare. This is an exciting time of tremendous opportunity to transform healthcare delivery models. And, this is a time to seek out new horizons. We have the opportunity to broaden our goals and models of care to help people really understand the complex issues of our time and make decisions that are in their best interests and can help them meet their goals. Although much work is still needed, we look forward to the launch of innovative service delivery models at a population level that leverage the numerous, effective, technology-based tools we have available to optimize the reach, impact, and cost-effectiveness of healthcare.",Journal of dual diagnosis,2013.0,10.1080/15504263.2012.750105,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
972b20f1a231cb991f43b5a8dc99b8892c6a3db9,https://www.semanticscholar.org/paper/972b20f1a231cb991f43b5a8dc99b8892c6a3db9,Stress Detection for Keystroke Dynamics,"Background. Stress can profoundly affect human behavior. Critical-infrastructure operators (e.g., at nuclear power plants) may make more errors when overstressed; malicious insiders may experience stress while engaging in rogue behavior; and chronic stress has deleterious effects on mental and physical health. If stress could be detected unobtrusively, without requiring special equipment, remedies to these situations could be undertaken. In this study a common computer keyboard and everyday typing are the primary instruments for detecting stress. Aim. The goal of this dissertation is to detect stress via keystroke dynamics – the analysis of a user’s typing rhythms – and to detect the changes to those rhythms concomitant with stress. Additionally, we pinpoint markers for stress (e.g., a 10% increase in typing speed), analogous to the antigens used as markers for blood type. We seek markers that are universal across all typists, as well as markers that apply only to groups or clusters of typists, or even only to individual typists. Data. Five types of data were collected from 116 subjects: (1) demographic data, which can reveal factors (e.g., gender) that influence subjects’ reactions to stress; (2) psychological data, which capture a subject’s general susceptibility to stress and anxiety, as well as his/her current stress state; (3) physiological data (e.g., heart-rate variability and blood pressure) that permit an objective and independent assessment of a subject’s stress level; (4) self-report data, consisting of subjective self-reports regarding the subject’s stress, anxiety, and workload levels; and (5) typing data from subjects, in both neutral and stressed states, measured in terms of keystroke timings – hold and latency times – and typographical errors. Differences in typing rhythms between neutral and stressed states were examined to seek specific markers for stress. Method. An ABA, single-subject design was used, in which subjects act as their own controls. Each subject provided 80 typing samples in each of three conditions: (A) baseline/neutral, (B) induced stress, and (A) post-stress return/recovery-to-baseline. Physiological measures were analyzed to ascertain the subject’s stress level when providing each sample. Typing data were analyzed, using a variety of statistical and machine learning techniques, to elucidate markers of stress. Clustering techniques (e.g., K-means) were also employed to detect groups of users whose responses to stress are similar. Results. Our stressor paradigm was effective for all 116 subjects, as confirmed through analysis of physiological and self-report data. We were able to identify markers for stress within each subject; i.e., we can discriminate between neutral and stressed typing when examining any subject individually. However, despite our best attempts, and the use of state-of-the-art machine learning techniques, we were not able to identify universal markers for stress, across subjects, nor were we able to identify clusters of subjects whose stress responses were similar. Subjects’ stress responses, in typing data, appear to be highly individualized. Consequently, effective deployment in a realworld environment may require an approach similar to that taken in personalized medicine.",,2018.0,10.1184/R1/6723227.V3,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7b95873764d7e169811e3e5106f50b167703713b,https://www.semanticscholar.org/paper/7b95873764d7e169811e3e5106f50b167703713b,Relevance Feature Discovery for Text Mining,"Relevance feature discovery for text mining is a big challenge to guarantee the quality of discovered relevance features in text documents for describing user preferences because of large scale terms and data patterns. Most existing popular text mining and classification methods have adopted term-based approaches. However, they have all suffered from the problems of polysemy and synonymy. Over the years, there has been often held the hypothesis that pattern-based methods should perform better than term-based ones in describing user preferences; yet, how to effectively use large scale patterns remains a hard problem in text mining. To make a breakthrough in this challenging issue, this paper presents an innovative model for relevance feature discovery. It discovers both positive and negative patterns in text documents as higher level features and deploys them over low-level features (terms). It also classifies terms into categories and updates term weights based on their specificity and their distributions in patterns. Substantial experiments using this model on RCV1, TREC topics and Reuters-21578 show that the proposed model significantly outperforms both the state-of-the-art term-based methods and the pattern based methods. Keywords— Text mining, text feature extraction, text classification 1.INTRODUCTION The objective of relevance feature discovery (RFD) is to find the useful features available in text documents, including both relevant and irrelevant ISRJournals and Publications Page 803 International Journal of Advanced Research in Computer Science Engineering and Information Technology Volume: 4, Issue: 3,Special Issue: 2 ,Apr,2016 ,ISSN_NO: 2321-3337 ones, for describing text mining results. This problem is also of central interest in many Web personalized applications, and has received attention from researchers in Data Mining, Machine Learning, Information Retrieval and Web Intelligence communities . There are two challenging issues in using pattern mining techniques for finding relevance features in both relevant and irrelevant documents . The first is the low-support problem. Given a topic, long patterns are usually more specific for the topic, but they usually appear in documents with low support or frequency. If the minimum support is decreased, a lot of noisy patterns can be discovered. The second issue is the misinterpretation problem, which means the measures (e.g., “support” and “confidence”) used in pattern mining turn out to be not suitable in using patterns for solving problems. For example, a highly frequent pattern (normally a short pattern)may be a general pattern since it can be frequently used in both relevant and irrelevant documents. Hence, the difficult problem is how to use discovered patterns to accurately weight useful features. There are several existing methods for solving the two challenging issues in text mining. Pattern taxonomy mining (PTM) models have been proposed in which, mining closed sequential patterns in text paragraphs and deploying them over a term space to weight useful features. Concept-based model (CBM) has also been proposed to discover concepts by using natural language processing (NLP) techniques. It proposed verbargument structures to find concepts in sentences. These pattern (or concepts) based approaches have shown an important improvement in the effectiveness. However, fewer significant improvements are made compared with the best term-based method because how to effectively integrate patterns in both relevant and irrelevant documents is still an open problem. Over the years, people have developed many mature term-based techniques for ranking documents, information filtering and text classification. ISRJournals and Publications Page 804 International Journal of Advanced Research in Computer Science Engineering and Information Technology Volume: 4, Issue: 3,Special Issue: 2 ,Apr,2016 ,ISSN_NO: 2321-3337 Recently, several hybrid approaches were proposed for text classification. To learn term features within only relevant documents and unlabelled documents,used two term-based models. In the first stage, it utilized a Rocchio classifier to extract a set of reliable irrelevant documents from the unlabeled set. In the second stage, it built a SVM classifier to classify text documents. A two-stage model was also proposed which proved that the integration of the rough analysis (a term-based model) and pattern taxonomy mining is the best way to design a two-stage model for information filtering systems. 2. PATTERN TAXONOMY MINING Pattern taxonomy mining (a term-based model) and is the best way to design a two-stage model for information filtering systems. Text mining also referred to as text data mining, roughly equivalent to text analytics, refers to the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. 2.1. FEATURE SELECTION Feature selection is a technique that selects a subset of features from data for modeling systems. Over the years, a variety of feature selection methods (e.g., Filter, Wrapper, Embedded and Hybrid approaches, and unsupervised or semi-supervised methods) have been proposed in various fields .Feature selection is also one of important steps for text classification and information filtering which is the task of assigning documents to predefined classes. 2.2.TEXT ANALYTICS SOFTWARE : Text analytics software can help by transposing words and phrases in unstructured data into numerical values which can then be linked with structured data in a database. With an iterative approach, an organization can successfully use text analytics to gain insight into content-specific values such as sentiment, ISRJournals and Publications Page 805 International Journal of Advanced Research in Computer Science Engineering and Information Technology Volume: 4, Issue: 3,Special Issue: 2 ,Apr,2016 ,ISSN_NO: 2321-3337 emotion, intensity and relevance. Because text analytics technology is still considered to be an emerging technology, however, results and depth of analysis can vary wildly from vendor to vendor. ANALYSING THE PERFORMANCE OF STUDENTS AND MARK",,2016.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
35dd53ad04b6fc39aa1f864dee15427fa4acac15,https://www.semanticscholar.org/paper/35dd53ad04b6fc39aa1f864dee15427fa4acac15,Question and Accepted Answer Recommendation System with a Generic Knowledge Mining Visualization Tool,"With the increasing software developer community, questions answering (QA) sites, such as StackOverflow, Quora, Yahoo Answer etc. have been gaining its popularity. Hence, in recent years, millions of users are posting in StackOverflow. With the increase of users their question/answer a copious amount of data is generated, which attracts both the research community to utilize this information for extracting knowledge and the industry for developing the knowledge based systems. Since it is really difficult to find a question that relates one’s expertise area in order to gain some extra reputation or bounty points, we have built a personalized recommender system named PRISM that will recommend posts to a user after login based on their expertise. We have also implemented our recommendation system web application which is currently deployed in our localhost. Again it takes an enormous amount of effort to find out the suitable answer of a question. Propitiously, StackOverflow allows their community members to label an answer as an accepted answer. However, in the most of the questions answers are not marked as accepted answers. Therefore, there is a need to build a recommender system which can accurately suggest the most suitable answer to the questions. Contrary to the existing systems, in this work, we have utilized the textual features of the answers comments with the other metadata of the answers to build a recommender system for predicting the accepted answer called RAiTA. We have also deployed our recommendation system web application, which is publicly accessible at http://210.4.73.237:8888/ Visualization of data, pattern mining from datasets and analyzing data drift for the different features are three highly used applications of machine learning and data science fields. A generic web-based tool integrated with such features will provide prodigious support for pre-processing the dataset and thus extracting accurate information. In this work, we propose such a data visualization tool, named VIM which is publicly accessible at http://210.4.73.237:9999/ In memory of our family,friends and our parents.",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
278ed93dee876d8fa5e876e0fe0440bda222962a,https://www.semanticscholar.org/paper/278ed93dee876d8fa5e876e0fe0440bda222962a,Digital epidemiology and geographic mapping of G6PD deficiency: retrospective analytic of trends database existing on the surface web,"Background: Glucose-6-phosphate dehydrogenase (G6PD) deficiency is an inherited X-linked recessive condition in which the body does not synthesise a sufficient quantity of the G6PD enzyme. Hemolytic anaemic episodes occur following exposure to some medications, foods, or even infections in G6PD-deficient individuals.Aims and Objectives: To assess the digital epidemiology as well as the geographic mapping of G6PD deficiency in the world including countries of the Mediterranean basin.Materials and Methods: This study is primarily based on a digital epidemiological analysis and geographic mapping based on data retrieved from Google Trends database, a very large database, existing on the surface web. A retrospective analysis was carried out for the period from the 22nd of March 2015 to the 24th of January 2016. Trends data will also be contrasted with a collateral data set of a cross-sectional analytic for the same period which was conducted based on the exploration of cases of G6PD deficiency admitted to Jordan University Hospital.Results: Concerning geographic mapping, countries of the Basin of the Mediterranean Sea and the Arabian Gulf accounted for one-third of the entire geographic map at 15.66% and 18.18% respectively. Countries of the Mediterranean basin included Jordan, Italy, Lebanon, Israel, Egypt, Greece, Syria, Cyprus, Tunisia, Algeria, and Morocco. Contributing countries surrounding the Arabian Gulf included Kuwait, Kingdom of Saudi Arabia, Bahrain, Iran, United Arab Emirates, Qatar, and Iraq. The Hashemite Kingdom of Jordan contributed to 2.78% of the global map which accounted for 17.74% of the entire Mediterranean basin. Concerning digital epidemiology, the prevalence was recorded as highest during May 2015 (12.10%), August 2015 (11.17%), and November 2015 (11.28%). Concerning the percentile contribution of monthly records, data were in harmony for those cases admitted to hospital and signals recorded via Google Trends. Both datasets averaged a percentile contribution of approximately 9% per month.Conclusion: This study is the first inferential research on G6PD deficiency based on data from a trends database and parallel data from Jordan University Hospital. Ambitious future research should deploy the use of machine learning for real-time and predictive analytics which will provide an excellent value for public health services and epidemiological studies.Asian Journal of Medical Sciences Vol.9(5) 2018 57-61",Asian Journal of Medical Sciences,2018.0,10.3126/AJMS.V9I5.20495,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d85e0d876fa2a02241b665087ccd7084f3843cf1,https://www.semanticscholar.org/paper/d85e0d876fa2a02241b665087ccd7084f3843cf1,Evaluation of the extent of integration of humanoids in tackling social-economic challenges confronting the Africa continent,"The sustenance and growth of any nation, depends critically and essentially on her level of development. A country seen as developed when is able to provide qualitative life for her citizenry.1 The African continent amidst her huge human, material and natural resources has been battling with the problems of development even after many of the countries in the region has gained her independence. Economic Researchers have overtime attributed this challenge to factors like: Inadequate executive capacity responsible for the formulation and implementation of developmental policies, Lack of good governance, High level of corruption and indiscipline and mono-economic base of most countries in the region. However, European and Asian countries experience enviable growth and development patterns attributable to : the development of agricultural sector, improved mass education of her citizenry, development of indigenous industries, export oriented strategy, disciplined leadership style, existence of efficient bureaucracy human resources development, encouragement of a dynamic private sector working in cooperation with the government towards a society-wide vision of development, institutional capacity building and attention to the problems of governance, consistency and policy stability.2–4 It is said that human and technology go hand in hand. There have been rapid advances in technology in most western world and this has led to a surge of both government and public interest in automation and robotics.5–7 Robotics, which is an out spring from Mechatronics, has overtime evolved due to advances in Control Technologies and Computer Science. The rapid progress in innovative Sensors and Actuators technologies and breakthroughs in Data Science specifically in the area of Artificial Intelligence (AI) and Machine Learning (ML) have resulted in the development of Intelligent Robots.8 It is believed that the future would be a place where man and robots cohabit for the realization of man’s all round needs. In recent times, various kinds of practical assistive robots have been developed to suits mans need in different fields. For instance in the medical and welfare fields, exoskeleton power assist robots, assistive manipulators, robotic beds, and intelligent wheelchairs have been developed to cater for critical challenges in these areas by providing direct and physical assist either disabled people or their helpers.9 Several robots have been developed for the Industries, Educational sector, Aviation and Transport sectors, Commercial sectors etc. The quest to build a human like robot led to the development of Humanoids. This kind of robots which is seen as the most advanced creature of Man is envisioned to be the most ideal assistants and companion of human beings. This set of robots can interact with man and carry out functions independently. Humanoids are believed to be more efficient and effective than man in some areas as they only a stable source of power to function. As such they can be deployed to undertaking physically and mentally challenging routines.10 Humans in different situations need companion at different point in time. Cases abound where man is not able to get companion or counsel from other humans immediately when sort. This could be due to the fact that they are far away off/out of communication or they are no more. Thus, humanoids could fit in to meet this need as they communicate in a socially intelligent manner, recognizes and can learn human behaviors and psychology.11–13 Many western countries have embraces the use of Automation and Robotics in ameliorating her development challenges as: Robots have increased their productivity and competitiveness, increased productivity has lead to increased demand thus creating new job opportunities. Automation also has led overall to an increase in labor demand and positive impact on wages, and Robots complement and augment labor. Currently, the level of integration of these intelligent robots in both domestic and public environments of most third world countries like Africa has been very insignificant. This to an extent could be attributed to the Government polices; Religious and Cultural believe of the people. Most of the robots in the regions are mostly industrial .Very few available robots in the region are humanoids. Many have not been really integrated into public place and private homes. This is due to the fact that majority are of them are not built to adapt properly with the culture, religion and psychology of the people in the region. As such people find it difficult to accept these robots as companions and helpers in either homes, schools, hospitals, shopping malls, airports and other public places. If Africa is to fully embrace humanoids, there is an urgent need to build humanoids that are adaptive to the cultural, religious and psychological inclination of people in this region. To be able realize this; there is great need for more advanced research cognitive science/reasoning, adaptive learning and human",ICRA 2018,2018.0,10.15406/IRATJ.2018.04.00103,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
bd1a8094a14f68528d8910b7455f630e6d4cd36c,https://www.semanticscholar.org/paper/bd1a8094a14f68528d8910b7455f630e6d4cd36c,SIGLOG monthly 198,"Deadline: January 15th, 2018 Submission Deadline: January 22nd, 2018 Rebuttal: March 22 25th, 2018 Notification: April 2nd, 2018 Camera-Ready: May 2nd, 2018 FSCD Conference: July 9 12th, 2018 FLoC Conference: July 6 19th, 2018 * PROGRAM COMMITTEE CHAIR Helene Kirchner, Inria * CONFERENCE & WORKSHOP CHAIR: Paula Severi, Leicester U. 13TH WORKSHOP ON GAMES FOR LOGIC AND PROGRAMMING LANGUAGES (GaLoP 2018) Call for Abstracts Thessaloniki, Greece, 14-15 April, 2018 http://www.gamesemantics.org * GaLoP is an annual international workshop on game-semantic models for logics and programming languages and their applications. This is an informal workshop that welcomes work in progress, overviews of more extensive work, programmatic or position papers and tutorials. * GaLoP XII will be held in Thessaloniki, Greece, on 14-15 April 2018 as a satellite workshop of ETAPS (http://www.etaps.org/). * IMPORTANT DATES Submission: 22 January 2018 Notification: 12 February 2018 Workshop: 14-15 April 2018 * Invited talks Guy McCusker (Bath) Matteo Mio (Lyon) Ulrich Schopp (Munchen) * Programme Chairs Ugo Dal Lago (Bologna, co-chair) Gabriel Sandu (Helsinki, co-chair) 4th ACM CYBER-PHYSICAL SYSTEM SECURITY WORKSHOP (ACM CPSS’18) Incheon, Korea, June 4, 2018 (in conjunction with ACM AsiaCCS’18) http://jianying.5gbfree.com/cpss/CPSS2018/ Extended submission deadline: Jan 27, 2018 (23:59 GMT) * CONFERENCE OUTLINE Cyber-Physical Systems (CPS) consist of large-scale interconnected systems of heterogeneous components interacting with their physical environments. There are a multitude of CPS devices and applications being deployed to serve critical functions in our lives. This workshop will provide a platform for professionals from academia, government, and industry to discuss how to address the increasing security challenges facing CPS. Besides invited talks, we also seek novel submissions describing theoretical and practical security solutions to CPS. Papers that are pertinent to the security of ACM SIGLOG News 37 January 2018, Vol. 5, No. 1 embedded systems, IoT, SCADA, smart grid, and critical infrastructure networks are all welcome. * IMPORTANT DATES Extended submission deadline: Jan 27, 2018 (23:59 GMT) Notification: Mar 10, 2018 Camera-ready due: Mar 31, 2018 * Program Chairs: Dieter Gollmann (Hamburg University of Technology, Germany & NTU, Singapore) Jianying Zhou (SUTD, Singapore) * Further information CPSS Home: http://jianying.5gbfree.com/cpss/ Email: cpss2018@easychair.org CONTINUITY, COMPUTABILITY, CONSTRUCTIVITY: FROM LOGIC TO ALGORITHMS 2017 Second Call for Submissions (Postproceedings) Deadline for submission: 1 February 2018 * After the successful start of the new EU-MSCA-RISE project ""Computing with Infinite Data"" (CID) and the excellent Workshop CCC 2017 in Nancy (France) in June this year, we are planning to publish a collection of papers dedicated to the meeting and to the project as a Special Issue in the open-access journal LOGICAL METHODS IN COMPUTER SCIENCE. * The issue should reflect progress made in Computable Analysis and related areas, and is not restricted to work in the CID project or presented at the Workshop. * Submissions are welcome from all scientists on topics in the entire spectrum from logic to algorithms including, but not limited to: Exact real number computation, Correctness of algorithms on infinite data, Computable analysis, Complexity of real numbers, real-valued functions, etc. Effective descriptive set theory, Constructive topological foundations, Scott’s domain theory, Constructive analysis, Category-theoretic approaches to computation on infinite data, Weihrauch degrees, Randomness and computable measure theory, Other related areas. * EDITORS: Ulrich Berger (Swansea, UK) Pieter Collins (Maastricht, NL) Mathieu Hoyrup (Nancy, FR) Victor Selivanov (Novosibirsk, RUS) Dieter Spreen (Siegen, DE) Martin Ziegler (KAIST, KR) * DEADLINE FOR SUBMISSION: 1 February 2018 * If you intend to submit a paper for the special issue, please inform us by sending email to: spreen@math.uni-siegen.de by 1 January 2018 COMPUTABILITY IN EUROPE: SAILING ROUTES IN THE WORLD OF COMPUTATION (CiE 2018) Final Call for Papers ACM SIGLOG News 38 January 2018, Vol. 5, No. 1 Kiel, Germany July 30 August 3, 2018 http://cie2018.uni-kiel.de * CiE 2018 is the fourteenth conference organized by CiE (Computability in Europe), a European association of mathematicians, logicians, computer scientists, philosophers, physicists and others interested in new developments in computability and their underlying significance for the real world. * THE PROGRAMME COMMITTEE cordially invites all researchers (European and non-European) to submit their papers in all areas related to computability for presentation at the conference and inclusion in the proceedings at https://easychair.org/conferences/?conf=cie2018 Submission guidelines are available on the conference web-site. * The CONFERENCE PROCEEDINGS will be published by LNCS, Springer Verlag. * IMPORTANT DATES: Deadline for abstract submission: January 17, 2018 Deadline for article submission: February 1, 2018 Notification of acceptance: April 6, 2018 Early registration before: May 30, 2018 * TUTORIAL SPEAKERS: Pinar Heggernes (Bergen, Norway), Bakhadyr Khoussainov (Auckland, NZ) * INVITED SPEAKERS: Kousha Etessami (Edinburgh, UK) Johanna Franklin (Hempstead, US) Mai Gehrke (Nice, France) Alberto Marcone (Udine, Italy) Alexandra Silva (London, UK) Jeffrey O. Shallit (Waterloo, Canada) * SPECIAL SESSIONS: Approximation and Optimisation, Bioinformatics and Bio-inspired Computing, Computing with Imperfect Information, Continuous Computation, History and Philosophy of Computing, SAT-Solving. * WORKSHOP: Women in Computability. * Check the web-site for further details on the conference, such as organisation and grants. SPECIAL ISSUE OF IJAR ON ""DEFEASIBLE AND AMPLIATIVE REASONING"" Call for Papers * Classical reasoning is not flexible enough when directly applied to the formalization of certain nuances of decision making as done by humans. These involve different kinds of reasoning such as reasoning with uncertainty, exceptions, similarity, vagueness, incomplete or contradictory information and many others. * Everyday reasoning usually shows the two salient intertwined aspects below: Ampliative aspect: augmenting the underlying reasoning by allowing more conclusions. Defeasible aspect: curtailing the underlying reasoning by either disregarding or disallowing some conclusions that somehow ought not ACM SIGLOG News 39 January 2018, Vol. 5, No. 1 to be sanctioned. * This special issue aims at bringing together work on defeasible and ampliative reasoning from the perspective of artificial intelligence, cognitive sciences, philosophy and related disciplines in a multi-disciplinary way, thereby consolidating the mission of the DARe workshop series. * The submission url is: http://www.evise.com/evise/jrnl/IJA * When submitting your manuscript, please select ‘‘VSI:DARe special issue’’ as the article type. * If you have any enquiries, please feel free to contact us at dare.to.contact.us@gmail.com * IMPORTANT DATES Submission deadline: 15 February 2018 Notification: 1 November 2018 Publication date: 1 January 2019 * Guest editors Richard Booth, Cardiff University, UK Giovanni Casini, University of Luxembourg Szymon Klarman, Semantic Integration Ltd., UK Gilles Richard, Universite Paul Sabatier, France Ivan Varzinczak, CRIL, Univ. Artois & CNRS, France THE 17TH IEEE INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS AND COGNITIVE COMPUTING (ICCI*CC’18) UC Berkeley, CA, USA July 15-18, 2018 http://www.ucalgary.ca/icci_cc/iccicc-18 * The IEEE ICCI*CC series is a flagship conference of its field sponsored by IEEE Computer, Computational Intelligence and SMC Societies. The theme of ICCI*CC’18 is on Cognitive Machine Learning, Brain-Inspired Systems and Cognitive Robotics. * You are welcome to submit a paper to IEEE ICCI*CC’18 or to organize a special session related to the theme of the conference. The Proceedings of ICCI*CC’18 will be published by IEEE CS Press (EI Indexed). A good rate of selected papers from the proceedings will be recommended to leading international journals and/or IEEE transactions with ISI/EI indexes. * IMPORTANT DATES Submission deadline: February 16, 2018. 25th WORKSHOP ON LOGIC, LANGUAGE, INFORMATION AND COMPUTATION (WoLLIC 2018) Call for Papers July 24th-27th, 2018, Bogota, Colombia http://wollic.org/wollic2018/ * WoLLIC is an annual international forum on inter-disciplinary research involving formal logic, computing and programming theory, and natural language and reasoning. Each meeting includes invited talks and tutorials as well as contributed papers. * Contributions are invited on all pertinent subjects, with particular ACM SIGLOG News 40 January 2018, Vol. 5, No. 1 interest in cross-disciplinary topics. Typical but not exclusive areas of interest are: foundations of computing and programming; novel computation models and paradigms; broad notions of proof and belief; proof mining, type theory, effective learnability; formal methods in software and hardware development; logical approach to natural language and reasoning; logics of programs, actions and resources; foundational aspects of information organization, search, flow, sharing, and protection; foundations of mathematics; philosophy of mathematics; philosophy of language; philosophical logic. * IMPORTANT DATES: Mar 11, 2018: Paper title and abstract deadline Mar 18, 2018: Full paper deadline Apr 15, 2018: Author notification Apr 22, 2018: Final version deadline (firm). * Further details: http://wollic.org/wollic2018/ WOMEN IN LOGIC WORKSHOP (WiL 2018) Call for Papers July 8, 2018, Oxford UK https://sites.google.com/site/womeninlogic2018/welcome/ * Affiliated with LICS (http://lics.siglog.org/lics18/) Held as part of FLoC (http://www.floc2018.org/) * We are holding the 2nd Women in Logic (WiL) workshop as a LICS 2018 associated w",SIGL,2018.0,10.1145/3183645.3183651,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c97fd176f5cfb75066b78324c4b7f7c0912ee2d6,https://www.semanticscholar.org/paper/c97fd176f5cfb75066b78324c4b7f7c0912ee2d6,3 Real time intelligent methods for energy-efficient approaches in CNN and biomedical applications,"Bio-medical image processing in the field of telemedicine, and in particular the definition of systems that allow medical diagnostics in a collaborative and distributed way is experiencing an undeniable growth. Due to the high quality of bio-medical videos and the subsequent large volumes of data generated, to enable medical diagnosis onthe-go it is imperative to efficiently transcode and stream the stored videos on real time, without quality loss. However, online video transcoding is a high-demanding computationally-intensive task and its efficient management in Multiprocessor Systems-on-Chip (MPSoCs) poses an important challenge. In this work we propose an efficient motionand texture-aware frame-level parallelization approach to enable online medical imaging transcoding on MPSoCs for next generation video encoders. By exploiting the unique characteristics of bio-medical videos and the medical procedure that enable diagnosis, we split frames into tiles based on their motion and texture, deciding the most adequate level of parallelization. Then, we employ the available encoding parameters to satisfy the required video quality and compression. Moreover, we propose a new fast motion search algorithm for bio-medical videos that allows to drastically reduce the computational complexity of the encoder, thus achieving the frame rates required for online transcoding. Finally, we heuristically allocate the threads to the most appropriate available resources and set the operating frequency of each one. We evaluate our work on an enterprise multi-core server achieving online medical imaging with 1.6x higher throughput and 44% less power consumption when compared to the state-of-the-art techniques. Download Paper (PDF; Only available from the DATE venue WiFi) 17:30 8.3.2 HIGHLY EFFICIENT AND ACCURATE SEIZURE PREDICTION ON CONSTRAINED IOT DEVICES Speaker: Farzad Samie, Karlsruhe Institute of Technology (KIT), DE Authors: Farzad Samie, Sebastian Paul, Lars Bauer and Joerg Henkel, Karlsruhe Institute of Technology, DE Abstract In this paper we present an efficient and accurate algorithm for epileptic seizure prediction on low-power and portable IoT devices. State-of-the-art algorithms suffer from two issues: computation intensive features and large internal memory requirement, which make them inapplicable for constrained devices. We reduce the memory requirement of our algorithm by reducing the size of data segments (i.e. the window of input stream data on which the processing is performed), and the number of required EEG channels. To respect the limitation of the processing capability, we reduce the complexity of our exploited features by only considering the simple features, which also contributes to reducing the memory requirements. Then, we provide new relevant features to compensate the information loss due to the simplifications (i.e. less number of channels, simpler features, shorter segment, etc.). We measured the energy consumption (12.41 mJ) and execution time (565 ms) for processing each segment (i.e. 5.12 seconds of EEG data) on a low-power MSP432 device. Even though the state-of-art does not fit to IoT devices, we evaluate the classification performance and show that our algorithm achieves the highest AUC score (0.79) for the held-out data and outperforms the state-of-the-art. Download Paper (PDF; Only available from the DATE venue WiFi) 18:00 8.3.3 A WEARABLE LONG-TERM SINGLE-LEAD ECG PROCESSOR FOR EARLY DETECTION OF CARDIAC ARRHYTHMIA Speaker: Muhammad Awais Bin Altaf, Lahore University of Management Sciences (LUMS), PK Authors: Syed Muhammad Abubakar, Wala Saadeh and Muhammad Awais Bin Altaf, Lahore University of Management Sciences (LUMS), PK Abstract Cardiac arrhythmia (CA) is one of the most serious heart diseases that lead to a very large number of annual casualties around the world. The traditional electrocardiography (ECG) devices usually fail to capture arrhythmia symptoms during patients' hospital visits due to their recurrent nature. This paper presents a wearable long term single-lead ECG processor for the CA detection at an early stage. To achieve on-sensor integration and long-term continuous monitoring, an ultra-low complexity feature extraction engine using reduced feature set of four (RFS4) is proposed. It reduces the area by >25% compared to the conventional QRS complex detection algorithms without compromising the accuracy. Moreover, RFS4 eliminates the need for complex machine learning decision logics for the detection of premature ventricular contraction (PVC) and nonsustained ventricular tachycardia (NVT). To ensure correct functional verification, the proposed system is implemented on FPGA and tested using the MIT-BIH ECG arrhythmia database. It achieves a sensitivity and specificity of 94.64% and 99.41%, respectively. The proposed processor is also synthesized using 0.18um CMOS technology with an overall energy efficiency of 139 nJ/detection. Download Paper (PDF; Only available from the DATE venue WiFi) 18:15 8.3.4 DRONET: EFFICIENT CONVOLUTIONAL NEURAL NETWORK DETECTOR FOR REAL-TIME UAV APPLICATIONS Speaker: Christos Kyrkou, University of Cyprus, KIOS CoE, CY Authors: Christos Kyrkou1, George Plastiras1, Stylianos Venieris2, Theocharis Theocharides1 and Christos Bouganis2 1University of Cyprus, CY; 2Imperial College London, GB Abstract Unmanned Aerial Vehicles (drones) are emerging as a promising technology for both environmental and infrastructure monitoring, with broad use in a plethora of applications. Many such applications require the use of computer vision algorithms in order to analyse the information captured from an on-board camera. Such applications include detecting vehicles for emergency response and traffic monitoring. This paper therefore, explores the trade-offs involved in the development of a single-shot object detector based on deep convolutional neural networks (CNNs) that can enable UAVs to perform vehicle detection under a resource constrained environment such as in a UAV. The paper presents a holistic approach for designing such systems; the data collection and training stages, the CNN architecture, and the optimizations necessary to efficiently map such a CNN on a lightweight embedded processing platform suitable for deployment on UAVs. Through the analysis we propose a CNN architecture that is capable of detecting vehicles from aerial UAV images and can operate between 5-18 frames-per-second for a variety of platforms with an overall accuracy of ~95%. Overall, the proposed architecture is suitable for UAV applications, utilizing low-power embedded processors that can be deployed on commercial UAVs. Download Paper (PDF; Only available from the DATE venue WiFi)Unmanned Aerial Vehicles (drones) are emerging as a promising technology for both environmental and infrastructure monitoring, with broad use in a plethora of applications. Many such applications require the use of computer vision algorithms in order to analyse the information captured from an on-board camera. Such applications include detecting vehicles for emergency response and traffic monitoring. This paper therefore, explores the trade-offs involved in the development of a single-shot object detector based on deep convolutional neural networks (CNNs) that can enable UAVs to perform vehicle detection under a resource constrained environment such as in a UAV. The paper presents a holistic approach for designing such systems; the data collection and training stages, the CNN architecture, and the optimizations necessary to efficiently map such a CNN on a lightweight embedded processing platform suitable for deployment on UAVs. Through the analysis we propose a CNN architecture that is capable of detecting vehicles from aerial UAV images and can operate between 5-18 frames-per-second for a variety of platforms with an overall accuracy of ~95%. Overall, the proposed architecture is suitable for UAV applications, utilizing low-power embedded processors that can be deployed on commercial UAVs. Download Paper (PDF; Only available from the DATE venue WiFi) 18:30 IP315, 889 ERROR RESILIENCE ANALYSIS FOR SYSTEMATICALLY EMPLOYING APPROXIMATE COMPUTING IN CONVOLUTIONAL NEURAL NETWORKS Speaker: Muhammad Abdullah Hanif, Vienna University of Technology, Vienna, AT Authors: Muhammad Abdullah Hanif1, Rehan Hafiz2 and Muhammad Shafique1 1TU Wien, AT; 2ITU, PK Abstract Approximate computing is an emerging paradigm for error resilient applications as it leverages accuracy loss for improving power, energy, area, and/or performance of an application. The spectrum of error resilient applications includes the domains of Image and video processing, Artificial intelligence (AI) and Machine Learning (ML), data analytics, and other Recognition, Mining, and Synthesis (RMS) applications. In this work, we address one of the most challenging question, i.e., how to systematically employ approximate computing in Convolutional Neural Networks (CNNs), which are one of the most compute-intensive and the pivotal part of AI. Towards this, we propose a methodology to systematically analyze error resilience of deep CNNs and identify parameters that can be exploited for improving performance/efficiency of these networks for inference purposes. We also present a case study for significance-driven classification of filters for different convolutional layers, and propose to prune those having the least significance, and thereby enabling accuracy vs. efficiency tradeoffs by exploiting their resilience characteristics in a systematic way. Download Paper (PDF; Only available from the DATE venue WiFi)Approximate computing is an emerging paradigm for error resilient applications as it leverages accuracy loss for improving power, energy, area, and/or performance of an application. The spectrum of error resilient applications includes the domains of Image and video processing, Artificial intelligence (AI) and Machine Learning (ML), data analytics, and oth",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
3a6c165565e0df0ec97609c24f7e2b025505076a,https://www.semanticscholar.org/paper/3a6c165565e0df0ec97609c24f7e2b025505076a,AI Infused Fragrance Systems for Creating Memorable Customer Experience and Venue Brand Engagement,,IHSI,2018.0,10.1007/978-3-319-73888-8_47,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
392ab2bf4e1ad37d55a55cdfffbd59d49c58940d,https://www.semanticscholar.org/paper/392ab2bf4e1ad37d55a55cdfffbd59d49c58940d,Assessing and managing Parkinson's disease from home: A 21st century vision closer to reality,"The application of technology to Parkinson’s disease (PD) holds a considerable potential to monitor disease features in clinical care and research, guide management, and as a therapeutic tool. In spite of this, digital health technology solutions have failed so far to fundamentally change how patients report their symptoms, physicians collect clinical information about their patients between clinical visits, or how we document the efficacy of novel experimental interventions in clinical trials. Zhan and colleagues provide an example of how such promises of a digital technology revolution may shape the near future. The Mobile Parkinson Disease Score (mPDS) is a summary measure of the severity of PD motor features generated solely by a smartphone-based application used remotely by patients. The use of a smartphone confers to the mPDS the unique ability to provide a rapid, remote, frequent, and objective assessment, which could inform care decisions, and allow for more patients with PD to take part in clinical trials, by reducing (though not expected to eliminate) the burden of in-person visits in a trial. To develop the mPDS, the researchers started by establishing its item structure from a pool of 435 unique features extracted from five tasks done on a smartphone in a development cohort of 129 PD patients. The final 15-item version of the mPDS was tested clinimetrically in a small group of PD patients (n = 23). The mPDS had a good-to-excellent correlation with well-validated clinical rating scales in PD (total MDS-UPDRS and corresponding part III, H & Y scale) and was sensitive to change in onversus off-dopaminergic medication conditions. This article provides a critical proof-of-concept study for the validation and deployment of a digital motor scale in PD, in contrast with other recent studies that explored isolated motor features using similar smartphone-based testing. The researchers adopted novel data analyses methods involving machine-learning that challenge the language and methods most clinical researchers know. Once a distant reality, these approaches are no longer science fiction and call for a novel set of skills by the clinical researcher for an appropriate critical appraisal and use. Providing a more home-based assessment is an intuitive example of how the use of technology can revolutionize PD care in the 21st century. Although accessibility is well addressed with the use of widely available smartphones some PD patients may not be tech savvy and may feel challenged by the small dimensions of a smartphone. Also, the study sample was not representative of a broad PD population, which raises concerns about the usability and compliance of the mPDS. These features need to be adequately assessed, given that these will likely determine its successful uptake. Co-design approaches that incorporate patient and care partners views from inception to full development of a health technology solution may be beneficial in the preparation of a future larger study for a more comprehensive validation of this promising digital measure in PD.",Movement disorders : official journal of the Movement Disorder Society,2018.0,10.1002/mds.112,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b4076733cf5d7b366ac33b0fbc3e06cc296cc652,https://www.semanticscholar.org/paper/b4076733cf5d7b366ac33b0fbc3e06cc296cc652,"2018 International Conference on Cloud Computing and Internet of Things ( CCIOT 2018 ) October 29-31 , 2018","The IANA central IPv4 address space has been fully depleted back in February 2011 making the deploying of new large-scale networks especailly IoT networks not scalable and not what IoT really stands for. Hence the new IP protocol version 6 (IPv6) has been designed to cater for this already back in the 90s and waiting for its killer apps to take off. 4G was the first one to adopt IPv6 in larger scale.The IPv6 Deployment worldwide is becoming a reality now with some countries achieving more than 50 % user penetration, with Belgium (58%) at the top ranking (http://labs.apnic.net/dists/v6dcc.html ) and reaching double digits v6 coverage on Google IPv6 stats. Many Autonomous Networks (ASN) reach more than 50% with v6 preferred or v6 capable penetration: (http://labs.apnic.net/ipv6-measurement/Economies/US/). Over 500 Million users are accessing the Internet over IPv6 and probably not even knowing it. The US was by far the biggest adopter of IPv6 with some 100 Million IPv6 users, but India has surpassed the US with over 250 M IPv6 users, followed by Germany, Japan and China with some 20 + M v6 users. Worldwide IPv6 deployment has passed the 20 % Google usage bar doubling every 12 months (http://www.google.com/intl/en/ipv6/statistics.html). If this trend continues, we should achieve 50% by 2020 which would be the inflection point when the full roll-out of IPv6 becomes a strategic plumbing decision of the networks, a topic that is avoided so far due to many strategic and resources issues (lack of top management decision-making, lack of v6 skilled engineers and v6 deployment best practices, very limited ISP v6 access deployment, ..). The deployment of Carrier-grade NAT is in full swing making networking and user experience more brittle converting IoT to NAT-IoT (InterNAT of Things). IPv6 will kick in big time for IoT to take it to the next level which is “Things-to-Things” beyond the current network of things under the non-IP IoT umbrella as Kevin Ashton coined the term IoT for RFID back in 1990 before even RFID suported the IP stack and still today don’t. This is another technology myth or fake news. IoT will suffer immensely under lack of built-in security which together cybersecurity issues are like always brushed over at this stage due mainly to lack of IPv6 security skills. New topics are more on the lime light such as Cloud Computing, SDN, NFV, 5G with no attention to the issues dragged by IPv4. These fields are taking IP networking for granted designing them on IPv4/NAT building non-scalable and non-end to end solutions. The IPv6 Forum is driving new initiatives to garner support and create awareness on the impact of IPv6 on topics such as real IoT, open Cloud Computing, openstack based SDN-NFV and IPv6 only 5G. 2018 International Conference on Cloud Computing and Internet of Things 9 Keynote Speaker II Prof. Xudong Jiang Nanyang Technological University, Singapore Prof. Xudong Jiang received the B.Sc. and M.Sc. degree from the University of Electronic Science and Technology of China, in 1983 and 1986, respectively, and received the Ph.D. degree from Helmut Schmidt University Hamburg, Germany in 1997, all in electrical and electronic engineering. From 1986 to 1993, he worked as Lecturer at the University of Electronic Science and Technology of China where he received two Science and Technology Awards from the Ministry for Electronic Industry of China. He was a recipient of the German Konrad-Adenauer Foundation young scientist scholarship. From 1993 to 1997, he was with Helmut Schmidt University Hamburg, Germany as scientific assistant. From 1998 to 2004, He worked with the Institute for Infocomm Research, A*Star, Singapore, as Senior Research Fellow, Lead Scientist and appointed as the Head of Biometrics Laboratory where he developed a fingerprint verification algorithm that achieved the fastest and the second most accurate fingerprint verification in the International Fingerprint Verification Competition (FVC2000). He joined Nanyang Technological University, Singapore as a faculty member in 2004 and served as the Director of the Centre for Information Security from 2005 to 2011. Currently, Dr Jiang is a tenured Associate Professor in School of Electrical and Electronic Engineering, Nanyang Technological University. Dr Jiang has published over hundred research papers in international refereed journals and conferences, some of which are well cited on Web of Science. He is also an inventor of 7 patents (3 US patents), some of which were commercialized. Dr Jiang is a senior member of IEEE and has been serving as Editorial Board Member,Guest Editor and Reviewer of multiple international journals, and serving as Program Committee Chair, Keynote Speaker and Session Chair of multiple international conferences. His research interest includes pattern recognition, computer vision, machine learning, image analysis, signal/image processing, machine learning and biometrics. 2018 International Conference on Cloud Computing and Internet of Things 10 Topic: “Machine Learning from Data: from Neural Networks to Deep Learning” Abstract: Discovering knowledge from data has many applications in various artificial intelligence (AI) systems. Machine learning from the data is a solution to find right information from the high dimensional data. It is thus not a surprise that learning-based approaches emerge in various AI applications. The powerfulness of machine learning was already proven 30 years ago in the boom of neural networks but its successful application to the real world is just in recent years after the deep convolutional neural networks (CNN) have been developed. This is because the machine learning alone can only solve problems in the training data but the system is designed for the unknown data outside of the training set. This gap can be bridged by regularization: human knowledge guidance or interference to the machine learning. This speech will analyze these concepts and ideas from traditional neural networks to the very hot deep CNN neural networks. It will answer the questions why the traditional neural networks fail to solve real world problems even after 30 years’ intensive research and development and how the deep CNN neural networks solve the problems of the traditional neural networks and now are very successful in solving various real world AI problems. Discovering knowledge from data has many applications in various artificial intelligence (AI) systems. Machine learning from the data is a solution to find right information from the high dimensional data. It is thus not a surprise that learning-based approaches emerge in various AI applications. The powerfulness of machine learning was already proven 30 years ago in the boom of neural networks but its successful application to the real world is just in recent years after the deep convolutional neural networks (CNN) have been developed. This is because the machine learning alone can only solve problems in the training data but the system is designed for the unknown data outside of the training set. This gap can be bridged by regularization: human knowledge guidance or interference to the machine learning. This speech will analyze these concepts and ideas from traditional neural networks to the very hot deep CNN neural networks. It will answer the questions why the traditional neural networks fail to solve real world problems even after 30 years’ intensive research and development and how the deep CNN neural networks solve the problems of the traditional neural networks and now are very successful in solving various real world AI problems. 2018 International Conference on Cloud Computing and Internet of Things 11 Keynote Speaker III Prof. Dimitrios Georgakopoulos Swinburne University of Technology, Australia Prof. Georgakopoulos is the Director of the Key IoT Lab at the Digital Innovation Platform of Swinburne University of Technology. Before that was Research Director at CSIRO’s ICT Centre and Executive Director of the Information Engineering Laboratory, which was the largest Computer Science program in Australia. Before CSIRO, he held research and management positions in several industrial laboratories in the US, including Telcordia Technologies (where he helped found two of Telcordia’s Research Centers in Austin, Texas, and Poznan, Poland); Microelectronics and Computer Corporation (MCC) in Austin, Texas; GTE (currently Verizon) Laboratories in Boston, Massachusetts; and Bell Communications Research in Piscataway, New Jersey. He was also a full Professor at RMIT University, and he is currently an Adjunct Prof. at the Australian National University and a CSIRO Adjunct Fellow. Prof. Georgakopoulos is an internationally known expert in IoT, process management, and data management. He has received 20+ industry and academic awards. His 170+ journal and conference publications, which include three seminal papers in the areas Service Computing, Workflow Management, Context Management for the Internet of Things (IoT), have received 12,400+ citations. Dimitrios’ research has attracted significant external research funding ($35M+) from various industry and government research funding agencies, ranging from DARPA and ARDA in the USA, to the Framework Program in the EU, to the Department of Human Services and 50+ industry partners in Australia. 2018 International Conference on Cloud Computing and Internet of Things 12 Topic: “From Internet Scale Sensing to Smart Services and Products” Abstract: The Internet of Things (IoT) is the latest Internet evolution that incorporates billions of Internet-connected devices that range from cameras, sensors, RFIDs, smart phones, and wearables, to smart meters, vehicles, medication pills, and industrial machines. Such IoT things are often owned by different organizations and people who are deploying and using them for their own purposes. Federations of such IoT devices (often referred to as IoT things) can also deliver timely and accurat",,2018.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
fbad4dbfff0d8b675af74a86e7e6bb9c389ae76b,https://www.semanticscholar.org/paper/fbad4dbfff0d8b675af74a86e7e6bb9c389ae76b,A Computing Perspective on Smart City,"DEVELOPING smart city is the key to the next generation urbanization process for improving the efficiency, reliability, and security of a traditional city. The concept of smart city includes various aspects such as environmental sustainability, social sustainability, regional competitiveness, natural resources management, cybersecurity, and quality of life improvement. With the massive deployment of networked smart devices/sensors, an unprecedentedly large amount of sensory data can be collected and processed by advanced computing paradigms, which are the enabling techniques for smart city. For example, given historical environmental, population, and economic information, salient modeling and analytics are needed to simulate the impact of potential city planning strategies, which will be critical for intelligent decision-making. Analytics are also indispensable for discovering the underlying structure from retrieved data in order to design the optimal policies for real time automatic control in the cyberphysical smart city system. Furthermore, uncertainties and security concerns in the data collected from heterogeneous resources aggravate the problem, which makes smart city planning, operation, monitoring, and control highly challenging. Green, sustainable, and secure computing in smart cities has recently become a very active area of research in academia and has attracted significant industry interest. Since the computing issues for smart cities are highly interdisciplinary and cover various topics, a special section of Smart City Computing in the IEEE Transactions on Computers becomes an ideal forum for presenting and discussing the latest research results. The goal of this special section is to present the outstanding research results dedicated to the topics of green, sustainable, and secure computing for smart cities. We have received 40 manuscript submissions in total and six papers have finally been accepted after several rounds of very constructive and deep reviews. The smart city brings convenience to users through providing personalized yet efficient services. However, it also introduces privacy and security issues. Two different methods are used to counteract these issues. The active method is to prevent the overcollection of private data (Dai et al.), while the passive method is to make private data more secure via certain encryption algorithms (Zhang et al.). Dai et al. study the current state of data overcollection and look at some of the most frequent cases of overcollected data. They present a mobile cloud framework, which is an active approach to eradicate data overcollection. Through putting all users’ data into a cloud, the security of the data can be greatly improved. Zhang et al. use the BGV encryption scheme to encrypt the private data and employ cloud servers to perform the high-order back-propagation algorithm on the encrypted data efficiently for deep computation model training. Furthermore, their proposed scheme approximates the sigmoid function as a polynomial function to support the secure computation of the activation function with the BGV encryption. Data infrastructures play an important role in smart city computing. Two interesting topics are included in this special section, which are wireless sensor network for data collection (Santos et al.) and spatial-temporal database for data storage (Ding et al.), respectively. Santos et al. propose a decentralized algorithm for detecting damage in structures using a WSAN. As key characteristics, beyond presenting a fully decentralized (in-network) and collaborative approach for detecting damage in structures, the algorithm makes use of cooperative information fusion for calculating a damage coefficient, which represents frequency and amplitude shifts simultaneously. Ding et al. propose the Parallel-Distributed Network-constrained Moving Objects Database (PD-NMOD), a general framework that manages big trajectory data in a scalable manner. The PD-NMOD provides an infrastructure that is able to support a wide variety of smart transportation applications, thus benefiting the smart city vision as a whole. Furthermore, two interesting applications are selected to instantiate the effect of smart city computing, which includes smart grid (Wei et al.) and smart railway transportation (Huang et al.). Concerning the application domain of the smart grid, Huang et al. propose a comprehensive framework to integrate the operations of smart buildings into the energy scheduling of bulk power systems through proactive building demand participation. This new scheme enables buildings to proactively express and communicate their energy consumption preferences to smart grid operators rather than passively receive and react to market signals and instructions such as time varying electricity prices. For smart railway transportation, Huang et al. propose an energy-efficient train control framework through integrating both offline and onboard optimization techniques. The offline processing builds a decision-tree based sketchy solution through a complete flow of sequence mining, optimization, and machine learning. The onboard system feeds the train parameters into the decision tree to derive an optimized control sequence. For this special section of the IEEE Transactions on Computers we have selected the above very interesting papers to represent some important advances in smart city computing. As a conclusion, it should be noted that the research L. Wang (corresponding author) is with the School of Computer Science, China University of Geosciences, Wuhan, P.R. China. E-mail: Lizhe.Wang@computer.org. S. Hu is with the Michigan Technological University. G. Betis is with the EIT ICT Labs, France. R. Ranjan is with the University of Newcastle, United Kingdom.",,2016.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0741a20d6548866ee4f958783f10db98c60cb421,https://www.semanticscholar.org/paper/0741a20d6548866ee4f958783f10db98c60cb421,Measuring Code Quality to Improve Specification Mining,"Every software Industry requires the quality of code. Formal specifications are mathematically based techniques whose purposes are to help with the implementation of systems and software. They are used to describe a system, to analyze its behavior, and to aid in its design by verifying key properties of interest through rigorous and effective reasoning tools. These specifications are formal in the sense that they have syntax, their semantics fall within one domain, and they are able to be used to infer useful information. Formal specifications can help with program testing, optimization, refactoring. However, they are difficult to write manually, and automatic mining techniques suffer from 90–99% false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We measure code quality by extracting additional information from the software engineering process, and using information from code that is more likely to be correct as well as code that is less likely to be correct. Keywords— Specification mining, machine learning, software engineering, code metrics, program understanding INTRODUCTION Incorrect and buggy behavior in deployed software costs up to $70 billion each year in the US [7]. Thus debugging, testing, maintaining, optimizing, refactoring, and documenting software, while time-consuming, remain critically important. Such maintenance is reported to consume up to 90% of the total cost of software projects .A key maintenance concern is incomplete documentation up to 60% of maintenance time is spent studying existing software(e.g.,[8]). Human processes and especially tool support for finding and fixing errors in deployed software often require formal specifications of correct program behavior(e.g.,[9]); it is difficult to repair a coding error without a clear notion of what ―correct‖ program behavior entails. Unfortunately, while low-level program annotations are becoming more and more prevalent, comprehensive formal specifications remain rare. Many large, preexisting software projects are not yet formally specified. Formal program specifications are difficult for humans to construct .and incorrect specifications are difficult for humans to debug and modify. Accordingly, researchers have developed techniques to automatically infer specifications from program source code or execution traces [2]. These techniques typically produce specifications in the form of finite state machines that describe legal sequences of program behaviors. Fig.1.Block Diagram Input code for the Quality Measuring Quality Measure Software Output with Percenta ge of Input Code International Journal of Engineering Research and General Science Volume 3, Issue 3, Part-2 , May-June, 2015 ISSN 2091-2730 448 www.ijergs.org LITERATURE SURVEY API-based and Information Theoretic Metrics for Measuring the Quality of Software Modularization [10]. This system is developed using Object oriented software system. Create a set design principles for code modularization and produce set of metrics. Modularization quality is calculated using metrics such as structural, architectural and notions. There are three contributions such as coupling, cohesion and complexity metrics to modularize the software. This metrics seek to characterize a body of software according to the enunciated principles. Provide two types of experiments to validate the metrics. Whaley et al. propose a static miner [1] that produces a single multi-state specification for library code. The JIST[2] miner refines Whaley et al.’s static approach by using techniques from software model checking to rule out infeasible paths. Gabel and Su [3] extend Perracotta using BDDs, and show both that two-state mining is NP-complete and some specifications cannot be created by composing two-state specifications. Lo et al. use learned temporal properties, such as those mined in this article, to steer the learning of finite state machine behavior models [4].Shoham et al. [5] mine by using abstract interpretation, where the abstract values are specifications MINING CHARACTERISTICS This section shows the underlying concepts of mining techniques and their limitations which encourage the researchers to step into incorporating code quality metrics. Specification mining techniques produces specifications but still they have high false positive rates. The Comparison between most of these approaches is provided in the Table 1. In WN miner [6] the specification mining was motivated by the observations of run-time error handling mistakes. In other approaches examining such mistakes, the code frequently violates simple API specifications in exceptional situations. Despite the proliferation of specification-mining research, there is not much report on issues pertaining to the quality of specification miners. This technique is same as that of Engler et al. but is based on assumptions about run time errors, chooses candidate event pairs differently, presents significantly fewer candidate specifications and ranks presented candidates differently. In a normal Table1.A Comparison study execution, events „a’ and „b’ may be separated by other events and difficult to discern as a pair. After an error has occurred, however, the cleanup code is usually much less cluttered and contains only operations required for correctness. The candidate specifications are filtered using varied criteria such as exceptional control flow, one error, data path etc. This highlights the practical importance of the algorithmic assumptions, in particular the use of exceptional control flow. It can serve as a requirement for acceptance. It can even assist inspections by helping to target effort at parts of a program that may need improvement. Though this miner select specifications from software artifacts and finds per-program specifications for error detection, it does not have profound results in bug finding. Strauss, ECC and WN technique were all good at yielding specifications that found bugs. The WN technique found all bugs reported by other techniques on these benchmarks and did so with the fewest false positives. QUALITY METRICS Code metrics like LOC and Cyclomatic Complexity examines the internal complexity of a procedure whereas this structure metrics examines the relationship between a section of code and the rest of the system. Process oriented metrics are used through the different phases of the software life cycle. Measurement on quality should concentrate on the early phases in the life cycle to improve the quality of software and decrease of development and maintenance costs. Defects must be tracked to the release origin which is the portion of the code that contains the defects and at what release the portion was added, changed, or enhanced. When calculating the defect rate of the entire product, all defects are used; when calculating the defect rate for the new and changed code, only defects of the release origin of the new and changed code are included. On the one hand, the process quality metrics simply means tracking defect arrival during formal machine testing for some organizations. On the other hand, some software organizations with well-established software metrics programs cover various parameters in each phase of the development cycle. International Journal of Engineering Research and General Science Volume 3, Issue 3, Part-2 , May-June, 2015 ISSN 2091-2730 449 www.ijergs.org ACKNOWLEDGMENT I would like to express my sincere gratitude towards my guide Prof.Umesh Kulkarni for the help, guidance an encouragement in the development of this methodology. They supported me with scientific guidance, advice and encouragement, and were always helpful and enthusiastic and this inspired me in my work. I have benefitted from numerous discussions with guide and other colleagues. CONCLUSION Formal specifications have a variety of applications including testing, maintenance, optimization, refactoring, documentation, and program repair. However, such specifications are difficult for human programmers to produce and verify manually, and existing Miners Characteristics Comment Engler et al. Use two state temporal properties. High false positive rates Whaley et al. Produces Single multi state specification Human intervention Strauss focuses on machine learning to learn a Single specification from traces Use of single specification is not sufficient JIST Refines Whaley et al. technique to mainly disregard infeasible paths Handles only simple subset of Java WN miner Selecting specifications from software artifacts Does not have profound results in finding bugs Claire Use measurements of trustworthiness of source code to mine specifications Does not give adequate results over precision. International Journal of Engineering Research and General Science Volume 3, Issue 3, Part-2 , May-June, 2015 ISSN 2091-2730 450 www.ijergs.org automatic specification miners that discover two-state temporal properties have prohibitively high false positive rates. The goal of this survey is to support the study on the legacy of generating specifications to the new automatic techniques. It helps to get an insight into this dynamic field of study in Specification Mining. Since the object orientation is emerging in all kinds of applications, it is also welcome in the specification mining process. It is mentioned to be dynamic, as these approaches are under development and it steps higher everyday to achieve efficiency in capturing specifications. REFERENCES: [1]J. Whaley, M. C. Martin, and M. S. Lam, ―Automatic extraction of object-oriented component interfaces,‖ in ISSTA, 2002. [2]R.Alur, P.Cerny, P.Madhusudan, and W. Nam, ―Synthesis of interface specifications for Java classes,‖ in POPL, 2005. [3]M. Gabel and Z. Su, ―Symbolic mining of temporal specifications, ―in ICSE, 2008, pp. 51–60. [4]D. Lo, L. Mariani, and M. Pezz`e, ―Automatic Steering of Behavioral Model Inference,‖ in FSE. ACM, 2009,",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
80040e993990f21db4741381c02165c3db06ace0,https://www.semanticscholar.org/paper/80040e993990f21db4741381c02165c3db06ace0,Modeling and Integrating Cognitive Agents Within the Emerging Cyber Domain,"One of the elements missing from virtual environments in the emerging cyber domain is an element of active opposition. For example, in a training simulation the instructor assigns the student a task or objective, and the student then practices within the environment (the “cyber range”) until they feel comfortable with the task or are able to demonstrate the requisite level of mastery. The environment may have static defenses, such as access control or firewalls, or a fixed set of intrusion methods to defend against, but it typically lacks any active opposition that might adapt defensive or offensive actions (e.g., monitor logs, blocked connections, exploit switching or information gathering). This is akin to training fighter pilots against adversaries who know how to use their weapons, but do not have any tactical or strategic goals beyond that. This is unfortunate for two reasons: 1) it trains cyber operators to behave as though opponents do not have a tangible existence or do not have higher-level goals, and 2) it ignores an opportunity to tailor the student’s learning experience through adjustable adversary behavior. Cognitive agents have the potential to transform the cyber operations training experience. The application of cognitive agents to the roles of cyber offense and defense would provide a more complete cyber ecology for training purposes and thus a more realistic training experience for the student. There are two key challenges to creating such cyber agents: 1) modeling the complex, and continually evolving, processes of cyber operations within a cognitive architecture, and 2) defining the tools and data standards to enable cognitive agents to interoperate with networks in a portable way. This paper discusses novel models of cyber offensive and defensive behavior based on observation and elaboration of human expertise, as well as an approach to the creation of software adapters that translate from task-level actions to network-level events to support agent-network interoperability. ABOUT THE AUTHORS Randolph M. Jones, PhD, is a senior artificial intelligence engineer at Soar Technology, and co-founded Soar Technology in 1998. Dr. Jones received his BS in mathematics and computer science from UCLA, and he received his M.S. and Ph.D. in information and computer science from the University of California, Irvine. Ryan O’Grady is the technical lead for Soar Technology’s emerging business area in cyberspace training and visualization, and a senior software engineer in the Intelligent Training business area. Mr. O’Grady received a BSE in Computer Science Engineering from the University of Michigan in 2004. Certifications: Security+, CPTE, OSCP Denise Nicholson, PhD, CMSP, is the Director of Soar Technology’s new Technology Area ""X"" leading an effort to explore, identify and pursue innovative applications of intelligent systems for critical and challenging problems, such as Cyber Security. Dr. Nicholson has a Ph.D. and M.S. in Optical Sciences from the University of Arizona, and a B.S. in Electrical Computer Engineering from Clarkson University. Robert Hoffman, PhD, is a Senior Research Scientist at the Florida Institute for Human and Machine Cognition (IHMC). He is senior editor of the Department on Human-Centered Computing of IEEE: Intelligent Systems. His latest book is Accelerated Expertise: Training to High Proficiency in A Complex World (2014, Taylor & Francis). Larry Bunch is a Senior Research Associate at IHMC. He received his BS in computer science from the University of West Florida and has published extensively concerning software agents, semantic policies and reasoning, and large-scale event visualizations. Jeffrey M. Bradshaw, PhD, is a Senior Research Scientist at IHMC. He co-edits the HCC Department of IEEE Intelligent Systems and has published widely in software agents, semantic technologies, digital policy management, and human-agent-robot teamwork (HART). Ami Bolton, PhD, is a Program Officer at the Office of Naval Research (ONR). Her programs focus on enhancing individual and team decision-making and combat effectiveness through advances that improve perception, cognition, and team coordination. Dr. Bolton received a M.S. in Human Factors from the Florida Institute of Technology, and Ph.D. in Applied Experimental & Human Factors Psychology from University of Central Florida. Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) 2015 2015 Paper No. #15232 Page 2 of 10 Modeling and Integrating Cognitive Agents Within the Emerging Cyber Domain Randolph M. Jones, Ryan O'Grady, Denise Nicholson, Robert Hoffman, Larry Bunch, Jeffrey Bradshaw, and Ami Bolton Soar Technology IHMC Office of Naval Research Ann Arbor, MI Pensacola, FL Arlington, VA rjones@soartech.com, ryan.ogrady@soartech.com, denise.nicholson@soartech.com, rhoffman@ihmc.us, lbunch@ihmc.us, jbradshaw@ihmc.us, amy.bolton@navy.mil Cyber warfare presents a persistent and evolving threat to military and civilian information systems. Both DoD (Parrish, 2013) and ODNI (Pellerin, 2013) rank cyber warfare as our top national security concern. In addition to threats to our defensive forces, cyber attacks pose an economic threat on the order of one trillion dollars (Ponemon, 2013). Although individual cyber-warfare tools operate at extremely fast speeds, aggressors increasingly pursue a “cyber kill-chain” (Hutchins et al., 2010) over days, weeks, or months. Would-be cyber aggressors are constantly changing their attack vectors to take advantage of security lapses by human resources and the latest vulnerabilities in information technology. These human-speed activities are guided by cognitive behavior that includes a variety of types of goals and expertise: script-kiddies, ideological activists, investigators, financial criminals, intelligence agents, or cyber warfighters (Lathrop et al., 2010). At the human, cognitive level, offense depends on and reacts to responses of defenders (Pfleeger & Caputo, 2012) and users (Bowen et al., 2012) that are also cognitively driven. Current cyber-warfare tools comprise suites of technical mechanisms that respond to the tools that aggressors and defenders use, but not to the individuals themselves. Human tactics are currently addressed through human-staffed wargames at cyber ranges (Merit, 2013; Pridmore, 2012). Human role-players are expensive, not repeatable, and not deployable as an automated system. There is an emerging need for cognitive-level synthetic cyber offense and defense, to ensure realistic cyber simulation and training. Building effective training systems for cyber warfare presents a suite of unique problems: Offensive and defensive activity is highly dynamic. The characteristics of target network environments are driven by the users of the system and their current activities, which are highly variable and unpredictable. User behavior often creates vulnerabilities that can be exploited. Cyber warfighters themselves are extremely adaptive and creative. In order to meet their objectives they will change tactics or tools based on opportunities detected in a computer network or responses initiated by adversaries or users. Current training environments do not adequately capture the dynamic and cognitive-level characteristics of cyber warfare. They are unable to capture the purposefulness, creativity, and adaptability of actual cyber warfighters. Studying previous offensive and defensive scenarios in a classroom environment is an effective means of understanding the building blocks of cyber warfare, but falls short of creating the skills needed to deal with a creative and time-sensitive event or a sophisticated but dynamic plan. Computerized unit tests can build fundamentals for dealing with individual components cyber warfare, but they do not help the trainee learn to recognize and make sense of the larger picture, nor do they capture the dynamic nature of networks and users. If cyber warfighters are to learn to respond to a cunning and adaptive opponent, they need to train against cunning and adaptive opponents. An effective cyber-warfare training system must be adaptable and deal with the changing nature of a networked environment. It must be able to model the dynamic nature of cyber aggressors, users, and defenders. It must create a virtual environment that replicates the environment that the trainee will ultimately operate in. An appropriate virtual environment also creates the opportunity for accurate post-event forensic analysis by providing access to databases, configurations, and system logs. This paper presents our efforts to address these issues through the development of cognitive agents for cyber offense and defense. The Soar cognitive architecture described in this paper is not to be confused with Soar Technology, the affiliation of some of the authors. Soar is not a commercial product, but is available under a General Public License from http://soar.eecs.umich.edu/ maintained by the University of Michigan. The Soar architecture provides the technological foundation for the cognitive agents described here. Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) 2015 2015 Paper No. #15232 Page 3 of 10 MODELING AND INTEGRATION CHALLENGES In order to build realistic cognitive agents, the agents must encode appropriate domain expertise, and they must interact with a realistic cyber environment (Jones & Laird, 1997). In addition to realism, cost effective cognitive agents also need to address these related issues: Reduce cost of realistic role playing in cyber-warfare simulation, system engineering, and analysis of cyber operations. Enable end-user updating of agent knowledge with minimal support from software engineers, both through coaching by instructor Subject-Matter Experts (SMEs) and through explicit addition of new knowledge about cyber tactics. Be readily adaptable to a wide range of network structures, devices, and protocols. In o",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
412d785808c109cc4c9af1b4b45feb14c9c781f5,https://www.semanticscholar.org/paper/412d785808c109cc4c9af1b4b45feb14c9c781f5,A Comparative Study of Selected Classification Algorithms of Data Mining,"Data Mining is the process of extracting hidden knowledge from large volumes of raw data. Data mining has been defined as the nontrivial extraction of previously unknown, implicit and potentially useful information from data. Classification techniques are being used in different industry to easily identify the type and group to which a particular tuple belongs. This work explores the five classification algorithms (Zero, PART, OneR, Prism, J48) are compared on the bases of two techniques (cross validation and percentage split). Experimental comparison was performed by considering the car evaluation dataset and analyzing them using data mining open source WEKA tool. Keywords— ―Data Mining, Classification Algorithms, Car evaluation data set‖ INTRODUCTION Data mining is the motion of all methods and techniques, which allow analyzing very large data sets to extract and discover previously unknown structures and relations out of such huge heaps of details. This information is filtered, prepared and classified so that it will be a valuable aid for decisions and strategies [2]. With the enormous amount of data stored in files, databases, and other repositories, it is increasingly important, if not necessary, to develop powerful means for analysis and perhaps interpretation of such data and for the extraction of interesting that could help in decision-making [3]. The title Data mining is the process of extracting patterns from data. As more data are gathered, with the amount of data doubling every three years [1]. Data mining is the process of discovering knowledge from large amounts of data stored either in databases or warehouses [2]. Data mining is becoming an increasingly important tool to transform these data into information. Data mining can also be referred as knowledge mining or knowledge discovery from data. Many techniques are used in data mining to extract patterns from large amount of database [1]. Classification is a data mining (machine learning) technique used to predict group membership for data instances. Classification is a supervised procedure that learns to classify new instances based on the knowledge learnt from a previously classified training set of instances [5]. It takes a set of data already divided into predefined groups and searches for patterns in the data that differentiate those groups supervised learning, pattern recognition and prediction. Classification consists of predicting a certain outcome based on a given input. It is one of the Data Ashish Kumar Dogra et al, International Journal of Computer Science and Mobile Computing, Vol.4 Issue.6, June2015, pg. 220-229 © 2015, IJCSMC All Rights Reserved 221 Mining techniques that is mainly used to analyze a given dataset and takes each instance of it and assigns this instance to a particular class with the aim of achieving least classification error. It is used to extract models that correctly define important data classes within the given dataset. It is a two-step process. In first step the model is created by applying classification algorithm on training data set. Then in second step, the extracted model is tested against a predefined test dataset to measure the model trained performance and accuracy. So, classification is the process to assign class label for this dataset whose class label is unknown [7]. Find a model that describes the data classes or concepts. This derived model is based on analysis of set of training data. The derived model can be presented in the following forms Classification (IF-THEN) Rules, Decision Trees, Mathematical Formulae and Neural Networks. Classification Algorithm Classification is a classic data mining technique based on machine learning. Basically classification is used to classify each item in a set of data into one of predefined set of classes or groups. Classification method makes use of mathematical techniques such as decision trees, linear programming, neural network and statistics [9]. Classification algorithm finds relationships between the values of the predictors and the values of the target. Components of Data Mining Algorithms i) Model Representation (Knowledge Representation) is the language for describing discoverable patterns / knowledge ii) Model Evaluation is estimating the predictive accuracy of the derived patterns iii) Search methods we have two type of broadly classified search method one is parameter search when the structure of a model is fixed, search for the parameters which optimise the model evaluation criteria and another is model search when the structure of the model is unknown iv) Learning Bias consist of ffeature selection and Pruning algorithm [11]. Data classification is a two steps process in which first step is the training phase where the classifier algorithm builds classifier with the training set of tuples and the second phase is classification phase where the model is used for classification and its performance is analyzed with the testing set of tuples. The algorithm can differ with respect to accuracy, time to completion, and transparency. In practice, it sometimes makes sense to develop several models for each algorithm, select the best model for each algorithm, and then choose the best of those for deployment. Algorithms Evaluated Five Classification algorithms considered for our study are  ZeroR  Part  Prism  OneR  J48 ZeroR:ZeroR is the simplest classification method which relies on the target and ignores all predictors. ZeroR classifier simply predicts the majority category (class). Although there is no predictability power in ZeroR, it is useful for determining a baseline performance as a benchmark for other classification methods [10]. It is the simplest method which relies on the frequency of target. ZeroR is only useful for determining a baseline performance for other classification methods. PART:PART is a partial decision tree algorithm, which is the developed version of C4.5 and RIPPER algorithms .PART is a separate-and-conquer rule learner proposed by Eibe and Witten [54]. The algorithm producing sets of rules called decision lists which are ordered set of rules. A new data is compared to each rule in the list in turn, and the item is assigned the category of the first matching rule (a default is applied if no rule successfully matches). PART builds a partial C4.5 decision tree in its each iteration and makes the best leaf into a rule. The algorithm is a combination of C4.5 and RIPPER rule learning [2]. Prism:This simple and straightforward covering algorithm work by first picking a class from the dataset for which to create a new rule having the class as its conclusion, and selectively adding tests to the condition of the rule, striving for maximum number of instances covered and 100% accuracy. The accuracy of a test is measured by the ratio of the number of positive instances p to the total number of instances covered by the rule. The positive instances covered by the new rule then are removed from the dataset for further rule generation. Then, negative instances should remain in the data set to await a later iteration of the process. This process continues until no more instances remain to covered. Ashish Kumar Dogra et al, International Journal of Computer Science and Mobile Computing, Vol.4 Issue.6, June2015, pg. 220-229 © 2015, IJCSMC All Rights Reserved 222 OneR:OneR or “One Rule” is a simple algorithm proposed by Holt. The OneR builds one rule for each attribute in the training data and then selects the rule with the smallest error rate as its one rule. The algorithm is based on ranking all the attributes based on the error rate .To create a rule for an attribute, the most frequent class for each attribute value must be determined [54]. The most frequent class is simply the class that appears most often for that attribute value. A rule is simply a set of attribute values bound to their majority class. OneR selects the rule with the lowest error rate. In the event that two or more rules have the same error rate, the rule is chosen at random [8]. The OneR algorithm creates a single rule for each attribute of training data and then picks up the rule with the least error rate [4]. J48:J48 classifier is a simple C4.5 decision tree for classification. It creates a binary tree. The decision tree approach is most useful in classification problem. With this technique, a tree is constructed to model the classification process. Once the tree is built, it is applied to each tuple in the database and results in classification for that tuple[1][3]. Tool Used for Performance Evaluation Platform SOFTWARE USED  Operating system Window 8  JAVA version 7  Tool-WEKA 3.7 Introduction to WEKA Waikato Environment for Knowledge Analysis (WEKA) tool is used for the performance evaluation. WEKA is a collection of machine learning algorithms for data mining tasks. WEKA is created by researchers at University of Waikato in New Zealand. It is written in Java and runs on almost any platform . The algorithms in WEKA can either be applied directly to a dataset or called from own Java code. It is also well-suited for developing new machine learning schemes. WEKA is open source software issued under the GNU General public License. It is a collection of open source ML algorithms pre-processing, classifiers, clustering, association rule and visualization [6]. Data Set The dataset is car evaluation data set has been used for the experiment. In this data set their are 1728 number of instances and 7 number of attributes. Result and Discussion Cross Validation Testing Method Cross-Validation is a statistical method of evaluating and comparing learning algorithms by dividing data into two segments: one used to learn or train a model and the other used to validate the model. In cross validation, the training and validation sets must cross-over in successive rounds such that each data point has a chance of being validated against. The basic form of cross-validation is k-fold ",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
510818b2ee749013ba7ace574c19d1352eaf5698,https://www.semanticscholar.org/paper/510818b2ee749013ba7ace574c19d1352eaf5698,MURFI 2016-FROM CARS TO MARS : APPLYING AUTONOMOUS VEHICLE NAVIGATION METHODS TO A SPACE ROVER MISSION,"The Mars Utah Rover Field Investigation (MURFI) 2016 mission was a Mars Rover field analogue mission run by the UK Space Agency (UKSA) in collaboration with the Canadian Space Agency (CSA). MURFI consisted of a “field team” at the Mars analogue site near Hanksville (Utah, USA), and an “Operations Team” based in the Mission Operations Centre (MOC) at UKSA’s Harwell Campus (Oxfordshire, UK). The Rover platform for the mission comprised the Oxford Robotics Institute’s (ORI) ARC Q14 Space Rover (Q14), and the mission provided a unique opportunity for ORI to test the performance of several of their advanced navigation and autonomous driving algorithms in realistic planetary exploration conditions over a period of several weeks (Figure 1). MURFI’s core objectives included a realistic imitation of the first 10 sols of a possible future Mars Rover mission. Following completion of the core mission, the ORI Rover team engaged in a series of ambitious trials and data gathering scenarios based on ORI’s broad suite of navigation and autonomy algorithms which have been developed primarily for on-road terrestrial applications. The objectives were to (i) assess the performance of these systems in this radically different environment, (ii) determine what systems modifications (if any) might be needed to operate effectively in a typical planetary surface scenario, and (iii) to identify which new capabilities these techniques might bring to the field of planetary surface navigation and exploration. The results of these trials were very encouraging, showing good baseline performance of the techniques deployed even though these had not been modified in any way to reflect the change of environment, and indicating several avenues for further exploration and development of ORI’s techniques which could generate substantial benefits for the Space community. During the core mission, the Rover drives for each sol were conducted autonomously using a single mastmounted stereo camera sensor. Waypoint files for the planned drives were designed by the MOC, transmitted to the field site, and uploaded to Q14 for autonomous execution using ORI’s OxfordVO visual odometry (VO) application the same application which forms the kernel of the VO software for the forthcoming ExoMars mission. The accuracy in following the planned drives during this phase was highly satisfactory, with the Rover typically finishing the drive within a few centimetres of the planned location. During the post-MURFI trials programme, the team implemented ORI’s “Dub4” suite to provide “teach-andrepeat” (T&R) functionality. Dub4 uses a single stereo camera to create vision-based maps in highly unstructured environments which are then used to localise and navigate autonomously. This facilitates safe, rapid retracing of a previously driven path, enabling interesting science sites subsequently identified through data analysis to be rapidly revisited by the Rover. Accuracy in following the previously driven path was good despite large areas of the driven environment being relatively featureless. The use of an affordable monocular camera as an effective localisation sensor using the feature-rich desert floor was investigated, with encouraging results. Data was collected so that dense reconstructions of the terrain around the rover can be generated in a future phase of our work. This reconstruction capability has the potential to create an extremely powerful visualisation tool for generating rich 3-D mesh representations to be utilised by mission scientists to more effectively focus the science effort. Future work will also concentrate on performance enhancements by adapting existing ORI techniques to the specifics of the planetary surface environment, and developing enhanced machine learning autonomy approaches along the path towards the implementation of a true “robot geologist”.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
a8ed4fdef91f515510432783aa2f9000df54d0e3,https://www.semanticscholar.org/paper/a8ed4fdef91f515510432783aa2f9000df54d0e3,Data-Driven Reservoir Management of a Giant Mature Oilfield in the Middle East,"A novel approach to reservoir management applied to a mature giant oilfield in the Middle East is presented. This is a prolific brown field producing from multiple horizons with production data going back to mid-1970s. Periphery water injection in this filed started in mid-1980s. The field includes more than 400 producers and injectors. The production wells are deviated (slanted) or horizontal and have been completed in multiple formations. An empirical, full field reservoir management technology, based on a data-driven reservoir model was used for this study. The model was conditioned to all available types of field data (measurements) such as production and injection history, well configurations, well-head pressure, completion details, well logs, core analysis, time-lapse saturation logs, and well tests. The well tests were used to estimates the static reservoir pressure as a function of space and time. Time-lapse saturation (PulseNeutron) logs were available for a large number of wells indicating the state of water saturation in multiple locations in the reservoir at different times. The data-driven, full field model was trained and history matched using machine learning technology based on data from all wells between 1975 and 2001. The history matched model was deployed in predictive mode to generate (forecast) production from 2002 to 2010 and the results was compared with historical production (Blind History Match). Finally future production from the field (2011 to 2014) was forecasted. The main challenge in this study was to simultaneously history match static reservoir pressure, water saturation and production rates (constraining well-head pressure) for all the wells in the field. History matches on a well-by-well basis and for the entire asset is presented. The quality of the matches clearly demonstrates the value that can be added to any given asset using pattern recognition technologies to build empirical reservoir management tools. This model was used to identify infill locations and water injection schedule in this field. RESERVOIR MANAGEMENT Reservoir management has been defined as use of financial, technological, and human resources, to minimizing capital investments and operating expenses and to maximize economic recovery of oil and gas from a reservoir. The purpose of reservoir management is to control operations in order to obtain the maximum possible economic recovery from a reservoir on the basis of facts, information, and knowledge (Thakur 1996). Historically, tools that have been successfully and effectively used in reservoir management integrate geology, petrophysics, geophysics and petroleum engineering throughout the life cycle of a hydrocarbon asset. Through the use of technologies such as remote sensors and simulation modeling, reservoir management can improve production rates and increase the total amount of oil and gas recovered from a field (Chevron 2012). Reservoir simulation and modeling has proven to be one of the most effective instruments that can integrate data and expertise from a wide range of disciplines such as geology, petrophysics, geophysics, reservoir and production engineering in order to model fluid flow in the reservoir. Reservoir simulation model is history matched using the pressure and production measurements from the asset in order to tune the geological understandings and provide predictive capabilities. Since no two hydrocarbon reservoirs are the same and each asset has its own unique geological characteristics and drive mechanisms, the art and science of reservoir simulation and modeling must be adapted to unique situations in order to be able to realistically model 2 Mohaghegh, et al. SPE 170660 the past and predict the future of a hydrocarbon producing reservoir. Although reservoir simulation and modeling remains one of the major contributors to reservoir management practices for the foreseeable future, its realistic application to reservoir management practices continues to face challenges. These challenges are related to exploration of a very large solution space that is a natural and required step during a reservoir management study. During reservoir management process it is required to generate, evaluate and rank multiple potential development scenarios as early as possible in the workflow. Furthermore, important practices such as quantification and analysis of uncertainties associated with the geological model as well as economic analysis and planning require large number of scenarios to be generated and evaluated in order to assist the decision making process. Performing reservoir management studies without such capabilities reduces the informed decision making to guess work, albeit, educated guess work. DATA DRIVEN RESERVOIR MODELING & MANAGEMENT Also referred to as “Fact-Based Reservoir Modeling”, data-driven reservoir modeling is a novel approach to build models representing fluid flow in hydrocarbon producing porous media that are completely based on field measurements. Instead of starting from first principle physics that result in partial differential equations such as the diffiusivity equation, data-driven reservoir modeling starts from field measurements such as well configurations, well completion, well logs, core analysis, well tests, and production/injection history. Contrary to the assessment of some critics, physics is not ignored during data-driven reservoir modeling. In data-driven reservoir modeling role of physics is changed from the architect of the governing equations to the guiding light and the blue-print that provides the framework for the model development process. Data-driven reservoir modeling does not adhere to the dogma that all modeling of natural phenomena must start with physics to have credibility, nor it finds credible the notion, by naïve statisticians, that no physics (or petroleum engineers) are needed for modeling and that data can be the answer to all problems in the upstream E&P industry. Data-driven reservoir modeling starts with the premise that data, especially in our discipline, carries information. Data collected during the drilling, reservoir, and production operations in an oilfield includes footprints, in space and time, of fluid flow in the porous media. If large enough volume of such data is assembled in a proper fashion, and appropriate tools are used in interpreting them by well-trained petroleum engineers, then there is a realistic chance of being able to build comprehensive and cohesive full field models that not only will not violate the known physics, but would be able to shed light on complex and highly nonlinear behavior that might have been missed by a purely physics -based approach. The reason is obvious. Physics-based approaches are bound to be limited to our current understanding of the natural phenomenon which continues to improve as a function of time. Our understanding of complexity of fluid flow in a large and diverse combinations of porous media is far more advance today than in was 40 years ago and it is bound to advance even further in the next 40 years. But the facts that are intrinsic to (and the patterns that exist in a collection of) data are permanent and do not change with time. The question is; do we have the tools and the techniques and the knowhow to extract them? The first and the most comprehensive data-driven reservoir modeling technology developed by reservoir engineers (not mathematicians or statisticians) is Top-Down Modeling (TDM). TDM was introduced a few years ago and has enjoyed continuous R&D to enhance its capabilities ever since. Several IOCs, NOCs, and independents have successfully adopted this technology and are benefiting from its results. Data-driven reservoir management is referred to as a process whereby the key models used to make critical decisions are data-driven models. The main advantages of data-driven reservoir models are (a) they are fact-based and include minimal pre-modeling interpretation and human biases, (b) time required for their development (training and validation of the predictive models) is a fraction of the time required for a comprehensive numerical simulation model, and (c) they have small computational footprint that accommodates large number of scenarios to be investigated in relatively short period of time. TOP-DOWN MODELING (TDM) TECHNOLOGY Traditional numerical reservoir simulation is the industry standard for reservoir management. It is used in all phases of field development in the oil and gas industry. The routine of numerical simulation studies calls for integration of static and dynamic measurements into a reservoir model that has been formulated based on our current understanding of fluid flow in porous media and numerical solution of the formulation in the context of an interpreted geological model. Numerical simulation is a bottom-up approach that starts with building a geological (geo-cellular or static) model of the 1 Although our skills and tools to extract the facts may change and advance with time. 2 This technology was invented and introduced to the E&P industry by Intelligent Solutions, Inc. in 2006. SPE 170660 Data-Driven Reservoir Management of a Giant Mature Oilfield in the Middle East 3 reservoir. Using modeling and geo-statistical manipulation of the data the geo-cellular model is populated with the best available geological and petrophysical information. Engineering fluid flow principles are added and solved numerically to arrive at a dynamic reservoir model. The dynamic reservoir model is calibrated using the production history of multiple wells by modification of several of the parameters involved in the geological model in a process called history matching and the final history matched model is used in predictive mode to strategize the field development in order to improve recovery. Characteristics of the numerical reservoir simulation and modeling include:  It takes a significant investment (time and mo",,2014.0,10.2118/170660-MS,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
500b7a57985be12e597754c13d4c4e2e61048d67,https://www.semanticscholar.org/paper/500b7a57985be12e597754c13d4c4e2e61048d67,Bringing models to the domain: deploying Gaussian processes in the biological sciences,"Recent developments in single cell sequencing allow us to elucidate 
processes of individual cells in unprecedented detail. This detail 
provides new insights into the progress of cells during cell type 
differentiation. Cell type heterogeneity shows the complexity of cells 
working together to produce organ function on a macro level. The 
understanding of single cell transcriptomics promises to lead to the 
ultimate goal of understanding the function of individual cells and 
their contribution to higher level function in their environment. 
 
Characterizing the transcriptome of single cells requires us to 
understand and be able to model the latent processes of cell functions 
that explain biological variance and richness of gene expression 
measurements. In this thesis, we describe ways of jointly modelling 
biological function and unwanted technical and biological confounding 
variation using Gaussian process latent variable models. In addition 
to mathematical modelling of latent processes, we provide insights 
into the understanding of research code and the significance of 
computer science in development of techniques for single cell 
experiments. 
 
We will describe the process of understanding complex 
machine learning algorithms and translating them into usable 
software. We then proceed to applying these algorithms. We show how 
proper research software design underlying the implementation can lead 
to a large user base in other areas of expertise, such as single cell gene 
expression. To show the worth of properly designed software underlying 
a research project, we show other software packages built upon the 
software developed during this thesis and how they can be applied to 
single cell gene expression experiments. 
 
Understanding the underlying function of cells seems within reach 
through these new techniques that allow us to unravel the 
transcriptome of single cells. We describe probabilistic techniques of 
identifying the latent functions of cells, while focusing on the 
software and ease-of-use aspects of supplying proper research code to 
be applied by other researchers.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
83033b64dfb41a92bc6c7946ba9829068077d95b,https://www.semanticscholar.org/paper/83033b64dfb41a92bc6c7946ba9829068077d95b,An Efficient improved technique to retrieve the bug repository,"Bug triage is the most important step in handling the bugs which occur during a software process. In manual bug triaging process the received bug is assigned to a tester or a developer by a triager, hence the bugs are received in huge numbers it is difficult to carry out the manual bug triaging process, and it consumes much resources both in the form of man hours and economy, hence there is a necessity to reduce the exploitation of resources. Hence a mechanism is proposed which facilitates much better and efficient triaging process by reducing the size of the bug data sets, the mechanism here involves techniques like clustering techniques and selection techniques, The approach proved much efficient than the manual bug triaging process when compared with bug data sets which were retrieved from the open source bug repository called bugzilla. INTRODUCTION Most of the software industries spend nearly half of their economy, that is more than 45 percent of their economy in finding out and correcting bugs alone, this is occurring due to the exploitation of resources both in the form of human labor and economy by implementing manual bug triaging process, here the bug are manually allocated to the testers or developers by the triagers, since the incoming bugs are all over from the world it is a tedious process and require much man power and economy to facilitate the process. So, there is a necessity to reduce the expenditure, to deal with this exploitation previously few techniques have been proposed like text classification techniques, Naïve bayes, Support vector machine which facilitated the automation of the process, and also selection mechanisms like feature selection and instance selection have been used to reduce the size of the bug data sets for a better triaging process. As few researchers proposed that triaging produces better results when the data undergoes preprocessing before any machine learning algorithms are applied to it, Hence a combination of clustering mechanism and selection mechanism is used for a better triaging process, primarily the data is prepared by pruning the data with the help of techniques like tokenization, stop words, stemming etc.., and an algorithm called x means algorithm is used to cluster the data, and two selection mechanism called instance selection and feature selection are used to reduce the size of the bug data set and provide better triaging process. ITERATURE SURVEY Software engineering data contains useful information such as code bases, execution traces, bug databases etc.., by exploiting such data we can build a more reliable software system by building dependencies based on the historical data produced in the software process. [2] A.E Hassan Trends in Engineering and Technology (NCTET-2K17) International Journal of Advanced Engineering Research and Science (IJAERS) Special Issue-3 https://dx.doi.org/10.22161/ijaers/nctet.2017.55 ISSN: 2349-6495(P) | 2456-1908(O) www.ijaers.com Page |200 stated that most commonly available repositories of a software system are source control repositories, bug repositories, archived communications, deployment logs and code repositories. The mining software repositories concept which is called as MSR primarily analyzes and cross references all the data which is present in the repository to extract the data which is considered to be useful in the repository, by evolving these repositories from basic repositories which contain records to dynamic repositories which can be used for decision making in the aspect of software engineering. [11] John anvik stated that open source projects generally support open repositories in which both the user and the developers can access and report bugs. The reports which are received in the repository should undergo the process of triaging to determine whether the report is worthy or not, if it is that particular report should be assigned to a developer for further rectification, Hence in the context of large data repositories the [4] Davor C Urbanic stated that, a bug tracking system is necessary to maintain bug reports on large software development projects. Research has been carried out by the researchers in the field of mining software repositories which augment traditional software engineering information by the use of tools and techniques, which are implemented in solving challenging problems in the field of software which the engineers face on a daily during the process. Triage is actually a medical term in which the patients are allocated based on their severity, in a similar way bug triage is a term in which the bugs are allocated to the testers based on their priority. [14]Silvia Breu stated that, in open source projects such as Mozilla, bugzilla and eclipse, the bug tracking systems interact with the user communities, so that the users can be a part of bug fixing process, hence this type of bug tracking systems play an important role in interacting and communicating with the users. [5] Dominuque Matter stated that, the daily incoming of bug reports is high on daily basis, hence triaging these incoming reports is necessary and it is a difficult task assigning a developer to a report is also a part of it. Traditional bug consuming is a very tedious and time consuming job, the repositories which contain the bugs are called bug repositories, bugs are collected into a data base, and the triaging team sort out the bugs based on their priority. [7]Gaeul Jeong stated that, It is important that the bugs should be identified and rectified within time in software engineering process, Most of the bugs are manually allocated by human triagers which is a tedious task. [12] Nicolas Bettenburg stated that bug reports acts as the main source of information for developers, but there exists a difference in the quality of each bug report, on the open source repositories like apache, eclipse and Mozilla a survey has been conducted with the help of the users and developers, in that survey more than 450 responses mismatched with the user requirement, to overcome this methods like reproducing, stack traces are used which are difficult to maintain for large number of bug reports. [13] Shivkumar Shivaji stated that prediction of bugs from source code files are done by using machine learning classifiers, primarily the classifier is linked with historical bug data and used for prediction the possible outcomes, yet these classifiers suffer from a drawback that is insufficient performance real world situations and slow prediction time, some of the classifiers are Naïve Bayes and support vector machine (SVM). Feature selection can defined as the process in which the irrelevant features are deducted and detecting only the relevant ones, an optimal selection of features can bring improvement in overall knowledge of domain, reduced size, generalization capacity etc.., [9] J.Arturo Olvera Lopez stated that sufficient identification of features is necessary in real world scenario, hence the identification of features is important. [20] Yiming Yang stated that, feature selection is the best solution for text classification problems it increases both the classification effectiveness and also computational efficiency. Instance selection is a process, in which the dataset size is reduced , which eventually decreases the runtime, especially in the case of instance based classifiers, the commonly used instance selection mechanisms are wrapper and filter, here Trends in Engineering and Technology (NCTET-2K17) International Journal of Advanced Engineering Research and Science (IJAERS) Special Issue-3 https://dx.doi.org/10.22161/ijaers/nctet.2017.55 ISSN: 2349-6495(P) | 2456-1908(O) www.ijaers.com Page |201 filtering mechanism is used called as Iterative case filtering(ICF) algorithm. [9] J. Arturo Ovlera Lopez stated that historical data that is previously known data is used to classify new instances.",,2017.0,10.22161/IJAERS/NCTET.2017.55,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
798812cbab154d46c293720360cb03bb3780c417,https://www.semanticscholar.org/paper/798812cbab154d46c293720360cb03bb3780c417,Estimación del riesgo de no superar una asignatura de evaluación continua mediante aprendizaje computacional,"El presente trabajo representa la finalizacion de los estudios del Grado en 
Ingenieria Informatica impartida en la Universidad Politecnica de Madrid, del alumno 
Jose Mario Lopez Leiva. Por tanto, este documento el culmen de un proceso de formacion 
que recoge la adquisicion de conocimiento en distintas ramas de las Ciencias de la 
Computacion, y en concreto, esta obra versa acerca de uno de los temas en boga dentro 
de esta rama del saber: el aprendizaje computacional (o automatico), que como su nombre 
sugiere, se trata del conjunto de tecnicas que permiten a una maquina aprender en base a 
una serie de ejemplos provistos. Como el proceso de aprendizaje automatico realizado se 
ha alcanzado con una combinacion de tecnicas y decisiones humanas y computacionales, 
el termino que se maneja en el trabajo es el de Mineria de datos. Este trabajo, pues, 
presenta un analisis de mineria de datos realizado sobre un conjunto de datos real y 
cercano: la coleccion de calificaciones de la asignatura de Logica impartida en la misma 
casa. El trabajo ha tratado, a grandes rasgos, de la elaboracion de una aplicacion web que 
se valga de metodos predictivos utilizando el aprendizaje computacional (la aplicacion 
debe aprender del conjunto de notas) para luego formular un modelo que permita estimar, 
con un cierto porcentaje, el riesgo o probabilidad de que un alumno suspenda (o apruebe) 
la asignatura en base a las calificaciones que ingrese en la aplicacion, notas que 
representan la realizacion de actividades evaluables dentro del proceso de evaluacion 
continua dentro de la materia. 
Por su parte, la memoria recoge los siguientes aspectos: una breve introduccion a 
la mineria de datos, para lograr entender el alcance de esta ciencia y sus beneficios; asi 
como la teoria matematica que se encuentra de detras de los algoritmos de mineria de 
datos elegidos -en concreto, se ha elegido la regresion logistica como metodo de 
prediccion en el contexto de la mineria de datos, aunque se han realizado pruebas con un 
algoritmo de arbol de decision, como lo es el J48-. A continuacion, se analiza el conjunto 
de datos y sus particularidades para proceder a un preproceso de los datos (parte del 
proceso de mineria de datos). Despues de este preprocesamiento, se efectua un filtrado de 
variables para proceder con el entrenamiento del modelo y los correspondientes analisis 
de bondad, que consisten en comprobar lo bueno y fiable que es el predictor obtenido. 
Una vez realizada la parte de mineria de datos, se ha elaborado una aplicacion 
web con la tecnologia Spring Boot que permite el desarrollo del programa en lenguaje 
Java y con un despliegue rapido. De esta manera, el usuario puede ingresar las 
calificaciones obtenidas en sus evaluaciones para conocer el riesgo de suspender, pero 
tambien se ha dotado a la aplicacion de una calculadora completa de la nota final de la 
asignatura, que indica que partes de la asignatura se guardan para la evaluacion 
extraordinaria, asi como un sencillo comparador grafico que permite comparar las 
calificaciones obtenidas con las medias de anos anteriores. En definitiva, se ha buscado 
que la aplicacion sea eficiente computacionalmente, y que la parte de mineria de datos 
sea facilmente interpretable y que cada decision dentro del proceso quede debidamente 
justificada.---ABSTRACT---This work represents the culmination of the Degree in Computing Science studies 
for Jose Mario Lopez Leiva. Thus, this document is the conclusion of the academic 
formation process that has allowed him to acquire the necessary knowledge in the field 
of the Computing Science, and specifically, this work is about the one of the latest trends 
in this area: The Machine Learning, that, as its very name indicates, is the set of 
techniques that allow a machine, namely a computer, to learn based on some given 
examples. As this Machine Learning process, has been achieved by combining human 
and computational decisions, the term used in is Data Mining instead. So, in this project, 
an analysis has been performed over a real and familiar dataset: the grades obtained in 
the Logic subject taught at this school. The main goal of the project is to develop a web 
application that estimates the probability of a student failing (or passing) the subject, 
trough Data Mining processes; based on the grades that the users input into the program, 
each meaning an evaluation in the context of the subject. 
As for this document, it is divided in the following sections: a brief introduction 
to Data Mining, made with the intent to make it easier to understand and have an idea of 
its reach and benefits; as well as the mathematical definitions behind the chosen Data 
Mining algorithm - Logistic Regression, although a decision tree algorithm, machine 
learning J48, has been used as a support tool-. Next, the document shows an analysis of 
the dataset, a must for the step of preprocessing, a vital part of the Data Mining Process. 
The next step is a features filtering process, so at the end, the model training process can 
be performed over the cleaned data. Lastly, the work concludes with the development of 
the Web Application, which has been created with the Spring Boot technology, mainly, 
to guarantee a quick deployment of the program. This way, the user can be aware of his 
chances of failing the signature, but the application has been enriched with a final grade 
calculator, as well as a comparator that allows to contrast the user’s grades with the means 
of previous courses.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
b7348e5a0ef15f4d352acf1c22ba62a6f912a38c,https://www.semanticscholar.org/paper/b7348e5a0ef15f4d352acf1c22ba62a6f912a38c,transforming obsolete data into contemporary data toc to provide terrain information,"It is conceivable to determine the state of the road from the driving experience of the vehicle as it covers an extent of it. This data when communicated can be made accessible to the future users of that road, so all around educated choices can be made about changing the speed or maintaining a strategic distance from deceptive streets even though this data is pointless to the vehicle that produced it. This paper displays the TOC framework which plans to record a vehicle's experience while at the same time driving that specific extent of the road. The information gathered from the vehicle is broke down and made accessible to the future clients of the street consequently effectively Transforming Obsolete information into Contemporary information. The primary subject of this paper will be one of the two calculations utilized by the TOC framework to examine the information produced by the vehicles. This algorithm is utilized to collect useful vehicular information from the rest. Keywords— vibration sensor, rash driving behaviour, terrain information 1, INTRODUCTION As of late with innovation introducing the Internet of Vehicles (a branch of the Internet of Things) the advantages of an interconnected group of drivers has become eminent. In the event that each traveler shares his or her experience, different travelers would without a doubt benefit from it. Outfitting the driver with data about the forthcoming way (unpleasant landscapes, reasonable streets, elusive surfaces) will permit better choices about slowing down, accelerating or picking an optional way by and large. Such a facility would avert mishaps as well as protect the suspension arrangement of the vehicle. On the boondocks of completely mechanized cars, this sort of machine to machine sharing can cross over any barrier between human instinct and machine learning. This paper displays the TOC framework which evaluates the driving experience and communicates this data. 2, PROPOSED SOLUTION A moving vehicle experiences numerous conditions, for example, movement, uneven streets, smooth streets or slippery surfaces. On each event the responses of the vehicle and that of the driver are one of a kind. For instance, a smooth street may urge the driver to increase speed while a dangerous street may trigger the ABS to be conveyed. By dissecting ISRJournals and Publications Page 1362 International Journal of Advanced Research in Computer Science Engineering and Information Technology Volume: 6 Issue: 3 Apr,2017,ISSN_NO: 2321-3337 the speed and driving conduct it is conceivable to decide the state of the street. The framework proposed here goes for utilizing the information produced by the vehicle and its sensors to convincingly decide the state of the street. 1) The information generated by the vehicle is sent to a server. 2) The server utilizes exceptional calculations to decide the street's condition 3) The street's condition, dictated by the calculation is refreshed to a guide which the future clients of the street can see on their smartphone. Figure.1 General outline of the system 3, EXPLANATION OF THE SYSTEM At the heart of this framework is an vibration sensor that measures the experience of the driver. Since driving examples are capricious a solitary sensor can't envision each situation. Accordingly we utilize other supporting sensors. To be specific1) A rash driving sensor: This sensor intermittently decides whether the driving conduct is good or rash. 2) A sensor to decide whether the ABS was deployed. 3) The data received from the vehicle isa. *Speed of the vehicle – Speed in km/hr b. *Vibration sensor data – Experience of the vehicle (1 = Good, 2 = Fair, 3 = Bad) c. Geographical co-ordinates of the vehicle – Map Co-ordinates d. *Rash driving sensor data – Binary value (1 = Driving is rash, 0 = Driving is normal) e. ABS deployment sensor data – Binary value ( 1 = Deployed, 0 = Not Deployed) * implies that this data will be used in the following algorithms. 4, PROBLEM STATEMENT The information is gotten from each vehicle on the road. This is favorable on the grounds that our calculation will have sufficient information to get conclusions from. This is ISRJournals and Publications Page 1363 International Journal of Advanced Research in Computer Science Engineering and Information Technology Volume: 6 Issue: 3 Apr,2017,ISSN_NO: 2321-3337 additionally a drawback on the grounds that distinguishing substantial information from spurious information ends up noticeably tricky. Without a doubt, a vehicle that drives impulsively will have a terrible driving experience. Presuming that the street surface is terrible from such information would prompt incorrect outcomes. Likewise, slowly moving vehicles will dependably have a smooth driving knowledge. It is false to presume that the streets are great from such information. Consequently the framework needs a calculation which can dispose of suspicious information and recognize the solid information. On the off chance that the calculation is excessively prohibitive, the framework will acknowledge information just from the most know-it-all drivers and the outcomes would be of no down to earth utilize. Then again, if the calculation is excessively liberal it might incorporate the information from problematic sources too . While recognizing the substantial information is a test in itself, the second portion of the issue is to decide the state of the street. Each street is assigned a level: Good, Fair or Bad. This review is finished up from the information got from the Vibration Sensor. The calculation needs to recognize the grade that a greater part of the vehicles have transmitted. Since the need of this framework is to caution the driver of unpleasant landscapes ahead, the calculation needs a critical viewpoint to decide the grade of the street. Thus the problem statement can be split into two parts, the first is Identifying the Valid Data and the second is Determining the Grade of the Road. Note: Grade 1 = Good; Grade 2 = Fair; Grade 3 = Bad. The terms ‘Grade’ and ‘Experience Value’ have been used interchangeably.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
99f205fe60616b4f087ba895589b0894e7b9dbca,https://www.semanticscholar.org/paper/99f205fe60616b4f087ba895589b0894e7b9dbca,EDXL-LD and Architectural Tactics towards Information Sharing and Interoperability in Emergency Context,"s of Scientific Papers-WADEM Congress on Disaster and Emergency Medicine 2017 EDXL-LD and Architectural Tactics towards Information Sharing and Interoperability in Emergency Context Julia Kantorovitch, Jarmo Kalaoja, Ilkka Niskanen VTT Technical Research Center of Finland, Espoo/Finland Study/Objective: The objective of this study is to propose a new architectural approach supported by information models, to manage knowledge in the dynamic emergency context towards interoperable knowledge sharing and reuse. Background: The knowledge systems for emergency management are based on evolvable information provided by various actors, by diverse collections of sensors and information supplied by human volunteers. In order to achieve a common operational picture situation awareness, various knowledge, vocabulary and information models need to be aligned. This requires extendable time, application context architecture and models representing detailed evolvable knowledge about the types of adverse events, their potential impact and the means and resources that are best suited for response. The existing semantic research has a potential to address the identified needs, however the reported ontologies are rarely publically available, and they are also disconnected from widely used standard data models, data-exchange formats, and protocols related to emergency management. Methods: The literature review and the inputs provided by domain experts in the CONCORDE consortium and WHO, have facilitated the addressing of shortcomings and challenges identified above. Results: The Emergency Data Exchange Language (EDXL) based domain specific standards are taken as a base to create domain specific vocabularies. Vocabularies are published as a Linked Data (LD) and can be downloaded from GitHub software repository https://github.com/OntoRep/EDXL. The Model-View-Presentation (MVP) based architectural tactics as a software engineering pattern (see below) are exploited to achieve a desired extensibility and dynamicity of the system at its deployment stage. Conclusion: By keeping applications’ business logic separate from data and semantics, the underlying knowledge models can evolve without necessarily requiring changes to the interfaces and applications built on top of the models. Prehosp Disaster Med 2017;32(Suppl. 1):s228 doi:10.1017/S1049023X17005878 The J-SPEED: A Medical Relief Activities Reporting System for Emergency Medical Teams in Japan Tatsuhikiko Kubo, Hisayoshi Kondo, Yuichi Koido 1. Department Of Public Health, University of Occupational and Environmental Health, Kitakyusyu/Japan 2. National Disaster Medical Center, Tokyo/Japan Study/Objective: To introduce the J-SPEED; medical relief activities reporting system for Emergency Medical Teams (EMTs) of Japan. Background: During a disaster, information gathering and analysis are key elements for better coordination and timely response. Previous cases revealed that EMTs sometimes became the only capacity which could report medical, or more broadly health situations to a coordination body, and standardization of the reporting process from EMTs to the EMT Cortication Cell (EMTCC) will allow for better coordination, and for strengthening of the disease early warning system, since EMTs will act as additional sentinel reporting sites. One good existing model for this issue is the Surveillance in Post Extreme Emergencies and Disasters (SPEED) system employed in the Philippines. The SPEED was developed by Philippine’s Department of Health and the WHO in 2010. Based on the lessons learned from relief mission of the Japan Disaster Relief Medical Team against the super typhoon Yolanda in 2013, a Japanese version of the SPEED, so called J-SPEED has been developed and published in 2015. Methods: Field study. Results: The J-SPEED was first activated at the Kumamoto earthquake which occurred on April 14, 2016. During the 48 days of response, EMTs from various affiliation sent 1,828 daily reports to the EMTCC, which represented medical demand of 8,089 patients. Standardized information processing and quantitative information made communications among stakeholders efficient, and supported evidence, consensus based decision making by the local authority. Conclusion: Employment of the J-SPEED drastically changed the EMT coordination in Japan. Countries which don’t have a relevant system can easily set up a national reporting system utilizing the SPEED framework. Prehosp Disaster Med 2017;32(Suppl. 1):s228 doi:10.1017/S1049023X1700588X How do we Measure Severity? An Assessment of Five Indexes used in Sudden Onset Disasters and Complex Emergencies to Measure Severity and Risk Anneli Eriksson, Martin Gerdin, Thorkild Tylleskär, Johan Von Schreeb 1. Public Health Science, Karolinska Institutet, Stockholm/Sweden 2. Centre For International Health, Bergen University, Bergen/Norway 3. Department Of Public Health Sciences, Karolinska Institutet, Stockholm/Sweden SITUATIONAL AWARENESS SYSTEMS Prehospital and Disaster Medicine Vol. 32, Supplement 1 Study/Objective: The aim was to, 1) study the relation between disaster outcomes after earthquakes, expressed as number of dead and injured, and the performance of five preidentified severity, and risk-scoring indexes, 2) to inform a model that in an initial phase of a disaster can be used to predict severity and levels of need, and thereby guide toward the appropriate levels of response. Background: A disaster is as an event that overwhelms local capacity, necessitating national or international assistance. Disasters can be categorized, based on the type of hazard causing them. An earthquake is a hazard that can lead to a disaster. The disaster-severity depends on the magnitude of the hazard, underlying vulnerability, the level of exposure, coping capacity and the disaster response. While assistance should be based on needs, determined by the severity of a situation, there is no recognized way to compare severity between disaster contexts. Several initiatives have been developed to provide information on global severity and risks in disaster situations. In this study we compare five indexes and their ability to define severity: GDACs, GEO, KI’s 7-need, INFORM and ECHO’s Crisis index. Methods: We did a mapping of the existing indexes and indicators used. Index-scores were standardized and then compared with the number of dead and injured as an absolute outcome, in earthquakes with magnitude equal to or higher than 6,5 that occurred in populated areas, between year 2001 and November 2016. Results: The five indexes evaluated were all indicating the severity after the examined earthquakes. There was not one single index that gave an absolute correlation. Indexes built on higher numbers of indicators had several indicators that gave identical information. Conclusion: It is possible to predict the severity of a disaster through proxy indicators. The number of indicators used is not automatically increasing the preciseness or validity of the outcome. Prehosp Disaster Med 2017;32(Suppl. 1):s228–s229 doi:10.1017/S1049023X17005891 Enhanced Situational Awareness through a Decision Support Service for Optimal Allocation of Resources and Response Capacity Irene Christodoulou, George M. Milis, Panayiotis Kolios, Christos Panayiotou, Marios Polycarpou, Ilkka Niskanen 1. Kios Research Center For Intelligent Systems And Networks, University of Cyprus, Nicosia/Cyprus 2. VTT Technical Research Center of Finland, Espoo/Finland Study/Objective: We designed and developed e-services, aiming to support the decision makers during various contexts of medical emergency response, offering them machine-aided enhanced situational awareness. Background: Currently, decisions are being made by human experts with hands-on experience in emergency fields. However, in most cases, experts do not have the required computational capacity to process the relevant heterogeneous information and perform informed decisions. Evidently, time is a very critical parameter in emergency situations, especially in large-scale incidents with large number of casualties. Methods: Taking this into account the services we develop, are based on the mathematical modeling of optimization problems for timely resources’ allocation, addressing different phases of the response. The formulated problems address: i) the optimal allocation of Emergency Medical Services (EMS) units (in terms of demand satisfaction and time), to active emergency incident fields, ii) the optimal allocation (in terms of exploiting their capacities and response time) of EMS staff to tasks on the incident field such, as triage and retrieval running, transferring of patients to medical treatment area, offering medical treatment, and iii) the optimal allocation (in terms of profile matching, demand satisfaction and time) of patients to EMS vehicles and subsequently to first receivers (hospitals). The services are supported by semantic modeling of EMS vehicles, hospital, staff and patients profiles, as well as by machine learning tools that estimate demand for resources given historical emergency incident data. The services offer clear interfaces, so as to be interoperable with existing emergency management systems, as long as access to the necessary information is given. Results: Our solution achieves the recommendation on allocation of resources, based on real-time collected information from the emergency field. Conclusion: Further work will focus on modeling different cost functions in the optimization, so as to customize the recommendations based on incident and/or decisionmaker needs. Prehosp Disaster Med 2017;32(Suppl. 1):s229 doi:10.1017/S1049023X17005908 Comparison of UAV Technology vs No UAV Technology in Identification of Hazards at a MCI Scenario in Primary Care Paramedic Students Trevor N. Jain, Aaron Sibley, Henrik Stryhn, Ives Hubloue 1. Biology (paramedicine), University of Prince Edward Island/ Holland College, Charlottetown/PE/Canada 2. H",Prehospital and Disaster Medicine,2017.0,10.1017/S1049023X17005878,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
5d4b799353e01e0bd4ae4e1ddc14c620302d58ec,https://www.semanticscholar.org/paper/5d4b799353e01e0bd4ae4e1ddc14c620302d58ec,Big Data Analysis of News and Social Media Content,"The analysis of media content has been vital in social sciences, due to the key role that media plays in determining public opinion. This kind of analysis typically relies on the coding of the text being examined; a step that involves reading, annotating it, and that limits the sizes of the corpora that can be analyzed. The use of modern technologies from Artificial Intelligence allows researchers to computerize the process of applying different codes in the same text. Computational technologies also enable the automation of data collection, preparation, management and visualization. This provides opportunities for performing massive scale investigations, real time monitoring, and systemlevel modelling of the global media system. We describe how the analysis of content on Twitter can reveal mood changes of entire populations, how the political relations among US leaders can be extracted from large corpora, how we can determine what news people really want to read, how writing-style in articles change among different outlets etc. Most importantly, this paper aims to demonstrate some of the steps that can be automated, allowing researchers to access patterns that would be otherwise out of reach. Introduction The ready availability of masses of data and the means to exploit them is changing the way we execute science in many domains (Cristianini, 2010; Halevy et al., 2009). Molecular biology, astronomy and chemistry have already been transformed by the data revolution, undergoing an actual paradigm shift. In other domains, like social sciences (Lazer et al., 2009; Michel et al., 2011) and humanities (Moretti, 2011), datadriven approaches are just beginning to be deployed. This delay was caused by the density of the social interactions and the unavailability of digital data (Watts, 2007). Examples of works that deploy computational methods in social sciences include: the study of human interactions using mobile phones data (Onnela et al., 2007; González et al., 2008); the study of human interactions in online games and environments (Szell et al., 2010); or automating text annotation for political science research (Cardie et al., 2008). In particular, the analysis of media content in the social sciences has been a very important concern, due to the important role that media play in reflecting and determining public opinion (Lewis, 2001). This includes the analysis of both traditional news outlets, such as newspapers and and modern social media, such as Twitter where content is generated by users. The analysis of news content is traditionally based on the coding of data by human coders. This is a labourintensive process that restricts the possibility and potential of this kind of investigation. Computational methods from text mining and pattern recognition, originally developed for applications like e-commerce and information retrieval, can be deployed to automate many aspects of data collection, annotation, analysis and visualization. Sentiment Analysis of Twitter Content Since opinions are key influencers of human behavior, measuring the current public mood is an important and challenging task. The traditional approach would require questioning a large number of people about their feelings and opinions. Social media, such as Twitter or Facebook, can easily become a valuable source of information about the public due to the fact that people use them to express their feelings in public. As demonstrated in the study by Lansdall-Welfare et al. (2012), it is feasible to capture the public mood by monitoring the stream of Twitter data. The dataset that was analyzed was comprised of 484 million tweets that were generated by more than 9.8 million users, between July 2009 and January 2012. The data were collected from the 54 largest cities in the UK. The research focuses on tracking the four moods which are “Fear”, “Joy”, “Anger” and “Sadness”. For each and every mood, we track a long list of related words and we count the frequencies that these words appear in tweets. This process generates one timeline of the volume of connected tweets for each emotion. The further analysis of these timelines reveals that each of these emotions changes over time in a quite predictable manner. For an example, we found a periodic peak of joy around Christmas and a periodic peak of fear around Halloween. More surprisingly, we found that negative moods started to dominate the Twitter content after the announcement of enormous cuts in public spending on October 2010. Also there was a significant increase in anger in the weeks before the summer riots of August 2011. International Journal of Scientific & Engineering Research Volume 7, Issue 12, December-2016 ISSN 2229-5518 85 IJSER © 2016 http://www.ijser.org IJSER Fig.1 we plot the mood levels for the period of study and we visualize them as a facial expression using the Grimace tool. In the project website we have an interactive demo that can be used to explore our data 2 . A detailed interpretation of the results of this study is a challenging task. A scholar needs to understand the causes of the measured statistical quantities and relate them with specific events that affect society. The interpretation of the results can not be automated by using computational methods and it is a task that needs to be undertaken by social scientists. Narrative Analysis of US Elections Content analysis sometimes involves the identification of essential narrative information in a corpus. For example, determining the key actors, the key actions, and how actors interact. Quantitative Narrative Analysis (Franzosi, 1987; Earl et al., 2004) extracts Subject-Verb-Object (SVO) triplets and their study in order to analyze a corpus, while “distant reading” in humanities transforms the novels into networks as a first step of analysis (Moretti, 2011). The extraction of SVO triplets and turning them into a network is a process that can be automated and scaled up, enabling the analysis of big corpora. In the work by Sudhahar et al. (2012), computational methods are used to perform narrative analysis of the US Elections 2012. The dataset we analyzed comprised of 125,254 articles that were automatically categorized as being associated to the US Elections using the machine learning Support Vector Machines (SVM) approach (Cristianini, 2000).T he corpus text is pre-processed using GATE (Cunningham, 2002) tools and then we use Minipar parser (Lin, 1998) to extract Subject-Verb-Object triplets. We identified 31,476 actors from these triplets. A statistical analysis is used to identify the key actors and actions. Furthermore, we separate actions into two categories, endorsement and opposition. Finally, the key actors and actions were used to conclude the network of the US Elections. This network is presented in Fig. 2 were the protagonists of the Elections, “Obama” and “Romney”, are easily observed to dominate the networkExploring this network can give a useful insight on the topics that are most discussed during the elections, who the most dominant or least reported people are, how frequently the key actors are presented by media as endorsing or differing topics etc. Our results can be accessed online through an interactive website that i s built for monitoring the elections 3 . The website is updated daily for the duration of the elections. A further analysis r eq u i r e s a social scientist to find and deeply understand the causes and relate them with raw data in order to interpret the patterns that surface. Figure2. The key actors and their relations automatically extracted from a corpus of 125,254 articles associated to US Elections 2012 (Saatviga et al. 2012). Comparison of News Outlets on the basis of the Topics, Writing Style and Gender Bias Detection of differences or biases among news outlets is a trendy field of research in media studies. Nowadays, several aspects of content analysis can be automated: using machine learning techniques such as Support Vector Machines (Cristianini et al., 2000) we can evaluate text and detect general topics, such as Politics or Sports, that are present in a corpus; we can use natural language processing techniques to compute properties of the writing style of the text or detect mentions of people and their gender. Figure1. Analysis of 484 million tweets over a period of three years can reveal changes in public mood. We track four basic emotions and we can associate mood changes with important events (Lansdall-Welfare et al., 2012). International Journal of Scientific & Engineering Research Volume 7, Issue 12, December-2016 ISSN 2229-5518 86 IJSER © 2016 http://www.ijser.org IJSER In the work by Flaounas et al. (2012), a dataset comprised of 2.5 million news articles collected from 498 news outlets over a period of 10 months is analyzed. For each article we identify the general topics that it covered, the people those are mentioned and their gender (Cunningham et al., 2002), as well as two fundamental writing style properties, namely the readability and linguistic subjectivity. Readability is a measure of how easy it is to read a text calculated by taking into account the length of sentences and the length of the words in syllables (Flesch, 1948). Linguistic subjectivity is a measure of the usage of words that carry sentiment, i.e. the more words with sentiment present in an article, the more linguistically subjective it is (Pang, 2008). The computation of the aforesaid quantities allows different comparisons of sets of articles. For example, for the articles of each subject we calculated: a) the average readability finding that articles about sports are easy to read while articles on politics are the hardest to read; b) Linguistic subjectivity finding that articles about fashion and arts are the most biased while business articles were the most objective; c) the ratio of males over females – finding that articles about fashion and art mention the most women while articles about sports ",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d544a18a912e4bdafa0363bd3d5f349a9a25f7b1,https://www.semanticscholar.org/paper/d544a18a912e4bdafa0363bd3d5f349a9a25f7b1,The Effect of Knowledge-Based Methodologies on Networking,"It is agreed by researchers that classical information technology is an new topic which interests many people in the field of programming langudages and cyberneticists concur. Taking the current status of psychoacoustic communication into account, systems engineers famously desire the construction of Lamport clocks. It embodies the theoretical principles of distributed hardware and architecture. In this work the use of wireless information is going to disprove the fact that the Ethernet and web browsers are continuously incompatible. Introduction It has been agrreed by many physicists that , if it had not been for Boolean logic, the emulation for thin clients might never have taken place [1]. Our method synthesizes thin clients. We emphasize that our application investigates stable modalities. Contrarily, rasterization [2] itself won’t be able to accmplish the need for the visualization of forward-error correction. This work shows us two advancement in the above previous work. At the very begining, we consider how model checking can be an application to the synthesis of local-area networks. On a similar note, we propose a novel framework for the visualization of context-free grammar (Kapia). we use it to confirm the fact that active networks and flip-flop gates are rarely incompatible. The roadmap of the paper is followed as below. We encourage the need for wide-area networks. What’s more, our work is placed by us in contexts with the related work in the area of programming. Third, our work is also placed in context with the previous work in this field. While such a hypothesis may seem to be perverse, it is supported by prior work in the field. In the end, we make conclusions about it. Related Work In this section, Discussions on related research into the synthesis of the lookaside buffer is made, Boolean logic, and superblocks. Continuing with this rationale, David Patterson. It is developed a similar heuristic, but we had a firm convinction that Kapia is efficient on a large scale. On a similar note, a recent unpublished undergraduate dissertation [4] introduced a similar idea for scatter/gather I/O [5]. We believe there is room for both schools of thought within the field of algorithms. An algorithm for the investigation of active networks [5] proposed by Brown and Bose fails to address several key issues that Kapia does surmount [6]. Although we have nothing against the related approach by Jones [6], we do not believe that solution is applicable to steganography [7]. Advances in Computer Science Research, volume 75 104 Kapia Synthesis In the section we are going to propose a design in order to emulate the investigating of multicast systems. This is a confirmed property of our method. We show Kapia's psychoacoustic prevention in the first Figure . This seems to be hold in most cases. Results which are evaluated in the previous time have been used for a basis for all of the assumptions is . Though this result might seem to be unexpected, it was buffetted by prior work in this field . Fig 1: relationship between our heuristic network Our methodology depends on the confirmed design which is outlined in recent infamous work by Sally Floyd in the field of machine learning. Any unproven visualization of introspective archetypes will obviously requires that systems can be made lossless, collaborative, and Bayesian; Kapia is no exception. The methodology for our methodology is cmposed of four components: empathic theory, heterogeneous archetypes, the Turing machine, and Moore's Law. We figure out that the relationship between Kapia and decentralized symmetries in Figure 1. Our explored results in the previous are used as basis in all the assumptions. Fig 2: Our methodology's highly-available prevention. We have a belief that perfect configurations could manage collaborative technology without the need to manage lossless theory. We assume that redundancy can deploy link-level acknowledgements without needing to provide homogeneous communications. It is a structured property for our method. Considering the early architecture by Williams; our methodology has a common with it, but will in fact fulfill the purpose.Our previously improved results are used as a basis for all the assumptions. Impledmentation The hacked operating system have not be implemented. For this is the least significant elements of Kapia. The codebase of 70 Dylan files and the server daemon must be run on the node of the same.Such a hypothesis At the first sight such a hypothesis seems unexpected.But the hypothesis fell in line with what we have expected. We have not yet carried out the monitor of virtual machine. As it is the least important component of our heuristic. Steganographers do have complete controls Advances in Computer Science Research, volume 75 105 Fig 4: The average latency of Kapia It took time to set up a sufficient environment for software.All software components were closely connected with the use of GCC 0c, Service Pack 9 built on K. It is for collectively investigating active networks. All software was connected using GCC 6.0.5, Service Pack 4 with the help of Adi Shamir's libraries for opportunistically simulating collectively wired median time. Further, we made addition of the support for our framework as application whic an embedded one. A Cinclusions are made for usto discuss software modifications. Fig 5: results obtained Advances in Computer Science Research, volume 75 106 Dogfooding Our Heuristic Fig 6: a phenomenon worth exploring in its own right Great efforts and pains have been taken to make a description of our evaluation method setup; And now the payoff, is to have discussion of our results. With these considerations in mind, we ran four novel experiments: (1) If the flip-flop gates were not used but wen use ran the extremely mutually exclusive superpages, we asked (and answered) what will happen; (2) Information retrieval systems are run on 35 nodes to spread all the 2-node network; (3) With a simulated instant messenger workload 28 trials are run , and we made compariosns with the results to our software emulation. Analyzing the climactic is our second experiments. On a similar note, the curve in Figure 3 should look familiar; it is better known as n n h log ) (  . Similarly, we rarely predicted to what extent our results have been inaccurate in the phase of the evaluation approach. In Figure 5 it shows, the second half of experiments attract us to our system's effective response time. These work factor observations contrast to those seen in earlier work [7], such as N. Takahashi's seminal treatise on Markov models and observed expected interrupt rate. Further, bugs in system are cause of the invisible behavior all over the experiment. In the end , we will have a discussion about the first two experiments. Even if the result is often an important ambition, it has ample historical precedence. The curve in Figure 3 should look familiar; it is better known as n n F  ) ( * . Second, unstable experimental results have been the result of Gaussian electromagnetic disturbances in our Internet overlay network c. The feedback loop shows the key in Figure3;the result is shown the fact Kapia's speed did not converge. Conclusion In this work we have a validation that public-private key pairs can be made event-driven, self-learning, and cooperative. Continuing with this rationale, we argued that security in Kapia is not a challenge. In reality, through our work contributions that wen have made is that we explored an analysis of vacuum tubes (Kapia).And we use it to show that extreme programming and XML are generally incompatible. Potentially one of the tremendous drawbacks of Kapia is that it can store Moore's Law; we intend to address it in the future work. we are hoping steganographers will study Kapia for the future years. Our purpose here is to set the record straight. Finally, not only did Advances in Computer Science Research, volume 75 107 we disprove that the well-known ambimorphic algorithm for the improvement of Internet QoS runs in time. However that is also true for RAID. References [1] E. Codd, ""A case for XML,"" Journal of Self-Learning Archetypes, vol. 12, pp. 20-24, Jan. 1991. [2] J. Kumar, ""PearlyShim: Adaptive theory,"" in Proceedings of the Conference on Pervasive Theory, May 2002. [3] F. Moore, ""Harnessing courseware and XML using Taws,"" Journal of Introspective, Electronic Algorithms, vol. 96, pp. 86-107, Sept. 2004. [4] L. Shastri, ""The impact of heterogeneous models on theory,"" Journal of Secure, Psychoacoustic Symmetries, vol. 13, pp. 76-88, Sept. 1990. [5] Perlis, A. Newell, and B. Thompson, ""Pervasive, interposable modalities,"" in Proceedings of PODC, Feb. 2005. [6] Z. Raman, ""On the construction of the transistor,"" Journal of Real-Time, Pervasive Algorithms, vol. 92, pp. 158-199, Jan. 1992. [7] Shamir, L. Sun, and C. Hoare, """"fuzzy"", extensible modalities,"" in Proceedings of the Symposium on Homogeneous Symmetries, Oct. 2002.O. Jones, ""The effect of signed information on cryptography,"" University of Northern South Dakota, Tech. Rep. 1257-507-39, Nov. 2004. [8] Daubechies, V. Ramasubramanian, DuHua, and N. Martinez, ""Interrupts considered harmful, Homogeneous Models, Dec. 2003. [9] P. Li, ""atomic algorithms"" in Proceedings of MOBICOM, June 1990. [10]V. Ramasubramanian, ""Relational theory,"" on Proceess of the Conference on Embedded, Concurrent Modalities, Nov. 2004.",,2017.0,10.2991/mcei-17.2017.22,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
65bd9ba380a94ed82f9f56ab47ce781260529e94,https://www.semanticscholar.org/paper/65bd9ba380a94ed82f9f56ab47ce781260529e94,YouTube QoE Classification Based on Monitoring of Encrypted Traffic,"With the move to traffic encryption adopted by many Over The Top (OTT) providers of video distribution services, Internet Service Providers (ISPs) are now facing the challenges of monitoring application performance and potential end user perceived service quality degradations. With lack of direct feedback from OTT providers, ISPs generally rely on passive traffic monitoring solutions deployed within their network for the purposes of monitoring OTT service performance. The aim of this research is to develop tools and methodologies that enable the estimation of end user QoE when watching YouTube videos using different platforms and access networks, based solely on the analysis of encrypted network traffic. I. METHODOLOGY AND CURRENT RESULTS Prior to the widespread use of encryption in OTT traffic, network operators could gain insight into application performance by extracting packet header information. Today, the inability to monitor service performance at an application level poses a threat to network providers, as they are potentially unable to detect problems and act accordingly. Poor performance further imposes the risk of losing customers, as customers often tend to blame network providers for poor QoE. Given the current situation, ISPs commonly rely on passive traffic monitoring solutions deployed within their network to obtain insight into user perceived degradations and their potential causes. To address these challenges faced by ISPs, we have been studying the feasibility of estimating YouTube QoE based on monitoring of encrypted network traffic, by using machine learning (ML) techniques. To do so, we are developing a system called YouQ. YouQ enables applicationand network-level data collection, data processing and building ML models that can subsequently be used to estimate QoE of new YouTube streaming sessions based solely on network traffic features. To develop such a system, there is a need to understand how application Key Performance Indicators (KPIs) calculated from application-level data (such as stalling duration, initial delay, etc.) affect end users’ QoE (QoE modelling problem)[1]. Another challenge lies in extracting the This research is conducted in the scope of projects ”Survey and analysis of monitoring solutions for YouTube network traffic and application layer KPIs” and ”QoMoVid: QoE Monitoring Solutions for Mobile OTT Video Streaming”, both funded by Ericsson Nikola Tesla, Croatia. This work has also been supported in part by the Croatian Science Foundation under the project UIP-2014-09-5605 (Q-MANIC). The authors would like to thank Illona Popic, Petra Rebernjak and Ivan Bartolec for their help in running experiments. Fig. 1. Approach for QoE classification based on network traffic features network traffic features that can be correlated to applicationlevel degradations [2][3]. Our developed YouQ system consists of an Android application that plays a requested number of YouTube videos and logs events at an application level (e.g., video buffering, quality switch), amount of video in the buffer, and URLs from all HTTP requests. This data is collected using the YouTube IFrame API. In parallel to logging of application-layer KPIs, network traffic is also captured. After all the videos are played, both application-level logs and network traffic are uploaded to a YouQ server and processed. Processing includes calculating traffic features (e.g., average throughput, average interarrival time) for each of the videos in the experiment, calculating application-level KPIs from the collected logs (e.g., percentage of time spent on each quality level, stalling duration), and assigning a QoE class (“high”, “medium”, “low” QoE) to each video according to calculated KPIs and QoE models defined in [4]. The output of this phase is a dataset for training ML models. For each viewed video, we extract 33 traffic features (listed in [4]) based on the analysis of encrypted traffic, and classify the video into one of the three aforementioned QoE classes. The approach is depicted in Figure 1. Our approach was tested in a laboratory environment shown in Figure 2. YouTube traffic between an Android smartphone (Samsung S6 with Android version 5.1.1) and YouTube servers is transmitted over an IEEE 802.11n wireless network and then routed through a PC running IMUNES (www.imunes.net), a general purpose IP network emulation/simulation tool enabling a test administrator to set up different bandwidth limitations and schedule bandwidth changes. Traffic is further sent through Albedo’s Net.Shark device (a portable network tap used for aggregating and mirroring network traffic) where it Fig. 2. YouQ lab setup is replicated and sent to a PC designated for network traffic capturing. The PC running IMUNES also has an OS layer, accessed by the YouQ client application to run a bandwidth scheduling script according to defined experiments. The script resets the bandwidth envelope for each video in the experiment, which enables all videos to be played under exactly the same network conditions. We collected a dataset corresponding to 1060 videos played under 39 differently defined bandwidth conditions, and trained models by using various ML algorithms. The models proved to be up to 84% accurate when 3 QoE classes were defined (“high”, “medium”, “low”). We also classified videos into 2 QoE classes (“high” and “low”) and repeated the procedure. These models achieved an accuracy of 91%. The exact measurement procedure, definition of QoE classes, statistics of the collected dataset, along with a more detailed view of the results were published in [5], while [4] gives an even more detailed view of the test methodology used to train ML models for QoE classification, and provides a more detailed interpretation of classification results. II. ONGOING ACTIVITIES AND CHALLENGES Our current activities aim to further develop the YouQ system so as to enable data collection, processing, and the training of QoE classification models for different usage scenarios. By this we refer to scenarios in which YouTube is delivered over different access networks (WiFi and mobile), using different clients (browser-based vs. YouTube App), using different transport protocols (TCP, QUIC), and with different types of user behaviour observed. As a standard for estimating QoE of adaptive streamed media has recently been published in ITU-T recommendation P.1203 [6], we plan to incorporate the defined model within the YouQ system instead of the simplified multidimensional model we are currently using. A. Different client applications Besides the YouTube IFrame API, that we used to develop the initial YouQ client application, YouTube also offers a native Android API. We have developed a version of the YouQ Android application based on this API, which enables us to observe YouTube KPIs and traffic behaviour in the case when a user access YouTube via the dedicated YouTube App. We noticed that in this case QUIC protocol [7] is used, as opposed to TCP, that was used in the IFrame case. Further comparison of the two applications also showed the differences in YouTube’s adaptation algorithm. Considering these differences, models built using the data from the IFrame case are not applicable in this one, and new models should be built for this scenario. Another activity we plan, regarding the client applications, is building the YouQ application for iOS. B. Different transport protocols Using as a client device Samsung S6, we found that in all access network cases (WiFi, 3G, 4G) and using both the Chrome browser (version 55.0.2883.91) and the YouTube app (version 12.01.55), QUIC was used as the default protocol (Feb. 2017). As QUIC was formally proposed to the IETF as an Internet standard [7], it seems highly likely that QUIC will in the future be the base for YouTube functionality. Therefore, we plan to do an in depth study of the QUIC protocol and its impact on performance of the YouTube service. We already defined a set of QUIC traffic features that can be correlated to QoE, incorporated the calculation of these features into YouQ, and plan on running experiments to collect data and build classification models.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
c5ce6ba14f75e9316f40e126ed994965d4516738,https://www.semanticscholar.org/paper/c5ce6ba14f75e9316f40e126ed994965d4516738,Raspberry Pi Based Energy Efficient Load Monitoring System,"Internet of Things (IoT) promises to be a key enabler in Smart Manufacturing and Smart Supply Chain. These systems are characterized by reliable sensing and reporting of multiple parameters within the factory floor. Industrial IoT (IIoT) systems could suffer from high and uneven energy consumption due to the nature of the network deployment. Services like Twitter, Facebook, and Weibo have established a novel information channel that constantly provides real-time observations and situation reports from a worldwide community of users. Once made accessible, data from these sources could tremendously help to support information gathering in domains like disaster response, critical infrastructure management, and general public safety. In this paper , We proposed that energy monitoring of loads can be made efficient by employing twitter, only social network which is open source one with help of Raspberry pi system. Further, we propose a heuristic and opportunistic link selection algorithm, HOLA, which not only reduces the overall energy consumption of the IoT network but also balances it across the network. HOLA achieves this energy-efficiency by opportunistically offloading the IoT device data to smart-devices being carried by the workforce in the factory settings. I . INTRODUCTION The IoT is the internetworking of physical devices, vehicles , buildings, and other items (smart devices) embedded with electronics , sensors and network connectivity that enable these objects to collect and exchange data. The IoT allows objects to be sensed and controlled remotely across existing network infrastructure. It results in improved efficiency, accuracy and economic benefit in addition to reduced human intervention. With the progress of science and technology and the continuous improvement of living standards, various electric equipments are used more and more widely. The electric power network will be overloaded so much so that result to a widespread power outage with the electrical equipments used so much. Therefore, monitoring the operation data of electric equipments is very important. House is an important place for people's daily life, work and entertainment, and the domestic use of electricity is the basic unit is of social modernized development. The monitoring of electrical equipments which helps users to know the status of household electricity load to further reasonable formulate energy-saving plan, reduce energy consumption and expenditure . Electricity load monitoring of appliances has become an important task considering the recent economic and ecological trends. In this game, machine learning has an important part to play, allowing for energy consumption understanding, critical equipment monitoring and even human activity recognition. Nowadays, appliances are responsible of a significant part of the electricity bill in residential and commercial buildings. For instance in U.S. residential building, lighting and appliances represent 30% of the electricity consumption . Two approaches are existing: 1) Non-Intrusive Load Monitoring NILM consists in measuring the electricity consumption using a smart meter, typically placed at the meter panel. Relying on a single point of measure it is also called one-sensor metering. The qualification of non-intrusive means that no extra equipment is installed in the house. With NILM, the appliance signatures are superposed and, for comprehending the contribution of single appliances, they have to be separated. This operation is called disaggregation of the total electricity consumption. 2) Intrusive Load Monitoring ILM consists in measuring the electricity consumption of one or few appliances using a low-end metering device. The term intrusive means that the meter is located in the habitation, typically close to the appliance that is monitored. Services like Twitter, Facebook, and Weibo have established a novel information channel that constantly provides real-time observations and situation reports from a worldwide community of users. Once made accessible, data from these sources could tremendously help to support information gathering in domains like disaster response, critical infrastructure management, and general public safety . The main components are the IoT devices, Base Station, and the Cloud service provider. The IoT devices in such deployments are typically powered by devices such as Arduino . The plug-and-play nature of the devices requires them to use batteries as a source of energy. To conserve energy, the IoT devices use a low Vol-3 Issue-3 2017 IJARIIE-ISSN(O)-2395-4396 5624 www.ijariie.com 3613 power wireless communication protocol such as Bluetooth. The IoT devices collect data from their sensors and send it to the Base Station (BS) or Access Point (AP). The Bluetooth protocol has a short communication range of approximately 10 m. A typical manufacturing factory has rectangular shape with typical dimensions of 1000 m * 900 m , and the BS is located at one end of the plant/factory. In order to be able to reach the BS, the IoT devices form a Peer-to-Peer (P2P) multihop network. Thus the IoT devices not only sense and report data collected from the sensors, but also the data arriving from neighboring nodes down the link that needs to be forwarded to the BS over the Bluetooth interface. The BS, for example , could typically be powered by a Raspberry Pi device . The BS collects all the sensor data from the IoT devices over Bluetooth interface and sends it to the Cloud for further proce ssing over wired Ethernet interface. The Cloud is powered by an analytics platform. Some examples of IoT platforms are the TCS Connected Universe Platform (TCUP) and the Splunk platform for machine data. The network topology in the Bluetooth System leads to reduced energy efficiency of the IoT devices. Specifically, we observe that the nature of the network topology leads to increased energy consumption and geophysically skewed energy consumption within the IoT devices. The IoT devices at one end of the network sense and transmit only their sensor data. This results in low congestion and low energy consumption in these IoT devices. However the IoT devices at the center of the network are not only sensing and reporting their sensor data, but al so that of the IoT devices from down the link. This leads to moderate congestion and energy consumption. The IoT devices closer to the BS have to sense and transmit their own sensor data and also transmit the sensor data arriving from the rest o f the network. This leads to high congestion and energy consum ption in these IoT devices. These additional transmission responsibilities result in the IoT device operating its Bluetooth antenna for long durations. This, in turn, increases the energy consumption of the IoT device. The drawback of existing system is that it has a very low range due to the use of a Bluetooth module. As the Bluetooth module has a very low bandwidth it cannot be used to transmit large amount of information over long distances. II. PROPOSED SYSTEM High energy consumption in I T devices in an Industrial Internet setting is not desirable since it results in reduced network lifetime, and increased ca rbon footprint. Skewed or uneven energy consumption is n ot desirable as it makes planned maintenance of IoT devices for battery replacement challenging and increases the over all down time. With this in mind, a Heuristic and Opportunistic Link selection Algorithm (HOLA), for IoT systems that improves the energy-efficiency of IoT systems by reducing the overall energy consumption and balancing it across the network. HOLA achieves this energy-efficiency by opportunistically offloading the IoT device data to smart-devices (e.g., smart phones, tablets, etc.) being carried by the workforce in factory settings.",,2017.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
27272dd4cb685fb46812c5f919e484630d550d84,https://www.semanticscholar.org/paper/27272dd4cb685fb46812c5f919e484630d550d84,Usage of Cloud Computing Simulators and Future Systems For Computational Research,"Cloud Computing is an Internet based computing, whereby shared resources, software and information, are provided to computers and devices on demand, like the electricity grid. Currently, IaaS (Infrastructure as a Service), PaaS (Platform as a Service) and SaaS (Software as a Service) are used as a business model for Cloud Computing. Nowadays, the adoption and deployment of Cloud Computing is increasing in various domains, forcing researchers to conduct research in the area of Cloud Computing globally. Setting up the research environment is critical for the researchers in the developing countries to evaluate the research outputs. Currently, modeling, simulation technology and access of resources from various university data centers has become a useful and powerful tool in cloud computing research. Several cloud simulators have been specifically developed by various universities to carry out Cloud Computing research, including CloudSim, SPECI, Green Cloud and Future Systems (the Indiana University machines India, Bravo, Delta, Echo and Foxtrot) supports leading edge data science research and a broad range of computing-enabled education as well as integration of ideas from cloud and HPC systems. In this paper, the features, suitability, adaptability and the learning curve of the existing Cloud Computing simulators and Future Systems are reviewed and analyzed.",ArXiv,2016.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2e70faad56cd51136204ffdf88f0e2e4a5e881e4,https://www.semanticscholar.org/paper/2e70faad56cd51136204ffdf88f0e2e4a5e881e4,Understanding and improving Web page load times on modern networks,"This thesis first presents a measurement toolkit, Mahimahi, that records websites and replays them under emulated network conditions. Mahimahi improves on prior record-andreplay frameworks by emulating the multi-origin nature of Web pages, isolating its network traffic, and enabling evaluations of a larger set of target applications beyond browsers. Using Mahimahi, we perform a case study comparing current multiplexing protocols, HTTP/1.1 and SPDY, and a protocol in development, QUIC, to a hypothetical optimal protocol. We find that all three protocols are significantly suboptimal and their gaps from the optimal only increase with higher link speeds and RTTs. The reason for these trends is the same for each protocol: inherent source-level dependencies between objects on a Web page and browser limits on the number of parallel flows lead to serialized HTTP requests and prevent links from being fully occupied. To mitigate the effect of these dependencies, we built Cumulus, a user-deployable combination of a content-distribution network and a cloud browser that improves page load times when the user is at a significant delay from a Web page’s servers. Cumulus contains a “Mini-CDN”—a transparent proxy running on the user’s machine—and a “Puppet”: a headless browser run by the user on a well-connected public cloud. When the user loads a Web page, the Mini-CDN forwards the user’s request to the Puppet, which loads the entire page and pushes all of the page’s objects to the Mini-CDN, which caches them locally. Cumulus benefits from the finding that dependency resolution, the process of learning which objects make up a Web page, accounts for a considerable amount of user-perceived wait time. By moving this task to the Puppet, Cumulus can accelerate page loads without modifying existing Web browsers or servers. We find that on cellular, in-flight Wi-Fi, and transcontinental networks, Cumulus accelerated the page loads of Google’s Chrome browser by 1.13–2.36×. Performance was 1.19–2.13× faster than Opera Turbo, and 0.99– 1.66× faster than Chrome with Google’s Data Compression Proxy. Thesis Supervisor: Hari Balakrishnan Title: Professor, Electrical Engineering and Computer Science",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d934f9f831f40e0a270caf242c641ea9c91bdb96,https://www.semanticscholar.org/paper/d934f9f831f40e0a270caf242c641ea9c91bdb96,A Survey On Channel Estimation In Mimo Ofdm Systems |,"include synchronization techniques, channel estimation methods, adaptive practical the abstract: In this thesis channel estimation techniques for LTE downlink named Least Square, Minimum Mean Square error and Maximum Likelihood estimation techniques are studied for the pilot symbol based channel estimation. In addition to this the performances of these three channel estimation techniques were also studied by introducing averaging, interpolation and hybrid methods. This work also investigates the complexity of the channel estimation techniques in terms of the number of complex multiplications and by varying the FFT size and number of CP. furthermore, the effect of varying the number of antennas at the transmitter and receiver ends, where 2 x 2 and 4 x 4 antenna arrangements are considered as a case studies. The performance of these channel estimation techniques is also studied for EVA standard channel model in LTE. The considered channel model is EVA standard channel model with Doppler shift of 300HZ. Simulation results in this thesis show that the ML channel estimation technique has the best performance. In terms of number of complex multiplications it is proved the ML has lower complexity. From the interpolating techniques it is shown the performance of the algorithm integrated with hybrid technique has the best performance. In addition to this it is shown that as the number of transmit and receive antennas increase from 2 x 2 to 4 x 4 the performance of the estimator increases.This book constitutes the refereed proceedings of the First International Conference on Advanced Hybrid Information Processing, ADHIB 2017, held in Harbin, China, in July 2017. The 64 full papers were selected from 134 submissions and focus on advanced methods and applications for hybrid information processing.Enabling Technologies for Next Generation Wireless Communications provides up-to-date information on emerging trends in wireless systems, their enabling technologies and their evolving application paradigms. This book includes the latest trends and developments toward next generation wireless communications. It highlights the requirements of next generation wireless systems, limitations of existing technologies in delivering those requirements and the need to develop radical new technologies. It focuses on bringing together information on various technological developments that are enablers vital to fulfilling the requirements of future wireless communication systems and their applications. Topics discussed include spectrum issues, network planning, signal processing, transmitter, receiver, antenna technologies, channel coding, security and application of machine learning and deep learning for wireless communication systems. book also provides information on enabling business models for future Abstract: Abstract thesis entitled ""Adaptive Packet Scheduling in OFDM Systems"" DIAO ZhiFeng for the degree of Doctor of Philosophy University in October 2005 In this thesis, we study adaptive packet scheduling in orthogonal frequency division multiplexing (OFDM) systems. We start with a comprehensive and in-depth survey on recent research in scheduling algorithms in OFDM systems. Various representative algorithms are examined, and their advantages and disadvantages are analyzed and compared. In sample-spaced, our analysis shows that the power of the channel tap will leak to other taps of the same antenna, also to taps belonging to other antennas. We demonstrate that the MSE performance can be improved if more pilots are used, or fewer channels are estimated simultaneously. Channel estimation errors will affect the performance of different space time codes. By analysis and simulations, we find that block space time codes are aDevice-to-Device (D2D) communication will become a key feature supported by next generation cellular networks, a topic of enormous importance to modern communication. Currently, D2D serves as an underlay to the cellular network as a means to increase spectral efficiency. Although D2D brings large in terms of system capacity, it also causes interference as well as increased computation complexity to cellular networks as a result of spectrum sharing. Thus, efficient resource management must be performed to guarantee a target performance level of cellular communication. This brief presents the state-of-the-art research on resource management for D2D communication underlaying cellular networks. work D2D will use this book’s to help ensure their work is as efficient as possible. Along with key and and big sensor RFID; Digital Communications: Theory and Applications of OFDM begins with a brief overview of multi-carrier communications. The authors then focus on the bandwidth efficient technology of OFDM, in particular the DSP techniques that have made the modulation format practical. Several chapters describe and analyze the sub-systems of an OFDM implementation, such as clipping, synchronization channel estimation, equalization, and coding. Analysis of performance over channels with various impairments is presented. The book continues with descriptions of three very important and diverse applications of OFDM that have been standardized and are now being deployed: ADSL, digital broadcasting, and wireless LANs for multi-Mbps communications. Finally, the book concludes with describing the OFDM-based multiple access techniques, ultra wideband technology, and WiMAX.These two volumes constitute the Proceedings of the 7th International Workshop on Soft Computing Applications (SOFA 2016), held on 24–26 August 2016 in Arad, Romania. This edition was organized by Aurel Vlaicu University of Arad, Romania, University of Belgrade, Serbia, in conjunction with the Institute of Computer Science, Iasi Branch of the Romanian Academy, IEEE Romanian Section, Romanian Society of Control Engineering and Technical Informatics (SRAIT) - Arad Section, General Association of Engineers in Romania - Arad Section, and BTM Resources Arad. The soft computing concept was introduced by Lotfi Zadeh in 1991 and serves to highli ght the emergence of computing methodologies in which the accent is on exploiting the tolerance for imprecision and uncertainty to achieve tractability, robustness and lower costs. Soft computing facilitates the combined use of fuzzy logic, neurocomputing, evolutionary computing and probabilistic computing, leading to the concept of hybrid intelligent systems. The rapid emergence of new tools and applications calls for a synergy of scientific and technological disciplines in order to reveal the great potential of soft computing in all domains. The conference papers included in these proceedings, published post-conference, were grouped into the following areas of research: in Textiles The book helps to disseminate advances in selected active research directions in the field of soft computing, along with current issues and applications of related topics. As such, it provides valuable information for professors, researchers and graduate students in the area of soft computing techniques and applications.This book provides an overview of positioning technologies, applications and services in a format accessible to a wide variety of readers. Readers who have always wanted to understand how satellite-based positioning, wireless network positioning, inertial navigation, and their combinations work will find great value in this book. Readers will also learn about the advantages and disadvantages of different positioning methods, their limitations and challenges. Cognitive positioning, adding the brain to determine which technologies to use at device runtime, is introduced as well. Coverage also includes the use of position information for Location Based Services (LBS), as well as context-aware positioning services, designed for better user experience.This book introduces the theoretical elements at the basis ofvarious classes of algorithms commonly employed in the physicallayer (and, in part, in MAC layer) of wireless communicationssystems. It focuses on single user systems, so ignoring multipleaccess techniques. Moreover, emphasis is put on single-inputsingle-output (SISO) systems, although some relevant topics aboutmultiple-input multiple-output (MIMO) systems are also illustrated. Comprehensive wireless specific guide to algorithmictechniques Provides a detailed analysis of channel equalization andchannel coding for wireless applications Unique conceptual approach focusing in single user systems Covers algebraic decoding, modulation techniques, channelcoding and channel equalisationSpace-time array communications have gained a great deal of interest in recent years. Its superior performance in practical multipath propagation environments has established it as a core aspect in next generation mobile networks, as well as several portable wireless communication systems. In fact the employment of the sensor array component has already been provided for in the current UMTS standard, and there is presently a major thrust to new applications. From to cross layer Adaptation in Wireless covers all aspects of in Each provides a unified for understanding and relates various specializations terminologies. addition to simplified cross layer design approaches, they also describe advanced techniques, such as adaptive 4G communications, and energy and mobility aware MAC protocols.The Accessible Guide to Modern Wireless for Undergraduates, Graduates, and Practicing Electrical Engineers Wireless is a critical discipline of and computer yet the concepts have remained elusive for students who are not area. This text makes digital and for graduates, Notably, the book does prior courses on or digital Introduction to Wireless Digital Communication establishes the principles of communication, from a digital signal processing including key mathematical background, transmitter and receiver signal processing algorithms, channel models, and generalizations to multiple",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6afcb962a010494b94ba09d36a330e59d560b433,https://www.semanticscholar.org/paper/6afcb962a010494b94ba09d36a330e59d560b433,Integration of High Performance Computing into Engineering Physics Education,"Computational skills are foundational in engineering physics education. Computational exercises, labs, and projects often employ instructive small­scale problems. These small­ scale problems serve to introduce content and process, and as such, serve the purpose for which they were intended.  Small­scale problems do not serve to introduce students to solving problems at industrial­scale or with research­quality as required in the workplace or graduate laboratory This paper describes the integration of industrial­scale and research­quality high­performance computing (HPC) into a senior/graduate level fluid dynamics course. This paper focuses on a combined senior level­graduate level course (enrollment of 12) in fluid dynamics at the University of Central Oklahoma, a predominantly undergraduate institution (PUI) . A HPC cluster, Buddy has been deployed recently at the UCO. The first author operates and administers the Buddy cluster and serves as instructor of the fluid dynamics course, providing an opportunity to advance the course outcomes to include a high impact project that takes advantage of distributed computing. These projects will be transformative for the students and expose them to HPC “at scale.” The projects require the use of computational fluid dynamics (CFD) on an HPC system; intentionally exposing students to a new way of doing things. The issues that students must confront include: 1) complex geometric modeling that result in very large file sizes, 2) meshing geometries that are large or require many nodes, 3) transitioning files generated on a desktop computer to a HPC environment, 4) understanding navigation and use of an HPC system, 5) understanding the use of parallelism in a distributed computing environment, 6) quantifying results, and 7) visualizing results. The goal of this work is to impact the student’s long term ability to deal with computationally intensive problems. Although we cannot determine the impact long term yet, we are using a rubric to gauge the immediate impact and surveying the students to determine their perceptions. Introduction The National Science Foundation (NSF) report entitled “Cyberinfrastructure Vision for 21st Century Discovery”​1​ addresses how high performance computing (HPC) is necessary to science and engineering disciplines to answer the most basic research questions and to solve technical problems of national need. More recently the White House has undertaken the National Strategic Computing Initiative​2​ which is a call to “maximize the benefits of high performance computing (HPC) research, development, and deployment.” The increased use of HPC clearly results in the need to train engineers how to appropriately use HPC in their work as HPC becomes more ubiquitous in industry. The use of computational tools in engineering education is so common it is essentially codified by ABET; currently as part of ​Criterion 3. Student Outcomes​3​. ​Accordingly, students across undergraduate engineering programs get exposed to computer programming, modeling software, mathematical engines, spreadsheets, and simulation. Specific engineering disciplines are exposed to more focused software for computer­aided design, circuit design, machining, data acquisition; and more and more students are using microcontrollers to implement their own electro­mechanical systems. These software, and where appropriate, the attached hardware, are almost exclusively run on or controlled by either desktop or laptop computers. As a result of the availability and accessibility of HPC resources some have been able to enhance traditional engineering and computing curricula using HPC​4­6​. Background This paper documents activities of integrating HPC at the University  of ___ (U__), which is a metropolitan university with an enrollment of over 17,000 students and a predominantly undergraduate institution (PUI). At UCO, undergraduate research has been supported and nurtured across campus; and recognized by the Council on Undergraduate Research (CUR) as a national model for implementing programs in undergraduate research​7​. Campuswide grant programs for faculty grants and student grants are in place. The student grants program is of particular note; in Research, Creative, and Scholarly Activities (RCSA) Grants encourage students to collaborate with a faculty member to write a grant proposal. If funded the student receives up to $500 for supplies and equipment, works five hours per week as a research assistant, and receives a partial tuition waiver. This program has grown considerably in the last several years and now funds over 130 students each year. Within the College of Mathematics and Science (CMS) additional programs are in place to cultivate undergraduate research. Center for Undergraduate Research and Education in Science, Technology, Engineering, and Mathematics (CURE­STEM) Scholars (approx. one­third of CMS faculty) receive funding for reassignment time, travel, student research assistants, and supplies. The CURE­STEM Scholars are required to submit one national­level (e.g. National Science Foundation ­ NSF) grant per year. This program has been in place for eight years and has shown a tremendous return on investment of over $10 brought in for every $1 invested. Co­author Lemley has been a CURE­STEM Scholar and also serves as the director of a computational center within CMS called CREIC (Center for Research and Education in Interdisciplinary Computation), whose goal is to stimulate and enable faculty and their students to embed computation into their research and classes. CREIC has been focused on establishing a HPC resource on campus for several years, and was successful in obtaining an NSF Major Research Instrumentation (MRI) grant, with co­author Lemley as the Principal Investigator (PI), in early 2015. The NSF­MRI has funded the first HPC cluster supercomputer (Buddy) on campus and now co­author Lemley is helping researchers at UCO use Buddy to perform their work. This study described in this paper was conducted, in part, during a 3­semester hour fluid dynamics course, ENGR 4533/5443, in Fall 2015 at UCO. This course is a follow­on course to a junior level engineering fluid mechanics course and was made up of six undergraduates and five graduates. This course covers continuum viscous fluid dynamics; the first portion of the course is focused on understanding and applying the Navier­Stokes equations (NSE), which are a set of partial differential equations describing fluid flow.. The latter part of the course is focused on using computational fluid dynamics (CFD) to solve the NSE. Individual CFD projects were completed by the students. In these projects, students were required to develop a problem that needed significant computational resources ­ such that it was not reasonable to run the simulations on a single computer workstation. The goals for implementing the CFD project in this way was to make an impact on the student’s long term ability to use HPC when they graduate, which is becoming a necessary engineering technical skill. Methodology The CFD projects in ENGR 4533/5443 were started in late September 2015. The students were asked to develop a problem that was either of research interest or industrial­scale, meaning, not only would CFD be required, but HPC using the Buddy supercomputer cluster would also be required. The project was carried out as shown in Table 1. Table 1.​ Details of CFD Project in EN__ 43__/54__. Assignment  Description When Credit Project Proposal  Up to two page proposal with citations. Late September  10% Homework / 2.5% Course Project Description   Five Minute Proposal to Class Early October  10% Homework / 2.5% Course Computer­Aided Design Model Email to Instructor  Mid­October  10% Homework / 2.5% Course Meshed Geometry  Email to instructor  Late October  10% Homework / 2.5% Course Project Poster Session  Large Format Poster ­ Interviews by faculty on oral presentation and appearance (rubric). Early December  50% Project / 12.5% Course Project Poster Technical Review Instructor technical review of poster results using rubric. Mid­December  50% Project / 12.5% Course Survey Survey of student perceptions of the project. January 2016  NA The ​Project Proposal​ was a two page proposal with citations that described the problem on which the student wanted to work. The requirements were the motivation for working on a given problem, the geometry that would be considered, and as much detail about boundary conditions as possible. This proposal was emailed to the instructor, who then either spoke with the student in class or emailed them about tweaking their proposal so that it was appropriate for the class and truly needed a cluster supercomputer for solution. The ​Project Description ​took place roughly two weeks after the ​Project Proposal​ and involved a five minute presentation in a regular class meeting. By this point the students were expected to have a sketch or initial computer­aided design (CAD) of the geometry and detailed boundary conditions. There was a small amount of time for questions by the audience. Over the next several weeks the instructor spent several class periods covering the topics in Table 2, with the goal of taking student through a complete project starting with a CAD drawing all the way to visualizing and calculating results. the goal was to ensure that students had seen the many issues that arise in what is often a tedious process. In addition, by having some sessions where the students worked on their projects using the Buddy Cluster, the instructor could see from the student perspective and see what did and did not work well for them. Also he could observe how well the documentation for the project and using Buddy had been written. Table 2.​ Learning Modules in the CFD Project. Learning Module Approximate Time Used in Class Importing CAD files into Meshing Software 10 minutes Meshing a Geometry and Preparation for CFD 30 minutes Logg",,2016.0,10.18260/p.25426,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
22ac89ac28b73aca8a72e8797b3a7e494306a657,https://www.semanticscholar.org/paper/22ac89ac28b73aca8a72e8797b3a7e494306a657,Big Data? More Challenges!,"Recent advances in data acquisition technologies have led to massive amount of data being collected routinely in the physical, chemical, and engineering sciences as well as information sciences and technology. In addition to volume, the data often have complicated structure. Examples of such Big Data include the data streams obtained from complex engineering systems, image sequences, climate data, website transaction logs, credit card records, and so forth. Because of their big volume and complicated structure, big data are difficult to handle using traditional database management and statistical analysis tools. They create many new challenges for statisticians to describe and analyze them properly. To face the challenges and promote new statistical methods in handling big data, Technometrics decided to have a special issue on that topic in late 2013, and a guest editorial board was established soon after the decision. The board includes Drs. Ming-Hui Chen, Radu V. Craiu, Robert B. Gramacy, Willis A. Jensen, Faming Liang, Chuanhai Liu, and William Q. Meeker as associate editors, and me as editor. The Call for Papers was published in the journal and some other media in early 2014. We received 23 high-quality submissions before the deadline. All submissions went through the regular review procedure of the journal. Besides the people in the guest editorial board, some associate editors on the regular editorial board of the journal also helped handle some submissions. Finally, 11 articles were selected to publish in the special issue, which cover a wide range of topics in describing, analyzing and computing big data. These articles are briefly discussed below. The first five articles proposed numerical algorithms that can analyze big data fast. In “Orthogonalizing EM: A Design-Based Least Squares Algorithm,” Shifeng Xiong, Bin Dai, Jared Huling, and Peter Z. G. Qian, propose an efficient iterative algorithm intended for various least squares problems, based on a design of experiments perspective. The algorithm, called orthogonalizing EM (OEM), works for ordinary least squares and can be extended easily to penalized least squares. The main idea of the procedure is to orthogonalize a design matrix by adding new rows and then solve the original problem by embedding the augmented design in a missing data framework. In “Speeding Up Neighborhood Search in Local Gaussian Process Prediction” by Robert B. Gramacy and Benjamin Haaland, the authors suggest an algorithm for speeding up neighborhood search in local Gaussian process prediction that is commonly used in various nonlinear and nonparametric prediction problems, particularly when deployed as emulators for computer experiments. The third article titled “A Bootstrap Metropolis-Hastings Algorithm for Bayesian Analysis of Big Data” by Faming Liang, Jinsu Kim, and Qifan Song proposes a so-called bootstrap MetropolisHastings (BMH) algorithm that provides a general framework to tame powerful MCMC methods for big data analysis. The main idea of the algorithm is to replace the full data log-likelihood by a Monte Carlo average of the log-likelihoods that are calculated in parallel from multiple bootstrap samples. The fourth article, “Compressing an Ensemble With Statistical Models: An Algorithm for Global 3D Spatio-Temporal Temperature,” by Stefano Castruccio and Marc G. Genton suggests an algorithm for compressing 3D spatio-temporal temperature using statistics-based approach that explicitly accounted for the space-time dependence of the data. The fifth article, “Partitioning a Large Simulation as It Runs” by Kary Myers, Earl Lawrence, Michael Fugate, Claire McKay Bowen, Lawrence Ticknor, Jon Woodring, Joanne Wendelberger, and Jim Ahrens covers analysis of data streams, in which data were generated sequentially and data storage, transferring, and analysis were all challenging. The authors suggested a so-called online in situ method for identifying a reduced set of time steps of the data and data analysis results to save in the storage facility, in order to significantly reduce the data transfer and storage requirements. The next two articles are about machine learning methods for handling big data. In the first article, “High-Performance Kernel Machines With Implicit Distributed Optimization and Randomization,” Haim Avron and Vikas Sindhwani propose a framework for massive-scale training of kernel-based statistical models, based on combining distributed convex optimization with randomization techniques. The second article, “Statistical Learning of Neuronal Functional Connectivity,” by Chunming Zhang, Yi Chai, Xiao Guo, Muhong Gao, David Devilbiss, and Zhengjun Zhang identifies the network structure of a neuron ensemble beyond the standard measure of pairwise correlations, which is critical for understanding how information is transferred within such a neural population. The spike train data posed a significant challenge to conventional statistical methods due to not only the complexity, massive size, and large scale, but also the high dimensionality. In this article, the authors propose a novel “structural information enhanced” (SIE)",Technometrics,2016.0,10.1080/00401706.2016.1196946,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
7b4b4b0c6aed5e6f850872b52ea1e4d3457b2b6c,https://www.semanticscholar.org/paper/7b4b4b0c6aed5e6f850872b52ea1e4d3457b2b6c,"The Semantic Web: Research and Applications, 5th European Semantic Web Conference, ESWC 2008, Tenerife, Canary Islands, Spain, June 1-5, 2008, Proceedings",,ESWC,2008.0,10.1007/978-3-540-68234-9,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4f2c4c608ebdb6264e9dcdd2004ebc1297dbb04d,https://www.semanticscholar.org/paper/4f2c4c608ebdb6264e9dcdd2004ebc1297dbb04d,Editorial: Special Issue on Space Robotics,"Space robotics continues to advance and to capture the imagination. In the past few years fictional depictions of robots – think Star Wars as well as many other popular movies, books and games – are notable for the narrowing gap with our current technical reality. Our space robotic systems are becoming more capable, intelligent, and sometimes even humanoid in form. This is the fifth special issue on space robotics published by the Journal of Field Robotics. Our previous issues in January 2007, March 2009, June 2012, and September 2013 each presented advances in robotics technologies for space applications. Each special issue has followed the biannual International Symposium on Artificial Intelligence, Robotics, and Automation in Space sponsored by the Canadian Space Agency, the German Aerospace Agency (DLR), the European Space Agency (ESA), the Japan Aerospace Exploration Agency and the U.S. National Aeronautics and Space Administration. Since our last special issue there have been many exciting developments in space exploration including a new rover, Yutu (“Jade Rabbit”), landed on the moon by the Chinese National Space Agency. The rover, part of the Chang’e 3 mission operated for one lunar day and returned video and scientific data. Robots landed on comets, Philae by the ESA/DLR made a first-ever soft landing, and flew by asteroids and planets while autonomously making observations. Spectacular images of Pluto were all targeted and acquired robotically. An exciting development in recent years is the proliferation of cubesats, which through small scale and standardized parts, dramatically lower the cost and complexity of orbital, and perhaps, surface exploration. An underlying goal of the research reported in this special issue is to improve autonomous operations in order to extend the capabilities of current and future robotic missions. They close the gap between fact and fiction. There are a number of robotic systems on the International Space Station, including SPHERES, an untethered, self-propelled platform of science and technology experiments. The creation and deployment has previously been reported in JFR. In “An Open Research Facility for VisionBased Navigation onboard the International Space Station,” Tweddle et al. report on the development and deployment of autonomous navigation by a free-flying robot in the space station. The VERTIGO Goggles for SPHERES is now operational and serving as a robotic platform for ongoing experiments. To truly automate a science-driven space mission, it is clear that we must take care of a number of engineering tasks such as state estimation and control. However, we may also need to automate aspects of the science itself. In “RealTime Orbital Image Analysis Using Decision Forests, with a Deployment Onboard the IPEX Spacecraft”, Altinok et al. describe a procedure to automatically recognize clouds that impede the ability to take atmospheric measurements for a satellite-based, Earth-observation mission. They show that their method is able to run in real-time onboard a real spacecraft, one of the first times a machine-learning approach has actually been deployed on a space mission. They provide insight lessons learned from their experience that may inform similar future implementations of science autonomy. Several future on-orbit mission concepts call for rendezvous and/or proximity operations in space between multiple spacecraft. Examples include on-orbit servicing, sample return, space station resupply, constellations of satellites, to name a few. When a human is not available to take control during rendezvous, a method of relative navigation is required during approach. In “Cooperative Relative Navigation for Space Rendezvous and Proximity Operations using Controlled Active Vision”, Zhang et al. demonstrate a new low-computational-cost relative navigation approach using a single monocular camera and a set of fiducial markers. Their experiments show that their approach achieves similar accuracy to the state of the art at a fraction of the computational cost. Their low-cost approach will hopefully find application on future space rendezvous and multi-craft missions where computational cost is a big constraint. On planetary surfaces, Bajpai et al. look at the problem of localization in feature-sparse terrain. With “Planetary Monocular Simultaneous Localization and Mapping” they detail the design and experimental verification of a method to detect salient features in the environment, localize and track them over time, and estimate rover path with higher accuracy than dead reckoning or visual odometry alone. Their approach is demonstrated in simulated lunar terrain and Mars-like imagery from the Atacama Desert in Chile. Similarly considering planetary surfaces there is the challenge of building accurate models of natural terrain. In “3D Scan Registration using Curvelet Features in Planetary Environments“Ahuja, et al. formulate a method of aligning point clouds that exceeds the accuracy of traditional algorithms by an order of magnitude. In examples from real scans of they show detailed rather than noisy reconstruction and resulting model that is useful for precision navigation. A technical journal can only equal the quality of the research reported, so we congratulate and thank all of the authors who present their work in this special issue. Our",J. Field Robotics,2016.0,10.1002/rob.21649,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d976c45b4b2b388b5eee4d29878ea7c94f373dfe,https://www.semanticscholar.org/paper/d976c45b4b2b388b5eee4d29878ea7c94f373dfe,Qurb: Qatar Urban Analytics,"Doha is one of the fastest growing cities of the world with a population that has increased by nearly 40% in the last five years. There are two significant trends that are relevant to our proposal. First, the government of Qatar is actively engaged in embracing the use of fine-grained data to “sense” the city for maintaining current services and future planning to ensure a high standard of living for its residents. In this line, QCRI has initiated several research projects related to urban computing to better understand and predict traffic mobility patterns in the city of Doha [1]. Second trend is the high degree of social media participation of the populace, providing a significant amount of time-oriented social sensing of the all types of events unfolding in the city. A key element of our vision is to integrate data from physical and social sensing, into what we call socio-physical sensing. Another key element of our vision is to develop novel analytics approaches to mine this cross-modal data to make various applications for residents smarter than they could be with a single mode of data. The overall goal is to help citizens in their every-day life in urban spaces, and also help transportation experts and policy specialists to take a real time data-driven approach towards urban planning and real time traffic planning in the city. Fast growing cities like Doha encounter several problems and challenges that should be addressed in time to ensure a reasonable quality of life for its population. These challenges encompass good transportation networks, sustainable energy sources, acceptable commute times, etc. and go beyond physical data acquisition and analytics. In the era of Internet of Things [5], it has become commonplace to deploy static and mobile physical sensors around the city in order to capture indicators about people's behaviour related to driving, polluting, energy consumption, etc. The data collected from physical as well as social sensors has to be processed using advanced exploratory data analysis, cleaned and consolidated to remove inconsistent, outlying and duplicate records before statistical analysis, data mining and predictive modeling can be applied. Recent advances in social computing have enabled scientists to study and model different social phenomena using user generated content shared on social media platforms. Such studies include the spread of diseases on social media [3] and studying food consumption in Twitter [4]. We envision a three layered setting: the ground, physical sensing layer, and social sensing layer. The ground represents the actual world (e.g., a city) with its inherent complexity and set of challenges. We aim at solving some of these problems by combining two data overlays to better model the interactions between the city and its population. QCRI vision is twofold: From a data science perspective: Our goal is to take a holistic cross-modality view of urban data acquired from disparate urban/social sensors in order to (i) design an integrated data pipeline to store, process and consume heterogeneous urban data, and (ii) develop machine learning tools for cross-modality data mining which aids decision making for the smooth functioning of urban services; From a social informatics perspective: Use social data generated by users and shared via social media platforms to enhance smart city applications. This could be achieved by adding a semantic overlay to data acquired through physical sensors. We believe that combining data from physical sensors with user generated content potentially leads to the design of better and smarter lifestyle applications such as “evening out experience” recommenders that optimize for the whole experience including driving, parking and restaurant quality; Cab finder that takes into account the current traffic status, etc. Figure 1. Overview of Proposed Approach. In Fig. 1 we provide a general overview of our cross-modality vision. While most of the effort toward building applications assisting people in their everyday life has focused on only one data overlay, we claim that combining the two overlays of data could generate a significant added value to applications on both sides. References [1] Chawla, S., Sarkar, S., Borge-Holthoefer, J., Ahamed, S., Hammady, H., Filali, F., Znaidi, W., “On Inferring the Time-Varying Traffic Connectivity Structures of an Urban Environment”, Proc. of the 4th International Workshop on Urban Computing (UrbComp 2015) in conjunction with KDD 2015, Sydney, Australia. [2] Sagl, G., Resch, B., Blaschke, T., “Contextual Sensing: Integrating Contextual Information with Human and Technical Geo-Sensor Information for Smart Cities”. Sensors 2015, 15, 17013–17035. [3] Sadilek, A., Kautz, H. A., Silenzio, V. “Modeling Spread of Disease from Social Interactions.” ICWSM. 2012. [4] Sofiane Abbar, Yelena Mejova, and Ingmar Weber. 2015. You Tweet What You Eat: Studying Food Consumption Through Twitter. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ‘15). ACM, New York, NY, USA, 3197–3206. [5] Atzori, L., Iera, A., Morabito, G. “The internet of things: A survey.” Computer networks 54.15 (2010): 2787–2805.",,2016.0,10.5339/QFARC.2016.ICTPP3360,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4f8c4025b2ce0c0eb837b7389bba31f6f1cae7c7,https://www.semanticscholar.org/paper/4f8c4025b2ce0c0eb837b7389bba31f6f1cae7c7,Implementing Operational Analytics using Big Data Technologies to Detect and Predict Sensor Anomalies,"Operational analytics when combined with Big Data technologies and predictive techniques have been shown to be valuable in detecting mission critical sensor anomalies that might be missed by conventional analytical techniques. Our approach helps analysts and leaders make informed and rapid decisions by analyzing large volumes of complex data in near real-time and presenting it in a manner that facilitates decision making. It provides cost savings by being able to alert and predict when sensor degradations pass a critical threshold and impact mission operations. Operational analytics, which uses Big Data tools and technologies, can process very large data sets containing a variety of data types to uncover hidden patterns, unknown correlations, and other relevant information. When combined with predictive techniques, it provides a mechanism to monitor and visualize these data sets and provide insight into degradations encountered in large sensor systems such as the space surveillance network. In this study, data from a notional sensor is simulated and we use big data technologies, predictive algorithms and operational analytics to process the data and predict sensor degradations. This study uses data products that would commonly be analyzed at a site. This study builds on a big data architecture that has previously been proven valuable in detecting anomalies. This paper outlines our methodology of implementing an operational analytic solution through data discovery, learning and training of data modeling and predictive techniques, and deployment. Through this methodology, we implement a functional architecture focused on exploring available big data sets and determine practical analytic, visualization, and predictive technologies. APPROACH This study developed an operational analytics implementation that uses Big Data technologies and machine learning algorithms to determine and predict sensor anomalies. A previous study [1] showed that Big Data Analytics can uncover anomalies that may be missed through conventional analyses. This study enhances that effort and shows a methodology to implement operational analytics that can be applied toward common solutions for data analysis. Our operational analytics implementation relies on continuous learning from historical data to analyze data in the stream of real-time operations. In the previous study, where data was identified that can be used to uncover anomalies, this implementation extends that approach and now identifies trends and correlations that reveal anomalies that can be missed by traditional analytic techniques with limited datasets. This study adopted a three-step methodology to implementing operational analytics – Discovery, Modeling and Operations as shown in Fig. 1. Copyright © 2016 Advanced Maui Optical and Space Surveillance Technologies Conference (AMOS) – www.amostech.com Fig. 1. Operational Implementation Approach Fig. 1 shows the three steps to implement operational analytics and the continuous feedback between learning and operational deployment. The following sections will elaborate on the methodology employed as applied to a realworld problem of analyzing large datasets such as would be encountered at an operational site.",,2016.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d4ef912f130eb2d791a1d65350e0d30d575f809d,https://www.semanticscholar.org/paper/d4ef912f130eb2d791a1d65350e0d30d575f809d,"Examining the Integration and Motivational Impact of Hands on Made4Me: Hands-on Machining, Analysis and Design Experiences for Mechanical Engineers","This paper reports our current progress towards introducing hands-on machining, analysis and design experiences in freshman, sophomore, and capstone design courses in the Department of Mechanical Engineering at the University of Massachusetts Lowell. The selection, assembly, and deployment of two low-cost, desktop computer-numerical-control (CNC) platforms is described along with our current plans for deploying desktop CNCs throughout a sequence of undergraduate engineering design courses. Finally, we present our proposed approach to evaluate the impact of curricular enhancement on our mechanical engineering students’ cognition, motivation and attitudes toward the profession. 1.0 Introduction The design of modern industrial products is an increasingly intricate process requiring a concomitant increase in the knowledge and skill-set of typical undergraduate engineering students who can successfully contribute to the product conception and creation process. The increased globalization of modern industry and its supply chain in new product development and manufacturing has resulted in the dispersion of the components and resources needed to realize new global products. The concept of “design anywhere – build anywhere” provides flexibility for U.S. manufacturers to remain globally competitive, yet results in a lack of proximity between the designer and the manufacturing elements of the product realization cycle. These evolving conditions create a pressing need for graduates capable of systems thinking and understanding the manufacturing and product development cycle, from making informed costand quality-based design decisions, analyzing these designs, to producing and ultimately testing these designs to ensure conformance with specifications. Our educational project aims to harness the recent proliferation of low-cost, multi-axis computernumerical-control machines to address these evolving market needs within the constraints of engineering design education. The lower-cost and lower-accuracy hobbyist CNC machines have largely benefited from the support of a growing hobbyist and open-source community eager to develop and capitalize on advanced machining and prototyping methods for home-use. The result has been a wide array of desktop tools that can be used for advanced prototyping with lowercost materials (machining wax, wood, PLA/ABS plastic filament, etc). These tools ultimately provide the end user with an opportunity for real, hands-on prototyping and advanced machining experiences at a fraction of the cost of commercially available machines. This project examines the selection, development and integration of desktop CNC technology throughout an undergraduate mechanical engineering curriculum and investigates how this technology can enhance student learning, motivation and attitudes towards engineering. Through the use of lower-cost desktop CNC machines, the students will be able to directly interface with advanced machining technology rather than treat these tools as expensive black boxes that demand more experienced operators. Moreover, through the desktop prototyping experiences, students will be able to explore multiple alternative conceptual designs and learn through iterative problem solving and design. As a result of this enhanced ability to quickly and efficiently implement ideas into physical models, we believe students will be encouraged to creatively approach engiP ge 24658.2 neering product design. With these lower-cost desktop CNC machines, we aim to introduce the science and engineering behind modern prototyping and manufacturing while enabling students to explore, experience and understand the role of prototyping in modern engineering within an integrated, hands-on, design oriented environment. This integration and expansion of hands-on, design-build-test coursework at the University of Massachusetts Lowell is expected to help undergraduate students to develop a more positive attitude, increased self-confidence and improved motivation towards design and manufacturing. We expect to produce future engineers with the increased skill-set necessary for the application of engineering and science principles in globalized product design and manufacturing. This paper and our poster highlight our preliminary project progress. We detail the prototyping platforms that have been selected and are actively being integrated into several core engineering courses (25.108 Introduction to Mechanical Engineering, 22.202 Mechanical Engineering Design Lab I and 22.423 Senior Capstone Design). 2.0 CNC Platform Selection This section presents a brief survey of desktop CNC machine platforms and the associated support hardware necessary to implement a safe and meaningful CNC machining laboratory experience. Platform selection in this first phase of the project is also described. 2.1 Desktop CNC Machines A broad range of commercial desktop CNC machines are now readily available in assembled and/or kit form. Numerous desktop CNC machine specifications were considered for student use within engineering curricula and include: overall dimensions, design extensibility, milling speed, milling area, machine footprint, and unit cost. Of the myriad desktop CNC machine designs available, we focused only on Cartesian linear-motion gantry designs that are more representative of high-end commercial CNC mills, rather than Delta-type designs that are more common in the 3D printing community. The characteristics of several candidate CNC machines were examined (Appendix A, Table A1) and a preliminary hands-on comparison of three selected machines has been performed. These three machines comprise two inexpensive, off-the-shelf, hobby CNC mills and our native first iteration of a University of Massachusetts, in-house design. A total of ten off-the-shelf CNC mills are being deployed in this initial phase of the project: 1. ShapeOko 2: Presently five ShapeOko 2 desktop CNC machines are deployed with two additional ShapeOko 2 CNCs under construction. The ShapeOko 2, shown in Figure 1, is an open source, x-, y-, z-axis, moving gantry design developed by Edward Ford. An aluminum extrusion MakerSlide rail and carriage system is used for both the axis structure and precision motion guide-rails – thereby reducing material use and costs. The drive system comprises a total of four stepper motors (dual motor x-axis). Timing pulleys and belts are coupled to the rail and carriage gantry to generate smooth motions. The ShapeOko 2 can be purchased as individual parts or in several self-assembly kit options from Inventables.com. The detailed unit deployment is presented in Appendix A, Table A2. Page 24658.3 2. Zen Toolworks 7” x 12”: Presently three Zen Toolworks desktop CNC machines are deployed. The Zen Toolworks CNC machine, shown in Figure 1, consists of a moving gantry (to generate motion in the vertical and one horizontal travel directions) and a translation table (to generate motion in the remaining orthogonal horizontal travel direction). The gantry and table structural components are constructed using quarter-inch PVC sheet stock. Motion in each Cartesian direction is generated using stepper motor-driven lead-screws. Only one stepper motor is required for each axis of motion, totaling three stepper motors per unit. The design is available in several kit options from ZenToolworks.com. The detailed unit deployment is presented in Appendix A, Table A3. These two desktop CNC mill designs were chosen to provide a comparison of machine durability, ease of lab-setup, design flexibility, machining efficiency, manufacturing precision and machine capability within a higher education laboratory setting. Our goal is to assess, via this initial deployment, whether a particular CNC machine design or material usage concept is preferable over another in a high-use academic environment. In addition to the two off-the-shelf machines, an affordable desktop CNC mill has been designed in-house and is being refined. This design maintains low deployment cost (less than $750 per unit) and is fabricated using a minimum of machinery and tools. A CAD rendering of the UMLdesigned low-cost CNC machine is shown in Figure 2. Figure 1: (left column) The Shapeoko 2 CNC machine with some custom rail-protection modifications. (right column) The Zen Toolworks CNC machine. The dimensions of the enclosure in the lower photos are 27” x 27” x 27”.",,2014.0,10.18260/p.24043,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2b7f025ca66bef246cbfdf33e9af3a949d1eeb3b,https://www.semanticscholar.org/paper/2b7f025ca66bef246cbfdf33e9af3a949d1eeb3b,Classification of Big Data Through Artificial Intelligence,"By technology innovations, there has been a large increase within the utilization of Bigdata knowledge, joined of the foremost most well-liked styles of media thanks to its content richness, for several vital applications. To sustain Associate in Nursing current ascension of knowledge Bigdata, there's Associate in Nursing rising demand for a complicated content-based knowledge classification system. Thanks to the chop-chop increasing massive knowledge, abundant analysis effort has been dedicated to develop classification primarily based massive knowledge retrieval ways which may efficiently retrieve knowledge of interest. Considering the restricted man-power, it's abundant expected to develop retrieval ways that use options mechanically extracted from massive knowledge. Through Architecture-Algorithm co-design for Bigdata processing Applications, a scalable. Manycore processor consists of classification of heterogeneous cores with stream process capabilities, and zero-overhead inter-process communication through computer science with a hardware-software mechanism has been designed. This is often designed for achieving superior and low-power consumption, particularly thus on cut back access needed for Bigdata processing Applications. Keywords— classification , Bigdata , PBO(pollination based optimization ) , BBO(biogeography based optimization ) , Apriori. Divya et al, International Journal of Computer Science and Mobile Computing, Vol.4 Issue.8, August2015, pg. 17-25 © 2015, IJCSMC All Rights Reserved 18 INTRODUCTION Big data technologies are important in providing more accurate analysis, which may lead to more concrete decision-making resulting in greater operational efficiencies, cost reductions, and reduced risks for the business. To harness the power of big data, you would require an infrastructure that can manage and process huge volumes of structured and unstructured data in real time and can protect data privacy and security. There are various technologies in the market from different vendors including Amazon, IBM, Microsoft, etc., to handle big data. While looking into the technologies that handle big data, we examine the following two classes of technology: A. Operational Big Data This includes systems like Mongo DB that provide operational capabilities for real-time, interactive workloads where data is primarily captured and stored. NoSQL Big Data systems are designed to take advantage of new cloud computing architectures that have emerged over the past decade to allow massive computations to be run inexpensively and efficiently. This makes operational big data workloads much easier to manage, cheaper, and faster to implement. Some NoSQL systems can provide insights into patterns and trends based on realtime data with minimal coding and without the need for data scientists and additional infrastructure. B. Analytical Big Data This includes systems like Massively Parallel Processing (MPP) database systems and MapReduce that provide analytical capabilities for retrospective and complex analysis that may touch most or all of the data. MapReduce provides a new method of analyzing data that is complementary to the capabilities provided by SQL, and a system based on MapReduce that can be scaled up from single servers to thousands of high and low end machines. These two classes of technology are complementary and frequently deployed together. BENEFITS OF BIG DATA Big data is really critical to our life and its emerging as one of the most important technologies in modern world. Follow are just few benefits which are very much known to all of us: USING THE INFORMATION KEPT IN THE SOCIAL NETWORK LIKE FACEBOOK, THE MARKETING AGENCIES ARE LEARNING ABOUT THE RESPONSE FOR THEIR CAMPAIGNS, PROMOTIONS, AND OTHER ADVERTISING MEDIUMS. USING THE INFORMATION IN THE SOCIAL MEDIA LIKE PREFERENCES AND PRODUCT PERCEPTION OF THEIR CONSUMERS, PRODUCT COMPANIES AND RETAIL ORGANIZATIONS ARE PLANNING THEIR PRODUCTION. USING THE DATA REGARDING THE PREVIOUS MEDICAL HISTORY OF PATIENTS, HOSPITALS ARE PROVIDING BETTER AND QUICK SERVICE. REVIEW Behrouz et. al.[15] A combination of multiple classifiers leads to a significant improvement in classification performance. Furthermore, by learning an appropriate weighting of the features used via a genetic algorithm (GA), we further improve prediction accuracy. The GA is demonstrated to successfully improve the accuracy of combined classifier performance, about Divya et al, International Journal of Computer Science and Mobile Computing, Vol.4 Issue.8, August2015, pg. 17-25 © 2015, IJCSMC All Rights Reserved 19 10 To 12% when comparing to non-GA classifier. This method may be of considerable usefulness in identifying students at risk early, especially in very large classes, and allow the instructor to provide appropriate advising in a timely manner. Riccardo et al. [14] proposed cognitive, and behavioural aspects of distance students. Course Vis is presented in the paper, and several examples of pictorial representations generated by the tool. Luo et. al. [21] Efficient meaning for sampling of data, reduction of data also needed to develop. Newly develop mining technique and searching algorithms that are suitable for extracting more different or complex relationship between fields. Youssef M.ESSA et. al. [25] The proposed framework is developed by using mobile agent and MapReduce paradigm under Java Agent Development Framework (JADE). JADE is a promising middleware based on the agent paradigm because it supports generic services such as communication support, resource discovery, content delivery, data encoding and agents mobility. Indeed, there are seven reasons for using mobile agents as follows: (1) Reduce the network load, (2) Overcome network latency, (3) Encapsulate protocols, (4) Execute asynchronously and autonomously, (5) Adapt dynamically, (6) Naturally heterogeneous and robust, and",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
00579247db57e24a6ad96292f850f74706a88b86,https://www.semanticscholar.org/paper/00579247db57e24a6ad96292f850f74706a88b86,Psycholinguistics and Sanskrit: Is Devabhasha Mother of Psycholinguistics?,"Psycholinguistics and Sanskrit Language, in other words the storehouse of all human Knowledge is represented by words and meanings. Language by itself has an Ontological structure, Epistemological underpinnings and Grammar. Across languages, even though words /usages differ, the concept of meanings remain the same in respective communications. Yet the ""Meanings"" are understood by human beings based on Contextual, Relative, Tonal and Gestural basis. The dictionary meanings or 'as it is' meanings are taken rarely into consideration,thus human language is ambigious in one sense and flexible in other. Computers on the other hand are hard-coded to go by the dictionary meanings. Thus teaching (programming) Computers to understand natural language (human language) has been the biggest challange haunting Scientists ever since the idea of Artificial Intelligence (AI) came into existance. In addition this has lead to the obvious question of ""What is intelligence"" from a Computation perspective. Defining intelligence precisely being impossible, this field of study has taken many shapes such as Computational Linguistics, Natural Language Processing and ""Machine Learning"" etc. Artificial Intelligence instead of being used as a blanket term, is now being used increasingly as ""Analytics"" in many critical applications. Sanskrit being the oldest is also the most Scientific and Structured language. Sanskrit has many hidden Algorithms built into it as part of its vast scientific treatises, for analysing ""Meanings"" or ""Word sense"" from many perspectives since time immemorial. ""It is perhaps our job to discover and convert the scientific methods inherent in Sanskrit into usable Computational models and Tools for Natural Language Processing rather than reinventing the wheel"" as some Scientists put it. This blog's purpose is to expose some of the hidden intricate tools and methodolgies used in Sanskrit for centuries to derive precise meanings of human language, to a larger audiance particularly Computational Linguists for futher study, analysis and deployment in Natural Language Processing. In addition, Sanskrit even though being flexible as a human language, is the least ambigious as the structure of the language is precisely difined from a semantical and syntactical point of view. From a Psycholinguistic perspective this blog could also give us a glimpse of the advanced linguistic capabilities of our forefathers as well their highly disciplined approach towards the structure and usage. Sanskrit and Psycholiguitics: Sanskrit scholar William Jones formulated the lexical affinities between Sanskrit, Greek, and Latin in his 1786 lecture for the Asian Society of Calcutta. Such affinities among Indo-European languages had been observed since medieval times, but the budding Romantic notion of evolution became the impetus of explaining these affinities from a common origin of these languages. There must have been some proto-language from which all languages in the family evolved. This raised the question of how primordial human beings began to speak such a simple proto-language. This, one realized, was a psychological issue. Ever since, the empirical study of language origins and language functions in human communication has been an important chapter of psycholinguistics. Studying the emergence of language, in particular of sign languages, is still a rich chapter of psycholinguistics. Origin ,Development and Growth of Psycholinguistics in India and World: In the nineteenth century, western philosophers began a serious study of the science of language. Vedic scholars had started investigating the etymology and linguistic development (nirukta) about two millennium previous. Yāska (fourth century B.C.E) noted the similarity between the roots of words in different languages and created a collection of Sanskrit roots, similar to the recent Proto-Indo European language development. Pāṇini(fourth century B.C.E) was a grammarian who is often considered the earliest known founder of linguistics. Psycholinguistics was first developed by Bhartṛhari in the fifth century A.D. He taught that the language we use indicates how we perceive and therefore how an individual creates their personal realityreality as perceived by them. Tāntric literature developed his psycholinguistics more deeply and religiously.",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
eacb55a3c1db199c90fe6b33da0f2f4d1534a84a,https://www.semanticscholar.org/paper/eacb55a3c1db199c90fe6b33da0f2f4d1534a84a,Utilization of a PC Based Network Router,"Old and obsolete computers are valuable sources of raw materials and computational processing power, if used properly. Otherwise these are just garbage and possible sources of toxins that are sent to landfills or incinerated. This paper focuses on the concept of 3R (Reduce, Recycle and Reuse). The objective is to create a network router that can be used to service a small network of computers. Using open source software and two salvaged defective computers, replacing and removing some parts of the computer to create a prototype personal computer network router. In the deployment and testing period, there were problems encountered. These were addressed, by adding and replacing memory and network cards and rewriting/reconfiguring the software. In consideration to the power consumption of the prototype, the hard drive was replaced with a compact flash card. The experiment gave positive results in its performance and is comparable with a more expensive similar device. Further, performance was found to be directly proportional to the memory size of the network router. This proves that old and obsolete computers can be maximized in terms of utilization. Based on the results, the prototype may be considered an alternative router device for a small network, considering its cost, power consumption, availability and simplicity. It is recommended that similar studies be made considering a larger network and different hardware configurations. Keywords—3R, pc router obsolete computers, open source software, pc recycling, electronic waste, waste management INTRODUCTION Everything is going new, discovering new technologies and new powerful computers, while there are so many old computers that are being ignored by many. Obsolescence has resulted in the growing number of surplus computers and electronic equipments around the globe and these are considered electronic wastes (e-Waste) [1]. Most of these resources are sent to landfills or incinerators and sometimes improperly disposed [2]. These computers are possible sources of toxins and carcinogens when improperly treated and may cause damage to the environment. Although old computers are obsolete, these are valuable sources of secondary materials. It is a common knowledge that these, contain plastics, metals and most of all computational power. The challenge is, how may these resources, be used intelligently, specific to the processing power of computers. The 3R (Reduce, Reuse, Recycle) refers to a hierarchy of waste management strategies to minimize waste [3]. Generally, the idea is to reduce or to buy less or use less, to reuse is to discard the items and used again and recycle, some parts are separated into materials that may be incorporated into other products. This is a fitting idea to address the aforementioned issues, computer recycling or electronic recycling [4]. It can be thought of as the recovery and reuse of computers or other electronic devices, which includes both finding another use for materials and having systems dismantled, in a manner that allows for the safe extraction of the constituent materials for reuse in other or similar products. This mechanism can reduce the negative impact of e-waste to the environment. A network consists of interconnected computers to share resources and services. As all the computers as endpoints in the network should have a connection between each other to communicate, send and receive Internet Protocol for the internet service, routers play an important role, which affects cost of ownership and network management. A router [5] is a special purpose dedicated device that connects several networks. Packets are switched between these networks in a process called forwarding. It may be repeated several times on a single packet by multiple router devices until it is delivered to its final destination. These routers are computers that have operating systems, the operating system runs different processes that take care of the hardware and provide interface for the user to this hardware. Of these processes, a routing daemon [6], [7], is handling all the routing related functions. This work is focused on rehabilitating old and defective personal computers by reusing working parts to build a working network router. The goal is to reuse and recycle the processing capability of these computers, introduce modifications on the configurations in the hard drive and memory, utilize and monitor the performance of the unit as a network router. RELATED WORK Reference [8], used Linux and a conventional PC hardware to build the Click Modular Router, it gave positive results in its performance over commercial routers however there are hardware limitations encountered. Obviously, it is by far is inexpensive over other routers. The work in [9], explored the utilization of FreeBSD operating system and was ported to Linux to perform routing functions, the experiment gave positive results in its routing performance, using a dedicated router device. In a similar study [10], they noted that, the better choice of software for a PC based router is Click, although it did not support large forwarding tables in the kernel mode International Journal of Engineering Research and General Science Volume 3, Issue 4, July-August, 2015 ISSN 2091-2730 16 www.ijergs.org operation, the Linux kernel networking stack was used instead. There are also inadequacies found in using a single general purpose computer as a router for a goal of high performance routing. Based on their work, a general purpose computer together with a Linux kernel proves the viability of creating a low cost network PC router. They further stressed that high end off the shelf computers performs well in small networks, considering also cost of building the device. Tuning the router to improve the throughput performance was suggested. As general purpose computers can be built as routers, the paper in [11], claimed that these routers offer certain advantage over the use of dedicated hardware, allowing open, public source code access to the forwarding, queuing and routing algorithms and the use of more flexible, commodity host interfaces and host CPUs. But there is also a drawback mentioned: its efficiency in supporting higher bandwidth interfaces due to the limitations of the host I/O bus throughput. There are several software routers available in the internet, these takes advantage of the Linux kernel networking stack for its simplicity and robustness. Some of these are machine specific and has different Linux flavor implementation. It can be from basic functionality to the more all in one software router implementation, which can be tuned or configured to the needs of the network administrator. There are two commonly available open source software routers for general purpose computers the IPCop [12] and M0n0wall [13]. M0n0wall is an embedded firewall distribution of FreeBSD, it provides a small image for CF Cards. It runs on a number of embedded platforms and generic PCs. The PC version can be run with just a CF Card with IDE adapter to store configuration data. This eliminates the need for hard drive which reduces noise and heat levels. IPCop on the other hand, is a downloadable Linux distribution which aims to provide a simple to manage firewall appliance based on PC hardware. Its sole purpose is to protect the networks it is installed on. It includes a simple user managed update mechanism to install security updates when required. It is also geared towards home and small office-home office (SOHO) users that provide a very user friendly web interface. Their simplicity for implementation does not need a high learning curve in the implementation. These are ready to deploy software routers and can be configured accordingly at the minimal knowledge of the user for a considerable network size. The study in reference [14], used a PC based software router for a residential gateway. Based on the results in the analysis of the performance of the router, a performance metrics expression was derived that includes, sojourn time, blocking probability and throughput using measured average time. Consequently, the author guarantees that the pc based router as a residential gateway is essential for entertainment based applications delivery over home networks. METHODS AND MATERIALS The study is an applied research where a prototype network router was created, tested, utilized and monitored. The process was based on the 3R (Reduce, Reuse and Recycle) concept of waste management. The prototype network outer was derived from the old computers with functional parts. Project Development The researcher used the diagrammatic representation shown in the figure in recapitulating the steps in the creation of the prototype. The product can be modeled based on the IPO model. The IPO model describes how a process can transform inputs to give a desired output. Figure 1. The Input Process Output framework of the study The router was deployed in two offices to provide internet service to an average of fifteen (15) users, with varying bandwidth utilization. The internet service is provided by a local service provider with a 3.5 Mbps DSL type of connection and a CIR of 128 Kbps. The service has a statically configured IP address from the provider. To visualize the system, Figure 2 shows the general architecture in providing service and the utilization of the low cost network router. It can be seen that the scenario is concentrated on the router being the center of information exchange. Modification was introduced to two(2) old defective computers specific to its hard drive, chassis and Network Interface Card (NIC). Defective parts were replaced with functional parts salvaged from other old and defective computers, specific to the NIC, memory, power supply and processor. Low Cost Network PC Router Utilization Performance (Temperature, CPU utilization, Memory, Throughput) 3R (Reduce, Reuse, Recycle) Free Open Source Software Router Old Ob",,2015.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
93cb9de2febf7c1601281f405fe195e4a1aedb4b,https://www.semanticscholar.org/paper/93cb9de2febf7c1601281f405fe195e4a1aedb4b,The Public As Collaborator: Towards Developing Crowdsourcing Models for Digital Research Initiatives,"Editors | Sarah Alexander and Jen Eilers Associate Editors | Wesley Teal and Meagan Tunink Copy Editor | Amanda Schreiner Faculty Advisor| Jim Elmborg Abstract: Digital research projects often seek out large-scale data sets but have a small budget to achieve them. In their pursuit of using technology to discover something new, some scholars have turned to crowdsourcing strategies, where the efforts of individual volunteers can contribute to collective, significant data outcomes. How can examples of successful crowdsourcing projects inform future digital research initiatives? By looking at current examples of digital research projects using crowdsourcing, this research proposes new models for amassing data through the assistance of engaged publics. Inspired by the problems posed by building a large-scale database of metadata from mid-20th century small-press ephemera, this inquiry explores what outreach strategies work for different kinds of projects and with which publics. This research performs a qualitative content analysis of more than thirty digital research initiatives that rely on crowdsourcing strategies to amass data. Through their project websites, the initiatives were coded to determine the factors that motivated contributors and the electronic interfaces employed for digital delivery. The models created from this research fall along a spectrum with minimal requirements for technology and programming capacity to deploy strategies at one end and sophisticated requirements at the other. Motivational factors discovered include competition and reward systems inspired by games, personal contributions to discovery and historical narratives, and the pure entertainment of interest-driven learning. By identifying strategies that can inform approaches to scaling up digital research initiatives, these models provide a guide for scholars with boundless ideas and limited budgets. Digital research projects often seek out large-scale data sets but have a small budget to achieve them. In their pursuit of using technology to discover something new, some scholars have turned to crowdsourcing strategies, where the efforts of individual volunteers can contribute to collective, significant data outcomes. How can examples of successful crowdsourcing projects inform future digital research initiatives? By looking at current examples of digital research projects using crowdsourcing, this research proposes new models for amassing data through the assistance of engaged publics. Inspired by the problems posed by building a large-scale database of metadata from mid-20th century small-press ephemera, this inquiry explores what outreach strategies work for different kinds of projects and with which publics. This research performs a qualitative content analysis of more than thirty digital research initiatives that rely on crowdsourcing strategies to amass data. Through their project websites, the initiatives were coded to determine the factors that motivated contributors and the electronic interfaces employed for digital delivery. The models created from this research fall along a spectrum with minimal requirements for technology and programming capacity to deploy strategies at one end and sophisticated requirements at the other. Motivational factors discovered include competition and reward systems inspired by games, personal contributions to discovery and historical narratives, and the pure entertainment of interest-driven learning. By identifying strategies that can inform approaches to scaling up digital research initiatives, these models provide a guide for scholars with boundless ideas and limited budgets. Keywords: Crowdsourcing | Digital Research | Digital Humanities | Project Management | Motivation The Public As Collaborator ! Towards Developing Crowdsourcing Models for Digital Research Initiatives! Melody Dworak! School of Library & Information Science !! Performing a content analysis, 26 digital research projects were coded to discover system design features and persuasive messages. All projects were found to have elements of human labor and machine labor. Mo?va?onal messages formed the categories of Gaming, Learning, Sharing, and Giving. By tapping mul?ple mo?va?ons and using compa?ble design elements, these projects show that crowdsourcing makes us . Methods & Results! What are for contributing to digital research projects?! • Only 5 of 26 projects attempted to maintain contributor engagement through gamification.! • Gamification is the process of using elements from video games to prompt a contributor to move through “levels” of “achievement”; the goal of the system designer is to keep the contributor engaged until the systemʼs purposes have been served.! • Examples of the Gaming motivation involve challenges posed to the contributor. The system rewards contributors when challenges are overcome.! Gaming! • 23 of 26 projects engaged the contributor in a learning outcome.! • Learning taps the source that has the potential to coax an engaged enthusiast out of a disinterested bystander.! • Examples form the greatest range seen in this study. One crowdsourcing project encourages contributors to translate web pages in order to learn a language; several others engage contributors in the personal stories of characters from the past. Other projects link learning and sharing. Those who share knowledge and stories learn from others who have likewise shared. ! Learning! • 10 out of 26 projects analyzed spoke to a personʼs desire to take what one knows and give to a community.! • Sharing includes both selfexpression and contributing knowledge based on personal position and experience. ! • Examples of Sharing include stories about personal experiences or the experiences of family members that affect the physical, mental, or emotional health of the individuals portrayed in the stories.! Sharing! • Nearly all projects explicitly or implicitly spoke to an altruistic rationale of contributing to a greater good.! • Giving is defined by the individual contributing labor to the end result. The individual decides what is a worthwhile contribution to the project, leaving little room for negotiating by the designers behind the digital research project. ! • Examples of Giving include contributing to further scientific discovery, to advance historical research, or simply to support the needs of an institution. ! Giving!",,2012.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
d5b014e55d39682ede47cf248d3c0a7d6973577b,https://www.semanticscholar.org/paper/d5b014e55d39682ede47cf248d3c0a7d6973577b,Beyond 5G Challenges: White Paper,"This white paper was prepared as an output from the highly successful EPSRC sponsored CommNet 5G workshop on 13 March 2014. The objective of the paper is to describe some of the major research challenges as perceived by the research community, where successful solutions ought to be implemented in future wireless systems in the next ten years. Introduction – Beyond 5G Wireless Standards Though the most recent fourth generation (4G) standard for cellular mobile communication systems, termed Long Term Evolution Advanced (LTE-A), targets downlink and uplink peak data rates of 1 Gbit/s and 500 Mbit/s, respectively, attention has already turned to a future 5G system where not only higher data rates are sought but also significant increases in throughput area efficiency (i.e. bit/s/km 2 ) to meet exponentially growing data demands. 5G will see a stronger interplay between communications, computing and control communities 1 . Rates at the wireless edge will be on a par with wireline connectivity, largely guaranteed by the heavy “densification” of access points and heavily liberated spectral resources. We will see the operators’ core network thinned and flattened further, due to e.g. a heavily reduced number of packet-gateways; many more direct Internet Protocol (IP)-injection points; the X2 interface carrying more traffic than just control messages; virtualization and cloudification being core to the design; etc. The Internet will be strictly software defined network (SDN)-enabled with many competing connectivity protocols offering a truly deregulated end-to-end connectivity to achieve a high Quality of Experience (QoE). For standards Beyond 5G (B5G) there are important challenges which require radical new solutions:  Ever-Growing Rate. Cisco forecasts 16 Exabytes of data sent through mobile networks in 2018, equivalent to 1.8 million years of HD video every month 2 , threatening a ‘radio frequency spectrum crunch’ in wireless communications. This trend is fuelled by new wireless paradigms such as the Internet-of-Everything (IoE), where devices rather than people wirelessly communicate over the Internet. The expectation is that by 2020, there will be over 1000 wireless devices per person on the planet. This will result in over 500 billion sensors being connected to the Internet in 2030. The IoE, a more general term for a wider range of applications than the Internet of Things (which envisages predominantly low data rate sensors), is heavily based on machine-to-machine (M2M) technology and facilitates markets such as the smart grid, home automation, e-health, and intelligent transportation. 1 See D. Soldani, ""Report on the 5G European Summit (5G ES), Munich, on Feb 10th,"" Next Generation Network Infrastructures, EIF Breakfast Debate, European Parliament, Feb. 19th, 2014. 2 See http://tinyurl.com/mokcut3  Architecture Deficiencies. Current network architectures lack the waveforms and signalling strategies and are too rigid to accommodate new and yet unforeseen services/functionalities and also inflexible in supporting a wide range of expected services of IoE, M2M and mobile broadband in general.  Energy Usage. Lastly, the operation of existing wireless networks requires an unsustainable amount of energy which is equivalent to the entire air-traffic network. In the year 2020, when B5G network designs are likely to commence, we expect to see a superstandard emerge which will orchestrate efforts of the 3GPP, the IETF, the IEEE, and other related standards development organisations. For the UK to take a leading role in research B5G systems there needs to be a solid University research base. Given the EPSRC typically funds such projects, an early lead with strong proposals will have a strong impact onto B5G developments globally. The following sections outline the technical challenges, as viewed by our academic community. Beyond 5G Architecture & Orchestration Any B5G communications architecture will necessitate a quantum-leap in design to cope with the predicted information tsunami, with inherent network scalability, intelligence and orchestration being core to the design. Scalability can only be achieved with a complete flattening, and thus complete removal, of the Core network. A flat architecture allows any IP-enabled system, whether incumbent or emerging, to offer seamless data delivery. This however poses significant research challenges related to core properties of mobile networks, such as mobility and billing. To this end, device-centric mobility solutions will likely emerge which will redefine our notion of cellular deployments. Furthermore, virtualized escrow-like control entities, completely decoupled from data bearers, will ensure appropriate billing and control. To cope with the exponentially increasing data and control flux, the fairly simple (in operation) network routers will need to be replaced by highly intelligent, individually as well as systememergent, entities which are able to curb traffic at the source as well as on the fly. Unprecedented research challenges lay ahead with self-learning middleware able to understand and interpret traffic injected from unknown systems, advanced data science methods understanding traffic content and thus facilitating best delivery mechanisms, quantumalgorithmic routers able to find (close-to) optimal delivery routes in shortest time, etc, all will be the norm in a B5G system. Ultra Dense Cell Networks Small cells, heterogeneous networks or Hetnets (which comprise multiple radio standards, e.g. cellular and Wi-Fi) and massive MIMO (multiple-in multiple-out) antenna technologies have been investigated separately, whereas significant benefits may be derived by integration of these technologies. A massive MIMO system scales up conventional MIMO by seeking to adopt possibly hundreds of antenna elements at a base station to simultaneously serve tens of high data rate users within the same frequency band. Aggressive spatial multiplexing and large array gain offers large increases in capacity combined with a reduction in transmission power per user. The high density deployment of small cells also offers high capacity gains in a cost and energy efficient manner through the intense reuse of the frequency spectrum. The shorter distances between the base station and user result in reduced path-loss and total network transmit power. By combining small and large cells in a heterogeneous network, an efficient architecture is achieved whereby small cells meet the needs of traffic hot spots while large cells provide reliable coverage for high mobility users. However, the dense deployment of small cells necessitates wideband backhaul connections. There is significant merit in combining these technologies such that the large cell massive MIMO technique is applied as both a backhaul connection to the network and as fronthaul to serve, via wireless, multiple small cell basestations with direct user support. Wireless backhaul is considerably easier to set-up than a cable backhaul and it enables simpler reconfiguration and upgrading. As massive MIMO must support a number of users directly as well as the small cells, the choice of frequency (in-band or out-band) has important implications for inter-cell interference management. In general, out-band signalling requires a less complex interference management regime but at the expense of failing to realise the largest possible capacity. Further options, in terms of time vs frequency division duplex operation, provide additional opportunities for system optimization. The following research is key to achieving the above goals:  New waveforms that allow scalability from ultra high density cells/high capacity to large coverage (100% coverage availability) and low data rates per device expected for IoE;  Interference management in ultra dense cells and in hetnets;  Hyper transceivers for cooperative ultra dense cells;  Co-existence between license and license-exempt bands;  Full duplex radio technologies;  Exploitation of different antenna polarizations as new degrees of freedom for data transmission. Reconfigurable Hardware design challenges from RF to THz It is inevitable fact that future spectrum is going to be highly fragmented, ranging from frequencies below 6 GHz to the millimeter(mm)-band and THz. New hardware design and manufacturing paradigms are key to the exploitation of such a fragmented spectrum and in particular for the mm-wave part of the spectrum, essential for achieving the data rates required for B5G systems. Electronically-steered high-gain 3D antennas and high levels of RF/DSP integration are essential, as demonstrated in Samsung’s 28 GHz 64-antenna system 3 . With unprecedented aggregate data rates in the backbone network, fibre connections and wireless backhaul at E/W-bands and beyond will be required. It is an immense challenge to realise hardware (transceivers, filters, power amplifiers, antennas) at frequencies from 28 GHz to 300 GHz, and beyond, with the low manufacturing costs demanded by network operators. Silicon RFIC technology is the key enabler, potentially even up to 1 THz, but considerable advances in RF design techniques are required to realise complete B5G subsystems, with research required in:  Concurrent and switchable multi-band mm-wave power amplifiers that are ultra linear and power efficient; 3 See: http://www.commnet.ac.uk/documents/comment_5g_workshop_130314/2_Samsung_5G.pdf  Reconfigurable and ultra broadband transceivers for all-spectrum access;  Tuneable filters and multiplexers with very high selectivity and linearity;  Steerable and multiband antennas integrated with RF transceivers and DSP with advanced new packaging solutions;  mm-wave phase-locked loops with fast switching and low spurious;  New transceiver architectures, working hand-in-hand with DSP researchers;  Low loss mm-wave to optical conversion. Furthermore, while mm-waves are key to providing the expected > 10 Gbit/s B5G sy",,2014.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
9fc6d3d9c58485c07c19b83d2474568cf2ada437,https://www.semanticscholar.org/paper/9fc6d3d9c58485c07c19b83d2474568cf2ada437,Keynote Speakers,"“High-performance computing and “big data” in integrative cancer informatics. Challenges and opportunities in intelligent molecular medicine.” Personalized medicine requires better identification of patient subgroups and novel treatment options. Since most cancer treatments only work for subsets of patients we need alternative treatment options. Single drugs are rarely sufficient; thus, we need to identify drug combinations, and new drugs. Data on thousands of cancer patient profiles from diverse technology platforms provide essential resources for molecular medicine. However, effectively integrating, annotating, and analyzing these high-dimensional, heterogeneous and distributed data with aim to create intelligent hypotheses and realistic models of human disease is not trivial. Systematic computational analysis has the potential to unravel mechanism of action for therapeutics, re-position existing drugs for novel, use and prioritize multiple candidates to best treat an individual patient. We need to integrate algorithms and systems from machine learning, databases, image and text analysis, ontologies, human-computer interaction, graph theory and visualization to tackle these diverse problems. Scale changes everything, and size does matter. Integrating intelligent heuristics with novel computing environments, such as grid and GPU computing, provides scalable platform for these big data challenges. Dr. Hossam Hassanein School of Computing, Queen’s University Biography: Hossam Hassanein is leading research in the areas of broadband, wireless and mobile networks architecture, protocols, control and performance evaluation. His record spans more than 500 publications in journals, conferences and book chapters, in addition to numerous keynotes and plenary talks in flagship venues. Dr. Hassanein has received several recognition and best papers awards at top international conferences. He is also the founder and director of the Telecommunications Research (TR) Lab at Queen’s University School of Computing, with extensive international academic and industrial collaborations. Dr. Hassanein is a senior member of the IEEE, and is the former chair of the IEEE Communication Society Technical Committee on Ad hoc and Sensor Networks (TC AHSN). Dr. Hassanein is an IEEE Communications Society Distinguished Speaker (Distinguished Lecturer 2008-2010). Abstract: “Global Resource Utilization in the Internet of Things” The Internet of Things (IoT) is envisioned as a paradigm shift, with a plethora of applications, on the premise of well-established enabling technologies; prominently Wireless Sensor Networks (WSNs) and RFIDs. The former has evolved to improve energy efficiency and resilient operation, yet, true scalability has only been recently probed and quite sparsely advancing. Moreover, the traditional approach whereby most WSN platforms are tailored for a single-application to meet a given efficiency metric, imposes significant rigidity in re-utilizing devised platforms for new applications, and limitations on re-using previously deployed ones. In remedy, we present a novel paradigm in WSNs to efficiently utilize network resources, and extend it to a platform for multiple applications to cross utilize resources over multiple WSNs. This paradigm presents a leap in scalability, not only in a WSN, but across multiple ones, with dynamicity to accommodate for varying resources being “Global Resource Utilization in the Internet of Things” The Internet of Things (IoT) is envisioned as a paradigm shift, with a plethora of applications, on the premise of well-established enabling technologies; prominently Wireless Sensor Networks (WSNs) and RFIDs. The former has evolved to improve energy efficiency and resilient operation, yet, true scalability has only been recently probed and quite sparsely advancing. Moreover, the traditional approach whereby most WSN platforms are tailored for a single-application to meet a given efficiency metric, imposes significant rigidity in re-utilizing devised platforms for new applications, and limitations on re-using previously deployed ones. In remedy, we present a novel paradigm in WSNs to efficiently utilize network resources, and extend it to a platform for multiple applications to cross utilize resources over multiple WSNs. This paradigm presents a leap in scalability, not only in a WSN, but across multiple ones, with dynamicity to accommodate for varying resources being introduced and removed; in addition to utilizing transient resources in their vicinity. To this end, we present a global architecture to efficiently adopt WSNs in IoT with changing demands and scale. Our approach is further explained and demonstrated via a detailed use case depicting the premise of IoT application. Dr. Monty Newborn School of Computer Science, McGill University Biography: Monty Newborn received his Ph. D. in Electrical Engineering from The Ohio State University in 1967. He was an assistant professor and then associate professor at Columbia University in the Department of Electrical Engineering and Computer Science from 1967-1975. In 1975, he joined the School of Computer Science at McGill University and has been with the School since then, serving as its director from 1976-1983. He has been an ACM Fellow since 1994 and a member of the Canadian Chess Hall of Fame since 2001. His research has focused on search problems in artificial intelligence where two areas are of particular interest: chess-playing programs and automated theoremproving programs. He has published seven books on these subjects and a number of research papers as well. He served as chairman of the ACM Computer Chess Committee from 1981 until 1997. In that capacity he organized the first Kasparov versus Deep Blue match (known as the ACM Chess Challenge) in 1996. The following year he served as head of the officials at the second Kasparov versus Deep Blue match won by Deep Blue. Through the 1970s and 1980s, his chess program Ostrich competed in five world championships, coming close to winning in 1974.",,2013.0,10.1111/j.1741-6612.2010.00429_6.x,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
4b0e4e35a1014e6a43f1df7bd66db965c1ce09d8,https://www.semanticscholar.org/paper/4b0e4e35a1014e6a43f1df7bd66db965c1ce09d8,Systematic approaches for modelling and visualising responses to perturbation of transcriptional regulatory networks,"One of the greatest challenges in modern biology is to understand quantitatively the mechanisms underlying messenger Ribonucleic acid (mRNA) transcription within the cell. To this end, integrated functional genomics attempts to use the vast wealth of data produced by modern large scale genomic projects to understand how the genome is deployed to create a diversity of tissues and species. The expression levels of tens or hundreds of thousands genes are profiled at multiple time points or different experimental conditions in the genomic projects. The profiling results are deposited in large scale quantitative data files that are not possible to analyse without systematic computational methods. In particular, it is much more difficult to experimentally measure the concentration level of transcription factor proteins and their affinity for the promoter region of genes, while it is relatively easy to measure the result of transcription using experimental techniques such as microarrays. In the absence of such biological experiments, it becomes necessary to use in silico techniques to determine the transcription factor regulatory activities given existing gene expression profile data. It therefore presents significant challenges and opportunities to the computer science community. This PhD Project made use of one such in silico technique to determine the differences (if any) in transcription factor regulatory activities of different experimental conditions and time points.The research aim of the Project was to understand the transcriptional regulatory mechanism that controls the sophisticated process of gene expression in cells. In particular, differences in the downstream signalling from which transcription factors can play a role in predisposition to diseases such as Parasitic disease, Cancer, and Neuroendocrine disease. To address this question I have had access to large integrated genomics datasets generated in studies on parasitic disease, lung cancer, and endocrine (hormone) disease. The current state-of-the-art takes existing knowledge and asks ""How do these data relate to what we already know?"" By applying machine learning approaches the project explored the role that such data can play in uncovering new biological knowledge.",,2013.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
93cf2bb3f576ae449d7a103e587a470ad2256dcc,https://www.semanticscholar.org/paper/93cf2bb3f576ae449d7a103e587a470ad2256dcc,Ordering Our Attributions-of-Order: Commentary on McMahon,"In her target article, Jennifer McMahon argues that we understand art not by explicitly interpreting “raw percepts,” but rather by engaging with our implicit tendencies to interpret complex stimuli in terms of culturally-engrained preconceptions and narratives. These attributions of order require a shared conceptual and cultural background, and thus one might worry that in denying access to raw percepts, the view dulls art’s critical edge. Against this worry, McMahon argues that art can continue to create and innovate by inviting us to critically reflect upon the very preconceptions on which our engagement with it necessarily depends. In this commentary, I place these attributions of order in historical and empirical context. In addition, I discuss a lingering, related mystery — the possibility of the occasionally punctuated character of artistic evolution, in which prevailing aesthetic conventions are replaced with almost entirely new ones. I suggest that such radical breaks with the past are possible even given the concept-ladeness of perception, but are only likely to succeed when they tap into a culturally-invariant bedrock of more basic attributions of order. In the classic Heider-Simmel demonstration (1944) — a foundational experiment in psychological research on social cognition — subjects watched a two-and-a-half minute animation depicting a stationary box and three moving geometric shapes: a large triangle, a small triangle, and a circle. During the animation, the large triangle enters from the left and moves to the right; subsequently, one of the box’s sides is divided in two and a smaller segment pivots outward, the large triangle then moving inside the box. Afterwards, the smaller segment pivots inwards and once again the box is whole. Then, the smaller triangle and the circle appear from the left and move near the part of the box that had pivoted, which pivots again. The larger triangle moves out of the box and comes near the smaller triangle, and they both shake for approximately five seconds, and... _____________________________ Corresponding Author: Cameron Buckner University of Houston; Ruhr-University, Bochum email – cameron2000@gmail.com Essays Philos (2012) 13:2 Buckner | 424 Perhaps the most remarkable thing about the animation (which can be found with a cursory Internet search) is how excruciating it is to describe its contents in this manner. Let us instead use some terms favored by subjects in the experiment: A burly villain breaks into the home of two young lovers and is discovered by them after they return from an evening on the town. This hardened criminal is overcome by malice at the sight of the happiness and innocence of the startled young lovers; he vigorously beats the poor young man and traps the terrified woman in the house, cornering her with dark motives brewing in his evil, triangular mind. After recovering from his thrashing, the weakened young man opens the door and rescues his love and, after a brief but harrowing pursuit by the burglar, they flee to safety. In her article, McMahon argues that the sort of narrative attribution-of-order vividly demonstrated in the Heider-Simmel experiment is both endemic to human cognition and essential to artistic experience. Perception is conceptually and culturally loaded, but this embeddedness is actually a prerequisite, rather than a threat, to art’s ability to engage in cultural critique. She grounds this embeddedness in what she calls the principle of aesthetic form, the key idea being that art needs some “aesthetic syntax” in order to transmit a message — including in this category perhaps mundane symbolism such as that in Medieval European art holiness is indicated by the presence of floating golden halos, as well as more complex aesthetic conventions such as that in Cubism scenes are simultaneously depicted from multiple angles. However, she augments this principle with a corollary principle of art, which holds (echoing Kant and Adorno) that despite necessarily relying upon such syntactic conventions, the artist remains free to critically comment upon them and to suggest new ones. Two questions are raised by McMahon’s contribution that I will briefly address in this commentary. First, if attribution-of-order is ubiquitous in human cognition, what is distinctive about the sort of attribution-of-order that occurs during the appreciation of artworks? Attribution-of-order occurs any time we explain an event in terms of a causal principle or interpret the actions of another as goal-directed, yet we are not entering the artistic mode every time we e.g. watch an object roll downhill or infer that the dog wants to go outside because it paces near the door. We want a kind of architectonic of attributionsof-order which would place all of these processes with respect to one another and show their dependencies. McMahon is rightly well-known for the sophisticated, empirically-grounded account of such processes offered in her latest book, but I briefly summarize empirical work in this area below for the purposes of context. Second, if art must make use of shared cultural background in order to communicate with a viewer, how are artists able to create Essays Philos (2012) 13:2 Buckner | 425 fundamentally new techniques and conventions? Notably, art does not remain trapped in a particular cultural milieu that it gradually tweaks and modifies — Western art is not merely footnotes to Da Vinci — but rather engages in occasional dramatic paradigm shifts that deploy an almost wholly distinct “aesthetic syntax.” One might worry that while McMahon has opened up some room for innovation, affirming the culture-ladenness of perception still renders mysterious the occasionally punctuated character of artistic evolution. First, let us consider a taxonomy of attributions-of-order. The first cut in the taxonomy might concern the type of regularities perceived: are they of causal principles or the actions of intentional agents? While, if we be physicalists, agency will be just another more complex sort of causal principle, it is certainly a distinct form that requires distinct biases and responses. Another cut would involve the hierarchical level of the perception, for another important shift occurs when the principles of attribution-of-order are applied recursively to themselves. For example, an organism capable of recursive attribution-oforder could begin to wonder about the status of its own perceptual apparatus, learning to confront cases of perceptual illusion and to compare the qualities of certain perceptions to those of others. And finally we might ask whether the predictions are generated implicitly or explicitly, for all of the principles discussed thus far can likely be mastered tacitly, through passive habituation — whereas explicit engagement of these capacities may introduce another degree of flexibility and control. Cognitive science has a great deal to contribute to this architectonic, as processes at every level of this scheme have been studied empirically. In particular, the idea that the mind consists of a hierarchical layering of attributions-of-order has recently been on the upswing. From this perspective, the brain can be characterized as a “prediction machine;” its goal is to quickly and efficiently predict the widest range of perceptual regularities across the widest range of situations. It lacks adequate space and processing power to simply memorize every statistical regularity in the environment, however, and so it must economize on representational resources. It accomplishes this trade-off by dividing the job into a multi-level, hierarchical process, where the task is broken down into layers, with each successive layer operating only on the output of the previous layer. Bi-directional inter-layer learning is governed by error-correction processes; when an important outcome is not predicted, the features that would have led to a correct prediction become more salient in the future. Over time, with enough learning (and perhaps some hardwired biases), the brain learns to organize raw perceptual experience into a fully-assembled, predictable world. Raw retinal input is broken down into features such as colors and locations, features are assembled into edges and boundaries, boundaries into shapes, shapes into figures, figures into scenes, and, Essays Philos (2012) 13:2 Buckner | 426 eventually, scenes into narratives. This hierarchical process is governed by what neuroscientists call “convergence,” a progressive winnowing down and abstraction; at each stage, only part of the possible input is attended to, the part deemed important or diagnostic given an inductive crystallization from prior experience. Babies may begin with something like James’s “buzzing, blooming confusion,” but once the systems get set up, raw sensory input is progressively broken down into its most essential features and re-assembled into meaningful objects, scenes, and narratives (and as McMahon correctly notes, conscious awareness typically only comes in at the later stages of the process). At each step the system gains predictive power by focusing only on the most valid aspects; but at the same time efficiency comes at a cost, for the brain must ignore a large amount of potential information from the previous layer. Crucially, these views often predict top-down regulation of perception, where the features that are consciously perceived (with many actual details ignored and some entirely fabricated) are determined in part by the high-level narratives those features might support. On this scheme, perhaps McMahon’s distinctive mode of art appreciation consists in a recursive, explicit deployment of attributions-of-order simultaneously to an artwork and to the form of one’s own aesthetic experience. As McMahon suggests, artworks can place us into a mode of explicit reflection; we appreciate not only the structural features of the artwork, but also the effects these fe",,2012.0,10.7710/1526-0569.1429,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
6e52345184e660e82b2954c8195f8015ffe4a4e9,https://www.semanticscholar.org/paper/6e52345184e660e82b2954c8195f8015ffe4a4e9,"Proceedings of the seventh ACM International Workshop on Mobility in the Evolving Internet Architecture, MobiArch 2012, Istanbul, Turkey, August 22, 2012","Welcome to the seventh ACM International Workshop on Mobility in the Evolving Internet Architecture (MobiArch'12) in Istanbul, Turkey! We are delighted to see the workshop has attracted high quality submissions from international researchers in both academia and industry. 
 
This workshop provides the opportunity to participate in the exploration of the state of the art research results on mobile Internet computing and in particular, pricing and mobile cloud research. 
 
As the number of mobile users proliferates, there is an ever increasing appetite for innovative mobile services. Service providers are often under tremendous pressure of deploying new services to users quickly and cost effectively. This requires us to critically rethink the mobile services architecture that will lay the foundation for a futuristic mobile Internet and a growing revenue base. The aim of this proposed workshop is to have an open forum discussing cutting edge research contributions with a focus on economic and incentive issues in mobile networks and technologies pertaining to mobile cloud. Mobile operators cannot compete solely on higher bandwidths supported by the underlying technology (e.g. HSPA+, LTE). Service differentiation using innovations along other dimensions like pricing, incentives, QoS, etc. is turning out to be essential to compete. As mobile technology has matured tremendously in the past five years, we are moving into the domain of sophisticated mobile applications that encompass the regular World Wide Web, mobile centric web and cloud based services. However, new challenges have to be met: 
How do we create service differentiation with mobile economic incentives in mind? 
What are some of the new ""incentive-aware"" mobile network architectures? 
Are there new theories and models for understanding network pricing and congestion control? 
 
 
 
To meet with these challenges, researchers from a wide range of academic fields, including theory and algorithms, data mining and machine learning, computer systems and networks, statistical physics and complex systems, economics and managerial science, etc., are all actively studying various aspects concerning mobile networks. 
 
This workshop is intended to present such an opportunity and serve as a forum to bring together people from various fields to exchange their latest research results and to discuss new ideas and directions to properly understand these networks. 
 
Highlights of the workshop include a keynote speech delivered by Lili Qiu (Associate Professor at University of Texas, Austin), world-renowned researchers in the fields of mobile computing systems including mobile pricing, incentives and mobile cloud technology and services. 
 
The technical sessions consist of presentations and discussions of the 5 accepted full papers and 2 short papers on a wide variety of issues of mobile networking and architecture research from different angles. These papers were selected from 12 submissions. The selected papers were chosen by a technical program committee (TPC) of 19 experts in various fields related to mobile pricing and cloud technology. The selection process started shortly after the submission deadline. Each paper was reviewed by at least three independent reviewers, and evaluated based on scientific novelty, technical quality, relevance to the topics, and contribution to the field.",MobiArch@MobiCom,2012.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
bdbd4968cf95800c6ebe9dfd549bd67fbefcc4e6,https://www.semanticscholar.org/paper/bdbd4968cf95800c6ebe9dfd549bd67fbefcc4e6,annual ESGF F2F Conference Abstracts 2018 Earth System Grid Federation Face-to-Face Infrastructure for the European Network for Earth System modelling: IS-ENES3 in review of sustainability,"The European Network for Earth System modelling (ENES) gathers the European community working on climate modelling. Its infrastructure project, IS-ENES3, will start its 3rd phase in January 2019 for 4 years, engaging 22 partners from 10 countries to support collaboration on software and data in Europe. It will implement its 2017 update of the ENES infrastructure strategy 2012-2022 (Joussaume et al., 2017). IS-ENES3 will support the exploitation of model data from the WCRP coordinated experiments, CMIP and CORDEX. It will maintain and develop the European component of ESGF with the aim of supporting CMIP6. It will support the infrastructure and governance of key metadata and data standards, such as the Climate Forecast conventions for NetCDF and the ES-DOC standards for documentation of models and simulations. It will also develop a new service to ease multi-model data analytics. IS-ENES3 will raise the standard for Earth system model evaluation by gathering more detailed understanding of user requirements, by promoting standards for the science provenance, and by developing a state-of-the-art community European model evaluation framework. IS-ENES3 aims at facilitating the exploitation of model data not only by the Earth system science community but also by the climate change impact community and the climate service community. It will invest in training as well as in the operation and further development of the climate4impact platform and the underlying services to enable customised access to data, documentation, and information about model evaluation to the climate impact community as well as climate service businesses and consultancies. With the support from the ENES data task force, huge datasets within limited memory spaces with interactive response times. EDAS services are accessed via a WPS API being developed in collaboration with the ESGF Compute Working Team to support server-side analytics for ESGF. Client packages in Python, Java/Scala, or JavaScript contain everything needed to build and submit EDAS requests. EDAS services include configurable high-performance neural network learning modules designed to operate on the products of EDAS workflows. As a science technology driver, we have explored the capabilities of these services for long-range forecasting of the interannual variation of important regional scale seasonal cycles. Neural networks were trained to forecast All-India Summer Monsoon Rainfall (AISMR) one year in advance using (as input) the top 8-64 principal components of the global surface temperature and 200 hPa geopotential height fields from NASA’s MERRA2 and NOAA’s Twentieth Century Reanalyses. The promising results from these investigations illustrate the power of easily accessible machine learning services coupled to huge repositories of earth science data. The EDAS architecture brings together the tools, data storage, and high-performance computing required for timely analysis of large-scale data sets, where the data resides, to ultimately produce societal benefits. The Ophidia project provides a complete environment for scientific data analysis on multidimensional datasets. It exploits data distribution and supports array-based primitives for mathematical and statistical operations, analytics jobs management and scheduling, and a native in-memory I/O server for fast data analysis. It also provides access through standards interfaces like WS-I+ and WPS. Its workflow engine interface allows to implement a variety of analytics and processing chains, parallelism in a transparent way through declarative statements and interleaved mechanisms to cross-link multiple workflows into complex experiments. Tracking and predicting extreme events in spatio-temporal climate data is a major challenge in climate science. Existing approaches to tracking extreme climate events require an appropriate feature selection from physical variables and thresholds by human expertise. To predict extreme climate events, existing methods rely on physics-based climate simulations demanding tremendous computing cost. The recent progress in deep learning provides technical insights by capturing the nonlinear spatio-temporal interactions between a variety of physical variables. We propose two deep-learning-based models to track and predict hurricane trajectories on massive scale climate reanalysis data. First, we address the spatio-temporal tracking as a mapping problem from time-series climate data to time-sequential hurricane heat maps using Convolutional LSTM (ConvLSTM) models. Our result shows that the proposed ConvLSTM-based regression models outperform conventional region-CNN-based detection methods. Second, we present a new trajectory prediction approach as a problem of sequential forecasting from past to future hurricane heat map sequences. Our prediction model using ConvLSTM achieves successful mapping from predicted heat maps to ground truth. A modern, Python-based diagnostics package for evaluating earth system models has been developed by the E3SM project. The goal of this work is to build a comprehensive diagnostics software package as an essential E3SM tool to facilitate the diagnosis of the next generation earth system models. This package is embedded into the E3SM automated process flow to enable seamless transition between model run and diagnostics. Modeled after NCAR’s atmosphere diagnostics package, this software is designed in a flexi ble, modular and object-oriented fashion, enabling users to manipulate different processes in a diagnostics workflow. Numerous configuration options for metrics computation (i.e., regridding options) and visualization (i.e., graphical backend, color maps, contour levels) are customizable. Built-in functions to generate derived variables and to select diagnostics regions are supported and can be easily expanded. An updated observational data repository is developed and maintained by this activity. The architecture of this package follows the Community Diagnostics Package framework, which is also applied by two other DOE funded diagnostics efforts (PCMDI metrics package and ARM diagnostics package), to facilitate effective interactions between different projects. The Coupled Model Intercomparison Project (CMIP) and its sister projects have generated massive dataset, leading climate researches to big-data intensive territory. One of challenges in for the CMIP is the development of objective metrics and measures of climate model evaluation and interdependency, which efficiently summarize collected PetaByte scale data to give better insight into the data. In management, analysis of model and expand to additional application communities through interaction with the Open Geospatial Consort ium (OGC). OGC’s to advance the development and use of international standards and supporting services for geospatial interoperability. mission, OGC serves as the global forum for the collaboration of geospatial data / solution providers and users. Coordination of OGC and ESGF here in three horizons: Next After-Next. ESGF Web Processing (WPS). WPS other OGC Standards provide machine-to-machine interoperability ESGF ESGF/WPS ESGF resource-oriented use of OpenAPI ESGF capabilities of WPS model outputs interoperability. relevant OGC activities by ESGF, e.g., OGC netCDF, OGC HDF5, security, grid/cloud, OGC Earth OGC CRIM of an ESGF Hybrid Climate Data Research After-Next, as part OGC roadmap predictive modeling, as ESGF, modeling, Roadmap is a climate data analytics ecosystem built around widely-used python infrastructure for scientific computation and analysis. The infrastructure includes a Jupyter notebook front end for interactive and collaborative analytics development; scientific computing middleware based on xarrays/scipy/numpy, and a DASK backend for scalable analytics that can be deployed on the cloud or on dedicated computing clusters. In this talk we present examples of Pangeo running in conjunction with the cloud-based invocation of an ESGF data node (see talks by Cinquini and Nikonov). We demonstrate task parallel analysis of CMIP6 data hosted on an ESGF data node hosted on the Google Cloud Platform. We describe a successful use case of processing Earth Science Data from NASA on Amazon Web Service (AWS) and potential benefit of utilizing AWS as a platform to support the Coordinated Regional Downscaling Experiment (CORDEX). CORDEX is an international modeling effort that parallels the Coupled Model Intercomparison Project (CMIP) but with a focus on regional-scale climate change. To complement CMIP, which is based on global climate model simulations at relatively coarse resolutions, CORDEX aims to improve our understanding of climate variability and changes at regional scales by providing higher resolution regional climate model (RCM) simulations for 14 domains around the world compared to CMIP global climate models. The Earth System Grid Federation (ESGF) already hosts a massive amount of RCM output for CORDEX. As more RCMs with high resolution participate in CORDEX by downscaling CMIP6 evaluation next-generation CORDEX RCM simulations Analysis of multiple RCMs with relatively high resolution also requires the appropriate architectural framework capable of manipulating large datasets. We believe that the geographically separated AWS regions can provide optimal infrastructure for hosting the high-resolution CORDEX RCM simulations, because most of the demand for RCM simulations for each CORDEX domain is from countries within the domain. Our training at a recent CORDEX training workshop demonstrated the value of maintaining a large amount of datasets from the NASA Earth Exchange (NEX) in AWS S3. The Regional Climate Model Evaluation System (RCMES) is a software package that offers a variety of tools to evaluate the CORDEX RCMs. The diagnostic tools included in RCMES have been also proven to be useful for processing and analyzing NEX datasets on AWS. RCMES will provide a complete star",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
787b8f43c78406d85919892b19122965a5026c31,https://www.semanticscholar.org/paper/787b8f43c78406d85919892b19122965a5026c31,VISUALISING DISTRIBUTED SIMULATION DESIGN AND DEPLOYMENT,"The “Hello World” code provided as a sample application for the High Level Architecture (HLA) framework comprises over 2500 lines of code, of which less than 50 provide any real simulation logic. This coding and integration overhead represents a considerable development effort. This overhead, and the lack of a standardized design pattern for separating simulation logic from integration code, often results in high levels of code coupling. This in turn leads to poor code structure and design, poor reuse, and increased maintenance costs. Significant improvements can be made in the design, development and deployment of HLA based simulations through the application of a Component-Based Development (CBD) approach and the use of visual design metaphors. These improvements are realized through the use of CBD patterns, which provide a mechanism to clearly separate the simulation logic from the integration requirements of the federate. Using this model, simulation logic remains separated from the integration code. Generated integration logic can then be used to manage the physical integration with the RTI, and any other required services, such as data transformations. The simulation logic and the integration logic collectively form the HLA federate. This separation also allows the developer to visually model the relationships between federates, allowing the creation of a simulation without the need to consider integration requirements. This visual design approach can be used to encapsulate the simulation workflow, the asynchronous ‘publish and subscribe’ relationships between the components and the FOM, the synchronous inter-component relationships (method calls between components outside of the RTI), any data transformation required to insure interoperability, and the deployment of the simulation. This paper examines a number of visualizations that can be used to significantly aid in the development and deployment of distributed simulations. It will also examine how a simulation can be directly generated, deployed and executed from the visual model. AUTHORS’ BIOGRAPHIES Dr Russell Keith-Magee is a Software Engineer at Calytrix Technologies with a research background in both Physics and Computing. His doctoral thesis was on biologically motivated models of machine learning and development. He also holds a Bachelor of Science with Honours in Computer Science and a Bachelor of Science in Physics. Shawn Parr is the co-founder and Chief Technology Officer at Calytrix Technologies, an Australian based research and development company specializing in component-based solutions and HLA simulation. Shawn has been working in the IT industry for over 10 years. He holds a research-based Masters degree, and a Bachelor of Science. Alex Radeski is a Senior Software Engineer at Calytrix Technologies and has a keen interest in Design Patterns, Component Based Development, and real-time visual simulation. Alex has been working as a software engineer for over 7 years, and holds a Bachelor of Science in Computing. VISUALISING DISTRIBUTED SIMULATION DESIGN AND DEPLOYMENT Dr Russell Keith-Magee, Shawn Parr, Alex Radeski Calytrix Technologies Pty Ltd Perth, Western Australia",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
8eb202d8d9c5f39219f70aeabc52db6e597848fa,https://www.semanticscholar.org/paper/8eb202d8d9c5f39219f70aeabc52db6e597848fa,A Framework for Action Detection in Virtual Training Simulations Using Synthetic Training Data,"In virtual military training, tracking and evaluating trainee behavior throughout a simulation exercise helps address specific training needs, improve the realism of simulations, and customize the training experience. While it is straightforward to parse the event log of a simulation to identify atomic behaviors such as unit movements or attacks, it remains difficult to fuse these events into higher-level actions that better characterize trainees’ intentions and tactics. For example, if each unit is controlled by an individual trainee, how should the movement information from all units be aggregated to determine what formation the group is moving in? Similarly, how can all of the information from nearby terrain environments be combined with kinetic actions to determine whether the trainees are executing an ambush attack, or is simply engaging the enemy group? While an experienced human observer-controller can quickly assess the battle map to provide an appropriate interpretation for such events, it remains a challenging task for computers to automatically detect such high-level behaviors when performed by human trainees. In this work, we proposed a machine-learning (ML) framework for recognizing tactical events in virtual training environments. In our approach, unit movements, surrounding environments, and other atomic events are rasterized as a 2D image, allowing us to solve the action detection problem as image classification and video temporal segmentation tasks. In order to bootstrap ML models for these tasks, we utilize synthetic training data to procedurally generate a large amount of annotated data. We demonstrate the effectiveness of this framework in the context of a virtual military training prototype, detecting troop formations and other tactical events such as ambush and patrolling. ABOUT THE AUTHORS Andrew Feng is a Research Scientist at the Institute for Creative Technologies at the University of Southern California, working on the One World Terrain project. Previously, he was a research associate focusing on character animation and automatic 3D avatar generation. His research work involves applying machine learning techniques to solve computer graphics problems such as animation synthesis, mesh skinning, and mesh deformation. He received his Ph.D. and MS degree in computer science from the University of Illinois at Urbana-Champaign. Email: feng@ict.usc.edu Andrew S. Gordon is a Research Associate Professor of Computer Science and Director of Interactive Narrative Research at the Institute for Creative Technologies at the University of Southern California. His research advances technologies for automatically analyzing and generating narrative interpretations of time-series data. He received his Ph.D. in 1999 from Northwestern University. Email: ​gordon@ict.usc.edu 2020 Paper No. 20302 Page 1 of 12 Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) A Framework for Action Detection in Virtual Training Simulations Using Synthetic Training Data Andrew Feng, Andrew S. Gordon Institute for Creative Technologies University of Southern California Los Angeles, CA USA feng@ict.usc.edu, gordon@ict.usc.edu INTRODUCTION Among the most compelling use-cases for interactive virtual simulations is for virtual team training exercises, e.g., where teams of warfighters control their own virtual avatars in simulated battles, practicing tactical maneuvers and essential skills in increasingly challenging scenarios. While these computer-based exercises lack the physical stresses of live training, their great potential lies in their capacity for guided deliberate practice of skills, where the environments themselves are tailored to the abilities of the team and responsive to their successes and failures in their performance. In both live and virtual training, however, this responsiveness has generally required the participation of human facilitators, i.e., Observer Controllers that track the progress of trainees toward learning objectives, and actively shape the learning environment to further this progress. Automating these capabilities of human Observer Controllers would have several practical benefits for virtual training for operational units. Replacing training support staff with software reduces labor costs, at the very least. More importantly, such automation can help remove the reliance of operational units on outside contractors and home-station training facilities in conducting virtual training exercises. As this reliance is reduced, commanders are more able to lead their own training exercises, on their own schedules, wherever their point of need.. Much of the difficulty in automating the Observer Controller stems from the simulation environment's lack of understanding of what the human trainees are actually doing in the virtual space. Seeing a given formation and orientation of trainee avatars on a virtual ridge line, for example, it might be obvious to a human Observer Controller that the trainees were setting up for a deliberate ambush of an approaching enemy force, prompting him or her to modify the enemy's reaction to reinforce lessons related to the performance of this tactical maneuver. The software of the virtual simulation environment, however, would likely be oblivious to the impending ambush altogether, aware only of the avatars' positions in virtual space and the avatar controls provided by the trainees' user interfaces. Automating the human Observer Controller, in this case, requires an ability to recognize complex group behaviors based on the positions and individual actions of avatars in the group, within the battlefield context. Analogous perceptual recognition tasks have been successfully automated in other domains using contemporary machine learning technologies, such as deep neural networks. Here, recognizing the complex behaviors of teams of trainees in virtual environments can be seen as a type of time-series classification task. The input consists of positional and low-level simulation event data over time, and output assigns labels to durations that best characterize the class of behavior that is being executed, from a given vocabulary. Today, a high-accuracy classifier of this sort could easily be constructed using supervised learning methods, provided that enormous amounts of data were available, expertly annotated with high levels of inter-rater agreement. However, the required amounts of data (perhaps tens of thousands of examples) is far beyond what might be reasonably obtained from the deployed use of any existing virtual training software. Even assuming such large-scale datasets could be collected, its annotation by teams of experts would be especially costly and difficult, given the sensitive nature of data generated during military training exercises. If contemporary machine learning technologies are to be used to automate functions of human Observer Controllers, an alternative approach to the collection and annotation of training data is needed. In this paper, we investigate an alternate approach to the collection and annotation of datasets for behavior recognition using machine learning methods. Our approach involves the automatic generation of synthetic training data, collected by authoring behavior programs to be executed thousands of times by teams of fully-autonomous 2020 Paper No. 20302 Page 2 of 12 Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) agents within the target simulation environment. Multiple programs are authored, one for each of the classes of behavior that is to be recognized in teams of human trainees, which allows durations of generated data to be automatically annotated with the correct class label. We demonstrate this approach by generating behaviors corresponding to different troop maneuver formations, as would be executed by squads of Army soldiers (teams of 9 trainees). We evaluate the effectiveness of the resulting behavior classifier using gold-standard test data collected using volunteers in a testbed multiplayer simulation environment. Results indicate that high-accuracy recognition is possible using this approach, but requires the application of domain adaptation techniques to bridge the gap between synthetic data and human performance data. RELATED WORK Behavior Recognition in Virtual Simulations and Video Games The problem of recognizing team behaviors in virtual simulations using supervised machine learning methods has been investigated previously by Sukthankar and Sycara (2006). In their work, two-person tactical maneuvers are recognized in a multiplayer, game-based simulation environment using hand-authored behavior templates, which are used to classify durations of gameplay data using trained Hidden Markov Models. Our approach is analogous in several respects, using hand-authored behavior programs rather than templates, but eliminates the additional step of hand annotating the training data used in supervised learning. Additionally, we address the more challenging problem of recognizing behaviors in larger teams (nine-person infantry squads). Behavior classification has also been investigated in the analysis of gameplay data from multiplayer video games. Ahmad et al. (2019) demonstrate a successful approach to behavior classification that minimizes the hand annotation of training data. In their approach, referred to as Interactive Behavior Analysis, large amounts of multiplayer gameplay data are interactively analyzed through an iterative process of visualization, labeling, and clustering. Aimed at analysts who are interested in understanding player strategies and tactics, this descriptive approach allows for quick ​post hoc analyses with minimal labeling. Our approach contrasts with this previous work in that we aim to identify behaviors ​in situ​, using prescriptive definitions, for the purpose of providing real-time responses within the simulation environment. Methods for plan a",,,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
11599868b3f8de81062d6fe50d8effb60ac0bcd3,https://www.semanticscholar.org/paper/11599868b3f8de81062d6fe50d8effb60ac0bcd3,Matching deformed and occluded iris patterns: a probabilistic model based on discriminative cues,"The complexity and stability of the human iris pattern make it well suited for the task of biometric recognition. However, any realistically deployed iris imaging system collects images from the same iris that exhibit variation in appearance. This variation originates from external factors (such as changes in lighting conditions) as well as the subject's physiological responses (such as pupil motion or eyelid occlusion), which make reliable recognition a difficult task. We discuss pre-processing techniques which isolate and normalize the iris pattern, and survey existing techniques for iris pattern matching. The standard iris matching algorithm aligns the iris pattern in a way that accounts for relative rotations of the eye. However, we assert that the matching performance becomes more robust when the matching algorithm explicitly models and estimates pattern deformation and partial occlusion, and uses this information to determine the final match score. The novel work in this dissertation is divided into three main areas; (i) the analysis of a variety of bandpass filter bank iris pattern representations and optimization of such a representation for its ability to discriminate between iris classes, (ii) the derivation and application of fusion correlation filters to generate distortion-tolerant similarity cues between iris patterns, and (iii) the design and implementation of an iris-specific probabilistic model on the hidden states of deformation and occlusion, capable of estimating the posterior distribution over these states for a given iris pattern match comparison. Fundamentally, this dissertation combines signal processing techniques with probabilistic machine learning techniques to solve the iris recognition problem. The performance of the proposed algorithm is compared to that of the standard iris matching algorithm on three datasets: one from the Chinese Academy of Sciences (CASIA), one from the NIST Iris Challenge Evaluation (ICE), and one collected by the authors at Carnegie Mellon University (CMU). In our experiments, we demonstrate the superior accuracy of the proposed technique using both real and artificially distorted iris data. Finally, we analyze of the computational cost of our iris recognition algorithms in terms of complexity and processing time.",,2007.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
2c8d9135b0b21434c9d652bbe9185715126966e6,https://www.semanticscholar.org/paper/2c8d9135b0b21434c9d652bbe9185715126966e6,COMPUTATIONAL MODELS FOR SUSTAINABLE DEVELOPMENT,"Genetic erosion is a serious problem and computational models have been developed to prevent it. The computational modeling in this field not only includes (terrestrial) reserve design, but also decision modeling for related problems such as habitat restoration, marine reserve design, and nonreserve approaches to conservation management. Models have been formulated for evaluating tradeoffs between socioeconomic, biophysical, and spatial criteria in establishing marine reserves. The percolation theory and shortest path modeling have also been used. In this article we discuss the computational models that have been developed keeping in mind the sustainable developmentConservationists estimate that alarming rate at which biological species are disappearing will have an indelible impact on humanity. Targets which were set in 2002 to reduce the biodiversity loss by 2010 have not been met. The third global diversity outlook report said that loss of wildlife and habitats could not only exacerbate climate change through rising emissions but could also have a negative impact on food sources and industry. Computational Models in Aid Of Developing Policies For Sustainable Development The development of right policies for sustainable development is very important and involves complex decision making about the judicious use of natural resources and about striking a balance between societal, economic and environmental needs. Computational models have been used for policy formulation for example, the impact project (http://www.policy-impact.eu/home, impact: integrated methods for policy making using argument modeling and computer assisted text analysis) aims to make progress in the area of state-of the art of computational models of argumentation about policy issues, contribute to computational linguistics by developing methods for mining arguments in natural language texts, find ways and means to increase the quantity and quality of public participation in consultation processes, and invent user friendly tools (such as graphic interfaces to increase public participation). These tools can also be used for policy formulation on Biodiversity. This will lead to involvement of more people, and key stakeholders to integrate biodiversity considerations into their work. The IMPACT argumentation toolbox aimed to consist of, firstly, an argument reconstruction tool: the manual reconstruction of arguments from natural language texts was done which was supported by a library of argumentation as a constituent of argument reconstruction tool. This was done in order to enable future web logs to mark up the structure of arguments in articles in a way which allows arguments to be automatically aggregated, analyzed and visualized. The legal knowledge extension format formed a part of the basis of this tool. A policy modeling and analysis tool based on the computational models of argumentation about alternative courses of action depending on the goals and values of multiple stakeholders was also included. Prior research on knowledge representation languages for concepts (ontologies), defeasible generalizations (rules) and precedent cases including the legal knowledge interchange format (lkif) was utilized for this. The lkif was developed in the Estrella project (http://www.estrellaproject.org, Estrella:The European project for Standardized Transparent Representations in order to Extend Legal Accessibility IST-2004-027655) aimed to develop and validate an “open, standards-based platform allowing public administrations to both develop and deploy comprehensive legal knowledge management solutions”. Legal document and data management, in addition to knowledge based systems is supported by Estrella, to provide a holistic solution for improving the efficiency and quality of public administration which requires the application of complex legislation and other legal sources. Both the legal and legislative data and its analysis including possible implications on past, present and future scenarios will have to be incorporated to arrive at informed and efficient solutions. The public administration and other users are provided with a variety of competing development environments, inference engines and other tools to choose from. The main technical objectives of the Estrella project are to “develop a Legal Knowledge Interchange Format (LKIF), building upon emerging XML-based standards of the Semantic Web, including RDF and OWL, and Application Programmer Interfaces (APIs) for interacting with legal knowledge-based systems”. The policy modeling and analysis tool is proposed to include a graphical user interface for a dialogue with an inference engine to simulate and analyze the consequences of a proposed policy. The tool has been proposed to be rich in graphical interfaces to enable clear visualization of its reasoning. The comparative analysis of different policy proposals will also be facilitated by this tool. Monendra Grover et al./ Indian Journal of Computer Science and Engineering (IJCSE) ISSN : 0976-5166 Vol. 2 No. 1 55 A structured consultation tool, based on prior research on the PARMENIDES system was a part of the toolbox. The PARMENIDES system was developed by University of Liverpool (http://cgi.csc.liv.ac.uk/~parmenides/index.php). The Parmenides system is a system for deliberative democracy and allows the government and public to interact in a two way fashion. It enables the government to present policy proposals to the public and lets the public submit their views on the policy. Parmenides exploits argumentation schemes and argumentation frameworks to graphically analyze the opinions submitted by the users. The structured consultation tool is an intelligent, advanced, polling and survey tool, based on the computational models of argumentation. The models of argumentation schemes together with the model of the issues and the arguments put forward previously in the ongoing consultation are used to generate questions in the surveys. The tool substantively increases the signal to noise ratio in online discussions, without restricting the solid arguments which can be made, by helping users to apply a model of rational argument. The arguments can be more easily tracked, mapped and visualized, since there is no need to manually reconstruct arguments from natural language texts. An argument analysis, tracking and visualization tool, based on computational models of argument and argument mapping methods is also a constituent of IMPACT argumentation toolbox. There are three main features of this tool. The analysis features of this tool enables citizens to identify the applied argumentation schemes, to list implicit premises helpful for asking questions. The tracking features of this tool enables users to register their interest in particular issues and request as well as receive notification whenever new arguments have been put forward which affect these interests. The visualization feature of this tool provides a variety of graphical and interactive views onto argument graphs. This will enable citizens to appreciate and analyze the complexity of the policy issues in their entirety and contribute to the policy formulation. Besides the policy formulation tools several other issues in sustainability have been addressed by using computational models as detailed below. Computational Sustainability Computational Sustainability is a highly interdisciplinary field, with the vision that information and computing science have a potential to play an indispensable role in increasing the efficiency and effectiveness of management and allocation of our natural resources. Some of the examples of studying computational biodiversity include more efficient use of natural resources, more realistic models of maintaining and increasing biodiversity and more effective large scale computational equilibrium models of renewable energy. Computational sustainability has a unique societal relevance and effective environmental component. Computational thinking and approach is essential to provide effective and efficient solutions which include balancing environmental, economic and societal needs. Computational sustainability takes a holistic approach and encompasses problems in diverse disciplines such as ecology, natural resources, atmospheric science, materials science, renewable energy and biological and environment engineering. Computational sustainability addresses the sustainability issues by translating them into decision and optimization problem. The field of computational sustainability not only draws from computer science and mathematics, it has pushed the boundaries of these disciplines itself. This is in view of the fact that sustainability issues are of unique scale complexity and impact. The sustainability problems require integration of a wide variety of techniques from various areas with in applied mathematics and computer science such as data mining, machine learning, optimization, constraint processing and dynamical systems. The field of complex systems is also relevant to computational sustainability. The systems studied in the realm of computational sustainability consist of highly interconnected components or agents, often with conflicting interests. The Institute of computational sustainability is one of the leading institutes in the world for computational sustainability (http://www.cis.cornell.edu/ics/). The multi-disciplinary, multi-institutional ICS research team is based at Cornell University of and includes leading computer and environmental scientists at Oregon State University, Bowdoin College, Howard University, and The Conservation Fund (TCF). The computational sustainability has had a direct impact on the sustainability research for example. The ICA has developed methods and models to design economically viable conservation corridors for grizly bears and other species in U.S. and understand the impact of climate change in terms of aerosol interac",,2011.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
84f536d330bc1b3de149fb88f11e005bb6433b4d,https://www.semanticscholar.org/paper/84f536d330bc1b3de149fb88f11e005bb6433b4d,2010 GCEP Report Organic Solar Cells,"In this project, we aim at combining the molecular design and device fabrication expertise of Bao, theoretical simulation expertise of Aspuru-Guzik, structural characterization expertise of Toney, and the large distributed computing power of IBM’s World Community Grid (WCG) to rationally design organic semiconductors for solar cells from a completely new angle. Instead of molecular design from intuition, we will combine powerful theoretical tools and various characterization techniques to develop an inverse rational design methodology for novel materials. By doing so, we see a feasible path towards breakthroughs in performance. Such a massive amount of computing resources has not been previously applied to atomic-scale modeling problems in material sciences. For organic semiconductors to find ubiquitous electronics applications, the development of new materials with high mobility and air stability is critical. Achievement 1: Designed and predicted high charge carrier mobility compounds, realized high charge carrier mobility experimentally: Despite the versatility of carbon, exploratory chemical synthesis in the vast chemical space can be hindered by synthetic and characterization difficulties. We show that in silico screening of organic semiconductor materials can lead to the discovery of a new highperformance semiconductor. This work involves the theoretical screening of eight novel derivatives of the dinaphtho[2,3-b:2’,3’-f]thieno[3,2-b]thiophene semiconductor which has a maximum mobility of 8.3 cmVs. Based on the charge transport parameters and the predicted crystal structures, we identified a novel compound expected to demonstrate a two-fold improvement in mobility over the parent molecule. Synthetic and electrical characterisation of the compound is reported with single crystal field-effect transistors, showing a remarkable mobility of 13.7 cmVs. This is one of the very few organic semiconductors with mobility greater than 10 cmVs reported to date. More importantly, this is a significant step towards rationally design organic semiconductors for efficient solar cells. For next year program, we plan to extend the theoretical prediction to exciton diffusion length prediction. Achievement 2: developed a solution processing technique that generated strained organic semiconductor lattice for the first time For organic semiconductors (OSCs), the molecular packing determines the charge transport of the resulting devices. It is desirable to control the molecular packing of small molecular OSCs through facile processing methods in order to tune the electrical properties of OSC devices. We describe the alteration of the 5,12-bis(triisopropylsilylethynyl) pentacene (TIPSE-pentacene) molecular packing by changing the conditions used during solution processing deposition. Our method deposits TIPSE-pentacene thin film in a non-equilibrium state, and the π-π stacking distance between the molecules can be tuned between 3.08 Å to 3.33 Å as a function of these conditions, which in turn significantly affects the electronic properties of TIPSE-pentacene. The charge carrier mobility was increased from 0.3 cm2/Vs to a record high mobility for TIPSE-pentacene at 4.6 cm/Vs. Control of the molecular packing using processing conditions will allow for the development of high performance OSC devices beyond traditional synthetic methods. This is the first time that a π-π stacking distance less than 3.2 Å is realized. Since the charge transport is exponentially dependent on the distance between molecules, such a strained structure is expected to significantly increase the charge transport. This is likely to significantly impact the exciton transport as well, which is the subject of year 2 investigation. Achievement 3: developed and deployed the computational high-throughput screening on the WCG In the spirit of the proof-of-principle studies discussed above we have devised the machinery for the large-scale characterization of OSC candidates. We have developed a molecule generator which has produced a primary library of 10,000,000 molecular motifs of potential interest. The quantum chemistry program package Q-Chem was ported to the WCG and we have so far characterized over 1,600,000 million of these oligomer sequences in more than 22,000,000 first-principles calculations. These calculations utilized 3,000 years of donated CPU time. We have set up the server and storage (including a 90TB hard drive pod which we custom built) for the vast influx of results from the WCG. The setup includes a software infrastructure for process automation. Our data collection is used to build up a massive reference database on organic electronics with an emphasis on photovoltaic applications. We have introduced a preliminary empirical calibration of the computed results to bridge the gap between theory and experiment. A first analysis of the current results points to about 25,000 systems with very favorably aligned energetic levels, ideally suited for high-performance OPVs with power conversion efficiencies of 10% or more. Parallel to the first-principles calculations we have been exploring the use of cheminformatics descriptors and ideas from machine learning, pattern recognition, and drug discovery to rapidly gauge the quality of candidates. We have successfully devised initial empirical models for the relevant performance parameters. In the following year we will extend our data mining and analysis capabilities and we will screen for other relevant features and properties in the materials candidates.",,2011.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0fb307f8c94835678bfb14d452aaf9bab651c010,https://www.semanticscholar.org/paper/0fb307f8c94835678bfb14d452aaf9bab651c010,"Web-based management of simulation models - concepts, technologies and the users' needs","Simulation models are commonly developed by scientists to evaluate scenarios that show potential developments of the Earth system in the past, present and future. In order to provide adequate information systems that facilitate access to simulation models to a broad, heterogeneous user community, the deployment of such models on web servers provides a technical fundament. However, many aspects need to be taken into account for setting up operational, user-friendly web-based systems that include access and administration tools for simulation models. Data integration, data exchange, scenario management, and visualisation are among the most important functionalities to be accounted for, while usability needs to be aimed at by choosing an appropriate abstraction level and providing a careful interface design. Usually simulation models encapsulate complex algorithms, which have been developed by domain experts and implemented based on very diverse technologies. In order to provide the functionality of such models to users over the Internet, standardisations such as the Web Processing Service developed by the Open Geospatial Consortium (OGC) help to specify the technological framework, but do not provide concepts for guaranteeing the aforementioned functionalities and usability. An additional requirement from the administrator and developer perspective is to offer a minimum level of flexibility in information architectures in order to adapt and exchange single components such as a simulator or data base. In many cases, this flexibility stands in conflict with a rapid, use-case specific development. In this paper, different integration concepts for hydrological simulation models into web-based management systems are compared to each other. All concepts were developed to fulfil the requirements of heterogeneous user groups, ranging from scientists to re-insurance companies. Their implementation in prototypical realworld systems was performed in inter-disciplinary groups of experts in Hydrology and Information Technology. While the first three integration concepts focus specifically on functionality (legacy model encapsulation, integration of real-time data, scenario management) and usability (user interface, visualisation) for single simulation models or static process chains, the fourth use case outlines a way towards more generic service composition based on a workflow management system. A comparison of the potential and limitations of these architectures results in a discussion of aspects to be taken into account for making simulation models accessible and usable for science, industry and governmental agencies. From our experience of designing, setting up and running the developed systems we conclude that functionality and usability are in the main interest of the end users of such systems. Each user group has different requirements, depending on their expertise and objectives. However, a clear, easy-to-use user interface is far less error-prone and avoids semantic problems for lay users, while experts require complex control mechanisms to run, calibrate or even re-design their modelling infrastructure. Integrating third-party data sources is possible, but requires well-defined machine readable user interfaces. For system administration and sustainability, system architectures incorporating a higher flexibility and implementation effort in the setup phase are seen to pay off in the long term. It is very important that all relevant aspects have to be specified in the design phase of a web-based management system for simulation models. Depending on this specification, the target system focuses either on implementation speed or flexibility, which comes with the cost of a more complex service-based infrastructure. The demand for using and accessing simulation models has increased in number and complexity in recent years. With the availability of appropriate concepts and technologies from information technology, integrating such systems into a web environment is a worthwhile, yet challenging task for the modelling and Information Technology communities.",,2009.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
23faf28ce69e1e19bca7c42f80918c7ca0028c0d,https://www.semanticscholar.org/paper/23faf28ce69e1e19bca7c42f80918c7ca0028c0d,"Classification, identification, and modeling of unexploded ordnance in realistic environments","Recovery of buried unexploded ordnance (UXO) is very slow and expensive due to the high false alarm rate created by clutter. Electromagnetic induction (EMI) has been shown to be a promising technique for UXO detection and discrimination. This thesis uses the EMI response of buried targets to identify or classify them. To perform such discrimination, accurate forward models of buried UXO are needed. This thesis provides a survey of existing target models: the dipole model, the spheroid model, and the fundamental mode model. Then the implementation of a new model, the spheroidal mode model, is described and validated against measurements of a UXO. Furthermore, an in-depth study of the effects of permeable soil, modeled as a permeable halfspace, is presented. This study concludes that the discontinuity created by the air to permeable soil interface produces minimal effect in the response of a buried object. The change is limited to a magnitude shift of the real portion of the EMI response and can be reproduced by superposition of a permeable halfspace response on the response of the same object in freespace. Accurate soil modeling also allows one to invert for soil permeability values from measured data if such data are in known units. However, the EMI sensor used in this study provides measurements in consistent but unknown units. Furthermore, the instrument is from a third party and is proprietary. Therefore, this thesis describes the development of a non-invasive method to model and calibrate non-adaptive instruments so that all measurements can be converted into units consistent with modeled data. This conversion factor is shown to be a constant value across various conditions, thus demonstrating its validity. Given that now a more complete model of the measurable response of a buried UXO is implemented, this study proceeds to demonstrate that EMI responses from UXO and clutter objects can be used to identify the objects through the application of Differential Evolution (DE), a type of Genetic Algorithm. DE is used to optimize the parameters of the UXO fundamental mode model to produce a match between the modeled response and the measured response of an unknown object. When this optimization procedure is applied across a library of models for possible UXO, the correct identity of the unknown object can be ascertained because the corresponding library member will produce the closest match. Furthermore, responses from clutter objects are shown to produce very poor matches to library objects, thus providing a method to discriminate UXO from clutter. These optimization experiments are conducted on measurements of UXO in air, UXO in air but obscured by clutter fragments, buried UXO, and buried UXO obscured by clutter fragments. It is shown that the optimization procedure is successful for shallow buried objects obscured by light clutter contributing to roughly 20 dB SNR, but is limited in applicability towards very deeply buried UXO or those in dense clutter environments. The DE algorithm implemented in this study is parallelized and the optimization results are computed with a multi-processor supercomputer. Thus, the computational requirement of DE is a considerable drawback, and the method cannot be used for real time, on-site inversion of measured UXO data. To address this concern, a different approach to inversion is also implemented in this study. Rather than identifying particular UXO, one may do a discrimination between general UXO and general clutter items. Previous work has shown that the expansion coefficients of EMI responses in the spheroidal coordinate system can uniquely characterize the corresponding targets. Therefore, these coefficients readily lend themselves for use as features by which objects can be classified as likely to be UXO or unlikely to be UXO. To do such classification, the relationship between these coefficients and the physical properties of UXO and clutter, such as differences in size or body-ofrevolution properties or material heterogeneity properties, must be found. This thesis shows that such relationships are complex and require the use of the automated pattern recognition capability of machine learning. Two machine learning algorithms, Support Vector Machines and Neural Networks, are used to identify whether objects are likely to be UXO. Furthermore, the effects of small diffuse clutter fragments and uncertainty about the target position are investigated. This discrimination procedure is applied on both synthetic data from models and measurements of UXO and clutter. It is found that good discrimination is possible for up to 20 dB SNR. But the discrimination is sensitive to inaccurate estimations of a target's depth. It is found that the accuracy must be within a 10 cm deviation of an object's true depth. The general conclusion forwarded by this work is that while increasingly accurate discrimination capabilities can be produced through more detailed forward modeling and application of robust optimization and learning algorithms, the presence of noise and clutter is still of great concern. Minimization or filtering of such noise is necessary before field deployable discrimination techniques can be realized. Thesis Supervisor: Bae-Ian Wu on behalf of Jin Au Kong Title: Professor of Electrical Engineering and Computer Science",,2008.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
1ef405e24c44c313e62dfb435cd8fea2e5c0a550,https://www.semanticscholar.org/paper/1ef405e24c44c313e62dfb435cd8fea2e5c0a550,Supporting Individuals with Special Needs through Intelligent Visual Schedules,"Interventions to support individuals with cognitive disabilities surrounding the organization of time and activities often include what are known as visual schedules. These artifacts use words, images, and sometimes tangible objects to represent activities that will take place (or have taken place) arranged in temporal order to augment understanding of time, events, and places. Use of visual schedules has been shown to reduce the symptoms associated with these disabilities, in particular for individuals with autism spectrum disorder (ASD). By further expanding these capabilities using interactive and intelligent systems, computing based visual scheduling systems can more fully realize the vision of alleviating the anxiety and disorder associated with unplanned and changing events. In this paper, we describe the motivation, approach, and plan for designing, developing, deploying, and evaluation such systems. Introduction and Motivation The structure needed to reduce anxiety and support better self-organization around time and activities for individuals with autism spectrum disorder (ASD) and other special needs is often provided through visual schedules. “Visual schedules display planned activities in symbols (words, pictures, photograph, icons, actual objects) that are understood in the order in which they will occur” (ICAN 2007). They present the abstract concepts of activities and time in concrete forms by using pictures, words, and other visual elements to describe what will happen, in what order, and where. They have been used successfully in classrooms, homes, and private practices to address difficulties with sequential memory, organization of time, and language comprehension and to lessen anxiety (Hodgdon, 1999; Mesibov et al., 2002; Savner & Myles, 2000). In schools, visual schedules can assist students with transitioning independently between activities and environments by telling them where to go and helping them to know what they will do when they get there. Figure 1: Visual schedules can be placed on the wall for group activities (left) or created for individual use in a mobile or carryable format (above). The personal “wallet” shown here was developed as part of an integrated communication strategy, wherein the teacher would use verbal cues coupled with visual cues to remind the student of his tasks and ask him which reward he would choose. Many experts in the design and use of paper-based visual schedules advocate a caregivers ca mple schedule...at n a piece of pa have a grid system...one box say s Then the TeachTown website). Visual schedu s broken down any experts, such n this... “ str e understanding of time and activities by s ose a larger activity and demonstrating visually a event at the end of the task. For example, “Handwashing” can be represented by “First turn on the water, then place your hands under the water”, etc. A caregiver might end the “handwashing” sequence with a picture of dinner, indicating that once “handwashing” is completed, the enjoyable activity of eating dinner will take place. The First/Then structure can also be used at a more macro level as in “First work, then play.” By providing structure, visual schedules reduce anxiety and support behavior intervention plans f severe behavior problems. They can also support individuals with less severe disabilities in entering the workplace by providing external direction for common workplace phenomena. Because the information must be kept up to date – an extremely onerous task – and the schedules themselves tend to be more effective when they are engaging to the individuals using them, the traditional pen and paper “low tech” assistive technology approach c edules can be even more useful and successful with the addition of omputing technologies. ping needs of Figure 2: This portable visual schedule demonstrates the ""First/Then"" model supported by many theories of designing visual schedules. First/Then approach. For example, Chris Whalen, Founder and Chief Science Officer at TeachTown, describes how n begin using simple schedules: “Provide a very si first. Make a grid with two boxes (this can be done o per, draw a line down the middle and then you s First this... and the other say this...” (Whalen 2006) (See also Figure 2, also from les can be used at the micro level supporting task into sub-elements. For this type of activity, m as those at TechTown advocate a “First this... The ucture. This structure serves to augment th howing both the sequence of events that comp reward or enjoyable ocused on students with an be improved. These sch interactive and intelligent c Background of the Researchers The two primary researchers on this work, Gillian R. Hayes and Donald J. Patterson, each individually focused on technologies related to augmented cognition as part of their dissertation work. Hayes completed her doctoral studies at the Georgia Institute of Technology. While there, she was supported by Cure Autism Now, the Organization for Autism Research, and through an BM PhD fellowship to focus her research on supporting the everyday record kee I caregivers for children with autism spectrum disorder. Her primary advisor was Gregory D. Abowd with whom she worked on CareLog and concepts of Selective Archiving. Patterson completed his doctoral studies at the University of Washington. While there, he was an NDSEG fellow and did research on artificial intelligence and its application to Alzheimer's Disease management and DNA biotechnology. His primary advisors were Henry Kautz and Dieter Fox with whom he worked on the Activity Compass also known as ""Opportunity Knocks."" Both are now Assistant Professors in the Department of Informatics at the University of California, Irvine focusing on ubiquitous computing and human computer interaction. Additionally, Patterson’s work continues to cross into artificial intelligence and machine learning while Hayes’s work ontinues to involve elements of collaborative and cooperative computing. c Our work includes projects focused on assisting caregivers of children with autism (Hayes & Abowd 2006, Kientz et al. 2007, Hayes et al. 2004), supporting the needs of elderly individuals both in their homes and while mobile (Liao et al.2004, Lundell & Hayes 2005, Patterson et al. 2006, Patterson et al. 2004, Patterson et al. 2002,), identifying paths and supporting wayfinding for a variety of individuals (Patterson et al. 2004) and supporting chronic care management (Abowd et al. 2006, Hayes et al. in preparation). These projects have been completed in cooperation with researchers at three Intel research sites and IBM TJ Watson as well as with the support of nonprofit and governmental organizations such as Cure Autism Now, the NSF, and the IH. Together, we are continuing threads of work in the areas of interactive, collaborative, and ividuals with special needs or in chronic care situations.",,2007.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
0ee3803919fe314e0e2542a4427bfe305332a061,https://www.semanticscholar.org/paper/0ee3803919fe314e0e2542a4427bfe305332a061,Artificial intelligence technologies in complex engineering design,"COMPUTATIONAL ENGINEERING AND DESIGN CENTER SCHOOL OF ENGINEERING SCIENCES Doctor of Philosophy Artificial Intelligence Technologies in Complex Engineering Design by Yew Soon Ong Engineering design optimization is an emerging technology whose application both tends to shorten design-cycle time and finds new designs that are not only feasible, but also nearer to optimum, based on specified design criteria. Its gain in attention in the field of complex designs is fuelled by advancing computing power now allowing increasingly accurate analysis codes to be deployed. Unfortunately, the optimization of complex engineering design problems remains a difficult task, due to the complexity of the cost surfaces and the human expertise necessary in order to achieve high quality results. This research is concerned with the effective use of past experiences and chronicled data from previous designs to mitigate some of the limitations of present engineering design optimization process. In particular, the present work leverages well established artificial intelligence technologies and extends recent theoretical and empirical advances, particularly in machine learning, adaptive hybrid evolutionary computation, surrogate modeling, radial basis functions and transductive inference, to mitigate the issues of i) choice of optimization methods and ii) dealing with expensive design problems. The resulting approaches are studied using commonly employed benchmark functions. Further demonstrations on realistic aerodynamic aircraft and ship design problems reveal that the proposed techniques not only generate robust design performance, they can also greatly decrease the cost of design space search and arrive at better designs as compared to conventional approaches.",,2002.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
20830c4e6040b95aa686be94ddb1a268e6450016,https://www.semanticscholar.org/paper/20830c4e6040b95aa686be94ddb1a268e6450016,Impact Statement,"My work in the domain of computer security looks into the design, analysis, integration, and evaluation of methods for decision automation and attribution of malicious actors, and has had various direct impacts on the practice, whether it is in the industry or academia. The core concepts of several of those works have been patented (see patents in the attached resume), and their implementations have been used in production to facilitate attribution and risk assessment of domain names of malicious software, domain name, or even systems. In the following, I elaborate on some of such impacts. Behavior-based malware detection (pioneering) My work on behavior-based malware analysis, which is captured in my system AMAL, is one of the earliest works in this domain, initially published in 2013. AMAL, a tool to classify and cluster malware using autonomous feature extraction, sandboxes malicious binaries to collect fine-grained behavioral artifacts that characterize malware’s usage of the file system, memory, network, and registry. A key contribution of AMAL is the extrapolation of labels on unlabeled malware samples. Since its inception (2013), AMAL was incorporated into iDefense (then a subsidary of Verisign) malware analysis toolset. Today, AMAL reduces the manual analysis effort by two orders of magnitude, while providing practical accuracies. Academically, and given that AMAL one of the earliest works in this domain, the work has been well-cited with more than 150 follow-up studies on the subject. Personally, this work was the start of a productive journey on ML/DL-based detection systems as well as adversarial machine learning techniques with 30+ publications published in top conferences, e.g., ACM WiSec, ICDCS, INFOCOM, DSN, etc. Authorship attribution using deep learning (answering open questions) Code authorship attribution is an important problem where it is required to map a set of authors to their code snippets. The problem has witnessed a recent surge in interest, featured by various systems using hand-picked features for conducting this mapping, make the problem intractable for large-scale attribution. My work in this space, through DL-CAIS (CCS 2018) and Multi-X (PETS 2020) develops tools for automatically generating high-quality feature representations for large-scale authorship attribution. Compared the prior work, we were able to maintain a high accuracy for 10x the number of authors at a fraction of the overhead. Through Multi-X, we were able to attribute code segments in snippets with multiple authors. Our work in this domain closes key challenges in the area, by showing the feasibility of deep learning based code authorship attribution. Securing blockchain systems (knowledge systemization) One of the key areas of effort that I have worked on during the past 5 years is blockchain systems security. My work has provided the underpinnings for foundations-driven guarantees of blockchain systems security through a deeper understanding of their attack surface. In this direction, my work explored various attack possibilities on widely deployed blockchain systems (e.g., Bitcoin and Ethereum), including partitioning attacks (ICDCS 2019), optimized mining attacks (CCS 2021), DDoS attacks (ICBC 2019), fair mining (TPDS 2021), among many others. My work has established the need for a systematic understanding of blockchain systems security by demonstrating various vulnerabilities. The impact of this work is not only to practice, but also in academia: our work systematically analyzing the attack surface of blockchains (IEEE Communications Surveys & Tutorials, 2020) has been the go-to resource to reference, as evident by more than 65 citations to date, in less than one year. Mixing time of social graphs (paradigm shift) Until 2010, it was widely believed that social networks are “fast-mixing”, and many Sybil defenses made crucial use of this property. An experimental verification of this property was lacking, and my work explored mathematical tools and used them to measure the mixing time of several social graphs. My findings show that the mixing time of social graphs is much larger than used in literature, which leads to several striking results. First, designs based on the fast-mixing property utilize a weaker property concerning the average mixing in the social graph (as opposed to the worst mixing, making most theoretical provable guarantees of these systems inaccurate). Second, current security systems based on fast-mixing properties have weaker guarantees and have to be less efficient in to compensate for the slower mixing graphs. The work presented a breakthrough, shifting interest in the community to other assumptions for system design, and is highly cited (200+ citations as of 2020). Impact through standardization (beyond academia) Along with collaborators from TU Eindhoven, TU Darmstadt, genua GmbH, and Radboud University, I led the effort of specifying the implementation of a hash-based signature in the Internet Engineering Task Force (IETF) Request for Comments (RFC 8391). RFC 8391 provides a standard of the first hash-based signature algorithm, which is today incorporated into widely used software (OpenSSL), and is expected to be used on millions of devices. The impact of this work is rarely seen in an academic work, as such efforts are only done in industry. Students and placement (biggest impact) At UCF (since 2017), I have advised 15 doctoral students to candidacy (+3 who switched advisors before passing candidacy). Out of those 15, ten have either graduated (7), or are to graduate within 2021 (3). Out of those 10, only one PhD student went to industry, and the rest went/will go to academia: five as tenuretrack assistant professors (Wayne, Layola, Niagara, etc.) and four as postdoctoral researchers (Georgia Tech, TAMU, Northeastern). At Buffalo, I mentored two postdocs who went to academia as tenure-track assistant professors. At Buffalo and UCF, I mentored more than a dozen M.Sc. and B.Sc. students who joined government and industrial entities. Recognitions To date, my work has received more than 3500 citations, with h-index of 31 and i10-index of 86, with a healthy trajectory. My work won the best paper award at IEEE Systems Journal 2020, IEEE DSC 2019, IEEE ICDCS 2017, WISA 2014, and IEEE CNS 2013, and was featured in news articles in various outlets, including MIT Technology Review, Science Daily, Scientific American, Financial Express, Slashdot, CBS news, The Verge, New Scientist, etc.",,1997.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
e0ff1e95e940e29e7cdc99a84492a37e3a051786,https://www.semanticscholar.org/paper/e0ff1e95e940e29e7cdc99a84492a37e3a051786,RIACS FY2001 Annual Report,"Recently, there has been shift from consideration of optimal decisions in games to a consideration of optimal decision-making programs for dynamic, inaccessible, complex environments such as the real world. Perfect rationality is impossible in these environments, because of prohibiting deliberation complexity. Anytime algorithms attempt to trade off result quality for the time or memory needed to generate results. Bounded rational agents are ones that always take the actions that are expected to optimize their performance measure, given the percept sequence they have seen so far and limited resources they have. Process algebras, with basic programming operators, has been used to study the behaviors of interactive multi-agent systems and leading to more expressive models than Turing Machines, e.g., Interaction Machines. By extending process algebra operators with von Neumann/Morgenstern’s costs/utilities, anytime algorithms can be viewed as a basis for a general theory of computation. As the result we shift a computational paradigm from the design of agents achieving one-time goals, to the agents who persistently attempt to optimize their happiness. We call this approach $-calculus (pronounced “cost-calculus”), which is a higher-order polyadic process algebra with a utility (cost) allowing to capture bounded optimization and metareasoning in distributed interactive AI systems. $-calculus extends performance measures beyond time to include answer quality and uncertainty, using k Omega-optimization to deal with spatial and temporal constraints in a flexible way. This is a very general model, just as neural networks or genetic algorithms, leading to a new programming paradigm (cost languages) and a new class of computer architectures (cost-driven computers). The NSERC supported project on $-calculus aims at investigation, design and implementation of a wide class of adaptive real-time distributed complex systems exhibiting meta-computation and optimization. It has also been applied to the Office of Naval Research SAMON robotics testbed to derive GBML (Generic Behavior Message-passing Language) for behavior planning, control and communication of heterogeneous Autonomous Underwater Vehicles (AUVs). Some preliminary ideas have also been utilized in the 5th Generation ESPRIT SPAN project on integration of objectoriented, logic, procedural and functional styles of programming in parallel architectures. It appears that $-calculus can be useful for the NASA Information Power Grid (IPG) Project. The IPG testbed provides access to a widely distributed network of high performance computers. $calculus resource-bounded optimization allows for flexible allocation of resources and scalability needed to tackle hard computation problems, thus $-calculus could provide a unifying metasystem framework for the Information Power Grid. Biosketch: Dr. Eberbach is a Professor at School of Computer Science, Acadia University and an Adjunct Professor at Faculty of Graduate Studies, Dalhousie University, Canada. Previously he was Senior Scientist at Applied Research Lab, The Pennsylvania State University, Visiting Professor at The University of Memphis, USA, Research Scientist at University College London, U.K., Assistant Professor in Poland, and he also has industrial experience. Professor Eberbach’s current work is in the areas of process algebras, resource bounded optimization, autonomous agents and mobile RIACS FY2001 Annual Report October 2000 through September 2001 -135robotics. General topics of interest are new computing paradigms, languages and architectures, distributed computing, concurrency and interaction, evolutionary computing and neural nets. More information about projects, publications, courses taught can be found at http://cs.acadiau.ca/~eberbach October 27, 2000: Feng Zhao, Ph.D.,Principal Scientist, Xerox PARC “Smart Sensors, Collaborative Sensemaking” Imagine a world in which we live where smart roads would be able to tell us when they need repair and which is the best direction to get to the Giants game, smart factories would stock up just enough inventory, ... The rapid advances in micro-electro-mechanical systems (MEMS) and lower-power wireless networking have enabled a new generation of tiny, cheap, networked sensors that can be “sprayed” on roads, across machines, and on walls. However, these massively distributed sensor networks must overcome a set of technological hurdles before they become widely deployable. Keeping up with the constant onslaught of sensory data from say 100,000 sensors is akin to drinking from a fire hose. The Xerox PARC Smart Matter Diagnostics and Collaborative Sensing Project studies the fundamental problems of distilling high-level, humaninterpretable knowledge from distributed heterogeneous sensor signals in a rapid and scalable manner. We are developing powerful algorithms and software systems to enable a wide range of applications, from sensor-rich health monitoring of electro-mechanical equipment to human-aware environments that leverage sensors to support synergistic interactions with the physical world. Biosketch: Feng Zhao is a Principal Scientist in the Systems and Practices Laboratory at Xerox PARC. Dr. Zhao leads the Smart Matter Diagnostics Project that investigates how sensors and networking technology can change the way we build and interact with physical devices and environments. His research interest includes distributed sensor data analysis, diagnostics, qualitative reasoning, and control of dynamical systems. Dr. Zhao received his PhD in Electrical Engineering and Computer Science from MIT in 1992, where he developed one of the first algorithms for fast N-body computation and phase-space nonlinear control synthesis. From 1992 to 1999, he was Assistant and Associate Professor of Computer and Information Science at Ohio State University. His INSIGHT Group developed the SAL software tool for rapid prototyping of spatio-temporal data analysis applications; the tool is currently used by a number of other research groups. Currently, he is also Consulting Associate Professor of Computer Science at Stanford. Dr. Zhao was National Science Foundation and Office of Naval Research Young Investigator, and an Alfred P. Sloan Research Fellow in Computer Science. He has authored or co-authored about 50 peer-reviewed technical papers in the areas of smart matter, artificial intelligence, nonlinear control, and programming tools. October 12, 2000: Irem Tumer, Intelligent Health and Safety Group NASA/Ames “Influence of Variations on Systems’ Performance And Safety” High-risk aerospace components have to meet very stringent quality, performance, and safety requirements. Any source of variation is of concern, as it may result in scrap or rework (translating into production delays), poor performance (translating into customer dissatisfaction), and potentially unsafe flying conditions (translating into catastrophic failures). As part of the Intelligent RIACS FY2001 Annual Report October 2000 through September 2001 -136Health and Safety group, we have been designing controlled experiments to understand various sources of variations in helicopter transmissions, collecting vibration data, and analyzing the data for indicators of the variations. We are looking for normal and abnormal sources of variation that affect performance and indicators of these variations to provide warning about potential failures during flight. The experiments include: • Flight tests using an AH-1 and an OH-58 helicopter, to determine the variations introduced due to regular maneuvering and the covariance with environmental conditions, engine torque, etc.; • OH-58 transmission test-rig tests to determine the effect of variations due to different levels of torque, mast bending, and mast lifting forces, as well as pinion reinstallation effects; • Machinery Fault Simulator tests to test the effect of prefabricated defects and inherent design and manufacturing variations on gears, bearings, etc. In this talk, I will present an overview of our group’s research goals, discuss the experiments and go over some of the results from the data analyses conducted so far. I will then discuss the current work and future directions in developing formalized methods for design and manufacturing engineers, using the variation information from empirical and analytical studies. RIACS FY2001 Annual Report October 2000 through September 2001 -137III.B RIACS-Supported Workshops As part of its mission of fostering ties with the academic community in IT, RIACS provides financial, administrative, and technical support for selected workshops involving RIACS scientists. The following workshops were supported during this reporting year: Workshop on Verification and Validation of Software The RIACS Workshop on the Verification and Validation of Autonomous and Adaptive Systems took place at Asilomar Conference Center, Pacific Grove, CA, 5-7 Dec 2000. Discussions included: V&V of Intelligent Systems: How to verify and validate systems featuring some form of AI-based technique, such as model-based, rule-based or knowledge-based systems. V&V of Adaptive Systems: How to verify and validate systems featuring adaptive behavior, either in the form of parametric adaptation (e.g. neural nets, reinforcement learning) or control adaptation (e.g. genetic programming). V&V of Complex Systems: How to verify and validate systems with different interacting parts, either within a given location (e.g. layered control architectures) and among several locations (homogenous or heterogeneous multi-agent systems). Workshop on Model-based Validation of Intelligence Lina Khatib (Kestrel) and Charles Pecheur co-organized a symposium on “Model-based Validation of Intelligence” as part of the AAAI Spring Symposium Series in March 2001. We provided the technical content (announcement, reviews and selection of articles, final program) while AAAI provided the logistics (rooms, registra",,2001.0,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
