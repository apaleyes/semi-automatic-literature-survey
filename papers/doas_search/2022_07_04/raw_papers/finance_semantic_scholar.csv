paperId,url,title,abstract,venue,year,externalIds.CorpusId,externalIds.DOI,database,query_name,query_value
4650205b4834825b72644bf9d35553f657e141a8,https://www.semanticscholar.org/paper/4650205b4834825b72644bf9d35553f657e141a8,Yes We Care! - Certification for Machine Learning Methods through the Care Label Framework,"Machine learning applications have become ubiquitous. Their applications from machine embedded control in production over process optimization in diverse areas (e.g., traffic, finance, sciences) to direct user interactions like advertising and recommendations. This has led to an increased effort of making machine learning trustworthy. Explainable and fair AI have already matured. They address knowledgeable users and application engineers. However, there are users that want to deploy a learned model in a similar way as their washing machine. These stakeholders do not want to spend time understanding the model. Instead, they want to rely on guaranteed properties. What are the relevant properties? How can they be expressed to stakeholders without presupposing machine learning knowledge? How can they be guaranteed for a certain implementation of a model? These questions move far beyond the current state-of-the-art and we want to address them here. We propose a unified framework that certifies learning methods via care labels. They are easy to understand and draw inspiration from well-known certificates like textile labels or property cards of electronic devices. Our framework considers both, the machine learning theory and a given implementation. We test the implementation’s compliance with theoretical properties and bounds. In this paper, we illustrate care labels by a prototype implementation of a certification suite for a selection of probabilistic graphical models. K. Morik, H. Kotthaus, L. Heppe, D. Heinrich, R. Fischer, S. Mücke, A. Pauly, M. Jakobs TU Dortmund University, Germany E-mail: {forename}.{surname}@tu-dortmund.de N. Piatkowski Fraunhofer IAIS, Germany E-mail: nico.piatkowski@iais.fraunhofer.de ar X iv :2 10 5. 10 19 7v 1 [ cs .L G ] 2 1 M ay 2 02 1 2 Katharina Morik et al.",ArXiv,2021.0,235125614,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
0a76317f02098e81b735b13722ae8c799169f5e0,https://www.semanticscholar.org/paper/0a76317f02098e81b735b13722ae8c799169f5e0,"Machine learning for quantitative finance: fast derivative pricing, hedging and fitting","In this paper, we show how we can deploy machine learning techniques in the context of traditional quant problems. We illustrate that for many classical problems, we can arrive at speed-ups of several orders of magnitude by deploying machine learning techniques based on Gaussian process regression. The price we have to pay for this extra speed is some loss of accuracy. However, we show that this reduced accuracy is often well within reasonable limits and hence very acceptable from a practical point of view. The concrete examples concern fitting and estimation. In the fitting context, we fit sophisticated Greek profiles and summarize implied volatility surfaces. In the estimation context, we reduce computation times for the calculation of vanilla option values under advanced models, the pricing of American options and the pricing of exotic options under models beyond the Black–Scholes setting.",Quantitative Finance,2018.0,158842912,10.1080/14697688.2018.1495335,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
1d3539a8d94bd3ab78993d7cc584efc06ed0e460,https://www.semanticscholar.org/paper/1d3539a8d94bd3ab78993d7cc584efc06ed0e460,Synthetic Benchmarks for Scientific Research in Explainable Machine Learning,"As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on realworld datasets. In this work, we address this issue by releasing XAI-BENCH: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate groundtruth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and across a variety of settings. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench.",NeurIPS Datasets and Benchmarks,2021.0,235606403,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
5ef27251f46f8afc45e5828beaf2f4627db8146e,https://www.semanticscholar.org/paper/5ef27251f46f8afc45e5828beaf2f4627db8146e,The absorption and multiplication of uncertainty in machine‐learning‐driven finance,"Abstract Uncertainty about market developments and their implications characterize financial markets. Increasingly, machine learning is deployed as a tool to absorb this uncertainty and transform it into manageable risk. This article analyses machine‐learning‐based uncertainty absorption in financial markets by drawing on 182 interviews in the finance industry, including 45 interviews with informants who were actively applying machine‐learning techniques to investment management, trading, or risk management problems. We argue that while machine‐learning models are deployed to absorb financial uncertainty, they also introduce a new and more profound type of uncertainty, which we call critical model uncertainty. Critical model uncertainty refers to the inability to explain how and why the machine‐learning models (particularly neural networks) arrive at their predictions and decisions—their uncertainty‐absorbing accomplishments. We suggest that the dialectical relation between machine‐learning models’ uncertainty absorption and multiplication calls for further research in the field of finance and beyond.",The British journal of sociology,2021.0,236450800,10.1111/1468-4446.12880,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
b5b98051b65da6b1b3b579862b0407d48c5bef48,https://www.semanticscholar.org/paper/b5b98051b65da6b1b3b579862b0407d48c5bef48,Principles and Practice of Explainable Machine Learning,"Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.",Frontiers in Big Data,2020.0,221878773,10.3389/fdata.2021.688969,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
c4d9d392926b7133302fa34b83a719c8a6e88ea5,https://www.semanticscholar.org/paper/c4d9d392926b7133302fa34b83a719c8a6e88ea5,Efficient Privacy-Preserving Machine Learning for Blockchain Network,"A blockchain as a trustworthy and secure decentralized and distributed network has been emerged for many applications such as in banking, finance, insurance, healthcare and business. Recently, many communities in blockchain networks want to deploy machine learning models to get meaningful knowledge from geographically distributed large-scale data owned by each participant. To run a learning model without data centralization, distributed machine learning (DML) for blockchain networks has been studied. While several works have been proposed, privacy and security have not been sufficiently addressed, and as we show later, there are vulnerabilities in the architecture and limitations in terms of efficiency. In this paper, we propose a privacy-preserving DML model for a permissioned blockchain to resolve the privacy, security, and performance issues in a systematic way. We develop a differentially private stochastic gradient descent method and an error-based aggregation rule as core primitives. Our model can treat any type of differentially private learning algorithm where non-deterministic functions should be defined. The proposed error-based aggregation rule is effective to prevent attacks by an adversarial node that tries to deteriorate the accuracy of DML models. Our experiment results show that our proposed model provides stronger resilience against adversarial attacks than other aggregation rules under a differentially private scenario. Finally, we show that our proposed model has high usability because it has low computational complexity and low transaction latency.",IEEE Access,2019.0,203165185,10.1109/ACCESS.2019.2940052,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
ed9b2c4d4ea93d86a6e1167c9e92cfa0a243c4bc,https://www.semanticscholar.org/paper/ed9b2c4d4ea93d86a6e1167c9e92cfa0a243c4bc,Challenges and Opportunities for Unikernels in Machine Learning Inference,"Machine Learning has become a value creator for many new and old businesses. However, efficient realworld machine learning deployments are still a challenge. Traditional Machine Learning deployments suffer from efficient resource utilization and achieving predictable latency. They cannot be treated in the same manner as other application server deployments. Unikernels are a method to specialize application deployment and performance to suit the needs of the application. Traditionally, building or porting applications to unikernels have been challenging. However, recent work has been into simplifying the development of unikernels. Real-world Unikernels as of now are only for specializing applications that run on the CPU. We survey machine learning practitioners and find out that the majority of machine learning practitioners are using the CPU for machine learning deployments, thus, creating an opportunity for unikernels to optimize the performance of these applications. We compare the architecture of two unikernels: nanos and Unikraft. We benchmarked scikit-learn, a popular machine library, inside a unikernel and found that it only offered a 1% advantage over a traditional deployment. However, our testing could not include more innovative systems like Unikraft due to their immaturity and inability to run machine learning libraries. We include a dependency analysis of three popular machine learning libraries Tensorflow Lite, PyTorch and ONNX, to help pave the way for building machine learning applications as Unikraft unikernels.","2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",2021.0,244159215,10.1109/icrito51393.2021.9596080,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
b14558c0b727af0ac9086f463b6030b9072dbe16,https://www.semanticscholar.org/paper/b14558c0b727af0ac9086f463b6030b9072dbe16,Methods for Automatic Machine-Learning Workflow Analysis,,ECML/PKDD,2021.0,236507098,10.1007/978-3-030-86517-7_4,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
56e63ffea11c875f7eee257798e18cd04e453b6c,https://www.semanticscholar.org/paper/56e63ffea11c875f7eee257798e18cd04e453b6c,Synthetic Benchmarks for Scientific Research in Explainable Machine Learning,"As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on realworld datasets. In this work, we address this issue by releasing XAI-BENCH: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate groundtruth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and across a variety of settings. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench.",,2021.0,244907580,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
6068d39e92aef1bb0e1291e9931894c35692a85e,https://www.semanticscholar.org/paper/6068d39e92aef1bb0e1291e9931894c35692a85e,Counterfactual Explanations for Machine Learning: A Review,"Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.",ArXiv,2020.0,224818450,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
774c9e4a613959fd07a604d07db88f37bb779c16,https://www.semanticscholar.org/paper/774c9e4a613959fd07a604d07db88f37bb779c16,Alternative data and sentiment analysis: Prospecting non-standard data in machine learning-driven finance,"Social media commentary, satellite imagery and GPS data are a part of ‘alternative data’, that is, data that originate outside of the standard repertoire of market data but are considered useful for predicting stock prices, detecting different risk exposures and discovering new price movement indicators. With the availability of sophisticated machine-learning analytics tools, alternative data are gaining traction within the investment management and algorithmic trading industries. Drawing on interviews with people working in investment management and algorithmic trading firms utilizing alternative data, as well as firms providing and sourcing such data, we emphasize social media-based sentiment analytics as one manifestation of how alternative data are deployed for stock price prediction purposes. This demonstrates both how sentiment analytics are developed and subsequently utilized by investment management firms. We argue that ‘alternative data’ are an open-ended placeholder for every data source potentially relevant for investment management purposes and harnessing these disparate data sources requires certain standardization efforts by different market participants. Besides showing how market participants understand and use alternative data, we demonstrate that alternative data often undergo processes of (a) prospecting (i.e. rendering such data amenable to processing with the aid of analytics tools) and (b) assetization (i.e. the transformation of data into tradable assets). We further contend that the widespread embracement of alternative data in investment management and trading encourages a financialization process at the data level which raises new governance issues.",Big Data & Society,2022.0,246099305,10.1177/20539517211070701,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
f72b55f4fe808b11c6c6c8fecc863cbc0bdda7b2,https://www.semanticscholar.org/paper/f72b55f4fe808b11c6c6c8fecc863cbc0bdda7b2,"Artificial Intelligence, Machine Learning, and Bias In Finance: Toward Responsible Innovation","Over the last decade, a growing number of digital startups launched bids to lure business from the financial services industry. Financial technology (“fintech”) firms deploying ever more complex and opaque algorithms assess the creditworthiness of consumers. Armed with vast quantities of data and complex algorithms to interpret the data, these firms are reigniting debates about how best to regulate financial institutions and technology firms engaged in consumer banking activities. <br><br>With a few quick taps on a smart phone, consumers can access a growing universe of apps that offer discounted interest rates on consumer loans. For proponents, the launch of fintech firms marks a new frontier in the ever-expanding utopian vision of the “technological sublime” or faith-like devotion to the potential for technology to transform us into a more equitable and just society. Consumer advocates are justifiably skeptical. While legally prohibited today, well-documented discriminatory, exclusionary, and predatory credit market practices persist. <br><br>This Essay describes fintech firms’ integration of learning algorithms and their anticipated economic and social welfare benefits — enhanced efficiency, accuracy, and accessibility. We then examine the emerging regulatory landscape. Over the last decade, federal banking regulators signaled and adopted policies that preempted state regulatory authority over fintech firms. A recent announcement by the Office of the Comptroller of the Currency (OCC) revealed the agency’s intention to allow fintech firms to apply for special purpose charters that would permit them to operate, in many respects, as national banks (“Fintech Charter Decision”). <br><br>The OCC’s Fintech Charter Decision creates gaps in the supervision of fintech firms and encourages market participants to engage in regulatory arbitrage. We argue that federal special purpose charters set the stage for regulatory arbitrage and may enable fintech firms to minimize their exposure to state antidiscrimination and consumer protection regulations. Reducing regulatory oversight of these important legal and ethical norms in a dynamic and evolving market defined by a technology that may import unconscious biases and disadvantage lower-income individuals and families raises red flags. We conclude with brief reflections regarding the necessity for courts and regulators to balance the promised benefits of fintech firms’ neo-banking initiatives with the historic and special gatekeeping role of banking platforms. Unilateral deregulatory action by state or federal regulators may undermine efforts to ensure effective oversight of fintech firms that seek to extend access to safe and affordable banking services.",,2019.0,211437887,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
6ea25b3405e8cd27a0f0a5a8e0f2dcbc053d6280,https://www.semanticscholar.org/paper/6ea25b3405e8cd27a0f0a5a8e0f2dcbc053d6280,Quantitative Finance Research Newsletter Oxford-Man Institute OMIReNew Recovering Missing Firm Characteristics with Attention-based Machine Learning,"model reconstructs firm characteristics with high accuracy and comfortably outperforms competing approaches. Revisiting the vast literature on risk factors in financial research reveals the of missing observations The work develops an approach for solving time-consistent risk-sensitive stochastic optimisation problems using model-free reinforcement learning. It assumes that agents assess the risk of a sequence of random variables using dynamic convex risk measures. They employ a time-consistent dynamic programming principle to determine the value of a particular policy, and develop policy gradient update rules that aid in obtaining optimal policies. They further develop an actor-critic style algorithm using neural networks to optimise over policies. Finally, the work demonstrates the performance and flexibility of their approach by applying it to three optimisation problems: statistical arbitrage trading strategies, obstacle avoidance robot control, and financial hedging. We present a simple and effective methodology for the generation of lexicons (word lists) that may be used in natural language scoring applications. In particular, in the finance industry, word lists have become ubiquitous for sentiment scoring. These have been derived from dictionaries such as the Harvard Inquirer and require manual curation. Here, we present an automated approach to the curation of lexicons, which makes automatic preparation of any initial word list immediate, which can then be further curated. We show that our automated word lists deliver comparable performance to traditional lexicons on machine learning classification tasks. This new approach will enable finance academics and practitioners to create and deploy new word lists in addition to the few traditional ones in a facile manner. The work studies the long-term impact of climate change on economic activity across countries, using a stochastic growth model where productivity is affected by deviations of temperature and precipitation from their long-term moving average historical norms. Using a panel data set of 174 countries the 1960 2014, they find that the per-capita real output divergence to The demonstrates the effectiveness with numerical experiments which highlight both removal and the fidelity of the calibrated simulator. on the estimation of the equity joint estimation of the The work investigates the impact of order flow imbalance (OFI) on price movements in equity markets in a multiasset setting. First, authors show that taking into account multiple levels of the order book when defining order book imbalance leads to higher explanatory power for the contemporaneous price impact of OFI. Using a principal component analysis of OFI across order book levels, they define a notion of integrated OFI which shows superior explanatory power for market impact both in-sample and out-of-sample. Second, they examine the notion of cross-impact and show that, once the information from multiple levels is included in OFI, multi-asset models with cross-impact do not provide additional explanatory power for contemporaneous impact compared to a sparse model without cross-impact terms. However, they find evidence that cross-impact terms provide additional information for intraday forecasting of future returns",,2022.0,248149090,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
52205c0ba2d87ea2eac5efe8e452d4c4d2666cbd,https://www.semanticscholar.org/paper/52205c0ba2d87ea2eac5efe8e452d4c4d2666cbd,A machine learning approach to predict the success of crowdfunding fintech project,"PurposeThe crowdfunding market has experienced rapid growth in recent years. However, not all projects are successfully financed because of information asymmetries between the founder and the providers of external finance. This shortfall in funding has made factors that lead to successful fundraising, a great interest to researchers. This study draws on the social capital theory, human capital theory and level of processing (LOP) theory to predict the success of crowdfunding projects.Design/methodology/approachA feature set is extracted and correlations between project success and features are utilized to order the features. The artificial neural network (ANN) is popularly applied to analyze the dependencies of the input variables to improve the accuracy of prediction. However, the problem of overfitting may exist in such neural networks. This study proposes a neural network method based on ensemble machine learning and dropout methods to generate several neural networks for preventing the problem of overfitting. Four machine learning techniques are applied and compared for prediction performance.FindingsThis study shows that the success of crowdfunding projects can be predicted by measuring and analyzing big data of social media activity, human capital of funders and online project presentation. The ensemble neural network method achieves highest accuracy. The investments rose from early projects and another platform by the funder serve as credible indicators for later investors.Practical implicationsThe managerial implication of this study is that the project founders and investors can apply the proposed model to predict the success of crowdfunding projects. This study also identifies the most influential features that affect fundraising outcomes. The project funders can use these features to increase the successful opportunities of crowdfunding project.Originality/valueThis study contributes to apply a new machine learning modeling method to extract features from activity data of crowdfunding platforms and predict crowdfunding project success. In addition, it contributes to the research on the deployment of social capital, human capital and online presentation strategies in a crowdfunding context as well as offers practical implications for project funders and investors.",,2020.0,225615418,10.1108/jeim-01-2019-0017,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
954650f25c20588538c903f70e88528b79b9f2f5,https://www.semanticscholar.org/paper/954650f25c20588538c903f70e88528b79b9f2f5,MPCLeague: Robust MPC Platform for Privacy-Preserving Machine Learning,"In the modern era of computing, machine learning tools have demonstrated their potential in vital sectors, such as healthcare and finance, to derive proper inferences. The sensitive and confidential nature of the data in such sectors raises genuine concerns for data privacy. This motivated the area of Privacy-preserving Machine Learning (PPML), where privacy of data is guaranteed. In this thesis, we design an efficient platform, MPCLeague, for PPML in the Secure Outsourced Computation (SOC) setting using Secure Multi-party Computation (MPC) techniques. MPC, the holy-grail problem of secure distributed computing, enables a set of n mutually distrusting parties to perform joint computation on their private inputs in a way that no coalition of t parties can learn more information than the output (privacy) or affect the true output of the computation (correctness). While MPC, in general, has been a subject of extensive research, the area of MPC with a small number of parties has drawn popularity of late mainly due to its application to real-time scenarios, efficiency and simplicity. This thesis focuses on designing efficient MPC frameworks for 2, 3 and 4 parties, with at most one corruption and supports ring structures. At the heart of this thesis are four frameworks - ASTRA, SWIFT, Tetrad, ABY2.0 - catered to different settings. The practicality of our framework is argued through improvements in the benchmarking of widely used ML algorithms -- Linear Regression, Logistic Regression, Neural Networks, and Support Vector Machines. We propose two variants for each of our frameworks, with one variant aiming to minimise the execution time while the other focuses on the monetary cost. The concrete efficiency gains of our frameworks coupled with the stronger security guarantee of robustness make our platform an ideal choice for a real-time deployment of PPML techniques.",ArXiv,2021.0,245501913,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
fd787b65bd7fdf7fc28e051acdc3d9310b3173dc,https://www.semanticscholar.org/paper/fd787b65bd7fdf7fc28e051acdc3d9310b3173dc,ANALYSIS AND FORECAST OF COMMODITY PRICE USING MACHINE LEARNING AND DEPLOYMENT IN WEB APPLICATION,"Prediction of financial market accurately is certainly significant. Commodity price fluctuations affects the global economic activity. Earning in export industry rely mainly on primary commodity and the movements of commodity prices has significant impact on overall economic progress for all countries. The method of forecasting plays a vital role in predicting adverse movements in case of commodity price prediction. In today’s world, the growth in deep learning outperforms in several demonstration in fields of financial market analysis. In this paper, we present a productionized commodity price analysis and prediction using LSTM model.  The LSTM model gathers data in a sequential order periodically of the commodity value.  Datasets used in our model are collected in real time from Yahoo Finance via API to reduce local storage.  The critical task is to transform a machine learning model into user accessible real time environment. [2]  Manual conversion of a machine learning model source code into a software is a time-consuming and error-prone task.  Investors and traders can buy and sell commodities directly in the spot market.",,2021.0,236955118,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
649024e1a5fa08d209e1b8a4882bcd9e5876061f,https://www.semanticscholar.org/paper/649024e1a5fa08d209e1b8a4882bcd9e5876061f,Editorial: Special Section on Services Computing Management for Artificial Intelligence and Machine Learning,"F IFTEEN years ago, few would have imagined that employees could work entirely remotely or that an entire business infrastructure could exist on the Internet. With the adoption of services computing, a service that allows companies to access processing and data storage through the Internet, these business models are becoming a reality. Services computing requires a multidisciplinary lens that integrates science and technology to bridge the gap between business services and information technology (IT) services [item 1) in the Appendix]. Services computing management involves 1) ensuring services computing strategy which is allied with how the organization manages IT and how IT is aligned with organizational strategy, 2) designing, building, sourcing, and deploying resilient computing solutions, trusted, efficient, and address quality of service (QoS) expectations, and 3) overseeing all matters related to business and IT services operations and resources both across business domains and within domains such as retail, finance, healthcare, logistics, and others [item 2) in the Appendix]. The goal of services computing is to enable IT services and computing technology to perform business services more efficiently and effectively [item 3) in the Appendix]. The pervasive nature of services computing management is exhibited in almost all industry settings [item 4) in the Appendix]. In everyday life, new business service innovations will give rise to an emergent dataand information-focused economy that will only pick up steam as both consumer and business utilization of Internet of Things are advanced. Concomitantly, we are moving toward an era of artificially intelligent (AI) (e.g., cognitive computing) services, which are deployed in multiscale, complex distributed architectures. Cognitive computing is the use of computerized models to simulate the human thought process in complex situations where the answers may be ambiguous and uncertain. Computers are increasingly capable of doing things that humans could once do exclusively. Today smart machines are becoming like humans by recognizing voices, processing natural language, learning, and interacting and learning with the physical world through their vision, smell, touch, and other senses, mobility, and motor control. In some cases, they do a much faster and better job than humans at recognizing patterns, performing rule-based analysis on a very large amount of data, and solving both structured and unstructured problems [item 5) in the Appendix].",IEEE Trans. Engineering Management,2021.0,228077504,10.1109/tem.2020.3024363,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
fce9518a4edff2c445d8d919bcee4440662101e7,https://www.semanticscholar.org/paper/fce9518a4edff2c445d8d919bcee4440662101e7,Using machine learning to assess public policies: a real case study for supporting SMEs development in Italy,"In recent years, several initiatives have been taken by governments to support investments in small and medium-sized enterprises. The aim is to foster their access to finance, and thus enhance their competitiveness. This paper investigates, through artificial intelligence, the socio-economic effects of these financial instruments on the performance and business continuity of the beneficiary companies. Moreover, this paper illustrates how artificial intelligence can support public decision-makers in creating and deploying regional policies. This study is a part of the collaboration among Arisk Srl and some policy-makers of the Regional Government of Piedmont (Italy).",2021 IEEE Technology & Engineering Management Conference - Europe (TEMSCON-EUR),2021.0,236481763,10.1109/TEMSCON-EUR52034.2021.9488581,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
b14692c1e0658dd1d814c737171ce714b1588360,https://www.semanticscholar.org/paper/b14692c1e0658dd1d814c737171ce714b1588360,An Intelligent UAV Deployment Scheme for Load Balance in Small Cell Networks Using Machine Learning,"In wireless networks, network load can be highly unbalanced due to the mobility of user equipments (UEs). Unmanned Aerial Vehicles (UAVs) supported base station with the advantage of flexible deployment, ubiquitous wireless coverage and high speed data rate, is a promising approach to handle with the foregoing problem. However, how to achieve cost-effective UAV deployment in an autonomous and dynamic manner is a significant challenge. Facing this problem, we propose a novel UAV base station intelligent deployment scheme based on machine learning and evaluate its performance on a realworld dataset. First, we conduct data preprocessing to process, clean, and transform raw data into formatted data. Missing values are filled by Conditional Mean Imputation (CMI) method and outliers are corrected by pauta criterion. Then, we use hybrid approach which contains ARIMA model and XGBoost model. Linear predictions are carried out by ARIMA model and later nonlinear model XGBoost are applied on residue of ARIMA. Resultant prediction is obtained by adding linear and nonlinear prediction, hybrid model is estimated by Root Mean Square Error (RMSE) and R2 score. Finally, according to predicted results, UAV base stations can be deployed to cater for dynamically changing demands in the hotspot areas and achieve cost-effective deployment. Simulation results show that the propose scheme is superior to other benchmark schemes in load balancing.",2019 IEEE Wireless Communications and Networking Conference (WCNC),2019.0,207757693,10.1109/WCNC.2019.8885648,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
19552c33b6144ba9cf02b52310cfdccdc66b14f2,https://www.semanticscholar.org/paper/19552c33b6144ba9cf02b52310cfdccdc66b14f2,Empirical observation of negligible fairness-accuracy trade-offs in machine learning for public policy,,Nat. Mach. Intell.,2020.0,234353731,10.1038/s42256-021-00396-x,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
0e54e1d7ca6d795bb5e6bd9ad9291af46fbcaa72,https://www.semanticscholar.org/paper/0e54e1d7ca6d795bb5e6bd9ad9291af46fbcaa72,Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning,"Machine learning has started to be deployed in fields such as healthcare and finance, which propelled the need for and growth of privacy-preserving machine learning (PPML). We propose an actively secure four-party protocol (4PC), and a framework for PPML, showcasing its applications on four of the most widely-known machine learning algorithms -- Linear Regression, Logistic Regression, Neural Networks, and Convolutional Neural Networks. 
Our 4PC protocol tolerating at most one malicious corruption is practically efficient as compared to the existing works. We use the protocol to build an efficient mixed-world framework (Trident) to switch between the Arithmetic, Boolean, and Garbled worlds. Our framework operates in the offline-online paradigm over rings and is instantiated in an outsourced setting for machine learning. Also, we propose conversions especially relevant to privacy-preserving machine learning. 
The highlights of our framework include using a minimal number of expensive circuits overall as compared to ABY3. This can be seen in our technique for truncation, which does not affect the online cost of multiplication and removes the need for any circuits in the offline phase. Our B2A conversion has an improvement of $\mathbf{7} \times$ in rounds and $\mathbf{18} \times$ in the communication complexity. In addition to these, all of the special conversions for machine learning, e.g. Secure Comparison, achieve constant round complexity. 
The practicality of our framework is argued through improvements in the benchmarking of the aforementioned algorithms when compared with ABY3. All the protocols are implemented over a 64-bit ring in both LAN and WAN settings. Our improvements go up to $\mathbf{187} \times$ for the training phase and $\mathbf{158} \times$ for the prediction phase when observed over LAN and WAN.",IACR Cryptol. ePrint Arch.,2019.0,208179672,10.14722/ndss.2020.23005,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
8a3282528f15b2f088a6a00d5d38442d4e496aa0,https://www.semanticscholar.org/paper/8a3282528f15b2f088a6a00d5d38442d4e496aa0,Concurrent 18. Presentation for: Minimising pipeline leaks and maximising operational life by application of machine learning at Cooper Basin,"Presented on Wednesday 18 May: Session 18 The development of technologies in the last few decades has enabled operators to collect significantly more data than previously possible. Despite availability, making data-driven decisions on asset health, and developing efficient asset management strategies, is not common. This is mainly due to challenges with compilation, and alignment of all the data into a comprehensive picture of pipeline integrity, as it consumes significant resources deploying conventional methods. A critical advantage of modern data storage, analysis and visualisation techniques is the relative ease of performing statistical assessments of integrity data. Analysis of correlated data can be equally challenging as algorithms used can be overly simplistic and inaccurate. Machine learning algorithms parse, analyse and learn from data, enabling the operators to make an educated decision. This has been extensively deployed in other industries such as finance, healthcare and supply chain management but has never been fully developed and enhanced in pipeline integrity industry until very recently. This paper provides an overview of the development in machine learning tools in pipeline integrity, allowing enhancement of asset performance, through the application of machine learning and automation, to predict integrity threats, and prevent leaks and failures. It provides a case study where a tool was developed, and this technique was successfully implemented across a significant number of upstream pipelines in the Cooper Basin, enabling the Santos integrity engineering team to make the most effective decisions on asset condition and to develop a data-driven asset management plan. To access the presentation click the link on the right. To read the full paper click here",The APPEA Journal,2022.0,249333760,10.1071/aj21363,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
9fae71ae9d497c5544549439d9bc6ee07c440ca2,https://www.semanticscholar.org/paper/9fae71ae9d497c5544549439d9bc6ee07c440ca2,Minimising pipeline leaks and maximising operational life by application of machine learning at Cooper Basin,"The development of technologies in the last few decades has enabled operators to collect significantly more data than previously possible. Despite availability, making data-driven decisions on asset health, and developing efficient asset management strategies, is not common. This is mainly due to challenges with compilation, and alignment of all the data into a comprehensive picture of pipeline integrity, as it consumes significant resources deploying conventional methods. A critical advantage of modern data storage, analysis and visualisation techniques is the relative ease of performing statistical assessments of integrity data. Analysis of correlated data can be equally challenging as algorithms used can be overly simplistic and inaccurate. Machine learning algorithms parse, analyse and learn from data, enabling the operators to make an educated decision. This has been extensively deployed in other industries such as finance, healthcare and supply chain management but has never been fully developed and enhanced in pipeline integrity industry until very recently. This paper provides an overview of the development in machine learning tools in pipeline integrity, allowing enhancement of asset performance, through the application of machine learning and automation, to predict integrity threats, and prevent leaks and failures. It provides a case study where a tool was developed, and this technique was successfully implemented across a significant number of upstream pipelines in the Cooper Basin, enabling the Santos integrity engineering team to make the most effective decisions on asset condition and to develop a data-driven asset management plan.",The APPEA Journal,2022.0,248765877,10.1071/aj21060,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
c1229b414c35defb0aa6027c60e562a14909c84c,https://www.semanticscholar.org/paper/c1229b414c35defb0aa6027c60e562a14909c84c,Towards Secure and Efficient Outsourcing of Machine Learning Classification,,ESORICS,2019.0,198313838,10.1007/978-3-030-29959-0_2,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
71adbc6367f7d10d3af87c1147fb57dd6925b101,https://www.semanticscholar.org/paper/71adbc6367f7d10d3af87c1147fb57dd6925b101,Prediction of cellulose sheet cutting using Machine Learning,"Cellulose is the main raw material for the production of paper. Companies that produce it present in their production line the cutting of the cellulose sheet. This failure is sporadic and has a high economic impact since it paralyzes the production line for several hours, incurring unproductive hours and a large deployment of human and financial resources. In this research, the use of Data Mining is proposed to define a machine learning algorithm that allows predicting the cutting of the cellulose sheet in a production line of a cellulose plant in Chile. The results show that by applying this technique it is possible to predict the cutting of the cellulose sheet well in advance to take corrective actions to avoid cutting and thus minimize the economic impact associated with the failure. 
Keywords: Data Mining, machine learning, cellulose, productivity. 
References 
[1]B. Ranaganth y G. Viswanath, «Application of artificial neural network for optimizing cutting variables in laser cutting of 304 grade stainless steel,» International Journal of Applied Engineering and Technology, vol. 1, nº 1, pp. 106-112, 2011. 
[2]M. Durica, J. Frnda y L. Svabova, «Decision tree based model of business failure prediction for Polish companies,» Oeconomia Copernicana, vol. 10, nº 3, pp. 453-469, 2019. 
[3]G. Köksal, İ. Batmaz y M. C. Testik, «A review of data mining applications for quality improvement in manufacturing industry,» Expert systems with Applications, vol. 38, nº 10, pp. 13448-13467, 2011. 
[4]H. Poblete y R. Vargas, «Relacion entre densidad y propiedades de tableros HDF producidos por un proceso seco,» Maderas. Ciencia y tecnología, vol. 8, nº 3, pp. 169-182, 2006. 
[5]B. Kovalerchuk y E. Vityaev, «Data mining for financial applications,» Data Mining and Knowledge Discovery Handbook, pp. 1203-1224, 2005. 
[6]U. Fayyad, G. Piatetsky-Shapiro, P. Smyth y R. Uthurusamy, «Advances in knowledge discovery and data mining,» American Association for Artificial Intelligence, 1996. 
[7]A. K. Pandey y A. K. Dubey, «Neuro fuzzy modeling of laser beam cutting process,» Applied Mechanics and Materials, vol. 110, pp. 4109-4117, 2012. 
[8]M. Németh y G. Michaľčonok, «Preparation and cluster analysis of data from the industrial production process for failure prediction,» Research Papers Faculty of Materials Science and Technology Slovak University of Technology, vol. 24, nº 39, pp. 111-116, 2016. 
[9]S. Ballı, «A data mining approach to the diagnosis of failure modes for two serial fastened sandwich composite plates,» Journal of Composite Materials, vol. 51, nº 20, pp. 2853-2862, 2017. 
[10]S. Dindarloo y E. Siami-Irdemoosa, «Data mining in mining engineering: results of classification and clustering of shovels failures data,» International Journal of Mining, Reclamation and Environment, vol. 31, nº 2, pp. 105-118, 2017. 
[11]E. e Oliveira, V. Miguéis, L. Guimarães y J. L. Borges, «Power Transformer Failure Prediction: Classification in Imbalanced Time Series,» U. Porto Journal of Engineering, vol. 3, nº 2, pp. 34-48, 2017. 
[12]A. Taghizadeh y N. Demirel, «Application of Machine Learning for Dragline Failure Prediction,» E3S Web of Conferences, vol. 15, p. 03002, 2017. 
[13]W. Chang, Z. Xu, M. You, S. Zhou, Y. Xiao y Y. Cheng, «A Bayesian Failure Prediction Network Based on Text Sequence Mining and Clustering,» Entropy, vol. 20, nº 12, p. 923, 2018. 
[14]K. Halteh, K. Kumar y A. Gepp, «Financial distress prediction of Islamic banks using tree-based stochastic techniques,» Managerial Finance, vol. 44, nº 6, pp. 759-773, 2018. 
[15]C.-H. Liu, C.-J. Lin, Y.-H. Hu y Z.-H. You, «Predicting the failure of dental implants using supervised learning techniques,» Applied Sciences, vol. 8, nº 5, p. 698, 2018. 
[16]B. Mohammed, I. Awan, H. Ugail y M. Younas, «Failure prediction using machine learning in a virtualised HPC system and application,» Cluster Computing, vol. 22, nº 2, pp. 471-485, 2019. 
[17]O. Sukhbaatar, T. Usagawa y L. Choimaa, «An artificial neural network based early prediction of failure-prone students in blended learning course,» International Journal of Emerging Technologies in Learning (iJET)}, vol. 14, nº 19, pp. 77-92, 2019. 
[18]Z. Wang, W. Zhao y X. Hu, «Analysis of prediction model of failure depth of mine floor based on fuzzy neural network,» Geotechnical and Geological Engineering, vol. 37, nº 1, pp. 71-76, 2019. 
[19]V. S. Gujre y R. Anand, «Machine learning algorithms for failure prediction and yield improvement during electric resistance welded tube manufacturing,» Journal of Experimental \& Theoretical Artificial Intelligence, vol. 32, nº 4, pp. 601-622, 2020. 
[20]P. du Jardin, «Forecasting corporate failure using ensemble of self-organizing neural networks,» European Journal of Operational Research, vol. 288, nº 3, pp. 869-885, 2021. 
[21]R. Brachman y T. Anand, «The process of knowledge discovery in databases,» Advances in knowledge discovery and data mining, pp. 37-57, 1996. 
[22]W. Frawley, G. Piatetsky-Shapiro y C. Matheus, «Knowledge discovery in databases: An overview,» AI magazine, vol. 13, nº 3, p. 57, 1992. 
[23]F. H. Troncoso Espinosa y J. V. Ruiz Tapia, «Predicción de fuga de clientes en una empresa de distribución de gas natural mediante el uso de minería de datos,» Universidad Ciencia y Tecnología, vol. 24, nº 106, pp. 79-87, 2020. 
[24]F. H. Troncoso, «Prediction of Recidivism in Thefts and Burglaries Using Machine Learning,» Indian Journal of Science and Technology, vol. 13, nº 6, pp. 696-711, March 2020. 
[25]M. Kantardzic, Data mining: concepts, models, methods, and algorithms, John Wiley & Sons, 2011. 
[26]F. H. Troncoso Espinosa, P. G. Fuentes Figueroa y I. R. Belmar Arriagada, «Predicción de fraudes en el consumo de agua potable mediante el uso de Minería de Datos,» Universidad Ciencia y Tecnología, vol. 24, nº 104, pp. 58-66, 2020. 
[27]C. Romero y S. Ventura, «Data mining in education,» Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 3, nº 1, pp. 12-27, 2013. 
[28]D. Larose y C. Larose, Discovering knowledge in data: an introduction to data mining, John Wiley & Sons, 2014.",Universidad Ciencia y Tecnología,2021.0,241913354,10.47460/uct.v25i110.481,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
d64c3b866d605f131611666191899d9aabf5cf59,https://www.semanticscholar.org/paper/d64c3b866d605f131611666191899d9aabf5cf59,"ISTHMUS: Secure, Scalable, Real-time and Robust Machine Learning Platform for Healthcare","In recent times, machine learning (ML) and artificial intelligence (AI) based systems have evolved and scaled across different industries such as finance, retail, insurance, energy utilities, etc. Among other things, they have been used to predict patterns of customer behavior, to generate pricing models, and to predict the return on investments. But the successes in deploying machine learning models at scale in those industries have not translated into the healthcare setting. There are multiple reasons why integrating ML models into healthcare has not been widely successful, but from a technical perspective, general-purpose commercial machine learning platforms are not a good fit for healthcare due to complexities in handling data quality issues, mandates to demonstrate clinical relevance, and a lack of ability to monitor performance in a highly regulated environment with stringent security and privacy needs. In this paper, we describe Isthmus, a turnkey, cloud-based platform which addresses the challenges above and reduces time to market for operationalizing ML/AI in healthcare. Towards the end, we describe three case studies which shed light on Isthmus capabilities. These include (1) supporting an end-to-end lifecycle of a model which predicts trauma survivability at hospital trauma centers, (2) bringing in and harmonizing data from disparate sources to create a community data platform for inferring population as well as patient level insights for Social Determinants of Health (SDoH), and (3) ingesting live-streaming data from various IoT sensors to build models, which can leverage real-time and longitudinal information to make advanced time-sensitive predictions.",ArXiv,2019.0,203593487,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
3f76444a7162dc16f261cf011a766d063f24bd90,https://www.semanticscholar.org/paper/3f76444a7162dc16f261cf011a766d063f24bd90,The deployment of Machine Learning in eBanking: A Survey.,"Thanks to the machine learning algorithms revolution, several organizations will be able to transform their services, automate functions and predict their customer’s behaviors. The fact that digital has begun a huge change in the world of finance such as the traditional bank with its physical breaches and advisers; the digital bank is one of the financial organizations integrates machine learning into its services. In order to properly guide future research and development, it will be extremely beneficial to carry out an integral and up-to-date study, focusing on banking fields integrating (or able to integrate) the machine learning techniques. This paper will allow managers and developers to carefully evaluate and embrace machine learning techniques to solve e-baking issues.",2019 Third International Conference on Intelligent Computing in Data Sciences (ICDS),2019.0,209495935,10.1109/icds47004.2019.8942379,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
65925b54d2aa25ebda5a7fe76603fb54ca70912a,https://www.semanticscholar.org/paper/65925b54d2aa25ebda5a7fe76603fb54ca70912a,Stock Market Prediction Using Facebook Prophet Machine Learning Techniques,"The prediction of a stock market direction may serve as an early recommendation system for short term investors and as an early financial distress warning system for long term shareholders. Forecasting accuracy is the most important factor in selecting any forecasting methods. Research efforts in improving the accuracy of forecasting models are increasing since the last decade. The appropriate stock selections those are suitable for investment is a very difficult task. The key factor for each investor is to earn maximum profits on their investments. Stock is an unpredictable curve. Prediction in stock market is covered with the complexity and instability. The main aim for the persuasion of the topic is to predict the stability in the future market stocks. Many researchers have performed their research on the movement of future market evolution. Stock consists of fluctuating data which makes data as an integral source of efficiency. Impact on the same chances the efficiency of the prediction. In the recent trend of Stock Market Prediction Technologies machine learning has integrated itself in the picture for deployment and prediction of training sets and data models. Machine Learning employs different predictive models and algorithms to predict and automate things of requirement. Here while developing this system we are focusing on the use of additive regression model to predict stock values. In this paper, we investigate the predictability of financial movement with Facebook prophet (additive regression model).These methods are applied on multiple years of data retrieved from Yahoo Finance. The results will be used to analyze the stock prices and their prediction in depth in future research efforts. In this project we build a stock prediction web app in Python using streamlet, Yahoo finance, and Facebook Prophet.",,2021.0,244951260,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
327f73d6cf239c568afc85796642f54b52941158,https://www.semanticscholar.org/paper/327f73d6cf239c568afc85796642f54b52941158,Dynamic filtering of malicious records using machine learning integrated databases,"Machine Learning, Deep Learning and Predictive Analytics are the key domains of research in assorted domains of implementations including engineering, finance, economics, real time imaging and many others. The researchers are working on different tools and technologies including open source and own developed frameworks so that the higher degree of accuracy can be achieved. The research reports from Market Research News US predicted that the global market size of machine learning based implementations will exceed 20 billion dollars in year 2024. Most of the government and social services are nowadays in process to be deployed with the advanced technologies of machine learning and deep learning so that the minimum error factor can be there. The key players in the industry include; Google, Facebook, IBM Watson, Baidu, Apple, Microsoft, Wipro, Amazon, Intel, Nuance and many others which are working on the advanced algorithms and implementation perspectives of machine learning.",,2019.0,210968367,10.21533/pen.v7i4.898,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
31f3dbc28dd7158e95d76bb0757030295c9f769d,https://www.semanticscholar.org/paper/31f3dbc28dd7158e95d76bb0757030295c9f769d,SoK: Privacy-Preserving Computation Techniques for Deep Learning,"Abstract Deep Learning (DL) is a powerful solution for complex problems in many disciplines such as finance, medical research, or social sciences. Due to the high computational cost of DL algorithms, data scientists often rely upon Machine Learning as a Service (MLaaS) to outsource the computation onto third-party servers. However, outsourcing the computation raises privacy concerns when dealing with sensitive information, e.g., health or financial records. Also, privacy regulations like the European GDPR limit the collection, distribution, and use of such sensitive data. Recent advances in privacy-preserving computation techniques (i.e., Homomorphic Encryption and Secure Multiparty Computation) have enabled DL training and inference over protected data. However, these techniques are still immature and difficult to deploy in practical scenarios. In this work, we review the evolution of the adaptation of privacy-preserving computation techniques onto DL, to understand the gap between research proposals and practical applications. We highlight the relative advantages and disadvantages, considering aspects such as efficiency shortcomings, reproducibility issues due to the lack of standard tools and programming interfaces, or lack of integration with DL frameworks commonly used by the data science community.",Proc. Priv. Enhancing Technol.,2021.0,236213847,10.2478/popets-2021-0064,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
28943e4f8380fc989b2fd6067c9b366f4cdd48cc,https://www.semanticscholar.org/paper/28943e4f8380fc989b2fd6067c9b366f4cdd48cc,Key Aggregation Cryptosystem and Double Encryption Method for Cloud-Based Intelligent Machine Learning Techniques-Based Health Monitoring Systems,"Cloud technology is a business strategy that aims to provide the necessary material to customers depending on their needs. Individuals and cloud businesses alike have embraced the cloud storage service, which has become the most widely used service. The industries outsource their data to cloud storage space to relieve themselves of the load of dealing with redundant data contents. This must be protected to prevent the theft of personal belongings, and privacy must be improved as well. Different research projects have been suggested to ensure the safe management of the information included within the data content. The security of current research projects, on the contrary, still needs improvement. As a result, this method has been suggested to address the security concerns associated with cloud computing. The primary goal of this study effort is to offer a safe environment for cloud users while also increasing the profit of cloud resource providers by managing and securely delivering data contents to the cloud users. The bulk of sectors, including business, finance, military, and healthcare industry, do not store data in cloud-based storage systems. This technique is used to attract these kinds of customers. Increasing public acceptance, medical researchers are drawn to cloud computing because it allows them to store their study material in a centralized location and distribute and access it in a more flexible manner. They were collected from numerous individuals who were being evaluated for medical care at the time. Scalable and enhanced key aggregate cryptosystem is a protected data protection method that provides highly effective security in the healthcare industry. When parties interested in a dispute disagree on the outflow of sensitive information, this technique manages the disputes and ensures the data security deployment of a cloud-based intelligent health monitoring system for the parties involved. The encrypted data structure of medical and healthcare prescriptions is recorded as they move through the hands of patients and healthcare facilities, according to the technique recommended. The double encryption approach is used in order to raise the overall degree of security. An encryption class is created by referring to the Ciphertext ID during the encryption procedure. The keyholder is a master secret key that facilitates in the recovery of the secret keys of various monsters and creatures by acting as a conduit between them. It is transferred and stored as a single aggregate for the benefit of the patient or customer in order to make decryption more convenient and efficient. A safe connection between cloud-based intelligent health monitoring systems and healthcare organizations and their patients may be established via the use of a key aggregation cryptosystem and a double encryption approach, according to the researchers. Because of this, when compared to earlier techniques, the findings reveal that the research methodology provides high levels of security in terms of confidentiality and integrity, in addition to excellent scalability.",Computational intelligence and neuroscience,2022.0,248328758,10.1155/2022/3767912,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
c54d8b94e3290ae5e02ffcfdc407e6d519ceec2f,https://www.semanticscholar.org/paper/c54d8b94e3290ae5e02ffcfdc407e6d519ceec2f,Medical Healthcare System with Hybrid Block based Predictive models for Quality preserving in Medical Images using Machine Learning Techniques,"Cloud technology is a business strategy that aims to provide the necessary material to customers depending on their needs. Individuals and cloud businesses alike have embraced the cloud storage service, which has become the most widely used service. The industries outsource their data to cloud storage space to relieve themselves of a load of dealing with redundant data contents. This must be protected to prevent the theft of personal belongings, and privacy must be improved as well. Different research projects have been suggested to ensure the safe management of the information included within the data content. The security of current research projects, on the other hand, still needs improvement. As a result, this method has been suggested to address the security concerns associated with cloud computing. The primary goal of this study effort is to offer a safe environment for cloud users while also increasing the profit of cloud resource providers by managing and securely delivering data contents to the cloud users. The bulk of sectors, including business, finance, the military, and the healthcare industry, do not store data in cloud-based storage systems. This technique is used to attract these kinds of customers. Increasing public acceptance Medical researchers are drawn to cloud computing because it allows them to store their study material in a centralized location and distribute and access it in a more flexible manner. They were collected from numerous individuals who were being evaluated for medical care at the time. Scalable and Enhanced Key Aggregate Cryptosystem is a protected data protection method that provides highly effective security in the health care industry. This approach handles disagreements in the outflow of sensitive information and guarantees the data security deployment of a Cloud-based Intelligent Health Monitoring system for the parties involved in the dispute. Using the suggested method, the encrypted data format of medical and health-care prescriptions is recorded as it passes through the hands of patients and healthcare institutions. To increase the level of security, the double encryption method is used. During the encryption process, the Ciphertext ID is referred to as a class. The keyholder is a master secret key that aids in the retrieval of the secret keys of different kinds of monsters and creatures. The extracted key is transmitted and kept as a single aggregate for the benefit of the patient or client to facilitate decryption. Between the use of a key aggregation cryptosystem and double encryption method, the Cloud-based Intelligent Health Monitoring systems may establish a secure link with Healthcare Organizations and patients. As a result, when compared to prior methods, the results demonstrate that the study methodology achieves high levels of security in terms of confidentiality and integrity, as well as great scalability.",2022 International Conference on Advanced Computing Technologies and Applications (ICACTA),2022.0,248183008,10.1109/ICACTA54488.2022.9753355,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
e6013556f4046c73618e17439c6b07ad7bbc2277,https://www.semanticscholar.org/paper/e6013556f4046c73618e17439c6b07ad7bbc2277,Machine Learning IP Protection,"Machine learning, specifically deep learning is becoming a key technology component in application domains such as identity management, finance, automotive, and healthcare, to name a few. Proprietary machine learning models - Machine Learning IP - are developed and deployed at the network edge, end devices and in the cloud, to maximize user experience. With the proliferation of applications embedding Machine Learning IPs, machine learning models and hyper-parameters become attractive to attackers, and require protection. Major players in the semiconductor industry provide mechanisms on device to protect the IP at rest and during execution from being copied, altered, reverse engineered, and abused by attackers. In this work we explore system security architecture mechanisms and their applications to Machine Learning IP protection.",2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),2018.0,53222562,10.1145/3240765.3270589,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
26937db6917952a9c13bfe152eddd7defc08e695,https://www.semanticscholar.org/paper/26937db6917952a9c13bfe152eddd7defc08e695,Guest Editorial: Special Issue on Machine Learning Implementations,,J. Signal Process. Syst.,2019.0,59158947,10.1007/s11265-018-1432-1,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
5994ea254d8e60e9b240cf0c3824b3e5f40df85a,https://www.semanticscholar.org/paper/5994ea254d8e60e9b240cf0c3824b3e5f40df85a,Machine Learning for Indoor Localization Using Mobile Phone-Based Sensors,"In this paper we investigate the problem of localizing a mobile device based on readings from its embedded sensors utilizing machine learning methodologies. We consider a realworld environment, collect a large dataset of 3110 datapoints, and examine the performance of a substantial number of machine learning algorithms in localizing a mobile device. We have found algorithms that give a mean error as accurate as 0.76 meters, outperforming other indoor localization systems reported in the literature. We also propose a hybrid instance-based approach that results in a speed increase by a factor of ten with no loss of accuracy in a live deployment over standard instance-based methods, allowing for fast and accurate localization. Further, we determine how smaller datasets collected with less density affect accuracy of localization, important for use in real-world environments. Finally, we demonstrate that these approaches are appropriate for real-world deployment by evaluating their performance in an online, in-motion experiment.",ArXiv,2015.0,9638379,10.1109/CCNC.2016.7444919,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
fbbee5da12c0e0f15813254b9ffca18e339d3546,https://www.semanticscholar.org/paper/fbbee5da12c0e0f15813254b9ffca18e339d3546,10 - Machine learning for future intelligent air quality networks,"During the last few years, machine learning emerged as a very effective tool for data analysis and sematic value extraction from the large amount of data generated from deployed chemical multisensors devices. Many works have now highlighted the potential impact on multisensor device calibration, drift counteraction, data assimilation, optimal deployment of these classes of algorithms. Unlike 5 years ago, the huge amount of available data make possible to confirm this potential on realworld long-term deployments. This work analyze the literature produced by EuNetAir partners extracting the lessons cooperatively learnt about their impact and propose a novel architecture for future intelligent air quality networks based on the machine learning emerging paradigm.",,2016.0,54652101,10.5162/6EuNetAir2016/10,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
c6bde5c3b6b5bd9cc4c9de2be3a3c4ed48fe4e86,https://www.semanticscholar.org/paper/c6bde5c3b6b5bd9cc4c9de2be3a3c4ed48fe4e86,"Tambe , Developing the Science and Applications of Security Games : Machine Learning , Uncertainty and Preference Elicitation in Game Theory for Security","Having successfully founded the research area of security games, which has led to real-world applications in scheduling the deployment of limited resources (patrols, checkpoints, inspections, etc.), we now provide fundamental advances by incorporating machine learning to enhance realworld security applications, new models of opportunistic security games, robust methods for handling uncertainty, and novel techniques for preference elicitation techniques.",,2015.0,16965186,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
f6dab84c2c00ab92d8ee9d9359d7e530512114f9,https://www.semanticscholar.org/paper/f6dab84c2c00ab92d8ee9d9359d7e530512114f9,"Finance Big Data: Management, Analysis, and Applications","Big Data is an emerging paradigm in almost all industries. Finance big data (FBD) is becoming one of the most promising areas of management and governance in the financial sector. It is significantly changing business models in financial companies. Many researchers argue that Big Data is fueling the transformation of finance and business at-large in the ways that we cannot as yet assess. A new research area is evolving to study quantitative models and econometric approaches for financial studies that can bridge the gap between empirical finance research and data science. In this fascinating area, experts and scientists can propose novel finance business models by using the Big Data methods, present sophisticated methods for risk control with machine learning tools, provide visualization tools for financial markets analysis, create new finance sentiment indexes by mining public feelings from the massive textual data from social networks, and deploy the information-based tools in other creative ways. Due to the 4V characteristics of Big Data—volume (large data scale), velocity (real-time data streaming), variety (different data formats), and veracity (data uncertainty)—a long list of challenges for FBD management, analytics, and applications exists. These challenges include (1) to organize and manage FBD in effective and efficient ways; (2) to find novel business models from FBD analytics; (3) to handle traditional finance issues like high-frequency trading, sentiments, credit risk, financial analysis, risk management and regulation, and others, in creative Big Data–driven ways; (4) to integrate the variety of heterogeneous data from different sources; and (5) to ensure the security and safety of finance systems and to protect the individual privacy in view of the availability of Big Data. To meet these challenges, we need fundamental research on both data analytics technology and finance business. This special issue, “Finance Big Data: Management, Analysis, and Applications,” of International Journal of Electronic Commerce, is motivated by the need to meet the challenges of the fast development of finance big data. The papers brought together in this special issue highlight research efforts focused on the development of methods, tools, and techniques for the handling of various aspects of FBD from academia and industries. Viktor Manahov and Hanxiong Zhang, in “Forecasting Financial Markets Using High-Frequency Trading Data: Examination with Strongly Typed Genetic Programming,” develop an artificial futures market populated with high-frequency (HF) traders and institutional traders using Strongly Typed Genetic Programming trading algorithm. The authors simulate real-life futures trading at the millisecond time frame by applying Strongly Typed",Int. J. Electron. Commer.,2019.0,59540861,10.1080/10864415.2018.1512270,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
362edc0f31ca39ad58d473a3872715a04fe516ed,https://www.semanticscholar.org/paper/362edc0f31ca39ad58d473a3872715a04fe516ed,Fair Policy Learning,"Ensuring machine learning algorithms deployed in the real world do not result in unexpected unfairness or social implications is becoming increasingly important. However, there exists a clear gap in literature for a measure of fairness that can detect discrimination against multiple sensitive attributes while also handling continuous or discrete outcomes. In this thesis, we propose a fairness measure, Fair-COCCO, based on the conditional cross-covariance operator on reproducing kernel Hilbert Spaces. This novel method generalise to the majority of existing fairness notions and naturally extends to settings with continuous outcomes and multidimensional sensitive attributes. Additionally, we demonstrate how the proposed measure can be readily implemented in stochastic gradient optimisation for fair policy learning in supervised learning settings. Empirical evaluations of Fair-COCCO on synthetic and realworld experiments reveal favourable comparisons to state-of-the-art techniques in balancing predictive power and fairness. We also see much potential in applying machine learning to analyse fairness in observed behaviour, especially in complex and high-dimensional real-world environments. To that end, we propose the first known definition of fairness for sequences of decisions and showcase how Fair-COCCO can be applied to quantify fairness in these problems. Building off these definitions, we turn to learning fair policies in real-world conditions, where learning is constrained to be performed offline. We propose Fair-PoLe, a novel inverse reinforcement learning that operates completely offline and is computationally efficient and functionally expressive when compared to existing methods. We illustrate the potential for Fair-PoLe to learn policies that balance imitation of expert policies with fair outcomes on the challenging problem of sepsis treatment.",,2021.0,247055707,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
f7fe5aee32641a18e1bb906d818af6e4a4bdcfb6,https://www.semanticscholar.org/paper/f7fe5aee32641a18e1bb906d818af6e4a4bdcfb6,Toward Scalable Artificial Intelligence in Finance,"Innovation in Artificial Intelligence (AI) continues to produce a wealth of techniques, mostly coming from the inductive form of AI also known as Machine Learning (ML). The majority of ML algorithms is industry-neutral and business process agnostic. ML innovation is propelled by publicly available research, which gets harvested into Open Source for wide distribution through software and Cloud vendors.Ongoing AI technology work creates an immense source of assets for data-driven modeling, delivered as software libraries. However, the application of these assets for data monetization in finance does not happen with nearly comparable success or speed. The latter challenge is commonly known as the ""scalability problem of AI"". As new techniques continue to grow vigorously, the investment from large finance institutions to cost-effectively produce applications for a variety of lines-of-business (LoBs) and business processes will increase. The availability of ML capabilities on Public Cloud is a way for enterprises to increase productivity by benefiting from the best AI assets available from providers and startups. But data is constrained in terms of location, access and use in most finance competences by either laws or internal Governance, Risk and Compliance (GRC) rules. Legal limitations include, and go beyond, Privacy Acts, impacting non-retail processes where AI techniques must be explained in layperson language to decision-makers and regulators before field deployment. The latter is not yet achieved satisfactorily. Lastly, a large percentage of AI projects fail, in part due to unsuitable ML modeling for analytics and forecasting problems in finance. The variety and complexity of human behavior present in most finance processes calls for understanding AI at a level of cognitive depth that has no precedent in other industries. It is imperative that AI be approached so that finance competence and functional specificity are embedded a-priori into ML techniques and not as use-case afterthoughts. For acceleration of AI assessments, it is critical that ML techniques available in software implement models readily aligned to finance problems.This paper presents an approach to building an Architecture for Artificial Intelligence (AI) in Finance by focusing on analytics and forecasting for business-to-business operations. This AI Architecture hinges on three axes and their interplay: Design Dimensions, Modeling Building-Blocks and Work-Practice. The goal is to support finance practitioners navigate the plethora of AI options more effectively and accelerate data monetization. While ML techniques in data analytics and forecasting apply to many scenarios, this paper focuses on selected competences in Banking, Financial Markets and Chief Finance Officer (CFO) operations.The architecture and method introduced in this paper is a first step toward a service practice. We harvest from our work carried out in banks, asset management firms and CFO lines-of-business as well as R&D experiences in new finance technologies for over one decade. As with any other architecture and deployment methodology, this work requires further harvesting, more information technology tools and sharing experiences across practitioners. It is hoped that finance organizations could adopt these new capabilities in their own Centers of Excellence or other internal organizations leading data-driven transformation and monetization across the firm.",2021 IEEE International Conference on Services Computing (SCC),2021.0,244151282,10.1109/scc53864.2021.00067,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
e545b981c54b55e01bf52e0c82d8eb589841302d,https://www.semanticscholar.org/paper/e545b981c54b55e01bf52e0c82d8eb589841302d,Systematic trading : calibration advances through machine learning,"Systematic trading in finance uses computer models to define trade goals, risk controls and rules that can execute trade orders in a methodical way. This thesis investigates how performance in systematic trading can be crucially enhanced by both i) persistently reducing the bid-offer spread quoted by the trader through optimized and realistically backtested strategies and ii) improving the out-of-sample robustness of the strategy selected through the injection of theory into the typically data-driven calibration processes. While doing so it brings to the foreground sound scientific reasons that, for the first time to my knowledge, technically underpin popular academic observations about the recent nature of the financial markets. The thesis conducts consecutive experiments across strategies within the three important building blocks of systematic trading: a) execution, b) quoting and c) risk-reward allowing me to progressively generate more complex and accurate backtested scenarios as recently demanded in the literature (Cahan et al. (2010)). The three experiments conducted are: 1. Execution: an execution model based on support vector machines. The first experiment is deployed to improve the realism of the other two. It analyses a popular model of execution: the volume weighted average price (VWAP). The VWAP algorithm targets to split the size of an order along the trading session according to the expected intraday volume's profile since the activity in the markets typically resembles convex seasonality – with more activity around the open and the closing auctions than along the rest of the day. In doing so, the main challenge is to provide the model with a reasonable expected profile. After proving in my data sample that two simple static approaches to the profile overcome the PCA-ARMA from Bialkowski et al. (2008) (a popular two-fold model composed by a dynamic component around an unsupervised learning structure) a further combination of both through an index based on supervised learning is proposed. The Sample Sensitivity Index hence successfully allows estimating the expected volume's profile more accurately by selecting those ranges of time where the model shall be less sensitive to past data through the identification of patterns via support vector machines. Only once the intraday execution risk has been defined can the quoting policy of a mid-frequency (in general, up to a week) hedging strategy be accurately analysed. 2. Quoting: a quoting model built upon particle swarm optimization. The second experiment analyses for the first time to my knowledge how to achieve the disruptive 50% bid-offer spread discount observed in Menkveld (2013) without increasing the risk profile of a trading agent. The experiment depends crucially on a series of variables of which market impact and slippage are typically the most difficult to estimate. By adapting the market impact model in Almgren et al. (2005) to the VWAP developed in the previous experiment and by estimating its slippage through its errors' distribution a framework within which the bid-offer spread can be assessed is generated. First, a full-replication spread, (that set out following the strict definition of a product in order to hedge it completely) is calculated and fixed as a benchmark. Then, by allowing benefiting from a lower market impact at the cost of assuming deviation risk (tracking error and tail risk) a non-full-replication spread is calibrated through particle swarm optimization (PSO) as in Diez et al. (2012) and compared with the benchmark. Finally, it is shown that the latter can reach a discount of a 50% with respect to the benchmark if a certain number of trades is granted. This typically occurs on the most liquid securities. This result not only underpins Menkveld's observations but also points out that there is room for further reductions. When seeking additional performance, once the quoting policy has been defined, a further layer with a calibrated risk-reward policy shall be deployed. 3. Risk-Reward: a calibration model defined within a Q-learning framework. The third experiment analyses how the calibration process of a risk-reward policy can be enhanced to achieve a more robust out-of-sample performance – a cornerstone in quantitative trading. It successfully gives a response to the literature that recently focusses on the detrimental role of overfitting (Bailey et al. (2013a)). The experiment was motivated by the assumption that the techniques underpinned by financial theory shall show a better behaviour (a lower deviation between in-sample and out-of-sample performance) than the classical data-driven only processes. As such, both approaches are compared within a framework of active trading upon a novel indicator. The indicator, called the Expectations' Shift, is rooted on the expectations of the markets' evolution embedded in the dynamics of the prices. The crucial challenge of the experiment is the injection of theory within the calibration process. This is achieved through the usage of reinforcement learning (RL). RL is an area of ML inspired by behaviourist psychology concerned with how software agents take decisions in an specific environment incentivised by a policy of rewards. By analysing the Q-learning matrix that collects the set of state/actions learnt by the agent within the environment, defined by each combination of parameters considered within the calibration universe, the rationale that an autonomous agent would have learnt in terms of risk management can be generated. Finally, by then selecting the combination of parameters whose attached rationale is closest to that of the portfolio manager a data-driven solution that converges to the theory-driven solution can be found and this is shown to successfully outperform out-of-sample the classical approaches followed in Finance. The thesis contributes to science by addressing what techniques could underpin recent academic findings about the nature of the trading industry for which a scientific explanation was not yet given: • A novel agent-based approach that allows for a robust out-of-sampkle performance by crucially providing the trader with a way to inject financial insights into the generally data-driven only calibration processes. It this way benefits from surpassing the generic model limitations present in the literature (Bailey et al. (2013b), Schorfheid and Wolpin (2012), Van Belle and Kerr (2012) or Weiss and Kulikowski (1991)) by finding a point where theory-driven patterns (the trader's priors tend to enhance out-of-sample robustness) merge with data-driven ones (those that allow to exploit latent information). • The provision of a technique that, to the best of my knowledge, explains for the first time how to reduce the bid-offer spread quoted by a traditional trader without modifying her risk appetite. A reduction not previously addressed in the literature in spite of the fact that the increasing regulation against the assumption of risk by market makers (e.g. Dodd–Frank Wall Street Reform and Consumer Protection Act) does yet coincide with the aggressive discounts observed by Menkveld (2013). As a result, this thesis could further contribute to science by serving as a framework to conduct future analyses in the context of systematic trading. • The completion of a mid-frequency trading experiment with high frequency execution information. It is shown how the latter can have a significant effect on the former not only through the erosion of its performance but, more subtly, by changing its entire strategic design (both, optimal composition and parameterization). This tends to be highly disregarded by the financial literature. More importantly, the methodologies disclosed herein have been crucial to underpin the setup of a new unit in the industry, BBVA's Global Strategies & Data Science. This disruptive, global and cross-asset team gives an enhanced role to science by successfully becoming the main responsible for the risk management of the Bank's strategies both in electronic trading and electronic commerce. Other contributions include: the provision of a novel risk measure (flowVaR); the proposal of a novel trading indicator (Expectations’ Shift); and the definition of a novel index that allows to improve the estimation of the intraday volume’s profile (Sample Sensitivity Index).",,2015.0,7848139,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
d5a30dc21233a9ee491f6d7050b59734f2f34901,https://www.semanticscholar.org/paper/d5a30dc21233a9ee491f6d7050b59734f2f34901,Model-Based Deep Reinforcement Learning for Autonomous Racing,"Reinforcement learning (RL) is currently one of the most active machine learning research fields. RL algorithms have been successfully deployed ubiquitously in many real-world application domains, such as autonomous vehicles, intelligent production sites, and finance. Despite the many advances made in recent years, there are still fundamental challenges that need to be addressed before RL can be reliably applied in industrial applications. One of these problems is the sheer amount of training data that is needed to train deep RL agents. Model-based RL is a branch of RL algorithms that learn a model of the agent or its environment which is then leveraged to generate new training data or to plan ahead. Model-based approaches are expected to reduce the required amount of training data to be sampled from an environment, down to a level that allows RL algorithms to be trained in environments where it is hard to generate sufficient data. The goal of this work is to investigate the advantages that model-based RL algorithms bring. To this end, we adapt an existing model-based RL algorithm and compare its performance with that of common, model-free RL algorithms that mark the current State-of-the-Art. The application domain in which we conduct the experiments is in the field of autonomous racing. In our experiments, agents are trained to minimize lap times in time-trial races. The experiments aim to evaluate algorithms, that were trained in simulation, with respect to their ability to be deployed in the real world. We also compare the flexibility of the algorithms to produce comparable results on other, unseen race tracks. Last but not least, we also investigate the training behavior of the different algorithms. The experiments are performed both in a simulation environment, implemented specifically for this work, and on a prototyping platform based on a small remote-controlled car.",,2021.0,238052973,10.34726/HSS.2021.86588,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
6492b573ac777a05642f0623e7a70c8d8695cda6,https://www.semanticscholar.org/paper/6492b573ac777a05642f0623e7a70c8d8695cda6,Gaussian Process Regression In Computational Finance,"Machine learning can be deployed not only in order to solve non trivial problems, but also faster than traditional implemented solutions. We illustrate several classical problems in the field of computational finance where it is possible to fit, with Gaussian process regressions, complex functions under and beyond the Black-Scholes model with high accuracy. The results from the regressions, in our examples, show speed-ups of several magnitudes compared to the classical implementations while keeping a precision well within the acceptable limits for practical use. The concrete examples consist in financial Greeks fitting, summarizing implied volatility surfaces, as well as estimating vanilla and exotic options while reducing the computation time.",,2020.0,211240387,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
7e2157572a2a5b643cace3f936d5f56116d70ac4,https://www.semanticscholar.org/paper/7e2157572a2a5b643cace3f936d5f56116d70ac4,"Modern Computational Finance: AAD and Parallel Simulations, parts I and II","Parts I and II out of three of the book Modern Computational Finance: AAD and Parallel Simulations, by Antoine Savine, John Wiley and Sons, 2018, complimentarily available on SSRN only in December 2020. 

Part I teaches necessary C++ foundations with a focus on parallel computing. Part II summarizes the theory of financial Derivatives and develops serial and parallel Monte-Carlo pricing and risk management engines.

Part III, not included in this preview, discusses and develops the critical adjoint differentiation (AD) technology, its professional implementation in C++ and its deployment in risk management platforms.

In recent years, AD revolutionized the field of quantitative finance. It brought us real-time risk reports and instantaneous calibration. It also enabled extremely promising new directions of research. For instance, the Risk article Differential Machine Learning by Brian Huge and Antoine Savine (also available on arXiv and SSRN) leverages pathwise gradients computed with AD to train a novel breed of deep learning models to effectively approximate pricing and risk functions of arbitrary financial products. 

Antoine Savine implemented AD in production at Danske Bank with his colleagues of the Quantitative Research department. He also teaches AD at Copenhagen University in the context of his graduate course on Computational Finance and Machine Learning in Finance. The book gives an exhaustive and pedagogical account of AD, its implementation in C++ and its deployment for the risk management of financial Derivatives.

Parts I and II are necessary pre-requisites to make the most of the critical part III.",,2020.0,229487988,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
6d0adac188152fbaa45a88ba4da788926ed8144a,https://www.semanticscholar.org/paper/6d0adac188152fbaa45a88ba4da788926ed8144a,Reinforcement Learning in Practice: Opportunities and Challenges,"This article is a gentle discussion about the ﬁeld of reinforcement learning in practice, about opportunities and challenges, touching a broad range of topics, with perspectives and without technical details. The article is based on both historical and recent research papers, surveys, tutorials, talks, blogs, books, (panel) discussions, and workshops/conferences. Various groups of readers, like researchers, engineers, students, managers, investors, ofﬁcers, and people wanting to know more about the ﬁeld, may ﬁnd the article interesting. In this article, we ﬁrst give a brief introduction to reinforcement learning (RL), and its relationship with deep learning, machine learning and AI. Then we discuss opportunities of RL, in particular, products and services, games, bandits, recommender systems, robotics, transportation, ﬁnance and economics, healthcare, education, combinatorial optimization, computer systems, and science and engineering. Then we discuss challenges, in particular, 1) foundation, 2) representation, 3) reward, 4) exploration, 5) model, simulation, planning, and benchmarks, 6) off-policy/ofﬂine learning, 7) learning to learn a.k.a. meta-learning, 8) explainability and interpretability, 9) constraints, 10) software development and deployment, 11) business perspectives, and 12) more challenges. We conclude with a discussion, attempting to answer: “Why has RL not been widely adopted in practice yet?” and “When is RL helpful?”. for discussing transfer domain randomization, knowledge distillation, imitation learning, meta-learning, robust RL.",,2022.0,248366754,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
8c8868d75f5fc7a055fdbc8610ab20b0a4304829,https://www.semanticscholar.org/paper/8c8868d75f5fc7a055fdbc8610ab20b0a4304829,Deep Reinforcement Learning: Opportunities and Challenges,"This article is a gentle discussion about the field of reinforcement learning for real life, about opportunities and challenges, with perspectives and without technical details, touching a broad range of topics. The article is based on both historical and recent research papers, surveys, tutorials, talks, blogs, and books. Various groups of readers, like researchers, engineers, students, managers, investors, officers, and people wanting to know more about the field, may find the article interesting. In this article, we first give a brief introduction to reinforcement learning (RL), and its relationship with deep learning, machine learning and AI. Then we discuss opportunities of RL, in particular, applications in products and services, games, recommender systems, robotics, transportation, economics and finance, healthcare, education, combinatorial optimization, computer systems, and science and engineering. The we discuss challenges, in particular, 1) foundation, 2) representation, 3) reward, 4) model, simulation, planning, and benchmarks, 5) learning to learn a.k.a. meta-learning, 6) off-policy/offline learning, 7) software development and deployment, 8) business perspectives, and 9) more challenges. We conclude with a discussion, attempting to answer: “Why has RL not been widely adopted in practice yet?” and “When is RL helpful?”.",ArXiv,2022.0,247058605,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
1dbd7e355f9e478c9aabe14b2bb8f4d3daad8a1d,https://www.semanticscholar.org/paper/1dbd7e355f9e478c9aabe14b2bb8f4d3daad8a1d,An Isolated Data Island Benchmark Suite for Federated Learning,"Federated learning (FL) is a new machine learning paradigm, the goal of which is to build a machine learning model based on data sets distributed on multiple devices--so called Isolated Data Island--while keeping their data secure and private. Most existing work manually splits commonly-used public datasets into partitions to simulate real-world Isolated Data Island while failing to capture the intrinsic characteristics of real-world domain data, like medicine, finance or AIoT. To bridge this huge gap, this paper presents and characterizes an Isolated Data Island benchmark suite, named FLBench, for benchmarking federated learning algorithms. FLBench contains three domains: medical, financial and AIoT. By configuring various domains, FLBench is qualified for evaluating the important research aspects of federated learning, and hence become a promising platform for developing novel federated learning algorithms. Finally, FLBench is fully open-sourced and in fast-evolution. We package it as an automated deployment tool. The benchmark suite will be publicly available from this http URL.",ArXiv,2020.0,221139951,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
aa7ed7e04182d0588bb53376e472afb59e236c36,https://www.semanticscholar.org/paper/aa7ed7e04182d0588bb53376e472afb59e236c36,Rethinking Explainability as a Dialogue: A Practitioner's Perspective,"As practitioners increasingly deploy machine learning models in critical domains such as healthcare, finance, and policy, it becomes vital to ensure that domain experts function effectively alongside these models. Explainability is one way to bridge the gap between human decision-makers and machine learning models. However, most of the existing work on explainability focuses on one-off, static explanations like feature importances or rule-lists. These sorts of explanations may not be sufficient for many use cases that require dynamic, continuous discovery from stakeholders that have a range of skills and expertise. In the literature, few works ask decision-makers such as doctors, healthcare professionals, and policymakers about the utility of existing explanations and other desiderata they would like to see in an explanation going forward. In this work, we address this gap and carry out a study where we interview doctors, healthcare professionals, and policymakers about their needs and desires for explanations. Our study indicates that decision-makers would strongly prefer interactive explanations. In particular, they would prefer these interactions to take the form of natural language dialogues. Domain experts wish to treat machine learning models as “another colleague”, i.e., one who can be held accountable by asking why they made a particular decision through expressive and accessible natural language interactions. Considering these needs, we outline a set of five principles researchers should follow when designing interactive explanations as a starting place for future work. Further, we show why natural language dialogues satisfy these principles and are a desirable way to build interactive explanations. Next, we provide a design of a dialogue system for explainability, and discuss the risks, trade-offs, and research opportunities of building these systems. Overall, we hope our work serves as a starting place for researchers and engineers to design interactive, natural language dialogue systems for explainability that better serve users’ needs.",ArXiv,2022.0,246607834,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
0966363ea46368f297999b026c827f0f5ea7fcc9,https://www.semanticscholar.org/paper/0966363ea46368f297999b026c827f0f5ea7fcc9,Efficient Machine-Type Communication Using Multi-Metric Context-Awareness for Cars Used as Mobile Sensors in Upcoming 5G Networks,"Upcoming 5G-based communication networks will be confronted with huge increases in the amount of transmitted sensor data related to massive deployments of static and mobile Internet of Things (IoT) systems. Cars acting as mobile sensors will become important data sources for cloud-based applications like predictive maintenance and dynamic traffic forecast. Due to the limitation of available communication resources, it is expected that the grows in Machine-Type Communication (MTC) will cause severe interference with Human-to-human (H2H) communication. Consequently, more efficient transmission methods are highly required. In this paper, we present a probabilistic scheme for efficient transmission of vehicular sensor data which leverages favorable channel conditions and avoids transmissions when they are expected to be highly resource-consuming. Multiple variants of the proposed scheme are evaluated in comprehensive realworld experiments. Through machine learning based combination of multiple context metrics, the proposed scheme is able to achieve up to 164% higher average data rate values for sensor applications with soft deadline requirements compared to regular periodic transmission.",2018 IEEE 87th Vehicular Technology Conference (VTC Spring),2018.0,3491918,10.1109/VTCSpring.2018.8417753,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
cda8943fafd9d2b66dd036105d755f0a68f3bf7e,https://www.semanticscholar.org/paper/cda8943fafd9d2b66dd036105d755f0a68f3bf7e,Explaining Credit Risk Scoring through Feature Contribution Alignment with Expert Risk Analysts,"Credit assessments activities are essential for financial institutions and allow the global economy to grow. Building robust, solid and accurate models that estimate the probability of a default of a company is mandatory for credit insurance companies, specially when it comes to bridging the trade finance gap. Automating the risk assessment process will allow credit risk experts to reduce their workload and focus on the critical and complex cases, as well as to improve the loan approval process by reducing the time to process the application. The recent developments in Artificial Intelligence are offering new powerful opportunities. However, most AI techniques are labelled as blackbox models due to their lack of explainability. For both users and regulators, in order to deploy such technologies at scale, being able to understand the model logic is a must to grant accurate and ethical decision making. In this study, we focus on companies credit scoring and we benchmark different machine learning models. The aim is to build a model to predict whether a company will experience financial problems in a given time horizon. We address the black box problem using eXplainable Artificial Techniques –in particular, post-hoc explanations using SHapley Additive exPlanations. We bring light by providing an expertaligned feature relevance score highlighting the disagreement between a credit risk expert and a model feature attribution explanation in order to better quantify the convergence towards a better human-aligned decision making.",ArXiv,2021.0,232232768,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
c817f98e7674dbdfd333231fe811b99b561be40d,https://www.semanticscholar.org/paper/c817f98e7674dbdfd333231fe811b99b561be40d,Feature Attributions and Counterfactual Explanations Can Be Manipulated,"As machine learning models are increasingly used in critical decision-making settings (e.g., healthcare, finance), there has been a growing emphasis on developing methods to explain model predictions. Such explanations are used to understand and establish trust in models and are vital components in machine learning pipelines. Though explanations are a critical piece in these systems, there is little understanding about how they are vulnerable to manipulation by adversaries. In this paper, we discuss how two broad classes of explanations are vulnerable to manipulation. We demonstrate how adversaries can design biased models that manipulate model agnostic feature attribution methods (e.g., LIME & SHAP) and counterfactual explanations that hill-climb during the counterfactual search (e.g., Wachter’s Algorithm & DiCE) into concealing the model’s biases. These vulnerabilities allow an adversary to deploy a biased model, yet explanations will not reveal this bias, thereby deceiving stakeholders into trusting the model. We evaluate the manipulations on real world data sets, including COMPAS and Communities & Crime, and find explanations can be manipulated in practice.",ArXiv,2021.0,235606229,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
617412eb294e5dbbcbc481237eaef8e2226d441f,https://www.semanticscholar.org/paper/617412eb294e5dbbcbc481237eaef8e2226d441f,E1 Reconceiving Machine Learning E2 Aims and Background,"Beware of the man of one method or one instrument, either experimental or theoretical. He tends to become method oriented rather than problem oriented. The method-oriented man is shackled: the problem-oriented man is at least reaching freely toward what is most important. 52 Context Machine Learning is a sub-discipline of Information and Communication Technology (ICT) that develops the technologies for machines to recognise and learn patterns in data. It is distinct from, although related to, statistics. It can be differentiated by its focus on creating technology rather than the human-centred analysis of data. It is the science and engineering behind Data Mining. Machine learning is pervasive: it plays a key role in all stages of the scientific process and across diverse fields including bioinformatics, engineering and finance. It is widely accepted that ICT plays an enabling role across almost all technological disciplines. Analogously, Machine Learning plays an enabling role across most parts of ICT, from embedded to enterprise systems, and consequently is a crucial enabler of the Digital Economy 16. Vast quantities of data are now routinely collected and stored because it is affordable to do so. Machine learning makes sense of this data flood. The Problem The massive reduction in the cost of collecting, storing, transporting and processing data has meant an increasing need for tools to make sense of it. Unfortunately, the deployment of modern machine learning tools is more akin to a craft than an engineering discipline: the inference problems to be solved are often under-specified or ill-posed and the available tools are often ad hoc — lacking generality, transparency, usability and interoperability. Our premise is that the root cause of these difficulties is a lack of a clear conceptual basis for machine learning as an information engineering discipline. Research in machine learning is currently organised by technique (e.g. The last of these organisational approaches tend to be monistic — proposing that all problems be framed according to their principles. The first two approaches make no attempt to be comprehensive. There are two key symptoms arising from this lack of conceptual foundations: A lack of direction leading to no clear research agenda and a plethora of incremental advances that do not help solve real problems 31 leading to "" errors of the third kind (giving the right answer to the wrong question) "" 32. Furthermore there is a lack of usability of the tools created. There is …",,,10467516,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
58aa420828b2bea86f12e9f69b13434b4a72d4c1,https://www.semanticscholar.org/paper/58aa420828b2bea86f12e9f69b13434b4a72d4c1,Bayesian machine learning for financial modeling,"Machine Learning (ML) is so pervasive in our todays life that we don't even realise that, more often than expected, we are using systems based on it. It is also evolving faster than ever before. When deploying ML systems that make decisions on their own, we need to think about their ignorance of our uncertain world. The uncertainty might arise due to scarcity of the data, the bias of the data or even a mismatch between the real world and the ML-model. Given all these uncertainties, we need to think about how to build systems that are not totally ignorant thereof. Bayesian ML can to some extent deal with these problems. The specification of the model using probabilities provides a convenient way to quantify uncertainties, which can then be included in the decision making process. In this thesis, we introduce the Bayesian ansatz to modeling and apply Bayesian ML models in finance and economics. Especially, we will dig deeper into Gaussian processes (GP) and Gaussian process latent variable model (GPLVM). Applied to the returns of several assets, GPLVM provides the covariance structure and also a latent space embedding thereof. Several financial applications can be build upon the output of the GPLVM. To demonstrate this, we build an automated asset allocation system, a predictor for missing asset prices and identify other structure in financial data. It turns out that the GPLVM exhibits a rotational symmetry in the latent space, which makes it harder to fit. Our second publication reports, how to deal with that symmetry. We propose another parameterization of the model using Householder transformations, by which the symmetry is broken. Bayesian models are changed by reparameterization, if the prior is not changed accordingly. We provide the correct prior distribution of the new parameters, such that the model, i.e. the data density, is not changed under the reparameterization. After applying the reparametrization on Bayesian PCA, we show that the symmetry of nonlinear models can also be broken in the same way. In our last project, we propose a new method for matching quantile observations, which uses order statistics. The use of order statistics as the likelihood, instead of a Gaussian likelihood, has several advantages. We compare these two models and highlight their advantages and disadvantages. To demonstrate our method, we fit quantiled salary data of several European countries. Given several candidate models for the fit, our method also provides a metric to choose the best option. We hope that this thesis illustrates some benefits of Bayesian modeling (especially Gaussian processes) in finance and economics and its usage when uncertainties are to be quantified.",,,244444837,10.21248/gups.64469,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
114aa720872462b0ca1b97bfdec0ebd56c36fd0a,https://www.semanticscholar.org/paper/114aa720872462b0ca1b97bfdec0ebd56c36fd0a,Towards Understanding and Mitigating Social Biases in Language Models,"Warning: this paper contains model outputs that may be offensive or upsetting. As machine learning methods are deployed in realworld settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for highfidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",ICML,2021.0,235623756,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
b4077248bf62b77d0a2dc7f13d0cea0a00d57706,https://www.semanticscholar.org/paper/b4077248bf62b77d0a2dc7f13d0cea0a00d57706,Learning Deep Transferability for Several Agricultural Classification Problems,"This paper addresses several critical agricultural classification problems, 
e.g. grain discoloration and medicinal plants identification and classification, 
in Vietnam via combining the idea of knowledge transferability and state-of-the-art deep 
convolutional neural networks. Grain discoloration disease of rice is an emerging threat 
to rice harvest in Vietnam as well as all over the world and it acquires specific attention 
as it results in qualitative loss of harvested crop. Medicinal plants are an important element of 
indigenous medical systems. These resources are usually regarded as a part of culture’s traditional 
knowledge. Accurate classification is preliminary to any kind of intervention and recommendation of 
services. Hence, leveraging technology in automatic classification of these problems has become essential. 
Unfortunately, building and training a machine learning model from scratch is next to impossible due 
to the lack of hardware infrastructure and finance support. It painfully restricts the requirements of 
rapid solutions to deal with the demand. For this purpose, the authors have exploited the idea of 
transfer learning which is the improvement of learning in a new prediction task through the 
transferability of knowledge from a related prediction task that has already been learned. 
By utilizing state-of-the-art deep networks re-trained upon our collected data, our extensive 
experiments show that the proposed combination performs perfectly and achieves the classification 
accuracy of 98.7% and 98.5% on our collected datasets within the acceptable training time on a normal laptop. A mobile application is also deployed to facilitate further integrated recommendation and services.",International Journal of Advanced Computer Science and Applications,2019.0,86519120,10.14569/ijacsa.2019.0100107,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
c0d630215ac7b29b6ca4a8698d044bb91f82b965,https://www.semanticscholar.org/paper/c0d630215ac7b29b6ca4a8698d044bb91f82b965,Monadic Pavlovian associative learning in a backpropagation-free photonic network,"Over a century ago, Ivan P. Pavlov, in a classic experiment, demonstrated how dogs can learn to associate a ringing bell with food, thereby causing a ring to result in salivation. Today, however, it is rare to find the use of Pavlovian type associative learning for artificial intelligence (AI) applications. Instead, other biologically-inspired learning concepts, in particular artificial neural networks (ANNs) have flourished, yielding extensive impact on a wide range of fields including finance, healthcare and transportation. However, learning in such ""conventional"" ANNs, in particular in the form of modern deep neural networks (DNNs) are usually carried out using the backpropagation method, is computationally and energy intensive. Here we report the experimental demonstration of backpropagation-free learning, achieved using a single (or monadic) associative hardware element. This is realized on an integrated photonic platform using phase change materials combined with on-chip cascaded directional couplers. We link associative learning with supervised learning, based on their common goal of associating certain inputs with ""correct"" outputs. We then expand the concept to develop larger-scale supervised learning networks using our monadic Pavlovian photonic hardware, developing a distinct machine-learning framework based on single-element associations and, importantly, using backpropagation-free single-layer weight architectures to approach general learning tasks. Our approach not only significantly reduces the computational burden imposed by learning in conventional neural network approaches, thereby increasing speed and decreasing energy use during learning, but also offers higher bandwidth inherent to a photonic implementation, paving the way for future deployment of fast photonic artificially intelligent machines.",Optica,2020.0,227228057,10.1364/optica.455864,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
ee9e36c0abe8fd1c48aa73d1f83bcbb22fe3670b,https://www.semanticscholar.org/paper/ee9e36c0abe8fd1c48aa73d1f83bcbb22fe3670b,Hyper-Tune: Towards Efficient Hyper-parameter Tuning at Scale,"The ever-growing demand and complexity of machine learning are putting pressure on hyper-parameter tuning systems: while the evaluation cost of models continues to increase, the scalability of state-of-the-arts starts to become a crucial bottleneck. In this paper, inspired by our experience when deploying hyper-parameter tuning in a real-world application in production and the limitations of existing systems, we propose Hyper-Tune, an efficient and robust distributed hyper-parameter tuning framework. Compared with existing systems, Hyper-Tune highlights multiple system optimizations, including (1) automatic resource allocation, (2) asynchronous scheduling, and (3) multi-fidelity optimizer. We conduct extensive evaluations on benchmark datasets and a large-scale realworld dataset in production. Empirically, with the aid of these optimizations, Hyper-Tune outperforms competitive hyper-parameter tuning systems on a wide range of scenarios, including XGBoost, CNN, RNN, and some architectural hyper-parameters for neural networks. Compared with the state-of-the-art BOHB and A-BOHB, Hyper-Tune achieves up to 11.2× and 5.1× speedups, respectively. PVLDB Reference Format: Yang Li, Yu Shen, Huaijun Jiang, Wentao Zhang, Jixiang Li, Ji Liu, Ce Zhang, and Bin Cui. Hyper-Tune: Towards Efficient Hyper-parameter Tuning at Scale. PVLDB, 14(1): XXX-XXX, 2020. doi:XX.XX/XXX.XX PVLDB Availability Tag: The source code of this research paper has been made publicly available at https://github.com/PKU-DAIR/HyperTune.",Proc. VLDB Endow.,2022.0,246035307,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
dba3478cb46874a97e301deb0f20f7692c1f1ae9,https://www.semanticscholar.org/paper/dba3478cb46874a97e301deb0f20f7692c1f1ae9,PenDer: Incorporating Shape Constraints via Penalized Derivatives,"When deploying machine learning models in the real-world, system designers may wish that models exhibit certain shape behavior, i.e., model outputs follow a particular shape with respect to input features. Trends such as monotonicity, convexity, diminishing or accelerating returns are some of the desired shapes. Presence of these shapes makes the model more interpretable for the system designers, and adequately fair for the customers. We notice that many such common shapes are related to derivatives, and propose a new approach, PenDer (Penalizing Derivatives), which incorporates these shape constraints by penalizing the derivatives. We further present an Augmented Lagrangian Method (ALM) to learn the joint unconstrained objective function. Experiments on three realworld datasets illustrate that even though both PenDer and state-of-the-art Lattice models achieve similar conformance to shape, PenDer captures better sensitivity of prediction with respect to intended features. We also demonstrate that PenDer achieves better test performance than Lattice while enforcing more desirable shape behavior.",AAAI,2021.0,235349211,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
f02f6e666aef6b0675cc4a189f9962b716c17487,https://www.semanticscholar.org/paper/f02f6e666aef6b0675cc4a189f9962b716c17487,FARE: Enabling Fine-grained Attack Categorization under Low-quality Labeled Data,"Supervised machine learning classifiers have been widely used for attack detection, but their training requires abundant high-quality labels. Unfortunately, high-quality labels are difficult to obtain in practice due to the high cost of data labeling and the constant evolution of attackers. Without such labels, it is challenging to train and deploy targeted countermeasures. In this paper, we propose FARE, a clustering method to enable fine-grained attack categorization under low-quality labels. We focus on two common issues in data labels: 1) missing labels for certain attack classes or families; and 2) only having coarsegrained labels available for different attack types. The core idea of FARE is to take full advantage of the limited labels while using the underlying data distribution to consolidate the lowquality labels. We design an ensemble model to fuse the results of multiple unsupervised learning algorithms with the given labels to mitigate the negative impact of missing classes and coarsegrained labels. We then train an input transformation network to map the input data into a low-dimensional latent space for fine-grained clustering. Using two security datasets (Android malware and network intrusion traces), we show that FARE significantly outperforms the state-of-the-art (semi-)supervised learning methods in clustering quality/correctness. Further, we perform an initial deployment of FARE by working with a large e-commerce service to detect fraudulent accounts. With realworld A/B tests and manual investigation, we demonstrate the effectiveness of FARE to catch previously-unseen frauds.",NDSS,2021.0,231660317,10.14722/NDSS.2021.24403,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
d3ac65b10af091759863b8e2c488036bf52a2ce6,https://www.semanticscholar.org/paper/d3ac65b10af091759863b8e2c488036bf52a2ce6,DAG Card is the new Model Card,"With the progressive commoditization of modeling capabilities, data-centric AI recognizes that what happens before and after training becomes crucial for realworld deployments. Following the intuition behind Model Cards, we propose DAG Cards as a form of documentation encompassing the tenets of a data-centric point of view. We argue that Machine Learning pipelines (rather than models) are the most appropriate level of documentation for many practical use cases, and we share with the community an open implementation to generate cards from code.",ArXiv,2021.0,239885637,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
34c59ec266e2f1798918d765c2a2519a2b8d03d2,https://www.semanticscholar.org/paper/34c59ec266e2f1798918d765c2a2519a2b8d03d2,ReLink: Complete-Link Industrial Record Linkage Over Hybrid Feature Spaces,"Record Linkage (ReL) is the task of identifying records from a pair of databases referring to the same realworld entity. This has many applications in organisations of all sizes where related data often exist in silos leading to inefficiency in data engineering and analytics applications as well as ineffectiveness of business applications (e.g., unable to personalise marketing campaigns).State-of-the-art (SOTA) machine learning and deep learning based ReL techniques use adaptive similarity measures and learn their relative contributions based on labeled data. However, we report here that they do not work with similar efficacy on industrial data owing to its fundamental differences such as magnitude of schema heterogeneity, need for leveraging structure of the data, lack of training data etc. Through our proposed system ‘ReLink’, we carefully mitigate these challenges and demonstrate that it not only significantly outperforms SOTA baselines on industrial datasets but also on majority of research benchmarks. ReLink introduces the notion of complete-linkage over attributes as well as uses hybrid feature spaces on lexical and semantic similarity measures using pre-trained models such as BERT. Going beyond empirical demonstration, we provide insights and prescriptive guidance on choice of ReL techniques in industrial settings from our observations and lessons learnt from the experience of transferring and deploying for real use-cases in a large financial services organization.",2021 IEEE 37th International Conference on Data Engineering (ICDE),2021.0,235615691,10.1109/ICDE51399.2021.00293,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
0f5dc98f4ad051b7e49aa31cfc8fae5c2b532a1e,https://www.semanticscholar.org/paper/0f5dc98f4ad051b7e49aa31cfc8fae5c2b532a1e,Using system context information to complement weakly labeled data,"Real-world datasets collected with sensor networks often contain incomplete and uncertain labels as well as artefacts arising from the system environment. Complete and reliable labeling is often infeasible for large-scale and long-term sensor network deployments due to the labor and time overhead, limited availability of experts and missing ground truth. In addition, if the machine learning method used for analysis is sensitive to certain features of a deployment, labeling and learning needs to be repeated for every new deployment. To address these challenges, we propose to make use of system context information formalized in an information graph and embed it in the learning process via contrastive learning. Based on realworld data we show that this approach leads to an increased accuracy in case of weakly labeled data and leads to an increased robustness and transferability of the classifier to new sensor locations.",ArXiv,2021.0,236154879,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
3352eb2b731571ca9ba85e671a214b2e2467d8d9,https://www.semanticscholar.org/paper/3352eb2b731571ca9ba85e671a214b2e2467d8d9,Online and Scalable Model Selection with Multi-Armed Bandits,"Many online applications running on live traffic are powered by machine learning models, for which training, validation, and hyperparameter tuning are conducted on historical data. However, it is common for models demonstrating strong performance in offline analysis to yield poorer performance when deployed online. This problem is a consequence of the difficulty of training on historical data in non-stationary environments. Moreover, the machine learning metrics used for model selection may not sufficiently correlate with real-world business metrics used to determine the success of the applications being tested. These problems are particularly prominent in the Real-Time Bidding (RTB) domain, in which ML models power bidding strategies, and a change in models will likely affect performance of the advertising campaigns. In this work, we present Automatic Model Selector (AMS), a system for scalable online selection of RTB bidding strategies based on realworld performance metrics. AMS employs Multi-Armed Bandits (MAB) to near-simultaneously run and evaluate multiple models against live traffic, allocating the most traffic to the bestperforming models while decreasing traffic to those with poorer online performance, thereby minimizing the impact of inferior models on overall campaign performance. The reliance on offline data is avoided, instead making model selections on a case-by-case basis according to actionable business goals. AMS allows new models to be safely introduced into live campaigns as soon as they are developed, minimizing the risk to overall performance. In livetraffic tests on multiple ad campaigns, the AMS system proved highly effective at improving ad campaign performance.",ArXiv,2021.0,231709647,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
d39b03a0395fdd1df09399cd2c77afa44821e2fe,https://www.semanticscholar.org/paper/d39b03a0395fdd1df09399cd2c77afa44821e2fe,Faking feature importance: A cautionary tale on the use of differentially-private synthetic data,"Synthetic datasets are often presented as a silver-bullet solution to the problem of privacy-preserving data publishing. However, for many applications, synthetic data has been shown to have limited utility when used to train predictive models. One promising potential application of these data is in the exploratory phase of the machine learning workflow, which involves understanding, engineering and selecting features. This phase often involves considerable time, and depends on the availability of data. There would be substantial value in synthetic data that permitted these steps to be carried out while, for example, data access was being negotiated, or with fewer information governance restrictions. This paper presents an empirical analysis of the agreement between the feature importance obtained from raw and from synthetic data, on a range of artificially generated and realworld datasets (where feature importance represents how useful each feature is when predicting a the outcome). We employ two differentially-private methods to produce synthetic data, and apply various utility measures to quantify the agreement in feature importance as this varies with the level of privacy. Our results indicate that synthetic data can sometimes preserve several representations of the ranking of feature importance in simple settings but their performance is not consistent and depends upon a number of factors. Particular caution should be exercised in more nuanced real-world settings, where synthetic data can lead to differences in ranked feature importance that could alter key modelling decisions. This work has important implications for developing synthetic versions of highly sensitive data sets in fields such as finance and healthcare.",ArXiv,2022.0,247223114,10.48550/arXiv.2203.01363,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
7d6e9e6e39c70b947296cb4c17aacf4452fc5b29,https://www.semanticscholar.org/paper/7d6e9e6e39c70b947296cb4c17aacf4452fc5b29,Exploring the Use of Synthetic Gradients for Distributed Deep Learning across Cloud and Edge Resources,"With the explosive growth of data, largely contributed by the rapidly and widely deployed smart devices on the edge, we need to rethink the training paradigm for learning on such realworld data. The conventional cloud-only approach can hardly keep up with the computational demand from these deep learning tasks; and the traditional back propagation based training method also makes it difficult to scale out the training. Fortunately, the continuous advancement in System on Chip (SoC) hardware is transforming edge devices into capable computing platforms, and can potentially be exploited to address these challenges. These observations have motivated this paper’s study on the use of synthetic gradients for distributed training cross cloud and edge devices. We employ synthetic gradients into various neural network models to comprehensively evaluate its feasibility in terms of accuracy and convergence speed. We distribute the training of the various layers of a model using synthetic gradients, and evaluate its effectiveness on the edge by using resource-limited containers to emulate edge devices. The evaluation result shows that the synthetic gradient approach can achieve comparable accuracy compared to the conventional back propagation, for an eight-layer model with both fully-connected and convolutional layers. For a more complex model (VGG16), the training suffers from some accuracy degradation (up to 15%). But it achieves 11% improvement in training speed when the layers of a model are decoupled and trained on separate resource-limited containers, compared to the training of the whole model using the conventional method on the physical machine.",HotEdge,2019.0,196182432,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
44b1fa2869065e62626afc7a8d8c12ba54625742,https://www.semanticscholar.org/paper/44b1fa2869065e62626afc7a8d8c12ba54625742,S-LIME: Stabilized-LIME for Model Explanation,"An increasing number of machine learning models have been deployed in domains with high stakes such as finance and healthcare. Despite their superior performances, many models are black boxes in nature which are hard to explain. There are growing efforts for researchers to develop methods to interpret these black-box models. Post hoc explanations based on perturbations, such as LIME [39], are widely used approaches to interpret a machine learning model after it has been built. This class of methods has been shown to exhibit large instability, posing serious challenges to the effectiveness of the method itself and harming user trust. In this paper, we propose S-LIME, which utilizes a hypothesis testing framework based on central limit theorem for determining the number of perturbation points needed to guarantee stability of the resulting explanation. Experiments on both simulated and real world data sets are provided to demonstrate the effectiveness of our method.",KDD,2021.0,235435888,10.1145/3447548.3467274,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
b1fe45eda204847f5f4c0b3b8eafaecaf184859c,https://www.semanticscholar.org/paper/b1fe45eda204847f5f4c0b3b8eafaecaf184859c,Consistent Counterfactuals for Deep Models,"Counterfactual examples are one of the most commonly-cited methods for explaining the predictions of machine learning models in key areas such as finance and medical diagnosis. Counterfactuals are often discussed under the assumption that the model on which they will be used is static, but in deployment models may be periodically retrained or fine-tuned. This paper studies the consistency of model prediction on counterfactual examples in deep networks under small changes to initial training conditions, such as weight initialization and leave-one-out variations in data, as often occurs during model deployment. We demonstrate experimentally that counterfactual examples for deep models are often inconsistent across such small changes, and that increasing the cost of the counterfactual, a stability-enhancing mitigation suggested by prior work in the context of simpler models, is not a reliable heuristic in deep networks. Rather, our analysis shows that a model’s Lipschitz continuity around the counterfactual, along with confidence of its prediction, is key to its consistency across related models. To this end, we propose Stable Neighbor Search as a way to generate more consistent counterfactual explanations, and illustrate the effectiveness of this approach on several benchmark datasets.",ArXiv,2021.0,238419007,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
376ed03fc9b0769fa61acfab5123a69c088be8b2,https://www.semanticscholar.org/paper/376ed03fc9b0769fa61acfab5123a69c088be8b2,Beyond Discriminant Patterns: On the Robustness of Decision Rule Ensembles,"Local decision rules are commonly understood to be more explainable, due to the local nature of the patterns involved. With numerical optimization methods such as gradient boosting, ensembles of local decision rules can gain good predictive performance on data involving global structure. Meanwhile, machine learning models are being increasingly used to solve problems in high-stake domains including healthcare and finance. Here, there is an emerging consensus regarding the need for practitioners to understand whether and how those models could perform robustly in the deployment environments, in the presence of distributional shifts. Past research on local decision rules has focused mainly on maximizing discriminant patterns, without due consideration of robustness against distributional shifts. In order to fill this gap, we propose a new method to learn and ensemble local decision rules, that are robust both in the training and deployment environments. Specifically, we propose to leverage causal knowledge by regarding the distributional shifts in subpopulations and deployment environments as the results of interventions on the underlying system. We propose two regularization terms based on causal knowledge to search for optimal and stable rules. Experiments on both synthetic and benchmark datasets show that our method is effective and robust against distributional shifts in multiple environments.",ArXiv,2021.0,237592591,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
f3639e6900498c39f0d72e034e84d4bd5c648b1c,https://www.semanticscholar.org/paper/f3639e6900498c39f0d72e034e84d4bd5c648b1c,On the Trustworthiness of Tree Ensemble Explainability Methods,,CD-MAKE,2021.0,237101212,10.1007/978-3-030-84060-0_19,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
98f40c015bd9f5b5a90f888f677aa9759a82c3a2,https://www.semanticscholar.org/paper/98f40c015bd9f5b5a90f888f677aa9759a82c3a2,Artificial Intelligence potential within airlines: a review on how AI can enhance strategic decision-making in times of COVID-19,"Purpose: Airline strategy relies on the competitive environment analysis and the management of resources. Artificial Intelligence (AI) algorithms are being increasingly deployed throughout several industries. COVID-19 has further stressed a sector where firms have historically struggled to sustain profitability.The purpose is to explore the potential of AI applications regarding strategic decision-making in airlines in times of crisis and to depict a roadmap to encourage scholars and practitioners to jointly implement these tools within corporations.Design/methodology/approach: This study firstly reviews the state-of-the-art regarding transport organization trends with focus on airline strategy and finance as well as AI tools, supported by the collaboration of a former airline digitalization strategist. Secondly, the potential of the latter to be applied in those functions is analyzed, considering different Machine Learning (ML) methods and algorithms.Findings: Some applications or pathways are identified as of particular interest for the airlines’ strategic decision-making process. Most of them are based on ML algorithms and training methods that are currently underused or disregarded in certain business areas, such as Neural Network models for unsupervised market analysis or supervised cost estimation.Research limitations/implications: Focus is on airline strategy and finance, keeping engineering or operational applications out of the scope.Practical implications: Proposed guidance may promote the deployment of AI tools which currently lack practical implementation in certain business areas.Social implications: Showcased guidance may revert into a closer collaboration between business and academia.Originality/value: Comprehensive review of current airlines’ strategic levers and identification of promising AI pathways to be further explored.",Journal of Airline and Airport Management,2021.0,245213212,10.3926/jairm.189,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
812a8867eb3fdbd2619dd66ae146d704a2fcb5da,https://www.semanticscholar.org/paper/812a8867eb3fdbd2619dd66ae146d704a2fcb5da,Classiﬁcation of Credit Card Fraudlent transactions using Neural Network and Oversampling Technique.,"Credit card fraud is a ﬁnancial type of fraud that involves the use of credit card details to purchase products and withdraw speciﬁc amounts without the permis-sion of the person who holds the credit card.Since the advent of the online payment method in the banking sector, there has always been someone or a group of individuals who have discovered new techniques or approaches to obtaining ﬁnance/funds through unlawful means.It is noted that the card owner is not aware of illegal transactions that have performed with his/her credit card until any kind of purchase is made, as physical credit cards are not used in online purchases. In recent years, many credit card companies have deployed an automated system with machine learning technique commonly known as Fraud Detection system so as to analyse fraudulent transaction.Every new fraudulent activity raises the demand for software systems that detect fraudulent credit card transactions. Based on the logs of transactions performed, many researchers have built credit card fraud detection systems that use various data mining and deep learning techniques, machine learning algorithms to determine whether the transaction performed is fraudulent.However, the intricacy of fraudulent transactions is created in such a way that it resembles the genuine ones every time.. For the identiﬁcation of frauds, the suggested method employs unbalanced severely skewed transactional data and a convolutional network. The dataset utilised here is the highly skewed machine learning kaggle dataset for credit card fraud detection. The characteristics that have been assessed are 1 for the fraud class and 0 for the non-fraud class.The present research uses credit card fraud dataset,where the dataset is preprocessed with the help of Principal Component Analysis , Adaptive Synthetic Sample technique and then Neural Network Classiﬁers are applied with diﬀerent number of hidden layers and performance of these classiﬁers has been evaluated on the basis of accuracy,precision and recall rate.",,2021.0,248237399,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
0f4516bec40919f75c347ae75b3d67b988d8051f,https://www.semanticscholar.org/paper/0f4516bec40919f75c347ae75b3d67b988d8051f,Assessing Bias in Medical AI,"Machine learning and artificial intelligence are increasingly deployed in critical societal functions such as finance, media and healthcare. Along with their deployment come increasing reports of their failure when viewed through the lens of ethical principles such as fairness, democracy and equal opportunity. As a result, research into fair algorithms and mitigation of bias in data and algorithms, has surged in recent years. However, while it might seem clear what fairness entails, and how to achieve it, in some applications, established concepts do not translate directly to other domains. In this work, we consider healthcare specifically, illustrating limitations and challenges of fair models within medical applications and give recommendations for the development of AI in healthcare.",,2021.0,237475267,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
c43fef8f1d8ebaeea92570cb911f1748072c5127,https://www.semanticscholar.org/paper/c43fef8f1d8ebaeea92570cb911f1748072c5127,Vision-based Conflict Detection within Crowds based on High-Resolution Human Pose Estimation for Smart and Safe Airport,"—Future airports are becoming more complex and congested with the increasing number of travellers. While the airports are more likely to become hotspots for potential conflicts to break out which can cause serious delays to flights and several safety issues. An intelligent algorithm which renders security surveillance more effective in detecting conflicts would bring many benefits to the passengers in terms of their safety, finance, and travelling efficiency. This paper details the development of a machine learning model to classify conflicting behaviour in a crowd. HRNet is used to segment the images and then two approaches are taken to classify the poses of people in the frame via multiple classifiers. Among them, it was found that the support vector machine (SVM) achieved the most performant achieving precision of 94.37%. Where the model falls short is against ambiguous behaviour such as a hug or losing track of a subject in the frame. The resulting model has potential for deployment within an airport if improvements are made to cope with the vast number of potential passengers in view as well as training against further ambiguous behaviours which will arise in an airport setting. In turn, will provide the capability to enhance security surveillance and improve airport safety.",ArXiv,2022.0,250244082,10.48550/arXiv.2207.00477,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
dfba12c15cf90d07e10f4f2dcfc2429706e3db30,https://www.semanticscholar.org/paper/dfba12c15cf90d07e10f4f2dcfc2429706e3db30,A Human-Centric Take on Model Monitoring,"Predictive models are increasingly used to make various consequential decisions in high-stakes domains such as healthcare, ﬁnance, and policy. It becomes critical to ensure that these models make accurate predictions, are robust to shifts in the data, do not rely on spurious features, and do not unduly discriminate against minority groups. To this end, several approaches spanning various areas such as explainability, fairness, and ro-bustness have been proposed in recent literature. Such approaches need to be human-centered as they cater to the understanding of the models to their users. However, there is a research gap in understanding the human-centric needs and challenges of monitoring machine learning (ML) models once they are deployed . To ﬁll this gap, we conducted an interview study with 13 practitioners who have experience at the intersection of deploying ML models and engaging with cus-tomers spanning domains such as ﬁnancial services, healthcare, hiring, online retail, computational advertising, and conversational assistants. We identiﬁed various human-centric challenges and requirements for model monitoring in real-world applications. Speciﬁcally, we found the need and the challenge for the model monitoring systems to clarify the impact of the monitoring observations on outcomes. Further, such insights must be actionable, robust, customizable for domain-speciﬁc use cases, and cognitively considerate to avoid information overload.",ArXiv,2022.0,249431865,10.48550/arXiv.2206.02868,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
e0a34688fe15ebbf792fbb995ead1bb00536804b,https://www.semanticscholar.org/paper/e0a34688fe15ebbf792fbb995ead1bb00536804b,Simple Regularisation for Uncertainty-Aware Knowledge Distillation,"Considering uncertainty estimation of modern neural networks (NNs) is one of the most important steps towards deploying machine learning systems to meaningful real-world applications such as in medicine, ﬁnance or autonomous systems. At the moment, ensembles of different NNs constitute the state-of-the-art in both accuracy and uncertainty estimation in different tasks. However, ensembles of NNs are unpractical under real-world constraints, since their computation and memory consumption scale linearly with the size of the ensemble, which increase their latency and deployment cost. In this work, we examine a simple regularisation approach for distribution-free knowledge distillation of ensemble of machine learning models into a single NN. The aim of the regularisation is to preserve the diversity, accuracy and uncertainty estimation characteristics of the original ensemble without any intricacies, such as ﬁne-tuning. We demonstrate the generality of the approach on combinations of toy data, SVHN/CIFAR-10, simple to complex NN architectures and different tasks.",ArXiv,2022.0,248887455,10.48550/arXiv.2205.09526,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
3a31b3354f9d5bc18dd0e3abf23dc9aedc5b8d1a,https://www.semanticscholar.org/paper/3a31b3354f9d5bc18dd0e3abf23dc9aedc5b8d1a,Is it a great Autonomous FX Trading Strategy or you are just fooling yourself,"There are many practitioners that create software to buy and sell financial assets in an autonomous way. There are some digital platforms that allow the development, test and deployment of trading agents (or robots) in simulated or real markets. In fact, autonomous trading robots have been studied in artificial intelligence area for some time. Some of these work focus on very short horizons of investment, while others deal with longer periods. The spectrum of used AI techniques in finance field is wide and it includes more recent approaches like convolutional neural networks and deep reinforcement learning. There are many cases, where the developers are successful in creating robots with great performance when executing with historical price series (so called backtesting). Furthermore, some electronic platforms make available thousands of robots that [allegedly] are able to be profitable in real markets. These autonomous strategies may be created with some simple idea (moving averages, for instance) or using complex machine learning schemes. Nevertheless, when these robots are used in real markets (or data not used in their training or evaluation) frequently they present very poor performance and high variance of returns. In this paper, we propose a method for testing Foreign Exchange (FX) trading strategies that can provide realistic expectations about strategy’s performance. This method addresses many pitfalls that can fool even experience practitioners and researchers. We present the results of applying such method in several famous autonomous strategies in many different financial assets. By the analysis of such results, we can realize that it is very hard to build a reliable strategy and many published strategies are far from being reliable vehicles of investment. Some periods of good performance can be observed, but they are not predictable and probably they happen by pure chance, rather than by the identification of real pattern in the input data. These facts can be maliciously used by those who try to sell such robots, by advertising such great (and non repetitive) results, while hiding the bad but meaningful results. The proposed method can be used to select among potential robots, establishes minimal periods and requirements for the test executions. In this way, the method helps to tell if you really have a great trading strategy or you are just fooling yourself.",ArXiv,2021.0,231632514,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
fc2c062436fdabe85b7eb2d59b282f3561b5ebce,https://www.semanticscholar.org/paper/fc2c062436fdabe85b7eb2d59b282f3561b5ebce,GenSyth: a new way to understand deep learning,"Deep learning is a branch in machine learning which focuses on learning hierarchical feature representations of data using neural networks, with each successive layer representing information at a progressively more abstract level than the previous one. The progress of research in deep learning has greatly accelerated in the past decade, with tremendous advances leading to state-of-the-art performance in a wide range of tasks well beyond conventional machine learning methods. For example, deep neural networks have excelled at visual perception tasks, leading to an increase in top-5 accuracy from 74.2% in 2011 (before deep learning) to 97.6% in 2019, which is beyond human performance. Similar demonstrations of state-of-the-art performance beyond past machine learning methods have also been achieved for audio perception [1], natural language processing [2], game playing [3], scientific discovery [4], and content generation [5]. Given all the recent success, there has now been tremendous interest and focus on not only the academic research community, but also significant investment by industry in the wide-spread adoption of deep learning for solving complex real-world problems such as autonomous driving, smart cities, manufacturing, and finance. Despite the advances in accuracy and performance gained via deep learning, one of the biggest challenges with widespread ‘operational’ adoption is the sheer complexity of these high-performant deep neural networks created by the research community. As much of the focus had been on modelling accuracy and performance, many of the created deep neural networks have highly complex architectures that are intractable from both computational and memory perspectives in real-world operational scenarios. This complexity issue is particularly challenging for edge and mobile scenarios, where on-device processing is highly desirable (and often necessary) for privacy and latency/bandwidth reasons, and the embedded chips have greatly restricted computational, memory, and energy resources. As such, the ability to design and create deep neural networks that are not only high-performing but also operate well on low-power embedded devices has become a crucial path in breaking the barrier towards real-world, operational use and deployment of deep learning within industrial scenarios.",Electronics Letters,2019.0,203123388,10.1049/el.2019.2376,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
81600fd653a828d69f6160705be6814dd101beb7,https://www.semanticscholar.org/paper/81600fd653a828d69f6160705be6814dd101beb7,From local explanations to global understanding with explainable AI for trees,,Nat. Mach. Intell.,2020.0,214265509,10.1038/s42256-019-0138-9,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
320e3a66dbc51a1331f65b5ac42b937a39d5e051,https://www.semanticscholar.org/paper/320e3a66dbc51a1331f65b5ac42b937a39d5e051,Physical Activity Recognition With Statistical-Deep Fusion Model Using Multiple Sensory Data for Smart Health,"Nowadays, enhancing the living standard with smart healthcare via the Internet of Things is one of the most critical goals of smart cities, in which artificial intelligence plays as the core technology. Many smart services, deployed according to wearable sensor-based physical activity recognition, have been able to early detect unhealthy daily behaviors and further medical risks. Numerous approaches have studied shallow handcrafted features coupled with traditional machine learning (ML) techniques, which find it difficult to model real-world activities. In this work, by revealing deep features from deep convolutional neural networks (DCNNs) in fusion with conventional handcrafted features, we learn an intermediate fusion framework of human activity recognition (HAR). According to transforming the raw signal value to pixel intensity value, segmentation data acquired from a multisensor system are encoded to an activity image for deep model learning. Formulated by several novel residual triple convolutional blocks, the proposed DCNN allows extracting multiscale spatiotemporal signal-level and sensor-level correlations simultaneously from the activity image. In the fusion model, the hybrid feature merged from the handcrafted and deep features is learned by a multiclass support vector machine (SVM) classifier. Based on several experiments of performance evaluation, our fusion approach for activity recognition has achieved the accuracy over 96.0% on three public benchmark data sets, including Daily and Sport Activities, Daily Life Activities, and RealWorld. Furthermore, the method outperforms several state-of-the-art HAR approaches and demonstrates the superiority of the proposed intermediate fusion model in multisensor systems.",IEEE Internet of Things Journal,2021.0,226420451,10.1109/JIOT.2020.3013272,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
36fa3a4d2138c9ec1830ae6f88f4b4ee03db9517,https://www.semanticscholar.org/paper/36fa3a4d2138c9ec1830ae6f88f4b4ee03db9517,Measures and Best Practices for Responsible AI,"The use of machine learning (ML) based systems has become ubiquitous including their usage in critical applications like medicine and assistive technologies. Therefore, it is important to determine the trustworthiness of these ML models and tasks. A key component in this determination is the development of task specific datasets, metrics, and best practices which are able to measure the various aspects of responsible model development and deployment including robustness, interpretability and fairness. Further, datasets are also key when training for a given task, be it coreference resolution in language modeling or facial recognition in computer vision. Imbalances and inadequate representation in datasets can have repercussions of an undesirable nature. Some common examples include how coreference resolution systems in NLU are often not all gender inclusive, discrepancies in the measurement of how robust and trustworthy machine predictions are in domains where the selective labels problem is prevalent, and discriminatory determination of pain or care levels of people belonging to different demographics in health science applications. Development of task specific datasets which do better in this regard is also extremely vital. In this workshop, we invite contributions towards different (i) datasets which help enhance task performance and inclusivity, (ii) measures and metrics which help in determining the trustworthiness of a model/dataset, (iii) assessment or remediation tools for fairer, more transparent, robust, and reliable models, and (iv) case studies describing responsible development and deployment of AI systems across fields such as healthcare, financial services, insurance, etc. The datasets, measures, mitigation techniques, and best practices could focus on different areas including (but not restricted to) the following: Fairness and Bias Robustness Reliability and Safety Interpretability Explainability Ethical AI Causal Inference Counterfactual Example Analysis They could also be focussed on the applications in diverse fields such as industry, finance, healthcare and beyond. Text based datasets can be in languages other than English as well.",KDD,2021.0,236980315,10.1145/3447548.3469458,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
c04ade88be4851818bad2f1542b87b0a9179f336,https://www.semanticscholar.org/paper/c04ade88be4851818bad2f1542b87b0a9179f336,Temporal Debiasing using Adversarial Loss based GNN architecture for Crypto Fraud Detection,"The tremendous rise of cryptocurrency in the payment domain has unlocked huge opportunities but also raised numerous challenges in parallel involving cybercriminal activities like money laundering, terrorist financing, illegal and risky services, etc, owing to its anonymous and decentralized setup. The demand for building a more transparent cryptocurrency network, resilient to such activities, has risen extensively as more financial institutions look to incorporate it into their network. While a plethora of traditional machine learning and graph based deep learning techniques have been developed to detect illicit activities in a cryptocurrency transaction network, the challenge of generalization and robust model performance on future timesteps still exists. In this paper, we show that the model learned on transactional feature set provided in dataset (Elliptic Dataset) carry a temporal bias, i.e. they are highly dependent on the timesteps they occur. Deploying temporally biased models limits their performance on future timesteps. To address this, we propose a temporal debiasing technique using GNN based architecture that ensures generalization by adversarially learning between fraud 1 classification and temporal classification. The adversarial loss constructed optimizes the embeddings to ensure they 1.) perform well on fraud classification task 2.) does not contain temporal bias. The proposed architecture capture the underlying fraud patterns that remain consistent over time. We evaluate the performance of our proposed architecture on the Elliptic dataset and compare the performance with existing machine learning and graph-based architectures.1Fraud and illicit are used interchangeably in this paper",2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA),2021.0,246291296,10.1109/ICMLA52953.2021.00067,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
a61f9078a489e29766ee90fa03286b5bc20cef4a,https://www.semanticscholar.org/paper/a61f9078a489e29766ee90fa03286b5bc20cef4a,Research on Strategy Discovery and Optimisation of an Intelligent Option Trading System,"'Blue Ocean' is a term that refers to an unexploited or uncontested market space. AI, machine learning and Big Data has presented a range of technologies for exploring Blue Ocean market spaces. For example, in the world of financial technology (fintech) introducing new investment strategies
 could have an impact on investment opportunities. Professor Chien-Feng Huang, Department of Computer Science and Information Engineering, National University of Kaohsiung, Taiwan, conducts novel research to discover and optimise an intelligent option trading system. In one line of research,
 he is working to solve the problem of combining different strategies to generate generalised models to tackle various financial circumstances while investing. A feasible way to do this, according to the studies of Huang and his team, is to utilise AI-based optimisation methodologies. Through
 their studies, the researchers are seeking to advance the research and applications of fintech and discover more efficient and effective investment models to create blue ocean strategies within finance. Huang is also working on autonomous self-evolving forecasting models for price movement
 in high frequency trading (HFT). He and his team have developed novel AI-based models for forecasting price movement in HFT and found that their proposed methods can increase the prediction accuracy for HFT price movement and advance the current state of HFT research. Huang has had encouraging
 results so far with the stock selection models, pairs-trading models and options-trading models developed and deployed in the market proving viable.",Impact,2022.0,250080278,10.21820/23987073.2022.3.26,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
2afcecbd007f45bee8fc48def7c72b636840f98e,https://www.semanticscholar.org/paper/2afcecbd007f45bee8fc48def7c72b636840f98e,Evaluation Optimal Prediction Performance of MLMs on High-volatile Financial Market Data,"The present study evaluates the prediction performance of the multi-machine learning models (MLMs) on high-volatile financial markets data sets since 2007 to 2020. The linear and nonlinear empirical data sets are comprised on stock price returns of Karachi stock exchange (KSE) 100-Index of Pakistan and currencies exchange rates of Pakistani Rupees (PKR) against five major currencies (USD, Euro, GBP, CHF & JPY). In the present study, the support vector regression (SVR), random forest (RF), and machine learning-linear regression model (ML-LRM) are under-evaluated for comparative prediction performance. Moreover, the findings demonstrated that the SVR comparatively gives optimal prediction performance on group1. Similarly, the RF relatively gives the best prediction performance on group2. The findings of study concludes that the algorithm of RF is most appropriate for nonlinear approximation/evaluation and the algorithm of SVR is most useful for high-frequency time-series data estimation. The present study is contributed by exploring comparative enthusiastic/optimistic machine learning model on multi-nature data sets. This empirical study would be helpful for finance and machine-learning pupils, data analysts and researchers, especially for those who are deploying machine-learning approaches for financial analysis. Keywords—Support vector regression; random forest; machine learning-linear regression model; optimal prediction performance; currencies exchange rates; stock price returns",International Journal of Advanced Computer Science and Applications,2022.0,246543957,10.14569/ijacsa.2022.0130129,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
69f91edaf5abc963a9b9b121d1f8bc851333e133,https://www.semanticscholar.org/paper/69f91edaf5abc963a9b9b121d1f8bc851333e133,Active Learning in the Era of Big Data,"Active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning. Because of this, real-world testing and comparing active learning algorithms requires collecting new datasets (adaptively), rather than simply applying algorithms to benchmark datasets, as is the norm in (passive) machine learning research. To facilitate the development, testing and deployment of active learning for real applications, we have built an open-source software system for large-scale active learning research and experimentation. The system, called NEXT, provides a unique platform for realworld, reproducible active learning research. This paper details the challenges of building the system and demonstrates its capabilities with several experiments. The results show how experimentation can help expose strengths and weaknesses of active learning algorithms, in sometimes unexpected and enlightening ways.",,2015.0,61563876,10.2172/1225849,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
6af4abe9f3360b2817db4a655e1d3486754ba6b0,https://www.semanticscholar.org/paper/6af4abe9f3360b2817db4a655e1d3486754ba6b0,Large-scale learning for media understanding,,EURASIP J. Image Video Process.,2015.0,32169272,10.1186/S13640-015-0080-7,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
641076d8786511559cd31b96fc2c93426120ad47,https://www.semanticscholar.org/paper/641076d8786511559cd31b96fc2c93426120ad47,Multiaccuracy: Black-Box Post-Processing for Fairness in Classification,"Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for ""black women"") even when the sensitive features (e.g. ""race"", ""gender"") are not given to the algorithm explicitly.",AIES,2018.0,44125328,10.1145/3306618.3314287,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
10bbc5556fe6b79b18bb986af5e601b7ac7ab114,https://www.semanticscholar.org/paper/10bbc5556fe6b79b18bb986af5e601b7ac7ab114,LTF: A Label Transformation Framework for Correcting Target Shift,"Distribution shift is a major obstacle to the deployment of current deep learning models on realworld problems. Let Y be the target (label) and X the predictors (features). We focus on one type of distribution shift, target shift, where the marginal distribution of the target variable PY changes but the conditional distribution PX|Y does not. Existing methods estimate the density ratio between the sourceand target-domain label distributions by density matching. However, these methods are either computationally infeasible for large-scale data or restricted to shift correction for discrete labels. In this paper, we propose an end-to-end Label Transformation Framework (LTF) for correcting target shift, which implicitly models the shift of PY and the conditional distribution PX|Y using neural networks. Thanks to the flexibility of deep networks, our framework can handle continuous, discrete, and even multidimensional labels in a unified way and is scalable to large data. Moreover, for high dimensional X , such as images, we find that the redundant information in X severely degrades the estimation accuracy. To remedy this issue, we propose to match the distribution implied by our generative model and the target-domain distribution in a low-dimensional feature space that discards information irrelevant to Y . Both theoretical and empirical studies demonstrate the superiority of our method over previous approaches. UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW 2008, Australia School of Mathematics and Statistics, The University of Melbourne Department of Philosophy, Carnegie Mellon University. Correspondence to: Jiaxian Guo <jguo5934@uni.sydney.edu.au>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).",,2020.0,220531550,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
29811399204f74db0951c04b5b018177e0e7b5a4,https://www.semanticscholar.org/paper/29811399204f74db0951c04b5b018177e0e7b5a4,Redefining Banking and Financial Industry through the application of Computational Intelligence,"Computational Intelligence applications in banking and finance have been developed and deployed in the recent past and have been offering business solutions in both front end and back end processes in order to create efficiency and exceptional customer experience. In recent times, AI and machine learning are perceived to be the most valuable enabler to achieve competitive advantage by enhancing the decision making capabilities and transforming the banking industry. This paper will highlight the applications of AI and evaluate its utility in different functional areas of financial industry focusing primarily on automation of banking operations and customer engagement. It concludes with an analysis of how banking and financial organizations frame their environment and effectively use computational intelligence to improve their business.",2019 Advances in Science and Engineering Technology International Conferences (ASET),2019.0,155109430,10.1109/ICASET.2019.8714305,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
ed6ce30c5bec85fc992bd9e814e0f77ea76cd18d,https://www.semanticscholar.org/paper/ed6ce30c5bec85fc992bd9e814e0f77ea76cd18d,AI explainability 360: hands-on tutorial,"This tutorial will teach participants to use and contribute to a new open-source Python package named AI Explainability 360 (AIX360) (https://aix360.mybluemix.net), a comprehensive and extensible toolkit that supports interpretability and explainability of data and machine learning models. Motivation for the toolkit. The AIX360 toolkit illustrates that there is no single approach to explainability that works best for all situations. There are many ways to explain: data vs. model, direct vs. post-hoc explanation, local vs. global, etc. The toolkit includes ten state of the art algorithms that cover different dimensions of explanations along with proxy explainability metrics. Moreover, one of our prime objectives is for AIX360 to serve as an educational tool even for non-machine learning experts (viz. social scientists, healthcare experts). To this end, the toolkit has an interactive demonstration, highly descriptive Jupyter notebooks covering diverse real-world use cases, and guidance materials, all helping one navigate the complex explainability space. Compared to existing open-source efforts on AI explainability, AIX360 takes a step forward in focusing on a greater diversity of ways of explaining, usability in industry, and software engineering. By integrating these three aspects, we hope that AIX360 will attract researchers in AI explainability and help translate our collective research results for practicing data scientists and developers deploying solutions in a variety of industries. Regarding the first aspect of diversity, Table 1 in [1] compares AIX360 to existing toolkits in terms of the types of explainability methods offered. The table shows that AIX360 not only covers more types of methods but also has metrics which can act as proxies for judging the quality of explanations. Regarding the second aspect of industry usage, AIX360 illustrates how these explainability algorithms can be applied in specific contexts (please see Audience, goals, and outcomes below). In just a few months since its initial release, the AIX360 toolkit already has a vibrant slack community with over 120 members and has been forked almost 80 times accumulating over 400 stars. This response leads us to believe that there is significant interest in the community in learning more about the toolkit and explainability in general. Audience, goals, and outcomes. The presentations in the tutorial will be aimed at an audience with different backgrounds and computer science expertise levels. For all audience members and especially those unfamiliar with Python programming, AIX360 provides an interactive experience (http://aix360.mybluemix.net/data) centered around a credit approval scenario as a gentle and grounded introduction to the concepts and capabilities of the toolkit. We will also teach all participants which type of explainability algorithm is most appropriate for a given use case, not only for those in the toolkit but also from the broader explainability literature. Knowing which explainability algorithms apply to which contexts and understanding when to use them can benefit most people, regardless of their technical background. The second part of the tutorial will consist of three use cases featuring different industry domains and explanation methods. Data scientists and developers can gain hands-on experience with the toolkit by running and modifying Jupyter notebooks, while others will be able to follow along by viewing rendered versions of the notebooks. Here is a rough agenda of the tutorial: 1) Overture: Provide a brief introduction to the area of explainability as well as introduce common terms. 2) Interactive Web Experience: The AIX360 interactive web experience (http://aix360.mybluemix.net/data) is intended to show a non-computer science audience how different explainability methods may suit different stakeholders in a credit approval scenario (data scientists, loan officers, and bank customers). 3) Taxonomy: We will next present a taxonomy that we have created for organizing the space of explanations and guiding practitioners toward an appropriate choice for their applications. 4) Installation: We will transition into a Python environment and ask participants to install the AIX360 package on their machines using provided instructions. 5) Example Use Cases in Finance, Government, and Healthcare: We will take participants through three use-cases in various application domains in the form of Jupyter notebooks. 6) Metrics: We will briefly showcase the two explainability metrics currently available through the toolkit. 7) Future Directions: The final segment will be to discuss future directions and how participants can contribute to the toolkit.",FAT*,2020.0,211041430,10.1145/3351095.3375667,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
6a2a534a9d01b76ecb5caba20032022a904cb50d,https://www.semanticscholar.org/paper/6a2a534a9d01b76ecb5caba20032022a904cb50d,COSMIC Semantic Segmentation Framework,"Deep space missions such as the Mars Reconnaissance Orbiter collect more data than can be sent back to Earth due to limited communications bandwidth. Machine learning algorithms can be deployed on board orbiters to prioritize the downlink of scientifically interesting images, such as those including fresh impact craters, recurring slope lineae, or dust devils. However, basic machine learning research is necessary to boost realworld performance, and numerous possible convolutional neural network architectures must be evaluated in terms of accuracy and compute requirements. A framework is designed to reduce redundant development, to standardize the algorithm testing process, and to allow developers to focus on the implementation details of novel machine learning algorithms. Three convolutional neural network implementations are included with the framework, pending use in future research. 1Content-based Onboard Summarization to Monitor Infrequent Change 2CL #18-465",,2019.0,212733017,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
a6c7b3c141f6c604e08fee902fa268665b8a4221,https://www.semanticscholar.org/paper/a6c7b3c141f6c604e08fee902fa268665b8a4221,Simple linear classifiers via discrete optimization: learning certifiably optimal scoring systems for decision-making and risk assessment,"Scoring systems are linear classification models that let users make quick predictions by adding, subtracting, and multiplying a few small numbers. These models are widely used in applications where humans have traditionally made decisions because they are easy to understand and validate. In spite of extensive deployment, many scoring systems are still built using ad hoc approaches that combine statistical techniques, heuristics, and expert judgement. Such approaches impose steep trade-offs with performance, making it difficult for practitioners to build scoring systems that will be used and accepted. In this dissertation, we present two new machine learning methods to learn scoring systems from data: Supersparse Linear Integer Models (SLIM) for decision-making applications; and Risk-calibrated Supersparse Linear Integer Models (RiskSLIM) for risk assessment applications. Both SLIM and RiskSLIM solve discrete optimization problems to learn scoring systems that are fully optimized for feature selection, small integer coefficients, and operational constraints. We formulate these problems as integer programming problems and develop specialized algorithms to recover certifiably optimal solutions with an integer programming solver. We illustrate the benefits of this approach by building scoring systems for realworld problems such as recidivism prediction, sleep apnea screening, ICU seizure prediction, and adult ADHD diagnosis. Our results show that a discrete optimization approach can learn simple models that perform well in comparison to the state-ofthe-art, but that are far easier to customize, understand, and validate. Thesis Supervisor: Cynthia Rudin Title: Associate Professor of Computer Science Duke University",,2017.0,4889054,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
c720b7dbbc679d1083aacd49e397ec167f81a1e6,https://www.semanticscholar.org/paper/c720b7dbbc679d1083aacd49e397ec167f81a1e6,Privacy-Preserving Scoring of Tree Ensembles: A Novel Framework for AI in Healthcare,"Machine Learning (ML) techniques now impact a wide variety of domains. Highly regulated industries such as healthcare and finance have stringent compliance and data governance policies around data sharing. Advances in secure multiparty computation (SMC) for privacy-preserving machine learning (PPML) can help transform these regulated industries by allowing ML computations over encrypted data with personally identifiable information (PII). Yet very little of SMC-based PPML has been put into practice so far. In this paper we present the very first framework for privacy-preserving classification of tree ensembles with application in healthcare. We first describe the underlying cryptographic protocols that enable a healthcare organization to send encrypted data securely to a ML scoring service and obtain encrypted class labels without the scoring service actually seeing that input in the clear. We then describe the deployment challenges we solved to integrate these protocols in a cloud based scalable risk-prediction platform with multiple ML models for healthcare AI. Included are system internals, and evaluations of our deployment for supporting physicians to drive better clinical outcomes in an accurate, scalable, and provably secure manner. To the best of our knowledge, this is the first such applied framework with SMC-based privacy-preserving machine learning for healthcare.",2018 IEEE International Conference on Big Data (Big Data),2018.0,54059829,10.1109/BigData.2018.8622627,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
b0ed25630d66d052bff2a9218d141a24b19e4d68,https://www.semanticscholar.org/paper/b0ed25630d66d052bff2a9218d141a24b19e4d68,THE BENEFITS OF APPLYING AI TO COMPRESSION,"Artificial intelligence (AI) is a popular subject today. Currently used across various verticals, from medicine to autonomous vehicles and finance, it is projected to have a significant impact. Today, AI is used for video compression, not just to provide bitrate savings but also to improve the quality of experience (QoE) and savings in processing power. This paper will present three applications of AI for video compression, explaining how each helps with the delivery of video content over broadcast and OTT networks. The applications that will be examined include Dynamic Encoding Style (DES), which enables a better trade-off between video quality and bitrate; Dynamic Resolution Encoding (DRE), which enables a superior QoE and density; and Dynamic Frame Rate Encoding (DFE), which allows for improved density and QoE. After a brief presentation of the methods, the paper will then present the results of implementing these technologies in the real world. INTRODUCTION Video compression for broadcast TV services started more than 20 years ago. Over time, several key improvements, such as dual-pass, statistical multiplexing, and software migration, were made to compression technology in order to boost performance. Artificial Intelligence (AI) is driving the next frontier of video compression enhancements. AI is effective at detecting objects and at surveillance. Machines are capable of detecting cancer cells with excellent accuracy, which can be a great help for medical doctors (1, 2). AI algorithms can also be useful at processing a lot of data. Some companies use it to clean large data sets, an activity called data wrangling. More and more, AI can be used for decision-making. The autonomous vehicle collapses many of these uses. Indeed, detection is important in an autonomous car, as other vehicles, persons, objects, and signs on the road need to be clearly identified along with their motion. Together with the internals of the car, it becomes a lot of data to process. The autonomous car has to constantly make decisions about the speed, direction, signaling, and more. In other terms, AI is very effective at predictions (3). More details on the evolution from human-designed algorithm to using AI for live video compression can be found in (10). In the VOD encoding domain, Netflix has been the pioneer in developing an AI-based system to assist file encoding, known as per-title or per-chunk encoding (4). Those techniques only apply to offline encoding and cannot be used for live video. This paper presents three examples of AI applied to live video encoding to optimize broadcast and OTT content delivery. The first three sections present the three examples. For each example, the paper presents a brief presentation of the methods followed by the results, including real-life effects. In this paper, both “AI” and “machine learning” expressions are used, knowing that machine learning is, in fact, a part of AI. DYNAMIC ENCODING STYLE (DES) OR CONTENT-AWARE ENCODING (CAE) FOR BITRATE SAVINGS In this first application, the video compression algorithm itself has improved thanks to machine learning technology. The goal is to improve the video quality/bitrate trade-off, meaning reducing the bitrate while maintaining the video quality or keeping a bitrate and improving the video quality. This is done by the means of encoding styles. Encoding styles are compression algorithm configurations well-suited for particular content. Results DES has been thoroughly tested across a lot of material, and it has shown a bitrate reduction vs. deployed system from 20% up to 30% on VBR content in broadcast applications, and 35% on average up to 50% compared with CBR for streaming applications. Table 1 shows the comparison of the AI-based algorithm with the deployed solution for a customer’s use case. The AI-based algorithm is run at different lower bitrates compared with the deployed solution, from 10% to 30% lower. At 10% the AI-based algorithm is better, at 20% it is equal and at 30% it is worse. The last two columns provide a comparison of lowering the bitrate for both algorithms for verification purposes. The conclusion is that the AI-based algorithm provides a 20% gain. Prog Channel AI version Pool bitrate -10% AI version Pool bitrate -20% AI version Pool bitrate -30% Both versions Pool bitrate -10% Both versions Pool bitrate -20% 1 Documentary = AI slightly lower AI lower AI better AI better 2 Cartoon = = AI lower = = 3 General Entertainment = = AI lower = AI slightly better 4 Movie = = AI slightly lower = AI slightly better 5 Sport AI better = AI lower AI better AI better 6 High action shows AI better AI slightly better = AI better AI better Table 1 – Video quality comparison on different channels between deployed and AI-based algorithm DES and CAE have been deployed in many streaming situations, with some examples and results shown below. The first example is a large streaming service with more than 1 million subscribers and more than 50 channels. This service supports live, VOD, cloud DVR, time-shift and serverside dynamic ad insertion. Due to the COVID-19 global health crisis, the service provider observed a dramatic increase in the bandwidth use and needed a solution to relieve the pressure without changing its infrastructure. By turning on DES and CAE the service provider saw significant improvements on their network. The backbone traffic was reduced by 50%, and the CDN peak usage was reduced by 30%. Figure 1 shows the backbone traffic reduction after DES/CAE was activated. Figure 1 Backbone traffic reduction thanks to DES and CAE The second example involves a large European streaming provider. The measurements were also made during the lockdown period due to COVID-19. In this example we show the average bitrate variation between normal compression and with DES/CAE turned on. For sports content, a bitrate reduction of 30% was measured, and for studio content a bitrate reduction of 40% was observed. Studio content includes television programs, such as talk shows and games shows. Figure 2 Studio content average bitrate reduction thanks to DES/CAE DES/CAE Activated",,2021.0,245443415,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
b1ec0fa804ad9d2b32678ab5286c6f1861cd3e2a,https://www.semanticscholar.org/paper/b1ec0fa804ad9d2b32678ab5286c6f1861cd3e2a,Editors' foreword,"During the last decade we have witnessed a rapid expansion of artificial intelligence (AI) applications and use of machine learning (ML) algorithms in an increasingly broad range of problems in finance. This development is fueled by a unique confluence of factors: an exponentially growing computational capacity that is available for enterprises, and similarly exponential growth in the amount of machine-readable data, along with improvements in the state of the art which allow ML and AI applications that were impractical ten or twenty years ago. There is an ambitious feeling emerging across industry and academia that some cognitive processes can be automated via ML and AI, radically expanding opportunities for automation especially in finance and the services industry. However, finance is very different from other domains, such as medical diagnosis, where ML and AI have been developed and successfully deployed. ML and AI require large and reliable training data sets to make machines ‘learn’ their models. But all financial data are not alike. We can characterize financial information as big data because of its large volume (financial time series data easily scales into petabytes), velocity (much of financial data is high-frequency), and variety (numerical, categorical, text, images, etc.). This data exhibits complex behavior: nonstationarity, nonlinear interactions, heteroscedasticity, and biases. The research goal in this domain is to find in this data relevant patterns that could be used for investment, risk management or trading decisions. Time series analysis and traditional statistics can facilitate the process of understanding, modeling, and forecasting the behavior of financial assets. Present day developments of ML and AI algorithms provide novel approaches and perspectives such as feature selection in high dimensional data that mixes large structured and unstructured datasets, and incorporates a",Quantitative Finance,2019.0,219715362,10.1080/14697688.2019.1638160,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
41721867d39baca59ccdabe6792f251e24a5a2d4,https://www.semanticscholar.org/paper/41721867d39baca59ccdabe6792f251e24a5a2d4,Neural Network Based Prediction Model for Job Applicants,"Predictive analytics, a division of the advanced analytics that uses various techniques like machine learning, data mining and so on, to predict the future events. Predictive analytics is summarized with the data collection, modelling, statistics and deployment. It can be used to predict
 the future possibilities in different areas like business, healthcare, telecom, finance. An effective technique for prediction is Artificial Neural Network. The model accuracy for prediction can be enhanced using neural networks. The model can also be used easily for prediction of output parameters
 because of its ability to solve the complex computation which are difficult to be solved by other techniques. In this paper, a brief review of Artificial Neural Network used for prediction analysis is presented with various techniques like Multi-Layer Perceptron, T-S Fuzzy Neural Networks,
 Support Vector Machine, Radial Basis Function Network, Levenberg-Marquardt Algorithm and Back Propagation and their applications are also presented. This paper also presents the neural network-based prediction model for job applicants which is used to predict the jobs of various applicants
 based on certain parameter ratings.",Journal of Computational and Theoretical Nanoscience,2019.0,209942517,10.1166/jctn.2019.8263,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
b5c9db485ba7b484125f74b1ac3b341619a11a23,https://www.semanticscholar.org/paper/b5c9db485ba7b484125f74b1ac3b341619a11a23,Domain Specific Concept Drift Detectors for Predicting Financial Time Series,"Concept drift detectors allow learning systems to maintain good accuracy on non-stationary data streams. Financial time series are an instance of non-stationary data streams whose concept drifts (market phases) are so important to affect investment decisions worldwide. This paper studies how concept drift detectors behave when applied to financial time series. General results are: a) concept drift detectors usually improve runtime over continuous learning, b) their computational cost is usually a fraction of the learning and prediction steps of even basic learners, c) it is important to study concept drift detectors in combination with the learning systems they will operate with, and d) concept drift detectors can be directly applied to the time series of raw financial data and not only to the model’s accuracy one. Moreover, the study introduces three simple concept drift detectors, tailored to financial time series, and shows that two of them can be at least as effective as the most sophisticated ones from the state of the art when applied to financial time series. Impact Statement—The finance industry exploits machine learning for the analysis of financial data. Financial time series are affected by concept drifts: their most evident manifestations happen when a market rotates among its bear, bull or stagnant/flat phases. The prompt detection of a new market phase could allow financial companies to timely retune the computational systems controlling their investment decisions. This study shows the ability of concept drift detectors to recognize market phases in several operating conditions and could impact on how the finance industry deploys learning systems. In particular, three simple concept drift detectors, tailored to financial time series, are introduced and it is shown that two of them can be very effective when applied to financial time series.",ArXiv,2021.0,232380379,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
f5c6af6cc5a545a24188e8e4198af9df66cf546b,https://www.semanticscholar.org/paper/f5c6af6cc5a545a24188e8e4198af9df66cf546b,Stock market analysis using social networks,"Nowadays, the use of social media has reached unprecedented levels. Among all social media, with its popular micro-blogging service, Twitter enables users to share short messages in real time about events or express their own opinions. In this paper, we examine the effectiveness of various machine learning techniques on retrieved tweet corpus. A machine learning model is deployed to predict tweet sentiment, as well as gain an insight into the correlation between twitter sentiment and stock prices. Specifically, that correlation is acquired by mining tweets using Twitter's search API and process it for further analysis. To determine tweet sentiment, two types of machine learning techniques are adopted including Naïve Bayes classification and Support vector machines. By evaluating each model, we discover that support vector machine gives higher accuracy through cross validation. After predicting tweet sentiment, we mine historical stock data using Yahoo finance API, while the designed feature matrix for stock market prediction includes positive, negative, neutral and total sentiment score and stock price for each day. In order to capturing the correlation situation between tweet opinions and stock market prices, hence, evaluating the direct correlation between tweet sentiments and stock market prices, the same machine learning algorithm is implemented for conducting our empirical study.",ACSW,2018.0,3460820,10.1145/3167918.3167967,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
8061d521fe7c62430e1ba2cb0b8f4d19f7366e6f,https://www.semanticscholar.org/paper/8061d521fe7c62430e1ba2cb0b8f4d19f7366e6f,Multi Uav Cooperative Surveillance With Spatio Temporal,"Deep Learning for Unmanned SystemsMultiple Heterogeneous Unmanned Aerial VehiclesAdvanced Mobile RoboticsSafe Robot Navigation Among Moving and Steady ObstaclesComputer Safety, Reliability, and SecurityAdvances in Swarm IntelligenceHolonic and Multi-Agent Systems for ManufacturingAdvances in Artificial Intelligence and Applied Cognitive ComputingUnmanned Aircraft SystemsIntelligent Computing Theories and ApplicationAutonomous Airborne Wireless NetworksAd Hoc NetworksEnabling Blockchain Technology for Secure Networking and CommunicationsUAV Sensors for Environmental MonitoringUnmanned Aerial Vehicles: Breakthroughs in Research and PracticeComputational Collective IntelligenceTime-Critical Cooperative Control of Autonomous Air VehiclesAdvances in Cooperative Control and OptimizationCooperative Robots and Sensor Networks 2015Artificial Intelligence and SecurityPRICAI 2016: Trends in Artificial IntelligenceClosing the Gap Between Research and Field Applications for Multi-UAV Cooperative MissionsMulti-rotor Platform Based UAV SystemsProceedings of the Future Technologies Conference (FTC) 2020, Volume 1Unmanned Aerial SystemsAdvanced Distributed Consensus for Multiagent SystemsCooperative Control of MultiAgent SystemsMulti-UAV Planning and Task AllocationMobile Internet SecurityCooperative Control of Multiple Unmanned Aerial Vehicles with Application to Forest Fire Detection and FightingMulti UAV Systems with Motion and Communication ConstraintsIntelligent Autonomy of UAVsIntelligent and Fuzzy Techniques in Big Data Analytics and Decision MakingIntelligent Autonomy of UAVsUAV Cooperative Decision and ControlCooperative Localization and NavigationAdvances in Guidance, Navigation and ControlMachine Learning and Intelligent CommunicationsUnmanned Aerial VehiclesThe Cognitive Approach in Cloud Computing and Internet of Things Technologies for Surveillance Tracking Systems Ad hoc networks, which include a variety of autonomous networks for specific purposes, promise a broad range of civilian, commercial, and military applications. These networks were originally envisioned as collections of autonomous mobile or stationary nodes that dynamically auto-configure themselves into a wireless network without relying on any existing network infrastructure or centralized administration. With the significant advances in the last decade, the concept of ad hoc networks now covers an even broader scope, referring to the many types of autonomous wireless networks designed and deployed for a specific task or function, such as wireless sensor networks, vehicular networks, home networks, and so on. In contrast to the traditional wireless networking paradigm, such networks are all characterized by sporadic connections, highly error-prone communications, distributed autonomous operation, and fragile multi-hop relay paths. The new wireless networking paradigm necessitates reexamination of many established concepts and protocols, and calls for developing a new understanding of fundamental problems such as interference, mobility, connectivity, capacity, and security, among others. While it is essential to advance theoretical research on fundamental and practical research on efficient policies, algorithms and protocols, it is also critical to develop useful applications, experimental prototypes, and real-world deployments to achieve an immediate impact on society for the success of this wireless networking paradigm.A comprehensive review of the state of the art in the control of multi-agent systems theory and applications The superiority of multi-agent systems over single agents for the control of unmanned air, water and ground vehicles has been clearly demonstrated in a wide range of application areas. Their large-scale spatial distribution, robustness, high scalability and low cost enable multi-agent systems to achieve tasks that could not successfully be performed by even the most sophisticated single agent systems. Cooperative Control of Multi-Agent Systems: Theory and Applications provides a wide-ranging review of the latest developments in the cooperative control of multi-agent systems theory and applications. The applications described are mainly in the areas of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). Throughout, the authors link basic theory to multi-agent cooperative control practice — illustrated within the context of highly-realistic scenarios of high-level missions — without losing site of the mathematical background needed to provide performance guarantees under general working conditions. Many of the problems and solutions considered involve combinations of both types of vehicles. Topics explored include target assignment, target tracking, consensus, stochastic game theory-based framework, event-triggered control, topology design and identification, coordination under uncertainty and coverage control. Establishes a bridge between fundamental cooperative control theory and specific problems of interest in a wide range of applications areas Includes example applications from the fields of space exploration, radiation shielding, site clearance, tracking/classification, surveillance, search-and-rescue and more Features detailed presentations of specific algorithms and application frameworks with relevant commercial and military applications Provides a comprehensive look at the latest developments in this rapidly evolving field, while offering informed speculation on future directions for collective control systems The use of multi-agent system technologies in both everyday commercial use and national defense is certain to increase tremendously in the years ahead, making this book a valuable resource for researchers, engineers, and applied mathematicians working in systems and controls, as well as advanced undergraduates and graduate students interested in those areas.Time-Critical Cooperative Control of Autonomous Air Vehicles presents, in an easy-to-read style, the latest research conducted in the industry, while also introducing a set of novel ideas that illuminate a new approach to problem-solving. The book is virtually self-contained, giving the reader a complete, integrated presentation of the different concepts, mathematical tools, and control solutions needed to tackle and solve a number of problems concerning time-critical cooperative control of UAVs. By including case studies of fixed-wing and multirotor UAVs, the book effectively broadens the scope of application of the methodologies developed. This theoretical presentation is complemented with the results of flight tests with real UAVs, and is an ideal reference for researchers and practitioners from academia, research labs, commercial companies, government workers, and those in the international aerospace industry. Addresses important topics related to time-critical cooperative control of UAVs Describes solutions to the problems rooted in solid dynamical systems theory Applies the solutions developed to fixed-wing and multirotor UAVs Includes the results of field tests with both classes of UAVsThis book provides the state-of-the-art intelligent methods and techniques for solving realworld problems along with a vision of the future research. The fifth 2020 Future Technologies Conference was organized virtually and received a total of 590 submissions from academic pioneering researchers, scientists, industrial engineers, and students from all over the world. The submitted papers covered a wide range of important topics including but not limited to computing, electronics, artificial intelligence, robotics, security and communications and their applications to the real world. After a double-blind peer review process, 210 submissions (including 6 poster papers) have been selected to be included in these proceedings. One of the meaningful and valuable dimensions of this conference is the way it brings together a large group of technology geniuses in one venue to not only present breakthrough research in future technologies, but also to promote discussions and debate of relevant issues, challenges, opportunities and research findings. The authors hope that readers find the book interesting, exciting and inspiringAdvanced Distributed Consensus for Multiagent Systems contributes to the further development of advanced distributed consensus methods for different classes of multiagent methods. The book expands the field of coordinated multiagent dynamic systems, including discussions on swarms, multi-vehicle and swarm robotics. In addition, it addresses advanced distributed methods for the important topic of multiagent systems, with a goal of providing a high-level treatment of consensus to different versions while preserving systematic analysis of the material and providing an accounting to math development in a unified way. This book is suitable for graduate courses in electrical, mechanical and computer science departments. Consensus control in multiagent systems is becoming increasingly popular among researchers due to its applicability in analyzing and designing coordination behaviors among agents in multiagent frameworks. Multiagent systems have been a fascinating subject amongst researchers as their practical applications span multiple fields ranging from robotics, control theory, systems biology, evolutionary biology, power systems, social and political systems to mention a few. Gathers together the theoretical preliminaries and fundamental issues related to multiagent systems and controls Provides coherent results on adopting a multiagent framework for critically examining problems in smart microgrid systems Presents advanced analysis of multiagent systems under cyberphysical attacks and develops resilient control strategies to guarantee safe operationComplete with online files and updates, this cutting-edge text looks at the next generation of unmanned flying machines. Aerial robots can be considered as an evolution of the Unmanned Aerial Vehicl",,2021.0,236965223,,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
2cc3338709ea9c14ff422025ae4a8ad09f9598ba,https://www.semanticscholar.org/paper/2cc3338709ea9c14ff422025ae4a8ad09f9598ba,Explainable AI: from black box to glass box,,Journal of the Academy of Marketing Science,2019.0,210119147,10.1007/s11747-019-00710-5,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
aa30901b02e9923851d507e1f2726bcf9ef1b068,https://www.semanticscholar.org/paper/aa30901b02e9923851d507e1f2726bcf9ef1b068,OccuSpace: Towards a Robust Occupancy Prediction System for Activity Based Workplace,"Workplace occupancy detection is becoming increasingly important in large Activity Based Work (ABW) environments as it helps building and office management understand the utilisation and potential benefits of shared workplace. However, existing sensor-based technologies detect workstation occupancy in indoor spaces require extensive installation of hardware and maintenance incurring ongoing costs. Moreover, accuracy can depend on the specific seating styles of workers since the sensors are usually placed under the table or overhead. In this research, we provide a robust system called OccuSpace to predict occupancy of different atomic zones in large ABW environments. Unlike fixed sensors, OccuSpace uses statistical features engineered from Received Signal Strength Indicator (RSSI) of Bluetooth card beacons carried by workers while they are within the ABW environment. These features are used to train state-of-the-art machine learning algorithms for prediction task. We setup the experiment by deploying our system in a realworld open office environment. The experimental results show that OccuSpace is able to achieve a high accuracy for workplace occupancy prediction.",2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),2019.0,174817291,10.1109/PERCOMW.2019.8730762,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
f2baccfce5f415c660cbf7bc9d651cfdf2aea5a7,https://www.semanticscholar.org/paper/f2baccfce5f415c660cbf7bc9d651cfdf2aea5a7,Chapter 13:Computational Discovery of Organic LED Materials,"Automation, software and machine learning are enabling a data-driven revolution in areas such as self-driving cars, logistics, manufacturing and finance. In this chapter, we describe how these tools are being combined for computer-driven discovery of thermally activated delayed fluorescence materials. We analyze the increasingly automated deployment of robust and accurate computer simulations to assess candidate molecules virtually and identify leads for experimental characterization. Recent advances in machine learning techniques to accelerate the screening process and to increase its accuracy are also described. The role of user-experience and custom experiment–theory interaction tools are described. Finally, we report how these computer-based efforts have resulted in novel high organic light-emitting diode materials.",,2018.0,10.1039/9781788010122-00423,88473079,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
d15a13a15d74ce0168293c6477d1eb37c5692744,https://www.semanticscholar.org/paper/d15a13a15d74ce0168293c6477d1eb37c5692744,Consumer Law and Artificial Intelligence: Challenges to the EU Consumer Law and Policy Stemming from the Business' Use of Artificial Intelligence - Final report of the ARTSY project,"Potential regulation of use of artificial intelligence by business should minimize the risks for consumers and the society without impeding the possible benefits. To do so, we argue, the legal reaction should be grounded in an empirical analysis and proceed case-by-case, bottom-up, as a series of responses to concrete research questions. The ambition of this report has been to commence and facilitate that process. We extensively document and evaluate the market practice of the corporate use of AI, map the scholarly debates about (consumer) law and artificial intelligence, and present a list of twenty five research questions which, in our opinion, require attention of regulators and academia. The report is divided into four sections. The first explains our understanding of the concepts of “artificial intelligence” (a set of socio-technological practices enabled by machine learning and big data) and “consumer law” (various legal instruments concretizing the principles of the weaker party protection, non-discrimination, regulated autonomy and consumer privacy). The second section documents the ways in which the business uses artificial intelligence in seven sectors of the economy: finance and insurance, information services, energy and “smart solutions”, retail, autonomous vehicles, healthcare and legal services. For each analyzed sector we study the gains for the businesses stemming from the deployment of AI, the potential gains, but also challenges for consumers, as well as third party effects. In the third section, we repeat the analysis through the lens of four general “uses” of AI by businesses in various sectors: knowledge generation, automated decision making, advertising and other commercial practices and personal digital assistants. Finally, in the fourth section, we present the questions which we believe should be addressed in the next stage of the research. We cluster them into: normative questions about regulatory goals, technological and governance questions about regulatory means, and theoretical questions about concepts and preconceptions.",,2018.0,10.2139/SSRN.3228051,169539509,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
37a76a01d91dc9026a738357b7d79276ef8855d0,https://www.semanticscholar.org/paper/37a76a01d91dc9026a738357b7d79276ef8855d0,Do Not Pull My Data for Resale: Protecting Data Providers Using Data Retrieval Pattern Analysis,"Data providers have a profound contribution to many fields such as finance, economy, and academia by serving people with both web-based and API-based query service of specialized data. Among the data users, there are data resellers who abuse the query APIs to retrieve and resell the data to make a profit, which harms the data provider's interests and causes copyright infringement. In this work, we define the ""anti-data-reselling"" problem and propose a new systematic method that combines feature engineering and machine learning models to provide a solution. We apply our method to a real query log of over 9,000 users with limited labels provided by a large financial data provider and get reasonable results, insightful observations, and real deployments.",SIGIR,2018.0,10.1145/3209978.3210158,49641791,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
bc3f75aed2902303aeaa4118e1ac157fbc8cfbe0,https://www.semanticscholar.org/paper/bc3f75aed2902303aeaa4118e1ac157fbc8cfbe0,Blockchain-based Smart P2P Lending using Neural Networks,"Over the past decade, there has been an exponential growth in the number and scale of online lending and crowdfunding platforms. However, these platforms lack a reliable and transparent metric to predict the credit-worthiness of an applicant. They also have a single point of failure and are vulnerable to certain security issues. This paper proposes a Blockchain-based decentralized lending platform that uses deep learning to predict the risk associated with an applicant. The paper also discusses how such a system can be implemented and deployed. The experimental results show how ensemble training can help lower the bias of individual neural networks and provide better predictions for this use case. General Terms Machine Learning, Artificial Intelligence, Finance, Lending",,2018.0,10.5120/ijca2018916888,56356584,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
9b43dade854555fdd06485a904bed119dbd77db6,https://www.semanticscholar.org/paper/9b43dade854555fdd06485a904bed119dbd77db6,An Exploratory Study of Sequence Alignment for Improved Sensor-Based Human Activity Recognition,"Sequence alignment (SA) is a well-established technique in bioinformatics for analyzing deoxyribonucleic acid (DNA), ribonucleic acid (RNA), or protein sequences and identifying regions of similarity. The main goal of SA is to discover relationships between strings of data by deploying a series of heuristic or probabilistic methods to align a new string (e.g., DNA of a new species) with an existing string (DNA of a known species). SA has also been used sporadically in linguistics, social sciences, and finance. In this paper, the authors explore the prospect of coupling machine learning (ML) and SA to improve the output of human activity recognition (HAR) methods. In particular, several field experiments are conducted to collect heterogeneous human motion data via wearable sensors. Collected data is further mined using ML to identify sequences of activities performed in each experiment. Given the inaccuracy of sensor readings and the limitations of ML algorithms especially in handling datasets from complex human activities such as those performed by construction workers, it is expected that the resulting activity sequences not fully match actual activity sequences as observed in the field. To further clean up this inherent noise, SA is deployed to refine imperfections in the resulting activity sequences by manipulating the output of HAR and ultimately aligning noisy activity sequences with ground truth sequences. The outcome of this work is a systematic method to improve the reliability of HAR from sensor readings, which can benefit decision-making as related to task planning, resource management, productivity monitoring, and ergonomic assessment. INTRODUCTION The sequence alignment (SA) technique was first developed during the 1980s by biochemists. SA primarily relies on a series of applied mathematical algorithms (Sankoff and Kruskal 1983) for holistic sequential analyses that could provide insight into long sequences of protein and deoxyribonucleic acid (DNA). The use of SA was expanded to other domains in the late 1990s (Abbott and Tsay 2000; Wilson 1998) primarily by social scientists (Abbott and Forrest 1986) to advance the analysis of socio-economic data by producing normalized data trends and comparing each data Construction Research Congress 2018 347",,2018.0,10.1061/9780784481264.034,70219520,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
fd9e632b6e9d761909847439bd3598b6f880af7b,https://www.semanticscholar.org/paper/fd9e632b6e9d761909847439bd3598b6f880af7b,Augmented Intelligence: Enhancing the Roles of Health Actuaries and Health Economists for Population Health Management.,"Achieving desired population health and business outcomes requires new levels of productivity, efficiency, and risk minimization. Machine learning (ML), cognitive computing, natural language processing, and other augmented intelligence (AI) tools are increasingly applied to health care initiatives. AI applies to a plethora of artificial intelligence technologies and tools currently used in health care. AI implies any kind of modeling that supplements humans, augmenting human intelligence, not replacing it. Health care decision makers are embracing AI, thereby leveraging information from multiple sources to enable improved individual and population outcomes. ML, for example, applies algorithms and decision-making tools to enormous amounts of clinical data from electronic health records, pharmaceutical databases, and unstructured text data. AI analytics are in predictive risk assessment, clinical decision support, home health monitoring, finance, and resource allocation. These applications are integral to population health management (PHM), Accountable Care Organizations (ACOs), Medicare Advantage, and public health initiatives. A recent hospital survey suggests a promising future for AI that is particularly relevant to PHM. The survey found that, of 7 applications, AI is likely to have the greatest initial impact on population health (24%), clinical decision support (20%), and patient diagnostic tools (20%), followed by precision medicine (14%), and hospital/physician workflow (8%). A variety of AI applications are being tested for health care. But growth and promise can only be met if the additional information is accessible and useful to CEOs and key policy makers. Clinical informatics underlies efforts to improve quality of care and PHM. Considerable work is being done by data scientists, such as Google/Sanofi’s joint venture, Onduo, that will collect and monitor real-time data on people with diabetes. This initiative, like many others, will fail unless there is a way to interpret the data, identify opportunities for intervention, and deliver the information in real time to clinicians and patients. Results are likely to be suboptimal if complex health care AI applications rely only on data scientists. Even with excellent abilities, most data scientists lack industry knowledge and the particular skills of health actuaries (HAs) and health economists (HEs), such as risk analysis and behavioral economics. HA and HE expertise is vital to maximizing value from the use of AI. These players guide health care business decisions, policy making and operations improvements, creating useful outputs for pricing, coverage decisions, business advances, and policy making by applying sophisticated statistical modeling and analytic tools. Although there is overlap between the abilities of actuaries and economists (Table 1), they bring distinct and essential skill sets to an analytic team. AI has been called the world’s most valuable resource as it allows users to extract increasing, ongoing actionable insights and value from myriad sources and uses of data. HAs and HEs can leverage AI to provide expanded and more specific guidance to payers, providers, suppliers, programs, and health systems. For example, AI-enhanced HA expertise helps ACOs to assess and pinpoint risk under capitated contracts. HEs can incorporate the HA evaluation and other AI-enabled analytics to recommend the optimal deployment of care management resources to achieve contracted clinical and financial outcomes. Blending PHM and AI-driven precision medicine could yield a new health care services paradigm. The shift to value-based care has increased the risk borne by providers significantly and increases pressure to identify and manage risk and evaluate outcomes. Innovative use of AI may help identify short-term clinical goals, reduce risk to increase savings, or serve as a return on investment indicator. Opportunities to enhance PHM exist with predictive modeling for identifying high-risk patients. AI tools aid prediction and help identify patients with multiple chronic conditions who are moving across disease states/trajectories that are often associated with increased resource use and cost. Gaps exist. Predictive modeling is hampered by missing pieces – what works and how it works – and gaps in comprehensive information. The common practice of segmenting populations by specific conditions (eg, diabetes) is highly inefficient because most people have multiple chronic conditions. Although HAs use predictive modeling",Population health management,2018.0,10.1089/pop.2017.0146,34878270,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
624edeb17b1871bc3c199f141aaafa3281b5967f,https://www.semanticscholar.org/paper/624edeb17b1871bc3c199f141aaafa3281b5967f,Secure and Automated Enterprise Revenue Forecasting,"
 
 Revenue forecasting is required by most enterprises for strategic business planning and for providing expected future results to investors. However, revenue forecasting processes in most companies are time-consuming and error-prone as they are performed manually by hundreds of financial analysts. In this paper, we present a novel machine learning based revenue forecasting solution that we developed to forecast 100% of Microsoft's revenue (around $85 Billion in 2016), and is now deployed into production as an end-to-end automated and secure pipeline in Azure. Our solution combines historical trend and seasonal patterns with additional information, e.g., sales pipeline data, within a unified modeling framework. In this paper, we describe our framework including the features, method for hyperparameters tuning of ML models using time series cross-validation, and generation of prediction intervals. We also describe how we architected an end-to-end secure and automated revenue forecasting solution on Azure using Cortana Intelligence Suite. Over consecutive quarters, our machine learning models have continuously produced forecasts with an average accuracy of 98-99 percent for various divisions within Microsoft's Finance organization. As a result, our models have been widely adopted by them and are now an integral part of Microsoft's most important forecasting processes, from providing Wall Street guidance to managing global sales performance.
 
",AAAI,2018.0,10.1609/aaai.v32i1.11385,19239271,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
a812368fe1d4a186322bf72a6d07e1cf60067234,https://www.semanticscholar.org/paper/a812368fe1d4a186322bf72a6d07e1cf60067234,Gaussian processes for modeling of facial expressions,"Automated analysis of facial expressions has been gaining significant attention over the past years. This stems from the fact that it constitutes the primal step toward developing some of the next-generation computer technologies that can make an impact in many domains, ranging from medical imaging and health assessment to marketing and education. No matter the target application, the need to deploy systems under demanding, realworld conditions that can generalize well across the population is urgent. Hence, careful consideration of numerous factors has to be taken prior to designing such a system. The work presented in this thesis focuses on tackling two important problems in automated analysis of facial expressions: (i) view-invariant facial expression analysis; (ii) modeling of the structural patterns in the face, in terms of well coordinated facial muscle movements. Driven by the necessity for efficient and accurate inference mechanisms we explore machine learning techniques based on the probabilistic framework of Gaussian processes (GPs). Our ultimate goal is to design powerful models that can efficiently handle imagery with spontaneously displayed facial expressions, and explain in detail the complex configurations behind the human face in real-world situations. To effectively decouple the head pose and expression in the presence of large outof-plane head rotations we introduce a manifold learning approach based on multi-view learning strategies. Contrary to the majority of existing methods that typically treat the numerous poses as individual problems, in this model we first learn a discriminative manifold shared by multiple views of a facial expression. Subsequently, we perform facial expression classification in the expression manifold. Hence, the pose normalization problem is solved by aligning the facial expressions from different poses in a common latent space. We demonstrate that the recovered manifold can efficiently generalize to various poses and expressions even from a small amount of training data, while also being largely robust to corrupted image features due to illumination variations. State-of-the-art performance is achieved in the task of facial expression classification of basic emotions. The methods that we propose for learning the structure in the configuration of the muscle movements represent some of the first attempts in the field of analysis and intensity estimation of facial expressions. In these models, we extend our multi-view approach to exploit relationships not only in the input features but also in the multi-output labels. The structure of the outputs is imposed into the recovered manifold either from heuristically defined hard constraints, or in an auto-encoded manner, where the structure is learned automatically from the input data. The resulting models are proven to be robust to data with imbalanced expression categories, due to our proposed Bayesian learning of the target manifold. We also propose a novel regression approach based on product of GP experts where we take into account people’s individual expressiveness in order to adapt the learned models on each subject. We demonstrate the superior performance of our proposed models on the task of facial expression recognition and intensity estimation.",,2016.0,10.25560/44106,49320920,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
a58765127a59ca9462abc175c31fb0021f96b2bd,https://www.semanticscholar.org/paper/a58765127a59ca9462abc175c31fb0021f96b2bd,Financial digitalization and its implication on jobs market structure,"Finance digitalization and digital transformation have become the most commonly used words in the last decade, but especially in recent years. The present paper emphasizes that, in order to remain competitive and achieve market longevity, organizational structures and financial services must keep up with the digital transformation. Improving the performance but also the survival of institutions is related to the adoption of innovation and the adoption of digital changes. This article aims to show how digital transformation and the adoption of new technologies have changed the approach of the workplace, the way of doing business and redefined the parameters of the financial products and services offered. The research finds out that the main impediments encountered in digitizing the activity of institutions is the efficient storage and utilization of the database, as well as the qualification and speed of adapting the workforce to the new requirements regarding the provision of innovative administrative and financial products and services through digital technology. In the same line, using macroeconomic indicators from different regions of Europe, related to the movements and new developments of the labor market, economic and social progress trend, the present work paper comes to sound an alarm signal for a new economic and technological education policy approach. The paper underlines the movements of the labor market and the possible solutions to redirect the specialization curriculums in order to equip people with those qualifications and skills which will make them employable on the current digitalized business environment. Introduction Digital solutions and new technologies offer great potential to overcome massive development challenges and can contribute to achieving the goal of universal access to all business services (Moshirian, 2011). Digitization and digital business promise a universe of applications and digitalized assets that are expected to work together to allow rapid development of new capabilities that will give competitive advantage (Moşteanu, 2019a). To remain competitive and achieve longevity in the market, financial services has to keep up with digital transformation. The survival of financial institutions is connected with the adoption of innovation, and embracing digital changes, to improve the efficiency and performance within the organization (Scardovi, 2017). Digital transformation and new technology adoption have changed the way of doing business and channels that offer banking and financial products and services are more intuitive and trustworthy (Mohamed and Ali, 2019). The present work paper purpose is to find out the main impediments encountered in digitizing the activity of institutions, which are the qualification or educational challenges to prepare the workforce for the new job market structure aligned with the new requirements coming from introduction and implementation of digital technology. The reconstruction of organizations design using actual financial technologies imply new educational specializations and developing new skills and competences to fulfill de challenges of different and new job requirements. Literature review Educational institutions and governing practices are increasingly augmented with digital database technologies that function as new kinds of policy instruments. Digital database technologies facilitate the generation, calculation, and circulation of the data required to (Williamson, 2016; Moşteanu et.al., 2020a, b) develop new educational curriculum to prepare people for successful employment in a Finance Digital era. Digital systems are becoming more and more used, representing a much faster, cheaper, and safer way when it comes to financial transactions. Access to modern telecommunications systems is a priority in all countries around the world, as in their evolution, financial and banking systems implement, use and The Business and Management Review, Volume 11 Number 1 August 2020 Conference proceedings of the Centre for Business & Economic Research, ICBED-2020, 20-22 August 306 encourage online services for domestic and international financial transfers (Moşteanu and Faccia, 2020). Digitization and digital transformation have become the most commonly used words in the last decade, but especially in recent years. There is an excess of definitions of this term, used to describe the offline-toonline migration of commercial operations and businesses, including those found in many published research works (Moşteanu, 2020a). Contemporaneous economists defined digitalization as the realignment of, or new investment in, advanced technology and business models to more effectively engage digital customers at every touchpoint in the customer experience lifecycle (Solis, et.al., 2014). The new demands of the labor market require more and more new skills. In this regard, new training companies are developing interactive educational programs for digital instruction for all those who are interested in learning online or at a distance, or developing educationally regarding new digital technologies, so that they can make decisions related to improving work performance. Digital education introduces a healthy corrective to exaggerated techno-optimism or techno-pessimism (Thomas, 2011). Now everything becomes digital: tools, literacy, solutions, data systems, education, generation, and types of markets. Institutional reforms in labor relations are carried out in the digital economy both in the content and in the form. New technologies restructure subject-object economic relations and change qualitatively a business model based on special algorithms application (Vovchenko, et.al., 2017). In this respect organizations and educational system is better to redesign their structure, work processes, and services provided in order to face the challenges of customers – workforce. So rapid are the developments, in fact, that while the digitization of everything has become a hallmark of tech’s promise of individual and business empowerment, it has also begun to prompt anxiety, including among workers who worry about their future in a world of brilliant machines (Muro, et.al., 2017; Moşteanu, 2020b; Moşteanu and Galea 2020; Moşteanu, 2020c). Currently, the European Commission is promoting various initiatives aimed at increasing training in digital skills for the workforce and for consumers; modernizing education across the European Union; harnessing digital technologies for learning and for the recognition and validation of skills; and anticipating and analyzing skills needs (European Commission, 2020). In the digital age, organizations need to reinvent themselves at a structural level and to become agiler (Moşteanu, 2019c). Digital maturity of human resources management implies a shift from traditional paradigm on the workplace towards engagement, learning, and development of employees and search for talent (Mihalcea, 2017; Moşteanu, 2019b; Moşteanu, 2011a). Research methodology The present work paper is exploratory research, based on investigative techniques. It is fundamental and qualitative research, which aims to identify the main impediments encountered in digitizing the activity of institutions, as well as the necessity of adapting the qualification and speed of adapting the workforce to the new requirements regarding the provision of innovative administrative and financial products and services through digital technology. The paper presents the new approach of finance and the necessity to review and adapt the management and supervision of any organization to compete with new digital technologies. The research paper uses macroeconomic indicators from different regions of Europe, related to the movements and new developments of the labor market, economic and social progress trend. Analyze and findings The digital revolution, or third industrial revolution, occurred in the second half of the 20th century (Scholz, 2013). The consequence of this was the automation of work technologies devices, with the insertion of computers, the massive use of the Internet, the development of microprocessors and high-tech communications impacting on society in a universal way, modifying the ways of life of ordinary citizens (Scribano and Lisdero, 2019; Beck, et.al., 1994). Digital transformation has come into wide use in contemporary business media to signify the transformational or disruptive implications of digital technologies for businesses (Nambisan, et.al., 2019) (new business models, new types of products and services, new types of customer experiences) (Boulton, 2018; Boutetiere and Reich, (2018) and more broadly, to indicate how existing companies may need to radically transform themselves to succeed in the emerging digital world (McAfee and Brynjolfsson, 2017; Brynjolfsson, 2011; Rogers, 2016; Venkatraman, The Business and Management Review, Volume 11 Number 1 August 2020 Conference proceedings of the Centre for Business & Economic Research, ICBED-2020, 20-22 August 307 2017). Digital transformation affects all businesses; however, SMEs are giving more attention in offering new digital solutions (Ayandibu and Houghton, 2017). To tackle the digital skills gap, significant investments are needed. In the current European Union’s budget, the Commission proposes coherent and comprehensive support for building up the digital skills needed to support reskilling and upskilling in Europe for a successful digital transformation. Different funds will target different skills needs. The Digital Europe Program with a budget of €700 million will expand the digital talent pool with around 256,000 people who will be able to deploy the latest technology in business throughout Europe. It will focus on three types of actions (European Commission, 2020): Master's Programs in cutting-edge digital technologies developed together with European Union excellence centers in artificial intelligence, cyber an",,2020.0,10.24052/bmr/v11nu01/art-32,225295687,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,https://www.semanticscholar.org/paper/cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,Comparing Human-Robot Proxemics between Virtual Reality and the Real World,"Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of Human-Robot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. Comparing Human-Robot Proxemics between Virtual Reality and the Real World Rui Li KTH Royal Institute of Technology Stockholm, Sweden Rui3@kth.se ABSTRACT Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other.Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. INTRODUCTION Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI) [1][2][3][4]. VR has been used to test teleoperation and collect demonstration data to train machine learning algorithms, which showcased the effectiveness of learning visuomotor skills using data collected by consumer-grade devices [1]. VR teleoperation systems were proposed to crowdsource robotic demonstrations at scale [2]. A VR simulation framework was also proposed to replace the physical robot, as VR can enable high level abstraction in embodiment and multimodal interaction [3]. VR has also been used as a rapid prototyping tool to design in-vehicle interactions and interfaces for self-driving cars, which showed the evocation to genuine responses from test participants [4]. Compared to other HRI experiment methods, VR as an emerging interactive media provides unique advantages. VR HRI has the potential of having higher immersion and fidelity than picture based HRI, video-based HRI and simulated HRI. In situations where the perception of the robot is challenging, compared to on-screen viewing, VR display showed significant improvement on collaborative tasks [5]. When comparing VR HRI to the physical, realworld interaction (Live HRI), there is a trade-off between the two. VR experiences still cannot replace physical experiences due to system limitation, and limited interaction modalities etc. [6]. For example, system limitations such as limited field of view and low display resolution could reduce immersion and presence of the VR experience, resulting in different behaviors from Live experiments. Limited interaction modalities, such as the absence of touch, means that the participant could not feel the robot or even go through the robot, which could potentially break the entire interaction. Figure 1: Photograph of the Live experiment setting However, with the help of the distribution of consumer-grade VR devices and online crowdsourcing platforms, VR HRI has the potential to gain massive data for training robotic behavior and studying HRI related issues. Data collection through VR can also reduce noise and improve the data quality [1], which help to ease data processing and algorithm training. Furthermore, VR HRI experiments can test concepts and interactions without physical robots, making it more resource efficient and less expensive than Live HRI. Less hardware also means that the experiment will be less cumbersome to set up, easier to be reproduced and to ensure experiment quality. In this study, HRI Proxemics (the preferred personal space between a human and a robot) was compared to give a better justification and more basic understanding of the relationship between Live and VR. Proxemics preferences rely on lower level intuition [7], therefore, reflect the differences in the perceptions between Live and VR better. Compared to other HRI subject such as conversational (audio) or gaze behavior (visual), which are more modality dependent, proxemics can give a comprehensive understanding of the human responses. In addition, variations of modalities in VR can greatly influence human perception. For example, a higher visual familiarity of the physical environment in VR can decrease the effect of distance distortion [8]. Auditory inputs play another important role in VR, the addition of spatial sound can increase the sense of presence in VR and provide sound localization [9]. Thus, this work also compares VR settings with variance in modalities to evaluate the impacts of visual familiarity and spatial sound on VR HRI experiments. A 2 x 3 mixed design experiment was conducted to evaluate the differences between Live and VR HRI, as well as the influence of visual familiarity and spatial sound in VR. For the Live HRI, the pepper robot from Softbank Robotics was used (Figure 1). In the VR HRI, a 3D model of the same robot was used. To measure visual familiarity, the VR scene was created in Blender based on a 3D scan of the physical lab. The spatial sound was created by enabling the movement of the physical robot, due to the difficulties of engineering spatial sound. The interaction was implemented in Unity. As an objective measurement for proxemics preference, the minimum comfort distance (MCD) was measured. In addition, for the psychological perception of the experience, the feeling of presence was measured with the SUS questionnaire. For the perception of the robot, two relevant factors, competence and discomfort was measured with the ROSAS questionnaire.",,2018.0,,52210376,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
59e10d1d4cd454635914cfd0ac5160a318fd0473,https://www.semanticscholar.org/paper/59e10d1d4cd454635914cfd0ac5160a318fd0473,UB09 Session 9,"In the domain of Wireless Sensor Networks (WSN), providing an effective security solution to protect the motes and their communications is challenging. Due to the hard constraints on performance, storage and energy consumption, normal network-security related techniques cannot be applied. Focusing on the ""Intrusion Detection"" problem, we propose a realworld application of our WSN Intrusion Detection System (WIDS). WIDS exploits the Weak Process Models to classify potential security issues in the WSN and to notify the operators when an attack tentative is detected. In this demonstration, we show how our IDS works, how it detects some basic attacks and how the IDS can evolve to fullfil the needs of secure WSN deployments. Download Paper (PDF) UB09.2 RESCUE: EDA TOOLSET FOR INTERDEPENDENT ASPECTS OF RELIABILITY, SECURITY AND QUALITY IN NANOELECTRONIC SYSTEMS DESIGN Authors: Cemil Cem Gürsoy1, Guilherme Cardoso Medeiros2, Junchao Chen3, Nevin George4, Josie Esteban Rodriguez Condia5, Thomas Lange6, Aleksa Damljanovic5, Raphael Segabinazzi Ferreira4, Aneesh Balakrishnan6, Xinhui Anna Lai1, Shayesteh Masoumian7, Dmytro Petryk3, Troya Cagil Koylu2, Felipe Augusto da Silva8, Ahmet Cagri Bagbaba8 and Maksim Jenihhin1 1Tallinn University of Technology, EE; 2Delft University of Technology, NL; 3IHP, DE; 4BTU Cottbus-Senftenberg, DE; 5Politecnico di Torino, IT; 6IROC Technologies, FR; 7Intrinsic ID B.V., NL; 8Cadence Design Systems GmbH, DE Abstract The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF)The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF) UB09.3 ASAM: AUTOMATIC SYNTHESIS OF ALGORITHMS ON MULTI CHIP/FPGA WITH COMMUNICATION CONSTRAINTS Authors: Amir Masoud Gharehbaghi, Tomohiro Maruoka, Yukio Miyasaka, Akihiro Goda, Amir Masoud Gharehbaghi and Masahiro Fujita, The University of Tokyo, JP Abstract Mapping of large systems/computations on multiple chips/multiple cores needs sophisticated compilation methods. In this demonstration, we present our compiler tools for multi-chip and multi-core systems that considers communication architecture and the related constraints for optimal mapping. Specifically, we demonstrate compilation methods for multi-chip connected with ring topology, and multi-core connected with mesh topology, assuming fine-grained reconfigurable cores, as well as generalization techniques for large problems size as convolutional neural networks. We will demonstrate our mappings methods starting from data-flow graphs (DFGs) and equations, specifically with applications to convolutional neural networks (CNNs) for convolution layers as well as fully connected layers. Download Paper (PDF) UB09.4 HEPSYCODE-MC: ELECTRONIC SYSTEM-LEVEL METHODOLOGY FOR HW/SW CO-DESIGN OF MIXED-CRITICALITY EMBEDDED SYSTEMS Authors: Luigi Pomante1, Vittoriano Muttillo1, Marco Santic1 and Emilio Incerto2 1Università degli Studi dell'Aquila DEWS, IT; 2IMT Lucca, IT Abstract Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF)Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF) UB09.5 CS: CRAZYSQUARE Authors: Federica Caruso1, Federica Caruso1, Tania Di Mascio1, Alessandro D'Errico1, Marco Pennese2, Luigi Pomante1, Claudia Rinaldi1 and Marco Santic1 1University of L'Aquila, IT; 2Ministry of Education, IT Abstract CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF)CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF) UB09.6 LABSMILING: A SAAS FRAMEWORK, COMPOSED OF A NUMBER OF REMOTELY ACCESSIBLE TESTBEDS AND RELATED SW TOOLS, FOR ANALYSIS, DESIGN AND MANAGEMENT OF LOW DATA-RATE WIRELESS PERSONAL AREA NETWORKS BASED ON IEEE 802.15.4 Authors: Carlo Centofanti, Luigi Pomante, Marco Santic and Walter Tiberti, University of L'Aquila, IT Abstract Low data-rate wireless personal area networks (LR-WPANs) are constantly increasing their presence in the fields of IoT, wearable, home automation, health monitoring. The development, deployment and testing of SW based on IEEE 802.15.4 standard (and derivations, e.g. 15.4e), require the exploitation of a testbed as the network grows in complexity and heterogeneity. This demo shows LabSmiling: a SaaS framework which connects testbeds deployed in a real-world-environment and the related SW tools that make available a meaningful (but still scalable) number of physical devices (sensor nodes) to developers. It provides a comforta",,2019.0,,195809644,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
9e0090f244d3da646aa472691dad616c2e59475f,https://www.semanticscholar.org/paper/9e0090f244d3da646aa472691dad616c2e59475f,Financial forecasting with Neural Networks and Reservoir Computing,"Financial forecasting has emerged as a key field of study in the last decades as budget planning, and decision-making processes play a vital part in establishing and maintaining a healthy and sturdy business. Last years, Machine Learning and Deep Learning’s concepts have become prominent in the financial industry due to their ability to handle vast amounts of data and extract time-dependent patterns from non-linear relationships along with the -nowadaysexponentially growing availability of computing power. More and more industries and business operations tend to deploy artificial intelligence technology to automize their processes, minimize their risks and optimize their development in terms of increasing their revenues as a consequence of high quality and robust productivity. The wide range of Deep Learning usage in the finance industry extends from security and fraud detection, underwriting, stock marketing predictions, and chatbot advisory. [1][2][3] The possible outcomes and variations of Machine Learning applications are fully capable to cover a wide spectrum of needs and ideas and in this work we have developed a theoretical framework to acquire fundamental understanding of the Deep Learning philosophy and concepts, studying a very popular Neural Network type by the name of Recurrent Neural Networks, and another promising one by the name Reservoir Computing and their predictive abilities on time series of data. In addition to this, our work aims to study systematically and evaluate different ways to convert a dataset into a more «friendly» form, to optimize the predicting ability of the models presented. Conversion of data series to stationary sequences proves to be an inevitable process to carry out time-series analysis efficiently and the second chapter of results is delving into this issue. Finally, this work is also enhanced by a short analysis and description of a «shifting» problem we encountered during different evaluation",,,,234755625,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
d91b1b996678425418321d9de5b251778eb506d7,https://www.semanticscholar.org/paper/d91b1b996678425418321d9de5b251778eb506d7,Knowledge engineering with semantic web technologies for decision support systems based on psychological models of expertise,"Machines that provide decision support have traditionally used either a representation of human expertise or used mathematical algorithms. Each approach has its own limitations. This study helps to combine both types of decision support system for a single system. However, the focus is on how the machines can formalise and manipulate the human representation of expertise rather than on data processing or machine learning algorithms. It will be based on a system that represents human expertise in a psychological format. The particular decision support system for testing the approach is based on a psychological model of classification that is called the Galatean model of classification. The simple classification problems only require one XML structure to represent each class and the objects to be assigned to it. However, when the classification system is implemented as a decision support system within more complex realworld domains, there may be many variations of the class specification for different types of object to be assigned to the class in different circumstances and by different types of user making the classification decision. All these XML structures will be related to each other in formal ways, based on the original class specification, but managing their relationships and evolution becomes very difficult when the specifications for the XML variants are text-based documents. For dealing with these complexities a knowledge representation needs to be in a format that can be easily understood by human users as well as supporting ongoing knowledge engineering, including evolution and consistency of knowledge. The aim is to explore how semantic web technologies can be employed to help the knowledge engineering process for decision support systems based on human expertise, but deployed in complex domains with variable circumstances. The research evaluated OWL as a suitable vehicle for representing psychological expertise. The task was to see how well it can provide a machine formalism for the knowledge without losing its psychological validity or transparency: that is, the ability of end users to understand the knowledge representation intuitively despite its OWL format. The OWL Galatea model is designed in this study to help in automatic knowledge maintenance, reducing the replication of knowledge with variant uncertainties and support in knowledge engineering processes. The OWL-based approaches used in this model also aid in the adaptive knowledge management. An adaptive assessment questionnaire is an example of it, which is dynamically derived using the users age as the seed for creating the alternative questionnaires. The credibility of the OWL Galatea model is tested by applying it on two extremely different assessment domains (i.e. GRiST and ADVANCE). The conclusions are that OWLbased specifications provide the complementary structures for managing complex knowledge based on human expertise without impeding the end users’ understanding of the knowledgebase. The generic classification model is applicable to many domains and the accompanying OWL specification facilitates its implementations.",,2016.0,,51886715,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
3af91b1be29551c1cf19866f47e7035b7e139343,https://www.semanticscholar.org/paper/3af91b1be29551c1cf19866f47e7035b7e139343,Objective Image Quality Assessment: Facing The Real-World Challenges,"There has been a growing interest in recent years in the development of objective image quality assessment (IQA) models, whose roles are not only to monitor image quality degradations and benchmark image processing systems, but also to optimize various image and video processing algorithms and systems. While the past achievement is worth celebrating, a number of major challenges remain when we apply existing IQA models in realworld applications. These include obvious ones such as the challenges to largely reduce the complexity of existing IQA algorithms and to make them easy-to-use and easy-to-understand. There are also challenges regarding the applicability of existing IQA models in many real-world problems where image quality needs to be evaluated and compared across dimensionality, across viewing environment, and across the form of representations − specific examples include quality assessment for image resizing, color-togray image conversion, multi-exposure image fusion, image retargeting, and high dynamic range image tone mapping. Here we will first elaborate these challenges, and then concentrate on a specific one, namely the generalization challenge, which we believe is a more fundamental issue in the development, validation and application of IQA models. Specifically, the challenge is about the generalization capability of existing IQA models, which achieve superior quality prediction performance in lab testing environment using a limited number of subject-rated test images, but the performance may not extend to the real-world where we are working with images of a much greater diversity in terms of content and complexity. We will discuss some principle ideas and related work that might help us meet the challenges in the future. Introduction Over the past decades, a growing number of researchers and engineers in the image processing community have started to realize the importance of image/video quality assessment (IQA/VQA) [40, 29, 4]. This is not surprising because no matter what image/video processing problems we are working on, the same issues repeatedly come up − How should we evaluate the images generated from our algorithms/systems? How do we know our algorithm/system is creating an improvement between the input and output images, and by how much? How can we know one algorithm/system performs better than another, and by how much? What should be the quality criterion for which the design of our algorithms/systems should be optimized? Since the human eyes are the ultimate receivers in most image processing applications, human subjective visual testing would be a reliable solution. However, with the exponential increase of the volume of image/video data being generated daily, it becomes impossible to address these quality issues in a timely manner by subjective visual testing, which is slow, cumbersome and expensive. Instead, only trusted objective IQA models may potentially meet these needs. In academia, objective IQA has been a hot research topic, especially in the past 15 years [35, 4, 29]. First, the commonly used numerical disotrtion/quality measures in the past − the mean squared error (MSE) and the peak signal-to-noise ratio (PSNR) − have been shown to correlate poorly with perceived image quality [28, 30]. Second, a large number of perceptually more meaningful IQA models have been proposed, including full-reference (where a perfect quality reference image is available when evaluating a distorted image) [35, 4, 29], no-reference (where the reference image is not accessible) [34, 24, 31], and reduced-reference (where only partial information about the reference image is available) models [39, 36, 31, 29]. Third, several design principles have been discovered and repeatedly demonstrated to be useful in the design and improvement of IQA models. These include psychophysical and physiological visibility models [35, 4], the structural similarity (SSIM) approaches [28, 32, 33, 20, 49], the natural scene statistics (NSS) and information theoretic approaches [36, 39, 21, 31], the visual saliency based approaches [50], and the machine learning based approaches [6]. Fourth, a number of subject-rated image quality databases have been created and made publicly available [22, 7, 8, 17, 16, 47]. They provide a common benchmark platform for the evaluation and comparison of IQA models, among which several algorithms have achieved high correlations with the subjective mean opinion scores (MOSs) of the test images [23, 38, 33, 49]. In the video delivery industry, perceptual objective IQA methods such as the SSIM algorithm have been incorporated into many practical hardware and software systems to monitor image/video quality degradations and to test/compare image/video encoders and transcoders [27, 25, 26]. The wide use of SSIM has resulted in a Primetime Engineering Emmy Award given by the Academy of Television Arts and Sciences [1]. The remarkable development and successful deployment of modern IQA methods are definitely worth celebrating. Nevertheless, this does not necessarily mean that the existing IQA models have already met the real-world challenges. Otherwise, they should have made a much stronger impact and become a gamechanging factor in the industry. Using the video delivery industry as an example, even now most practitioners are still equating bitrate with quality in the practical design of video delivery architectures. However, using the same bitrate to encode different video content could result in dramatically different visual quality. Clearly, the perceptual quality of the video itself, which is presumably the ultimate evaluation criterion of the whole video delivery system, has not been placed at the driver’s seat. While it is understandable that quality degradation is inevitable at many stages in the video delivery chain due to practical constraints, the real concern here is that there is no existing protocol to monitor and control such quality degradation. As a result, various tricks have been used to manipulate the video content and network resources are allocated in suboptimal ways, leaving the creative intent of the content producers unprotected. While it is certain that the industry needs to be better informed about the great potentials of making the best use of IQA/VQA models, we believe that an equally important aspect that slows down the process is that the existing IQA/VQA models still do not meet many real-world challenges. In the following sections, we will elaborate some of these challenges and then focus on a specific one, namely the generalization challenge. We wish our discussions on some fundamental ideas could provide some useful insights for the future development of IQA models that may meet these real-world challenges. The Real-World Challenges Here we make a list of real-world challenges, many of which are described in more details through examples of practical scenarios. 1. It is highly desirable to reduce the complexity of the IQA/VQA algorithms so that they can be computed in realtime or in an even faster speed. This is especially useful in time-sensitive applications such as live broadcasting and videoconferencing. Many existing models are far from meeting this challenge. 2. It is essential to make the IQA/VQA scores easy-to-use and easy-to-understand. For example, the raw SSIM score does not have an explicit perceptual meaning, making it difficult to determine what level of SSIM index can warrant an excellent video quality and how much improvement in the SSIM index is sufficient to create visible quality improvement. Mapping the raw scores into a perceptually linear domain that is easily linked to human expressions about image quality is desirable. 3. The same video stream shown on different display devices could result in very different perceptual quality. For example, a strongly compressed video that exhibits very annoying artifacts on a large TV could appear to have fine quality when viewed on the screen of a smartphone. The quality may also change significantly when the video is watched on the same TV but at two different viewing distances, one at the default distance and the other at a very close distance. However, existing IQA/VQA models give the same score based on the video stream only, completely ignorant of the viewing device and viewing condition. 4. In a video-on-demand application, a high-quality highresolution (e.g., 4K) source video may be encoded into multiple video streams of different resolutions (e.g., 1080p, 720p, 360p, 240p, etc.) and different bit rates, aiming for satisfying a variety of user needs. In order to measure the quality of the encoded videos, most existing VQA models cannot be computed because the source (reference) and test videos have different spatial resolutions. 5. An image or video may need to be displayed on a screen that has a spatial resolution higher than that of the image resolution. As a result, spatial interpolation is performed. Again, most existing VQA models are not applicable because the reference and test images have different spatial resolutions. 6. An image or video of imperfect quality (e.g., being compressed at an earlier stage) is received and then transcoded to multiple images or videos with different bitrates and resolutions. Most existing IQA/VQA models are not applicable not only because they do not allow for cross-resolution quality assessment, but also because they assume the original reference image/video to have perfect quality, which is not the case here. How to carry out “degraded reference” IQA/VQA is a major challenge. 7. A high dynamic range (HDR) image (e.g., the pixels are in 10 or more bit depths) is tone mapped to a standard dynamic range (SDR) image (8 bits per pixel) in order to be visualized on an SDR display. There is certainly information loss that we would like to capture. However, most existing IQA models do not apply because they cannot compare images/videos with different dynamic ranges.",IQSP,2016.0,10.2352/ISSN.2470-1173.2016.13.IQSP-205,46570946,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
3f3f00fc84c5ea2f6fba2dea24d139dfcc2278bb,https://www.semanticscholar.org/paper/3f3f00fc84c5ea2f6fba2dea24d139dfcc2278bb,Visual Model Interpretation for Epidemiological Cohort Studies,"Epidemiological cohort studies investigate the cause and development of diseases in human populations. Conventional analyses are challenged by recently increasing study sizes, which is why the incorporation of machine learning gains popularity. State-of-the-art classifiers are however often hard to interpret – an important requirement in medical applications. This thesis addresses the gap between predictive power and interpretability in the context of cohort study analysis. Main contribution is the development of an interactive visual interface for the interpretation and comparison of probabilistic classifiers. It supports the analysis of important features at both global and individual level, computation of partial dependence, and iterative construction of meaningful feature groups. To analyse the longitudinal influence of features, the user can modify the feature set by removing a feature or replacing its value by a previous examination record. The developed visual interface is evaluated in two case studies in order to test its effectiveness for the generation and validation of research hypotheses. The case studies include a realworld epidemiological cohort study and synthetic data. The results indicate the interface’s usefulness for epidemiological research, but also reveal necessary further work for the deployment into a productive environment.",,2018.0,,199400002,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
2ba48a39ffff2a1d45f7ac6614a3f5137e028d60,https://www.semanticscholar.org/paper/2ba48a39ffff2a1d45f7ac6614a3f5137e028d60,IEEE Access Special Session Editorial: Big Data Services and Computational Intelligence for Industrial Systems,"The pervasive nature of big data technologies as witnessed in industry services and everyday life has given rise to an emergent, data-focused economy stemming from many aspects of industrial applications. The richness and vastness of these services are creating unprecedented research opportunities in a number of industrial fields including public health, urban studies, economics, finance, social science, and geography. We are moving towards the era of Big Data Services , which are deployed in a multi-scale complex distributed architecture. These services can be formed a high-level computational intelligence based on emerging analytical techniques such as big data analytics and web analytics. In this context, computational intelligence employs software tools from advanced analytics disciplines such as data mining, predictive analytics, and machine learning. At the same time, it becomes increasingly important to anticipate technical and practical challenges and to identify best practices learned through experience. This special session has included nine papers, and a brief summary about each paper is presented as follows.",IEEE Access,2015.0,10.1109/ACCESS.2016.2516178,34408773,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
542438fa30d30cf9be1fd01b46ab1ad8dad1a06a,https://www.semanticscholar.org/paper/542438fa30d30cf9be1fd01b46ab1ad8dad1a06a,The Value of Big Data for Urban Science,"Introduction The past two decades have seen rapid advances in sensors, database technologies, search engines, data mining, machine learning, statistics, distributed computing, visualization, and modeling and simulation. These technologies, which collectively underpin ‘big data’, are allowing organizations to acquire, transmit, store, and analyze all manner of data in greater volume, with greater velocity, and of greater variety. Cisco, the multinational manufacturer of networking equipment, estimates that by 2017 there will be three networked devices for every person on the globe. The ‘instrumenting of society’ that is taking place as these technologies are widely deployed is producing data streams of unprecedented granularity, coverage, and timeliness. The tsunami of data is increasingly impacting the commercial and academic spheres. A decade ago, it was news that Walmart was using predictive analytics to anticipate inventory needs in the face of upcoming severe weather events. Today, retail (inventory management), advertising (online recommendation engines), insurance (improved stratification of risk), finance (investment strategy, fraud detection), real estate, entertainment, and political campaigns routinely acquire, integrate, and analyze large amounts of societal data to improve their performance. Scientific research is also seeing the rise of big data technologies. Large federated databases are now an important asset in physics, astronomy, the earth sciences, and biology. The social sciences are beginning to grapple with the implications of this transformation. The traditional data paradigm of social science relies upon surveys and experiments, both qualitative and quantitative, as well as exploitation of administrative records created for non-research purposes. Well-designed surveys generate representative data from comparatively small samples, and the best administrative datasets provide high-quality data covering a total population of interest. The opportunity now presents to understand how these traditional tools can be complemented by large volumes of ‘organic’ data that are being generated as a natural part of a modern, technologically advanced society. Depending upon how sampling errors, coverage errors, and biases are accounted for, we believe the combination can yield new insights into human behavior and social norms.",,2014.0,10.1017/CBO9781107590205.009,150839968,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
8f2a47cd8a04aa7985d36c1106b01f35290be30f,https://www.semanticscholar.org/paper/8f2a47cd8a04aa7985d36c1106b01f35290be30f,Development of 80- and 100- Mile Work Day Cycles Representative of Commercial Pickup and Delivery Operation,"When developing and designing new technology for integrated vehicle systems deployment, standard cycles have long existed for chassis dynamometer testing and tuning of the powertrain. However, to this day with recent developments and advancements in plug-in hybrid and battery electric vehicle technology, no true “work day” cycles exist with which to tune and measure energy storage control and thermal management systems. To address these issues and in support of development of a range-extended pickup and delivery Class 6 commercial vehicle, researchers at the National Renewable Energy Laboratory in collaboration with Cummins analyzed 78,000 days of operational data captured from more than 260 vehicles operating across the United States to characterize the typical daily performance requirements associated with Class 6 commercial pickup and delivery operation. In total, over 2.5 million miles of realworld vehicle operation were condensed into a pair of duty cycles, an 80-mile cycle and a 100-mile cycle representative of the daily operation of U.S. class 3-6 commercial pickup and delivery trucks. Using novel machine learning clustering methods combined with mileage-based weighting, these composite representative cycles correspond to 90th and 95th percentiles for daily vehicle miles traveled by the vehicles observed. In addition to including vehicle speed vs time drive cycles, in an effort to better represent the environmental factors encountered by pickup and delivery vehicles operating across the United States, a nationally representative grade profile and key status information were also appended to the speed vs. time profiles to produce a “work day” cycle that captures the effects of vehicle dynamics, geography, and driver behavior which can be used for future design, development, and validation of technology. Introduction Under DOE-FOA-0001349 FY15 Award for Mediumand Heavy-Duty Vehicle Powertrain Electrification, Cummins and PACCAR jointly proposed the development of a range-extending plug-in hybrid electric Class 6 pickup and delivery truck. The goal of this project is to demonstrate an electrified vehicle that would deliver a minimum of 50% reduction in fuel consumption across a range of representative drive cycles. In addition to achieving the 50% fuel reduction target, the vehicle also needs to demonstrate as good or better drivability and performance while still meeting emissions requirements when compared to existing conventionally fueled baseline vehicles. Most existing duty cycles used to test conventional internal combustion powered vehicles are of a limited time duration. For example, the Hybrid Truck Utility Forum Class 6 Pickup and Delivery cycle is slightly more than one hour. When testing a system using only fuel as its energy source, this is acceptable; a onehour duty cycle can be used to represent the vehicle operation for the entire work day (e.g., fuel consumption in the middle of the day is very similar to fuel consumption at the end of the day). However, with plug-in electric vehicles, the system (battery characteristics and thermal management systems) may operate differently throughout the work day (especially near the end of the day). For example, the available battery energy may be completely spent prior to the completion of the route. A short duty cycle cannot simply be extrapolated. Evaluating the vehicle over the entire work day also provides the ability to interject appropriate stops that are typical of the Class 6-7 pickup and delivery application. These stops can range from several minutes to much longer and can have significant thermal effect on the vehicle and powertrain systems. These stops may also have a large impact on overall duty cycle mileage (and other duty cycle characteristics such as average speed) as the stops may account for roughly half of the work day. As part of the research and development team, the National Renewable Energy Laboratory (NREL) was been NREL/CP-5400-70943. Posted with permission. Presented at WCX 18: SAE World Congress Experience, 10-12 April 2018, Detroit, Michigan.",,2018.0,10.4271/2018-01-1192,115957682,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
8a5da1080ac931af6755afe1a4259cde704ab145,https://www.semanticscholar.org/paper/8a5da1080ac931af6755afe1a4259cde704ab145,Risks and Opportunities of RegTech and SupTech Developments,"In my short remarks I would like to focus on a few key-points relating to the opportunities and vulnerabilities associated with the implementation of new technologies in the financial sector, with particular regard to the RegTech topic—that implies the deployment and regulation of information technologies used in the context of regulatory compliance, including tasks such as regulatory reporting, securities transactionmonitoring, and riskmanagement—and the SupTech topic, related to the technologies used by supervisory authorities1. As everyone knows, what we intend as “FinTech” is the abbreviation for “Financial Technology,” namely the nowadays ubiquitous application of technology to the delivery of financing, payment, investment, and consulting services, which has become a powerful driver of innovation in the financial services market2. Among the main trends, we can identify key areas of application, including payments, personal finance, lending, investments, banking, and the new developments in robo-advisory. These new services offer the advantage of being “on-the-go,” efficient, easily accessible and convenient. Relevant developments are also taking place in relation to applications of distributed ledger technology (DLT), artificial intelligence (AI)3, Machine Learning techniques4, Big Data Analytics5, RegTech6, and SupTech7, precisely. In particular, it should be borne in mind that after an initial phase in which most regulators have chosen to observe, sometimes closely, the potential of technology start-ups (a sort of RegTech 1.0), it is high time that start-ups work alongside regulators in meeting challenges (that is the emergence of Regtech 2.0).",Front. Artif. Intell.,2019.0,10.3389/frai.2019.00014,198963210,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
fbf4f7c50cdea4b6ebc445c82fe916ac19297865,https://www.semanticscholar.org/paper/fbf4f7c50cdea4b6ebc445c82fe916ac19297865,"Holistic Digital Transformation – Key Enabler to Enhance Productivity, Flexibility and Reliability for an E&P Company","
 The upstream E&P industry encompasses diverse range of activities. The core activities may be broadly classified as G&G, Drilling, Production and Transportation and non-core activities supporting efficient execution of core activities and the business as a whole may be classified as Finance, Contracts and Purchase Management, HRM and Engineering services. E&P industry is one of the earliest adopters of digital technologies in its operations to efficiently transform complex reservoirs into revenues.
 Oil India Limited (OIL), the 2nd largest national E&P Company of India too adopted digital technology in its G&G activities encompassing Seismic data acquisition, processing, interpretation and logging in 1970s. It was followed by use of high end workstations for reservoir studies & modelling in 80's. Over the years, OIL deployed number of digital interventions in both core and non-core activities. However, like other E&P companies, induction of digital technology is slow in core activities like Drilling and Production. As far as non-core activities are concerned, advanced digitisation methodology adopted in the area helped OIL in enhancing efficiency and ease of data access in B2B, B2C relationship.
 Crude oil and Natural Gas price volatility demands that E&P organizations to be more agile and competitive. Thus there is need of collaborative workplace with real time data analysis and efficient decision making facility. As a solution, number of intelligent IT technologies in the form of IoT, Data Analytics, Machine learning, Artificial Intelligence, Augmented Reality etc. are emerging at an affordable cost. Adopting these technologies in a holistic way integrating all core and non-core activities of an E&P industry can fulfil the need of above desired solution enhancing productivity, flexibility, reliability and RoI.
 Believing ""Holistic Digital Transformation"" is the success mantra to become more agile and competitive, OIL has embarked in the process of implementing new digital technologies integrating all its core and non-core activities. The paper describes OIL's methodology towards holistic digital transformation.",,2019.0,10.2118/196396-ms,208131382,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
125791c5dbd1d5a52fdf3333989f16c03c681baf,https://www.semanticscholar.org/paper/125791c5dbd1d5a52fdf3333989f16c03c681baf,Editor’s Introduction,"The special issue that opens this 23rd volume of the International Journal of Electronic Commerce is titled “Finance Big Data: Management, Analysis, and Applications” and has been guest edited by Yunchuan Sun, Yufeng Shi, and Zhengjun Zhang. The guest editors will introduce to you the four papers that present research contributions to several aspects of this important current domain. The availability of the massive data covering all aspects of our lives and of business activity is bringing profound changes to the financial sector of our economy, a principal component and the enabler of e-commerce. The financial technology (fintech) emerging on the foundation of the Internet– Web has been driving a fundamental change to the functioning of financial institutions. In fact, it is changing the ecosystem of the institutions that provide financial services. Startups of a few years of age challenge the very position of banks, insurance companies, asset-management firms, and exchanges in their core business. A principal reason they can is the collection and activation of massive aggregates of comprehensive data, both longitudinal and real-time in a multiplicity of formats, and bringing it to bear on the marketplaces and services with machine learning and other algorithm classes. The almost universal internetwork connectivity and mobile accessibility; the exponential growth of computational capabilities, doubling approximately biannually on a very high base in “the second half of the chessboard” as forecasted by Moore’s Law; the progress of machine learning; and the move to cloud computing are all mutually reinforcing and underwrite the Fintech Revolution we are witnessing. We may ascribe the origin of the fintech to the move to electronic screen-based trading on the London Stock Exchange during the Big Bang in 1986. The true revolution has been intensifying over the past two decades, if we wish to date its inception to the launch of PayPal as a person-to-person payment platform in 1998. At the core, the advantages derived from the deployment of the Big Data in finance can be traced to two overlapping sources. These are the close matching of buyer’s preferences with the seller’s offerings and a more precise risk assessment. The speed and personalization of algorithmic decision making amplify these advantages, hence more efficient markets, innovative business models for the financial intermediaries, and overall advantages to the economies. An insurance firm with access to a more detailed data profile of an applicant for a policy can offer the terms more closely matching the risk involved than its competitors. It may indeed insure based on a wider set of criteria (say, educational background and stability indicators) than a single credit-rating number. By using machine learning systems to derive new risk criteria from large and expanding data sets, the firm can offer more closely matching terms and bundles to various tranches of prospects and grow its business. The expansion of the business leads to more data and thus more refined neural network algorithms. Hence, the Big Data tips the markets",Int. J. Electron. Commer.,2019.0,10.1080/10864415.2018.1512269,13529518,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
cce600d54ef32b3f6312b9166da097e0f9eded8a,https://www.semanticscholar.org/paper/cce600d54ef32b3f6312b9166da097e0f9eded8a,Designing a Visual Analytics System for Industry-Scale Deep Neural Network Models,"The complexity of industry-scale deep learning models and datasets pose unique design, visualization, and system challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have designed, developed, and deployed ACTIVIS, a visual analytics system for interpreting industry-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both instanceand subset-level. ACTIVIS has been deployed on Facebook’s machine learning platform. This article is a summary for the VAST’17 paper (TVCG track) ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models [2]. 1 DESIGNING FOR INDUSTRY-SCALE MODELS Despite the increasing interest in developing visualization tools for deep learning interpretation [5–7], the complexity of large-scale models and datasets used in industry pose unique design challenges that are inadequately addressed by existing work. For example, while most existing visualization tools target image datasets, deep learning tasks in industry often involve different types of data, including text and numerical data. Furthermore, in designing tools for realworld deployment, it is a high priority that the tools be flexible and scalable, adapting to the wide variety of models and datasets used. These observations motivate us to design and develop ACTIVIS [2], a visual analytics system for deep neural network models, now deployed on Facebook’s machine learning platform. Since the ACTIVIS project started in April 2016, we have conducted participatory design sessions with over 15 Facebook engineers, researchers, and data scientists across multiple teams to learn about their visual analytics needs. We identified six key design challenges — for data, model, and analytics — that have not been adequately addressed by existing deep learning visualization tools. The challenges include the need to support: (1) diverse input data sources, (2) high data volume, (3) complex model architecture, (4) a great variety of models, (5) diverse subset definitions for analytics, and (6) both instanceand subset-level analyses. These challenges shape the main design goals of ACTIVIS. 2 ACTIVIS CONTRIBUTIONS ACTIVIS’s main contributions include: • A novel visual representation that unifies instanceand subsetlevel inspections of neuron activation, facilitating comparison of activation patterns for multiple instances. *e-mail: kahng@gatech.edu †e-mail: mortimer@fb.com ‡e-mail: adityakalro@fb.com §e-mail: polo@gatech.edu • An interface that tightly integrates an overview of graph-structured complex models and local inspection of neuron activations, allowing users to explore the model at different levels of abstraction. • A deployed system scaling to large datasets and models. • Case studies with Facebook engineers and data scientists that highlight how ACTIVIS helps them with their work. ACTIVIS’s multiple coordinated views help users get a high-level overview of the model from which the user can drill down to perform localized inspection of activations. ACTIVIS visualizes how neurons are activated by user-specified instances or instance subsets, to help users understand how a model derives its predictions. The subsets can be flexibly defined using data attributes, features, or output results, enabling model inspection from multiple angles. While many existing deep learning visualization tools support instancelevel exploration [6, 7], ACTIVIS is the first tool that simultaneously supports instanceand subset-level exploration. Both exploration strategies are common and effective, and they offer complementary analytics benefits. Instance-based analysis instructs how individual instances contribute to a model’s accuracy, but it is tedious to inspect many instances one by one. Subset-based analysis leverages input features or instance subsets to help reveal relationships between data attributes and machine learning algorithms’ outputs [3]. It is especially beneficial when dealing with huge datasets in industry, which may consist of millions or billions of data points. By exploring instance subsets and enabling their comparison with individual instances, users can learn how them models respond to many different slices of the data. We refer our readers to the longer version of our ACTIVIS [2] VAST’17 paper published in IEEE Transactions on Visualization and Computer Graphics.",,2017.0,,39107483,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
bc28397dd0e7af63c8ab944becf4c7c9f2404afe,https://www.semanticscholar.org/paper/bc28397dd0e7af63c8ab944becf4c7c9f2404afe,B. Project Summary,"Insider attacks present an extremely serious and pervasive security problem. In the national defense realm such attacks indicate espionage activities or at the very least lead to breaches that compromise national security. In the finance and banking sector, a single insider attack might cost billions of dollars in damages. Accurate insider threat detection has proved to be a very challenging problem, as many studies have shown that it is difficult to even cleanly define the notion of insider threats. This proposal takes a step toward addressing the above challenge by formulating and devising machine learning-based solutions to the insider attack problem on relational database management systems (RDBMS), which are ubiquitous and are highly susceptible to insider attacks. We propose concrete plans to validate and evaluate our solutions by intimately collaborating with a large financial institution to build a prototype insider threat detection engine operating on their presently operational RDBMS. Limiting the scope of the insider threat detection problem to RDBMS allows us to make measurable progress on various technical fronts. We have a novel approach for insider threat detection in RDBMS whose potential has been demonstrated in a preliminary work. The main idea, also our conviction, is that the best way to distinguish normal vs. abnormal access patterns is to statistically model the semantics of users' queries. The key piece of information capturing a query semantics is the data region that the user is trying to access. This query-semantics-based approach to user profiling is the foundation upon which our proposed research is built. The main thrusts of the proposed research program include: (1) expressive and accurate nested hierarchical and mixture learning models and effective detection algorithms based on the query semantics idea, developed along side a realistic threat model and evaluated on large and real-world RDBMS, (2) incorporate expert domain knowledge into the statistical model to improve false positive and false negative rates, (3) develop a set of statistical and mathematical tools for dealing with dynamic database updates, stealthy and collaborative attacks, and (4) address database performance and software tool development issues arising when evaluate and deploy the system in practice. Intellectual Merit. First, the insider threat problem is a notoriously hard security problem to pin down. A technically feasible definition of "" insider threats "" not only helps design a solution, but also paves the way for feasible independent benchmarking and validation. Second, coming up with learning models and …",,,,18690649,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
a8f59ac02f845e7932528a28d013cbf25bdcf324,https://www.semanticscholar.org/paper/a8f59ac02f845e7932528a28d013cbf25bdcf324,"Advances in Data Analysis, Data Handling and Business Intelligence - Proceedings of the 32nd Annual Conference of the Gesellschaft für Klassifikation e.V., Joint Conference with the British Classification Society (BCS) and the Dutch/Flemish Classification Society (VOC), Helmut-Schmidt-University, Ha",,GfKl,2010.0,10.1007/978-3-642-01044-6,13448905,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
ea89d5ab69627a9361a340d43866cc0950a50fa1,https://www.semanticscholar.org/paper/ea89d5ab69627a9361a340d43866cc0950a50fa1,Distributed Learning Algorithms for Sensor Networks,"Wireless sensor networks have received significant attention in the last decade owing to their widespread use not only in monitoring the physical world but also in surveillance. The energy and communication constraints of sensor nodes, coupled with distributed processing of sensed signals, lead to challenges in developing effective methods to perform desired inference tasks such as object detection or classification. Further, the lack of well-calibrated sensors is a major obstacle for the rapid deployment of sensor networks. This dissertation develops gossip-based learning algorithms for distributed signal processing in sensor networks. In gossip-based algorithms, sensor nodes share information with local neighbors to converge upon common knowledge about the sensed environment. Gossip-based methods allow for manageable communication among energy-constrained nodes and also accommodate changing network communication topologies. We consider three related problems and develop gossip-based processing solutions. We first consider the problem of joint signature estimation and node calibration using distributed measurements over a large-scale sensor network. We develop a new Distributed Signature Learning and Node Calibration algorithm, called D-SLANC, which estimates the signature of a commonly-sensed source signal and simultaneously estimates calibration parameters local to each sensor node. The approach we take is ii to model the sensor network as a connected graph and make use of the gossip-based distributed consensus to update the estimates at each iteration of the algorithm. We prove convergence of the algorithm to the centralized data pooling solution. We also compare its performance with the Cramér-Rao bound (CRB), and study the scaling performance of both the CRB and the D-SLANC algorithm. Secondly, we develop a gossip-based algorithm for distributed `1-optimization in a large-scale sensor network setting. Specifically, we consider sensor nodes which can measure only a part of the entire measurement vector. We formulate the `1optimization problem as quadratic optimization and develop a distributed, gossipbased algorithm using the projected-gradient approach. We analyze the performance of the proposed algorithm using synthetic data and compare it with a standard `1 solver. Third, we consider the problem of distributed classifier learning in a large-scale sensor network setting. We adopt a machine learning approach to the problem and develop a distributed, gossip-based algorithm that learns the optimal (large-margin) hyperplane separating the two classes, using the projected-gradient approach. We illustrate the performance of the proposed algorithm using both synthetic and realworld datasets.",,2010.0,,63126931,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
16765019448e16e5ed3b1d2d031dcdd7aae68706,https://www.semanticscholar.org/paper/16765019448e16e5ed3b1d2d031dcdd7aae68706,Pro Spark Streaming: The Zen of Real-Time Analytics Using Apache Spark,"Learn the right cutting-edge skills and knowledge to leverage Spark Streaming to implement a wide array of real-time, streaming applications. Pro Spark Streaming walks you through end-to-end real-time application development using real-world applications, data, and code. Taking an application-first approach, each chapter introduces use cases from a specific industry and uses publicly available datasets from that domain to unravel the intricacies of production-grade design and implementation. The domains covered in the book include social media, the sharing economy, finance, online advertising, telecommunication, and IoT. In the last few years, Spark has become synonymous with big data processing. DStreams enhance the underlying Spark processing engine to support streaming analysis with a novel micro-batch processing model. Pro Spark Streaming by Zubair Nabi will enable you to become a specialist of latency sensitive applications by leveraging the key features of DStreams, micro-batch processing, and functional programming. To this end, the book includes ready-to-deploy examples and actual code. Pro Spark Streaming will act as the bible of Spark Streaming. What You'll Learn:Spark Streaming application development and best practices Low-level details of discretized streams The application and vitality of streaming analytics to a number of industries and domains Optimization of production-grade deployments of Spark Streaming via configuration recipes and instrumentation using Graphite, collectd, and Nagios Ingestion of data from disparate sources including MQTT, Flume, Kafka, Twitter, and a custom HTTP receiver Integration and coupling with HBase, Cassandra, and Redis Design patterns for side-effects and maintaining state across the Spark Streaming micro-batch model Real-time and scalable ETL using data frames, Spark SQL, Hive, and Spark R Streaming machine learning, predictive analytics, and recommendations Meshing batch processing with stream processing via the Lambda architecture Who This Book Is For: The audience includes data scientists, big data experts, BI analysts, and data architects.",,2016.0,,62927802,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
6f070c05dbd342d3906ee2fbdd2c3529ed558851,https://www.semanticscholar.org/paper/6f070c05dbd342d3906ee2fbdd2c3529ed558851,Designing and evaluating techniques to mitigate misinformation spread on microblogging web services,"Online social media is a powerful platform for dissemination of information during important realworld events. Beyond the challenges of volume, variety and velocity of content generated on online social media, veracity poses a much greater challenge for effective utilization of this content by citizens, organizations, and authorities. Veracity of information refers to the trustworthiness / credibility / accuracy / completeness of the content. Over last few years social media has also been used to disseminate misinformation in the form of rumors, hoaxes, fake images, and videos. We aim to address this challenge of veracity or trustworthiness of content posted on social media. The spread of such untrustworthy content online has caused the loss of money, infrastructure and threat to human lives in the offline world. We focus our work on Twitter, which is one of the most popular microblogging web service today. We provide an in-depth analysis of misinformation spread on Twitter during real-world events. We propose and evaluate automated techniques to mitigate misinformation spread in real-time. The main contributions of this work are: (i) we analyzed how true versus false content is propagated through the Twitter network, with the purpose of assessing the reliability of Twitter as an information source during real-world events; (ii) we showed the effectiveness of automated techniques to detect misinformation on Twitter using a combination of content, meta-data, network, user profile and temporal features; (iii) we developed and deployed a novel framework for providing indication of trustworthiness / credibility of tweets posted during events. We evaluated the effectiveness of this real-time system with a live deployment used by real Twitter users. First, we analyzed Twitter data for 25+ global events from 2011-2014 for the spread of fake images, rumors, and untrustworthy content. Some of the prominent events analyzed by us are: Mumbai blasts (2011), England Riots (2011), Hurricane Sandy (2012), Boston Marathon Blasts (2013), Polar Vortex (2014). We identified tens of thousands of tweets containing fake images, rumors, fake websites, and by malicious user profiles for these events. We performed an in-depth characterization study of how this false versus the true data is introduced and disseminated in the Twitter network. Second, we showed how features of meta-data, network, event and temporat from user-generated content can be used effectively to detect misinformation and predict its propagation during realworld events. Third, we proposed and evaluated an automated methodology for assessing credibility of information in tweets using supervised machine learning and relevance feedback approach. We developed and deployed a real-time version in TweetCred, a system that assigns a credibility score to tweets. TweetCred, available as a browser plug-in, has been installed and used by 1,808 real Twitter users. During ten months of its deployment, the credibility score for about 12 million tweets was computed, allowing us to evaluate TweetCred in terms of accuracy, performance, effectiveness and usability. The system TweetCred built as part of this thesis work is used effectively by emergency responders, firefighters, journalists and general users to obtain credible content from Twitter. This thesis work has shown that measuring credibility of the Twitter content is possible using semi-automated techniques, and the results can be valuable to the real-world users. The insights obtained from this research and deployment provide a basis for building more sophisticated technology to tackle similar problems on different social media.",,2015.0,,166728787,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
4867a63e5edd7ee10f28067d6dd5b07d69321de7,https://www.semanticscholar.org/paper/4867a63e5edd7ee10f28067d6dd5b07d69321de7,Thesis Title: A Framework for Improving the Performance of Signature-based Network Intrusion Detection Systems,"Network Intrusion detection systems (NIDSs) have been widely deployed in different network environments (e.g., banks, schools) to defend against a variety of network attacks (e.g., Trojans, worms). Generally, a network intrusion detection system can be classified into two categories: signature-based NIDS and anomaly-based NIDS. In realworld applications, the signature-based NIDS is more prevalent than the anomaly-based detection as the false alarm rate of the former is much lower than the latter. However, we identify three major issues that can greatly affect the performance of a signature-based NIDS. Expensive signature matching. The traditional signature matching in a signature-based NIDS is too expensive that the computing burden is at least linear to the size of an incoming string. Therefore, the operational burden of a signature-based NIDS could be significantly increased in a large-scale network environment. Overhead network packets. In a large-scale network environment, a signature-based NIDS usually has to drop lots of network packets since the number of incoming packets exceeds its maximum processing capability. Massive false alarms. Although the false alarm rate of a signature-based NIDS is much smaller than that of an anomaly-based NIDS. The number of false alarms generated by a signature-based NIDS can still increase the difficulty in analyzing true alarms and adversely affect the analysis results. To mitigate the above issues, in this thesis, we propose several approaches in improving the performance of a signature-based NIDS such as Snort in the following three aspects: Signature matching improvement.We design an exclusive signature matching scheme to help perform a more efficient signature matching with the purpose of enhancing the performance of signature matching in a heavy traffic environment. Network packet filtration and reduction. To mitigate this issue, we advocate the method of constructing a packet filter such as blacklist-based packet filter, list-based packet filter and trust-based packet filter to help filter out target network packets for a signature-based NIDS such as Snort in terms of IP reputation. This packet filter can be deployed in front of a signature-based NIDS and reduce its workload in an intensive traffic network. False alarm reduction. To resolve this issue, we design several false alarm filters such as machine-learning based false alarm filters, alarm filters using knowledge-based alert verification and context-based alarm filters to help reduce false alarms (or non-critical alarms) that are generated by a signature-based NIDS. A Framework. In addition, we further propose a framework by combining the above work to overall improve the performance of a signature-based NIDS such as Snort. As a case study of the framework, we implement an enhanced filter mechanism (shortly EFM) that consists of three major components: a context-aware blacklist-based packet filter, an exclusive signature matching component and a KNN-based false alarm filter. In particular, the component of context-aware blacklist-based packet filter is responsible for filtering out network packets in terms of IP reputation. The exclusive signature matching component is implemented in the context-aware blacklist-based packet filter and aims to speed up the signature matching. At last, the component of KNN-based false alarm filter is responsible for filtering out false alarms which are produced by the context-aware blacklist-based packet filter and the NIDS. In the evaluation, the experimental results demonstrate that our framework is promising and by deploying with the EFM, the performance of a signature-based NIDS such as Snort can be improved in the aspects of network packet filtration, signature matching improvement and false alarm reduction.",,2013.0,,209450909,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
88ea92d8b618525ff2a7815c08f0fc2c68d8daa2,https://www.semanticscholar.org/paper/88ea92d8b618525ff2a7815c08f0fc2c68d8daa2,Human Activity Inference via physical sensing in support of Industrial Equipment Maintenance,"The paper describes an active research project at Intel’s High Volume Manufacturing (HVM) facility located at Leixlip, Co. Kildare, Ireland. The project explores the practical aspects of deploying RFID transponders, subtle sensing platforms and machine learning based inferencing in a harsh, realworld environment. The key features of the sensing platform, the data collection process and the translation of data into information using visualization and inferencing techniques are described.",,2006.0,,17053776,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
a40cc32b94e828b64707aca8f026cd46a4fa7cfe,https://www.semanticscholar.org/paper/a40cc32b94e828b64707aca8f026cd46a4fa7cfe,Proceedings of First International Conference on Advanced Trends in ICT and Management (ICAITM),"S.No. Paper Title Author Name Page No. 1 Using A Remote Training Model For Technical / Vocational Education And Apprenticeship Enhancement Mr. Emmanuel Fon Tata 1 2 Challenges In Application Of ELearning To Extension In Kenya: A Case Of South Western Kenya Prof. Anakalo Shitandi 2 3 Cloud Computing And Its Related Technologies: Issues And Challenges Mr. Sulaimon Mr. Hakeem Adewale 2 4 A Unified Customer Service Model Optimizing Customer Service Quality for the Nigeria Electric Power Industry through the Implementation of an ITIL Based Service Management Paradigm Mr. Femi Akin-Laguda 3 5 Analysis Of Train Energy Consumption Reduction By Passing Low Passenger Flow Stations In Off-Peak Hour For Addis Ababa LRT Mr. Eshetu Shewalema 4 6 Higher Performance And Cloud Computing Mr. Foldestine Paye 5 7 Multitemporal Remote Sensing Of Landscape Dynamics And Pattern Change In Dire District, Southern Ethiopia Mr. Berhanu Keno 5 8 Assessing Governance Practices Among Micro-Finance Organisations: A Case Study Of Selected Firms In The MicroFinance Industry In Ghana Mr. Justice Paul Donkor 6 9 M-Commerce The New Phase Of Doing Business In The 21St Century Opportunities And Challenges Mr. Darko-Ampem A. Emmanuel 7 10 Influnce of Digital Audio Technicals on research and production in the music Industry in Africa : A case of CoTE D'ivoire, Dr. Anoha Clokou 7 11 Small And Medium Enterprises: A Survival Strategy For Nigeria Youth Empowerment Mr. Agboje Egbonimali Shadrack 8 12 Development Programmes And Follow-Up Procedures For Nigeria Colleges Of Education Trainers Using Cloud Computing Dr. A.A.Ladan 9 13 Work Place Violence, Aggressive Behaviour And Organizational Productivity Mr. Abdulsabur Hassan 9 14 Electricity Challenges On Small Business Enterprises In Nigeria Mr. Sidi Jelani 10 15 E-Commerce And Consumer Rights: Applicability Of Consumer Protection Laws In Online Transactions In East Africa. Mr. Sadat Mulongo Lunani 11 S.No. Paper Title Author Name Page No. 16 Change Management And Organizational Performance Measures Mr. Agboje Chukwuma 12 17 The Leadership And Funding Challenges Of University Information Technology Centres: A Case Study Dr. O. Osunade 13 18 The Impact Of Digital Divide On Cloud Migration In Africa: A Case Ke ya’S Te h ologi al State. Mr. Mainye Marcella Moraa 14 19 MOOCs In Nigeria: Awareness And Adoption Mr. Shamsuddeen H. Muhammad 14 20 Applying Rough Set Theory To Yorùbá Language Translation Mr. Fagbolu O O, Obalalu B S, Udoh S.S 15 21 Enterprise Resource Planning In Africa: A Case Study Of University Of Ibadan O.Osunade,O.Oladele, O.Omolola. 16 22 A Frame Work for Implementation of an E-Classroom System Mr. Shamsuddeen H. Muhammad 16 23 The Need To Fasten Cloud Adoption In Developing Countries Of Africa Mr. Shamsuddeen H. Muhammad 17 24 Usi g Co puter Ga es To I prove First Year Stude ts’ Learning Of Computer Programming Ms. Esther Gyimah 17 25 Impact Of Stakeholder Consultation On The Success Of Road Construction Project Ghana. Mr. Kwame Ofori 18 26 Security Issues Associated with Cloud Computing Mr. Mustapha Muhammad Sani 19 27 The Myths And Facts About Cloud Computing – Examining The Positions Of Start-Ups/SMEs Mr. Edward Daniels 20 28 The Impact Of Employee Empowerment On Service Quality Delivery And Customer Satisfaction At Chicken Republic . Mr. Daniel Adjei 20 29 The Influence Of Television Advertisement On The Youth Purchasing Behaviour Mr.Daniel Adjei Ms. Eunice Akorfa 21 30 Ethical Issues In Business Conduct Mr. Daniel Adjei Ms. Eunice Akorfa 22 31 Adolescent Socialization Environment As Predictors Of Unsafe Internet Behaviour Among Secondary School Students In Ibadan North Local Government Area Of Oyo State, Nigeria Mr. Ruth Ochanya Adio-Moses 23 32 Cloud Computing, An Avenue For Enhancing E-Procurement For Sustainable Development. Mr. Omane Kofi Wilson 23 33 The Introduction Of Human Resource Management In Ghana: Public Universities And Private Sector Perspectives Mr. Abdul-Kahar Adam 25 34 Statistical Analysis Of Knowledge And Utilization Of Cloud Computing By Smart Phone Users – A Case Study Of MTN Data Shop Customers Mr. Akpor-Mensah Edmund ,Mr. Dzivor Nelson Doe 26 35 Statistical Modelling Of Cloud Computing Utility In Tertiary Institutions In Accra (Ghana) Mr. Akpor-Mensah Edmund, Mr. Dzivor Nelson Doe 26 S.No. Paper Title Author Name Page No. 36 Adoption Of Cloud Computing As A Key Strategic Tool For Business Sustainability Mr. Akpor-Mensah Edmund, Mr. Dzivor Nelson Doe 27 37 Web Based Housing Management System Mr. Paul Adeoye Omosebi 27 38 Politics In the Cloud : An Argument for Cloud Based Software in Politics Mr. Nana Amankwah Peprah Mr. Kamal Kant Hiran 28 39 Mobile Assemblages and Development (maendeleo) in Marakwet Kenya Dr. Leah Jerop Komen 30 40 Investigating the Impact of ICT on the Enhancement of Learning amongst Special Needs Students Mr. Emmanuel Fon Tata 30 41 A Remote Training Model: A New Paradigm For Technical and Vocational Apprenticeship In Ghana Mr. Emmanuel Fon Tata 31 42 Cloud Computing as a Suitable Alternative to The Traditional On-Premise ERP And Massive Data Storage Mr. Mbanzabugabo Jean Baptiste 32 43 Gha a’s ICT4AD Poli y Do u e t – A Diminishing Significance Mr. Kubuga, Kumangkem Kennedy Mr. J. Kok Konjaang 33 44 Improving Health Care Delivery in Ghana: A Need of Urgency for NHIS Card Upgrade Mr. Mensah Sitti 33 45 Cloud Computing: A Catalyst in the Agenda of Education for All Prof. Patrick E. Eya, Dr. Samson Sunday Afolabi 34 46 For better or for worse: Effect of technological revolution on family communication Mr. Albert AnaniBossman 35 47 Application of the Excellent Principles of Public Relations in a Different Cultural Context: The Case Study of Ghana Mr. Albert AnaniBossman 36 48 Social Networking and Interpersonal Communication: how online identities impact on off line relationships Mr. Albert AnaniBossman 38 49 The Adoption and Deployment of Technology in Inventory Management Systems of Public Institutions. A Case Study of Electoral Commission of Ghana (EC) Ms. Priscilla Hanson 41 50 Effective use of Cloud Computing Services in Higher Education Mr. Sujith Jayaprakash 41 51 Antecedents of Employee Job Stress: Evidence from the Insurance Industry in Ghana Ms. Evelyn Twumasi 42 52 Analysis of Elman Neural Networks for Wavelet Transforms Based Feature Extraction in the Classification of Epilepsy Risk Levels from EEG Signals Dr. Vijayakumar T and Prof. T. Harikumar 43 53 An analysis of Challenges and opportunities for using Electronic Commerce in Ethiopia. Dr. S. Anbarasu 44 54 Paper Title Author Name Page No. S.No. Internet Use among Senior High School Students in Ghana: A Study of La Presbyterian Senior High School Mr. Philip Dornyo, Ms. Eunice Akorfa Adiko 44 55 Management of Technology and InnovationPerspectives on the Indian Banking Industry Dr. M. Thanikaivel 45 56 Cloud Computing – The Pathway and the Future Hope for Afri a’s sustai a le edu atio Ms. Eva Esther Shalin Mr. Samual Edem 46 57 Botnet Detection Using Data mining Techniques in Cloud networks Mr. S. Nagendra prabhu Dr. D. Shanthi 46 58 Online Password Protection Using Persuasive Cued Click Point Method Dr. Ra. Parivallal Dr. V. S. Prakash Mr. Manikandan 47 59 Knowledge Based Analysis of Microarray Gene Expression Data in Oncology Dr. W. Jai Singh Ms. N. Nivetha Rani Ms. S. Nivitha 47 60 Network On Chip: A New Frontier For Highly Scalable And Energy Efficient Multicore Systems Mr. Charles Saah 48 61 Auto Recovery Of Virtual Machine In A Cloud Based System After Memory Error Attack Mr. Charles Saah 49 62 A Study On Security Issues In Cloud Computing Mr. Vijesh Krishnamoorthy 49",,2016.0,,201728374,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
b34fd3c8de0b2b85b044fd96022b063a7aada8a3,https://www.semanticscholar.org/paper/b34fd3c8de0b2b85b044fd96022b063a7aada8a3,A Study of the Internet Financial Interest Rate Risk Evaluation Index System in Context of Cloud Computing,,3PGCIC,2016.0,10.1007/978-3-319-49109-7_77,46652488,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
c1f295836b4bdcb6fdaeb77724f04aec938299c2,https://www.semanticscholar.org/paper/c1f295836b4bdcb6fdaeb77724f04aec938299c2,“Approximate dynamic programming: Solving the curses of dimensionality” by Warren B. Powell,"Dynamic programming is a rich area with numerous books already available. Professor Powell’s book on approximate dynamic programming sheds a completely new light on this exciting area. Instead of focusing on traditional approaches to dynamic programming, most of the content is devoted to recent advances in solving large-scale dynamic programs. Professor Powell definitely breaks the longstanding myth of the curse of dimensionality by presenting several state-ofthe-art solution methodologies capable of solving problems of unthinkable size. The book exposes the reader to an excellent mixture of operations research, mathematical programming and artificial intelligence techniques. The transitions and the interplay among these three areas are presented in a coherent way, thus making it an extremely readable book. The spread of topics is also amazing, and yet the book is focused on approximate dynamic programming. Stochastic optimization, machine learning, mathematical programming modeling, finite and infinite time horizon problems, and value function approximations are all nicely blended within a single book. The material is presented at various levels and several different aspects are addressed. The book can easily be used for either an undergraduate or a graduate course textbook. There is abundant technical material to design a comprehensive graduate level course. The book also includes a wide range of exercises. Perhaps the most appealing aspect of Professor Powell’s book is the fact that it spans both theory and practice. On the theoretical side, treatment of convergence is provided for several algorithms and various technical statements are rigorously proved, including aspects of the optimality equation. On the practical side, an important contribution is the modeling framework. Throughout the book several realworld examples are discussed, from transportation to the energy sector and finance. All of them are presented in the same modeling framework. From a practitioner’s perspective, it is perhaps even more important to stress the computational tractability of the presented solution methodologies. Problems deemed intractable a few years ago are now easily solved by using the exhibited techniques in Professor Powell’s book. This clears the way to more comprehensive models and to real-time dynamic decision making. I would strongly recommend the book to any practitioner facing complex, dynamic models involving constantly changing information streams. The first two chapters expose the reader to the basics of dynamic programming. They nicely outline the computational challenges in solving large-scale dynamic programs and several illustrative examples are provided. A more traditional treatment of dynamic programming is provided in the third chapter. The standard value and policy iteration algorithms are the bulk of this chapter. Convergence proofs are also provided. Modern aspects of approximate dynamic programming span the remaining chapters. The important concept of post-decision modeling is introduced in the fourth chapter. This modeling paradigm is the basis for most of the algorithms. The first exposure to reinforcement learning, and thus artificial intelligence, is also provided in this chapter. Through the evolution of dynamic programming, several modeling styles have developed. In the fifth chapter the book discusses a brand new modeling approach, suitable for many complex problems, with intriguing decision making and exogenous information processes. Every reader should bear in mind the importance of this modeling approach. It makes the remaining chapters substantially more readable and easier to understand. The modeling principles were developed by focusing on practical aspects and the ease of modeling the ever evolving real-world situations. Chapter 6 serves as the springboard to the following chapters by covering the stochastic approximation methods. The focus is on the so-called stochastic gradient algorithm. An important parameter of this algorithm is the step size. The book provides a thorough treatment of the step size selection process. In addition to standard step size formulas, a substantial portion of the chapter is devoted to selecting an optimal step size. The chapter is concluded with two convergence results, which are presented in a tutorial fashion, as an introduction to stochastic convergence theory. The two proofs illustrate an older proof technique and a more modern technique. The main concept behind approximate dynamic programming is the notion of value function approximations. Chapter 7 provides a comprehensive treatment of this topic. A very recent development particularly suited for extremely",,2009.0,10.1080/07408170802189500,116889949,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
43d424a53d42820c3173fd29f6a49f0044fc1c38,https://www.semanticscholar.org/paper/43d424a53d42820c3173fd29f6a49f0044fc1c38,"Adequacy and sustainability of secondary schools' computerization in meeting instructional needs in selected schools in Kitui County, Kenya","One of the main problems in deployment of computer technology in Kenyan schools is unclear objectives and misplaced priorities. Many schools have initiated expensive computer projects, whose sustainability is questionable. The software installed in these machines might not be tailored to meet specific instructional needs and therefore computers are likely to be underused and unappreciated by those they were meant to benefit. This study sought to assess the sustainability of these projects and their appropriateness in meeting teaching and learning needs in Kitui County, Kenya. Users' accessibility to computers, project costs, specific instructional needs and user-friendliness of hardware and software was investigated. To achieve these objectives, the study investigated a host of indicators identified during the literature review. These indicators included financing initiatives, the nature and numbers of computers, availability of digital content, and the level of ICT skills among teachers .The study employed survey design. Quantitative and qualitative approaches were used in the collection and the analysis of data. The target population consisted of teachers, students and administrators in provincial secondary schools. Six out of 19 provincial secondary schools were purposively selected. A sample size of 120 randomly selected teachers of various subjects, 6 principals, 6 teachers of Computer Studies and 163 students participated in the study. The researcher used questionnaires, interview schedules, checklists and document analysis in data collection. Collected data was scrutinized for completeness and analysed using MS-excel for quantitative and CpC EZ-text software. for qualitative data. It was then presented by using relevant themes, tables and percentages. The findings of the survey revealed that Schools lack a comprehensive ICT policy to guide acquisition, use and sustenance of computerisation. Consequently, they did not have reliable financing arrangements to set up and run computerization. The study observed that this inadequate funding impacted negatively on the number of use able machines and the availability of uptodate user-friendly software, severely hampering access to the machines and digital content. In addition, the study found out that although the teachers and learners are aware of their ICT instructional needs, there is little content in schools that can be integrated in teaching and learning. The study concluded that secondary school computerization is neither adequate nor sustainable in its current state. The researcher recommends that the government should get more involved in establishing lCT centres in schools and in conducting a sensitization campaign to equip teachers with knowledge on ICT integration into their daily instruction tasks. The findings of the study have implications for teachers practices and the policy approach to secondary computerization. These results can be used by school administrators as a benchmark for improving ICT policies in their schools and to facilitate optimum access to and productive use of available computers. These results can also be used by the government to improve e-preparedness of schools as we move towards the realization of computerization of all schools by the year 2012. Finally, suggestions for further research are made.",,2013.0,,153020426,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
f126d77ca134bf7ae35416ab45b4d2eefffe5cb1,https://www.semanticscholar.org/paper/f126d77ca134bf7ae35416ab45b4d2eefffe5cb1,"Case studies: Commercial, multiple mining tasks systems: clementine","Launched in 1994, SPSS' Clementine was one of the first commercially available tools for data mining. Clementine uses an interface style known as visual programming in which numerous tools used in different stages of the data mining process are represented by icons that are strung together to form visual streams of data mining solutions. Streams can include machine learning and other modeling technologies, data access facilities, record and field manipulation, interactive visualization, conventional analyses, and reporting. The main aim of Clementine was to put data mining in the hands of business users, rather than just technology specialists. This is achieved partly by the easy-to-learn visual programming interface, and also by insulating users from technology details; by default, the modeling algorithms are automatically configured to suit a given data set. Clementine has also proved popular with more sophisticated power analyst users. For them, the visual programming environment provides a highly productive, fluent environment. Compared to a component-based approach, the proportion of the analysts' time needed for noncore tasks such as data manipulation and transformation is reduced, freeing them to concentrate on building models to solve problems. Both types of users value Clementine's openness, which enables integration with existing IT infrastructure, and comprehensive approaches to scalability and deployment of solutions. These approaches take into account all the stages in the data mining process so that, for example, data processing steps are scaled and all steps are exported for deploying solutions to decision makers. Clementine is used by hundreds of organizations worldwide, in sectors including finance, retail, telecommunications, government, and manufacturing. Many applications involve marketing and other aspects of customer relationship management (CRM), but the product has also been used to tackle data mining problems such as product quality analyses, toxicity prediction, and fraud detection.",,2002.0,,108203205,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
c66fdd3d870e96586f52247a47a6e1f435f7f681,https://www.semanticscholar.org/paper/c66fdd3d870e96586f52247a47a6e1f435f7f681,Artificial Neural Networks for Forecasting Stock Price Statistical Arbitrage using Artificial Neural Networks,"Statistical arbitrage strategies have always been popular since the advent of algorithmic trading. In particular, Exchange traded fund (E.T.F.) arbitrage has attracted much attention. Trading houses have tried to replicate ETF arbitrage to other stocks. Thus, the objective is to be able to develop a long term pricing relationship between stocks and profit from their divergence from this relationship. In this paper, we have developed a feasible trading strategy on this concept. Artificial neural networks have been deployed to model the pricing relationship between elements in a sector. All prices have been considered at the same instant, thereby allowing us to make trading decisions in accordance with our predictions. Supervised learning algorithms have been used to train the network. INTRODUCTION Artificial Neural Networks (ANN) have been shown to be an efficient tool for non-parametric modeling of data in a variety of different contexts where the output is a non-linear function of the inputs. These include business forecasting, credit scoring, bond rating, business failure prediction, pattern recognition and image processing. Furthermore, in the field of finance and stock price prediction, some work has been undertaken to model stock prices using ANN. Yet, there doesn’t exist any feasible statistically profitable strategy that tends to employ ANN efficiently. The price of a stock is influenced by: 1. Company news – Any news pertaining to the company being studied. 2. Macro-economic news – Any news pertaining to the broader section of the stock market. Company specific news is significantly lower in frequency than macroeconomic news and thus its important point to automatically profit from macro-economic news. We would like to do this algorithmically owing to the large volume of information involved. Stock price prediction, market sentiment analysis and arbitrage are the commonly applied concepts in development of a trading strategy. Artificial Neural Networks for Forecasting Stock Price 2008 Page 3 of 15 The major problem with stock price prediction is accounting for the vast volume of information affecting stock prices. As George Soros put it in 1987: One of the major problems faced in modeling financial market movements is the fact that information comes in from a very large number of sources. Keeping this in mind, this study discounts all non-specific news by its impact on other stocks in the same sector. It is assumed that in the absence of company specific news, there exists pricing relationships between elements in a sector. Further stock prices conform to a pricing relationship that can be ‘learnt’ from its historical data. Thus, basically the neural network will model a response relationship amongst elements from a sector. We have studied two sectors, the Indian Construction sector and the U.S. technology sector. Data from different time periods and different nature has been used in the study. While intraday data for October 1, 2004 has been used in training and testing the neural network associated with Yahoo (U.S. technology sector), end of day data from July 24 ‘07– 4 November ’08 has been employed for Unitech (Indian construction company). ARTIFICIAL NEURAL NETWORKS In recent years some work has been reported on the use of ANNs for analysis of financial markets. In what follows we provide a very brief introduction to ANNs following Stern (1996). The three essential features of ANN are basic computing elements referred to as neurons, the network architecture describing the connections between the neurons and the training algorithm used to find values of the network parameters for performing a particular task. Each neuron performs a simple calculation, a scalar function of a scalar input. Suppose we label the neurons with positive integers and denote the input to the k neuron as ik and the output from the k neuron as ok. Then ok = fk(ik) where f(.) is a specified function that is typically monotone but otherwise arbitrary. The neurons are connected to each other in the sense that the output from one unit can serve as part of the input to another. There is a weight associated with each connection; the weight from unit j to unit k is denoted as wjk. Let N(k) denote the set of units that are connected to unit k. The input to unit k is then ik = ∑jєN(k)wjkoj. Network architecture refers to the organisation of the neurons and the types of connections permitted. In the multilayer feed forward network, the type used in this article, neurons are organised in a series of layers. Information flows only in one direction; units receive information only from units in higher layers of the network. Artificial Neural Networks for Forecasting Stock Price 2008 Page 4 of 15 ANN can be ‘trained’ to perform a specific task by adjusting these weights. The weights are continually adjusted by comparing the output of the network with the target until the output of the network ‘matches’ the target in the sense that the error function measuring the difference between the target and the output is minimised. Many pairs of input and output are used to train the network and this mode of adjustment is called ‘supervised’ learning. There are several approaches for minimisation of the error function including standard optimisation techniques like conjugate gradient and Newton-type methods. However, the most popular algorithm in this context is the backpropagation algorithm of Rumelhart et al. (1986). The algorithm is extremely simple to program but tends to converge slowly. Training of a network can take two forms: ‘incremental’ or ‘batch’. In case of incremental training, individual pairs are fed one at a time to the network. Output is compared with the target for each input and adjustments of the weights are made using a training algorithm, most often the backpropagation algorithm. The next pair is then fed to the network after making the adjustment for the previous pair and the same process is repeated. Incremental training is sometimes referred to as ‘on line’ or ‘adaptive’ training. In case of batch training, all the pairs of input and output are fed to the network at the initial stage and the weights are adjusted. In case of unsupervised learning, outputs for a given set of inputs are not available. Unsupervised learning is mostly used for networks that perform clustering and pattern recognition. Cheng and Titterington (1994) provide a comprehensive review of ANNs from a statistical perspective. A well-trained ANN can exploit the underlying non-linear relationships that drive security prices. Further, the networks can be retrained using newer data. Thus, a network can adapt to new information as it comes. This makes ANNs a very promising tool for security price modelling. The vast increase in computing power has made modelling of complex systems relatively easy and less time consuming than it was in the past. LITERATURE REVIEW ANNs are being used in various fields of application including business forecasting, credit scoring, bond rating, business failure prediction, medicine, pattern recognition, image processing, speech processing, computer vision and control systems. In the context of financial forecasting, Kuan and Liu (1995) discuss forecasting of foreign exchange rates using ANNs. They show that a properly designed ANN has lower out-of-sample mean squared prediction error relative to the random walk model. Jasic and Wood (2004) discuss the profitability of trading signals generated from the out-of-sample short-term predictions for daily returns of S&P 500, DAX, TOPIX and FTSE stock market indices evaluated over the period 1965–99. The out-of sample prediction performance of neural networks is compared against a benchmark linear autoregressive model. They find that the buy and sell signals derived from neural network predictions are significantly different from unconditional one-day mean return and are likely to Artificial Neural Networks for Forecasting Stock Price 2008 Page 5 of 15 provide significant net profits for reasonable decision rules and transaction cost assumptions. Cao et al. (2005) provide a comparison between the Fama and French model and the ANN model in the context of prediction of the Chinese stock market. They report that ANNs outperform the linear models from financial forecasting literature in terms of its predictive power. Tkacz (2001) provides an interesting study regarding the use of leading indicator neural network models for forecasting Canadian GDP growth. It is reported that the neural network models yield statistically lower forecast errors for the year-over year growth rate of real GDP relative to linear and univariate models. Huang et al. (2004) report a comparative study of application of Support Vector Machines (SVM) and Backpropagation Neural Networks (BNN) for an analysis of corporate credit ratings. They report that the performances of SVM and BNN in this problem were comparable and both these models achieved about 80 per cent prediction accuracy. Pendharkar (2005) discusses the application of ANNs for the bankruptcy prediction problem. It is reported that ANNs perform better than the statistical discriminant analysis both for training and hold-out samples. HYPOTHESIS We hypothesize that stock prices move in a complex fashion, yet this pattern can be ‘learnt’ over the short term using technical tools like ANN. Learnt on historical data, and in between periods of company specific news, ANN’s can be deployed to produce a feasible trading strategy. Further, a strategy mirroring index arbitrage can be developed based on the ANN, which will statistically yield profits. TIME SERIES ECONOMETRICS VERSUS NEURAL NETWORKS Time series forecasting analyzes past data and projects estimates of future data values. Basically, this method attempts to model a nonlinear function by a recurrence relation derived from past values. The recurrence relation can then be used to predict new values in the time se",,2008.0,,16620176,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
7a0392ef4c607392de92ee1f7c84e68780e7976b,https://www.semanticscholar.org/paper/7a0392ef4c607392de92ee1f7c84e68780e7976b,"Cybersecurity, Data Privacy and Blockchain: A Review",,SN Comput. Sci.,2022.0,10.1007/s42979-022-01020-4,245906509,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
a081aafec23202fbdf4ad2a6223d0633bce921bd,https://www.semanticscholar.org/paper/a081aafec23202fbdf4ad2a6223d0633bce921bd,Photovoltaics in 2021,"Since its inception in 1977 the European Photovoltaic Solar Energy Conference (EU PVSEC) has played a part in the success story that photovoltaics is today. Photovoltaic technology is now expected to be one of the key elements of a future sustainable energy system. As outlined in the IEA’s recent flagship report Net Zero by 2050 A Roadmap for the Global Energy Sector which provides a comprehensive study of how to transition to a net zero energy system by 2050, this will result in a clean, dynamic and resilient energy economy dominated by renewables like solar and wind instead of fossil fuels. The EU PVSEC covers topics relevant to photovoltaic specialists and other stakeholders across the full value chain, ranging from fundamental concepts to policy. In more detail, the topics are New Materials and Concepts; Silicon Materials and Cells; Perovskites and Other Non-Silicon Materials and Devices, Tandems; Modules and Balance of System Components; PV Systems – Modelling, Design, Operation and Performance of Systems; Applications, Integration and Storage; and Finance, Markets and Policies. The articles in this special issue represent a unique collection of the latest progress reported at the 2021 conference and highlight important developments in a broad range of areas. It is to be noted that the partnership with Solar RRL covers a subset of the topic areas, and a complementary selection of highlighted papers is included in a special issue of Progress in Photovoltaics. As well as individually demonstrating the state of the art in its own field, the selected articles taken together provide an insight into the areas currently the focus of the photovoltaics research community. A brief overview of the articles grouped by topic follows. The listed article titles contain clickable hyperlinks for convenience. Crystalline silicon remains by far the largest segment of the PV market and, although it is a very mature technology, developments continue towards improving materials, device structures (e.g. heterojunction and TOPCon) and reliability. The increasing use of digital techniques and machine learning is noteworthy: Electronic Properties and Structure of Boron–Hydrogen Complexes in Crystalline Silicon; Influence of the Bulk Resistivity on Silicon Heterojunction Solar Cells and Module Reliability; Atmospheric Pressure Dry Etching of Polysilicon Layers for Highly Reverse Bias-Stable TOPCon Solar Cells; Learning an Empirical Digital Twin from Measurement Images for a Comprehensive Quality Inspection of Solar Cells; Explaining the Efficiencies of Mass-Produced p-Type Cz-Si Solar Cells by Interpretable Machine Learning. Other technologies also continue to push the boundaries, and here we have an example of progress in organic PV: Effect of Additives and Annealing on the Performance of NonfullereneBased Binary and Ternary Organic Photovoltaics. Two papers look at reliability of PV Modules and performance characterisation using advanced computing techniques: Reliability Evaluation of Photovoltaic Modules Fabricated from Treated Solar Cells by Laser-Enhanced Contact Optimization Process; Toward Megapixel Resolution Compressed Sensing Current Mapping of Photovoltaic Devices Using Digital Light Processing. Reliable inverters are critical to the operation of PV plants as proposed in: Analysis and Development of a Modular FaultTolerant Multistring Power Converter for Solar Photovoltaic Applications. A number of articles cover systems and applications, including building-integrated photovoltaics (BIPV), e-mobility, solar resource and forecasting, as well as for hydrogen generation. BIPV: Long-Term Performance and Shade Detection in Building Integrated Photovoltaic Systems; Photovoltaic Modules with the Look and Feel of a Stone Façade for Building Integration. E-mobility: Development of High-Efficiency Solar Cell Modules for Photovoltaic-Powered Vehicles; Demonstration of Feeding Vehicle-Integrated Photovoltaic-Converted Energy into the High-Voltage On-Board Network of Practical Light Commercial Vehicles for Range Extension; Hybrid PV Systems and Colocalization of Charging and Filling Stations for Electrification of Road Transport Sector. Solar resource: Uncertainty Calculation Method for Photodiode Pyranometers; A Hybrid Solar Irradiance Nowcasting Approach: Combining All Sky Imager Systems and Persistence Irradiance Models for Increased Accuracy; A Comprehensive Workflow for High Resolution 3D Solar Photovoltaic Potential Mapping in Dense Urban Environment: A Case Study on Campus of Delft University of Technology. Electrolosys, hydrogen: Integrating Solar Energy, Desalination and Electrolysis; Development of Various Photovoltaic-Driven Water Electrolysis Technologies for Green Solar Hydrogen Generation. Finally, a number of articles look at cost, especially of solar hydrogen, and policy issues. Cost: True Cost of Solar Hydrogen; Levelized Cost of Hydrogen Calculation from Off-Grid Photovoltaic Plants Using Different Methods; Marginal Effect of Variation in Photovoltaic System Configuration’s Generation Profiles on Price Stabilization in the Netherlands Compared with Deployment of Flexible Demand and Supply. R. P. Kenny Energy Efficiency and Renewables Unit Directorate for Energy, Transport and Climate European Commission Joint Research Centre Via Enrico Fermi 2749, 21027 Ispra (VA), Italy E-mail: robert.kenny@ec.europa.eu",Solar RRL,2022.0,10.1002/solr.202200288,248573028,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
8881dadf57a7515039c978bf3755c336fbb417af,https://www.semanticscholar.org/paper/8881dadf57a7515039c978bf3755c336fbb417af,Customer Relationship Management Customer Satisfaction Ebooks Download,"This book introduces a fuzzy classification approach, which combines relational databases with fuzzy logic for more effective and powerful customer relationship management (CRM). It shows the benefits of a fuzzy classification in contrast to the traditional sharp evaluation of customers for the acquisition, retention and recovery of customers in online shops. The book starts with a presentation of the basic concepts, fuzzy set theory and the combination of relational databases and fuzzy classification. In its second part, it focuses on the customer perspective, detailing the central concepts of CRM, its theoretical constructs and aspects of analytical, operational and collaborative CRM. It juxtaposes fuzzy and sharp customer classes and shows the implications for customer positioning, mass customization, personalization, customer assessment and controlling. Finally, the book presents the application and implementation of the concepts in online shops. A detailed case study presents the application and a separate chapter introduces the fuzzy Classification Query Language (fCQL) toolkit for implementing these concepts. In its appendix the book lists the fuzzy set operators and the query language’s grammar. This work offers a state-of-the art survey of information systems research on electronic customer relationship management (eCRM). It provides important new frameworks derived from current cases and applications in this emerging field. Each chapter takes a collaborative approach to eCRM that goes beyond the analytical and operational perspectives most often taken by researchers in the field. Chapters also stress integration with other enterprise information systems. The book is organized in four parts: Part I presents an overview of the role of CRM and eCRM in marketing and supply chain management; Part II focuses on the organizational success factors behind eCRM implementation; Part III presents cases of eCRM performance enhancement; and Part IV addresses eCRM issues in business-to-consumer commerce. Continuous improvements in digitized practices have created opportunities for businesses to develop more streamlined processes. This not only leads to higher success in day-today production, but it increases the overall success of businesses. Enterprise Information Systems and the Digitalization of Business Functions is a key resource on the latest advances and research for a digital agenda in the business world. Highlighting multidisciplinary studies on data modeling, information systems, and customer relationship management, this publication is an ideal reference source for professionals, researchers, managers, consultants, and university students interested in emerging developments for business process management. Research Paper (postgraduate) from the year 2019 in the subject Business economics Customer Relationship Management, CRM, grade: 1.5, Kwame Nkrumah University of Science and Technology, language: English, abstract: Customer Relationship Management (CRM) practices are business strategies designed to reduce costs and increase profitability by solidifying customer loyalty. With intense competition among insurance companies in Ghana, this study sought to assess Customer Relationship Management practices and Customer Retention in NSIA Insurance. The study was conducted to identify critical factors necessary for customer retention in carrying out customer relationship management practices in the selected insurance company and to develop effective customer relationship management practices to manage customer retention for sustainability within the insurance industry using NSIA Insurance as a case study. Well structured questionnaires and face-to-face interview were the methods adopted for the investigation of the study. A sample size of 40 respondents was considered, they were made up of customers and the staff who are fully involved in customer relationship management of the insurance company. Data collected from the completed questionnaires and the interviews were grouped into frequency tables and expressed in percentages. The researcher relied on the SPSS in interpreting the collected data. The study shows that even though NSIA insurance has policies on customer relationship management practices, these policies are not carried out fully to accomplish the ultimate goal of customer retention. The study recommends that for the insurance company to command an adequate number of loyal customers, NSIA Insurance should consistently improve on its quality of service to address the preference of the customers and consider the five service quality constructs of reliability, assurance, tangibility, empathy and responsiveness. The two-volume set CCIS 143 and CCIS 144 constitutes the refereed proceedings of the International Conference on Electronic Commerce, Web Application, and Communication, ECWAC 2011, held in Guangzhou, China, in April 2011. The 148 revised full papers presented in both volumes were carefully reviewed and selected from a large number of submissions. Providing a forum for engineers, scientists, researchers in electronic commerce, Web application, and communication fields, the conference will put special focus also on aspects such as e-business, e-learning, and e-security, intelligent information applications, database and system security, image and video signal processing, pattern recognition, information science, industrial automation, process control, user/machine systems, security, integrity, and protection, as well as mobile and multimedia communications. Customer Relationship Management, Fourth Edition, is a much-anticipated update of a bestselling textbook, including substantial revisions to bring its coverage up to date with the very latest in CRM practice. The book introduces the concept of CRM, explains its benefits, how and why it can be used, the technologies that are deployed, and how to implement it, providing you with a guide to every aspect of CRM in your business or your studies. Both theoretically sound and managerially relevant, the book draws on academic and independent research from a wide range of disciplines including IS, HR, project management, finance, strategy and more. Buttle and Maklan, clearly and without jargon, explain how CRM can be used throughout the customer life cycle stages of customer acquisition, retention and development. The book is illustrated liberally with screenshots from CRM software applications and case illustrations of CRM in practice. New to this Edition: Updated instructor support materials online Full colour interior Brand",,2021.0,,244129074,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
8d299258291512b11750cd61746a4f24193f7c07,https://www.semanticscholar.org/paper/8d299258291512b11750cd61746a4f24193f7c07,It’s All About Data: How to Make Good Decisions in a World Awash with Information,"The rise of big and alternative data has created significant new business opportunities in the financial sector. As we start on this journey of fast-moving technology disruption, financial professionals have a rare opportunity to balance the exponential growth of artificial intelligence (AI)/data science with ethics, bias, and privacy to create trusted data-driven decision making. In this article, the authors discuss the nuances of big data sets that are critical when one considers standards, processes, best practices, and modeling algorithms for the deployment of AI systems. In addition, this industry is widely guided by a fiduciary standard that puts the interests of the client above all else. It is therefore critical to have a thorough understanding of the limitations of our knowledge, because there are many known unknowns and unknown unknowns that can have a significant impact on outcomes. The authors emphasize key success factors for the deployment of AI initiatives: talent and bridging the skills gap. To achieve a lasting impact of big data initiatives, multidisciplinary teams with well-defined roles need to be established with continuing training and education. The prize is the finance of the future. TOPICS: Simulations, big data/machine learning Key Findings • The rise of alternative data in finance is creating major opportunities in all areas of the financial industry, including risk management, portfolio construction, investment banking, and insurance. • To build trusted outcomes in AI/ML initiatives, financial professionals’ roles are critical. Given the many nuances in using big data, there is a need for vetted protocols and methods in selecting data sets and algorithms. Best practices and guidelines are effective in reducing the risks of using AI/ML, including overfitting, lack of interpretability, biased inputs, and unethical use of data. • Given the major shortage of talent in AI/data science in finance, practical training of employees and continued education are keys to scale roll out to enable future of finance.",,2020.0,10.3905/jfds.2020.1.025,216224349,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
981d1f317673c1ae3c0942e73207c85909e6490c,https://www.semanticscholar.org/paper/981d1f317673c1ae3c0942e73207c85909e6490c,Bringing RPA to the next level using AI,"Introduction: Artificial Perspicacity (AI) is transmuting the digital landscape in every field it reaches. The Robotic Process Automation (RPA) revolution goes hand in hand with the advances that Artificial Astuteness is making to transform industries ecumenical. Ultimately, Artificial Perspicacity augments RPA and its implements to surpass prospects. With this already established, it’s valid to verbally express that the world is gearing up for the robotic revolution. Albeit we are already experiencing many of its applications, there’s still plenty of potentials to tap into. What is RPA precisely? RPA or Robotic Process Automation is software technology, as dictated by business logic and structured inputs, aimed to program applications or robots to perform rudimental tasks, just like humans would, in an automated setting. RPA bots can mimic virtually any human action, emulating and integrating actions with digital systems to execute a business process. Robotic Process Automation (RPA) enables organizations to engender virtual workforces that increase process efficiency, truncate errors, and cut operational costs. However, even when enterprises implement RPA, many are challenged with scaling it across the organization and identifying all the potential business processes that can and should be automated for maximum ROI. This presentation will discuss how companies overcome these challenges utilizing AI powered process revelation. Automation has been transmuting the nature of work for over a decennium. It has superseded labour intensive tasks, undertaken perpetual ones, escalated the haste of engenderment and engendered incipient streams of work. Most organizations are already on RPA journey, which has resulted in productivity amendment, cost savings, process time amelioration with perpetual and rule-predicated processes. The commencement of the peregrination involves POCs, pilots, and initial automations. The next frontier is about scaling the  
deployment and adoption of cognitive technologies such as AI, analytics, machine learning that emulate human comportment. The transition involves transformation from running RPA on a few processes to scaling up RPA across the enterprise. This session would deal with strategies & challenges in enterprise wide adoption of Automation - crossing over from RPA to RPA+Cognitive , identifying the right operating model & establishing governance, Leadership & aptitude development , orchestrating stakeholders and board level endorsement. According to Investopedia, Robotic Process Automation is the “software that can be facilely programmed to do rudimentary tasks across applications just as human workers do.” Consequentiality of Robotic Process Automation According to insights developed by McKinsey&Company, RPA offers the potential ROI of 30-200% in the first year of avail alone. This staggering figure is met with the verbal expression made by Leslie Willcocks that “RPA takes the robot out of the human.” Companies and employees are taking notice, which is why everyone is so agog to invest in robotics and its emerging technologies. In essence, RPA is consequential because it is transforming the way businesses operate by availing automate perpetual tasks that are a component of a quotidian routine with a higher degree of efficiency than if performed by a human. Akin to cognitive automation, chatbots, and artificial perspicacity, RPA performs significantly more expeditious and more cost-efficaciously than human resources. Many fear that RPA implements and technologies can be perilous as they take jobs out of human hands. But you shouldn’t authentically worry as there is more to gain than lose when it comes to RPA. To put it into further context, implementing Robotic Process Automation into your workplace can avail with tasks such as monitoring customer activity to discover opportunities to upsell, monitoring client comportment to identify areas of opportunity, truncating cycle times in perpetual tasks to gain competitive advantage, capturing and analyzing bulks of information to provide more expeditious replication times, and more. In a wide range of industries that span healthcare, indemnification, finance, and more, there are many cumbersomely hefty-hitters that have already adopted RPA implements into their processes, including Wal-Mart, Ernst & Puerile, Walgreens, American Express, and more. These early-adopters have benefitted from minimizing staffing costs and human errors with the implementation of RPA technology.",,2020.0,,226688221,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
a5d85748055ff954e38c4c1f24138d614d4323fa,https://www.semanticscholar.org/paper/a5d85748055ff954e38c4c1f24138d614d4323fa,SmartDashCam: Automatic Live Calibration for DashCams,"Dashboard camera installations are becoming increasingly common due to various Advanced Driver Assistance Systems (ADAS) based services provided by them. Though deployed primarily for crash recordings, calibrating these cameras can allow them to measure real-world distances, which can enable a broad spectrum of ADAS applications such as lane-detection, safe driving distance estimation, collision prediction, and collision prevention Today, dashboard camera calibration is a tedious manual process that requires a trained professional who needs to use a known pattern (e.g., chessboard-like) at a calibrated distance. In this paper, we propose SmartDash-Cam, a system for automatic and live calibration of dashboard cameras which always ensures highly accurate calibration values. Smart-DashCam leverages collecting images of a large number of vehicles appearing in front of the camera and using their coarse geometric shapes to derive the calibration parameters. In sharp contrast to the manual process we are proposing the use of a large amount of data and machine learning techniques to arrive at calibration accuracies that are comparable to the manual process. SmartDashCam implemented using commodity dashboard cameras estimates realworld distances with mean errors of 5.7 % which closely rivals the 4.1% mean error obtained from traditional manual calibration using known patterns.",2019 18th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN),2019.0,10.1145/3302506.3310397,102349388,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
bded5869c90dd5590b141c8be5a97ae309f46482,https://www.semanticscholar.org/paper/bded5869c90dd5590b141c8be5a97ae309f46482,A . Profile of the Initiative Geographic Region Western Europe and Others Name of CountryTerritory United Kingdom,"Short description of the innovative initiative being submitted for Award.(150 words max) WE/GBR/MILTON KEYNES/0273 Every day, millions of us take to the road to drive or cycle to work, to walk to school, or just to go about our business. We take the transport networks for granted, and assume cities will deal with everything. But, how do cities understand the transport network? How do cities know where to invest millions upgrading the roads? How can they make roads less congested, safer, and more sustainable for everyone? The ‘Citireps’ project (MKC and Vivacity Labs initiative) was implemented in Milton Keynes in 2018 and is helping Milton Keynes by providing live and historic data on the types of vehicles and activities on its networks. The system consisting of 2500 sensors covers all major junctions, city car parking, bus networks and pedestrian areas. This provides visibility travel patterns and real time city busyness. This has been achieved by deploying low cost, highly capable sensors at scale across the city. The sensors equipped with on-board cameras and processors, (after discarding images so within data protection regulations) provide movement data in real time. This data when fed into a reporting system, gives cost effective, accurate, live information on activity. Additionally, with machine learning (AI) capabilities the system can accurately predict future levels of congestion, identify routing choices for traffic, Parking availability, busyness of buses and the impacts incidents. City planners are now deploying the information to traffic control systems so that the city continues to operate efficiently for all its citizens, answering the questions set in the opening paragraph. C. Background Information Describe the legislative or policy framework under which the initiative is taking place, for example, a public policy document at the central or local government level, a policy statement, a covenant, a compact or internationally recognized agreement including, for example, the Sustainable Development Goals, and/or the New Urban Agenda MKC adopted its new Mobility strategy in 2018 (Local Transport Plan). This strategy places the introduction of technology at the forefront in assisting with the delivery of improved sustainable mobility for all citizens, visitors and businesses. The policy aligns with national UK strategy objectives, and the project was supported by UK government funding to deploy city-scale sensor technology providing real time data. The project is held up as part of the UKs grand challenge to deliver future of mobility. D. Summary of the Origins of the Initiative Describe what sparked the initiative and how the initiative came about and what challenges or issues of sustainable development it is meant to address using the following lead questions as your guide (350 words max) WE/GBR/MILTON KEYNES/0273 1. Describe briefly the reason(s) for undertaking the initiative including social, economic, political or cultural challenge(s) or issues confronting the city/region/community. Include where relevant number of people, enterprises or institutions affected. Milton Keynes is one of the fastest growing areas in the UK. Current plans will see the city double in size over the next 20 years to a population approaching 500,000. Whilst the city currently enjoys low levels of congestion, much of its mobility is currently met by private vehicles and this trend is likely to continue in the short to medium term. Without the resources to double the size of the transport network to meet the forecast doubling of travel demand the city needs to explore how it can sustainable increase its growth without suffering from increases in congestion, worsening air quality and reducing economic outputs, leading to an overall degradation of service to citizens. Taking forward cost-effective technological solutions was a key condition for supporting growth, and the council in developing its futures strategy tested this strategy with key stakeholders and citizens and found that the population fully supported the development of a Smart Shared Sustainable Mobility initiative which look to apply relevant technology-based solutions for addressing mobility challenges. Importantly the remit was also to ensure that mobility improvements needed to be inclusive for all citizens and helped address mobility equality along with ensuring sustainable economic growth. 2. Describe the goals of the initiative in terms of desired change or outcome and timeframe for achieving the change or outcome (for example, change in policy, strategy, business model, technology, means of implementation, financing arrangements, human development and empowerment, measuring and evaluating progress and impact, etc.). The goal of the initiative is to provide real time accessible information around travel options to all residents and visitors to the city. The information, provided as live real time snap shops can provide stimuli for travel behaviour change potentially away from unsustainable modes, or negative impacts of travel. By providing a ‘busyness’ indicator of all travel options, informed choice can be made around the options that best meet the needs of the traveller. (practical application of Mobility as a Service) Potentially the choice can be influenced by intervention either by the transport authority or service provider which align to wider objectives. This could take the form of dynamic pricing of services such as parking, or bus transit, to lower or raise prices to influence demand that best meets the agreed objectives. The system has a unique capability around Artificial intelligence, so can forecast impact based on machine learning, meaning it can prevent negative impacts by taking advanced decisions on how and when information is given 3. Describe whether the innovation involves any partnership (public-public, public-private, public-community, etc.) and if yes, who is or was the leading partner(s), the role they each played and whether other parties have benefited or are benefiting from your innovation and how? WE/GBR/MILTON KEYNES/0273 The delivery of the project was undertaken through a partnership between the technology suppliers Vivacity Labs and Milton Keynes Council. Milton Keynes Council outlined the challenges cities faces, particularly medium sized cities tacking the challenge of increasing demand on city resources and the negative impacts this was creating. MKC made senior experts available to discuss issues faced and how typically these were approached. It was quickly recognised that traditional approaches of increase travel supply could not meet forecast demand particularly for individual car travel options. However, it was established that most travel demand could be met if demand was spread between all available modes, and a barrier to achieving this was the availability of relevant up to date information. Knowing what options were available for transport users that was reliable and accurate was seen as key. Working in partnership the first stage was to develop initial capability of sensor technology to capture information. Small pilot trial was successfully concluded in 2015/16. The challenge was then to deploy this capability at scale that was economically and technically viable. Vivacity Labs clearly had the technical capability to develop the system, so following the development of a full business case the proposal was presented to UK Government as a first of a kind initiative. UK Government supported the application and provided partial funding to launch a city scale initiative which deployed 2500 sensors across MK transport network. The initiative was developed to demonstrate large scale deployment and the tech delivered the defined outputs in the business case (ie it worked) The potential therefore realise in a real city. The benefits now accruing are that through a practical demonstration that the system can be fitted in a city in a short period of time and can output valuable data. this has led to the initiatibve being roled out in other cities incuding Oxford, Cambridge , Nottingham and on the entire UK canal network. in each of the following locations the system has been tailored to meet particular challenges, eg Nottingham to report on Air quality, in Oxford Parking in remote areas and for the canals a permit monitoring capability. 4. Describe the resources used for implementing the initiative, including funding/financing strategy or arrangements and any significant contributions that are not in cash, for example, in human, technical or managerial resources. As described above the project recieved expert input from senior professional officers from the city. This was also supported by input from academic experts (Cambridge University) and business advisors providing advice from a commercial angle. This was achieved through a specially created advisory board that assited in developing the initial business case to be put to the Government funding body (innovate UK). Professional planners, highway engineers and transport planner also supported the installation process and worked with Vovacity Labs to develop the back office reporting systems. This partnership approach to delivery instigated by MKC was designed to ensure the system met the needs of both MKC and through this potentially all other transport suthorities (cities) in the UK and beyond. In terms of financial resources, the project secured approximately £1.7m investment form UK governement, matched by approximately £1.3m from private invetors. the council supported via free advice and technical support for installations WE/GBR/MILTON KEYNES/0273 E. Summary of the Innovative Aspect(s) of the",,2019.0,,221795915,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
5caa35987b5df145a5bdc8dd4766dbeff952ebe2,https://www.semanticscholar.org/paper/5caa35987b5df145a5bdc8dd4766dbeff952ebe2,Research on Application and Development of Financial Big Data,"This paper discuss the basic thinking, methods and tools of software engineering, masters the financial business knowledge, the analysis and design theory and methods of financial information systems, and has the ability to analyze, design, implement and maintain financial information systems, and can be used in financial applications. IT companies such as development companies and financial information system providers engage in the analysis, design and implementation of software, or engage in the analysis, design, implementation, maintenance and management of financial information systems in the IT departments of various financial institutions such as banks, securities and insurance. A financial information-based composite software talent with a solid professional foundation, broad knowledge, and the ability to adapt to the future development of information technology. Specifically, it is reflected in four aspects: knowledge system, professional skills, project experience and comprehensive quality: Introduction On July 1, 2017, the General Office of the State Council issued the “Opinions on Strengthening the Service and Supervision of Market Subjects by Using Big Data”; on July 4, the State Council issued the “Guiding Opinions on Actively Promoting the “Internet+” Action” On September 5, the State Council issued the ""Outline for the Promotion of Big Data Development."" The intensive introduction of these heavy documents marks the official establishment of China's big data strategic deployment and top-level design. Benefiting from the rapid expansion of the big data market, the demand for related IT support has exploded. Among them, enterprises that provide big data infrastructure, big data software technology services, and industry big data content consulting services have brought unprecedented Customer group. IDC predicts that by 2020, the company's expenditure based on big data computing and analysis platform will exceed 500 billion US dollars, and the compound growth rate will reach 34.1% in the next 5 years; in the next 3 to 5 years, China needs 1.8 million data talents, but currently only About 300,000 people. At the same time, China's colleges and universities in cloud computing, data science and other majors are still in their infancy, and the talents cultivated each year are far from meeting the needs of the industry. Therefore, it is imperative to open a big data major and accelerate the cultivation of talents. The financial industry is the industry that relies most on data and is the easiest to realize data. In recent years, emerging financial institutions such as consumer loans and P2P are the products of the combination of big data technology and finance. At present, the demand for big data talents in finance is extremely strong in China. Only Internet finance is one year, and the growth rate is 3-5 times per year. It is generally believed that there will be a gap of 1 million talents in Internet finance, and the most lacking is big data risk control talents, including data mining and statistical modeling talents from primary to advanced. Core and Featured Courses Cloud Computing and Introduction to Big Data As an introductory course in the direction of this major, this course introduces students to the concepts, technologies and applications related to cloud computing and big data, and enables students to establish a preliminary understanding of the relevant knowledge, technology and development prospects of the profession, as a guide for the follow-up course. Distributed Computing Framework Foundation As the foundation and core technology course of this major, this course introduces students to the basic concepts, installation and configuration of the Hadoop distributed computing system, distributed programming model (Map/Reduce), distributed file system (HDFS), and related scheduling. , monitoring and maintenance tools enable students to build a basic understanding of distributed computing systems, master the primary distributed application design and implementation methods, and lay the theoretical and practical foundation for subsequent in-depth courses. Distributed Database Management and Development As a core technical course in this major, this course introduces students to the basic concepts of distributed databases, installation and configuration, management and maintenance, data access and development. The course focuses on NoSQL databases such as HBase, MongoDB, Redis, etc., and describes their use and development in a distributed environment. To enable students to establish a basic understanding of distributed databases, master the primary distributed database application system design and development methods, and lay the theoretical and practical foundation for the subsequent in-depth courses. Distributed Computing Framework Component Technology As a core advanced course in this major, this course introduces students to mainstream components on the Hadoop distributed computing platform, including Hive, Pig, Sqoop, Flume, Kafka, Zookeeper and more. Enable students to have a complete Hadoop ecosystem-based design and implementation of big data applications. Real-time Calculation and Memory Calculation As a core advanced course in this major, this course introduces students to high-performance distributed computing frameworks, including Storm and Spark, as a more powerful alternative to the Hadoop framework. Data Visualization Technology As an elective course in this major, this course introduces students to the basics of data visualization and the design and use of platforms and development tools, including Excel, Reporting Services, Chart.js, D3.js, Tableau, etc. Through this course, students will be able to present the results of big data processing in an efficient, flexible and friendly manner. Data Statistics and Analysis As a core advanced course in this major, this course introduces students to statistical analysis techniques based on Python and R. Including data file editing and finishing, basic statistical analysis, parameter estimation and hypothesis testing, non-parametric testing, analysis of variance, correlation analysis, regression analysis, cluster analysis, discriminant analysis, factor analysis, correspondence analysis, reliability analysis, survival Analysis, time series analysis, and the drawing of statistical graphs enable students to master the processing and analysis methods of typical industry business data. Knowledge System Mathematical basis: Including calculus, linear algebra, probability statistics, numerical analysis, etc. IT foundation: Including operating systems, networks, databases, software engineering, programming techniques, data structures and algorithms, etc. Knowledge base in the financial sector. Including international finance, marketing, insurance, securities investment, etc. Professional Skills Database system management and development: MySQL, MongoDB, Redis, HBase, etc. Big Data Application Development Language: Java as the core, supplemented by Python, Scala, R, etc. Construction, configuration, development and deployment of big data processing frameworks: Hadoop, Storm, Spark, etc. Use of data analysis and presentation tools: reporting tools, D3.js, etc. Project Experience Familiar with enterprise software project life cycle, development process, specification, etc. Understand and implement software quality requirements: performance, security, scalability, maintainability, reliability, etc. Understand the financial industry: industry background, business model, market characteristics, and how data and IT systems are used in the financial industry Comprehensive Quality Good professional basic qualities: document writing, presentation reporting, business communication, etc. Strong learning ability and study habits, has a certain degree of microinnovation, data awareness Course Settings Table The professional competence-course structure derivation process mainly includes two stages: “computation ability theme” and “capability-curriculum structure transformation”. The main process of the first phase of the ""computational power theme"" is as follows: Figure 1. The data analysis process of ""ability topic calculation. Hadoop Big Data Integrated Experiment System This experimental system is designed to provide students with a complete set of Hadoop and its environment, design, development, monitoring, maintenance tools, software and services. With this experimental system, the experimental and training environment requirements of the core technology courses of this major can be met. This experimental system is divided into two major components: A virtual lab environment for students to learn big data. The environment is carried out by means of the aforementioned virtualized desktop teaching system, and the network administrator configures the big data learning virtual machine in advance for the students to use. The real environment for research or large-scale case presentations. This environment is carried out through several servers. Main Function A. Basic platform: The basic platform for big data storage and processing, which can realize the storage and management of massive data, support common components of platforms such as Hive, Impala, Pig, Spark, and Yarn, and provide support for data analysis services on the platform. These common components increase the ease of use of platform data, making data manipulation and data analysis easier to use, saving labor and reducing labor time. B. Data integration: support the unified storage of massive structured data, semi-structured data, and unstructured data, deepen the expansion of enterprise intelligence and service capabilities, and improve the decision-making level of enterprises. We can use enterprise-level data ETL tools or open source ETL tools. For example, Flume, Sqoop, Kafka, etc., integrate externally structured, semi-structured and unstructured data into big data platforms. Through this pla","DEStech Transactions on Social Science, Education and Human Science",2019.0,10.12783/dtssehs/aems2018/28012,169539554,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
972b20f1a231cb991f43b5a8dc99b8892c6a3db9,https://www.semanticscholar.org/paper/972b20f1a231cb991f43b5a8dc99b8892c6a3db9,Stress Detection for Keystroke Dynamics,"Background. Stress can profoundly affect human behavior. Critical-infrastructure operators (e.g., at nuclear power plants) may make more errors when overstressed; malicious insiders may experience stress while engaging in rogue behavior; and chronic stress has deleterious effects on mental and physical health. If stress could be detected unobtrusively, without requiring special equipment, remedies to these situations could be undertaken. In this study a common computer keyboard and everyday typing are the primary instruments for detecting stress. Aim. The goal of this dissertation is to detect stress via keystroke dynamics – the analysis of a user’s typing rhythms – and to detect the changes to those rhythms concomitant with stress. Additionally, we pinpoint markers for stress (e.g., a 10% increase in typing speed), analogous to the antigens used as markers for blood type. We seek markers that are universal across all typists, as well as markers that apply only to groups or clusters of typists, or even only to individual typists. Data. Five types of data were collected from 116 subjects: (1) demographic data, which can reveal factors (e.g., gender) that influence subjects’ reactions to stress; (2) psychological data, which capture a subject’s general susceptibility to stress and anxiety, as well as his/her current stress state; (3) physiological data (e.g., heart-rate variability and blood pressure) that permit an objective and independent assessment of a subject’s stress level; (4) self-report data, consisting of subjective self-reports regarding the subject’s stress, anxiety, and workload levels; and (5) typing data from subjects, in both neutral and stressed states, measured in terms of keystroke timings – hold and latency times – and typographical errors. Differences in typing rhythms between neutral and stressed states were examined to seek specific markers for stress. Method. An ABA, single-subject design was used, in which subjects act as their own controls. Each subject provided 80 typing samples in each of three conditions: (A) baseline/neutral, (B) induced stress, and (A) post-stress return/recovery-to-baseline. Physiological measures were analyzed to ascertain the subject’s stress level when providing each sample. Typing data were analyzed, using a variety of statistical and machine learning techniques, to elucidate markers of stress. Clustering techniques (e.g., K-means) were also employed to detect groups of users whose responses to stress are similar. Results. Our stressor paradigm was effective for all 116 subjects, as confirmed through analysis of physiological and self-report data. We were able to identify markers for stress within each subject; i.e., we can discriminate between neutral and stressed typing when examining any subject individually. However, despite our best attempts, and the use of state-of-the-art machine learning techniques, we were not able to identify universal markers for stress, across subjects, nor were we able to identify clusters of subjects whose stress responses were similar. Subjects’ stress responses, in typing data, appear to be highly individualized. Consequently, effective deployment in a realworld environment may require an approach similar to that taken in personalized medicine.",,2018.0,10.1184/R1/6723227.V3,67285768,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
232d11a7a3181f071c4d82a3578930eff9af8d93,https://www.semanticscholar.org/paper/232d11a7a3181f071c4d82a3578930eff9af8d93,"Advances in Grid Computing - EGC 2005, European Grid Conference, Amsterdam, The Netherlands, February 14-16, 2005, Revised Selected Papers",,EGC,2005.0,10.1007/b137919,46196158,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
3894983b1d61413fd98ef4e9bcc8ab892aa8a0cb,https://www.semanticscholar.org/paper/3894983b1d61413fd98ef4e9bcc8ab892aa8a0cb,Urban Robotics: Achieving Autonomy in Design and Regulation of Robots and Cities,"Are cities ready for self-driving, artificially intelligent, vehicles and robotics? The urban marketplace is increasingly filled with products emblematic of “smart” cities, from widely discussed autonomous vehicles to smaller variations on the theme, such as robotics for delivery, security, and entertainment. Altogether, such urban robotics represent a new wave of technology in which digital sensors, networked devices, and their associated data stores are given the algorithmic, physical, and legal means to move in public space. As time goes on, the public is increasingly likely to encounter self-driving vehicles, robots, and drones on city streets, sidewalks, and in urban airspace. How should cities respond to these new and impending technologies? 
 
Firms have technological, market, and financial interest in testing and deploying their products in public space, but the implications for cities span a broad array of intended and unintended consequences. Cities are natural sites of experimentation for firms interested in bringing these products to market, and the perception of economic opportunity associated with tech firms is leading city representatives to reclaim public spaces, transforming them into testbeds for product development. It is worth noting, however, that experimentation involves trial and error, and there are limitations to the ability of artificial intelligence to navigate the wide range of conditions and events that comprise the urban environment. Ultimately, the design of the environment may be as important to the success of urban robotics as the design of the hardware and software that comprise these products. These are matters that city officials care about; the design, function, and finance of urban built environments is generally the purview of local government. Additionally, artificial intelligence imbues devices with the purpose of replacing as well as augmenting the roles and responsibilities of persons, and this tension exists at the local level. Significant new questions in law, such as legal liability for the performance of artificially intelligent devices, are being addressed as autonomous vehicles and devices enter public rights-of-way. Lastly, these products generate rich data stores about the public, bringing market potential along with the coupled moral hazard of data monetization and loss of privacy, including surveillance. Which parties are positioned to benefit from this experimentation, and which will absorb the costs? In the face of these potentially widespread and enduring industrial and technological changes, how might cities act in the public interest? 
 
The answers to these questions lie as much in the institutional arrangements designed to govern this new wave of technologies as it does in the intrinsic capabilities of these products. Anyone evaluating the existing policy environment for artificially intelligent devices today would find technological optimism as well as pessimism, conflicting perspectives of the public interest, and preemptive acts at the state and federal levels. In particular, preemption in current policy-making raises issues, because the consequences and cost of product design, including safety and surveillance as well as convenience and expense, play out at the local level. As city officials ask their residents to co-exist with robots and negotiate with firms over the transaction costs that accompany these products, they need the flexibility and funding necessary to adapt to market conditions and the authority to act as market makers. In the best of circumstances, federal agencies provide guidance and domain expertise, while states provide a supportive framework for cities to operate in, with a backstop against the expansive possibility of harm. In the most egregious cases, preemption threatens to revoke the rights of the persons who, at the local level, are asked to bear the risk and cost of residing with robots, and to prevent the resolution of conflicts through local levels of government. Preemption debates in technology law have already arisen around net neutrality, sharing economy platforms, and municipal broadband, with important consequences. Some proposed federal and state laws and existing state statutes already preempt cities on robotics in several important ways. 
 
The purpose of this article is to provide a framework for public decision-makers to engage effectively with the firms that are bringing artificially intelligent robotics to market in public space. With an institutional economic perspective, this article suggests a means for evidence-based policymaking by breaking down design and its evaluation into constituent sequential components, recognizing the private and social costs of experimentation in cities, and recommending a limited scope for state and federal intervention. Part II begins by defining the characteristics of the current wave of robotics entering public space, placing public-facing robotics within the theory of the nature of technology, and elaborating on the process of product design with algorithmic feedback for machine learning in complex urban environments. Part III explores the opportunities and hazards that await cities as sites of experimentation, and introduces a comparative approach to policy-making to forestall social externalities while permitting technological change. Part IV explores the policy environment that is already taking shape for governing artificially intelligent robotics in public space. Part V draws on the arguments from the preceding parts of the paper to recommends against broad express preemption or field preemption at the state and federal level of local governments in robotics law. Part VI addresses possible counter arguments. Part VII concludes with a research agenda for urban robotics going forward.",,2018.0,10.2139/SSRN.3145460,86693718,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
ded67de0581d3c8b410473e123a9ba0edd7237a7,https://www.semanticscholar.org/paper/ded67de0581d3c8b410473e123a9ba0edd7237a7,"Dry Rivers, Scary Strangers: Are Financial And Cyber Crises Alike?","The internet and the financial system show crucial affinities: both are tightly interconnected global networks whose orderly functioning is a prerequisite for economic prosperity. In financial and cyber crises alike, vulnerabilities are a consequence of distorted economic incentives, contagion is fast, and the most serious risk is loss of trust. Lessons learned from financial meltdowns translate to the cybersecurity world: stability cannot be achieved until policies are in place to address all of these issues. Steps have been taken to rectify incentives, as exemplified by recent European Union legislation. Data that helps identify weak nodes are still scarce, notwithstanding recent efforts. The preservation of trust is the hardest challenge: in the financial system, a global governance framework was put in place to help maintain and rebuild confidence at critical junctions, but conflicting national interests make it difficult to establish a cyber equivalent. 1 Please cite as: Biancotti, Claudia and Paolo Ciocca, “Dry Rivers, Scary Strangers: Are Financial and Cyber Crises Alike?,” in Demchak, Chris C. and Benjamin Schechter, eds. Military Cyber Affairs: Systemic Cyber Defense 3, no. 2 (2018). 2 Senior Economist at the Bank of Italy, the Italian central bank. The views here expressed are those of the authors and should not be attributed to the Bank of Italy. 3 Commissioner at Consob, the Italian Securities and Exchange Commission. The views here expressed are those of the authors and should not be attributed to Consob. 1 Biancotti and Ciocca: Are Financial And Cyber Crises Alike? Published by Scholar Commons, 2018 2 Advanced economies are immersed in cyberspace. In 2016, 95 percent of businesses in OECD countries had a broadband internet connection; 77 percent had a web presence. More than half of the adult population had purchased a product or service online, compared to 36 per cent in 2010. As digitalization progresses, a growing share of production and consumption activities depend on connectivity. From an economist’s point of view, there are evident analogies between the internet and the global financial system: both are tightly interconnected networks that provide lifeblood to the real economy, via transfers of information and funding, respectively. Indeed, a financial crisis and a cyber crisis look alike in three key dimensions: (i) vulnerabilities accumulate because of excessive risk-taking on the part of some agents, which eventually translates to systemic risk on account of interdependencies; (ii) disruption can start at a single weak point and spread to the whole system in a matter of days or even hours; (iii) the ultimate casualty is trust: once it is lost, transactions – and the whole economy – can grind to a halt as counterparties disconnect from each other. Policy responses can be deployed to address these problems so that crises can be prevented or at least managed effectively. In the financial system, safeguards have been established over time: examples are strict capital requirements for lenders, orderly resolution procedures for failing institutions, and collection of micro-level data aimed at identification of individual weak nodes. They are not all-encompassing, but they do help reduce the risk. Where cyberspace is concerned, this process is still in its infancy. Some results have been achieved with respect to (i) above. A small but insightful literature on the economics of cybersecurity points out that distorted economic incentives, rather than technically sophisticated attacks, are at the heart of the problem. Software is born vulnerable because of network externalities. For products such as operating systems and messaging platforms, the value increases with the size of the installed base; developers 4 OECD, OECD Digital Economy Outlook 2017, (Paris: OECD Publishing, 2017), 161-171. 5 European Central Bank, “The Eurosystem Household Finance and Consumption Survey: Results from the First Wave” , Statistics Papers Series no. 2 (April 2013): 7-8. 6 See among others: Ross Anderson, “Why Internet Security Is Hard – An Economic Perspective”, Proceedings of the 17 Annual Computer Security Applications Conference (December 2001). Hal Varian, “Managing Online Security Risks”, The New York Times, June 1, 2000. Tyler Moore and Ross Anderson, “Internet Security”, in The Oxford Handbook of the Internet Economy, ed. Martin Peitz and Joel Waldfogel (Oxford: Oxford University Press, 2011). 2 Military Cyber Affairs, Vol. 3 [2018], Iss. 2, Art. 7 https://scholarcommons.usf.edu/mca/vol3/iss2/7 DOI: https://doi.org/10.5038/2378-0789.3.2.1061 3 forego security as they scramble to get to the market first, attract a critical mass of users, and shut competition off. The absence of developer liability for buggy software does not help. The market for cyber defense is plagued by information asymmetries. Vendors know more than their customers: they may have an opportunity to push whatever solution maximizes their own profit, independent of how effective it is. Finally, the cost of cyber attacks in many cases is not fully internalized by the immediate victims: for example, the owner of a vulnerable IoT device that gets recruited into a botnet typically has no statutory liability (yet) for damage caused by the botnet. Some corrective measures have already been introduced, while others are being drafted. In the European Union, the General Data Protection Regulation (GDPR) – coming into force in May 2018 – imposes steep fines to businesses that put the confidentiality of personal data at risk and mandates disclosure of breaches to both authorities and data subjects. The Directive on Security of Networks and Information Systems (NIS), also coming into force this year, introduces cyber protection requirements and incident disclosure obligations for key players in sectors such as energy, finance, and healthcare. A regulation proposal put forth by the European Commission envisages an EU-wide framework for security certification of hardware and software, fashioned after the notoriously strict CE scheme for safety, health and environmental protection. These are necessary steps, but they are not enough; the effects of GDPR and NIS are confined to certain cases or sectors. More theoretical work is needed to define broader principles: generalized liability for damage caused to third parties may be a good idea, yet an ordinary citizen whose email account gets spoofed by phishers should probably not be forced to compensate victims. With respect to (ii), there is still a significant knowledge gap about the location and interconnections of weak nodes. As pointed out by the G7 Finance Ministers and Central Bank Governors in 2017, “reliable, impartial, comprehensive and widely accessible” data on the frequency and economic impact of cyber attacks are still rare. The same goes for information on network and economic connections, e.g. through digital and physical supply chains. Evidence from the Bank of Italy’s business surveys suggests that cyber risk may be concentrated among high-tech, non-ICT businesses, which are more exposed and interesting to attackers compared to low-tech ones, and less proficient at defense than the ICT sector. The data also suggest that mass adoption of relatively simple internet-based solutions, such as e-commerce 7 G7 Finance Ministers and Central Bank Governors, Bari Communiqué, May 12-13, 2017. 8 Claudia Biancotti, “The Price of Cyber (In)security: Evidence from the Italian Private Sector”, Bank of Italy Occasional Papers no. 407 (November 2017). 3 Biancotti and Ciocca: Are Financial And Cyber Crises Alike? Published by Scholar Commons, 2018 4 platforms, cloud computing services or IoT devices is a stronger risk factor than the selective adoption of advanced technologies, like machine learning or industrial robotics. These indications are vital in understanding which sectors of the economy need urgent intervention, be it in terms of awareness campaigns, dedicated incentives, or regulation. The problem sub (iii) is the hardest to solve. After the 2009 financial meltdown, trust was only restored after a series of large-scale, highly controversial injections of public money in the banking system, and substantial reinforcement of a global governance framework which encompasses broad-based organizations such as the Financial Stability Board, the Bank for International Settlements and the International Monetary Fund, regional institutions such as the European System of Financial Supervision, and national authorities. In a cyber crisis, there is no immediately evident equivalent of a public-sector backstop. Perhaps more importantly, a global governance framework is very hard to build because in cyberspace a crisis is generally triggered by an intentional act of aggression; an adversary is in the picture and may serve, directly or indirectly, the interests of a nation-state. In this sense, the right comparison might be with currency or trade wars; the problem is mostly one of political, diplomatic and military relations, especially in a world where the weight of authoritarian governments increases. This is where input from economics is not sufficient and must be complemented by scholarship in the various facets of international relations. 4 Military Cyber Affairs, Vol. 3 [2018], Iss. 2, Art. 7 https://scholarcommons.usf.edu/mca/vol3/iss2/7 DOI: https://doi.org/10.5038/2378-0789.3.2.1061",Military Cyber Affairs,2018.0,10.5038/2378-0789.3.2.1061,169335086,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
d2de228ac4fdb29c39d9302ddc4839f9028fff53,https://www.semanticscholar.org/paper/d2de228ac4fdb29c39d9302ddc4839f9028fff53,SU-E-I-69: A Cloud Based Application for MRI Brain Image Processing,"Purpose: In this study we present a cloud based application dedicated to MRI brain image processing, providing the flexibility to end users for managing and processing medical image data, dynamically allocating computational resources. Methods: The proposed application is based on the Software as a Service (SaaS) cloud model and it can be accessible by any available browser. The developed cloud takes advantage of the Infrastructure as a Service (IaaS) in order to easily scale its capabilities. Virtual Machines are used on demand of the users’ requested pipeline. The implemented techniques were incorporated on a cloud application framework consisting of a web interface where the user can upload data (NIfTI format) and initiate the processing algorithms. The proposed application is providing the following functionalities: a) a user-based authentication and authorization mechanism, b) an application data manager which is a tool enabling the user to manage his/her uploaded image data, c) a “Data Processing Task” where the user is able to select the appropriate processing algorithm to define his/her processing pipeline on demand. Results: The proposed implementation has already been tested in several end-users’ devices executing a pipeline for brain tumor detection in followup scans. A pipeline processing has been implemented, where the user uploads two scans of a patient (before and after treatment), they are registered, normalized and a difference map is calculated for quantitative evaluation of tumor growth or reduction. Results are stored in the cloud storage space and are available to the user anytime, at request. Conclusion: Cloud computing is one of the current biggest trends in Information Technology providing and accessing computing resources via Internet adhoc. Our goal is to offer a cloud implementation providing a way to deploy medical image processing as an application, enabling algorithm sharing without the hustle of maintaining complex infrastructures. This research has been co-financed by the European Union (European Social Fund ESF) and Greek national funds through the Operational Program “Education and Lifelong Learning” of the National Strategic Reference Framework (NSRF) Research Funding Program: Thales. Investing in knowledge society through the European Social Fund.",,2015.0,10.1118/1.4924066,118669676,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
4f8c4025b2ce0c0eb837b7389bba31f6f1cae7c7,https://www.semanticscholar.org/paper/4f8c4025b2ce0c0eb837b7389bba31f6f1cae7c7,Implementing Operational Analytics using Big Data Technologies to Detect and Predict Sensor Anomalies,"Operational analytics when combined with Big Data technologies and predictive techniques have been shown to be valuable in detecting mission critical sensor anomalies that might be missed by conventional analytical techniques. Our approach helps analysts and leaders make informed and rapid decisions by analyzing large volumes of complex data in near real-time and presenting it in a manner that facilitates decision making. It provides cost savings by being able to alert and predict when sensor degradations pass a critical threshold and impact mission operations. Operational analytics, which uses Big Data tools and technologies, can process very large data sets containing a variety of data types to uncover hidden patterns, unknown correlations, and other relevant information. When combined with predictive techniques, it provides a mechanism to monitor and visualize these data sets and provide insight into degradations encountered in large sensor systems such as the space surveillance network. In this study, data from a notional sensor is simulated and we use big data technologies, predictive algorithms and operational analytics to process the data and predict sensor degradations. This study uses data products that would commonly be analyzed at a site. This study builds on a big data architecture that has previously been proven valuable in detecting anomalies. This paper outlines our methodology of implementing an operational analytic solution through data discovery, learning and training of data modeling and predictive techniques, and deployment. Through this methodology, we implement a functional architecture focused on exploring available big data sets and determine practical analytic, visualization, and predictive technologies. APPROACH This study developed an operational analytics implementation that uses Big Data technologies and machine learning algorithms to determine and predict sensor anomalies. A previous study [1] showed that Big Data Analytics can uncover anomalies that may be missed through conventional analyses. This study enhances that effort and shows a methodology to implement operational analytics that can be applied toward common solutions for data analysis. Our operational analytics implementation relies on continuous learning from historical data to analyze data in the stream of real-time operations. In the previous study, where data was identified that can be used to uncover anomalies, this implementation extends that approach and now identifies trends and correlations that reveal anomalies that can be missed by traditional analytic techniques with limited datasets. This study adopted a three-step methodology to implementing operational analytics – Discovery, Modeling and Operations as shown in Fig. 1. Copyright © 2016 Advanced Maui Optical and Space Surveillance Technologies Conference (AMOS) – www.amostech.com Fig. 1. Operational Implementation Approach Fig. 1 shows the three steps to implement operational analytics and the continuous feedback between learning and operational deployment. The following sections will elaborate on the methodology employed as applied to a realworld problem of analyzing large datasets such as would be encountered at an operational site.",,2016.0,,221800632,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
c1fe76025999ecbc22b7c721fffb4fb668bbf0f9,https://www.semanticscholar.org/paper/c1fe76025999ecbc22b7c721fffb4fb668bbf0f9,Google's Go and Dart: parallelism and structured web development for better analytics and applications,"Big Data -- the new buzz word in the IT world -- is synonymous with the concept of data-driven decision making. All across the industry, enterprises are collecting all sorts of data including client preferences, trends amongst social networks and information about competing businesses, at an unprecedented scale with the focus on making intelligent decisions. The decisions made from processing such data have a direct impact on both the businesses and its clients through higher profit margins and smarter services respectively. The combination of sophisticated analytics and data-driven decision making enables new types of solutions, from mining the human genome to deriving sentiments from social networks. This is all leveraged through consistent innovation of underlying technology, increased competition and cloud-based SaaS. Clouds are the frontiers for most business IT solutions as they idealize the ""as a service"" methodology, interoperability between services and correct billing metrics. Thus, it easily follows that most data analytic tools are designed to be run on cloud clusters rather than traditional data centres. 
 
In a world where there is an emergence of extensive use of analytics, data and fact-based decision making, spontaneous sorting of data becomes imperative. Industries like finance, pharmacy and others rely heavily on data to assess their strategies and customer requirements, and to respond quickly for the better. Hence, analytics are crucial for knowledge discovery, business growth and technological improvements. Every second, massive quantities of data are being generated and there is a need for data analytic tools that are easily integrated, scalable, and informative. While we are promisingly producing vast amounts of information, there is currently a wide gap between its potential and its realization. There are many problems with Big Data, including the heterogeneity of data, scale, readiness and complexity. The dynamic business environment requires a company to quickly adapt to newer technologies that can provide better solutions. A high throughput of services translates into improved business efficiency. It is often the case that an innocent shift in development tools can catapult a paradigm altering change. In this workshop, we discussed two emerging technologies recently launched by Google -- Go and Dart; which through their advents in scalability, improved parallelism, and structured web development greatly enhanced the capacity and quality of building applications designed to work from handheld mobile devices, to data crunching frameworks such as Google's Map Reduce, IBM's Big Insights and others. 
 
The initiative behind Go is to create a language that allows programmers to exploit concurrency in programs by providing simple yet powerful features built into the language that do not require complex code and high maintenance. With the introduction of GPGPU's for execution, it is essential to have concurrency dependent applications in order to improve their performance. Go provides simplicity and improvement in leveraging processing power while providing simplified features of traditional programming languages. Not following the traditional object oriented programming model, Go was initially targeted for system programming including applications for distributed systems, storage infrastructure, networking infrastructure and the RPC layer, however, its features make it an excellent language deploying application on concurrent systems. Go has higher throughput, as compared to Java or Python because of its dynamic stack- and core- focused features for speed and concurrency. Being a hybrid between C and Python, Go is type-safe and memory-safe, has fast start-ups, latency-free garbage collection, and high-speed compilation. Businesses gain the advantage of availability of services by using a language that is designed to be concurrent. Go's ""lightweight concurrency"" allows developers to create sets of lightweight communicating processes called goroutines. The programmer doesn't need to worry about synchronization, locking mechanisms such as semaphores and race conditions. This affects the cost of back-end and front-end instances which allows tiers to deliver more responses per instance. 
 
Moreover, Go uses significantly less memory as compared to Java which is JVM dependent. Go is predominantly used because of its easy integration with frameworks like Map Reduce. Go provides static typing and makes unit testing much easier to the languages like Java and C. Go is designed specifically to provide easy integration with Google App Engine and Google Compute Engine. Finally, Go compiles static binaries with no dependencies, so Go programs can simply be dropped onto a server and deployed. 
 
Today, the web is everywhere -- from handheld mobile devices to television sets to traditional desktops. It is very easy to write a web page that is universally accessible on any device via a browser. There are no installation or update procedures, making the user experience very pleasant. JavaScript is now supported by almost every browser because of its convenient functionality. However, alongside the beauty of JavaScript is its unpleasantness in debugging, performance across browsers, and security on client devices. Dart is a new web programming language and methodology in development by Google for creating structured web applications. It is developed with the aim to encompass aspects such as simplicity, efficiency, and scalability while combining new language features with familiar language constructs into a clear, fluent syntax. The fundamental reason for its creation is to ultimately replace JavaScript because of its unstructured and inconvenient development paradigm. Dart aims to tackle these issues by providing lexical scoping, closures, and optional static typing; not to mention integrated development and debugging are also provided in the Dart Editor and SDK, which supports major common interactions such as refactoring, breakpoints, code completion, code navigation and much more. In addition, the SDK provides a standalone virtual machine, package manager, and Chromium with an embedded Dart VM. Dart can also be compiled to JavaScript, which makes it usable on all modern desktop and mobile browsers with an additional plus of being able to run on a server in a stand-alone Dart VM. Fundamentally aiming to create structure and flexibility for the web, focusing on supporting a full range of devices --- including phones, tablets, laptops, and servers, and providing environmental and supportive tools to run efficiently across all major modern browsers, Go and Dart are developed specifically for a simplification of the coding paradigm and better integration with existing technology. Both Go and Dart aim to improve the modern day programming model and unstructured web source code. 
 
This full-day workshop focused on the core technologies of Go and Dart, including building and deploying applications integrated with analytic frameworks on cloud clusters. The workshop was split into three segments. In the first segment, the objective was to take a hands on approach to walk through the features of Go and Dart. The participants wrote code themselves and explored both the languages on a first hand basis. We introduced the languages and discussed their basic constructs and some advanced features. 
 
In the second half, we examined an in depth example of creating a web application using Go, Dart, Map Reduce, and Google App Engine. The goal was to demonstrate Go's simplicity, power of concurrency, and structured web development and integration with existing API's. In the last segment, we concluded with a discussion about the impact Go and Dart can have on different parts of the IT industry. As the shift to development for cross mobile platform web applications increases - we expose the usefulness of using Dart instead of JavaScript for building mobile web applications with frameworks like Apache Cordova and IBM Worklight. Furthermore, we discussed questions like how these languages are being used in the real world today; the learning curve, cost and complexity behind the languages; the value of the products that are produced by using such programming tools. 
 
There are many subtle challenges and opportunities associated with Big Data, which require restructuring the data management platforms to better suit the needs of businesses. When making a shift to newer technologies, there are always doubts and expectations. Hence, it is beneficial to look into the integration of emerging technologies with existing systems for the constant improvement.",CASCON,2012.0,,38228244,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
5d85b10ed56bcbc556c54916ce520b53209837e8,https://www.semanticscholar.org/paper/5d85b10ed56bcbc556c54916ce520b53209837e8,Pattern-Oriented Application Frameworks for Domain Experts to Effectively Utilize Highly Parallel Manycore Microprocessors,"Manycore microprocessors are powerful computing engines that are architected to embrace the use of parallelism to extract computational throughput from the continued improvements in the semiconductor manufacturing process. Yet the performance of the software applications running on these microprocessors is highly sensitive to factors such as data layout, data placement, and synchronization. These factors are not usually part of an application domain experts daily concerns, as they look to utilize the powerful compute capabilities of manycore microprocessors for their applications, but failure to carefully address these concerns could mean an order of magnitude of loss in application execution latency and/or throughput. With the proliferation of manycore microprocessors from servers to laptops and portable devices, there is increasing demand for the productive development of computationally efficient business and consumer applications in a wide range of usage scenarios. The sensitivity of execution speed to software architecture and programming techniques can impede the adoption of the manycore microprocessors and slow the momentum of the semiconductor industry. 
This thesis discusses how we can empower application domain experts with pattern-oriented application frameworks, which can allow them to effectively utilize the capabilities of highly parallel manycore microprocessors and productively develop efficient parallel software applications. Our pattern-oriented application framework includes an application context for outlining application characteristics, a software architecture for describing the application concurrency exploited in the framework, a reference implementation as a sample design, and a set of extension points for flexible customization. 
We studied the process of accelerating applications in the fields of machine learning and computational finance, specifically looking at automatic speech recognition (ASR), financial market value-at-risk estimation (VaR), and financial potential future exposure (PFE). We present a pattern-oriented application framework for ASR, as well as efficient reference implementations of VaR and PFE. For the ASR framework, we demonstrate its construction and two separate deployments, one of which flexibly extends the ASR framework to enable lip-reading in high-noise recognition environments. The framework enabled a Matlab/Java programmer to effectively utilize a manycore microprocessor to achieve a 20x speedup in recognition throughput as compared to a sequential CPU implementation. 
Our pattern-oriented application framework provides an approach for crystallizing and transferring the often-tacit knowledge of effective parallel programming techniques while allowing for flexible adaptation to various application usage scenarios. We believe that the pattern-oriented application framework will be an essential tool for the effective utilization of manycore microprocessors for application domain experts.",,2010.0,,18610096,semantic_scholar,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy')
