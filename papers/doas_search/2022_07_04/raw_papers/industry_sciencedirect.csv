id,type,publication,publisher,publication_date,database,title,url,abstract,query_name,query_value
10.1016/j.rcim.2022.102365,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2022-12-01,sciencedirect,A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping,https://api.elsevier.com/content/abstract/scopus_id/85131222861,"Deep reinforcement learning (DRL) has proven to be an effective framework for solving various complex control problems. In manufacturing, industrial robots can be trained to learn dexterous manipulation skills from raw pixels with DRL. However, training robots in the real world is a time-consuming, high-cost and of safety concerns process. A frequently adopted approach for easing this is to train robots through simulations first and then deploy algorithms (or policies) on physical robots. How to transfer policies of robot learning from simulation to the real world is a challenging issue. Digital twin that is able to create a dynamic, up-to-date representation of a physical robotic grasping system provides an effective approach for addressing this issue. In this paper, we focus on the scenario of DRL-based assembly-oriented industrial grasping and propose a digital twin-enabled approach for achieving effective transfer of DRL algorithms to a physical robot. Two parallel training systems, i.e., the physical robotic system and corresponding digital twin system, respectively, are established, which take virtual and real images as inputs. The output of the digital twin system is used to correct the real grasping point so that accurate grasping can be achieved. Experimental results verify the effectiveness of the intelligent grasping algorithm and the digital twin-enabled sim-to-real transfer approach and mechanism.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2022.117649,Journal,Expert Systems with Applications,scopus,2022-11-15,sciencedirect,Bridging the gap between complexity and interpretability of a data analytics-based process for benchmarking energy performance of buildings,https://api.elsevier.com/content/abstract/scopus_id/85132331460,"Artificial intelligence (AI) is fast becoming a general purpose technology with outstanding impacts in industries worldwide, thus supporting the Industry 4.0 revolution. In particular, the energy sector is one of those that has taken more advantages from the implementation of AI approaches, especially Machine Learning models, for several applications, including energy performance benchmarking of buildings. However, the black-box approach could lead to a lack of result interpretability thus preventing the effective application of AI in some real-world scenarios. For this reason, eXplainable Artificial Intelligence (XAI) tools can be effectively embedded within an AI-based Energy Analytics methodology in order to enhance the explainability of the model results. In this paper, we propose an explainable AI-based benchmarking framework for estimating the membership to specific energy performance classes of a large set of Energy Performance Certificates (EPCs) of flats. The classification is obtained by leveraging different black-box classifiers characterized by high accuracy, yet their inference mechanism is not human-readable. Therefore, a generalizable XAI methodology, based on the combination of a local explainer together with a clustering algorithm, is employed to explain the model results and causal effects between the predictors and target variable to better understand the model behaviour, and the motivations behind correct and wrong performed classifications. The paper provides a general methodological approach capable to exploit a limited number of instances to extract, explain and interpret inference mechanisms learnt by the model that are useful for the end-user. The framework was tested on about 100,000 EPCs of flats located in Italy.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijepes.2022.108291,Journal,International Journal of Electrical Power and Energy Systems,scopus,2022-11-01,sciencedirect,Density-based clustering algorithm for associating transformers with smart meters via GPS-AMI data,https://api.elsevier.com/content/abstract/scopus_id/85130630920,"The ongoing deployment of Distributed Energy Resources, while bringing benefits, introduces significant challenges to the electric utility industry, especially in the distribution grid. These challenges call for closer monitoring through state estimation, where real-time topology recovery is the basis for accurate modeling. Previous methods either ignore geographical information, which is important in connectivity identification or are based on an ideal assumption of an isolated sub-network for topology recovery, e.g., within one transformer. This requires field engineers to identify the association, which is costly and may contain errors. To solve these problems, we propose a density-based topology clustering method that leverages both voltage domain data and the geographical space information to segment datasets from a large utility customer pool, after which other topology reconstruction methods can carry over. Specifically, we show how to use voltage and GPS information to infer associations within one transformer area, i.e., to identify the meter-transformer connectivity. To give a guarantee, we show a theoretic bound for our clustering method, providing the ability to explain the performance of the machine learning method. The proposed algorithm has been validated by IEEE test systems and Duquesne Light Company in Pittsburgh, showing outstanding performance. A utility implementation is also demonstrated.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2022.117634,Journal,Expert Systems with Applications,scopus,2022-10-15,sciencedirect,RADIS: A real-time anomaly detection intelligent system for fault diagnosis of marine machinery,https://api.elsevier.com/content/abstract/scopus_id/85131074045,"By enhancing data accessibility, the implementation of data-driven models has been made possible to empower strategies in relation to O&M activities. Such models have been extensively applied to perform anomaly detection tasks, with the express purpose of detecting data patterns that deviate significantly from normal operational behaviour. Due to its preeminent importance in the maritime industry to adequately identify the behaviour of marine systems, the Real-time Anomaly Detection Intelligent System (RADIS) framework, constituted by a Long Short-Term Memory-based Variational Autoencoder in tandem with multi-level Otsu’s thresholding, is proposed. RADIS aims to address the current gaps identified within the maritime industry in relation to data-driven model applications for enabling smart maintenance. To assess the performance of such a framework, a case study on a total of 14 parameters obtained from sensors installed on a diesel generator of a tanker ship is introduced to highlight the implementation of RADIS. Results demonstrated the capability of RADIS to be part of a diagnostic analytics tool that will promote the implementation of smart maintenance within the maritime industry, as RADIS detected an average of 92.5% of anomalous instances in the presented case study.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.bios.2022.114475,Journal,Biosensors and Bioelectronics,scopus,2022-10-01,sciencedirect,Biosensor prototype for rapid detection and quantification of DNase activity,https://api.elsevier.com/content/abstract/scopus_id/85132387568,"DNases are enzymes that cleave phosphodiesteric bonds of deoxyribonucleic acid molecules and are found everywhere in nature, especially in bodily fluids, i.e., saliva, blood, or sweat. Rapid and sensitive detection of DNase activity is highly important for quality control in the pharmaceutical and biotechnology industries. For clinical diagnostics, recent reports indicate that increased DNase activity could be related to various diseases, such as cancers. In this paper, we report a new bioelectronic device for the determination of nuclease activity in various fluids. The system consists of a sensor electrode, a custom design DNA target to maximize the DNase cleavage rate, a signal analysis algorithm, and supporting electronics. The developed sensor enables the determination of DNase activity in the range of 3.4 × 10−4 – 3.0 × 10−2 U mL−1 with a limit of detection of up to 3.4 × 10−4 U mL−1. The sensor was tested by measuring nuclease activity in real human saliva samples and found to demonstrate high accuracy and reproducibility compared to the industry standard DNaseAlert™️. Finally, the entire detection system was implemented as a prototype device system utilizing single-use electrodes, custom-made cells, and electronics. The developed technology can improve nuclease quality control processes in the pharmaceutical/biotechnology industry and provide new insights into the importance of nucleases for medical applications.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rcim.2022.102371,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2022-10-01,sciencedirect,A grasps-generation-and-selection convolutional neural network for a digital twin of intelligent robotic grasping,https://api.elsevier.com/content/abstract/scopus_id/85129983234,"Robotic grasping plays an essential role in human-machine cooperation in various household and industrial applications. Although humans can instinctively execute grasps in an accurate, stable, and rapid way even under a constantly changing environment, intelligent grasping remains a challenging task for robots. As a prerequisite for grasping, robots need to correctly identify the best grasping location of unknown objects often based on an artificial intelligence approach, which is still a challenging problem. This paper proposes a new grasps-generation-and-selection convolutional neural network (GGS-CNN), which is trained and implemented in a digital twin of intelligent robotic grasping (DTIRG). By defining a grasp with 3-D position, rotation angle, and gripper width, the GGS-CNN generates grasp candidates by transforming the red–green-blue-depth images (RGB-D images) into feature maps and evaluating the quality of selected grasps. The GGS-CNN is trained in the virtual environment and the real world of the DTIRG to detect accurate grasps. In the grasping tests, the proposed GGS-CNN achieves grasping success rates of 96.7% and 93.8% for grasping single objects and cluttered objects, respectively, and obtains the best grasp from the RGB-D image in less than 40 ms.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rcim.2022.102351,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2022-10-01,sciencedirect,Cloud-edge-device collaboration mechanisms of deep learning models for smart robots in mass personalization,https://api.elsevier.com/content/abstract/scopus_id/85127198043,"Personalized products have gradually become the main business model and core competencies of many enterprises. Large differences in components and short delivery cycles of such products, however, require industrial robots in cloud manufacturing (CMfg) to be smarter, more responsive and more flexible. This means that the deep learning models (DLMs) for smart robots should have the performance of real-time response, optimization, adaptability, dynamism, and multimodal data fusion. To satisfy these typical demands, a cloud-edge-device collaboration framework of CMfg is first proposed to support smart collaborative decision-making for smart robots. Meanwhile, in this context, different deployment and update mechanisms of DLMs for smart robots are analyzed in detail, aiming to support rapid response and high-performance decision-making by considering the factors of data sources, data processing location, offline/online learning, data sharing and the life cycle of DLMs. In addition, related key technologies are presented to provide references for technical research directions in this field.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.biortech.2022.127456,Journal,Bioresource Technology,scopus,2022-09-01,sciencedirect,Moisture content monitoring in industrial-scale composting systems using low-cost sensor-based machine learning techniques,https://api.elsevier.com/content/abstract/scopus_id/85132318701,"Moisture is a key aspect for proper composting, allowing greater efficiency and lower environmental impact. Low-cost real-time moisture determination methods are still a challenge in industrial composting processes. The aim of this study was to design a model of hardware and software that would allow self-adjustment of a low-cost capacitive moisture sensor. Samples of organic composts with distinct waste composition and from different composting stages were used. Machine learning techniques were applied for self-adjustment of the sensor. To validate the model, results obtained in a laboratory by the gravimetric method were used. The proposed model proved to be efficient and reliable in measuring moisture in compost, reaching a correlation coefficient of 0.9939 between the moisture content verified by gravimetric analysis and the prediction obtained by the Sensor Node.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.suscom.2022.100743,Journal,Sustainable Computing: Informatics and Systems,scopus,2022-09-01,sciencedirect,Improving sugarcane production in saline soils with Machine Learning and the Internet of Things,https://api.elsevier.com/content/abstract/scopus_id/85129609524,"The Indian sugar industry is the second largest in the world. Sugar is an essential domestic grocery item required in India producing over 25 million tonnes per annum. Sugarcane is the root of sugar products that grow in over 5 million hectares all over India. However, nearly 1.5 million hectares of overall farms are saline soil lands (high salt content). This leads to lower yields in sugarcane agriculture than what would be expected. Therefore, tackling the salinity problem is crucial to achieve strong food security as well as tackle the sustainability of farming practices in India that have reach beyond just sugarcane. This research proposes efficient, sustainable, smart farming techniques for sugarcane cultivation in salt-affected lands with the help of the Internet of Things (IoT) and Machine Learning (ML). The proposed model has been implemented in a real-world two hectare sugar cane field cultivated from saline soils using Raspberry PI IoT nodes to control the drip irrigation (water supply). The Naïve Bayes model has been used to train and predict the leaching requirement suggested by Food and Agriculture Organization of the United Nations (FAO) and United Nations Educational, Scientific and Cultural Organization (UNESCO) for efficient leaching water requirements. The performance of the proposed model has been evaluated in terms of sugar cane growth, cost of cultivation, as well as water requirements leading to an improved outlook for future use. Moreover, our results have been compared with regular sugar cane cultivation to show their effectiveness.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ecmx.2022.100261,Journal,Energy Conversion and Management: X,scopus,2022-08-01,sciencedirect,Multi-objective optimization of turbocharger turbines for low carbon vehicles using meanline and neural network models,https://api.elsevier.com/content/abstract/scopus_id/85133274528,"Due to slow turnover of the global vehicle parc internal combustion engines will remain a primary means of motive power for decades, so the automotive industry must continue to improve engine thermal efficiency to reduce 
                        
                           
                              
                                 CO
                              
                              
                                 2
                              
                           
                        
                      emissions, since savings will be compounded over the long lifetime of millions of vehicles. Turbochargers are a proven efficiency technology (most new vehicles are turbocharged) but are not optimally designed for real-world driving. The aim of this study was to develop a framework to optimize turbocharger turbine design for competing customer objectives: minimizing fuel consumption (and thus 
                        
                           
                              
                                 CO
                              
                              
                                 2
                              
                           
                        
                      emissions) over a representative drive cycle, while minimizing transient response time. This is achieved by coupling engine cycle, turbine meanline, and neural network inertia models within a genetic algorithm-based optimizer, allowing aerodynamic and inertia changes to be accurately reflected in drive cycle fuel consumption and transient performance. Exercising the framework for the average new passenger car across a drive cycle representing the Worldwide harmonized Light vehicles Test Procedure reveals the trade-off between competing objectives and a turbine design that maintains transient response while minimizing fuel consumption due to a 3 percentage-point improvement in turbine peak efficiency, validated by experiment. This optimization framework is fast to execute, requiring only eight turbine geometric parameters, making it a commercially viable procedure that can refine existing or optimize tailor-made turbines for any turbocharged application (whether gasoline, diesel, or alternatively fuelled), but if applied to turbocharged gasoline cars in the EU would lead to lifetime 
                        
                           
                              
                                 CO
                              
                              
                                 2
                              
                           
                        
                     
                     savings of 
                        
                           >
                        
                     
                     290,000 tonnes per production year, and millions of tonnes if deployed worldwide.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compchemeng.2022.107884,Journal,Computers and Chemical Engineering,scopus,2022-08-01,sciencedirect,One step forward for smart chemical process fault detection and diagnosis,https://api.elsevier.com/content/abstract/scopus_id/85132568028,"Process fault detection and diagnosis (FDD) is an essential tool to ensure safe production in chemical industries. After decades of development, despite the promising performance of some FDD methods on specific tasks, most FDD methods are not smart enough to tackle the complex challenges in real industrial processes, rendering an absence of commercialized FDD tools. Therefore, the implementation of smart FDD becomes an ambitious goal for process safety. In this paper, we provide an overview of the concept and major challenges of smart FDD. Recent FDD methods are comprehensively evaluated with respect to the characteristics of smart FDD. We also present the researches done by our group, which we believe would be a step forward for smart FDD. A range of future opportunities and new perspectives are further discussed. This review aims to illuminate potential directions for process safety and to contribute to the realization of commercial FDD tools.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.oceaneng.2022.111579,Journal,Ocean Engineering,scopus,2022-08-01,sciencedirect,Machine learning enhancement of manoeuvring prediction for ship Digital Twin using full-scale recordings,https://api.elsevier.com/content/abstract/scopus_id/85132435355,"Digital Twins have much attention in the shipping industry, attempting to support all phases of a vessel’s life cycle. With several tools appearing in Digital Twin software suites, high-quality manoeuvring and performance prediction remain cornerstones. Propulsion efficiency is in focus while in service. Simulator-based training is in focus to ensure safety of manoeuvring in confined waters and harbours. Prediction of ships’ velocity and turn rate are essential for correct look and feel during training, but phenomena like dynamic inflow to propellers, bank and shallow water effects limit simulators’ accuracy, and master mariners often comment that simulations could be in better agreement with actual behaviours of their vessel. This paper focuses on digital twin enhancements to better match reality. Using data logged during in-service operation, we first consider a system identification perspective, employing a first-principles model structure. Showing that a complete first-principles model is not identifiable under the excitation met in service, we employ a Recurrent Neural Network to predict deviations between measured velocities and the model output. The outcome is a hybrid of a first-principles model with a machine learning generic approximator add-on. The paper demonstrates significant improvements in prediction accuracy of both in-harbour manoeuvring and shallow water passage conditions.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2022.108302,Journal,Computers and Industrial Engineering,scopus,2022-08-01,sciencedirect,Multi-strategy hybrid heuristic algorithm for single container weakly heterogeneous loading problem,https://api.elsevier.com/content/abstract/scopus_id/85132231873,"Three-dimensional single container weakly heterogeneous loading problem is one of the most classical tasks which has various applications in manufacture and logistics industry. Solving this problem could improve transportation efficiency to bring great benefit to shipping customers. During the last two decades, many heuristic, meta-heuristic and hybrid algorithms have been proposed to maximize container volume utilization to reduce the waste of container space significantly. Despite their success in many real-world applications, it is still a challenging task to recommend satisfactory loading levels within a limited time frame when clients approach for options of different combinations of shipping items. In this paper, we propose a novel multi-strategy hybrid heuristic algorithm to achieve timely planning for clients in a required short time frame. In specific, a probabilistic model is used to combine the strength of two optimization strategies, i.e. an ant colony method and a constructive greedy method, to speed up the optimization process and ensure better convergence. In addition, a tree pruning strategy is designed to further improve the efficiency of the hybrid heuristic algorithm. Extensive experiments demonstrate the effectiveness of our method in terms of both volume utilization rate and algorithm processing speed compared to state-of-the-art methods. Based on the comparison results by using BR dataset, we achieved averagely 94.31% volume utilization rate and 50.16 s processing speed, which is the best performance by considering both algorithm effectiveness and efficiency. Further, our proposed method has been deployed in a real business case to provide plan solutions to individual customer shipping requests and achieved high customer satisfaction rate.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.apor.2022.103222,Journal,Applied Ocean Research,scopus,2022-08-01,sciencedirect,Data-Driven system identification of 6-DoF ship motion in waves with neural networks,https://api.elsevier.com/content/abstract/scopus_id/85131715143,"Critical evaluation of ship responses in the ocean is important for not only the design and engineering of future platforms but also the operation and safety of those that are currently deployed. Short-term temporal predictions of ship responses given the current wave environment and ship state would enable enhanced decision-making onboard and reduce the overall risk for both manned and unmanned vessels, especially as the marine industry trends towards more autonomy. However, state-of-the-art numerical hydrodynamic simulation tools are too computationally expensive to be employed for real-time ship motion forecasting. Thus, a methodology is needed to provide fast predictions with levels of accuracy closer to the higher-fidelity tools. A methodology is developed with long short-term memory (LSTM) neural networks to represent the motions of a free running David Taylor Model Basin (DTMB) 5415 destroyer operating at 20 knots in Sea State 7 stern-quartering long-crested irregular seas. Case studies are performed for both course-keeping and turning circle scenarios. An estimate of the vessel’s encounter frame is made with the trajectories observed in the training dataset. Wave elevation time histories are given by artificial wave probes that travel with the estimated encounter frame and serve as input into the neural network, while the output is the 6-DOF temporal ship motion response. Overall, the neural network is able to predict the temporal response of the ship due to unseen wave sequences accurately. The methodology, the dependence of model accuracy on wave probe and training data quantity and the estimated encounter frame are all detailed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.mechatronics.2022.102833,Journal,Mechatronics,scopus,2022-08-01,sciencedirect,Control framework for collaborative robot using imitation learning-based teleoperation from human digital twin to robot digital twin,https://api.elsevier.com/content/abstract/scopus_id/85130606803,"Despite the deployment of collaborative robots for various industrial processes, their teaching and control remain comparatively difficult tasks compared with general industrial robots. Various imitation learning methods involving the transfer of human poses to a collaborative robot have been proposed. However, most of these methods depend heavily on deep learning-based human recognition algorithms that fail to recognize complicated human poses. To address this issue, we propose an automated/semi-automated vision-based teleoperation framework using human digital twin and a collaborative robot digital twin models. First, a human pose is recognized and reasoned to a human skeleton model using a convolution encoder-decoder architecture. Next, the developed human digital twin model is taught using the skeletons. As human and collaborative robots have different joints and rotation architectures, pose mapping is achieved using the proposed Bezier curve-based smooth approximation. Then, a real collaborative robot is controlled using the developed robot digital twin. Furthermore, the proposed framework works successfully using a human digital twin in the case of recognition failures of human poses. To verify the effectiveness of the proposed framework, transfers of several human poses to a real collaborative robot are tested and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ssci.2022.105766,Journal,Safety Science,scopus,2022-08-01,sciencedirect,Industrial internet of things and unsupervised deep learning enabled real-time occupational safety monitoring in cold storage warehouse,https://api.elsevier.com/content/abstract/scopus_id/85127200076,"Occupational safety and health (OSH) has always been a big concern in the labor-intensive warehouse industry, especially under peculiar circumstances like a low temperature. Accordingly, this paper aims to propose a framework of a smart system using the Industrial Internet of Things (IIoT) and digital twin (DT) technologies to realize real-time occupational safety monitoring in the warehouse and ensure synchronized cyber-physical spaces for information traceability and visibility. The unsupervised deep neural structure of stacked auto-encoder (SAE) is designed to identify abnormal stationary from human motion status, which is perceived as a sign of potential accident. The model is developed to automatically update online by cooperating with calibration samples so as to keep in accordance with the evolution of surroundings. The Bluetooth low energy (BLE) and a log-distance path loss model are used to fulfill indoor localization in order for managers to promptly respond to an incident on site. Besides, some intelligent services are enabled to promote the efficiency of safety management. A real-life case study is carried out in an air cargo cold storage warehouse to illustrate the viability and rationality of the proposed system and methods. The elaboration of the implementation is envisioned to facilitate replication and reproduction effectively. The impact of learning features concerned with distance and vibration on the performance of anomaly detection has also been analyzed by experiments. The insights and lessons gained in this study hold the promise of providing a reference or sparking new ideas for researchers and practitioners to meet similar needs in practice.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rcim.2022.102321,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2022-08-01,sciencedirect,AR-assisted digital twin-enabled robot collaborative manufacturing system with human-in-the-loop,https://api.elsevier.com/content/abstract/scopus_id/85124244817,"The teleoperation and coordination of multiple industrial robots play an important role in today’s industrial internet-based collaborative manufacturing systems. The user-friendly teleoperation approach allows operators from different manufacturing domains to reduce redundant learning costs and intuitively control the robot in advance. Nevertheless, only a few preliminary works have been introduced very recently, let alone its effective implementation in the manufacturing scenarios. To address the gap, this research proposes a novel multi-robot collaborative manufacturing system with human-in-the-loop control by leveraging the cutting-edge augmented reality (AR) and digital twin (DT) techniques. In the proposed system, the DTs of industrial robots are firstly mapped to physical robots and visualize them in the AR glasses. Meanwhile, a multi-robot communication mechanism is designed and implemented, to synchronize the state of robots in the twin. Moreover, a reinforcement learning algorithm is integrated into the robot motion planning to replace the conventional kinematics-based robot movement with corresponding target positions. Finally, three interactive AR-assisted DT modes, including real-time motion control, planned motion control, and robot monitoring mode are generated, which can be readily switched by human operators. Two experimental studies are conducted on (1) a single robot with a commonly used peg-in-hole experiment, and (2) the motion planning of multi-robot collaborative tasks, respectively. From the experimental results, it can be found that the proposed system can well handle the multi-robot teleoperation tasks with high efficiency and owns great potentials to be adopted in other complicated manufacturing scenarios in the near future.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.knosys.2022.108933,Journal,Knowledge-Based Systems,scopus,2022-07-19,sciencedirect,Learning software requirements syntax: An unsupervised approach to recognize templates,https://api.elsevier.com/content/abstract/scopus_id/85130127513,"Requirements are textual representations of the desired software capabilities. Many templates have been used to standardize the structure of requirement statements such as Rupps, EARS, and User Stories. Templates provide a good solution to improve different Requirements Engineering (RE) tasks since their well-defined syntax facilitates the different text processing steps in RE automation researches. However, many empirical studies have concluded that there is a gap between these RE researches and their implementation in industrial and real-life projects. The success of RE automation approaches strongly depends on the consistency of the requirements with the syntax of the predefined templates. Such consistency cannot be guaranteed in real projects, especially in large development projects, or when one has little control over the requirements authoring environment.
                  In this paper, we propose an unsupervised approach to recognize templates from the requirements themselves by extracting their common syntactic structures. The resultant templates reflect the actual syntactic structure of requirements; hence it can recognize both standard and non-standard templates. Our approach uses techniques from Natural Language Processing and Graph Theory to handle this problem through three main stages (1) we formulate the problem as a graph problem, where each requirement is represented as a vertex and each pair of requirements has a structural similarity, (2) We detect main communities in the resultant graph by applying a hybrid technique combining limited dynamic programming and greedy algorithms, (3) finally, we reinterpret the detected communities as templates.
                  Our experiments show that the suggested approach can detect templates that follow well-known standards with a 0.90 F1-measure. Moreover, the approach can detect common syntactic features for non-standard templates in more than 73.5% of the cases. Our evaluation indicates that these results are robust regardless of the number and the length of the processed requirements.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compag.2022.107097,Journal,Computers and Electronics in Agriculture,scopus,2022-07-01,sciencedirect,Application of multispectral imaging combined with machine learning models to discriminate special and traditional green coffee,https://api.elsevier.com/content/abstract/scopus_id/85131453324,"Non-destructive techniques aided by machine learning models are widely implemented in food analysis. To discriminate between ‘special’ and ‘traditional’ classes of green coffee beans, an advanced multispectral imaging technique based on reflectance and autofluorescence data was employed in combination with four machine learning algorithms (SVM, RF, XGBoost, and CatBoost). Of the four algorithms, SVM showed superior accuracy (0.96) for the test dataset. Analysis using PCA and SVM algorithms showed that autofluorescence data from excitation/emission combination of 405/500 nm contributed most to the discrimination of special green coffee from the traditional class. Fluorophores that can be linked to green fluorescence, namely catechin, caffeine and 4-hydroxybenzoic, synapic and chlorogenic acids, were found to have a considerable influence on the differentiation of specialty and traditional coffees. Analysis based on multispectral autofluorescence imaging combined with SVM models was proven to be a valuable tool for future applications in the food industry for the non-destructive and real-time classification of special and traditional green coffee.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmsy.2022.05.018,Journal,Journal of Manufacturing Systems,scopus,2022-07-01,sciencedirect,Graph neural network and multi-agent reinforcement learning for machine-process-system integrated control to optimize production yield,https://api.elsevier.com/content/abstract/scopus_id/85131416727,"In this paper, an integrated control framework is proposed for the optimization of the production yield by integrating different levels of a manufacturing system, including system, process, and machine levels. The manufacturing system is modeled as a graph by treating machines as nodes and material flows as links. The graph model enjoys high flexibility and is able to incorporate all relevant real-time information across all levels of the manufacturing system in the dynamic node feature. Since the real-time tool state is essential for decision making, Recursive Bayesian Estimation (RBE) is adopted to reduce the tool state observations through sensors and machine learning models and provide more accurate tool state estimation to be included into the graph node feature. With the graph model, Graph Neural Network (GNN) is applied to process the node features to generate node embedding that reflects both local and global information. For the integrated control purpose, each machine node is then be treated as a distributed agent in Multi-Agent Reinforcement Learning (MARL) that conditions its policy on the node embedding from GNN. State-of-the-art GNN and MARL algorithms, namely Graph Attention Network (GAT) and Value Decomposition Actor Critic (VDAC), are implemented to train learnable parameters in GNN-MARL networks to learn the optimal multi-agent policy. Extensive numerical experiments and analysis proves the effectiveness of the proposed integrated control framework.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2022.108231,Journal,Computers and Industrial Engineering,scopus,2022-07-01,sciencedirect,Learning algorithms to deal with failures in production planning,https://api.elsevier.com/content/abstract/scopus_id/85130737868,"New technologies and the advancement of Industry 4.0 have led to significant changes in production processes, e.g., increased use of data for decision making, faster production speed due to automation, and shorter planning horizons. Soon, the resolution methods and scenarios consolidated in the literature may not adapt to this situation. Therefore, decision-makers must consider this new context in the production planning phase. This work proposes approaches to solve production planning problems in uncertain environments and introduces a framework that predicts the best strategy for implementation according to the specific problem instance. We incorporate characteristics of Industry 4.0 into our study, considering that the delivery of products at their due dates to customers will be more relevant than minimizing costs. The proposed proactive approaches use machine learning algorithms to predict disruptions on the shop floor. We compare strategies considering feedback information on real failures to modify the planning of future periods. Further, we propose a proactive-online approach integrating proactive and real-time decisions, comparing the results with a corrective strategy. Based on computational tests performed with a proposed benchmark, we conclude that the proactive and proactive-online approaches resulted in lower total weighted tardiness in comparison to the corrective method. Regarding the proactive and proactive-online approaches, we observe that their results depend on the set of analyzed instances, justifying the proposition of the framework. Lastly, for most cases, the strategies predicted by the framework achieved lower total weighted tardiness when compared with the average results obtained by all the strategies studied in this work.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2022.108233,Journal,Computers and Industrial Engineering,scopus,2022-07-01,sciencedirect,A voice of the customer real-time strategy: An integrated quality function deployment approach,https://api.elsevier.com/content/abstract/scopus_id/85130169493,"In the era of Industry 4.0, the rapid development of information technology in the last decade has provided new challenges for product improvement by enabling users to give their feedback and sentiments in real time. In this paper, combining with genetic algorithm back propagation neutral network, fuzzy inference method, and entropy-based synthesis evaluation method, we raise a full-process product improvement solution driven by online reviews, from the initial online review collection to the final engineering characteristic prioritization. The proposed novel integrated quality function deployment-based approach adheres to the customer-oriented design principle, allowing manufacturers to strengthen the launched product based on the spontaneously-articulated voice of customer, rather than the traditional expertise. In this way, an off-the-shelf product improvement strategy is available for enterprises, and its special advantages like fast adaptation and real-time responsiveness, would significantly reduce management costs, shorten response time to market dynamics, and enhance customer satisfaction. In addition, a case study in smartphone industry is conducted for illustration, and the results clearly demonstrate the effectiveness and practicability of the treatment.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.autcon.2022.104268,Journal,Automation in Construction,scopus,2022-07-01,sciencedirect,Predicting communication quality in construction projects: A fully-connected deep neural network approach,https://api.elsevier.com/content/abstract/scopus_id/85129633119,"Establishing high-quality communication in construction projects is essential to securing successful collaboration and maintaining understanding among project stakeholders. Indeed, poor communication results in low productivity, poor efficiency, and substandard deliverables. While high-quality communication is recognized as contingent on the interpersonal skills of workers, the impacts of communication quality on job performance remain unknown. This study addresses this deficiency by developing a method to evaluate construction workers' communication quality. A literature review is undertaken to capture salient interpersonal skills. Leadership style, listening, team building, and clarifying expectations are identified. A questionnaire survey is drafted to capture construction practitioners' perception of these skills' effects on communication quality, returning 180 responses. Next, an artificial neural network model, or communication quality predictor (CQP), is developed, able to predict the quality of workers' interpersonal communication. The model accuracy on training is 87%; for testing, 79%. Finally, CQP is deployed in a real-time context in order to validate the reliability, returning an 80% prediction accuracy. This study is the first of its kind in offering a quantified, predictive model associating interpersonal skills with quality of communications in the context of the construction sector. In practical terms, the CQP can flag interpersonal conflicts before they escalate, while also guiding construction managers in the design of interpersonal skills training",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmapro.2022.04.033,Journal,Journal of Manufacturing Processes,scopus,2022-07-01,sciencedirect,Data-driven prediction of next-layer melt pool temperatures in laser powder bed fusion based on co-axial high-resolution Planck thermometry measurements,https://api.elsevier.com/content/abstract/scopus_id/85129451177,"Uncontrolled process variability, stemming from geometry, machine, or parameter variation, can lead to metallurgical defects such as keyhole porosity and lack of fusion as well as geometrical defects such as increased surface roughness or increased deformation in the produced parts. This lack of control is a pressing problem in laser powder bed fusion (L-PBF) processes. One way to reduce this variability is to use model-based predictive control. Process parameters such as laser power and scan speed can be adjusted during the process based on in-situ measurements of process conditions such as melt pool size or temperature. In this paper, a predictive model that is an essential element in a larger predictive control ecosystem for L-PBF is developed and tested. The proposed machine learning-based regression model is trained using high-resolution co-axial melt pool temperature measurements from the previous layers. The machine learning model can predict the melt pool temperatures along the toolpath for the next layer assuming processing parameters remain the same. The paper describes the development of the machine learning-based prediction model and presents the guidelines for the design and selection of the features in feature vector. The estimation of the prediction performance based on real physical data is presented followed by suggestions of future work. The main limitation of the current approach is the relatively high computational cost. Some guidelines for implementation and possible improvements are given in the discussion of results.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.hcc.2022.100050,Journal,High-Confidence Computing,scopus,2022-06-01,sciencedirect,Informer: Irregular traffic detection for containerized microservices RPC in the real world,https://api.elsevier.com/content/abstract/scopus_id/85132034611,"Containerized microservices have been widely deployed in the industry. Meanwhile, security issues also arise. Many security enhancement mechanisms for containerized microservices require predefined rules and policies. However, it is challenging when it comes to thousands of microservices and a massive amount of real-time unstructured data. Hence, automatic policy generation becomes indispensable. In this paper, we focus on the automatic solution for the security problem: irregular traffic detection for RPCs.
                  We propose Informer, a two-phase machine learning framework to track the traffic of each RPC and automatically report anomalous points. We first identify RPC chain patterns using density-based clustering techniques and build a graph for each critical pattern. Next, we solve the irregular RPC traffic detection problem as a prediction problem for attributed graphs with time series by leveraging spatial-temporal graph convolution networks. Since the framework builds multiple models and makes individual predictions for each RPC chain pattern, it can be efficiently updated upon legitimate changes in any graphs.
                  In evaluations, we applied Informer to a dataset containing more than 7 billion lines of raw RPC logs sampled from a large Kubernetes system for two weeks. We provide two case studies of detected real-world threats. As a result, our framework found fine-grained RPC chain patterns and accurately captured the anomalies in a dynamic and complicated microservice production scenario, which demonstrates the effectiveness of Informer. Furthermore, we extensively evaluated the risk of adversarial attacks for our prediction model under different reality constraints and showed that the model is robust to such attacks in most real-world scenarios.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rineng.2022.100478,Journal,Results in Engineering,scopus,2022-06-01,sciencedirect,"Future prospects of computer-aided design (CAD) – A review from the perspective of artificial intelligence (AI), extended reality, and 3D printing",https://api.elsevier.com/content/abstract/scopus_id/85131835154,"Computer-aided design (CAD) is the use of computer-based software to aid in design modeling, design analysis, design review, and design documentation. Nevertheless, the benefits of CAD can be elevated in combination with artificial intelligence (AI), extended reality, and manufacturing. AI can create an intelligent graphics interface and change tedious design processes into sophisticated ones. In extended reality technology, simulation can take place in a 3D virtual environment, thereby providing excellent interaction and better analysis. In manufacturing, as seen in 3D printing technology, CAD systems can be directly connected to manufacturing to produce complex parts easily and rapidly. In this paper, the integration of artificial intelligence (AI) in CAD, as well as CAD application in extended reality and 3D printing is examined. The primary aim of this review is to present an overview of current state-of-the-art CAD and its applications, as well as to forecast its future prospects. The article is written using a systematic review of journal papers with a focus on a wide spectrum of potentially relevant researches on CAD. The benefits of incorporating AI into the CAD systems, the use of CAD in extended reality and 3D printing, and finally a brief discussion of issues that are pushing CAD to new levels are all discussed. Finally, the review concluded that the demand for several varied products based on a single object input, immersive and interactive simulation, and direct design-to-manufacturing integration is driving CAD to new levels.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.heliyon.2022.e09634,Journal,Heliyon,scopus,2022-06-01,sciencedirect,Smart deployment of IoT-TelosB service care StreamRobot using software-defined reliability optimisation design,https://api.elsevier.com/content/abstract/scopus_id/85131417211,"Intelligent service care robots have increasingly been developed in mission-critical sectors such as healthcare systems, transportation, manufacturing, and environmental applications. The major drawbacks include the open-source Internet of Things (IoT) platform vulnerabilities, node failures, computational latency, and small memory capacity in IoT sensing nodes. This article provides reliable predictive analytics with the optimisation of data transmission characteristics in StreamRobot. Software-defined reliable optimisation design is applied in the system architecture. For the IoT implementation, the edge system model formulation is presented with a focus on edge cluster log-normality distribution, reliability, and equilibrium stability considerations. A real-world scenario for accurate data streams generation from in-built TelosB sensing nodes is converged at a sink-analytic dashboard. Two-phase configurations, namely off-taker and on-demand, link-state protocols are mapped for deterministic data stream offloading. An orphan reconnection trigger mechanism is used for reliable node-to-sink resilient data transmissions. Data collection is achieved, using component-based programming in the experimental testbed. Measurement parameters are derived with TelosB IoT nodes. Reliability validations on remote monitoring and prediction processes are studied considering neural constrained software-defined networking (SDN) intelligence. An OpenFlow-SDN construct is deployed to offload traffic from the edge to the fog layer. At the core, fog detection-to-cloud predictive machine learning (FD-CPML) is used to predict real-time data streams. Prediction accuracy is validated with decision tree, logistic regression, and the proposed FD-CPML. The data streams latency gave 40.00%, 33.33%, and 26.67%, respectively. Similarly, linear predictive scalability behaviour on the network plane gave 30.12%, 33.73%, and 36.15% respectively. The results show satisfactory responses in terms of reliable communication and intelligent monitoring of node failures.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.tifs.2022.04.021,Journal,Trends in Food Science and Technology,scopus,2022-06-01,sciencedirect,Augmented/mixed reality technologies for food: A review,https://api.elsevier.com/content/abstract/scopus_id/85129273095,"Background
                  The topic of food is broad and global, thereby representing an influential sector of the economy. Motivated by the advent of Industry 4.0, massive potential exists to implement cutting-edge technologies in the food industry. Recent years have seen a growing interest towards the applications of augmented/mixed reality (AR/MR) in the food sector.
               
                  Scope and approach
                  An extensive search of online journals focusing on Scopus was conducted using terms including ‘augmented reality’, ‘mixed reality’ and ‘food’ in the search fields of Title, Abstract, and Keywords. Full paper reading was implemented and ineligible articles (i.e., non-English-language, review articles, not peer-reviewed and without full paper) were removed.
               
                  Key findings and conclusions
                  Our systematic search resulted in 111 eligible articles, eight of which related to MR technology. There is an overall increasing trend in the number of publications appearing annually since the first relevant publication in 2010. Analysing these publications demonstrates the multidisciplinary nature of this technology which is closely linked to machine learning, computer vision, the Internet of Things (IoT), and artificial intelligence. Our findings also revealed that AR/MR technology is mainly applied in the following areas: dietary assessment, food nutrition and traceability, food sensory science, retail food chain applications, food education and learning, and precision farming. Furthermore, we highlight the limitations and analytical challenges that hinder the application of AR/MR to food-related research, such as the lack of reliable wireless connection and the difficulty in recognizing food objects in a complex environment, while also describing future research needs and directions.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2022.108140,Journal,Computers and Industrial Engineering,scopus,2022-06-01,sciencedirect,Bilevel learning for large-scale flexible flow shop scheduling,https://api.elsevier.com/content/abstract/scopus_id/85127805979,"Many industrial practitioners are facing the challenge of solving large-scale scheduling problems within a limited time. In this paper, we propose a novel bilevel scheduler based on constraint Markov Decision Process to solve large-scale flexible flow shop scheduling problems (FFSP). There are many intelligent algorithms proposed to solve FFSP, but they take quite long time to execute or are even not working for large-scale problems. Our scheduler is able to decide the sequence of a large number of jobs in a limited time with the objective to minimize makespans. The upper level is designed to explore an initial global sequence, whereas the lower level aims to look for partial sequence refinements. In the implementation, Double Deep Q Network (DDQN) is used in the upper level and Graph Pointer Network (GPN) lies within the lower level. The two levels are connected by a sliding-window sampling mechanism. Based on datasets from public benchmarks and real-world industrial scenarios with over 5000 jobs, experiments show that our bilevel scheduler significantly outperforms seven baseline algorithms, including three state-of-the-art heuristics, three deep learning based algorithms, and another bilevel model, in terms of makespans and computational time. In particular, it only takes less than 200 s to get solutions of large-scale problems with up to 5000 jobs and matches the performance of the state-of-the-art heuristics.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2022.108062,Journal,Computers and Industrial Engineering,scopus,2022-06-01,sciencedirect,Factors affecting Industry 4.0 adoption – A hybrid SEM-ANN approach,https://api.elsevier.com/content/abstract/scopus_id/85125996767,"Industry 4.0 is a technology-driven digital transformation to enable data-driven decision-making based on real-time data to enhance the competitiveness of traditional manufacturing. Moving forward, adopting Industry 4.0 is an evident requirement for manufacturers to remain competitive. Currently, in the early stages of adoption in most industries and countries, there is an evident lack of foundational Industry 4.0 knowledge among decision-makers. There are few studies on the adoption of Industry 4.0; however, they focus on specific domains like cloud computing, virtual reality, IoT, etc., and are primarily set in developed countries. This research proposes a multi-stage hybrid analytic approach whereby the research model was tested using Structural Equation Modelling (SEM). The SEM results were used as inputs for the Artificial Neural Networks (ANN) to determine significant predictors for the adoption of Industry 4.0 in Indian manufacturing industries. A comprehensive sample of 350 responses from various Indian manufacturers was collected and analyzed. Results revealed that the factors viz. Software Infrastructure (SI), System Flexibility (SF), Operational Accuracy (OA), and the Technical Capabilities (TC) play a dominant role in the successful adoption intentions of Industry 4.0 in India. Manufacturers from India and other emerging economies will benefit from the findings of this study by concentrating and improving on the dominant adoption factors of Industry 4.0. Concluding, we discuss the study’s results and derive lessons learned for stakeholders, including managers, consultants, policymakers, and regulatory authorities.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.isatra.2021.06.010,Journal,ISA Transactions,scopus,2022-06-01,sciencedirect,A real-world application of Markov chain Monte Carlo method for Bayesian trajectory control of a robotic manipulator,https://api.elsevier.com/content/abstract/scopus_id/85108508566,"Reinforcement learning methods are being applied to control problems in robotics domain. These algorithms are well suited for dealing with the continuous large scale state spaces in robotics field. Even though policy search methods related to stochastic gradient optimization algorithms have become a successful candidate for coping with challenging robotics and control problems in recent years, they may become unstable when abrupt variations occur in gradient computations. Moreover, they may end up with a locally optimal solution. To avoid these disadvantages, a Markov chain Monte Carlo (MCMC) algorithm for policy learning under the RL configuration is proposed. The policy space is explored in a non-contiguous manner such that higher reward regions have a higher probability of being visited. The proposed algorithm is applied in a risk-sensitive setting where the reward structure is multiplicative. Our method has the advantages of being model-free and gradient-free, as well as being suitable for real-world implementation. The merits of the proposed algorithm are shown with experimental evaluations on a 2-Degree of Freedom robot arm. The experiments demonstrate that it can perform a thorough policy space search while maintaining adequate control performance and can learn a complex trajectory control task within a small finite number of iteration steps.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.comnet.2022.108895,Journal,Computer Networks,scopus,2022-05-22,sciencedirect,Machine learning based fast self optimized and life cycle management network,https://api.elsevier.com/content/abstract/scopus_id/85127244338,"6G system targets many emerging industrial verticals, including industry 4.0 and autonomous driving, with extreme service requirements and a high amount of resources, which further overload mobile networks. These industrial verticals are characterized by intense, continuous, and conflicting requirements that harden the mission of the next generation. To achieve the coveted objectives and deal with these shortcomings, the end-to-end network communication should be self-managed, self-orchestrated, and self-optimized, including network edges and clouds. Besides software-defined networking (SDN) and network function virtualization (NFV) that offer network softwarization, elasticity, and flexibility, Machine Learning (ML) is expected to play a vital role in the next-generation of networks. The decisions made at the orchestration plan should be quick besides their accuracy and optimality to overcome the foreseeable challenges. In this paper, we propose a generic framework, named Deep Learning Optimization for Service Function Chaining (DLO-SFC), that provides fast optimal configurations using deep learning techniques and network optimization. We have designed the proposed framework to be generic and orthogonal to a specific use case. Indeed, the framework can be leveraged for any networking configuration and orchestration problems with a slight modification. We evaluate our proposed framework performances showing that it reduces the OPEX costs while optimizing the execution time.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aca.2021.339411,Journal,Analytica Chimica Acta,scopus,2022-05-08,sciencedirect,A video processing and machine vision-based automatic analyzer to determine sequentially total suspended and settleable solids in wastewater,https://api.elsevier.com/content/abstract/scopus_id/85123884378,"The monitoring of total suspended (TSS) and settleable (SetS) solids in wastewater is essential to maintain the quality parameters for aquatic biota because they can transport pollutants and block light penetration. Determining them by their respective reference methods, however, is laborious, expensive, and time consuming. To overcome this, we developed a new analytical instrument called Solids in Wastewater's Machine Vision-based Automatic Analyzer (SWAMVA), which is equiped with an automatic sampler and a software for real-time digital movie capture to quantify sequentially the TSS and SetS contents in wastewater samples. The machine vision algorithm (MVA) coupled with the Red color plane (derived from color histograms in the Red-Green-Blue (RGB) system) showed the best prediction results with R2 of 0.988 and 0.964, and relative error of prediction (REP) of 6.133 and 9.115% for TSS and SetS, respectively. The constructed models were validated by Analysis of Variance (ANOVA), and the accuracy and precision of the predictions by the t- and F-tests, respectively, at a 0.05 significance level. The elliptical joint confidence region (EJCR) test confirmed the accuracy, while the coefficient of variation (CV) of 6.529 and 10.908% confirmed the good precisions, respectively. Compared with the reference method (Standard Methods For the Examination of Water and Wastewater), the proposed method reduced the analysis volume from 1.5 L to just 15 mL and the analysis time from 12 h to 24 s per sample. Therefore, SWAMVA can be considered an important alternative to the determination of TSS and SetS in wastewater as an automatic, fast, and low-cost analytical tool, following the principles of Green Chemistry and exploiting Industry 4.0 features such as intelligent processing, miniaturization, and machine vision.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compeleceng.2022.107965,Journal,Computers and Electrical Engineering,scopus,2022-05-01,sciencedirect,Real-Time intelligent Elevator Monitoring and Diagnosis: Case Studies and Solutions with applications using Artificial Intelligence,https://api.elsevier.com/content/abstract/scopus_id/85127311531,"Under ""Industry 4.0"", the implementation and application of big data and Artificial Intelligence (AI) technology in the elevator industry has become more and more common. With the surge of the elevator operation data and higher requirements for its real-time performance, the traditional elevator fault monitoring is inaccurate, which needs to be solved urgently. In this paper, a fault monitoring and diagnosis method of elevator based on AI and big data is proposed. Firstly, the elevator system and its fault types and causes are analyzed. Then in order to select the best big data processing tools, the performance of Flink and Spark Streaming is compared. The results show that Flink features faster computing speed and is more suitable for handling big data. Thirdly, a pattern recognition algorithm based on finite state machine (FSM) is proposed to monitor the running state for whole elevator control system. Finally, simulation experiment has been made.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.apr.2022.101388,Journal,Atmospheric Pollution Research,scopus,2022-05-01,sciencedirect,Estimating the effect of the COVID-19 pandemic on pollutant emissions in Europe,https://api.elsevier.com/content/abstract/scopus_id/85127172045,"Changes in primary emissions due to the COVID-19 lockdowns in Europe for the year 2020 have been estimated by considering fully open-access and near-real-time measured activity data from a wide range of information sources and with simple computational techniques. The estimates consist on a dataset of reduction factors that are both time- and country-dependent and provided for the following source categories: energy industry (power plants), manufacturing industry, road traffic, aviation, shipping and other stationary combustion activities such as residential and commercial-institutional activities. Inspired in other authors’ estimates for COVID reductions, the advantage of this methodology is that there is no use of machine learning, making this procedure more accessible to the general scientific community. We have followed a fast methodology that takes advantage of observed relationships between variables (e.g. temperature and energy demand) without needing special algorithms for finding those relationships. The comparison of our estimates with others from other authors indicate a reasonable agreement and pointing out that emissions dropped by a 17% on average in Europe, with large differences between sectors of activities and spatial heterogeneity. The most affected sector was aviation, with a spatial-averaged variation of −63% in emissions since the implementation of first restrictions with respect business-as-usual values. 2020 emission changes with respect to business-as-usual values in countries ranges from a −13% in Norway and Poland to a more than −20% in several Mediterranean countries as well as the United Kingdom. Two main periods of emission reductions have been identified.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rser.2022.112128,Journal,Renewable and Sustainable Energy Reviews,scopus,2022-05-01,sciencedirect,"Data-driven probabilistic machine learning in sustainable smart energy/smart energy systems: Key developments, challenges, and future research opportunities in the context of smart grid paradigm",https://api.elsevier.com/content/abstract/scopus_id/85125617590,"The current trend indicates that energy demand and supply will eventually be controlled by autonomous software that optimizes decision-making and energy distribution operations. New state-of-the-art machine learning (ML) technologies are integral in optimizing decision-making in energy distribution networks and systems. This study was conducted on data-driven probabilistic ML techniques and their real-time applications to smart energy systems and networks to highlight the urgency of this area of research. This study focused on two key areas: i) the use of ML in core energy technologies and ii) the use cases of ML for energy distribution utilities. The core energy technologies include the use of ML in advanced energy materials, energy systems and storage devices, energy efficiency, smart energy material manufacturing in the smart grid paradigm, strategic energy planning, integration of renewable energy, and big data analytics in the smart grid environment. The investigated ML area in energy distribution systems includes energy consumption and price forecasting, the merit order of energy price forecasting, and the consumer lifetime value. Cybersecurity topics for power delivery and utilization, grid edge systems and distributed energy resources, power transmission, and distribution systems are also briefly studied. The primary goal of this work was to identify common issues useful in future studies on ML for smooth energy distribution operations. This study was concluded with many energy perspectives on significant opportunities and challenges. It is noted that if the smart ML automation is used in its targeting energy systems, the utility sector and energy industry could potentially save from $237 billion up to $813 billion.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.apm.2022.01.008,Journal,Applied Mathematical Modelling,scopus,2022-05-01,sciencedirect,Deep learning for gas sensing using MOFs coated weakly-coupled microbeams,https://api.elsevier.com/content/abstract/scopus_id/85123830545,"Gas sensors have been increasingly employed in a wide range of applications such as air quality monitoring, disease diagnosis, potential leakage of exhaust gases in industrial facilities, and food quality control. We propose a novel MEMS gas sensor made of two mechanically-coupled microbeams coated with metal organic frameworks and subject to electric actuation. The objective is to exploit the dynamic features of the microstructure to simultaneously detect the presence of two gases, namely carbon dioxide (CO
                        
                           
                           2
                        
                     ) and methane (CH
                        
                           
                           4
                        
                     ), and estimate their concentrations. A nonlinear mathematical model of the gas sensor is developed and verified in the nonlinear operating regime against experiments reported in the literature. Deep learning methods are integrated with the sensor’s model to predict the gases’ characteristics from the dynamic features of the coupled microbeams. To do so, we generate a large set of dynamic responses of the sensor for varying operating conditions and use it to train deep neural networks to capture the nonlinear dependencies in the data. The results show evidence of high prediction accuracy in terms of gas type and concentration estimation. The optimal accuracy is achieved when all dynamic features of the two coupled beams are used as inputs. These include the first six natural frequencies, the RMS, minimum, and maximum values of the dynamic response of the coupled microbeams when actuated by a combination of DC and AC voltages near the veering point. Yet, the use of the dynamic features of one of the two microbeams is found to still predict the concentrations of both gases with a good accuracy level. As such, one can deploy one beam for actuation and the other beam for sensing both gases thanks to their mechanical coupling. We also compare our results to those obtained from classical statistical approaches, namely linear regression and support vector regression. The neural network based approach outperforms these two classical methods, especially when a limited number of data features is used as input.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.conbuildmat.2022.127129,Journal,Construction and Building Materials,scopus,2022-04-25,sciencedirect,Determination of pith location along Norway spruce timber boards using one dimensional convolutional neural networks trained on virtual timber boards,https://api.elsevier.com/content/abstract/scopus_id/85126599219,"Knowledge of pith location is needed for modelling of sawn timber and for real time assessment of wood material in the wood working industry. However, the methods that are available and implemented in optical scanner today seldom meet customer requirements on accuracy and/or speed. In the present research data of greyscale images of the four longitudinal sides of board and a one-dimensional convolutional neural network were used to determine pith location along Norway spruce timber boards. A novel stochastic model was developed to generate thousands of virtual timber boards, with photo-realistic surfaces and known pith location, by which the network was trained before it was successfully applied to determine pith location along real boards.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpharm.2022.121604,Journal,International Journal of Pharmaceutics,scopus,2022-04-25,sciencedirect,Reliable stability prediction to manage research or marketed vaccines and pharmaceutical products. “Avoid any doubt for the end-user of vaccine compliance at time of administration”,https://api.elsevier.com/content/abstract/scopus_id/85126311953,"A major challenge for the pharmaceutical/vaccine industry is to anticipate and test/control product stability, regardless of the time/temperature profile of the product, from release to administration. Current empirical stability protocols performed to ensure product stability remain limited to the prediction of product stability in a thermal excursion (cold chain break) during their long-term storage. As recently recommended by the World Health Organization, mathematical models can be used for shelf-life and stability predictions. Therefore, various approaches have been published with good performance for simple chemical reactions. However, for biomolecules/vaccines, more complex reaction profiles require more complex models to predict their stability with a good level of confidence. This complexity constitutes a real scientific challenge because the number of model parameters increases with model complexity and need to be balanced with the limited number and quality of the available experimental data.
                  We have developed a dedicated method/software based on different vaccines/pharmaceutical case studies. This predictive method considers phenomenological models, five levels of model confidence assessment, predictive quality value and simulated designs of experiment to improve and define the limits within which the prediction models can be used, and to increase model/prediction confidence to the required regulatory and scientific levels. This artificial intelligence system should help to avoid any doubt of stability at time of vaccine injection.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.enbuild.2022.111988,Journal,Energy and Buildings,scopus,2022-04-15,sciencedirect,A Digital Twin predictive maintenance framework of air handling units based on automatic fault detection and diagnostics,https://api.elsevier.com/content/abstract/scopus_id/85125646504,"The building industry consumes the most energy globally, making it a priority in energy efficiency initiatives. Heating, ventilation, and air conditioning (HVAC) systems create the heart of buildings. Stable air handling unit (AHU) functioning is vital to ensuring high efficiency and extending the life of HVAC systems. This research proposes a Digital Twin predictive maintenance framework of AHU to overcome the limitations of facility maintenance management (FMM) systems now in use in buildings. Digital Twin technology, which is still at an initial stage in the facility management industry, use Building Information Modeling (BIM), Internet of things (IoT) and semantic technologies to create a better maintenance strategy for building facilities. Three modules are implemented to perform a predictive maintenance framework: operating fault detection in AHU based on the APAR (Air Handling Unit Performance Assessment Rules) method, condition prediction using machine learning techniques, and maintenance planning. Furthermore, the proposed framework was tested in a real-world case study with data between August 2019 and October 2021 for an educational building in Norway to validate that the method was feasible. Inspection information and previous maintenance records are also obtained through the FM system. The results demonstrate that the continually updated data combined with APAR and machine learning algorithms can detect faults and predict the future state of Air Handling Unit (AHU) components, which may assist in maintenance scheduling. Removing the detected operating faults resulted in annual energy savings of several thousand dollars due to eliminating the identified operating faults.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2021.116323,Journal,Expert Systems with Applications,scopus,2022-04-15,sciencedirect,Deep multi-agent reinforcement learning for multi-level preventive maintenance in manufacturing systems[Formula presented],https://api.elsevier.com/content/abstract/scopus_id/85121584320,"Designing preventive maintenance (PM) policies that ensure smooth and efficient production for large-scale manufacturing systems is non-trivial. Recent model-free reinforcement learning (RL) methods shed lights on how to cope with the non-linearity and stochasticity in such complex systems. However, the action space explosion impedes RL-based PM policies to be generalized to real applications. In order to obtain cost efficient PM policies for a serial production line that has multiple levels of PM actions, a novel multi-agent modeling is adopted to support adaptive learning by modeling each machine as cooperative agent. The evaluation of system-level production loss is leveraged to construct the reward function. An adaptive learning framework based on value-decomposition multi-agent actor–critic algorithm is utilized to obtain PM policies. In simulation study, the proposed framework demonstrates its effectiveness by leading other baselines on a comprehensive set of metrics whereas the centralized RL-based methods struggles to converge to stable policies. Our analysis further demonstrates that our multi-agent reinforcement learning based method learns effective PM policies without any knowledge about the environment and maintenance strategies.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aei.2022.101600,Journal,Advanced Engineering Informatics,scopus,2022-04-01,sciencedirect,Simulation and sensor data fusion for machine learning application,https://api.elsevier.com/content/abstract/scopus_id/85130248837,"The performance of machine learning algorithms depends to a large extent on the amount and the quality of data available for training. Simulations are most often used as test-beds for assessing the performance of trained models on simulated environment before deployment in real-world. They can also be used for data annotation, i.e, assigning labels to observed data, providing thus background knowledge for domain experts. We want to integrate this knowledge into the machine learning process and, at the same time, use the simulation as an additional data source. Therefore, we present a framework that allows for the combination of real-world observations and simulation data at two levels, namely the data or the model level. At the data level, observations and simulation data are integrated to form an enriched data set for learning. At the model level, the models learned from observed and simulated data separately are combined using an ensemble technique. Based on the trade-off between model bias and variance, an automatic selection of the appropriate fusion level is proposed. Our framework is validated using two case studies of very different types. The first is an industry 4.0 use case consisting of monitoring a milling process in real-time. The second is an application in astroparticle physics for background suppression.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmsy.2022.04.014,Journal,Journal of Manufacturing Systems,scopus,2022-04-01,sciencedirect,Four Rs Framework for the development of a digital twin: The implementation of Representation with a FDM manufacturing machine,https://api.elsevier.com/content/abstract/scopus_id/85130236591,"This work considers the conceptualization and design of a 4 Rs framework for creating a general purpose, modular Digital Twin. The 4 Rs, correspond to the 4 different phases of a Digital Twin implementation, namely Representation, Replication, Reality, and Relational. Representation is about understanding the physical system, its behavior, actions, components, relationships and describing the significant features for the identified use case as data and algorithms. Replication duplicates the chosen components/variables in a virtual environment from a set of inputs identified in Representation. Reality employs machine learning to produce a virtual device that runs independent of the physical device with the ability to make predictions, enhance models, provide alternative scenarios and optimizations. Reality enhances the virtual system to become autonomous and self-aware with the ability to make decisions and take corrective actions. We introduce these phases and outline their core elements and principles. We showcase the implementation of phase 1, Representation, using a Fused Deposition Modeling (FDM) additive manufacturing machine via temperature and position sensors. We evaluate their precision in representing the actual FDM machine and lay the foundation work for the implementation of the 4R framework in our next work.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cma.2022.114778,Journal,Computer Methods in Applied Mechanics and Engineering,scopus,2022-04-01,sciencedirect,A comprehensive and fair comparison of two neural operators (with practical extensions) based on FAIR data,https://api.elsevier.com/content/abstract/scopus_id/85126008298,"Neural operators can learn nonlinear mappings between function spaces and offer a new simulation paradigm for real-time prediction of complex dynamics for realistic diverse applications as well as for system identification in science and engineering. Herein, we investigate the performance of two neural operators, which have shown promising results so far, and we develop new practical extensions that will make them more accurate and robust and importantly more suitable for industrial-complexity applications. The first neural operator, DeepONet, was published in 2019 (Lu et al., 2019), and its original architecture was based on the universal approximation theorem of Chen & Chen (1995). The second one, named Fourier Neural Operator or FNO, was published in 2020 (Li et al., 2020), and it is based on parameterizing the integral kernel in the Fourier space. DeepONet is represented by a summation of products of neural networks (NNs), corresponding to the branch NN for the input function and the trunk NN for the output function; both NNs are general architectures, e.g., the branch NN can be replaced with a CNN or a ResNet. According to Kovachki et al. (2021), FNO in its continuous form can be viewed conceptually as a DeepONet with a specific architecture of the branch NN and a trunk NN represented by a trigonometric basis. In order to compare FNO with DeepONet computationally for realistic setups, we develop several extensions of FNO that can deal with complex geometric domains as well as mappings where the input and output function spaces are of different dimensions. We also develop an extended DeepONet with special features that provide inductive bias and accelerate training, and we present a faster implementation of DeepONet with cost comparable to the computational cost of FNO, which is based on the Fast Fourier Transform.
                  We consider 16 different benchmarks to demonstrate the relative performance of the two neural operators, including instability wave analysis in hypersonic boundary layers, prediction of the vorticity field of a flapping airfoil, porous media simulations in complex-geometry domains, etc. We follow the guiding principles of FAIR (Findability, Accessibility, Interoperability, and Reusability) for scientific data management and stewardship. The performance of DeepONet and FNO is comparable for relatively simple settings, but for complex geometries the performance of FNO deteriorates greatly. We also compare theoretically the two neural operators and obtain similar error estimates for DeepONet and FNO under the same regularity assumptions.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.csi.2021.103595,Journal,Computer Standards and Interfaces,scopus,2022-04-01,sciencedirect,A reference framework for the implementation of data governance systems for industry 4.0,https://api.elsevier.com/content/abstract/scopus_id/85125880773,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This is mainly based on the digitalization of the industrial environment by means of the convergence of Information Technologies (IT) and operational Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT) and the use of data generated in real time for gaining insights and making decisions. Therefore data becomes a critical asset for Industry 4.0 and must be managed and governed like a strategic asset. We rely on Data Governance (DG) as a key instrument for carrying out this transformation. This paper presents the design of a specific governance framework for Industry 4.0. First, this contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, the cloud and edge computing, the artificial intelligence and the current regulations. Next, we formally define a reference framework for the implementation of Data Governance Systems for Industry 4.0 using international standards and providing several examples of architecture building blocks.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2022.108546,Journal,Applied Soft Computing,scopus,2022-04-01,sciencedirect,Adaptive non-linear soft sensor for quality monitoring in refineries using Just-in-Time Learning—Generalized regression neural network approach,https://api.elsevier.com/content/abstract/scopus_id/85125169846,"Real time estimation of target quality variables using soft sensor relevant to time varying process conditions will be a significant step forward in effective implementation of Industry 4.0. Generalized Regression neural network (GRNN) has been used as a steady state quality monitoring soft sensor with reasonable estimation accuracy. However, the accurate prediction capability of GRNN has rarely been explored in a time varying environment. This article reports design of adaptive soft sensor using GRNN as a local model in Just-in-Time learning (JITL-GRNN) framework. The JITL-GRNN adaptive soft sensing technique is further investigated in various dimensions such as, the effect of different similarity index criteria and relevant dataset size on model prediction accuracy and model computation time. Performance of the proposed JITL-GRNN soft sensor is investigated by assessing its prediction accuracy on two benchmark industrial datasets. In addition, dynamic Non-linear autoregressive with exogenous inputs (NARX) neural network model is also developed and the performance of NARX model was compared with the proposed JITL-GRNN model. Results show that the JITL-GRNN adaptive soft sensor has at par or better prediction capability than the NARX model and many other models reported in literature.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2022.104729,Journal,Engineering Applications of Artificial Intelligence,scopus,2022-04-01,sciencedirect,A multi–modal unsupervised fault detection system based on power signals and thermal imaging via deep AutoEncoder neural network,https://api.elsevier.com/content/abstract/scopus_id/85124798795,"In this paper a multi-modal unsupervised Deep Learning based algorithm for fault detection is proposed. Such method is applied to real data from a testing procedure implemented on an industrial production line. Both thermal images and current and power measurements coming from industrial refrigerators are collected. The considered dataset is highly unbalanced with the vast majority of samples being healthy. Thermal images are processed via a Deep Convolutional neural network. The features extracted from the thermal images are thus merged to structured data of power, current and temperature. Therefore, a Deep Auto-Encoder is trained on the dataset to signal anomalies corresponding to faults in the refrigerators. Three different methods are trained and compared: (1) an automatic method in which an expert extracts relevant features from thermal images without using the image recognition module; (2) a semi-automatic method where the convolutional neural network is applied to regions of interest within the thermal images selected by an expert operator; (3) a fully automatic method in which the Deep convolutional network processes the whole thermal image without any human intervention. The three methods show comparable results with nevertheless slight differences.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aap.2022.106594,Journal,Accident Analysis and Prevention,scopus,2022-04-01,sciencedirect,Artificial intelligence-aided railroad trespassing detection and data analytics: Methodology and a case study,https://api.elsevier.com/content/abstract/scopus_id/85124455242,"The railroad industry plays a principal role in the transportation infrastructure and economic prosperity of the United States, and safety is of the utmost importance. Trespassing is the leading cause of rail-related fatalities and there has been little progress in reducing the trespassing frequency and deaths for the past ten years in the United States. Although the widespread deployment of surveillance cameras and vast amounts of video data in the railroad industry make witnessing these events achievable, it requires enormous labor-hours to monitor real-time videos or archival video data. To address this challenge and leverage this big data, this study develops a robust Artificial Intelligence (AI)-aided framework for the automatic detection of trespassing events. This deep learning-based tool automatically detects trespassing events, differentiates types of violators, generates video clips, and documents basic information of the trespassing events into one dataset. This study aims to provide the railroad industry with state-of-the-art AI tools to harness the untapped potential of video surveillance infrastructure through the risk analysis of their data feeds in specific locations. In the case study, the AI has analyzed over 1,600 h of archival video footage and detected around 3,000 trespassing events from one grade crossing in New Jersey. The data generated from these big video data will potentially help understand human factors in railroad safety research and contribute to specific trespassing proactive safety risk management initiatives and improve the safety of the train crew, rail passengers, and road users through engineering, education, and enforcement solutions to trespassing.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmatprotec.2021.117474,Journal,Journal of Materials Processing Technology,scopus,2022-04-01,sciencedirect,Deep DIC: Deep learning-based digital image correlation for end-to-end displacement and strain measurement,https://api.elsevier.com/content/abstract/scopus_id/85123699938,"Digital image correlation (DIC) has become an industry standard to retrieve accurate displacement and strain measurement in tensile testing and other material characterization. Though traditional DIC offers a high precision estimation of deformation for general tensile testing cases, the prediction becomes unstable at large deformation or when the speckle patterns start to tear. In addition, traditional DIC requires a long computation time and often produces a low spatial resolution output affected by filtering and speckle pattern quality. To address these challenges, we propose a new deep learning-based DIC approach – Deep DIC, in which two convolutional neural networks, DisplacementNet and StrainNet, are designed to work together for end-to-end prediction of displacements and strains. DisplacementNet predicts the displacement field and adaptively tracks a region of interest. StrainNet predicts the strain field directly from the image input without relying on the displacement prediction, which significantly improves the strain prediction accuracy. A new dataset generation method is developed to synthesize a realistic and comprehensive dataset, including the generation of speckle patterns and the deformation of the speckle image with synthetic displacement fields. Though trained on synthetic datasets only, Deep DIC gives highly consistent and comparable predictions of displacement and strain with those obtained from commercial DIC software for real experiments, while it outperforms commercial software with very robust strain prediction even at large and localized deformation and varied pattern qualities. In addition, Deep DIC is capable of real-time prediction of deformation with a calculation time down to milliseconds.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.conengprac.2022.105071,Journal,Control Engineering Practice,scopus,2022-04-01,sciencedirect,Iterative learning and feedback control for the curvature and contact force of a metal strip on a roll,https://api.elsevier.com/content/abstract/scopus_id/85123698182,"This paper presents a novel model-based control strategy for the curvature and the contact force at the contact point of a metal strip and a roll. Due to the plastic deformation of the strip, the bending history of the strip has to be considered in the controller design. A multivariable two-degrees-of-freedom (2-DOF) control structure is used to track user-defined periodic references for the outputs. The proposed multi-input/multi-output (MIMO) feedback control structure consists of a spatial iterative learning control (ILC) combined with a multivariable PI controller. The control strategy is implemented in a real-time system and verified by measurements on an experimental test rig.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmatprotec.2022.117495,Journal,Journal of Materials Processing Technology,scopus,2022-04-01,sciencedirect,Real-time anomaly detection using convolutional neural network in wire arc additive manufacturing: Molybdenum material,https://api.elsevier.com/content/abstract/scopus_id/85123250275,"Wire arc additive manufacturing (WAAM) has received attention because of its high deposition rate, low cost, and high material utilization. However, quality issues are critical in WAAM because it builds upon arc welding technology, which can result in low precision and poor quality of the melted parts. Hence, anomaly detection is essential for identifying abnormal behaviors and process instability during WAAM to reduce the time and cost of post-process treatment. The relevant studies have been conducted on anomaly detection algorithms using machine learning in fused deposition modeling and laser powder bed fusion; however, they have less investigated the implementation for in situ quality monitoring in WAAM. This work presents a real-time anomaly detection method that uses a convolutional neural network (CNN) in WAAM. The proposed method enables creation of CNN-based models that detect abnormalities by learning from the melt pool image data, which are pre-processed to increase learning performance. A prototype system was implemented to classify melt pool images into “normal” and “abnormal” states, with the latter accounting for balling and bead-cut defects. Experiments were conducted using molybdenum, a cost-intensive and hard-to-machine material. Four CNN-based models were created using MobileNetV2, DenseNet169, Resnet50V2, and InceptionResNetV2. Then, their performances were validated in terms of classification accuracy and processing time. The MobileNetV2 model yielded the best performance with 98 % of classification accuracy and 0.033 s/frame of processing time. This model was also compared with an object detection algorithm named “YOLO”, which yielded 73.5 % of classification accuracy and 0.067 s/frame of processing time.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmatprotec.2021.117476,Journal,Journal of Materials Processing Technology,scopus,2022-04-01,sciencedirect,Correlating in-situ sensor data to defect locations and part quality for additively manufactured parts using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85123196207,"In this work, process monitoring data, including layerwise imagery, multi-spectral emissions, and laser scan vector data, were collected during laser-based powder bed fusion additive manufacturing and correlated to fatigue performance. All parts were X-ray CT scanned post-build, and internal flaws were identified via an automated defect recognition software. Convolutional neural networks were trained to discriminate flaws from nominal build conditions using in situ data modalities only. Trained classifiers were then tested against a previously unseen data set collected from an independent build, and classification performance and metrics for information content provided by each individual modality were formally established. Correlations were drawn between the detected flaw populations and the corresponding fatigue properties, demonstrating that fatigue critical lack-of-fusion flaws can be detected via machine learning of in situ sensor data. The present results also show that, at least from a classification accuracy perspective, flaw detection via ML on process monitoring data is a viable path forward for real-time flaw detection and automated, interlayer repair strategies. However, strategies for extracting and analyzing sensor data in real-time without incurring excessive increases in build time must first be developed. These developments represent necessary components to draw direct correlations between in situ data modalities, internal part quality, and fatigue performance.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.scs.2021.103663,Journal,Sustainable Cities and Society,scopus,2022-04-01,sciencedirect,"Federated learning enabled digital twins for smart cities: Concepts, recent advances, and future directions",https://api.elsevier.com/content/abstract/scopus_id/85123031989,"Recent advances in Artificial Intelligence (AI) and the Internet of Things (IoT) have facilitated continuous improvement in smart city based applications such as smart healthcare, transportation, and environmental management. Digital Twin (DT) is an AI-based virtual replica of the real-world physical entity. DTs have been successfully adopted in manufacturing and industrial sectors, they are however still at the early stage in smart city based applications. The major reason for this lag is the lack of trust and privacy issues in sharing sensitive data. Federated Learning (FL) is a technology that could be integrated along with DT to ensure privacy preservation and trustworthiness. This paper focuses on the integration of these two promising technologies for adoption in real-time and life-critical scenarios, as well as for ease of governance in smart city based applications. We present an extensive survey on the various smart city based applications of FL models in DTs. Based on the study, some prominent challenges and future directions are presented for better FL–DT integration in future applications.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jnca.2021.103309,Journal,Journal of Network and Computer Applications,scopus,2022-04-01,sciencedirect,Industrial digital twins at the nexus of NextG wireless networks and computational intelligence: A survey,https://api.elsevier.com/content/abstract/scopus_id/85122979494,"By amalgamating recent communication and control technologies, computing and data analytics techniques, and modular manufacturing, Industry 4.0 promotes integrating cyber–physical worlds through cyber–physical systems (CPS) and digital twin (DT) for monitoring, optimization, and prognostics of industrial processes. A DT enables interaction with the digital image of the industrial physical objects/processes to simulate, analyze, and control their real-time operation. DT is rapidly diffusing in numerous industries with the interdisciplinary advances in the industrial Internet of things (IIoT), edge and cloud computing, machine learning, artificial intelligence, and advanced data analytics. However, the existing literature lacks in identifying and discussing the role and requirements of these technologies in DT-enabled industries from the communication and computing perspective. In this article, we first present the functional aspects, appeal, and innovative use of DT in smart industries. Then, we elaborate on this perspective by systematically reviewing and reflecting on recent research trends in next-generation (NextG) wireless technologies (e.g., 5G-and-Beyond networks) and design tools, and current computational intelligence paradigms (e.g., edge and cloud computing-enabled data analytics, federated learning). Moreover, we discuss the DT deployment strategies at different communication layers to meet the monitoring and control requirements of industrial applications. We also outline several key reflections and future research challenges and directions to facilitate industrial DT’s adoption.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cej.2021.134424,Journal,Chemical Engineering Journal,scopus,2022-04-01,sciencedirect,Magnetic flexible sensor with tension and bending discriminating detection,https://api.elsevier.com/content/abstract/scopus_id/85122619916,"The flexible wearable sensors with high sensitivity and stability provide wide potential in artificial intelligence and human–machine interaction. However, dual-modal sensor with contact and non-contact working mode based on a facile and cost-effective methodology which can recognize external stimulus forms remains a challenge. Especially, magnetic non-contact sensing performance has gained increasing attention in smart wearable device. This work reports a flexible and magnetic sensor based on a sandwich structure film (SSF) which can detect both tension and bending stimuli by supplementing opposite electric signal feedbacks. Notably, the 
                        
                           Δ
                           R
                           /
                           
                              R
                              0
                           
                        
                      of SSF sensor reaches 44.1% at 5% tensile strain and it presents excellent stability even after 10,000 cycles. Moreover, the 
                        
                           Δ
                           R
                           /
                           
                              R
                              0
                           
                        
                      of SSF sensor maintains at −17.2% under 2 mm bending displacement. Additionally, SSF sensor can be employed as electronic skin to perceive the human joint motion in real-time. Furthermore, the direction and density of magnetic field applied to SSF sensor can be clearly discriminated, thus a non-contact magnetic keyboard has been designed and developed. As a result, the facile manufacturing processes and outstanding multifunctional sensing characteristics endow SSF sensor with high implementation potential in the next-generation intelligent electronic equipment or systems.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ndteint.2021.102597,Journal,NDT and E International,scopus,2022-04-01,sciencedirect,Approach to weld segmentation and defect classification in radiographic images of pipe welds,https://api.elsevier.com/content/abstract/scopus_id/85122502573,"Detecting indications (points suspicious for defects) in weld radiographs is an important research topic in the field of industrial non-destructive testing. Many computer-aided detection techniques have been designed for such applications as detecting indications occurrence, segmentation of the indication area, classification of the indications. However, these techniques are mainly focused on only one of the listed problems. Different defects may exhibit different visual properties in shapes, sizes, textures, contrasts, and positions, that often leading to ad-hoc solutions. The paper investigates to the fine tuning of the machine learning approach to high resolution radiograph processing produced by real-time radiography using digital detector arrays (DDA) method. The main contributions of this work are the preprocessing feature for human readability of the radiographic images and the proposed neural network-based solutions for all stages, from weld detection on the radiographic images and its segmentation to indication segmentation and classification. The designed approach was implemented as a web service with web site front-end. The demo version of the software is available with this instruction https://github.com/NastyaMittseva/DefectRecognitionSystem.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cor.2021.105674,Journal,Computers and Operations Research,scopus,2022-04-01,sciencedirect,Balancing and scheduling assembly lines with human-robot collaboration tasks,https://api.elsevier.com/content/abstract/scopus_id/85122478755,"In light of the Industry 5.0 trend towards human-centric and resilient industries, human-robot collaboration (HRC) assembly lines can be used to enhance productivity and workers’ well-being, provided that the optimal allocation of tasks and available resources can be determined. This study investigates the assembly line balancing problem (ALBP), considering HRC. This problem, abbreviated ALBP-HRC, arises in advanced manufacturing systems, where humans and collaborative robots share the same workplace and can simultaneously perform tasks in parallel or in collaboration. Driven by the need to solve the more complex assembly line-balancing problems found in the automotive industry, this study aims to address the ALBP-HRC with the cycle time and the number of operators (humans and robots) as the primary and secondary objective, respectively. In addition to the traditional ALBP constraints, the human and robot characteristics, in terms of task times, allowing multiple humans and robots at stations, and their joint/collaborative tasks are formulated into a new mixed-integer linear programming (MILP) model. A neighborhood-search simulated annealing (SA) is proposed with customized solution representation and neighborhood search operators designed to fit into the problem characteristics. Furthermore, the proposed SA features an adaptive neighborhood selection mechanism that enables the SA to utilize its exploration history to dynamically choose appropriate neighborhood operators as the search evolves. The proposed MILP and SA are implemented on real cases taken from the automotive industry where stations are designed for HRC. The computational results over different problems show that the adaptive SA produces promising solutions compared to the MILP and other swarm intelligence algorithms, namely genetic algorithm, particle swarm optimization, and artificial bee colony. The comparisons of human/robot versus HRC settings in the case study indicate significant improvement in the productivity of the assembly line when multiple humans and robots with collaborative tasks are permissible at stations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.mee.2022.111750,Journal,Microelectronic Engineering,scopus,2022-03-15,sciencedirect,Flexible pressure sensor with a wide pressure measurement range and an agile response based on multiscale carbon fibers/carbon nanotubes composite,https://api.elsevier.com/content/abstract/scopus_id/85125486815,"As an important branch of wearable electronics, flexible pressure sensors have aroused extensive concern owing to their wide range of applications, such as human health monitoring, human–machine interfaces, and artificial intelligence. Therefore, higher requirements are put forward for the dominating performances of sensitivity, pressure measurement range and response time about flexible pressure sensors. Here, we present a flexible piezoresistive pressure sensor (FPPS) based on carbon fibers (CFs)/carbon nanotubes (CNTs)-polydimethylsiloxane (PDMS) composite and interdigital electrode, which displays a wide measurement range of stress and an agile response at loading and unloading forces. Due to the cooperating advantages of the multiscale carbon fillers, the microstructures on the surface of the piezoresistive film and the interdigital electrode, the FPPS reveals a wide measurement range (from 0 kPa to 185.9 kPa) and a high sensitivity (2.02 kPa‐1 at pressures between 36 Pa and 50.2 kPa). Meanwhile, owing to the carefully designed trilateral double-sided adhesive structure, the FPPS exhibits an agile response (response time of 43 ms) and a stable output (test time of 9000 s). Moreover, the FPPS is capable of identifying clapping and trampling motions, demonstrating its effectiveness as a flexible sensing unit. The novel pressure sensor was integrated with additional software and hardware to realize a real-time pressure acquisition and display system, providing a potential use of the device in an intelligent industry application.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.measurement.2022.110819,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2022-03-15,sciencedirect,ChickTrack – A quantitative tracking tool for measuring chicken activity,https://api.elsevier.com/content/abstract/scopus_id/85124237105,"The automatic detection, counting and tracking of individual and flocked chickens in the poultry industry is of paramount to enhance farming productivity and animal welfare. Due to methodological difficulties, such as the complex background of images, varying lighting conditions, and occlusions from e.g., feeding stations, water nipple stations and barriers in the chicken rearing production floor, it is a challenging task to automatically recognize and track birds using computer software. Here, a deep learning model based on You Only Look Once (Yolov5) is proposed for detecting domesticated chickens from videos with varying complex backgrounds. A multiscale feature is being adapted to the Yolov5 network for mapping modules in the counting and tracking of the trajectories of the chickens. The Yolov5 network was trained and tested on our dataset which resulted in an enhanced tracking precision accuracy. Using Kalman Filter, the proposed model was able to track multiple chickens simultaneously with the focus to associate individual chickens across the frames of the video for real time and online applications. By being able to detect the chickens amid diverse background interference and counting them precisely along with tracking the movement and measuring their travelled path and direction, the proposed model provides excellent performance for on-farm applications. Artificial intelligence enabled automatic measurements of chicken behavior on-farm using cameras offers continuous monitoring of the chicken's ability to perch, walk, interact with other birds and the farm environment, as well as the assessment of dustbathing, thigmotaxis, and foraging frequency, which are important indicators for their ability to express natural behaviors. This study highlights the potential of automated monitoring of poultry through the usage of ChickTrack model as a digital tool in enabling science-based animal husbandry practices and thereby promote positive welfare for chickens in animal farming.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ces.2021.117373,Journal,Chemical Engineering Science,scopus,2022-03-15,sciencedirect,An explainable artificial intelligence based approach for interpretation of fault classification results from deep neural networks,https://api.elsevier.com/content/abstract/scopus_id/85122261466,"Process monitoring is crucial to ensure operational reliability and to prevent industrial accidents. Data-driven methods have become the preferred approach for fault detection and diagnosis. Specifically, deep learning algorithms such as Deep Neural Networks (DNNs) show good potential even in complex processes. A key shortcoming of DNNs is the difficulty in interpreting their classification result. Emerging approaches from explainable Artificial Intelligence (XAI) seek to address this shortcoming. This paper proposes a method based on the Shapley value framework and its implementation using integrated gradients to identify those variables which lead a DNN to classify an input as a fault. The method estimates the marginal contribution of each variable to the DNN, averaged over the path from the baseline (in this case, the process’ normal state) to the current sample. We illustrate the resulting variable attribution using a numerical example and the benchmark Tennessee Eastman process. Our results show that the proposed methodology provides accurate, sample-specific explanations of the DNN’s prediction. These can be used by the offline model developer to improve the DNN if necessary. It can also be used by the plant operator in real-time to understand the black-box DNN’s predictions and decide on operational strategies.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2021.116203,Journal,Expert Systems with Applications,scopus,2022-03-15,sciencedirect,Semantic segmentation based stereo visual servoing of nonholonomic mobile robot in intelligent manufacturing environment,https://api.elsevier.com/content/abstract/scopus_id/85119331942,"In the interest of developing an intelligent manufacturing environment with an agile, efficient, and optimally utilized transportation system, mobile robots need to achieve a certain level of autonomy as they play an important role in carrying out transportation tasks. Bearing this in mind, in the paper we propose a novel stereo visual servoing method for nonholonomic mobile robot control based on semantic segmentation. Semantic segmentation provides a rich body of information required for an adequate decision-making process in a clustered, dynamic, and ever-changing manufacturing environment. The innovative idea behind the new visual servoing system is to utilize semantic information of the scene for visual servoing, as well as for other mobile robot tasks, such as obstacle avoidance, scene understanding, and simultaneous localization and mapping. Semantic segmentation is carried out by exploiting fully convolutional neural networks. The new visual servoing algorithm utilizes an intensity-based image registration procedure, which results in the image transformation matrix. The transformation matrix encompasses the relations of images taken at the current and desired pose, and that information is directly used for visual servoing. The developed algorithm is deployed on our own developed wheeled differential drive mobile robot RAICO (Robot with Artificial Intelligence based COgnition). The experimental evaluation is carried out in the 3D simulation environment and in the laboratory model of the real manufacturing environment. The experimental results show that the accuracy of the proposed approach is improved when compared to the state-of-the-art approaches while being robust to the partial occlusions of the scene and illumination changes.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eclinm.2022.101320,Journal,eClinicalMedicine,scopus,2022-03-01,sciencedirect,Economic costs of childhood stunting to the private sector in low- and middle-income countries,https://api.elsevier.com/content/abstract/scopus_id/85126568488,"Background
                  Stunting during childhood has long-term consequences on human capital, including decreased physical growth, and lower educational attainment, cognition, workforce productivity and wages. Previous research has quantified the costs of stunting to national economies, however, beyond a few single-country datasets there has been a limited number of which have used diverse datasets and have had a dedicated focus on the private sector, which employs nearly 90% of the workforce in many low- and middle-income countries (LMICs). We aimed to examine (i) the impact of childhood stunting on income loss of private sector workforce in LMICs; (ii) to quantify losses in sales to private firms in LMICs due to childhood stunting; and (iii) to estimate potential gains (benefit-cost ratios) if stunting levels are reduced in select high prevalence countries.
               
                  Methods
                  This multiple-methods study engaged multi-disciplinary technical advisers, executed several literature reviews, used innovative statistical methods, and implemented health and labor economic models. We analyzed data from seven longitudinal datasets (up to 30+ years of follow-up; 1982–2016; Peru, Ethiopia, India, Vietnam, Philippines, Tanzania, Brazil), 108 private firm datasets (spanning 2008–2020), and many global datasets including Joint Malnutrition Estimates, and World Development Indicators to produce estimates for 120+ LMICs (with estimates up to 2021). We studied the impact of childhood stunting on adult cognition, education, and height as pathways to wages/productivity in adulthood. We employed cloud-based artificial intelligence (AI) platforms, and conducted comparative analyses using three analytic approaches: traditional frequentist statistics, Bayesian inferential statistics and machine learning. We employed labour and health economic models to estimate wage losses to the private sector worker and firm revenue losses due to stunting. We also estimated benefit-cost ratios for countries investing in nutrition-specific interventions to prevent stunting.
               
                  Findings
                  Across 95 LMICs, childhood stunting costs the private sector at least US$135.4 billion in sales annually. Firms from countries in Latin America and the Caribbean and East Asia and Pacific regions had the greatest losses. Total sales losses to the private sector accumulated to 0.01% to 1.2% of national GDP across countries. Sectors most affected by childhood stunting were manufacturing (non-metallic mineral, fabricated metal, other), garments and food sectors. Sales losses were highest for larger sized private firms. Across regions (representing 123 LMICs), US$700 million (Middle East and North Africa) to US$16.5 billion (East Asia and Pacific) monthly income was lost among private sector workers. Investing in stunting reduction interventions yields gains from US$2 to US$81 per $1 invested annually (or 100% to 8000% across countries). Across sectors, the highest returns were in elementary occupations (US$46) and the lowest were among agricultural workers (US$8). By gender, women incurred a higher income penalty from childhood stunting and earned less than men; due to their relatively higher earnings, the returns for investing in stunting reduction were consistently higher for men across most countries studied.
               
                  Interpretation
                  Childhood stunting costs the private sector in LMICs billions of dollars in sales and earnings for the workforce annually. Returns to nutrition interventions show that there is an economic case to be made for investing in childhood nutrition, alongside a moral one for both the public and private sector. This research could be used to motivate strong public-private sector partnerships to invest in childhood undernutrition for benefits in the short and long-term.
               
                  Funding
                  The Power of Nutrition (UK); Patrick J McGovern Foundation (USA).",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rineng.2022.100362,Journal,Results in Engineering,scopus,2022-03-01,sciencedirect,Machine learning based adaptive soft sensor for flash point inference in a refinery realtime process,https://api.elsevier.com/content/abstract/scopus_id/85124665324,"In industrial control processes, certain characteristics are sometimes difficult to measure by a physical sensor due to technical and/or economic limitations. This fact is especially true in the petrochemical industry. Some of those quantities are especially crucial for operators and process safety. This is the case for the automotive diesel Flash Point Temperature (FT). Traditional methods for FT estimation are based on the study of the empirical inference between flammability properties and the denoted target magnitude. The necessary measures are taken indirectly by samples from the process and analyzing them in the laboratory, this process implies time (can take hours from collection to flash temperature measurement) and thus make it very difficult for real-time monitorization, which in fact results in security and economical losses. This study defines a procedure based on Machine Learning modules that demonstrate the power of real-time monitorization over real data from an important international refinery. As input, easily measured values provided in real-time, such as temperature, pressure, and hydraulic flow are used and a benchmark of different regressive algorithms for FT estimation is presented. The study highlights the importance of sequencing preprocessing techniques for the correct inference of values. The implementation of adaptive learning strategies achieves considerable economic benefits in the productization of this soft sensor. The validity of the method is tested in the reality of a refinery. In addition, real-world industrial data sets tend to be unstable and volatile, and the data is often affected by noise, outliers, irrelevant or unnecessary features, and missing data. This contribution demonstrates with the inclusion of a new concept, called an adaptive soft sensor, the importance of the dynamic adaptation of the conformed schemes based on Machine Learning through their combination with feature selection, dimensional reduction, and signal processing techniques. The economic benefits of applying this soft sensor in the refinery's production plant and presented as potential semi-annual savings.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.techfore.2021.121448,Journal,Technological Forecasting and Social Change,scopus,2022-03-01,sciencedirect,"Digital twin for sustainable manufacturing supply chains: Current trends, future perspectives, and an implementation framework",https://api.elsevier.com/content/abstract/scopus_id/85121967867,"A digital twin is an integration of virtual and physical systems using disruptive technologies. More precisely, it is a method of developing sustainable, intelligent manufacturing systems for attaining robust quality, reducing time, and customized products using real-time information throughout the product life cycle. This paper presents a systematic literature review of 98 research papers on various digital supply chain twin dimensions with sustainable performance objectives. The selected papers were reviewed and classified into three broad categories: components of the digital twin, applications in the manufacturing supply chain, and sustainability. Based on the review and future perspectives from the study, we suggest that advancements in technologies such as IoT, cloud computing, and blockchain have increased the potential of digital twin applications in the supply chain. The results indicate that a digital supply chain twin should include the things and humans from the entire supply chain and not be restricted to the local manufacturing systems. Based on our review findings, we present a sustainable digital twin implementation framework for supply chains. The proposed framework will guide future practitioners and researchers.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cose.2021.102585,Journal,Computers and Security,scopus,2022-03-01,sciencedirect,CNN based method for the development of cyber-attacks detection algorithms in industrial control systems,https://api.elsevier.com/content/abstract/scopus_id/85121922460,"Extensive communication between smart devices in contemporary Industrial Control Systems (ICS) opens up a vast area for different cyber-attacks and malicious threats. The negative effects of these attacks can not only disrupt or completely disable the system functioning, but also they can have serious safety related consequences. Therefore, cybersecurity in ICS becomes one of the most important issues. In this paper we propose a method for the design of algorithms for the detection of cyber-attacks on communication links between smart devices. The method belongs to the class of semi-supervised data driven approaches and it is based on Convolutional Neural Networks (CNN). Starting from a predefined range of network hyperparameters and data obtained from system operation without attacks, the proposed method autonomously selects suitable CNN architecture and thresholds for online intrusion detection. Following the characteristics of ICS, the proposed intrusion detection is host based, and in our research we consider the structure of ICS and the feasibility of the attack detection algorithm implementation on control system devices. The method is experimentally verified using two case studies. In the first case study that refers to the publicly available dataset obtained from Secure Water Treatment (SWaT) testbed, we present a comparative analysis of the developed method with alternative approaches. The second case study considers a custom developed electro-pneumatic positioning system; in this system we carry out the real-world implementation and validation of the intrusion detection algorithm developed using the proposed method.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.comnet.2021.108719,Journal,Computer Networks,scopus,2022-02-26,sciencedirect,NFStream: A flexible network data analysis framework,https://api.elsevier.com/content/abstract/scopus_id/85122589400,"Network traffic analytics have increased in relevance as researchers promoted machine learning techniques to tackle several traffic management challenges. Over the past decade, the research community and the networking industry have investigated, proposed, and developed a growing number of solutions. However, a large subset of proposed approaches is based on unreliable measurement tools and methodologies. Additionally, some findings are reported on private datasets, which results in a lack of applicability and reproducibility. This paper covers the design and implementation of NFStream, a flexible network data analysis framework. Its key features are flexibility, real-time statistical analysis, and the ability to provide reliable ground truth for modern network usage. NFStream provides the community with a common research framework that can help stimulate research in this field and develop more efficient, reproducible solutions.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compag.2022.106688,Journal,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,Implementation of a decision support system for prediction of the total soluble solids of industrial tomato using machine learning models,https://api.elsevier.com/content/abstract/scopus_id/85122635918,"Tomato is the second most important vegetable in the world, both in terms of production and consumption. Especially for the cultivation of industrial tomato, harvest is conducted when the total soluble solids, a major quality characteristic, are as high as possible. Advancements in technology have made Decision Support Systems simpler and more applicable in an everyday basis. Data Analysis, combined with Machine Learning algorithms are considered the future of sustainable agriculture, allowing farmers to be advised about the best possible decisions for their cultivation. Farmers need to adopt this kind of technology in order to be able to know when the quality of tomatoes is at its peak, in order to gather their product from the field. The implementation of a Decision Support System to predict the total soluble solids was conducted,based on data from previous years, including quality data (pH, Bostwick, L, a/b, Mean Weight, °Brix), the type of hybrid used, weather data and soil data from the fields. Data derived from fields in 6 different regions in the northwestern Peloponnese, Greece over 6 cultivation periods, created a dataset of 33 different inputs. Thirteen different algorithms were put into evaluation in order to find the best one in terms of speed and efficiency. In this research, we developed a Decision Support System using the K-nearest algorithm, which proved to be the best for our dataset. The predicted °Brix were following the same pattern as the actual °Brix. This means that the DSS could advise the farmer about the ideal harvesting period where the °Brix will be maximized. This DSS which is using real time weather data as an input is expected to be a valuable tool for the farmers.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.autcon.2021.104088,Journal,Automation in Construction,scopus,2022-02-01,sciencedirect,Vision-based high-precision intelligent monitoring for shield tail clearance,https://api.elsevier.com/content/abstract/scopus_id/85120874971,"Real-time shield tail clearance measurement and monitoring is a key task during shield tunneling construction. The shield tail clearance measurement and monitoring technology development is still in its infancy, the current methods are mainly designed manually based on intuition. In order to fill the gap between the requirement of shield tail clearance measurement and monitoring and the limitations of the current methods, this paper systematically studies the existing mechanisms related to shield tail clearance measurement and monitoring, and develops a high-precision intelligent monitoring system for shield tail clearance. The proposed monitoring system includes four components: 1) two types of shield tail clearance calculation models, 2) the integrated hardware of the monitoring system which is composed of a data acquisition unit, a signal transmission unit and a control unit, 3) the region of interest (ROI) extraction method based on deep neural network, and the image processing algorithms for image enhancement and feature extraction, 4) the custom-developed software built on mature integrated development environment (IDE). After the calculation model of shield tail clearance is established, the system uses monitoring devices equipped with industrial cameras to obtain the on-site image, and then applies image processing technologies along with deep learning approach to extract the key features, which are brought into the model to calculate the values of shield tail clearance, finally displays these values and simulates the current tunneling attitude of the shield machine in real time. The experimental results show that the system proposed in this paper achieves the goal of high precision measuring and real-time monitoring of the shield tail clearance.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jisa.2021.103046,Journal,Journal of Information Security and Applications,scopus,2022-02-01,sciencedirect,AICrit: A unified framework for real-time anomaly detection in water treatment plants,https://api.elsevier.com/content/abstract/scopus_id/85119422439,"Industrial Control Systems (ICS) in public infrastructure, such as water treatment and distribution plants, have become a target of sophisticated cyber-attacks. Given the ever-present insider and other threats in such systems, there is a need to deploy mechanisms for defense and incidence response beyond the traditional. In this work we present AICrit that operates over the physical constraints and domain norms for accurate and timely detection of process anomalies. AICrit learns system-wide normal behavior using design knowledge and machine learning algorithms to recognize abnormal or irregular behavioral patterns resulting due to process anomalies. AICrit was implemented and evaluated in SWaT by launching several real-time stealthy and coordinated attacks. Experimental results attest to the effectiveness of AICrit in the timely detection of process anomalies with a low occurrence of false alarms. The underlying methodology used in the design of AICrit is generic and applicable to other ICS in various domains such as power, energy, and transportation.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.apenergy.2021.118127,Journal,Applied Energy,scopus,2022-02-01,sciencedirect,Data-driven control of room temperature and bidirectional EV charging using deep reinforcement learning: Simulations and experiments,https://api.elsevier.com/content/abstract/scopus_id/85118721393,"The control of modern buildings is a complex multi-loop problem due to the integration of renewable energy generation, storage devices, and electric vehicles (EVs). Additionally, it is a complex multi-criteria problem due to the need to optimize overall energy use while satisfying users’ comfort. Both conventional rule-based (RB) controllers, which are difficult to apply in multi-loop settings, and advanced model-based controllers, which require an accurate building model, cannot fulfil the requirements of the building automation industry to solve this problem optimally at low development and commissioning costs. This work presents a fully data-driven pipeline to obtain an optimal control policy from historical building and weather data, thus avoiding the need for complex physics-based modelling. We demonstrate the potential of this method by jointly controlling a room temperature and an EV to minimize the cost of electricity while retaining the comfort of the occupants. We model the room temperature with a recurrent neural network and use it as a simulation environment to learn a deep reinforcement learning (DRL) control policy. It achieves on average 17% energy savings and 19% better comfort satisfaction than a standard RB room temperature controller. When a bidirectional EV is connected additionally and a two-tariff electricity pricing is applied, it successfully leverages the battery and decreases the overall cost of electricity. Finally, we deployed it on a real building, where it achieved up to 30% energy savings while maintaining similar comfort levels compared to a conventional RB room temperature controller.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ssci.2021.105529,Journal,Safety Science,scopus,2022-02-01,sciencedirect,A novel decision support system for managing predictive maintenance strategies based on machine learning approaches,https://api.elsevier.com/content/abstract/scopus_id/85118705579,"Nowadays, the industrial environment is characterised by growing competitiveness, short response times, cost reduction and reliability of production to meet customer needs. Thus, the new industrial paradigm of Industry 4.0 has gained interest worldwide, leading many manufacturers to a significant digital transformation. Digital technologies have enabled a novel approach to decision-making processes based on data-driven strategies, where knowledge extraction relies on the analysis of a large amount of data from sensor-equipped factories. In this context, Predictive Maintenance (PdM) based on Machine Learning (ML) is one of the most prominent data-driven analytical approaches for monitoring industrial systems aiming to maximise reliability and efficiency. In fact, PdM aims not only to reduce equipment failure rates but also to minimise operating costs by maximising equipment life. When considering industrial applications, industries deal with different issues and constraints relating to process digitalisation. The main purpose of this study is to develop a new decision support system based on decision trees (DTs) that guides the decision-making process of PdM implementation, considering context-aware information, quality and maturity of collected data, severity, occurrence and detectability of potential failures (identified through FMECA analysis) and direct and indirect maintenance costs. The decision trees allow the study of different scenarios to identify the conditions under which a PdM policy, based on the ML algorithm, is economically profitable compared to corrective maintenance, considered to be the current scenario. The results show that the proposed methodology is a simple and easy way to implement tool to support the decision process by assessing the different levels of occurrence and severity of failures. For each level, savings and the potential costs have been evaluated at leaf nodes of the trees aimed at defining the most suitable maintenance strategy implementation. Finally, the proposed DTs are applied to a real industrial case to illustrate their applicability and robustness.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2021.116045,Journal,Expert Systems with Applications,scopus,2022-02-01,sciencedirect,POSIMNET-R: An immunologic resilient approach to position routers in Industrial Wireless Sensor Networks,https://api.elsevier.com/content/abstract/scopus_id/85117584055,"Industry 4.0 has increased the interest in employing Industrial Wireless Sensor Network (IWSN) technologies in industrial automation. The advantages range from ease of installation and maintenance to reduced deployment time and infrastructure costs. However, industrial automation has critical requirements regarding network infrastructure, such as reliability and failure tolerance. Therefore, it is imperative to have an adequate placement of sensor and router nodes, to obtain a network with multiple paths, allowing the data to reach management systems within a reasonable time, even in the event of failures. The placement of router nodes has to consider latency, network lifespan, connectivity, and failure tolerance aspects in a possibly hostile environment, with classified areas and obstacles such as silos, tanks and buildings. We present a new approach, called POSIMNET-R, to place IWSN routing nodes in an industrial configuration, which circumvents forbidden areas and obstacles, based on Artificial Immunological Networks. The resulting network offers low failure rates and path redundancy criteria. The results have shown that POSIMNET-R was capable of providing a reliable network with multiple paths and resilience of the used routers equal to 81.50% in the basic case study and 73.66% in the real case scenario.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ress.2021.108119,Journal,Reliability Engineering and System Safety,scopus,2022-02-01,sciencedirect,Prognostics and Health Management (PHM): Where are we and where do we (need to) go in theory and practice,https://api.elsevier.com/content/abstract/scopus_id/85117331443,"We are performing the digital transition of industry, living the 4th industrial revolution, building a new World in which the digital, physical and human dimensions are interrelated in complex socio-cyber-physical systems. For the sustainability of these transformations, knowledge, information and data must be integrated within model-based and data-driven approaches of Prognostics and Health Management (PHM) for the assessment and prediction of structures, systems and components (SSCs) evolutions and process behaviors, so as to allow anticipating failures and avoiding accidents, thus, aiming at improved safe and reliable design, operation and maintenance.
                  There is already a plethora of methods available for many potential applications and more are being developed: yet, there are still a number of critical problems which impede full deployment of PHM and its benefits in practice. In this respect, this paper does not aim at providing a survey of existing works for an introduction to PHM nor at providing new tools or methods for its further development; rather, it aims at pointing out main challenges and directions of advancements, for full deployment of condition-based and predictive maintenance in practice.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.future.2021.08.030,Journal,Future Generation Computer Systems,scopus,2022-02-01,sciencedirect,A wearable-based posture recognition system with AI-assisted approach for healthcare IoT,https://api.elsevier.com/content/abstract/scopus_id/85115908462,"Human posture recognition is a challenging task in the medical healthcare industry, when pursuing intelligence, accuracy, security, privacy, and efficiency, etc. Currently, the main posture recognition methods are captured-behaviors-based visual image analysis and wearable devices-based signal analysis. However, these methods suffer from issues such as high misjudgment rate, high-cost and low-efficiency. To address these issues, we propose a collaborative AI-IoT-based solution (namely, WMHPR) that embeds with advanced AI-assisted approach. In WMHPR, we propose the multi-posture recognition (MPR), an offline algorithm is implemented on wearable hardware, to identify posture based on multi-dimensions data. Meanwhile, an AI-based algorithm running on the cloud server (online), named Cascade-AdaBoosting-CART (CACT), is proposed to further enhance the reliability and accuracy of MPR. We recruit 20 volunteers for real-life experiments to evaluate the effectiveness, and the results show our solution is significantly outstanding in terms of accuracy and reliability while comparing with other typical algorithms.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.comcom.2021.10.036,Journal,Computer Communications,scopus,2022-01-15,sciencedirect,LSTM-MFCN: A time series classifier based on multi-scale spatial–temporal features,https://api.elsevier.com/content/abstract/scopus_id/85119299619,"Time series classification (TSC) task attracts huge interests, since they correspond to the real-world problems in a wide variety of fields, such as industry monitoring. Deep learning methods, especially CNN and FCN, shows competitive performance in TSC task by their virtue of good adaption for raw time series and self-adapting extraction of features. Then various variants of CNN are proposed so as to make further breakthrough by the better perception to characteristics of data. Among them, LSTM-FCN and GRU-FCN who learn spatial and temporal features simultaneously are the most remarkable ones, achieving state of the art results. Therefore, inspired by their success and in consideration of the discriminative features implied in time series are diverse in size, a multimodal network LSTM-MFCN composed of multi-scale FCN (MFCN) and LSTM are proposed in this work. The gate-based network LSTM naturally fits to various terms time dependencies, and FCN with multi-scale sets of filters are capable to perceive spatial features of different range from time series curves. Besides, dilation convolution is deployed to build multi-scale receptive fields in larger level without increasing the parameters to be trained. The full perception of large multi-scale spatial–temporal features lead LSTM-MFCN to possess comprehensive and thorough grasp to time series, thus achieve even better accuracies. Finally, two representative architectures are presented specifically and their experiments on UCR datasets reveals the effectiveness and superiority of proposed LSTM-MFCN.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.energy.2021.122359,Journal,Energy,scopus,2022-01-15,sciencedirect,Fuzzy inference system application for oil-water flow patterns identification,https://api.elsevier.com/content/abstract/scopus_id/85117714992,"Prediction of oil-water two-phase flow pattern provides an effective solution for reducing oil production costs. In this research, the fuzzy inference system (FIS) is utilized to predict fluid flow patterns and establish a new adaptable prediction model. This paper takes No. 10 industrial white oil and tap water as the research objects to simulate fluids, and analyzes the changes of the pipeline angle, the total flow of oil-water two-phase flow and the convective pattern of water cut. A data set containing 60 samples was used to create the model, and the Mamdani fuzzy model was established using MATLAB software. The results show that compared with the BP neural network algorithm, the model set forth in the present paper has higher accuracy and reliability, and can achieve real-time monitoring and effectively reduce errors, especially in the case of decision-making. In addition, the fuzzy model is demonstrated that in the entire production logging process of non-vertical wells, the use of a fuzzy inference system to predict fluid flow patterns can greatly save production costs while ensuring the safe operation of production equipment.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2022.05.031,Conference Proceeding,Procedia CIRP,scopus,2022-01-01,sciencedirect,Explainable Predictive Quality Inspection using Deep Learning in Electronics Manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85132311241,"The linkage of machines in the context of Industry 4.0 through information and communication technology (ICT) to cyber-physical systems with the aim of monitoring, controlling, and optimizing complex production systems, enables real-time capable approaches for data acquisition, analysis, and process knowledge generation. In this context, surface mount technology (SMT) in electronics manufacturing is increasingly enhanced by digitalizing the process. This allows the collection and analysis of sensor data to predict the process quality in real-time. Process control interventions can then be derived in a timely manner based on quality predictions. To further support decision-making for process control by domain experts, explanations for the model-based quality predictions should be supplemented in addition. More specifically, we employ a 1D-convolutional neural network for quality prediction of well-defined Fields Of Views (FOVs) of Printed Circuit Boards (PCBs). Explanations for the model’s predictions are provided under various perspectives using a heat-mapping-based technique to highlight the contribution of both local and global PCBs’ characterizing features to the quality predictions. This helps to reveal the most decisive features for a given quality assignment and understand which process parts are the most responsible for such decision. Finally, the deployment of the model-based predictive and parts of the prescriptive analytics supported by the provided explanations, are achieved using Edge Cloud Computing technology.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2022.05.137,Conference Proceeding,Procedia CIRP,scopus,2022-01-01,sciencedirect,Assessing Energy Efficiency Measures for Hydraulic Systems using a Digital Twin,https://api.elsevier.com/content/abstract/scopus_id/85132303566,"As manufacturing companies around the world face the challenge of reducing CO2 emissions and achieving their climate goals, increasing energy efficiency provides a promising solution while potentially reducing costs. Hydraulic systems are used in a wide range of applications such as heating, ventilation, air conditioning or machine tools and account for approximately 11 % of the electric energy demand in the German industry in 2017. Furthermore, up to 25 million tons of CO2 are emitted annually in Germany as a result of their operation. Against this background, the following paper aims to increase the energy efficiency of hydraulic systems through automated assessment of energy efficiency measures during system operation. Therefore, we present a modular approach for real-time assessing of energy efficiency measures using a digital twin, which contains an expert system combined with real-time simulation models. To detect inefficiencies without time consuming analysis and substantial user expertise, the expert system automatically identifies system leakage and increased flow resistance using a multi-output regression model. Finally, the expert system aims at engaging operators to implement energy efficiency measures by quantifying their respective energy saving potentials. The proposed measures are applied to the virtual representation of a hydraulic system in real-time. Therefore, a Modelica simulation model is developed, which is exported as a functional mock-up unit (FMU) and integrated into a Python framework. If measures lead to an improvement in energy efficiency, these are recommended to the operator. The overall concept is validated using a physical hydraulic system within the ETA Research Factory. The validation of the prototype shows that the developed approach can be applied to industrial applications and help in reducing their energy consumption.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2022.05.142,Conference Proceeding,Procedia CIRP,scopus,2022-01-01,sciencedirect,Enabling deep learning using synthetic data: A case study for the automotive wiring harness manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85132291263,"The wiring harness manufacturing follows a high-variant manufacturing philosophy with a high degree of manual labor to produce customized products. Therefore, the process step of optical inspection is crucial to monitor, assess, and proactively safeguard the quality of produced wiring harnesses. To automate the optical inspection, which is currently manually conducted by operators, deep learning has become a powerful algorithm outperforming traditional computer vision approaches. Deep learning-based automated optical inspection systems can robustly and reliably detect rigid and deformable components, such as connectors, clips, single wires, and wire bundles. However, the bottleneck for scalable deep learning solutions in the industrial environment and high deep learning model performance is the database for model training and optimization. To address this research gap, we propose a deep learning-based data processing pipeline for automated optical inspection of wiring harnesses using real and synthetically generated point clouds. This paper outlines the process of real and synthetic data generation and evaluates the potential of synthetic data to enrich real data for model training. The data processing pipeline is implemented and experimental findings are generated to deduct important parameters for synthetic data generation and deep learning model training which impact model performance.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2022.05.124,Conference Proceeding,Procedia CIRP,scopus,2022-01-01,sciencedirect,A toolbox of agents for scheduling the paint shop in bicycle industry,https://api.elsevier.com/content/abstract/scopus_id/85132286258,"Modern manufacturing environments strive to support a great variety of products, driven by the large customization demand, while at the same time maintain low cost and fast response to the market. To cope with the challenging manufacturing requirements, organizations rely on agile decision-making systems to maintain a smooth production function. In the paint shop environment, where hanging positions are included on a circular conveyor belt for different type of items to be transferred through the painting cabins, the provision of a near optimal scheduling solution is a rather important and complex aspect due to the diversity in colors, sizes, deadlines and more. This research paper aims to address this issue with the development of a toolbox of autonomous agents that provide near optimum solutions in the paint shop scheduling problem, by optimizing the sequence and combination of items transferred through the paint-shop conveyor. The proposed approach considers sequence-based setup time dependencies and capacity constrains for the different item types, while aiming for maximum conveyor utilization and minimum production flowtime. The smart agents’ toolbox is empowered with both model-based and data-driven optimization methods allowing it to avoid long computational delays by exploiting the different benefits of each scheduling agent. The agent interface was implemented in Java, whereas Python was used for mathematical modeling, analysis, and optimization. The research validation was performed based on real industrial data provided by a bicycle industry where the agents were tested in scheduling the painting department. Results from the performance of the different optimization methodologies within the pilot case, were used to indicate the advantages and disadvantages of each method with respect to the problem’s characteristic.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2022.05.033,Conference Proceeding,Procedia CIRP,scopus,2022-01-01,sciencedirect,Ground Control: an Acquisition and Control System Architecture for LMD,https://api.elsevier.com/content/abstract/scopus_id/85132272629,"The additive manufacturing technology known as Laser Metal Deposition (LMD) is achieving widespread diffusion, thanks to its capacity of repairing or manufacturing entirely new, highly customized functional parts with complex geometries. For this reason, researchers and practitioners are continuously trying to improve the productivity and quality of this process, in particular by applying cutting-edge machine learning and control approaches. The implementation of these methodologies requires the availability of a data management infrastructure, capable of communicating with multiple, heterogeneous sources (sensors and machines), logging such process data acquired at high speed, and exposing such information to machine learning and control functionalities. This work describes the Ground Control framework, a monitoring and control system architecture designed and implemented on LMD machines, which supports simultaneous acquisition of process data at frequencies up to 500 Hz (workpiece temperature, melt pool images and temperature, in-process 3D scans, and machine data) and control of laser power. This information is organized in a customized database, used for experimental characterization and optimization of the process. The results and performances of the designed system are described by examples extracted from real deposition experiment",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2022.03.068,Conference Proceeding,Procedia Computer Science,scopus,2022-01-01,sciencedirect,The Security Concerns on Cyber-Physical Systems and Potential Risks Analysis Using Machine Learning,https://api.elsevier.com/content/abstract/scopus_id/85132207250,"The use of engineering to drive down costs and improve productivity has been an ongoing business exercise since the first Industrial Revolution. The term Cyber-Physical System is a wide range of different computing technologies embedded with the next-generation engineered systems into the physical world. Connected Cyber-Physical Systems (CPS) improve the lives of people and increase industry and manufacturing efficiency. It is affecting many branches of life such as transportation, healthcare and medicine, the environment, and energy. Industry 4.0 integrates humans, machines, and data to provide a holistic and interlinked approach to manufacturing, hence, increasing privacy concerns. For example, Autonomous Vehicles (AV) can be driven without a pilot and those systems can be hacked if there is a breach in the system. Nowadays, most of the systems are interconnected to the internet and nothing can be considered fully safe. Therefore, with this increase of security threats and privacy concerns, there is a need to assess and evaluate the trade-off between enhancements and improvements in manufacturing and the possible threats and security risks in the context of Cyber-Physical Systems. We need to bridge the gaps and overcome some of these limitations. In this work, we studied the security concerns emerging from interconnected Cyber-Physical systems, devices, and services in Industry 4.0. To identify security vulnerabilities, we have chosen the energy dataset because energy is the key point of every Cyber-Physical system so aimed to show the importance of energy, and the K-Means algorithm implemented which is an advanced Machine Learning and potential risks detected.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2022.03.008,Conference Proceeding,Procedia Computer Science,scopus,2022-01-01,sciencedirect,Predicting truck parking occupancy using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85132166620,"The logistics industry faces an increasing shortage of truck parking spots. This results in illegal parking or fatigued driving with hazardous consequences for traffic safety, as truck drivers have no insight into future availability of parking spots. Accurate short-term predictions of parking lot occupation are required to aid drivers in planning their routes and rest stops. To obtain such predictions, this research compares a variety of machine learning algorithms, concluding that decision trees are most suitable for real-time application. The model is trained on real-world data containing 1.5 years of truck parking measurements, obtained from a truck parking in Deventer, the Netherlands. We find that – contrasting to car parking, which is influenced by factors such as the weather – a model using only temporal features and historical occupancy yields the best results. For one-hour ahead predictions, we obtain an RMSE of 0.0029, with a training time of 4 seconds and predictions being sufficiently fast for real-time deployment. The main contributions of this research are (i) a machine learning approach for predicting truck parking occupation, (ii) insights into relevant predictive features, and (iii) a case study. From a practical perspective, we propose an architecture for a dynamic prediction tool, which can be used by truck drivers, parking managers and road authorities to improve truck parking utilization. Future research can build upon the machine learning approach and use the prediction model for other truck parking areas.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2022.03.033,Conference Proceeding,Procedia Computer Science,scopus,2022-01-01,sciencedirect,A proof of concept for providing traffic data by AI based computer vision as a basis for smarter industrial areas,https://api.elsevier.com/content/abstract/scopus_id/85132132654,"Algorithms and data serve as the foundation for any SMART approach, whether smart cities or smart urban environments. Availability of near real-time data for such smart applications in a wider geographic area is an expensive undertaking on both the technical and financial front. We introduce an approach to reduce technical and financial barriers to enable near real-time data collection. Additionally, our approach can be scaled with no significant technical depth. Our proposed approach was applied to meet the challenge of underutilized collaboration and data in a semi-closed harbor situated near Neuss and Düsseldorf, Germany. Specifically, we targeted the traffic problems causing adverse economic and environmental effects within the harbor area. By utilizing open-source projects and economically viable hardware, we offer a dashboard with information sourced from multiple data streams in an ad-hoc manner. We leverage crowd solving by providing ready-made solutions that each partner can acquire without technical expertise and in return provide near-real-time traffic data from which all parties can benefit. The work is part of the research project Logistics.NRW funded by Europäischer Fonds für regionale Entwicklung, Leitmarkt Mobilität & Logistik.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.entcom.2022.100496,Journal,Entertainment Computing,scopus,2022-01-01,sciencedirect,Particle swarm optimization for procedural content generation in an endless platform game,https://api.elsevier.com/content/abstract/scopus_id/85130317786,"Given the ever-increasing competition in the digital gaming industry, induced by a market of an exponentially growing gamer population, the production of creative, coherent, and appealing games has become inherently more complex. Creating game content by hand is both costly and time-consuming. By automating or assisting programmers and designers in their tasks, the techniques of procedural content generation (PCG) for games may address these challenges. PCG is not new, being active for several decades. However, the more traditional (and popular) form of PCG is somehow limited. It relies on some basic techniques ranging from simple pseudo-random number generators, generative grammars, image filtering, and spatial algorithms. The most advanced forms of PCG may utilize the modeling and simulation of complex systems and techniques from Artificial Intelligence (AI). In this context, this paper proposes a novel PCG approach, which is based on an optimization algorithm, known as Particle Swarm Optimization (PSO). Our approach is tested on a 2D endless platform runner game. The game structure (based on the Godot Game Engine) and a previous solution using Genetic Algorithms (GA) were first proposed and evaluated in an earlier work. In this paper, besides proposing the novel solution using PSO, we performed a comparative evaluation between the novel and previous solutions. The fitness function, utilized by both algorithms, takes into account the game’s environment aesthetics, physics, and some rules associated with gameplay, so that the generated environments are both enjoyable and playable. Our experiments evaluated time viability for in-game real-time generation and convergence to high/stable fitness values. We discovered hyper parameter ranges that yielded viable solutions. In the end, PSO has proven to be better suited to the investigated PCG task than the GA, since it presented faster convergence time and higher fitness function values.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-823978-0.00003-4,Book,Applications of Computational Intelligence in Multi-Disciplinary Research,scopus,2022-01-01,sciencedirect,IoT in healthcare ecosystem,https://api.elsevier.com/content/abstract/scopus_id/85130106885,"In recent years, IoT has been revolutionizing the technology landscape, leading to explosive growth in fields like automated manufacturing, asset management, and wearable consumer healthcare products. IoT’s presence can be seen in all domains. Ever since IoT gained its entry into the medical field, there has been a magnificent transformation in the healthcare domain. Personal healthcare is now made a reality with IoT offering solutions in several dimensions like remote healthcare; smart clothing such as smartwatches, smart bands, and smart pants; and telemedicine like smart pills and personal care robots. This chapter gives a comprehensive walkthrough of IoT in the healthcare ecosystem, addressing the different applications of IoT in healthcare, the architecture models, challenges faced by IoT in healthcare, security practices and issues, and the future of IoT in the domain. In the first section, IoT applications in healthcare are discussed, which include patient-centric applications like remote health monitoring and critical care monitoring; and hospital-centric IoT applications such as the deployment of the staff, reducing charting times, and real-time location of medical equipment, subsequently followed by a discussion on how the data collected from the patient-centric and hospital-centric applications contribute to the ease of other domains like health insurance; and then, IoT’s support toward the pharmaceutical industry to restrict counterfeit medicine is discussed. Secondly, the implementation designs of healthcare IoT are discussed. Apart from the traditional cloud services, new offerings like fog and edge computing have seen a spike in recent years. Fog and edge computing are considered intelligent and flexible architectures. The subsequent section deals with architecture designs and the advantages and challenges of the two computing models. The next section demonstrates the actual implementation methodologies of the two applications in the following domains in detail: (1) Heart disease prediction and (2) healthcare IoT-based affective state mining using deep convolutional neural networks. The following section discusses the challenges faced by IoT in the healthcare domain. In general, the challenges can be categorized into technological challenges; people-oriented challenges like the acceptance of IoT in the healthcare domain; and finally, security bottlenecks. The data generated and maintained by the IoT platforms serves as a gold mine for different healthcare professionals for future research and development in the medical field and the health insurance providers and pharmaceutical industries for their benefits. Hence, more emphasis is given to the security and privacy aspects of how the domain handles sensitive data of the patients. The next section provides insights into the security issues along with the cyber threats and attacks faced by healthcare IoT and the defensive mechanisms. Furthermore, this chapter deals with the IoT’s role in combatting the novel coronavirus that has caused an unprecedented global pandemic. Finally, the future of IoT is talked about. With the advent of 5G and an upsurge in artificial intelligence, the different dimensions of the IoT that are expected to see an outburst of growth are discussed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-323-85214-2.00004-5,Book,Deep Learning for Sustainable Agriculture,scopus,2022-01-01,sciencedirect,Automated real-time forecasting of agriculture using chlorophyll content and its impact on climate change,https://api.elsevier.com/content/abstract/scopus_id/85129903499,"The core theme of this project is to assess the economic impact of climate change on Indian agriculture. Climate change is caused due to the emission of greenhouse gases like carbon dioxide (CO2), methane (CH4), and nitrous oxide from various industrial sources. Neyveli, being the source of heavy megawatt-generating stations, let out flue gases, which contain CO2, carbon monoxide, oxides of sulfur, CH4, and oxides of nitrogen. These harmful gases are responsible for depletion of the ozone layer, which has a significant effect on variation in weather and agricultural output and sometimes even produces acid rainfall. Considering the probable effects of climatic change on agriculture has motivated a vital change in the yield of agricultural products, livestock yields and also changes in the food production pattern and prices. This estimation of chlorophyll content can be done by extracting green colored pixels from the satellite images or images captured by the vision sensors and soil moisture sensor placed in the Indian agricultural area. These images are preprocessed for noise removal using edge detection technique. From the preprocessed images, feature descriptors like histogram of oriented gradients (HoG) are extracted. The HoG values are fused with the information gathered from soil moisture sensor. The extracted features are reduced using principal component analysis (PCA). The feature set is thereafter used as inputs to artificial neural networks using feed-forward structure trained with backpropagation algorithm (BPA). These estimates done using data analytics will lend a helping hand to the farmers to adapt themselves to the year within annual weather shocks. It can be inferred that the estimates, derived from short term, are capable of predicting the short- and medium-term impacts of climate change, which would direct the farmers to adapt rapidly to the changing climatic conditions. These short- and medium-term impacts of climate change are found to reduce the agricultural productivity by 4%–6% and 6%–9%, respectively. Hence it is inferred that the climate change entails significant impact on the revenue of the Indian economy until and unless the farmers can promptly identify and adjust to decreasing rainfalls and increasing atmospheric temperatures. The first challenge lies in analyzing the satellite images of the farmlands using efficient image processing algorithms to extract useful and meaningful information. This data extracted would be of a very large quantity and needs to be handled using some data analytics algorithm like BPA, whose prediction efficiency will be determined and also validated. The second challenge lies in mapping the emission of greenhouse gases with the images of the farmlands under three categories, namely, highly productive farmlands, medium productive farmlands, and less productive farmlands and correlating the yield of farmlands with respect to emission levels of greenhouse gases in particular environment under study.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.fmre.2022.03.025,Journal,Fundamental Research,scopus,2022-01-01,sciencedirect,Artificial Intelligence Reinforced Upconversion Nanoparticle-based Lateral Flow Assay via Transfer Learning,https://api.elsevier.com/content/abstract/scopus_id/85129516599,"The combination of upconverting nanoparticles (UCNPs) and immunochromatography has become a widely used and promising new detection technique for point-of-care testing (POCT). However, their low luminescence efficiency, non-specific adsorption, and image noise have always limited their progress toward practical applications. Recently, artificial intelligence (AI) has demonstrated powerful representational learning and generalization capabilities in computer vision. We report for the first time a combination of AI and upconversion nanoparticle-based lateral flow assays (UCNP-LFAs) for the quantitative detection of commercial internet of things (IoT) devices. This universal UCNPs quantitative detection strategy combines high accuracy, sensitivity, and applicability in the field detection environment. By using transfer learning to train AI models in a small self-built database, we not only significantly improved the accuracy and robustness of quantitative detection, but also efficiently solved the actual problems of data scarcity and low computing power of POCT equipment. Then, the trained AI model was deployed in IoT devices, whereby the detection process does not require detailed data preprocessing to achieve real-time inference of quantitative results. We validated the quantitative detection of two detectors using eight transfer learning models on a small dataset. The AI quickly provided ultra-high accuracy prediction results (some models could reach 100% accuracy) even when strong noise was added. Simultaneously, the high flexibility of this strategy promises to be a general quantitative detection method for optical biosensors. We believe that this strategy and device have a scientific significance in revolutionizing the existing POCT technology landscape and providing excellent commercial value in the in vitro diagnostics (IVD) industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmsy.2022.04.011,Journal,Journal of Manufacturing Systems,scopus,2022-01-01,sciencedirect,Machine learning classification of surface fracture in ultra-precision diamond turning using CSI intensity map images,https://api.elsevier.com/content/abstract/scopus_id/85129481921,"With increasing focus on automation of machining processes in a smart factory, attention has turned to implementation of machine learning (ML) not only for tasks such as process parameter optimization, tool wear monitoring, and surface roughness prediction, but also for near-real-time determination of part quality using image data for inline inspection. As a first step, this work compares the performance of ML approaches using post-process image data, as opposed to numerical machining process data, for the discrete prediction of surface fracture in single-point diamond turned germanium for IR optics. Typical classifiers such as Random Forest and Support Vector Machine are employed for processing numerical machining parameters, while feedforward neural networks (FNNs), convolutional neural networks (CNNs), and deep CNNs (DCNNs) are utilized for processing intensity map images of the turned surfaces. The feasibility of using FNNs, CNNs, and DCNNs for identifying fracture in intensity maps is initially explored on raw images, and then by employing edge segmentation and edge detection techniques such as the Sobel, Prewitt, and Laplacian operators, as well as denoising methods as the Adaptive Gaussian Threshold filter. Finally, hybrid CNN and hybrid FNN models that combine numerical process data and image data are explored.
                  Keywords",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2022.05.017,Conference Proceeding,Procedia CIRP,scopus,2022-01-01,sciencedirect,Insights and Example Use Cases on Industrial Transfer Learning,https://api.elsevier.com/content/abstract/scopus_id/85128695950,"Despite the high solution potential of machine learning for common problems in automation technology, there are only few examples of its application in real-world manufacturing practice. In order to find the reason for this phenomenon, the authors identify the hurdles for conventional machine learning using four exemplary use cases namely self-learning robots, wear prediction, visual object detection, and predictive quality in manufacturing. While these use-cases differ in principle, the problems engineers face when using conventional machine learning approaches to solve them are related, such as the lack of manifold training data or high dynamics of industrial processes. The authors showcase that utilizing deep transfer learning and continual learning approaches in the industrial context – subsumed under the term industrial transfer learning – can overcome these hurdles. Even for industrial transfer learning, there is a deficiency regarding preconditions for the large-scale deployment of such approaches, but unlike in conventional machine learning, it is principally possible to establish those. The article concludes with a discussion of these prerequisites and makes suggestions as to how they could be fulfilled.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-821750-4.00002-5,Book,"Endorobotics: Design, R and D and Future Trends",scopus,2022-01-01,sciencedirect,Artificial intelligence for medical robotics,https://api.elsevier.com/content/abstract/scopus_id/85128569934,"The shift in surgery toward minimally invasive approaches requires transitioning from an analog world to a digitally transformed system and presents a huge opportunity in this emerging field. Artificial intelligence (AI) in healthcare has the potential to transform the role of doctors and revolutionize the practice of medicine. The convergence between AI and medical robotic technologies creates an interesting area for research and development activities for the medical technology industry.
               Despite rapid improvements in robotic-assisted surgery over the past decade, the level of adoption remains low due to high costs, which is cited as a major challenge. This chapter outlines the current trends and perspectives of AI in medical robotics, with a rapid review of AI-supported robotics in allied health, radiology, rehabilitation medicine, with a specific focus on surgical applications.
               The chapter looks at how AI is being used in medical robotics for teaching and training, through to surgical planning and robotic-assisted surgery, using case studies.
               Some challenges with the use of AI in medicine include the issue of legal liability and attribution of negligence when errors occur. The chapter provides insights on ethical and legal issues of AI in medical robotics, with a discussion around implementation and adoption of this new technology.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2022.01.388,Conference Proceeding,Procedia Computer Science,scopus,2022-01-01,sciencedirect,Real-time Detection of Worker's Emotions for Advanced Human-Robot Interaction during Collaborative Tasks in Smart Factories,https://api.elsevier.com/content/abstract/scopus_id/85127826978,"Human-robot collaboration (HRC) has become increasingly popular in modern assembly systems because of the flexibility of human capabilities and the precision and efficiency of the fellow robot. However, previous research has identified challenges to achieve a genuine and natural human-robot interaction, one being the real-time robot behavior adaptation depending on the worker’s emotions revealed by facial or body signals. Human emotional state recognition has been widely explored in the fields of human–machine interaction and affective computing, but a practical implementation of the technology in real-time during a collaborative task hides complexities and challenges. In this paper, the authors tested and compared twelve different models, all based on Deep Learning and Convolutional Neural Networks (CNN), to recognize emotions using the datasets CK+ and Fer2013. DeepFace algorithm resulted to be the most accurate and was further tested on real subjects in working and industry-like contexts to determine the actual validity and necessary modifications for a possible large-scale industrial application. A discussion about all the main challenges to face for a practical application of this technology on field is presented.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2022.01.318,Conference Proceeding,Procedia Computer Science,scopus,2022-01-01,sciencedirect,"Predictive maintenance on sensorized stamping presses by time series segmentation, anomaly detection, and classification algorithms",https://api.elsevier.com/content/abstract/scopus_id/85127782163,"Sheet metal forming tools, like stamping presses, play an ubiquitous role in the manufacture of several products. With increasing requirements of quality and efficiency, ensuring maximum uptime of these tools is fundamental to marketplace competitiveness. Using anomaly detection and predictive maintenance techniques, it is possible to develop lower risk and more intelligent approaches to maintenance scheduling, however, industrial implementations of these methods remain scarce due to the difficulties of obtaining acceptable results in real-world scenarios, making applications of such techniques in stamping processes seldom found. In this work, we propose a combination of two distinct approaches: (a) time segmentation together with feature dimension reduction and anomaly detection; and (b) machine learning classification algorithms, for effective downtime prediction. The approach (a)+(b) allows for an improvement rate up to 22.971% of the macro F1-score, when compared to sole approach (b). A ROC AUC index of 96% is attained by using Randomized Decision Trees, being the best classifier of twelve tested. An use case with a decentralized predictive maintenance architecture for the downtime forecasting of a stamping press, which is a critical machine in the manufacturing facilities of Bosch Thermo Technology, is discussed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2022.01.249,Conference Proceeding,Procedia Computer Science,scopus,2022-01-01,sciencedirect,Data Conceptual Model for Smart Poultry Farm Management System,https://api.elsevier.com/content/abstract/scopus_id/85127766373,"Internet of things provides ways to obtain information about farm production; however, the more data there is, the harder it is to determine the correct way to process these data. The solutions on the market are aimed specifically at gathering data, rather than processing and analyzing them. Poultry farming is one of the fields, where the application of Industry 4.0 principles is becoming a necessity, especially in the case of greenhouse gas control. Minimum standards and rules in the poultry management are regulated by EU Directives and required parameters must be ensured at all times, therefore, real-time environmental monitoring must be performed. The only practically viable approach is to automate data gathering using appropriate sensors, which is only possible if a standardized data structure model is defined.
                  In this research the Cyber-Physical Model is the proposed as a basis to development of smart poultry farm management system that is adjustable to particular production needs. Three main data groups are defined – necessary data to ensure requirements of EU Directives, farm monitoring data for business analysis and optimization, and additional data that can influence overall poultry farm management. The proposed data set was implemented into the existing infrastructure of a Baltic poultry farm. Multiple CO2 (carbon dioxide) and NH3 (ammonia) sensors were installed in order to gather data, measurements of which previously were taken manually by the farm’s staff. The solution was developed on centralized cloud-based data processing system, where MQTT Broker was used for security measures. The processed data is used by the decision support system with the aim to define optimal feeding and housing.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2022.01.300,Conference Proceeding,Procedia Computer Science,scopus,2022-01-01,sciencedirect,An I4.0 data intensive platform suitable for the deployment of machine learning models: A predictive maintenance service case study,https://api.elsevier.com/content/abstract/scopus_id/85127760892,"The Artificial Intelligence is one of the key enablers of the Industry 4.0. The building of learning models as well as their deployment in environments where the rate of data generation is high and their analysis must meet real time requirements lead to the need of selecting a big data platform suitable for this purpose. The heterogeneous and distributed nature of I4.0 environments where data becomes highly relevant requires the use of a data centric, distributed and scalable platform where the different applications are deployed as services. In this paper we present an I4.0 digital platform based on RAI4.0 reference architecture on which a predictive maintenance service has been built and deployed in Amazon Web Service cloud. Different strategies to build the predictor are described as well as the stages carried out for its construction. Finally, the predictor built with k-nearest algorithm is chosen because it is the fastest in producing an answer and its accuracy of 99.87% is quite close to the best model for our case study.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2022.01.242,Conference Proceeding,Procedia Computer Science,scopus,2022-01-01,sciencedirect,An IoT architecture based on the control of Bio Inspired manufacturing system for the detection of anomalies with vibration sensors,https://api.elsevier.com/content/abstract/scopus_id/85127750673,"This work presents an IoT architecture for the detection of anomalies in motors with vibration sensors using a real-time autoen-coder, based on a new bio-inspired control architecture for recently proposed manufacturing systems. Unlike other approaches, this work analyzes the behavior of the anomaly detection system in real time, seeking to cover the new requirements for real-time processing and scalability in control systems. A neural network is implemented to control anomalies based on a bio-inspired architecture, achieving the detection of anomalies in the time domain based on the evaluation of different models based on recurrent neural networks. Similarly, an evaluation is shown regarding the latency of each component of the system, thus finding possible bottlenecks in real-time operation. The system was implemented on a prototype conveyor belt with low-cost accelerometers, commercial-use microcontrollers, and free software.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2022.01.362,Conference Proceeding,Procedia Computer Science,scopus,2022-01-01,sciencedirect,Revolution of Retail Industry: From Perspective of Retail 1.0 to 4.0,https://api.elsevier.com/content/abstract/scopus_id/85127748373,"When Industry 4.0 was first introduced in 2010, it also brought the retail industry into the fourth revolution. Retail 4.0, on the other hand, appears to be a novel concept for retailers worldwide. When Industry 4.0 technologies such as the Artificial Intelligent (AI), Internet of Things (IoT), Cloud Computing, Big Data Analytical (BDA), and Augmented Reality (AR) were implemented in the retail industry, the term Retail 4.0 arose from Industry 4.0. This paper examines Retail 4.0 technologies and their application in the retail industry. The retail industry’s revolution is also discussed in this paper. The final section examines the extent of implementation of retail 4.0 technology in various nations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/bs.adcom.2022.02.001,Book Series,Advances in Computers,scopus,2022-01-01,sciencedirect,Exploring the edge AI space: The industry use cases,https://api.elsevier.com/content/abstract/scopus_id/85127675740,"Now, edge devices, through a plethora of technological innovations and disruptions, are being stuffed with more memory, storage and networking capacities and processing capability. Thereby, edge devices are joining in mainstream computing. That is, the centralized and consolidated computing moves over to edge devices to be decentralized and disaggregated. AI libraries are being deployed in IoT edge devices to do proximate and real-time data processing. This is termed as on-device intelligence. Such a strategically sound shift is to bring forth a dazzling array of advancements and automations not only for business houses but also for common people in their everyday assignments. This chapter is to throw some light on the theoretical and the practical aspect of the edge AI paradigm.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2022.02.172,Conference Proceeding,Procedia CIRP,scopus,2022-01-01,sciencedirect,Open-Digital-Industrial and Networking pilot lines using modular components for scalable production - ODIN project approach,https://api.elsevier.com/content/abstract/scopus_id/85127520324,"While robots have very well proven their flexibility and efficiency in mass production and are recognized as the production resource of the future, their adoption in lower volume, diverse environment is heavily constrained. The main reason for this is the high integration and deployment complexity that overshadows the performance benefits of this technology. This paper presents the vision of ODIN European funded project which is to strengthen the EU production companies’ trust in utilizing advanced robotics, by demonstrating that novel robot-based production systems are not only technically feasible, but also efficient and sustainable for immediate introduction at the shopfloor. To achieve that, ODIN brings together, by means of hardware and software, the latest technological advancements in the fields of a) collaborating robots and human robot collaborative workplaces, b) autonomous robotics and AI based task planning, c) mobile robots and reconfigurable tooling, d) Digital Twins and Virtual Commissioning and e) Service Oriented Robotics Integration and Communication Architectures. ODIN will provide a systematic approach for integrating these technologies under modular and reconfigurable large-scale robotic pilots. The performance of these robotic pilots will be tested and validated in three case studies, from the automotive, the white goods and the aeronautics industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2022.02.192,Conference Proceeding,Procedia CIRP,scopus,2022-01-01,sciencedirect,An IIoT approach for edge intelligence in production environments using machine learning and knowledge graphs,https://api.elsevier.com/content/abstract/scopus_id/85127513501,"The mining industries need novel solutions to reduce production stoppages. Predictive maintenance solutions and especially the hardware components, cannot operate properly under such harsh conditions, as high concentration of dust and other chemical material may lead into fault sensor measurements. This study presents a solution for condition monitoring and predictive maintenance and productions status monitoring for assets operating in harsh operating environments. First, an edge device is connected to multiple sensors monitoring critical parameters related to the operating conditions of an asset. In particular, the device is in charge of data collection, filtering and smart data generation for further analysis and processing. At a later stage, the collected data are pushed to a cloud platform where predictive analytics, as well as production status analytics, are estimated. The condition monitoring and predictive maintenance component utilizes machine learning in order to estimate the Remaining Useful Life of the monitored asset(s). The production status component utilizes knowledge graphs that are populated with data provided by the edge device. The combination of these two components aims to provide meaningful insight to field personnel supporting them in decision making and production supervision. A prototype IIoT system has been implemented and tested in a use case related to an aluminium producing company with its results demonstrating the applicability of the proposed solutions for real-world application.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2022.02.162,Conference Proceeding,Procedia CIRP,scopus,2022-01-01,sciencedirect,Operator training framework for hybrid environments: An Augmented Reality module using machine learning object recognition,https://api.elsevier.com/content/abstract/scopus_id/85127505635,"As market demands are characterized by more customized products with shorter lifecycles, it is obligatory for modern operators to manage recurrent product or manufacturing system changes. In contrary to previous years, adaptation to such changes prerequires memorization of more information, and familiarization with more complex systems and resources in a shorter period of time. This manuscript presents a novel operator training framework based on Augmented Reality (AR) technology. More specifically, intuitive instructions enhanced with machine learning-based physical object detection are used for making steeper learning curves and providing hands-on experience to operators. The implemented application also supports a walkthrough mode where users can get familiarized with Information and Communication Technologies (ICT) data streams besides fenceless Human-Robot coexistence in collaborative schemes. An automotive case study is used for evaluating the performance of the training framework through a Human-Robot Collaboration (HRC) assembly scenario.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2022.02.171,Conference Proceeding,Procedia CIRP,scopus,2022-01-01,sciencedirect,AI-based vision system for collision detection in HRC applications,https://api.elsevier.com/content/abstract/scopus_id/85127499215,"Human-Robot Collaboration (HRC) enabling mechanisms require real-time detection of potential collisions among human and robots. Taking under consideration the already existing standards and the literature, most of collision detection techniques require the integration of sensorial systems on the robot aiming to identify the contact events. This paper deals with a novel approach for the identification of human and robot collision based on vision systems. Moreover, Artificial Intelligent (AI) algorithms are required to classify the captured data near real-time and to provide a score about the collision status (contact or non-contact) between a human and the robot. Accordingly, the AI models should be trained using the appropriate image data enabling an accurate classification. The proposed system has been developed in a lab environment. A detailed presentation of the system implementation, its performance and the potential integration in a real industrial environment are discussed in this paper.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2022.02.170,Conference Proceeding,Procedia CIRP,scopus,2022-01-01,sciencedirect,Manufacturing improvement capabilities of machine learning algorithms: Evaluation using an electrode-separator compound handling demonstrator,https://api.elsevier.com/content/abstract/scopus_id/85127459032,"Machine learning based analyses of the production data promise direct improvements of manufacturing conditions and offer a way to justify involved investment costs for the data provision. In the current literature, explanatory and predictive capabilities of machine learning algorithms are described. The explanatory capabilities focus on root cause and structural analyses, while the predictive capabilities concentrate on real-time predictions. The aim of this contribution is to evaluate the different capabilities of data analyses using a handling device responsible for positioning and transporting electrodes for the lithium-ion cell assembly. The evaluation shows that the root cause analyses offer tools to improve especially simple use cases. The real-time prediction promises a reduction of production costs by classifying the final position of the electrodes and enables an early outlier detection. Additionally, an approach is presented to detect relevant production parameters of the resulting quality of produced cells.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.isatra.2022.01.014,Journal,ISA Transactions,scopus,2022-01-01,sciencedirect,"Intelligent framework for automated failure prediction, detection, and classification of mission critical autonomous flights",https://api.elsevier.com/content/abstract/scopus_id/85123893673,"Autonomous flights are the major industry contributors towards next-generation developments in pervasive and ubiquitous computing. Modern aerial vehicles are designed to receive actuator commands from the primary autopilot software as input to regulate their servos for adjusting control surfaces. Due to real-time interaction with the actual physical environment, there exists a high risk of control surface failures for engine, rudder, elevators, and ailerons etc. If not anticipated and then timely controlled, failures occurring during the flight can have severe and cataclysmic consequences, which may result in mid-air collision or ultimate crash. Humongous amount of sensory data being generated throughout mission-critical flights, makes it an ideal candidate for applying advanced data-driven machine learning techniques to identify intelligent insights related to failures for instant recovery from emergencies. In this paper, we present a novel framework based on machine learning techniques for failure prediction, detection, and classification for autonomous aerial vehicles. The proposed framework utilizes long short-term memory recurrent neural network architecture to analyze time series data and has been applied at the AirLab Failure and Anomaly flight dataset, which is a comprehensive publicly available dataset of various fault types in fixed-wing autonomous aerial vehicles’ control surfaces. The proposed framework is able to predict failure with an average accuracy of 93% and the average time-to-predict a failure is 19 s before the actual occurrence of the failure, which is 10 s better than current state-of-the-art. Failure detection accuracy is 100% and average detection time is 0.74 s after happening of failure, which is 1.28 s better than current state-of-the-art. Failure classification accuracy of proposed framework is 100%. The performance analysis shows the strength of the proposed methodology to be used as a real-time failure prediction and a pseudo-real-time failure detection along with a failure classification framework for eventual deployment with actual mission-critical autonomous flights.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmsy.2022.01.010,Journal,Journal of Manufacturing Systems,scopus,2022-01-01,sciencedirect,"Towards edge computing in intelligent manufacturing: Past, present and future",https://api.elsevier.com/content/abstract/scopus_id/85123859503,"Industry 4.0 (I4.0) is the fourth industrial revolution and a synonym for intelligent manufacturing. It drives the convergence of several cutting-edge technologies to provoke autonomous, fully integrated, collaborated, highly automated, and customized industries. Edge Computing (EC), a highly distributed framework, emerged a couple of years ago and embraced the industry to leverage the benefit of low latency and near real-time performance. It brings computation and storage in the close proximity of end devices and reduces the cloud overhead. In addition to improved operational efficiency, storage, and latency, EC further reduces the cost, improves productivity with higher quality maintenance and customer satisfaction. At the digital-to-digital stage of the Physical-Digital-Physical (PDP) loop, adapting EC can furnish tremendous benefits and further accelerate the next stages of the loop. This survey identifies the past and present works oriented towards Intelligent Manufacturing integrated with the EC platform and categorizes the research based on architecture, intelligence platform, edge objectives, and application. Herein, the authors have incorporated; (1) The progress in I4.0 following the PDP loop; (2) The discussion on EC in I4.0 and their Research Trend; (3) Methods to bring intelligence to the edge. To the best of our knowledge, it is the first review article that focuses on the applications and objectives of EC in Intelligent Manufacturing. It also outlines the optimum solutions to bring intelligence to the edge by overcoming the resource and complexity-bound with accuracy and latency constraints for the decision-making processes. Future directions include the less explored research areas, challenges in edge deployment in industries, and the integration of trending technologies such as Blockchain, Software Defined Networking, and 5 G with EC to excite the EC researchers. A few collaborative edge scenarios are discussed for the promotion and application of EC in I4.0. Nevertheless, efficient edge deployments face many challenges since studies are still limited to conceptual levels or design steps, and future orientation to application strategies for Smart Manufacturing is required.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2021.107824,Journal,Computers and Industrial Engineering,scopus,2022-01-01,sciencedirect,New perspectives and results for Smart Operators in industry 4.0: A human-centered approach,https://api.elsevier.com/content/abstract/scopus_id/85122422552,"Digital solutions (including Extended Reality as well as web, mobile and AI technologies), among others, are entering a broad variety of industries and modifying the capabilities of operators through (close to) real-time display of context-dependent information. However, little is known with respect to two major issues: (i) how these technologies can be seamlessly integrated for product/process and overall manufacturing system management providing of syntactic, semantic and functional interoperability and reusability among the different manufacturing systems departments; (ii) how they can be conveyed to operators also taking into account the cognitive load that is incurred by the operators.
                  To this end, starting from a previous article (Longo et al., 2017), this article designs and proposes the KNOW4I approach and its practical implementation in an ICT platform (the KNOW4I platform) to further empower the Smart Operators concept.
                  Two major objectives are pursued. The former is to set a standard referred to as the KNOW4I methodological approach for knowledge representation, knowledge management and digital contents management within the Smart Operator domain. The latter is the implementation of the aforementioned approach as part of the KNOW4I platform that includes a suite of Smart Utilities and Objects intelligently and interactively linked with a newly released version of the Sophos-MS digital and intelligent Assistant. Experimentations and results based on multiple KPIs are carried out to account for the effectiveness of the proposed framework.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.softx.2021.100956,Journal,SoftwareX,scopus,2022-01-01,sciencedirect,tx2_fcnn_node: An open-source ROS compatible tool for monocular depth reconstruction,https://api.elsevier.com/content/abstract/scopus_id/85121968187,"We present tx2_fcnn_node – a Robot Operating System (ROS) compatible tool that is aimed at seamless integration of various monocular depth reconstruction neural networks to the robotic software based on ROS (which is a de-facto standard in the area of robotics). Our tool simplifies the process of deploying, evaluating, and comparing depth reconstruction neural networks both on real robots and in simulation. We complement our software with a set of the precompiled neural networks which can be used off the shelf, with some of them being able to demonstrate near real-time performance when running onboard compact embedded platforms, e.g. Nvidia Jetson TX2, that are often used nowadays both in academia and industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.suscom.2021.100650,Journal,Sustainable Computing: Informatics and Systems,scopus,2022-01-01,sciencedirect,A conceptual framework for the implementation of Industry 4.0 in legal informatics,https://api.elsevier.com/content/abstract/scopus_id/85121918847,"The growing number of applications of Industry 4.0 in the field of legal informatics offers huge opportunities for data scientists and academic researchers. The term Industry 4.0 is defined as “the fourth industrial revolution that connects embedded systems to Cyber-Physical-Systems”. It emphasizes the end-to-end digitalization of all physical resources and integrating digital environments with the value chain organizations. Industry 4.0 comprises a variety of technologies such as Cyber-Physical-Systems, Big Data, Internet of Things, Artificial Intelligence, Cloud Computing and Cybersecurity. It has been found that the implementation of these technologies may be useful in achieving the objective of legal informatics. It can help lawmakers to align jurisprudence by providing modern computational technologies to improve and advance the traditional legal justice system. Further, the integration of legal informatics with Industry 4.0 will be strengthening the legal justice system by providing decision-making support, data transparency, real-time monitoring, cost-effective solution, and triple-bottom-line performance in the future. Therefore, the article aims to determine the implementation patterns of Industry 4.0 technologies in legislative institutions and administrations. The study proposes a conceptual framework that integrates Industry 4.0 with legal informatics. The findings show that implementing Industry 4.0 technologies such as Artificial Intelligence, Big Data, and Cloud Computing plays a vital role for legal firms that are currently in the nascent stage of development.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compag.2021.106635,Journal,Computers and Electronics in Agriculture,scopus,2022-01-01,sciencedirect,Intelligent IoT-multiagent precision irrigation approach for improving water use efficiency in irrigation systems at farm and district scales,https://api.elsevier.com/content/abstract/scopus_id/85121511874,"The fourth industrial revolution in agriculture seeks the automation of traditional practices, using modern smart technologies. Advances in electronics, computation and the internet of things are integrated for improving field inputs management. The aim of this paper is to present the design and implementation of an intelligent IoT-multiagent precision irrigation approach for improving water use efficiency in irrigation systems. The study site was the large-scale irrigation and drainage district of Chicamocha and Firavitoba (Usochicamocha) located in Boyacá - Colombia, where water is distributed from the Chicamocha riverbed. In the proposed system, irrigation is supervised and controlled in each field by an intelligent irrigation agent that autonomously prescribes and applies water amounts with agronomical criteria. The methodology was applied with real (cyber-physical) and virtual (simulated) intelligent agents and was extended to eleven pump stations that supply water to 5911 fields. Using a MQTT protocol, hundreds of irrigation intelligent agents report water prescriptions and crop characteristics to a master agent in each pump station, who creates a regional irrigation map to manage georeferenced field information and performs negotiation of water resources between agents according to supply availability. Field maps and intelligent irrigation agents can be visualized using devices with internet access. Results demonstrated that irrigation amounts were correctly applied on the fields, thus improving the water use efficiency. This technology is a novel support to decision-making in water resources management applications at field and district scales.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ergon.2021.103234,Journal,International Journal of Industrial Ergonomics,scopus,2022-01-01,sciencedirect,Industrial intelligence in the care of workers’ mental health: A review of status and challenges,https://api.elsevier.com/content/abstract/scopus_id/85120173556,"Mental health is a current concern because people worldwide have been committed to disorders that impair lives as a whole, affecting emotional states, behaviors, and body responses. These disorders decrease worker's productivity, impact industries economically, and cause serious psycho-physical conditions. However, technological advances have leveraged the industry to a novel phase where digitalization and automation provide a new reality. Hence, this industrial transformation may contribute to assists human beings in the workplace with a focus on mental health. This article presents a systematic literature review to investigate studies regarding technologies employed in the care of worker's mental health and the industrial role in this scenario. Three general, three focused, and three descriptive questions highlight the academic progress of industrial concern on mental health, implemented systems and cases, and research challenges. As a result, the review discussed 31 studies, extracted from an initial corpus of 25269, ranging from January 2010 to November 2020. The studies approached stress as the most frequent mental issue in the industry and Support Vector Machine (SVM) as the most used machine learning algorithm, where biomarkers presented the primary data extractors to deal with this theme. Moreover, information fusion methods improved the accuracy of specific cases. However, a growing interest in mental health care has emerged only in recent years, and several challenges require efforts before applying systems in real industrial environments.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.enconman.2021.115030,Journal,Energy Conversion and Management,scopus,2022-01-01,sciencedirect,Deep reinforcement learning based energy management strategy of fuel cell hybrid railway vehicles considering fuel cell aging,https://api.elsevier.com/content/abstract/scopus_id/85119905389,"In the rail transportation industry, growing energy and environmental awareness requires the use of alternatives to combustion engines. These include hybrid electrically driven railway vehicles powered by fuel cells and batteries. The cost of hydrogen consumption and the lifetime of fuel cells are currently the main challenges that need to be addressed before widespread deployment of fuel cell railway vehicles can be realized. With this in mind, this work focuses on the energy management system with emphasis on optimizing the energy distribution to reduce the overall operational cost. The presented energy management strategy (EMS) aims at minimizing hydrogen consumption and fuel cell aging costs while achieving a favorable balance between battery charging and discharging. In order to take fuel cell aging into account in energy management and mitigate fuel cell aging trough power distribution, an online fuel cell aging estimation model based on four operation modes is introduced and applied. Moreover, the advanced deep reinforcement learning method Twin Delayed Deep Deterministic Policy Gradient (TD3) is used to obtain a promising EMS. To improve the adaptability of the strategy, a stochastic training environment, which is based on real measured speed profiles considering passenger numbers is used for training. Assuming different environmental and passenger transport volumes, the results confirm that the proposed TD3-EMS achieves battery charge-sustaining at low hydrogen consumption while slowing down fuel cell degradation.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cose.2021.102500,Journal,Computers and Security,scopus,2022-01-01,sciencedirect,AntiViruses under the microscope: A hands-on perspective,https://api.elsevier.com/content/abstract/scopus_id/85118529412,"AntiViruses (AVs) are the main defense line against attacks for most users and much research has been done about them, especially proposing new detection procedures that work in academic prototypes. However, as most current and commercial AVs are closed-source solutions, in practice, little is known about their real internals: information such as what is a typical AV database size, the detection methods effectively used in each operation mode, and how often on average the AVs are updated are still unknown. This prevents research work from meeting the industrial practices more thoroughly. To fill this gap, in this work, we systematize the knowledge about AVs. To do so, we first surveyed the literature and identified existing knowledge gaps in AV internals’ working. Further, we bridged these gaps by analyzing popular (Windows, Linux, and Android) AV solutions to check their operations in practice. Our methodology encompassed multiple techniques, from tracing to fuzzing. We detail current AV’s architecture, including their multiple components, such as browser extensions and injected libraries, regarding their implementation, monitoring features, and self-protection capabilities. We discovered, for instance, a great disparity in the set of API functions hooked by the distinct AV’s libraries, which might have a significant impact in the viability of academically-proposed detection models (e.g., machine learning-based ones).",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compind.2021.103556,Journal,Computers in Industry,scopus,2022-01-01,sciencedirect,C-Ports: A proposal for a comprehensive standardization and implementation plan of digital services offered by the “Port of the Future”,https://api.elsevier.com/content/abstract/scopus_id/85118477493,"In this paper we address the topic of a possible path to standardize the ICT services expected to be delivered by the so-called “Port of the Future”. How the most relevant technologies and Information Systems are used by the Port Communities for their businesses is discussed together with a detailed analysis of the on-going actions carried on by Standard Setting Organizations. Considering the examples given by the C-ITS Platform and the C-Roads programme at EU level, a proposal of contents to be considered in a comprehensive standardization action is given. The innovation services are therefore grouped into four bundles: (i) Vessel & Marine Navigation, (ii) e-Freight & (Intermodal) Logistics, (iii) Passenger Transport, (iv) Environmental sustainability. The standardized version of these applications will be finally labeled as C-Port services. Alongside the standardization plan, a proposal for ranking the ports on the basis of a specially-defined C-Port vector is discussed with the purpose of addressing the well-known lack of consensus around the mathematical definition of the Smart Port Index. Considering the good practice and the background offered by the Port of Livorno in terms of innovation actions, the prospected final user applications are then labeled as Day 1, Day 1.5, and Day 2 services in consideration of the technical and commercial gaps to be filled. As a case study about the evolution in the C-Port vector experienced by the Port of Livorno in the last years will also be discussed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.dss.2021.113653,Journal,Decision Support Systems,scopus,2022-01-01,sciencedirect,AI-based industrial full-service offerings: A model for payment structure selection considering predictive power,https://api.elsevier.com/content/abstract/scopus_id/85114151068,"Artificial Intelligence and servitization reshape the way that manufacturing companies derive value. Aiming to sustain competitive advantage and intensify customer loyalty, full-service providers offer the use of their products as a service to achieve continuous revenues. For this purpose, companies implement AI classification algorithms to enable high levels of service at controllable costs. However, traditional asset sellers who become service providers require previously atypical payment structures, as classic payment methods involving a one-time fee for production costs and profit margins are unsuitable. In addition, a low predictive power of the implemented classification algorithm can lead to misclassifications, which diminish the achievable level of service and the intended net present value of the resultant service. While previous works focus solely on the costs of such misclassifications, our decision model highlights implications for payment structures, service levels, and – ultimately – the net present value of such data-driven service offerings. Our research suggests that predictive power can be a major factor in selecting a suitable payment structure and the overall design of service level agreements. Therefore, we compare common payment structures for data-driven services and investigate their relationship to predictive power. We develop our model using a design science methodology and iteratively evaluate our results using a four-step approach that includes interviews with industry experts and the application of our model to a real-world use case. In summary, our research extends the existing knowledge of servitization and data-driven services in the manufacturing industry through a quantitative decision model.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.knosys.2021.107607,Journal,Knowledge-Based Systems,scopus,2021-12-25,sciencedirect,Adaptive multi-objective service composition reconfiguration approach considering dynamic practical constraints in cloud manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85117828056,"Dynamic uncertainty factors such as equipment faults are common in practically implemented cloud manufacturing (CMfg) environments, often causing the manufacturing service to be invalidated. In that case, efficient reconfiguration of the original service composition under practical constraints is critical; however, existing research scarcely focuses on it. This paper proposes a dynamic service composition reconfiguration model to bridge the gap by considering practical constraints (DSCRPC) in a real-life cloud manufacturing environment. Based on the constraints considered in this study, the DSCRPC model redefines three objectives: time (T*), cost (C*), and product service quality (Q*S*). To optimize the DSCRPC model, this study developed an adaptive multi-population multi-objective whale optimization algorithm (AMPOWOA) based on the Pareto strategy. The algorithm adopts four balancing strategies and adaptively optimizes and adjusts the key parameters under various balancing strategies through well-designed reinforcement learning models. Finally, we conduct numerical experiments and actual application case tests to compare the performances of AMPOWOA and other algorithms (MOWOA, MOHHO, NSGA-II). The results show that DSCRPC can continuously tackle the cloud manufacturing service composition (CMSC) reconfiguration issue with constraints until an order is completed. Moreover, AMPOWOA is superior to the other algorithms optimizing the DSCRPC model. This significantly enhances the robustness of service composition reconfiguration in real-life CMfg.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.apenergy.2021.117857,Journal,Applied Energy,scopus,2021-12-15,sciencedirect,A hybrid deep learning-based online energy management scheme for industrial microgrid,https://api.elsevier.com/content/abstract/scopus_id/85115173233,"The fluctuations in electricity prices and intermittency of renewable energy systems necessitate the adoption of online energy management schemes in industrial microgrids. However, it is challenging to design effective and optimal online rolling horizon energy management strategies that can deliver assured optimality, subject to the uncertainties of volatile electricity prices and stochastic renewable resources. This paper presents an adaptable online energy management scheme for industrial microgrids that minimizes electricity costs while meeting production requirements by repeatedly solving an optimization problem over a moving control window, taking advantage of forecasted future prices and renewable energy profiles implemented by a hybrid deep learning model. The predicted values over the control horizon are assumed to be uncertain, and a multivariate Gaussian distribution is used to handle the variations in electricity prices and renewable resources around their predicted nominal values. Simulation results under different scenarios using real-world data verify the effectiveness of the proposed online energy management scheme, assessed by the corresponding gaps with respect to several selected benchmark strategies and the ideal boundaries of the best and worst known solutions. Furthermore, the robustness of the scheme is verified by considering severe errors in forecasted electricity prices and renewable profiles.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.apenergy.2021.117733,Journal,Applied Energy,scopus,2021-12-15,sciencedirect,Controlling distributed energy resources via deep reinforcement learning for load flexibility and energy efficiency,https://api.elsevier.com/content/abstract/scopus_id/85114713033,"Behind-the-meter distributed energy resources (DERs), including building solar photovoltaic (PV) technology and electric battery storage, are increasingly being considered as solutions to support carbon reduction goals and increase grid reliability and resiliency. However, dynamic control of these resources in concert with traditional building loads, to effect efficiency and demand flexibility, is not yet commonplace in commercial control products. Traditional rule-based control algorithms do not offer integrated closed-loop control to optimize across systems, and most often, PV and battery systems are operated for energy arbitrage and demand charge management, and not for the provision of grid services. More advanced control approaches, such as MPC control have not been widely adopted in industry because they require significant expertise to develop and deploy. Recent advances in deep reinforcement learning (DRL) offer a promising option to optimize the operation of DER systems and building loads with reduced setup effort. However, there are limited studies that evaluate the efficacy of these methods to control multiple building subsystems simultaneously. Additionally, most of the research has been conducted in simulated environments as opposed to real buildings. This paper proposes a DRL approach that uses a deep deterministic policy gradient algorithm for integrated control of HVAC and electric battery storage systems in the presence of on-site PV generation. The DRL algorithm, trained on synthetic data, was deployed in a physical test building and evaluated against a baseline that uses the current best-in-class rule-based control strategies. Performance in delivering energy efficiency, load shift, and load shed was tested using price-based signals. The results showed that the DRL-based controller can produce cost savings of up to 39.6% as compared to the baseline controller, while maintaining similar thermal comfort in the building. The project team has also integrated the simulation components developed during this work as an OpenAIGym environment and made it publicly available so that prospective DRL researchers can leverage this environment to evaluate alternate DRL algorithms.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2021.108296,Journal,International Journal of Production Economics,scopus,2021-12-01,sciencedirect,An integrated Delphi-MCDM-Bayesian Network framework for production system selection,https://api.elsevier.com/content/abstract/scopus_id/85114948077,"Several attempts are needed to choose the most compatible production system for achieving the desired manufacturing outputs. The significant role of manufacturing strategy deployment is selecting the production system best suited for a manufacturing firm. The appropriately chosen production system (strategic process choice) facilitates a firm to produce “order winning” outputs and provides a production competence to achieve business success. This research presents a novel framework to determine the compatible production system for a manufacturing firm. An integrated three-stage Delphi-MCDM-Bayesian Network (BN) framework has been proposed. The process choice criteria (PCC) considered for deciding production systems are identified through an in-depth literature review and then validated by experts through a Delphi method in the first stage. It resulted in the determination of twenty-six PCC. In the second stage, the multi-criteria decision-making (MCDM) based voting analytical hierarchy process (VAHP) method is adopted to determine each criterion's relative importance for a firm. The relative weights obtained are then used as input for the machine learning (ML) technique- Bayesian network (BN) in the third stage. The BN model quantifies the selection probability of production systems. The proposed Delphi-MCDM-BN framework is demonstrated using a real-life case of a “hydraulic and pneumatic valve” manufacturing firm to select a suitable production system. The three-stage framework is a novel contribution to the literature, which can be used by researchers, practitioners, and manufacturing strategists to choose an appropriate production system for any manufacturing firm.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2021.107859,Journal,Applied Soft Computing,scopus,2021-12-01,sciencedirect,Securing Smart Cities using LSTM algorithm and lightweight containers against botnet attacks,https://api.elsevier.com/content/abstract/scopus_id/85114806873,"Smart Cities contains millions of IoT sensors supporting critical applications such as Smart Transport, Buildings, Intelligent Vehicles, and Logistics. A central administrator appointed by the government manages and maintains the security of each node. Smart City relies upon millions of sensors that are heterogeneous and do not support standard security architecture. Different manufacturers have weak protection protocols for their products and do not update their firmware upon newly identified operating systems’ vulnerabilities. Adversaries using brute force methods exploit the lack of inbuilt security systems on IoT devices to grow their bot network. Smart cities require a standard framework combining soft computing and Deep Learning (DL) for device fleet management and complete control of sensor operating systems for absolute security. This paper presents a real-world application for IoT fleet management security using a lightweight container-based botnet detection (C-BotDet) framework. Using a three-phase approach, the framework using Artificial Intelligence detects compromised IoT devices sending malicious traffic on the network. Balena Cloud revokes API keys and prevents a compromised device from infecting other devices to form a more giant botnet. VPN (Virtual Private Network) prevents inter-device communication and routes all malicious traffic through an external server. The framework quickly updates the standard Linux-based operating system IoT device fleet without relying on different manufacturers to update their system security individually. The simulation and analysis of the C-BotDet framework are presented in a practical working environment to demonstrate its implementation feasibility.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.elerap.2021.101098,Journal,Electronic Commerce Research and Applications,scopus,2021-11-01,sciencedirect,An intelligent knowledge-based chatbot for customer service,https://api.elsevier.com/content/abstract/scopus_id/85119699964,"This study proposes an intelligent knowledge-based conversational agent system architecture to support customer services in e-commerce sales and marketing. A pilot implementation of a chatbot for customer services is reported in a leading women’s intimate apparel manufacturing firm. The proposed system incorporates various emerging technologies, including web crawling, natural language processing, knowledge bases, and artificial intelligence. In this study, a prototype system is built in a real-world setting. The results of the system prototype evaluation are satisfactory and support the contention that the system is effective. The study also discusses the challenges and lessons learned during system implementation and the theoretical and managerial implications of this study.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.livsci.2021.104700,Journal,Livestock Science,scopus,2021-11-01,sciencedirect,A review of deep learning algorithms for computer vision systems in livestock,https://api.elsevier.com/content/abstract/scopus_id/85118744270,"In livestock operations, systematically monitoring animal body weight, biometric body measurements, animal behavior, feed bunk, and other difficult-to-measure phenotypes is manually unfeasible due to labor, costs, and animal stress. Applications of computer vision are growing in importance in livestock systems due to their ability to generate real-time, non-invasive, and accurate animal-level information. However, the development of a computer vision system requires sophisticated statistical and computational approaches for efficient data management and appropriate data mining, as it involves massive datasets. This article aims to provide an overview of how deep learning has been implemented in computer vision systems used in livestock, and how such implementation can be an effective tool to predict animal phenotypes and to accelerate the development of predictive modeling for precise management decisions. First, we reviewed the most recent milestones achieved with computer vision systems and the respective deep learning algorithms implemented in Animal Science studies. Then, we reviewed the published research studies in Animal Science which used deep learning algorithms as the primary analytical strategy for image classification, object detection, object segmentation, and feature extraction. The great number of reviewed articles published in the last few years demonstrates the high interest and rapid development of deep learning algorithms in computer vision systems across livestock species. Deep learning algorithms for computer vision systems, such as Mask R-CNN, Faster R-CNN, YOLO (v3 and v4), DeepLab v3, U-Net and others have been used in Animal Science research studies. Additionally, network architectures such as ResNet, Inception, Xception, and VGG16 have been implemented in several studies across livestock species. The great performance of these deep learning algorithms suggests an improved predictive ability in livestock applications and a faster inference. However, only a few articles fully described the deep learning algorithms and their implementation. Thus, information regarding hyperparameter tuning, pre-trained weights, deep learning backbone, and hierarchical data structure were missing. We summarized peer-reviewed articles by computer vision tasks (image classification, object detection, and object segmentation), deep learning algorithms, animal species, and phenotypes including animal identification and behavior, feed intake, animal body weight, and many others. Understanding the principles of computer vision and the algorithms used for each application is crucial to develop efficient systems in livestock operations. Such development will potentially have a major impact on the livestock industry by predicting real-time and accurate phenotypes, which could be used in the future to improve farm management decisions, breeding programs through high-throughput phenotyping, and optimized data-driven interventions.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmapro.2021.09.048,Journal,Journal of Manufacturing Processes,scopus,2021-11-01,sciencedirect,Joint active search and neuromorphic computing for efficient data exploitation and monitoring in additive manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85117322411,"The recent integration of imaging technology with additive manufacturing (AM) leads to the plethora of in-process and high-dimensional data. Machine learning (ML) methods have been implemented to improve understanding of defect formation in AM-built parts and controlling process variability in real-time. However, modern ML methods, in particular deep neural networks, are empowered by massive high-quality labeled data, which are limited in AM due to the following reasons: First, large data labeling is often tedious, costly, and requires substantial human efforts with considerable expertise. Second, the performance of the learning methods depends to a great extent on the presence of positive data instances (i.e., defective) as they are more informative for monitoring. Third, the rare positives result in a severe imbalanced dataset poses critical challenges in training ML methods designed with the assumption that the input contains an equal number of instances from each class. In this research, we propose novel annotation and learning with limited number of data through the integration of active search and hyperdimensional computing (HDC). The active search is developed to benefit from a single bandit model to learn about the data distribution (exploration) while sampling from the regions potentially containing more positives (exploitation). HDC is introduced as an alternative computing method that mimics important brain functionalities and encodes data with high-dimensional vectors, thereby enabling single-pass learning with just a few samples. Experimental results on a real-world case study of drag link joint build show the proposed model locates the rare positives thoroughly and detects lack of fusion defects with the accuracy of 89.58%, in 3.221 ± 0.029 second training time and with only 66 sample data. The joint active search and neuromorphic computing framework is shown to have strong potentials for general applications in a diverse set of domains with in-situ imaging data.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.addma.2021.102328,Journal,Additive Manufacturing,scopus,2021-11-01,sciencedirect,In situ infrared temperature sensing for real-time defect detection in additive manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85115355988,"Melt pool temperature is a critical parameter for the majority of additive manufacturing processes. Monitoring of the melt pool temperature can facilitate the real-time detection of various printing defects such as voids, over-extrusion, filament breakage, clogged nozzle, etc. that occur either naturally or as the result of malicious hacking activity. This study uses an in situ, multi-sensor approach for monitoring melt pool temperature in which non-contact infrared temperature sensors with customized field of view move along with the extruder of a fused deposition modeling-based printer and sense melt pool temperature from a very short working distance regardless of its X-Y translational movements. A statistical method for defect detection is developed and utilized to identify temperature deviations caused by intentionally implemented defects. Effective detection for multiple defect types and sizes is demonstrated using both a simple L-shaped test geometry and a more complex industry standard test article. Strengths and limitations of this approach are presented, and the potential for expansion via more advanced data analysis techniques such as machine learning are discussed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.psj.2021.101437,Journal,Poultry Science,scopus,2021-11-01,sciencedirect,Pharmacokinetic/pharmacodynamic profiles of baicalin against Mycoplasma gallisepticum in an in vivo infection model,https://api.elsevier.com/content/abstract/scopus_id/85115144118,"Mycoplasma gallisepticum (
                        M. gallisepticum
                     ), a devastating avian pathogen that commonly causes chronic respiratory disease in chicken, is responsible for tremendous economic losses to the poultry industry. Baicalin is the main constituent of Scutellaria baicalensis that shows potential therapeutic effects against M. gallisepticum. However, the pharmacokinetic/pharmacodynamics (PK/PD) profiles of baicalin against M. gallisepticum are not well understood. The main objective of the present study was to determine the relationship between the PK/PD index and efficacy of baicalin in the M. gallisepticum infection model in chickens. The experiments were carried out on 10-day-old chickens that were challenged with M. gallisepticum in the bilateral air sacs. While, baicalin was orally administrated once in a day for 3 consecutive days, started from d 3 postinfection. Ultra-performance liquid chromatography (UPLC) was used to evaluate the PK parameters of baicalin at doses of 200, 400, and 600 mg/kg in M. gallisepticum-infected chickens. Real-time PCR (RT-PCR) was used for the quantitative detection of M. gallisepticum in lungs. The PK and PD data were fitted to WinNonlin software to evaluate the PK/PD profiles of baicalin against M. gallisepticum. The minimum inhibitory concentration (MIC) of baicalin against M. gallisepticum strain Rlow was 31.25 µg/mL. The in vivo data suggested that baicalin concentration in the lung tissues was higher than plasma (1.21–1.73 times higher). The ratios of AUC24h/MIC of baicalin against bacteriostatic, bactericidal, and eradication were 0.62, 1.33, and 1.49 h, respectively. In conclusion, these results provided potential reference for future clinical dose selection of baicalin and evaluation of susceptibility breakpoints.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmbbm.2021.104728,Journal,Journal of the Mechanical Behavior of Biomedical Materials,scopus,2021-11-01,sciencedirect,What can artificial intelligence and machine learning tell us? A review of applications to equine biomechanical research,https://api.elsevier.com/content/abstract/scopus_id/85112485329,"Artificial intelligence (AI) and machine learning (ML) are fascinating interdisciplinary scientific domains where machines are provided with an approximation of human intelligence. The conjecture is that machines are able to learn from existing examples, and employ this accumulated knowledge to fulfil challenging tasks such as regression analysis, pattern classification, and prediction. The horse biomechanical models have been identified as an alternative tool to investigate the effects of mechanical loading and induced deformations on the tissues and structures in humans. Many reported investigations into bone fatigue, subchondral bone damage in the joints of both humans and animals, and identification of vital parameters responsible for retaining integrity of anatomical regions during normal activities in all species are heavily reliant on equine biomechanical research. Horse racing is a lucrative industry and injury prevention in expensive thoroughbreds has encouraged the implementation of various measurement techniques, which results in massive data generation. ML substantially accelerates analysis and interpretation of data and provides considerable advantages over traditional statistical tools historically adopted in biomechanical research. This paper provides the reader with: a brief introduction to AI, taxonomy and several types of ML algorithms, working principle of a feedforward artificial neural network (ANN), and, a detailed review of the applications of AI, ML, and ANN in equine biomechanical research (i.e. locomotory system function, gait analysis, joint and bone mechanics, and hoof function). Reviewing literature on the use of these data-driven tools is essential since their wider application has the potential to: improve clinical assessments enabling real-time simulations, avoid and/or minimize injuries, and encourage early detection of such injuries in the first place.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2021.107784,Journal,Applied Soft Computing,scopus,2021-11-01,sciencedirect,Towards learning behavior modeling of military logistics agent utilizing profit sharing reinforcement learning algorithm,https://api.elsevier.com/content/abstract/scopus_id/85112396580,"Agent-based modeling has become a beneficial tool in describing the complex and intelligent decision-making behaviors of military logistics entities, which is essential in exploring military logistics system. A challenging task in this field is the learning behavior modeling of military logistics agents. Profit sharing (PS) reinforcement learning algorithm is a representative exploitation-oriented method describing empirical reinforcement learning mechanism, and has been successfully applied to a variety of real-world problems. However, constructing the learning behavior model of military logistics agents is difficult by merely using the original PS algorithm. This difficulty is due to the actual characteristics of equipment support operations and military requirements, such as experience sharing, cooperative action, and hierarchical control. To address this issue, we propose an improved PS algorithm by introducing cooperative task reward correction parameters, experience sharing learning function, and superior command controlled function. We use the research methodology centering on the basic process of the improved PS algorithm as basis to construct the architecture of the learning behavior model of military logistics agents and its corresponding model of elements. Furthermore, we design the implementation algorithm of the learning behavior model. Lastly, we conduct a case study of a tactical military industrial logistics simulation system, thereby verifying the feasibility and effectiveness of the learning behavior model. We find that the improved PS algorithm and corresponding learning behavior model have more advantages than the original PS algorithm.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aquaeng.2021.102192,Journal,Aquacultural Engineering,scopus,2021-11-01,sciencedirect,"An integrated framework of sensing, machine learning, and augmented reality for aquaculture prawn farm management",https://api.elsevier.com/content/abstract/scopus_id/85112001426,"The rapid growth of prawn farming on an international scale will play an important role in meeting the protein requirements of an expanding global population. Efficient management of the commercial ponds for healthy production of prawns is the key mantra of success in this industry. It is a necessity to maintain the water quality parameters in these ponds within specific ranges to create an ideal environment of optimal growth of healthy prawns. The current practice of water quality data collection and their usage for decision making on most farms is not efficient and does not take full advantage of the latest technologies. The research presented in this paper aimed at addressing this problem by systematic investigation and development of an integrated framework where (i) modern sensors were investigated for their suitability and deployed for continuous monitoring of the water quality variables in prawn ponds; (ii) novel machine learning models were investigated based on collected data and deployed to accurately forecast pond status over next 24 h. This provides farmers insight into upcoming situations and take necessary measures to avoid catastrophic situations; and (iii) augmented reality-based visualisation methods were investigated for improved data capture process and efficient decision making through real-time interactive interfaces. The paper presents the integrated framework as well as the details of sensing, machine learning, and augmented reality components. We found that (i) YSI EXO2 Multi-Sonde is the best sensor for continuous monitoring of prawn ponds; (ii) ForecastNet (our developed machine learning model) provides best forecasting results with symmetric mean absolute percentage error of 6.1 %, 9.6 %, and 8.5 % for dissolved oxygen, pH, and temperature; and (iii) augmented reality-based interactive interface achieves accuracy as high as 89.2 % for management decisions with at least 41 % less time. The experience of the project as presented in this paper can act as a guide for researchers as well as prawn farmers to take advantage of latest sensors, machine learning algorithms and augmented reality tools.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ssci.2021.105407,Journal,Safety Science,scopus,2021-11-01,sciencedirect,Highway 4.0: Digitalization of highways for vulnerable road safety development with intelligent IoT sensors and machine learning,https://api.elsevier.com/content/abstract/scopus_id/85111074950,"According to United Nations (UN) 2030 agenda, the transportation system needs to be enhanced for the establishment of access to safe, affordable, accessible, and sustainable transport systems along with enhanced road safety. The highway road transport system is one of the transport systems that enables to transits goods and humans from one location to another location. The agenda of UN 2030 for the transport system will be accomplished with the assistance of digital technologies like the internet of things (IoT) and artificial intelligence (AI). The implementation of these digital technologies on highways empowers to provide reliable, smarter, intelligent, and renewable energy sources experience to the users travelling along the highways. This study discusses the significance of the digitalization of highways that supporting and realizing a sustainable environment on the highways. To discuss the significance of digitalization, the study has categorized digitalization into five subcomponents namely smart highway lighting system, smart traffic and emergency management system, renewable energy sources on highways, smart display and AI in highways. An architecture-for smart highway lighting, smart traffic, and emergency management are proposed and discussed in the study. The significance of implementing smart display boards and renewable sources with real-time applications is also addressed in this study. Moreover, the integration of AI in highways is addressed with the perspective of enhancing road safety. The integration of deep learning (DL) in the edge-based vision node for predicting the patterns of traffic flow, highway road safety, and maintenance of quality roads have been addressed in the discussion section. Embedding the deep learning techniques in the vison node at the traffic junction and the highway lighting controller is able to deliver an intelligent system that provides sustained experience and management of the highways. Smart reflectors, adoption of renewable energy, developing vehicle-to-vehicle communication in vehicles, and smart lamppost are the few recommendations for the implementation of digitalizing highways.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.talanta.2021.122608,Journal,Talanta,scopus,2021-11-01,sciencedirect,"PIXE based, Machine-Learning (PIXEL) supported workflow for glass fragments classification",https://api.elsevier.com/content/abstract/scopus_id/85109431731,"This paper presents a structured workflow for glass fragment analysis based on a combination of Elemental Analysis using PIXE and Machine Learning tools, with the ultimate goal of standardizing and helping forensic efforts. The proposed workflow was implemented on glass fragments received from the Israeli DIFS (Israeli Police Force's Division of Identification and Forensic Sciences) that were collected from various vehicles, including glass fragments from different manufacturers and years of production. We demonstrate that this workflow can produce models with high (>80%) accuracy in identifying glass fragment's origins and provide a test-case demonstrating how the model can be applied in real-life forensic events. We provide a standard, reproducible methodology that can be used in many forensic domains beyond glass fragments, for example, Gun Shot Residue, flammable liquids, illegal substances, and more.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.renene.2021.05.155,Journal,Renewable Energy,scopus,2021-11-01,sciencedirect,A deep learning approach towards the detection and recognition of opening of windows for effective management of building ventilation heat losses and reducing space heating demand,https://api.elsevier.com/content/abstract/scopus_id/85107941088,"Building ventilation accounts for up to 30% of the heat loss in commercial buildings and 25% in industrial buildings. To effectively aid the reduction of energy consumption in the building sector, the development of demand-driven control systems for heating ventilation and air-conditioning (HVAC) is necessary. In countries with temperate climates such as the UK, many buildings depend on natural ventilation strategies such as openable windows, which are useful for reducing overheating prevalence during the summer. The manual opening and adjustment of windows by occupants, particularly during the heating season, can lead to substantial heat loss and consequent energy consumption. This could also result in the unnecessary or over ventilation of the space, or the fresh air is more than what is required to ensure adequate air quality. Furthermore, energy losses build up when windows are left open for extended periods. Hence, it is important to develop control strategies that can detect and recognise the period and amount of window opening in real-time and at the same time adjust the HVAC systems to minimise energy wastage and maintain indoor environment quality and thermal comfort. This paper presents a vision-based deep learning framework for the detection and recognition of manual window operation in buildings. A trained deep learning model is deployed into an artificial intelligence-powered camera. To assess the proposed strategy's capabilities, building energy simulation was used with various operation profiles of the opening of the windows based on various scenarios. Initial experimental tests were conducted within a university lecture room with a south-facing window. Deep learning influenced profile (DLIP) was generated via the framework, which uses real-time window detection and recognition data. The generated DLIP were compared with the actual observations, and the initial detection results showed that the method was capable of identifying windows that were opened and had an average accuracy of 97.29%. The results for the three scenarios showed that the proposed strategy could potentially be used to help adjust the HVAC setpoint or alert the occupants or building managers to prevent unnecessary heating demand. Further developments include enhancing the framework ability to detect multiple window opening types and sizes and the detection accuracy by optimising the model.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ymssp.2021.107915,Journal,Mechanical Systems and Signal Processing,scopus,2021-11-01,sciencedirect,Machine learning based frequency modelling,https://api.elsevier.com/content/abstract/scopus_id/85103975336,"Detection of cracks in structures has always been an important research topic in the industrial domain closely associated with aerospace, mechanical, marine and civil engineering. The presence of the cracks alters the dynamic response properties. Hence, it becomes crucial to locate these cracks in the structures to avoid any catastrophic failures and maintain structural integrity and performance. The study's objective is to propose two distinct statistical procedures for conducting the machine learning experiment for modelling the frequency and show the effect of experiment design on the results. In the study, the predictive performance of machine learning models and their ensembles is compared within each experiment design and between two experimental designs for the task of prediction of first six natural frequencies of a fixed ended cracked beam. The study highlights the significance of more than one experimental design to reduce the confirmation bias in the research and discusses the proposed methods' generalizability over the different modelling constraints and modelling parameters. The study also discusses a real-world implementation of the learned machine learning models from the perspective of Bayesian optimization.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jhlste.2020.100275,Journal,"Journal of Hospitality, Leisure, Sport and Tourism Education",scopus,2021-11-01,sciencedirect,Industry 4.0 technologies in tourism education: Nurturing students to think with technology,https://api.elsevier.com/content/abstract/scopus_id/85092173436,"The Industry 4.0 revolution is bringing major transformations in the tourism systems design suitable for technologically oriented consumers. Indeed, methods and technologies introduced by Big Data, Automation, Virtual and augmented reality, Robotics and ICT well fit with the Tourism 4.0 paradigm. However, tourism students are not yet trained on techniques, issues and methods related to the Industry 4.0 framework.
                  Hence, relying on a careful examination of the literature on tourism market trends linked to the offer of innovative technological services, we identified conceptual, methodological, technological and practical skills to be developed in an academic curriculum for Tourism Science students. Learning path were focused on: i) processes of data acquisition from social media, ii) data analysis using Machine Learning techniques and iii) data design into significant elements useful to implement communication systems in the tourism field.
               
                  Results
                  showed that the most of participants achieved a medium-high evaluation for the implementation of the communication systems, applying appropriately techniques and tools learned along the course. Furthermore, the high percentage of students satisfaction registered in relation to the course, revealed that students enjoyed this experience. Outcomes reflects the acquisition and the awareness of those skills that will enable students to be conscious protagonists of their role in tourism 4.0.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.egyr.2021.09.031,Journal,Energy Reports,scopus,2021-10-01,sciencedirect,Backward simulation of temperature changes of District Heating networks for enabling loading history in predictive maintenance,https://api.elsevier.com/content/abstract/scopus_id/85122686801,"District Heating (DH) networks, like most of industries, are in transition to the fourth​ industry age and they are retrofitting themselves with different sensing and inspection technologies to enable cyber connectivity for different purposes, such as system optimization, failure detection, maintenance, etc. Since DH pipes show different ageing behaviour under different conditions and initially the pre-insulated bounded pipes had been designed for a minimum of 30 years life span, a long-term loading history is required for predictive maintenance (PdM) purposes and it is necessary to understand the ageing of the DH pipes. These historical temperature changes of the networks are not available for such a long period and they are usually limited to the past few years. To exploit the available implemented technologies for PdM , the missing data must become available to understand the ageing patterns and expand the ageing model to the pipes in use. In this research, various Machine Learning (ML) techniques such as Support Vector Machine (SVM), Random Forest algorithm (RF), Artificial Neural Networks (ANN) have been tested to train a model and backward simulate the temperature changes of the system based on recorded weather data. Various none-temperature variables have been used to enhance the prediction qualities to the real-world data. The historical temperature changes of the system shall be used for different ageing estimation such as fatigue cycles or remaining useful life of the polyurethane (PUR) foam.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.probengmech.2021.103173,Journal,Probabilistic Engineering Mechanics,scopus,2021-10-01,sciencedirect,Machine learning based digital twin for stochastic nonlinear multi-degree of freedom dynamical system,https://api.elsevier.com/content/abstract/scopus_id/85117922944,"The potential of digital twin technology is immense, specifically in the infrastructure, aerospace, and automotive sector. However, practical implementation of this technology is not at an expected speed, specifically because of lack of application-specific details. In this paper, we propose a novel digital twin framework for stochastic nonlinear multi-degree of freedom (MDOF) dynamical systems. The proposed digital twin has four modules — (a) a physics-based nominal model, (b) a data collection module, (c) algorithm for real-time update of the digital twin and (d) module for predicting future state. The modules for real-time update and prediction are based on the so-called gray-box modeling approach, and utilizes both physics based and data driven frameworks; this enables the proposed digital twin to generalize and predict future responses. The gray box modeling framework used within the digital twin is developed by coupling Bayesian filtering and machine learning algorithm. Although, the proposed digital twin can be used with any machine learning regression algorithm, we have used Gaussian process in this study. Performance of the proposed approach is illustrated using two examples. Results obtained indicate the applicability and excellent performance of the proposed digital twin framework.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jocs.2021.101443,Journal,Journal of Computational Science,scopus,2021-10-01,sciencedirect,Towards versatile conversations with data-driven dialog management and its integration in commercial platforms,https://api.elsevier.com/content/abstract/scopus_id/85115363526,"Conversational interfaces have recently become a ubiquitous element in both the personal sphere by easing access to services, and industrial environments by the automation of services, improved customer support and its corresponding cost savings. However, designing the dialog model used by these interfaces to decide system responses is still a hard-to-accomplish task for complex conversational interactions. This paper describes a data-driven dialog management technique, which provides flexibility to develop, deploy and maintain this module. Various configurations for classification algorithms are assessed with two dialog corpora of different application domains, size, dimensionalities and set of possible system responses. The results of the evaluation show satisfactory accuracy and coherence rates in both tasks. As a proof of concept, our proposal has also been integrated with DialogFlow, a platform provided by Google to design conversational user interfaces. Our proposal has been assessed with a real use case, proving that it can be deployed in conjunction with commercial platforms, obtaining satisfactory results for the objective and subjective assessments completed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.techfore.2021.120986,Journal,Technological Forecasting and Social Change,scopus,2021-10-01,sciencedirect,Big data and firm marketing performance: Findings from knowledge-based view,https://api.elsevier.com/content/abstract/scopus_id/85113910474,"A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications. This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms’ strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms’ activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2021.107702,Journal,Applied Soft Computing,scopus,2021-10-01,sciencedirect,OrbitNet: A new CNN model for automatic fault diagnostics of turbomachines,https://api.elsevier.com/content/abstract/scopus_id/85111487515,"Unplanned outage due to faults in a high-fidelity turbomachine such as steam turbine and centrifugal compressor often results in the reduced reliability and productivity of a factory while increasing its maintenance costs. Shaft orbit images generated from turbomachine vibration signals have been used to diagnose component faults. However, the existing methods were developed mostly by either using features extracted from orbits or utilizing simulation data which may produce inaccurate results in practical applications due to system complexity and data uncertainties. This paper presents a novel deep learning convolution neural network methodology for accurately automatic diagnostics of multiple faults in general rotating machines by adeptly integrating advanced signal processing with orbit images augmentation, considering the high non-linearity and uncertainty of sensed vibration signals. Environmental noise in vibration signals are filtered through the integration of multiresolution discrete wavelet packet transform and Bayesian hypothesis testing-based automatic thresholding. Shaft orbit images generated from the cleansed vibration data are augmented to increase their representativity and generalization. A novel multi-layer convolutional neural network model, OrbitNet, is specially designed to improve its generality and robustness while avoid possible overfitting in fault identification of various turbomachines. The proposed model retains the pattern information in the axis trajectory to the greatest extent, with the ability of accurately capturing features of various faults in different turbomachines. A generic implementation procedure is proposed for automatic fault diagnosis of rotating machinery based on the presented methodology. A comparison study is conducted to demonstrate the effectiveness and feasibility of the proposed methodology by using the sensed vibration signals collected from three real-world centrifugal compressors, two steam turbines and one generator with four different fault modes including imbalance, friction, misalignment and oil whirl.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.conengprac.2021.104903,Journal,Control Engineering Practice,scopus,2021-10-01,sciencedirect,Synthesizing labeled data to enhance soft sensor performance in data-scarce regions,https://api.elsevier.com/content/abstract/scopus_id/85111246946,"Quality variables are key indicators of the operating performance in industrial processes. Because they are difficult to measure, soft sensor models can be adopted to predict them timely. For accurate prediction, sufficient training data are necessary to construct a good soft sensor model. In practical industrial processes, however, data labeled with quality variables are usually deficient in the desired region. Particularly, when the process is just switched to a new mode, available data in this new mode are initially quite a few. In this paper, a novel data synthesis method based on the regressor-embedded semi-supervised variational autoencoder (RSSVAE) model is proposed to generate synthetic labeled data when the original labeled data are inadequate. The proposed model utilizes not only the original data in the data-scarce region but also the data in other regions, which share some common information with the scarce data. Meanwhile, data synthesis and model correction mechanism are implemented iteratively to avoid model biases. Once the synthetic labeled data of the data-scarce region are acquired, they are combined with the original labeled data to establish a local soft sensor and predict the quality variables of the unlabeled data. Finally, a real ammonia synthesis process is introduced to demonstrate the effectiveness of the proposed method.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2021.107644,Journal,Applied Soft Computing,scopus,2021-10-01,sciencedirect,Production scheduling in industrial mining complexes with incoming new information using tree search and deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85109174667,"Industrial mining complexes have implemented digital technologies and advanced sensors to monitor and gather real-time data about their different operational aspects, starting from the supply of materials from the mineral deposits involved to the products provided to customers. However, technologies are not available to respond in real-time to the incoming new information to adapt the short-term production schedule of a mining complex. A short-term production schedule determines the daily/weekly/monthly sequence of extraction, the destination of materials and utilization of processing streams. This paper presents a novel self-learning artificial intelligence algorithm for mining complexes that learns, from its own experience, to adapt the short-term production scheduling decisions by responding to incoming new information. The algorithm plays the game of short-term production scheduling on its own using a Monte Carlo tree search to train a deep neural network agent that adapts the short-term production schedule with incoming new information. The deep neural network agent evaluates the short-term production scheduling decisions and, in parallel, performs searches using the Monte Carlo tree search to generate experiences. The experiences are then used to train the agent. The agent improves the strength of the tree search, which results in an even stronger self-play to generate better experiences. An application of the proposed algorithm at a real-world copper mining complex shows its exceptional performance to adapt the 13-week short-term production schedule almost in real-time. The adapted production schedule successfully meets the different production requirements and makes better use of the processing capabilities, while also increasing copper concentrate production by 7% and cash flows by 12% compared to the initial production schedule. A video of the proposed algorithm can be found at https://youtu.be/_gSbzxMc_W8.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cofs.2021.03.014,Journal,Current Opinion in Food Science,scopus,2021-10-01,sciencedirect,Novel digital technologies implemented in sensory science and consumer perception,https://api.elsevier.com/content/abstract/scopus_id/85104656313,"New and emerging digital technologies have been implemented in sensory science, which minimize subjectivity and biases in data acquisition and interpretation compared to traditional methods. These technologies have enabled the incorporation of physiological and emotional responses of panelists elicited by food, beverage, and packaging stimuli through accurate and unbiased information from different sensor technologies. This review focused on recent advances of digital technologies used for sensory science, such as (i) software for sensory science, (ii) integration of biometrics to assess physiological and emotional responses of panelists, (iii) incorporation of virtual, augmented, and mixed reality, and (iv) sensor technology (electronic noses and tongues) for sensory analysis. Rapid data acquisition and results’ interpretation could open the way to automation and implementation of Artificial Intelligence that could revolutionize the food and beverage industries. It also presents a proposed framework for integrating and implementing digital technologies through the food chain from farm/manufacturing facilities to the palate.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rcim.2021.102176,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2021-10-01,sciencedirect,Robotic grasping: from wrench space heuristics to deep learning policies,https://api.elsevier.com/content/abstract/scopus_id/85104603575,"The robotic grasping task persists as a modern industry problem that seeks autonomous, fast implementation, and efficient techniques. Domestic robots are also a reality demanding a delicate and accurate human–machine interaction, with precise robotic grasping and handling. From decades ago, with analytical heuristics, to recent days, with the new deep learning policies, grasping in complex scenarios is still the aim of several works’ that propose distinctive approaches. In this context, this paper aims to cover recent methodologies’ development and discuss them, showing state-of-the-art challenges and the gap to industrial applications deployment. Given the complexity of the related issue associated with the elaborated proposed methods, this paper formulates some fair and transparent definitions for results’ assessment to provide researchers with a clear and standardised idea of the comparison between the new proposals.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.knosys.2021.107261,Journal,Knowledge-Based Systems,scopus,2021-09-27,sciencedirect,Federated conditional generative adversarial nets imputation method for air quality missing data,https://api.elsevier.com/content/abstract/scopus_id/85111226892,"The air quality is a topic of extreme concern that attracts a lot of attention in the world. Many intelligent air quality monitoring networks have been deployed in various places, especially in big cities. These monitoring networks collect air quality data with some missing data for some reasons which pose an obstacle for air quality publishing and studies. Generative adversarial nets (GAN) methods have achieved state-of-the-art performance in missing data imputation. GAN-based imputation method needs enough training data while one monitoring network has just a few and poor quality monitoring data and these data sets do not meet the independent identical distribution (IID) condition. Therefore, one monitoring network side needs to utilize more monitoring data from other sides as far as possible. However, in the real world, these air quality monitoring networks are owned by different organizations — companies, the government even some secret units. Many of them cannot share detailed monitoring data due to security, privacy, and industrial competition. In this paper, it is the first time to propose a conditional GAN imputation method under a federated learning framework to solve the data sets that come from diverse data-owners without sharing. Furthermore, we improve the vanilla conditional GAN performance with Wasserstein distance and “Hint mask” trick. The experimental results show that our GAN-based imputation methods can achieve the best performance. And our federated GAN imputation method outperforms the GAN imputation method trained locally for each participant which means our imputation model can work. Our proposed federated GAN method can benefit model quality by increasing access to air quality data through private multi-institutional collaborations. We further investigate the effects of data geographical distribution across collaborating participants on model quality and, interestingly, we find that the GAN training process with a federated learning framework performs more stable.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.pmcj.2021.101445,Journal,Pervasive and Mobile Computing,scopus,2021-09-01,sciencedirect,Towards generating a reliable device-specific identifier for IoT devices,https://api.elsevier.com/content/abstract/scopus_id/85111971982,"A significant number of IoT devices are being deployed in the wild, mostly in remote locations and in untrusted conditions. This could include monitoring an electronic perimeter fence or a critical infrastructure such as telecom and power grids. Such applications rely on the fidelity of data reported from the IoT devices, and hence it is imperative to identify the trustworthiness of the remote device before taking decisions. Existing approaches use a secret key usually stored in volatile or non-volatile memory for creating an encrypted digital signature. However, these techniques are vulnerable to malicious attacks and have significant computation and energy overhead. This paper presents a novel device-specific identifier, IoT-ID that captures the device characteristics and can be used towards device identification. IoT-ID is based on physically unclonable functions (PUFs), that exploit variations in the manufacturing process to derive a unique fingerprint for integrated circuits. In this work, we design novel PUFs for Commercially Off the Shelf (COTS) components such as clock oscillators and ADC, to derive IoT-ID for a device. Hitherto, system component PUFs are invasive and rely on additional dedicated hardware circuitry to create a unique fingerprint. A highlight of our PUFs is doing away with special hardware. IoT-ID is non-invasive and can be invoked using simple software APIs running on COTS components. IoT-ID has the following key properties viz., constructability, real-time, uniqueness, and reproducibility, making them robust device-specific identifiers.
                  We present detailed experimental results from our live deployment of 50 IoT devices running over a month. Our edge machine learning algorithm has 100% accuracy in uniquely identifying the 50 devices in our deployment and can run locally on the resource-constrained IoT device. We show the scalability of IoT-ID with the help of numerical analysis on 1000s of IoT devices. Further, we discuss approaches to evaluate and improve the reliability of the IoT-ID.
                        1
                     
                     
                        1
                        This manuscript is an extension of the paper ‘IoT-ID: A Novel Device-Specific Identifier Based on Unique Hardware Fingerprints’ Vaidya et al. (2020) published in 2020 IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation (IoTDI).",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijcip.2021.100424,Journal,International Journal of Critical Infrastructure Protection,scopus,2021-09-01,sciencedirect,Industrial intrusion detection based on the behavior of rotating machine,https://api.elsevier.com/content/abstract/scopus_id/85111013464,"In this study, a new industrial intrusion detection method is introduced for the control system of rotating machines as critical assets in many industries. Data tampering is a major attack on the control systems which disrupts the functionality of the asset. Hence, our objective is to detect data manipulations in the system. We use the behavior of the rotating machine to propose new industrial intrusion detection for the control system of the rotating machine by machine learning techniques. The behavior is elicited by the data of sensors under all the conditions of the rotating machine operation. In this work, the nonlinear regression, novelty detection, outlier detection, and classification approaches are implemented to create behavioral model. On each implementation, online data are compared with the real data of behavior prediction model during the operation of the rotating machine to detect any abnormality. According to our experimental results, the accuracy of the behavioral models created by the One-classSVM novelty detection, k- Nearest Neighbor (kNN) outlier detection, decision tree classifier, k-Neighbors classifier, random forest classifier, and AdaBoost classifier is obtained as 0.98, 0.994, 0.999, 0.999, 0.999, and 0.999, respectively. The results indicate that the proposed industrial intrusion detection method is able to detect the data tampering attacks on the control system of the rotating machines very accurately.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.segan.2021.100511,Journal,"Sustainable Energy, Grids and Networks",scopus,2021-09-01,sciencedirect,Multi-interval programming based scheduling of appliances with user preferences and dynamic pricing in residential area,https://api.elsevier.com/content/abstract/scopus_id/85110609599,"In industrial and commercial sectors, numerous countries had successfully implemented the dynamic pricing as a solution to the problem of high power demand in peak hours. But, an extensive use of real-time pricing in the residential electricity sector is hugely missing. In order to boost the efficiency of electricity market by demand response, real-time pricing needs to be implemented into residential sector also. In this paper the proposed algorithm is implemented for residential consumers of different categories with real time pricing data of ComEd, Northern Illinois Power Company, and Alactra Utilities Corporation. The proposed algorithm incorporates single interval and multi interval programming for different power pricing schemes. The proposed algorithm is suggested using metaheuristic optimization techniques viz. cuckoo search (CS), adaptive cuckoo search (ACS) and Hybrid GA–PSO for the optimum scheduling of residential appliances. The objective of this paper is to minimize the monthly electricity bill cost as well as peak demand under uncertain electricity prices. The comparative analysis of optimal solutions obtained by various artificial intelligence techniques validates the high performance of proposed algorithm. It facilitates both the residential consumer and utilities with benefits.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijcip.2021.100452,Journal,International Journal of Critical Infrastructure Protection,scopus,2021-09-01,sciencedirect,Adversarial attacks and mitigation for anomaly detectors of cyber-physical systems,https://api.elsevier.com/content/abstract/scopus_id/85107972529,"The threats faced by cyber-physical systems (CPSs) in critical infrastructure have motivated research into a multitude of attack detection mechanisms, including anomaly detectors based on neural network models. The effectiveness of anomaly detectors can be assessed by subjecting them to test suites of attacks, but less consideration has been given to adversarial attackers that craft noise specifically designed to deceive them. While successfully applied in domains such as images and audio, adversarial attacks are much harder to implement in CPSs due to the presence of other built-in defence mechanisms such as rule checkers (or invariant checkers). In this work, we present an adversarial attack that simultaneously evades the anomaly detectors and rule checkers of a CPS. Inspired by existing gradient-based approaches, our adversarial attack crafts noise over the sensor and actuator values, then uses a genetic algorithm to optimise the latter, ensuring that the neural network and the rule checking system are both deceived. We implemented our approach for two real-world critical infrastructure testbeds, successfully reducing the classification accuracy of their detectors by over 50% on average, while simultaneously avoiding detection by rule checkers. Finally, we explore whether these attacks can be mitigated by training the detectors on adversarial samples.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2021.107574,Journal,Applied Soft Computing,scopus,2021-09-01,sciencedirect,Nonlinear-based Chaotic Harris Hawks Optimizer: Algorithm and Internet of Vehicles application,https://api.elsevier.com/content/abstract/scopus_id/85107718159,"Harris Hawks Optimizer (HHO) is one of the many recent algorithms in the field of metaheuristics. The HHO algorithm mimics the cooperative behavior of Harris Hawks and their foraging behavior in nature called surprise pounce. HHO benefits from a small number of controlling parameters setting, simplicity of implementation, and a high level of exploration and exploitation. To alleviate the drawbacks of this algorithm, a modified version called Nonlinear based Chaotic Harris Hawks Optimization (NCHHO) is proposed in this paper. NCHHO uses chaotic and nonlinear control parameters to improve HHO’s optimization performance. The main goal of using the chaotic maps in the proposed method is to improve the exploratory behavior of HHO. In addition, this paper introduces a nonlinear control parameter to adjust HHO’s exploratory and exploitative behaviors. The proposed NCHHO algorithm shows an improved performance using a variety of chaotic maps that were implemented to identify the most effective one, and tested on several well-known benchmark functions. The paper also considers solving an Internet of Vehicles (IoV) optimization problem that showcases the applicability of NCHHO in solving large-scale, real-world problems. The results demonstrate that the NCHHO algorithm is very competitive, and often superior, compared to the other algorithms. In particular, NCHHO provides 92% better results in average to solve the uni-modal and multi-modal functions with problem dimension sizes of D = 30 and 50, whereas, with respect to the higher dimension problem, our proposed algorithm shows 100% consistent improvement with D = 100 and 1000 compared to other algorithms. In solving the IoV problem, the success rate was 62.5%, which is substantially better in comparison with the state-of-the-art algorithms. To this end, the proposed NCHHO algorithm in this paper demonstrates a promising method to be widely used by different applications, which brings benefits to industries and businesses in solving their optimization problems experienced daily , such as resource allocation, information retrieval, finding the optimal path for sending data over networks, path planning, and so many other applications.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jss.2021.110993,Journal,Journal of Systems and Software,scopus,2021-09-01,sciencedirect,Automated defect prioritization based on defects resolved at various project periods,https://api.elsevier.com/content/abstract/scopus_id/85107687766,"Defect prioritization is mainly a manual and error-prone task in the current state-of-the-practice. We evaluated the effectiveness of an automated approach that employs supervised machine learning. We used two alternative techniques, namely a Naive Bayes classifier and a Long Short-Term Memory model. We performed an industrial case study with a real project from the consumer electronics domain. We compiled more than 15,000 issues collected over 3 years. We could reach an accuracy level up to 79.36% and we had 3 observations. First, Long Short-Term Memory model has a better accuracy when compared with a Naive Bayes classifier. Second, structured features lead to better accuracy compared to textual descriptions. Third, accuracy is not improved by considering increasingly earlier defects as part of the training data. Increasing the size of the training data even decreases the accuracy compared to the results, when we use data only regarding the recently resolved defects.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jnca.2021.103116,Journal,Journal of Network and Computer Applications,scopus,2021-09-01,sciencedirect,Traffic Engineering in Hybrid Software Defined Network via Reinforcement Learning,https://api.elsevier.com/content/abstract/scopus_id/85107660469,"The emergence of Software Defined Network (SDN) provides a centralized and flexible approach to route network flows. Due to the technical and economic challenges in upgrading to a fully SDN-enabled network, hybrid SDN, with a partial deployment of SDN switches in a traditional network, has been a prevailing network architecture. Meanwhile, Traffic Engineering (TE) in the hydbrid SDN has attracted wide attentions from academia and industry. Previous studies on TE in the hybrid SDN are either traffic-oblivious or time-consuming, which causes routing schemes failed in responding to the dynamically-changing traffic rapidly and intelligently. Therefore, in this paper, we propose a Reinforcement Learning (RL) based method, which learns a traffic-splitting agent to address the dynamically-changing traffic and achieve the link load balancing in the hybrid SDN. Specifically, to rapidly and intelligently determine a routing scheme to the new traffic demands, a traffic-splitting agent is designed and learnt offline by exploiting the RL algorithm to establish the direct relationship between traffic demands and traffic-splitting policies. Once the traffic-splitting agent is learnt, the effective traffic-splitting policies, which are used to determine the traffic-splitting ratios on SDN switches, can be generated rapidly. Additionally, to meet the interactive requirements for learning a traffic-splitting agent, a reasonable simulation environment is proposed to be constructed to avoid routing loops when traffic-splitting policies are taken. Extensive evaluations on different topologies and real traffic demands demonstrate that the proposed method achieves the comparable network performance and performs superiorities in rapidly generating the satisfying routing schemes.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.scs.2021.103009,Journal,Sustainable Cities and Society,scopus,2021-09-01,sciencedirect,Applying machine learning in intelligent sewage treatment: A case study of chemical plant in sustainable cities,https://api.elsevier.com/content/abstract/scopus_id/85106305327,"Nowadays, sewage treatment in sustainable cities attracts more researchers both from academic and industrial communities. Especially, since industrial sewage is normally highly toxic, which could cause serious pollution in a city and lead to health problems of residents, it is critical to monitor and predictably maintain sewage treatment facilities in cities. This paper presents an intelligent sewage treatment system based on machine learning and Internet of Things sensors to assist to manage the sewage treatment in a fine chemical plant. The implemented system has operated for twenty months, acquired multi-dimension data such as temperatures in different treatment processes, operation parameters of devices, and real-time Chemical Oxygen Demand (COD). Since the change trend of outflow COD is highly related to operation status, this paper innovatively uses different types of temperature and water inflow data as model inputs and applies three algorithms to make prediction, which are Support Vector Regression (SVR), Long Short-Term Memory (LSTM) neural network, and Gated Recurrent Unit (GRU) neural network. The experimental results show that GRU model performs better (MAPE = 10.18%, RMSE = 35.67, MAE = 31.16) than LSTM and SVR. This study can be extended to various sewage treatment scenarios in sustainable cities.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.energy.2021.120700,Journal,Energy,scopus,2021-09-01,sciencedirect,Nonlinear generalized predictive controller based on ensemble of NARX models for industrial gas turbine engine,https://api.elsevier.com/content/abstract/scopus_id/85105736036,"New design and operation of modern gas turbine engines (GTEs) are becoming more and more complex where several limitations and control modes should be fulfilled at the same time to accomplish a safe and ideal performance for the engine. For this purpose, a constrained multi-input multi-output (MIMO) non-linear model predictive controller (NMPC) based on neural network model is designed to fulfill the control requirements of a Siemens SGT-A65 three-spool aero-derivative gas turbine engine (ADGTE) used for power generation. However, the implementation of NMPC in real time has two challenges: Firstly, the design of an accurate non-linear model, which can run many times faster than real time. Secondly, the usage of a rapid and reliable optimization algorithm to solve the optimization problem in real time. To solve these issues, the constrained MIMO NMPC is created based on the generalized predictive control (GPC) algorithm as a result of its clarity, ease of use, and capacity to deal with problems in one algorithm. In addition, seven ensembles of eight multi-input single-output (MISO) non-linear autoregressive network with exogenous inputs (NARX) models are used as a base model for the GPC controller to predict the future process outputs. Estimation of free and forced responses of the GPC based on the neural network (NN) model of the plant each sampling time without performing instantaneous linearization is proposed in this study, which reduces the NMPC optimization problem to a linear optimization problem at each sampling step. In addition, the Hildreth's quadratic programming algorithm is used to solve the quadratic optimization problem within the NMPC controller, which offers ease of use and reliability in real time applications. To demonstrate the performance of the NNGPC controller developed in this study, we have compared the performance of the neural network generalized predictive control (NNGPC) controller to the existing controller of the SGT-A65 engine. The simulation results show that the NNGPC has demonstrated output responses with less oscillatory behavior and smoother control actions to the sudden variation in the electric load disturbance than those observed in the existing min-max controller. However, the min-max controller has faster response than that of the NNGPC controller.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2021.107465,Journal,Applied Soft Computing,scopus,2021-09-01,sciencedirect,Click-event sound detection in automotive industry using machine/deep learning,https://api.elsevier.com/content/abstract/scopus_id/85105315919,"In the automotive industry, despite the robotic systems on the production lines, factories continue employing workers in several custom tasks getting for semi-automatic assembly operations. Specifically, the assembly of electrical harnesses of engines comprises a set of connections between electrical components. Despite the task is easy to perform, employees tend not to notice that a few components are not being connected properly due to physical fatigue provoked by repetitive tasks. This yields a low quality of the assembly production line and possible hazards. In this work, we propose a sound detection system based on machine/deep learning (ML/DL) approaches to identify click sounds produced when electrical harnesses are connected. The purpose of this system is to count the number of connections properly made and to feedback to the employees. We collect and release a public dataset of 25,000 click sounds of 25 ms length at 22 kHz during three months of assembly operations in an automotive production line located in Mexico. Then, we design an ML/DL-based methodology for click sound detection of assembled harnesses under real conditions of a noisy environment (noise level ranging from 
                        
                           −
                           16
                           .
                           67
                        
                      dB to 
                        
                           −
                           12
                           .
                           87
                        
                      dB) including other machinery sounds. Our best ML/DL model (i.e., a combination between five acoustic features and an optimized convolutional neural network) is able to detect click sounds in a real assembly production line with an accuracy of 
                        
                           94
                           .
                           55
                           ±
                           0
                           .
                           83
                        
                      %. To the best of our knowledge, this is the first time a click sounds detection system in assembling electrical harnesses of engines for giving feedback to the workers is proposed and implemented in a real-world automotive production line. We consider this work valuable for the automotive industry on how to apply ML/DL approaches for improving the quality of semi-automatic assembly operations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.simpa.2021.100081,Journal,Software Impacts,scopus,2021-08-01,sciencedirect,OpenICS: Open image compressive sensing toolbox and benchmark[Formula presented],https://api.elsevier.com/content/abstract/scopus_id/85115856142,"The real-world application of image compressive sensing is largely limited by the lack of standardization in implementation and evaluation. To address this limitation, we present OpenICS, an image compressive sensing toolbox that implements multiple popular image compressive sensing algorithms into a unified framework with a standardized user interface. Furthermore, a corresponding benchmark is also proposed to provide a fair and complete evaluation of the implemented algorithms. We hope this work can serve the growing research community of compressive sensing and the industry to facilitate the development and application of image compressive sensing.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cag.2021.04.035,Journal,Computers and Graphics (Pergamon),scopus,2021-08-01,sciencedirect,DIMNet: Dense implicit function network for 3D human body reconstruction,https://api.elsevier.com/content/abstract/scopus_id/85105858338,"In recent years, with the improvement of artificial intelligence technology, it has become possible to reconstruct high-precision 3D human body models based on ordinary RGB images. The current 3D human body reconstruction technology requires complex external equipment to scan all angles of the human body, which is complicated to be implemented and cannot be popularized. In order to solve this problem, this paper applies deep learning models on reconstructing 3D human body based on monocular images. First of all, this paper uses Stacked Hourglass network to perform convolution operations on monocular images collected from different views. Then Multi-Layer Perceptrons (MLPs) are used to decode the encoded high-level images. The feature codes in the two views(main and side) are fused, and the interior and exterior points are classified by the fusion features, so as to obtain the corresponding 3D occupancy field. At last, the Marching Cube algorithm is used for 3D reconstruction with a specific threshold and then we use Laplace smoothing algorithm to remove artifacts. This paper proposes a dense sampling strategy based on the important joint points of the human body, which has a certain optimization effect on the realization of high-precision 3D reconstruction. The performance of the proposed scheme has been validated on the open source datasets, MGN dataset and the THuman dataset, provided by Tsinghua University. The proposed scheme can reconstruct features such as clothing folds, color textures, and facial details,and has great potential to be applied in different applications.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2021.114820,Journal,Expert Systems with Applications,scopus,2021-08-01,sciencedirect,Machine Learning for industrial applications: A comprehensive literature review,https://api.elsevier.com/content/abstract/scopus_id/85102967505,"Machine Learning (ML) is a branch of artificial intelligence that studies algorithms able to learn autonomously, directly from the input data. Over the last decade, ML techniques have made a huge leap forward, as demonstrated by Deep Learning (DL) algorithms implemented by autonomous driving cars, or by electronic strategy games. Hence, researchers have started to consider ML also for applications within the industrial field, and many works indicate ML as one the main enablers to evolve a traditional manufacturing system up to the Industry 4.0 level. Nonetheless, industrial applications are still few and limited to a small cluster of international companies. This paper deals with these topics, intending to clarify the real potentialities, as well as potential flaws, of ML algorithms applied to operation management. A comprehensive review is presented and organized in a way that should facilitate the orientation of practitioners in this field. To this aim, papers from 2000 to date are categorized in terms of the applied algorithm and application domain, and a keyword analysis is also performed, to details the most promising topics in the field. What emerges is a consistent upward trend in the number of publications, with a spike of interest for unsupervised and especially deep learning techniques, which recorded a very high number of publications in the last five years. Concerning trends, along with consolidated research areas, recent topics that are growing in popularity were also discovered. Among these, the main ones are production planning and control and defect analysis, thus suggesting that in the years to come ML will become pervasive in many fields of operation management.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2021.114753,Journal,Expert Systems with Applications,scopus,2021-08-01,sciencedirect,A league-winner algorithm for defect classification in an industrial web inspection system,https://api.elsevier.com/content/abstract/scopus_id/85102641846,"This paper presents a modification to be added to multiclass classifiers, that improves their performance when classifying, in this case, defects appearing in polyethylene films. It aims to classify a new defect by confronting every defect type against each of the other types. In a simplified way, the type that results winner in more matches is the type that the defect belongs to. Different ways of implementing neural networks have been tested, using Gradient Descent and techniques for backpropagation. These techniques have been formally and understandably explained. In addition, a method based on decision trees has been included for comparison. Different issues related to the practical implementation of the detection and identification system within an installed production chain are addressed. The resulting system has been incorporated as a real inspection automatism in a polyethylene manufacturing line, and trained with defects previously obtained from the same line.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jclepro.2021.127385,Journal,Journal of Cleaner Production,scopus,2021-07-25,sciencedirect,Improving degradation of real wastewaters with self-heating magnetic nanocatalysts,https://api.elsevier.com/content/abstract/scopus_id/85105709482,"Industrial effluents contain a wide range of organic pollutants that present harmful effects on the environment and deprived communities with no access to clean water. As this organic matter is resistant to conventional treatments, Advanced Oxidation Processes (AOPs) have emerged as a suitable option to counteract these environmental challenges. Engineered iron oxide nanoparticles have been widely tested in AOPs catalysis, but their full potential as magnetic induction self-heating catalysts has not been studied yet on real and highly contaminated industrial wastewaters. In this study we have designed a self-heating catalyst with a finely tuned structure of small cores (10 nm) aggregates to develop multicore particles (40 nm) with high magnetic moment and high colloidal stability. This nanocatalyst, that can be separated by magnetic harvesting, is able to increase reaction temperatures (up to 90 °C at 1 mg/mL suspension in 5 min) under the action of alternating magnetic fields. This efficient heating was tested in the degradation of a model compound (methyl orange) and real wastewaters, such as leachate from a solid landfill (LIX) and colored wastewater from a textile industry (TIW). It was possible to increase reaction rates leading to a reduction of the chemical oxygen demand of 50 and 90%, for TIW and LIX. These high removal and degradation ability of the magnetic nanocatalyst was sustained with the formation of strong reactive oxygen species by a Fenton-like mechanism as proved by electron paramagnetic resonance. These findings represent an important advance for the industrial implementation of a scalable, non-toxic, self-heating catalysts that can certainly enhance AOP for wastewater treatment in a more sustainable and efficient way.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2021.06.013,Conference Proceeding,Procedia Computer Science,scopus,2021-07-01,sciencedirect,Mathematical model of chemical process prediction for industrial safety risk assessment,https://api.elsevier.com/content/abstract/scopus_id/85112600838,The article presents a mathematical model of the functioning of the technological process of styrene production using neural network technologies. The use of a direct propagation neural network with a single hidden layer trained on an experimental sample is considered. An algorithm for forming a neural network is proposed. The model is implemented as a software module. The results of predicting the process of chemical production of styrene based on real data and recommendations for using the developed model in the process of assessing the industrial safety of particularly dangerous production processes are presented.,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmsy.2021.04.005,Journal,Journal of Manufacturing Systems,scopus,2021-07-01,sciencedirect,LearningADD: Machine learning based acoustic defect detection in factory automation,https://api.elsevier.com/content/abstract/scopus_id/85106283308,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.tifs.2021.04.042,Journal,Trends in Food Science and Technology,scopus,2021-07-01,sciencedirect,Efficient extraction of deep image features using convolutional neural network (CNN) for applications in detecting and analysing complex food matrices,https://api.elsevier.com/content/abstract/scopus_id/85105814254,"Background
                  The development of techniques and methods for rapidly and reliably detecting and analysing food quality and safety products is of significance for the food industry. Traditional machine learning algorithms based on handcrafted features normally have poor performance due to their limited representation capacity for complex food characteristics. Recently, the convolutional neural network (CNN) emerges as an effective and potential tool for feature extraction, which is considered the most popular architecture of deep learning and has been increasingly applied for the detection and analysis of complex food matrices.
               
                  Scope and approach
                  In the current review, the structure of CNN, the method of feature extraction based on 1-D, 2-D and 3-D CNN models, and multi-feature aggregation methods are introduced. Applications of CNN as a depth feature extractor for detecting and analyzing complex food matrices are discussed, including meat and aquatic products, cereals and cereal products, fruits and vegetables, and others. In addition, data sources, model architecture and overall performance of CNN with other existing methods are compared, and trends of future studies on applying CNN for food detection and analysis are also highlighted.
               
                  Key findings and conclusions
                  CNN combined with nondestructive detection techniques and computer vision system show great potential for effectively and efficiently detecting and analysing complex food matrices, and the features based on CNN show better performance and outperform the features handcrafted or those extracted by machine learning algorithms. Although there still remains some challenges in using CNN, it is expected that CNN models will be deployed on mobile devices for real-time detection and analysis of food matrices in future.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ins.2021.01.013,Journal,Information Sciences,scopus,2021-07-01,sciencedirect,Attributed community search based on effective scoring function and elastic greedy method,https://api.elsevier.com/content/abstract/scopus_id/85101624086,"In recent years, with the proliferation of rich attribute information available for entities in real-world networks and the increasing demand for more personalized community searches, attributed community search (ACS), an upgraded version of the community search problem, has attracted great attention from the both academic and industry areas. Some algorithms have been proposed to solve this novel research problem. However, they have a deficiency in evaluating the quality of the attributed community structure, which may mislead them and discover less valuable structures. In this paper, we make up for this defect, and propose the SFEG algorithm to better solve the ACS problem. SFEG designs a more effective scoring function to measure the quality of the discovered attributed community structure, and presents an elastic greedy optimization method to quickly maximize the function value to determine the target community with a specific meaning. The extensive experiments conducted on the attributed graph datasets with ground-truth communities show that our algorithm significantly outperforms the state-of-the-art.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ymssp.2020.107510,Journal,Mechanical Systems and Signal Processing,scopus,2021-06-16,sciencedirect,Metric-based meta-learning model for few-shot fault diagnosis under multiple limited data conditions,https://api.elsevier.com/content/abstract/scopus_id/85100211264,"The real-world large industry has gradually become a data-rich environment with the development of information and sensor technology, making the technology of data-driven fault diagnosis acquire a thriving development and application. The success of these advanced methods depends on the assumption that enough labeled samples for each fault type are available. However, in some practical situations, it is extremely difficult to collect enough data, e.g., when the sudden catastrophic failure happens, only a few samples can be acquired before the system shuts down. This phenomenon leads to the few-shot fault diagnosis aiming at distinguishing the failure attribution accurately under very limited data conditions. In this paper, we propose a new approach, called Feature Space Metric-based Meta-learning Model (FSM3), to overcome the challenge of the few-shot fault diagnosis under multiple limited data conditions. Our method is a mixture of general supervised learning and episodic metric meta-learning, which will exploit both the attribute information from individual samples and the similarity information from sample groups. The experiment results demonstrate that our method outperforms a series of baseline methods on the 1-shot and 5-shot learning tasks of bearing and gearbox fault diagnosis across various limited data conditions. The time complexity and implementation difficulty have been analyzed to show that our method has relatively high feasibility. The feature embedding is visualized by t-SNE to investigate the effectiveness of our proposed model.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.chemolab.2021.104314,Journal,Chemometrics and Intelligent Laboratory Systems,scopus,2021-06-15,sciencedirect,A scalable approach for the efficient segmentation of hyperspectral images,https://api.elsevier.com/content/abstract/scopus_id/85105360467,"The number of applications of hyperspectral imaging (HSI) is steadily increasing, as technology evolves and cameras become more affordable. However, the volume of data in a hyperspectral image is large (order of Gigabytes) and standard off-the-shelf algorithms for multi-channel image analysis cannot be readily applied, due to the prohibitive computational time and large memory requirements. Therefore, new scalable approaches are required to perform hyperspectral image analysis. In this article we address an efficient methodology for conducting Unsupervised Image Segmentation – one of the basic and most fundamental image analysis operations. In the methodology proposed, unsupervised segmentation is conducted after transforming the spectral and spatial dimensions of the raw hyperspectral image into a more compact representation using multivariate and multiresolution techniques. The clusters identified in the compact image representation are then used to train a discriminative classifier. The classifier is then adapted and transferred for application to the raw image, where it will efficiently label all the original pixels. With the proposed methodology, the computational expensive operations (unsupervised clustering and classifier learning) are minimized, whereas the efficient implementation of the classifier guarantees the analysis at the native resolution. The effectiveness of the proposed methodology was tested on a real case study considering an industrial hyperspectral image capturing the reflectance spectrum for several objects made of different unknown materials. A significant reduction in the computational cost was achieved without compromising the quality of the unsupervised segmentation, demonstrating the potential of the proposed approach.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.addma.2021.101961,Journal,Additive Manufacturing,scopus,2021-06-01,sciencedirect,Deep representation learning for process variation management in laser powder bed fusion,https://api.elsevier.com/content/abstract/scopus_id/85105695571,"Laser Powder Bed Fusion (LPBF) is an additive manufacturing process where laser power is applied to fuse the spread powder and fabricate industrial parts in a layer by layer fashion. Despite its great promise in fabrication flexibility, print quality has long been a major barrier for its widespread implementation. Traditional offline post-manufacturing inspections to detect the defects in finished products are expensive and time-consuming and thus cannot be applied in real-time monitoring and control. In-situ monitoring methods by relying on the in-process sensor data, on the other hand, can provide viable alternatives to aid with the online detection of anomalies during the process. Given the crucial importance of melt pool characteristics to the quality of final products, this paper provides a framework to process the melt pool images by a configuration of Convolutional Auto-Encoder (CAE) neural networks. The network’s corresponding bottleneck layer learns a deep yet low-dimensional representation from melt pools while preserving the spatial correlation and complex features intrinsic in the images. As opposed to the manual annotation of data by X-ray imaging or destructive tests, an agglomerative clustering algorithm is applied to these representations to automatically extract the anomalies and annotate the data accordingly. A control charting scheme based on Hotelling’s T
                     2 and S
                     2 statistics is then developed to monitor the process’s stability by keeping track of the learned representations and residuals obtained from the reconstruction of original images. Testing the proposed methodology on the collected data from an experimental build demonstrates that the method can extract a set of complex features that are inextricable otherwise by using hand-crafted feature engineering methods. Moreover, through extensive numerical studies, it is shown that the proposed feature extraction and statistical process monitoring scheme is capable of detecting the anomalies in real-time with accuracy and F
                     1 score of about 95% and 82%, respectively.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compeleceng.2021.107121,Journal,Computers and Electrical Engineering,scopus,2021-06-01,sciencedirect,Efficient neural networks for edge devices,https://api.elsevier.com/content/abstract/scopus_id/85103242184,"Due to limited computation and storage resources of industrial internet of things (IoT) edge devices, many emerging intelligent industrial IoT applications based on deep neural networks (DNNs) heavily depend on cloud computing for computation and storage. However, cloud computing faces technical issues in long latency, poor reliability, and weak privacy, resulting in the need for on-device computation and storage. On-device computation is essential for many time-critical industrial IoT applications, which require real-time data processing. In this paper, we review three major research areas for on-device computation, specifically quantization, pruning, and network architecture design. The three techniques could enable a DNN model to be deployed on edge devices for real-time computation and storage, mainly due to the reduction of computation and space complexity. More importantly, these techniques could make DNNs applicable to industrial IoT devices.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.renene.2021.03.008,Journal,Renewable Energy,scopus,2021-06-01,sciencedirect,Intelligent energy management based on SCADA system in a real Microgrid for smart building applications,https://api.elsevier.com/content/abstract/scopus_id/85102248554,"Energy management is one of the main challenges in Microgrids (MGs) applied to Smart Buildings (SBs). Hence, more studies are indispensable to consider both modeling and operating aspects to utilize the upcoming results of the system for the different applications. This paper presents a novel energy management architecture model based on complete Supervisory Control and Data Acquisition (SCADA) system duties in an educational building with an MG Laboratory (Lab) testbed, which is named LAMBDA at the Electrical and Energy Engineering Department of the Sapienza University of Rome. The LAMBDA MG Lab simulates in a small scale a SB and is connected with the DIAEE electrical network. LAMBDA MG is composed of a Photovoltaic generator (PV), a Battery Energy Storage System (BESS), a smart switchboard (SW), and different classified loads (critical, essential, and normal) some of which are manageable and controllable (lighting, air conditioning, smart plugs operating into the LAB). The aim of the LAMBDA implementation is making the DIAEE smart for energy saving purposes. In the LAMBDA Lab, the communication architecture consists in a complex of master/slave units and actuators carried out by two main international standards, Modbus (industrial serial standard for electrical and technical monitoring systems) and Konnex (an open standard for commercial and domestic building automation). Making the electrical department smart causes to reduce the required power from the main grid. Hence, to achieve the aims, results have been investigated in two modes. Initially, the real-time mode based on the SCADA system, which reveals real daily power consumption and production of different sources and loads. Next, the simulation part is assigned to shows the behavior of the main grid, loads and BESS charging and discharging based on energy management system. Finally, the proposed model has been examined in different scenarios and evaluated from the economic aspect.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cja.2020.09.011,Journal,Chinese Journal of Aeronautics,scopus,2021-06-01,sciencedirect,Framework and development of data-driven physics based model with application in dimensional accuracy prediction in pocket milling,https://api.elsevier.com/content/abstract/scopus_id/85097765922,"In the manufacturing of thin wall components for aerospace industry, apart from the side wall contour error, the Remaining Bottom Thickness Error (RBTE) for the thin-wall pocket component (e.g. rocket shell) is of the same importance but overlooked in current research. If the RBTE reduces by 30%, the weight reduction of the entire component will reach up to tens of kilograms while improving the dynamic balance performance of the large component. Current RBTE control requires the off-process measurement of limited discrete points on the component bottom to provide the reference value for compensation. This leads to incompleteness in the remaining bottom thickness control and redundant measurement in manufacturing. In this paper, the framework of data-driven physics based model is proposed and developed for the real-time prediction of critical quality for large components, which enables accurate prediction and compensation of RBTE value for the thin wall components. The physics based model considers the primary root cause, in terms of tool deflection and clamping stiffness induced Axial Material Removal Thickness (AMRT) variation, for the RBTE formation. And to incorporate the dynamic and inherent coupling of the complicated manufacturing system, the multi-feature fusion and machine learning algorithm, i.e. kernel Principal Component Analysis (kPCA) and kernel Support Vector Regression (kSVR), are incorporated with the physics based model. Therefore, the proposed data-driven physics based model combines both process mechanism and the system disturbance to achieve better prediction accuracy. The final verification experiment is implemented to validate the effectiveness of the proposed method for dimensional accuracy prediction in pocket milling, and the prediction accuracy of AMRT achieves 0.014 mm and 0.019 mm for straight and corner milling, respectively.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.apenergy.2021.116688,Journal,Applied Energy,scopus,2021-05-15,sciencedirect,Advanced price forecasting in agent-based electricity market simulation,https://api.elsevier.com/content/abstract/scopus_id/85102042154,"Machine learning and agent-based modeling are two popular tools in energy research. In this article, we propose an innovative methodology that combines these methods. For this purpose, we develop an electricity price forecasting technique using artificial neural networks and integrate the novel approach into the established agent-based electricity market simulation model PowerACE. In a case study covering ten interconnected European countries and a time horizon from 2020 until 2050 at hourly resolution, we benchmark the new forecasting approach against a simpler linear regression model as well as a naive forecast. Contrary to most of the related literature, we also evaluate the statistical significance of the superiority of one approach over another by conducting Diebold–Mariano hypothesis tests. Our major results can be summarized as follows. Firstly, in contrast to real-world electricity price forecasts, we find the naive approach to perform very poorly when deployed model-endogenously (mean absolute percentage error 0.40–0.53). Secondly, although the linear regression performs reasonably well (mean absolute percentage error 0.17–0.32), it is outperformed by the neural network approach (mean absolute percentage error 0.17–0.21). Thirdly, the use of an additional classifier for outlier handling substantially improves the forecasting accuracy, particularly for the linear regression approach. Finally, the choice of the model-endogenous forecasting method has a clear impact on simulated electricity prices. This latter finding is particularly crucial since these prices are a major results of electricity market models.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.comnet.2021.107955,Journal,Computer Networks,scopus,2021-05-08,sciencedirect,Elastic Computing Resource Virtualization Method for a Service-centric Industrial Internet of Things,https://api.elsevier.com/content/abstract/scopus_id/85102477910,"The industrial Internet of Things (IIoT) enables the interconnection of machines, devices, resources, and computing technologies to improve the reliability of manufacturing services. The role of Software-Defined Networks (SDNs) and Network Function Virtualization (NFV) are exploited in the IIoT environment to ensure effective management and computing resource utilization. Based on the SDN and NFV paradigms, this article introduces a novel elastic computing resource virtualization (ECRV) method to improve the flexibility of resource management in the IIoT. The need for virtualization is obtained by identifying the control and process platforms used in industrial task management. Support vector machine-based classification learning is used to achieve balanced identification, and prevents unnecessary distribution of limited resources, Support vector machine helps to retain flexibility in task control processes that use available industrial resources. By separating the process and control platforms, service dissemination is improved and backlogs in task processing are decreased. The proposed method could provide flexible virtualization and reduces the service response time and task failure.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eti.2021.101527,Journal,Environmental Technology and Innovation,scopus,2021-05-01,sciencedirect,Barriers to the digitalisation and innovation of Australian Smart Real Estate: A managerial perspective on the technology non-adoption,https://api.elsevier.com/content/abstract/scopus_id/85104052467,"The real estate sector brings a fortune to the global economy. But, presently, this sector is regressive and uses traditional methods and approaches. Therefore, it needs a technological transformation and innovation in line with the Industry 4.0 requirements to transform into smart real estate. However, it faces the barriers of disruptive digital technology (DDT) adoption and innovation that need effective management to enable such transformation. These barriers present managerial challenges that affect DDT adoption and innovation in smart real estate. The current study assesses these DDTs adoption and innovation barriers facing the Australian real estate sector from a managerial perspective. Based on a comprehensive review of 72 systematically retrieved and shortlisted articles, we identify 21 key barriers to digitalisation and innovation. The barriers are grouped into the technology-organisation-external environment (TOE) categories using a Fault tree. Data is collected from 102 real estate and property managers to rate and rank the identified barriers. The results show that most of the respondents are aware of the DDTs and reported AI (22.5% of respondents), big data (12.75%) and VR (12.75%) as the most critical technologies not adopted so far due to costs, organisation policies, awareness, reluctance, user demand, tech integration, government support and funding. Overall, the highest barrier (risk) scores are observed for high costs of software and hardware (T1), high complexity of the selected technology dissemination system (T2) and lack of government incentives, R&D support, policies, regulations and standards (E1). Among the TOE categories, as evident from the fault tree analysis, the highest percentage of failure to adopt the DDT is attributed to E1 in the environmental group. For the technological group, the highest failure reason is attributed to T2. And for the organisational group, the barrier with the highest failure chances for DDT adoption is the lack of organisational willingness to invest in digital marketing (O4). These barriers must be addressed to pave the way for DDT adoption and innovation in the Australian real estate sector and move towards smart real estate.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2020.114399,Journal,Expert Systems with Applications,scopus,2021-05-01,sciencedirect,A reinforcement learning-based algorithm for the aircraft maintenance routing problem,https://api.elsevier.com/content/abstract/scopus_id/85098682553,"With recent developments in the airline industry worldwide, the competition among the industry has increased largely with many key players in the market. In order to generate profits, the industry has paid much attention to generate optimal routes that are maintenance feasible. The main aim of operational aircraft maintenance routing problem (OAMRP) is to generate these optimal routes for each aircraft that are maintenance feasible and follow the constraints defined by the Federal Aviation Administration (FAA). In this paper, the OAMRP is studied with two main objectives. First, to propose a formulation of a network flow-based Integer Linear Programming (ILP) framework for the OAMRP that considers three main maintenance constraints simultaneously: maximum flying-hour, limit on the number of take-offs between two consecutive maintenance checks and the work-force capacity. Second, to develop a new reinforcement learning-based algorithm which can be used to solve the problem, quickly and efficiently, as compared to commonly available optimization software. Finally, the evaluation of the proposed algorithm on real case datasets obtained from a major airline located in the Middle East verifies that the algorithm generates high-quality solutions quickly for both medium and large-scale flight schedule dataset.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmsy.2021.02.012,Journal,Journal of Manufacturing Systems,scopus,2021-04-01,sciencedirect,Robust diagnosis with high protection to gas turbine failures identification based on a fuzzy neuro inference monitoring approach,https://api.elsevier.com/content/abstract/scopus_id/85101807612,"Modern industry requires the development of new monitoring and diagnostic procedures, which enable the detection, localization, and isolation of faults. For sustainable solutions in terms of operational safety and availability, while bringing out zero accidents, zero downtime, and zero faults, for a trend acting on environmental issues. Towards this development, this work proposes solutions for the monitoring of gas turbines and their real-time implementation, in order to approximate and predict the degradation of the components of this system, by an approach of faults detection and isolation, based on an adaptive neural-fuzzy inference system. This will develop a reliable approach to maintain and monitor gas turbines, in case of failure or accident to prevent in real-time and makes it possible to achieve high power with efficiency and small footprint with High performance by operating this rotating machine. However, the application of the Adaptive Neuro-Fuzzy Inference System Observer-Based Approach, makes it possible to increase the life of the examined turbine and keep better reliability for their monitoring system and satisfy the techno-economic and environmental performance impacts. For the purpose of controlling failures and the occurrence of turbine system malfunctions, and avoiding their consequences on the safety and productivity of the installation.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.enconman.2021.113856,Journal,Energy Conversion and Management,scopus,2021-04-01,sciencedirect,The mutual benefits of renewables and carbon capture: Achieved by an artificial intelligent scheduling strategy,https://api.elsevier.com/content/abstract/scopus_id/85101129959,"Renewable power and carbon capture are key technologies to transfer the power industry into low carbon generation. Renewables have been developed fast, however, the intermittent nature has imposed higher requirement for the flexibility of the power grid. Retrofitting carbon capture technologies to existing fossil-fuel fired power plants is an important solution to avoid the “lock-in” of emissions, but the high operating costs hinders their large scale application. The coexistence of renewable power and carbon capture opens up a new avenue that the deployment of carbon capture can provide additional flexibility for better accommodation of renewable power while excess renewables can be used to reduce the operating costs of carbon capture. To this end, this paper proposes an artificial intelligence based optimal scheduling strategy for the power plant-carbon capture system in the context of renewable power penetration to show that the mutual benefits between carbon capture and renewable power can be achieved when the carbon capture process is made fully adjustable. An artificial intelligent deep belief neural network is used to reflect the complex interactions between carbon, heat and electricity within the power plant carbon capture system. Multiple operating goals are considered in the scheduling such as minimizing the operating costs, renewable power curtailment and carbon emission, and the particle swarm heuristic optimization is employed to find the optimal solution. The impacts of carbon capture constraint mode, carbon emission penalty coefficient, carbon dioxide production constraints and renewable power installed capacity are investigated to provide broader insight on the potential benefit of carbon capture in future low-carbon energy system. A case study using real world data of weather condition and load demand shows that renewable power curtailment can be reduced by 51% with the integration of post-combustion capture systems and 35% of total carbon emission are captured by the use of excess renewable power through optimal scheduling. This paper points out a new way of using artificial intelligent technologies to coordinate the couplings between carbon and electricity for efficient and environmentally friendly operation of future low-carbon energy system.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2020.107069,Journal,Applied Soft Computing,scopus,2021-04-01,sciencedirect,Deep learning feature exploration for Android malware detection,https://api.elsevier.com/content/abstract/scopus_id/85098947132,"Android mobile devices and applications are widely deployed and used in industry and smart city. Malware detection is one of the most powerful and effective approaches to guarantee security of Android systems, especially for industrial platform and smart city. Recently, researches using machine learning-based techniques for Android malware detection increased rapidly. Nevertheless, most of the appeared approaches have to perform feature analysis and selection, so-called feature engineering, which is time-consuming and relies on artificial experience. To solve the inefficiency problem of feature engineering, we propose TC-Droid, an automatic framework for Android malware detection based on text classification method. The core idea of TC-Droid is derived from the field of text classification. TC-Droid feeds on the text sequence of APPs analysis reports generated by AndroPyTool, applies a convolutional neural network (CNN) to explore significant information (or knowledge) under original report text, instead of manual feature engineering. In an evaluation with different number of real-world samples, TC-Droid outperforms state-of-the-art model (Drebin) and several classic models (NB, LR, KNN, RF) as well. With multiple experimental settings and corresponding comparisons, TC-Droid achieves effective and flexible performance in Android malware detection task.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2020.10.097,Journal,Neurocomputing,scopus,2021-03-21,sciencedirect,3D-RVP: A method for 3D object reconstruction from a single depth view using voxel and point,https://api.elsevier.com/content/abstract/scopus_id/85097471582,"Three-dimensional object reconstruction technology has a wide range of applications such as augment reality, virtual reality, industrial manufacturing and intelligent robotics. Although deep learning-based 3D object reconstruction technology has developed rapidly in recent years, there remain important problems to be solved. One of them is that the resolution of reconstructed 3D models is hard to improve because of the limitation of memory and computational efficiency when deployed on resource-limited devices. In this paper, we propose 3D-RVP to reconstruct a complete and accurate 3D geometry from a single depth view, where R, V and P represent Reconstruction, Voxel and Point, respectively. It is a novel two-stage method that combines a 3D encoder-decoder network with a point prediction network. In the first stage, we propose a 3D encoder-decoder network with residual learning to output coarse prediction results. In the second stage, we propose an iterative subdivision algorithm to predict the labels of adaptively selected points. The proposed method can output high-resolution 3D models by increasing a small number of parameters. Experiments are conducted on widely used benchmarks of a ShapeNet dataset in which four categories of models are selected to test the performance of neural networks. Experimental results show that our proposed method outperforms the state-of-the-arts, and achieves about 
                        
                           2.7
                           %
                        
                      improvement in terms of the intersection-over-union metric.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.fbp.2020.12.009,Journal,Food and Bioproducts Processing,scopus,2021-03-01,sciencedirect,Study of Galactooligosaccharides production from dairy waste by FTIR and chemometrics as Process Analytical Technology,https://api.elsevier.com/content/abstract/scopus_id/85099356128,"Galactooligosaccharides (GOS) production from whey, a relevant by-product of dairy industry, answers to the Circular Economy principle of extending the life cycle of products. Indeed, it allows the reuse of dairy waste to produce prebiotics to be used in functional food preparations. For this purpose, the effective monitoring of GOS production should be performed in real time and by environmentally friendly techniques. Thus, FTIR spectroscopy, combined with different chemometric approaches, has been tested to assess a Process Analytical Technology to follow GOS production from cheese whey. Partial Least Square regression models were reliable for lactose, glucose and galactose determination (Root Mean Square Error of Prediction of 21.9, 11.1 and 12.4 mg mL−1, respectively). Furthermore, Multivariate Curve Resolution – Alternating Least Square models were proposed to describe trends of the reaction components along the process being an interesting alternative to chromatographic determinations. The real time implementation of the proposed approach will provide the dairy industry with a reliable and green Process Analytical Technology for dairy waste reallocation, avoiding sample pre-processing, large use of organic solvents and long times of analysis.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.future.2020.10.031,Journal,Future Generation Computer Systems,scopus,2021-03-01,sciencedirect,DISCERNER: Dynamic selection of resource manager in hyper-scale cloud-computing data centres,https://api.elsevier.com/content/abstract/scopus_id/85096157784,"Data centres constitute the engine of the Internet, and run a major portion of large web and mobile applications, content delivery and sharing platforms, and Cloud-computing business models. The high performance of such infrastructures is therefore critical for their correct functioning. This work focuses on the improvement of data-centre performance by dynamically switching the main data-centre governance software system: the resource manager. Instead of focusing on the development of new resource-managing models as soon as new workloads and patterns appear, we propose DISCERNER, a decision-theory model that can learn from numerous data-centre execution logs to determine which existing resource-managing model may optimise the overall performance for a given time period. Such a decision-theory system employs a classic machine-learning classifier to make real-time decisions based on past execution logs and on the current data-centre operational situation. A set of extensive and industry-guided experiments has been simulated by a validated data-centre simulation tool. The results obtained show that the values of key performance indicators may be improved by at least 20% in realistic scenarios.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.future.2020.10.018,Journal,Future Generation Computer Systems,scopus,2021-03-01,sciencedirect,Large-scale online multi-view graph neural network and applications,https://api.elsevier.com/content/abstract/scopus_id/85095762066,"Recently popularized Graph Neural Network (GNN) has been attaching great attention along with its successful industry applications. This paper focuses on two challenges traditional GNN frameworks face: (i) most of them are transductive and mainly concentrate on homogeneous networks considering single typed nodes and edges; (ii) they are difficult to handle the real-time changing network structures as well as scale to big graph data. To address these issues, a novel attention-based Heterogeneous Multi-view Graph Neural Network (aHMGNN) solution is introduced. aHMGNN models a more intricate heterogeneous multi-view network, where various node and edge types co-exist and each of these objects also contain specific attributes. It is end-to-end, and two stages are designed for node embeddings learning and multi-typed node and edge representations fusion, respectively. Experimental studies on large-scale spam detection and link prediction tasks clearly verify the efficiency and effectiveness of our proposed aHMGNN. Furthermore, we have implemented our approach in one of the largest e-commerce platforms which further verifies that aHMGNN is arguably promising and scalable in real-world applications.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.knosys.2020.106679,Journal,Knowledge-Based Systems,scopus,2021-02-15,sciencedirect,Federated learning for machinery fault diagnosis with dynamic validation and self-supervision,https://api.elsevier.com/content/abstract/scopus_id/85098734354,"Intelligent data-driven machinery fault diagnosis methods have been successfully and popularly developed in the past years. While promising diagnostic performance has been achieved, the existing methods generally require large amounts of high-quality supervised data for training, which are mostly difficult and expensive to collect in real industries. Therefore, it is motivated that the distributed data of multiple clients can be integrated and exploited to build a powerful data-driven model. However, that basically requires data sharing among different users, and is not preferred in most industrial cases due to potential conflict of interests. In order to address the data island problem, a federated learning method for machinery fault diagnosis is proposed in this paper. Model training is locally implemented within each participated client, and a self-supervised learning scheme is proposed to enhance the learning performance. The server aggregates the locally updated models in each training round under the dynamic validation scheme, and a global fault diagnosis model can be established. Only the models are mutually communicated rather than the data, which ensures data privacy among different clients. The experiments on two datasets suggest the proposed method offers a promising approach on confidential decentralized learning.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.abb.2020.108730,Journal,Archives of Biochemistry and Biophysics,scopus,2021-02-15,sciencedirect,Artificial intelligence in the early stages of drug discovery,https://api.elsevier.com/content/abstract/scopus_id/85098095696,"Although the use of computational methods within the pharmaceutical industry is well established, there is an urgent need for new approaches that can improve and optimize the pipeline of drug discovery and development. In spite of the fact that there is no unique solution for this need for innovation, there has recently been a strong interest in the use of Artificial Intelligence for this purpose. As a matter of fact, not only there have been major contributions from the scientific community in this respect, but there has also been a growing partnership between the pharmaceutical industry and Artificial Intelligence companies. Beyond these contributions and efforts there is an underlying question, which we intend to discuss in this review: can the intrinsic difficulties within the drug discovery process be overcome with the implementation of Artificial Intelligence? While this is an open question, in this work we will focus on the advantages that these algorithms provide over the traditional methods in the context of early drug discovery.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.patter.2020.100195,Journal,Patterns,scopus,2021-02-12,sciencedirect,Topic classification of electric vehicle consumer experiences with transformer-based deep learning,https://api.elsevier.com/content/abstract/scopus_id/85100638713,"The transportation sector is a major contributor to greenhouse gas (GHG) emissions and is a driver of adverse health effects globally. Increasingly, government policies have promoted the adoption of electric vehicles (EVs) as a solution to mitigate GHG emissions. However, government analysts have failed to fully utilize consumer data in decisions related to charging infrastructure. This is because a large share of EV data is unstructured text, which presents challenges for data discovery. In this article, we deploy advances in transformer-based deep learning to discover topics of attention in a nationally representative sample of user reviews. We report classification accuracies greater than 91% (F1 scores of 0.83), outperforming previously leading algorithms in this domain. We describe applications of these deep learning models for public policy analysis and large-scale implementation. This capability can boost intelligence for the EV charging market, which is expected to grow to US$27.6 billion by 2027.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijepes.2021.107505,Journal,International Journal of Electrical Power and Energy Systems,scopus,2021-02-01,sciencedirect,Global sensitivity analysis for a real-time electricity market forecast by a machine learning approach: A case study of Mexico,https://api.elsevier.com/content/abstract/scopus_id/85113278481,"The study presents the hybridization of global sensitivity analysis with data-driven techniques to evaluate the Mexican electricity market interaction and assess the impact of individual parameters concerning locational marginal prices. The study case pertains to Yucatan, Mexico's electricity grid and market characteristics. A comparison of three artificial intelligence techniques in the electricity market is presented to forecast electricity prices in real-time market conditions. The study contemplates exogenous input parameters classified as regional, operational, meteorological, and economic indicators. A sensitivity analysis was carried out to the model with the best performance of the Artificial Intelligence techniques. The results showed that the impact of the variables fluctuates according to market and consumption conditions. In this study, the most relevant variables were electricity generation (17.06%), fossil fuel costs (natural gas 12.54% and diesel 8.63%), load zone (11.17%), and the day of the year (8.51%). From the qualitative point of view, the complex behavior of the parameters was analyzed; moreover, the quantitative results weighted the relevance of the variables in the Locational Marginal Prices. The meteorological and economic parameters allow assessing the environment where it interacts and serves as an instrument for decision-making in the planning of the energy sector. The presented methodology can be implemented as an alternative tool for market participants to analyze electricity prices.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmapro.2020.12.050,Journal,Journal of Manufacturing Processes,scopus,2021-02-01,sciencedirect,Online tool condition monitoring for ultrasonic metal welding via sensor fusion and machine learning,https://api.elsevier.com/content/abstract/scopus_id/85099501543,"In ultrasonic metal welding (UMW), tool wear significantly affects the weld quality and tool maintenance constitutes a substantial part of production cost. Thus, tool condition monitoring (TCM) is crucial for UMW. Despite extensive literature focusing on TCM for other manufacturing processes, limited studies are available on TCM for UMW. Existing TCM methods for UMW require offline high-resolution measurement of tool surface profiles, which leads to undesirable production downtime and delayed decision-making. This paper proposes a completely online TCM system for UMW using sensor fusion and machine learning (ML) techniques. A data acquisition (DAQ) system is designed and implemented to obtain in-situ sensing signals during welding processes. A large feature pool is then extracted from the sensing signals. A subset of features are selected and subsequently used by ML-based classification models. A variety of classification models are trained, validated, and tested using experimental data. The best-performing classification models can achieve close to 100% classification accuracy for both training and test datasets. The proposed TCM system not only provides real-time TCM for UMW but also can support optimal decision-making in tool maintenance. The TCM system can be extended to predict remaining useful life (RUL) of tools and integrated with a controller to adjust welding parameters accordingly.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.micpro.2020.103628,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Consumer decision-making and smart logistics planning based on FPGA and convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85097718891,"In the fourth Industrial Revolution, cost-effective planning and rational management were the key to the success of the revolution. This paper mainly studies the development and application of models in machine learning technology. The abnormal activities monitored in real time are rectified so that the customer's electronic orders can be displayed through the support of big data, thus laying the foundation for the development of intelligent logistics. Under the data system, an exception model is created and classified and regressed. In this model, the security and stability of customer orders in the network can be automatically detected, and the abnormal data can be analyzed and evaluated. Unusual circumstances of this kind need to be in an intelligent logistics environment, and delivery tasks must be called intuitive for special care. Early detection of abnormal order events is expected to improve the accuracy of delivery planning. To enable new technical solutions, the logistics industry and economic decision-makers often lack the IT background and expertise needed to start developing new systems and technical solutions. Evaluate the benefits of using. Implementation and integration complexity is seen as one of the three major obstacles to the success of the IoT above. This is by hindering long-term investment in new technologies from slowing down digitization.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.micpro.2020.103594,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Enterprise financial risk management platform based on 5 G mobile communication and embedded system,https://api.elsevier.com/content/abstract/scopus_id/85097578772,"5 G technology has been applied to the financial sector. As a mortgage and supervision of sensors, network cameras, mobile device to improve the financial performance of real-time data generated by the data, bank loan credit risk management has been used. There are many risks of financial credit in modern society, the most important of which is the financial danger on mobile Internet. In the case of mobile phone payment popularity, financial risk has also greatly improved. This makes the traditional statistics and models can not fully meet the needs of the development of modern society. Bank credit risk has also improved to some extent. Therefore, there is a practical need for a more robust risk prediction model of artificial intelligence to predict the default behavior with good accuracy and competency-based big data analytics. This paper presents data mining method optimization and 5 G mobile communications and embedded systems commercial banks, based on financial risk management. It is safe to protect personal privacy, consider these requirements 5 G system design. To successfully connect to the ability to make money, telecom service providers need to ask their players to match their products and the industry. In addition to simple connections, they need a unified high-level function, such as coordination of network resources, analytical capabilities, and automated business and operations. Mob ileum provides Business Assurance Analytics to improve and develop a strong customer value proposition during 5 G technologies deployment. Experimental results show that the risk management models have fast convergence, powerful forecasting capabilities, and effectively perform screening default behavior. Simultaneously, distributed significant data clusters to achieve significantly reduce the processing time model training and testing.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.apenergy.2020.116049,Journal,Applied Energy,scopus,2021-02-01,sciencedirect,Adaptive prognostics in a controlled energy conversion process based on long- and short-term predictors,https://api.elsevier.com/content/abstract/scopus_id/85097470918,"The pulp and paper industry is a fundamental sector of the economy of many countries. However, this sector requires real collaboration and initiatives from stakeholders to reduce its significant consumption of energy and emission of greenhouse gases. Heat exchangers are examples of equipment in pulp mills that are subjected to undesirable and complex phenomena such as evolution of fouling over time, which leads to inefficiency in terms of energy consumption and unplanned shutdowns, resulting in ineffective maintenance strategies and production costs. Therefore, there is a clear need to develop an accurate predictive maintenance tool that helps mill operators avoid such situations. It is necessary for that tool to effectively track the fouling evolution level and, based on it, deploy a reliable prognostics approach to estimate more accurately the time-to-clean of this equipment. This study presents a new hybrid prognostics approach for fouling prediction in heat exchangers. The proposed approach relies on the fusion of information of different prediction horizons to estimate the time-to-clean. Employing long short-term memory, it allows adaptation of long-term predictions by accurate short-term predictions using multiple non-linear auto-regressive exogenous models. This fusion not only captures the changes in degradation trend over time, but also ensures a good accuracy of prognostics results in both the short- and long-term horizons for planning maintenance actions. The effectiveness of the proposed approach was successfully proven on real industrial data collected from a pulp mill heat exchanger located in Canada.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.apenergy.2020.116297,Journal,Applied Energy,scopus,2021-02-01,sciencedirect,An Echo State Network for fuel cell lifetime prediction under a dynamic micro-cogeneration load profile,https://api.elsevier.com/content/abstract/scopus_id/85097452614,"Improving Proton Exchange Membrane Fuel Cell durability is a key that paves the way to its large scale industrial deployment. During the last five years, the prognostics discipline emerged as an interesting field for Proton Exchange Membrane Fuel Cell state of health prediction and lifetime estimation. The information provided by the prognostic module is crucial for optimizing the control strategy to extend the fuel cell lifetime. In this paper, an approach based on Echo State Network for fuel cell prognostics under a variable load is developed. The novelty of this paper is to perform prognostics under a variable load profile without prior knowledge of this latter. Two solutions are developed in this work. The first one consists of evaluating the remaining useful lifetime under a repeated load cycle. The second one is based on using Markov chains to generate estimations of the future load profile, allowing thus to overcome the need of real future load profile prior knowledge. Both proposed solutions give accurate prediction results of proton exchange membrane fuel cell remaining useful lifetime, with low uncertainties.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.micpro.2020.103579,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Development of cultural tourism platform based on FPGA and convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85097346419,"Data mining can be described as a typical analysis of large datasets to investigate early unknown types, styles, and interpersonal relationships to generate the right decision information. It improves their markets and today to maintain control over whether these companies are forced into the data mining tools and technologies they use to develop and manage tourism products and services in the market. It is falling out of the favorable situation of the travel and tourism industry. Objective work is to provide and display its application in data mining and tourism. Advances in mobile technology provide an opportunity to obtain real-time information of travelers, such as time and space behavior, at the destination they visit. This study analyzed a large-scale mobile phone data set to capture the mobile phone traces of international tourists who visited South Korea. We adopt the trajectory data mining method to understand tourism activities’ spatial structure in three different destinations. The research reveals tourist destinations and multiple “hot spots” (or popular areas) that interact spatially in these places through spatial cluster analysis and sequential pattern mining. Therefore, this article provides the planning of spatial model destinations to integrate important tourism influences, which is based on tourism design. The proposed system is modelled in Field Programmable Gate Array (FPGA) using Xilinx software.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.micpro.2020.103318,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,Enterprise financial cost management platform based on FPGA and neural network,https://api.elsevier.com/content/abstract/scopus_id/85094858518,"At present, the domestic costs of most construction companies are relatively scattered with the cost data of various business agents. Unless it is controlled by an experienced manager, decision-makers cannot have the real-time dynamic cost of a project. In the information age, it is of vital importance to use the information to control the cost of construction projects dynamically. Cost management and the establishment of an information platform are ways to control the platform integrated cost data, operators, computer software and hardware, and corresponding method information, and its core is cost data information. The place for financial cost analysis and decision making is a conceptually rich field where information is a commercial product which is complicated, extensive, and invaluable. In this model, first,a set of extracts from the macro-credit feature space is designed and then, FPGA and neural network (FPGA, NN) models for credit evaluation is built based on these indicators, eventually it is applied scientifically, and reasonably, practically. Several state credit metrics are randomly selected. Our model shows applications that are both practical and competent. Using this model, authorities can analyze local credit conditions, allowing investors to make wise decisions to invest while saving on operating and credit costs. Most importantly, this model can help impulsive local government leaders, businesses, and even everyone to enhance competitiveness and capacities of attractive regions, thereby foster a good atmosphere for a credit culture.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.micpro.2020.103301,Journal,Microprocessors and Microsystems,scopus,2021-02-01,sciencedirect,IoT enabled cancer prediction system to enhance the authentication and security using cloud computing,https://api.elsevier.com/content/abstract/scopus_id/85094168107,"In recent days, Internet of Things, Cloud Computing, Deep learning, Machine learning and Artificial Intelligence are considered to be an emerging technologies to solve variety of real world problems. These techniques are importantly applied in various fields such as healthcare systems, transportation systems, agriculture and smart cities to produce fruitful results for number of issues in today's environment. This research work focuses on one such application in the field of IoT together with cloud computing. More number of sensors that are deployed in human body is used to collect patient related data such as deviation in body temperature and others which leads to variation in blood cells that turned to be cancerous cells. Main intention of this work is design a cancer prediction system using Internet of Things upon extracting the details of blood results to test whether it is normal or abnormal. In addition to this, encryption is done on the blood results of cancer affected patient and store it in cloud for quick reference through Internet for the doctor or healthcare nurse to handle the patient data secretly. This research work concentrates on enhancing the health care computations and processing. It provides a framework to enhance the performance of the existing health care industry across the globe. As the entire medical data has to be saved in cloud, the traditional medical treatment limitations can be overcome. Encryption and decryption is done using AES algorithm in order to provide authentication and security in handling cancer patients. The main focus is to handle healthcare data effectively for the patient when they are away from the home town since the needed cancer treatment details are stored in cloud. The task completion time is greatly reduce from 400 to 160  by using VMs. CloudSim gives an adaptable simulation structure that empowers displaying and reproduced results.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.isatra.2020.08.024,Journal,ISA Transactions,scopus,2021-02-01,sciencedirect,Data-driven adaptive modeling method for industrial processes and its application in flotation reagent control,https://api.elsevier.com/content/abstract/scopus_id/85089898141,"In real industrial processes, new process “excitation” patterns that largely deviate from previously collected training data will appear due to disturbances caused by process inputs. To reduce model mismatch, it is important for a data-driven process model to adapt to new process “excitation” patterns. Although efforts have been devoted to developing adaptive process models to deal with this problem, few studies have attempted to develop an adaptive process model that can incrementally learn new process “excitation” patterns without performance degradation on old patterns. In this study, efforts are devoted to enabling data-driven process models with incremental learning ability. First, a novel incremental learning method is proposed for process model updating. Second, an adaptive neural network process model is developed based on the novel incremental learning method. Third, a nonlinear model predictive control based on the adaptive process model is implemented and applied for flotation reagent control. Experiments based on historical data provide evidence that the newly developed adaptive process model can accommodate new process “excitation” patterns and preserve its performance on old patterns. Furthermore, industry experiments carried out in a real-world lead–zinc froth flotation plant provide industrial evidence and show that the newly designed controller is promising for practical flotation reagent control.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rcim.2020.102029,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2021-02-01,sciencedirect,Towards manufacturing robotics accuracy degradation assessment: A vision-based data-driven implementation,https://api.elsevier.com/content/abstract/scopus_id/85088120602,"In this manuscript we report on a vision-based data-driven methodology for industrial robot health assessment. We provide an experimental evidence of the usefulness of our methodology on a system comprised of a 6-axis industrial robot, two monocular cameras and five binary squared fiducial markers. The fiducial marker system permits to accurately track the deviation of the end-effector along a fixed non-trivial trajectory. Moreover, we monitor the trajectory deflection using three gradually increasing weights attached to the end-effector. When the robot is loaded with the maximum allowed payload, a deviation of 0.77mm is identified in the Z-coordinate of the end-effector. Tracing trajectory information, we train five supervised learning regression models. Such models are afterwards used to predict the deviation of the end-effector, using the pose estimation provided by the visual tracking system. As a result of this study, we show that this procedure is a stable, robust, rigorous and reliable tool for robot trajectory deviation estimation and it even allows to identify the mechanical element producing non-kinematic errors.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-323-85854-0.00033-2,Book,Wearable Telemedicine Technology for the Healthcare Industry: Product Design and Development,scopus,2021-01-01,sciencedirect,Wearable Telemedicine Technology for the Healthcare Industry: Product Design and Development,https://api.elsevier.com/content/abstract/scopus_id/85129829828,Unknown,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-323-85193-0.00012-7,Book,"Microbial Management of Plant Stresses: Current Trends, Application and Challenges",scopus,2021-01-01,sciencedirect,Advances in sensing plant diseases by imaging and machine learning methods for precision crop protection,https://api.elsevier.com/content/abstract/scopus_id/85128582282,"Plant diseases are one of the primary causes of major economic losses in the agriculture industry worldwide. The continuous monitoring of plant health and early detection of pathogens are crucial to reduce the disease spread and help effective disease management. The traditional methods of plant disease management generally rely on the spraying of chemical pesticides in the entire field, irrespective of its real requirement or not. Such blind application of these chemicals leads to undesirable effects on soil chemistry and microbiota. Following the second green revolution utilizing genomic advancements, smart or precision farming is changing the agricultural landscape at a very fast pace across the world. Precision agriculture relies on the implementation of modern-day advanced imaging and information technologies in disease identification. These intelligent and noninvasive methods use near real-time observations to protect crop damages caused by plant diseases. From a huge landscape of precision agriculture, the present chapter concentrates on the imaging-based approaches for biotic stress detection in plants. In this chapter, the machine learning methods including support vector machines, neural networks, and deep learning are also highlighted for the detection of plant diseases. These algorithms help in making smart decisions for the actual requirement and the adequate application of crop protection resources. Both imaging and machine learning methods are powerful and unparalleled tools for sustainable agriculture. They effectively detect biotic stress in plants and can provide data directly from different geographical scales.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-323-90118-5.00009-6,Book,Machine Reading Comprehension: Algorithms and Practice,scopus,2021-01-01,sciencedirect,Machine Reading Comprehension: Algorithms and Practice,https://api.elsevier.com/content/abstract/scopus_id/85127761783,Unknown,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-821229-5.00008-2,Book,Machine Learning and the Internet of Medical Things in Healthcare,scopus,2021-01-01,sciencedirect,Artificial itelligence in medicine,https://api.elsevier.com/content/abstract/scopus_id/85127629358,"Modern lifestyle and the polluted environment are the main causes of different types of diseases. Some diseases are curable through primary medication but some are more severe and require proper medication. In clinical treatment, different categories of medicines such as allopathic, homeopathy, herbal, art therapy, homemade medicine, etc. are applied to cure the diseases. The prediction of an appropriate medicine as per the symptoms of the disease is a challenging task for the clinicians. In this context, intelligent systems could be very helpful to predict the right medicine to the right people. Artificial Intelligence (AI) is a kind of intelligent system that applies different techniques to work with a huge amount of data for real-time analysis and better prediction to attain the required outcome. Also in the medicine industry, the process of discovering new medicines needs several clinical trials and requires approval by the concerned authority to deploy in the market. AI can improve decision-making and assist in the search for better medicines. Machine Learning is another revolution from AI that learns from the preexisting data sets and improves its accuracy in decision-making. This chapter presents a detailed literature survey on different AI techniques, followed by recent developments and applications in the medical industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-819742-4.09991-1,Book,"Machine Learning and Data Science in the Power Generation Industry: Best Practices, Tools, and Case Studies",scopus,2021-01-01,sciencedirect,"Machine Learning and Data Science in the Power Generation Industry: Best Practices, Tools, and Case Studies",https://api.elsevier.com/content/abstract/scopus_id/85126829359,Unknown,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2021.11.106,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Machine Vision and Radio-Frequency Identification (RFID) based Real-Time Part Traceability in a Learning Factory,https://api.elsevier.com/content/abstract/scopus_id/85121617332,"Visual inspection-based quality control systems are deployed to meet the growing demand of high-quality products. In this paper, an inexpensive RFID (Radio-Frequency Identification) technology and a machine vision system have been integrated within an existing learning factory. RFID technology is used to trace a product/part from its origin, enabling the visibility of the product/part’s entire movement in the value chain. The goals of the paper are to track the workpieces in real time (i) to provide immediate feedback, through visibility, to operators and floor managers and (ii) to develop a database to trace parts, in future, backward through value chain. The system can be used to predict the probability of defective parts in the value chain using machine learning algorithms. This will help the manufacturers to trace the parts in value chain at any point of time including aftersales. Traceability has been a pain point for manufacturers during product recalls.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2021.11.057,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,An architecture for sim-to-real and real-to-sim experimentation in robotic systems,https://api.elsevier.com/content/abstract/scopus_id/85121579023,Research in the area of robotic systems has greatly benefited from the use of simulation models. Recent approaches allow the transfer of developed algorithms from simulation to reality (sim-to-real) and increasingly accurate representations of real systems as simulation models (real-to-sim). The paper presents an architecture based on open software that supports simultaneous experiments on real robots and their simulation models. Two illustrative examples are shown: a digital twin of an industrial robot and a sim-to-real transfer in an autonomous mobile robot system. The possibilities of future research on the interaction between robotic systems and their simulation models are discussed.,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aiia.2021.11.004,Journal,Artificial Intelligence in Agriculture,scopus,2021-01-01,sciencedirect,Automation and digitization of agriculture using artificial intelligence and internet of things,https://api.elsevier.com/content/abstract/scopus_id/85120995341,"The growing population and effect of climate change have put a huge responsibility on the agriculture sector to increase food-grain production and productivity. In most of the countries where the expansion of cropland is merely impossible, agriculture automation has become the only option and is the need of the hour. Internet of things and Artificial intelligence have already started capitalizing across all the industries including agriculture. Advancement in these digital technologies has made revolutionary changes in agriculture by providing smart systems that can monitor, control, and visualize various farm operations in real-time and with comparable intelligence of human experts. The potential applications of IoT and AI in the development of smart farm machinery, irrigation systems, weed and pest control, fertilizer application, greenhouse cultivation, storage structures, drones for plant protection, crop health monitoring, etc. are discussed in the paper. The main objective of the paper is to provide an overview of recent research in the area of digital technology-driven agriculture and identification of the most prominent applications in the field of agriculture engineering using artificial intelligence and internet of things. The research work done in the areas during the last 10 years has been reviewed from the scientific databases including PubMed, Web of Science, and Scopus. It has been observed that the digitization of agriculture using AI and IoT has matured from their nascent conceptual stage and reached the execution phase. The technical details of artificial intelligence, IoT, and challenges related to the adoption of these digital technologies are also discussed. This will help in understanding how digital technologies can be integrated into agriculture practices and pave the way for the implementation of AI and IoT-based solutions in the farms.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2021.08.152,Conference Proceeding,IFAC-PapersOnLine,scopus,2021-01-01,sciencedirect,Barriers to predictive maintenance implementation in the Italian machinery industry,https://api.elsevier.com/content/abstract/scopus_id/85120714897,"Predictive maintenance (PM) involves the use of internet of things and machine learning techniques applied to machinery for remote monitoring of different variables to timely detect problems, before they require costly maintenance or generate customers’ complaints. Thus, it minimises the probability that the machine will break, extends its lifecycle and reduces the number of corrective actions. Despite the importance of PM and the growing implementation of Industry 4.0 technologies, a limited number of Italian machinery companies today includes PM systems in their products. Additionally, the topic has received little attention by literature. Consequently, there is a need to identify the barriers to the implementation of PM in the Italian machinery industry. Therefore, the aim of this research is to categorise the challenges to be considered when implementing a PM and propose a set of possible countermeasures. In doing so, the study reviews the existing literature on this topic and empirically explores three cases in the machinery industry. The results of the literature review show a list of barriers to PM implementation that can be related to the machinery industry. Then, the barriers are empirically validated, and inductively extended, and final set of countermeasures is proposed to overcome these challenges, in help of managers that are interested in adopting PM.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2021.08.033,Conference Proceeding,IFAC-PapersOnLine,scopus,2021-01-01,sciencedirect,A new simulation-based approach in the design of manufacturing systems and real-time decision making,https://api.elsevier.com/content/abstract/scopus_id/85120689702,"The principles and tools made available by the Industry 4.0, smart factories, or the Internet of Things (IoT), along with the adoption of more comprehensive simulation models, can significantly help the industry to face the current, huge external and internal challenges. This paper presents a new simulation-based approach to support decision making in the design and operational management of manufacturing systems. This approach is used to evaluate different layouts and resources allocation, and help managing operations, by integrating a simulation software with real-time data collected from the production assets through an IoT platform. The developed methodology uses a digital representation of the real production system (that may be viewed as a form of a digital twin) to assess different production scenarios. A set of key performance indicators (e.g. productivity) provided by the simulation can be used by the Manufacturing Execution System (MES) to generate production schedules. The developed approach was implemented and assessed in a real case study, showing its robustness and application potential. Its extension to other industrial contexts and sectors seems, therefore, quite promising.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2021.08.062,Conference Proceeding,IFAC-PapersOnLine,scopus,2021-01-01,sciencedirect,"Indoor positioning, artificial intelligence and digital twins for enhanced robotics safety",https://api.elsevier.com/content/abstract/scopus_id/85120685413,"Flexible robotics safety solutions allowing the implementation of fenceless robot cells are becoming a reality nowadays. Safety approved sensors such as light curtains, safety scanners, and safety cameras have been deployed already successfully in various industrial robotic solutions. Still, as these safety systems are installed in fixed locations, monitoring predefined regions, the systems can be rigid and inflexible. This paper introduces a novel hybrid safety solution. The solution comprises safety-approved sensors, additional sensors, and artificial intelligence analysis. The system increases flexibility, especially in cases where collaborating humans and robots need monitoring in larger areas. Typically, in such environments, work objects are large and heavy, introducing additional challenges. In addition, the proposed system includes a digital twin implementation that allows a connection between the real and virtual worlds. Already virtual models and robot simulation have been used for designing safe robot applications. However, the efficient use of digital twins in safety planning and safety monitoring is still uncommon.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2021.08.077,Conference Proceeding,IFAC-PapersOnLine,scopus,2021-01-01,sciencedirect,A digital twin-based approach for multi-objective optimization of short-term production planning,https://api.elsevier.com/content/abstract/scopus_id/85120684598,"Short-term multi-product production planning strongly depends on manifold considerations which can be related to backlog production, product-type prioritization, machine capacity, and set-ups. Different manufacturing systems conditions may lead to different objectives. In this context, it is rather difficult to define a-priori optimal production plans. This paper presents an effective approach for short-term production planning of multi-product systems based on the integration of a Digital Twin and a multi-objective optimization method. The proposed approach has been implemented in a real industrial case of the railway sector. Numerical results show that useful insights can be inferred from the proposed methodology.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2021.08.106,Conference Proceeding,IFAC-PapersOnLine,scopus,2021-01-01,sciencedirect,How to characterize a digital twin: A usage-driven classification,https://api.elsevier.com/content/abstract/scopus_id/85120682487,"Last years, publications on the Digital Twin have been increasing sharply without any real coherence, representing very different realities. In order to clarify its scope, we propose not only a global definition, but also a complete, usage-driven classification methodology. This enables to integrate data and models as well as applications in a generic overview. Moreover, the usage-driven deployment of the Digital Twin allows to be user-centric, in order to foster acceptance. We give various examples of the interest of this classification to support the deployment of the Digital Twin in all industrial applications.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2021.08.144,Conference Proceeding,IFAC-PapersOnLine,scopus,2021-01-01,sciencedirect,Fingerprint analysis for machine tool health condition monitoring,https://api.elsevier.com/content/abstract/scopus_id/85120666283,"One of the pillars of the smart factory concept within the Industry 4.0 paradigm is the capability to monitor the health conditions of production systems and their critical components in a continuous and effective way. This could be enabled through the implementation of innovative diagnosis, prognosis and predictive maintenance actions. A wide literature has been devoted to methodologies to monitor the manufacturing process and the tool wear. A parallel research field is dedicated to isolate the health condition of the machine tool from the production process and external source of noise. This study presents a novel solution for machine health condition monitoring based on the so-called “fingerprint” cycle approach. A fingerprint cycle is a pre-defined test cycle in no-load conditions, where the axes and the spindle are activated in a sequential order. Several signals are extracted from the machine controller to characterize the current health state of the machine. The method is suitable to separate drifts, trends and shifts in CNC signals caused by a change in machine tool health condition from any variation related to the cutting process and external factors. A machine learning method that combines Principal Component Analysis and statistical process monitoring allows one to quickly detect degraded conditions affecting one or multiple critical components. A real case study is presented to highlight the potentials and benefits provided by the proposed approach.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2021.10.022,Conference Proceeding,Procedia Manufacturing,scopus,2021-01-01,sciencedirect,Comparison of nominal and real 2D contours of manufactured products using Ant Colony Optimisation of shape landmarks,https://api.elsevier.com/content/abstract/scopus_id/85120628164,"In studying manufacturing processes it is often necessary to compare nominal or reference product shapes that are expected to be ideally achieved after applying a nominal set of process parameters and the actual product shape achieved. The latter is usually obtained by metrological equipment such as profilometers, tomographs and 3D scanners, whilst the former may also result from manufacturing process numerical simulation software. In this work, comparison of 2D contours is studied by adopting the well-established Procrustes method. The method relies on minimizing deviation of landmark points belonging to the shapes under scrutiny. Landmarks that need to be compared pairwise should be chosen in an optimal way, especially when the contours contain different numbers of points, when they differ locally etc. Optimal determination of landmarks and as a result implementation of contour comparison is performed through ant colony optimization implemented on MatlabTM platform. Indicative results are shown where deviations discovered signify shrinkage of a die-cast clay part.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2021.10.048,Conference Proceeding,Procedia Manufacturing,scopus,2021-01-01,sciencedirect,Natural language processing for comprehensive service composition in cloud manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85120604855,"Cloud manufacturing (CMfg) is a relatively new manufacturing paradigm which promotes easier collaboration among geographically-dispersed manufacturers. Various manufacturing resources can be shared via cloud manufacturing platform as services. The efficacy of such resource sharing and thus collaboration in this service-oriented mode of manufacturing is highly dependent to the efficiency of underlying mechanisms used for processing these services. Activities such as service matching, service retrieval, service composition and optimal selection, and service scheduling are among the most critical of those service processing mechanisms. In this paper, we introduce and implement a comprehensive service composition and optimal selection (SCOS) technique that takes advantage of a real-world manufacturing capability dataset, deep learning models as well as Word2Vec technique to retrieve appropriate candidate sets for each submitted manufacturing subtask in the cloud manufacturing platform. Then, a genetic algorithm is implemented to obtain a near-optimal composite service that achieves the highest Quality of Service (QoS). A series of experiments were conducted to prove the feasibility and efficacy of the approach and results were presented.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2021.06.086,Conference Proceeding,Procedia Manufacturing,scopus,2021-01-01,sciencedirect,Pervasive environmental sensing for Industry 4.0 as an educational tool,https://api.elsevier.com/content/abstract/scopus_id/85117930435,"The reduced cost of implementing pervasive industrial sensing networks enables universities to incorporate these tools in engineering curricula. They provide engineering students from increasingly computerized backgrounds, such as mechanical and automotive engineering, the opportunity to work alongside students from technical schools who bring different skill sets than what students may be used to, synthesize historical data, and drive the sensing system’s physical system design and implementation. This paper outlines this convergent curriculum’s initial implementation stage, including the wireless environmental sensing Internet of Things (IoT) network, focusing on laboratory environmental sensing. Students placing many sensors around the lab and on equipment generates a wealth of real-time and historical data for use in the classroom and provides them a tangible example of learning to measure the world around them. This setup parallels the current varied Industry 4.0 state of the manufacturing industry, where Big Data exists but is underutilized, and where additional sensors and intelligent machine data streams are added each year. Students in each class are given a defined portion of a broader roadmap to a fully instrumented and intelligent laboratory environment. In the first step, student-programmed environmental sensors were placed around the lab and provide temperature, humidity, pressure, and gas mixture measures every five minutes. Classroom use of the aggregated data includes visualizing the laboratory and essential equipment’s current status using a Microsoft PowerBI dashboard and historical data visualization and analysis through trend forecasting and outlier detection in Python JupyterLab notebooks. The IoT system’s installation also provided an infrastructure for further study of future student-designed IoT projects.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.orp.2021.100204,Journal,Operations Research Perspectives,scopus,2021-01-01,sciencedirect,A review of approximate dynamic programming applications within military operations research,https://api.elsevier.com/content/abstract/scopus_id/85117385700,"Sequences of decisions that occur under uncertainty arise in a variety of settings, including transportation, communication networks, finance, defence, etc. The classic approach to find an optimal decision policy for a sequential decision problem is dynamic programming; however its usefulness is limited due to the curse of dimensionality and the curse of modelling, and thus many real-world applications require an alternative approach. Within operations research, over the last 25 years the use of Approximate Dynamic Programming (ADP), known as reinforcement learning in many disciplines, to solve these types of problems has increased in popularity. These efforts have resulted in the successful deployment of ADP-generated decision policies for driver scheduling in the trucking industry, locomotive planning and management, and managing high-value spare parts in manufacturing. In this article we present the first review of applications of ADP within a defence context, specifically focusing on those which provide decision support to military or civilian leadership. This article’s main contributions are twofold. First, we review 18 decision support applications, spanning the spectrum of force development, generation, and employment, that use an ADP-based strategy and for each highlight how its ADP algorithm was designed, evaluated, and the results achieved. Second, based on the trends and gaps identified we discuss five topics relevant to applying ADP to decision support problems within defence: the classes of problems studied; best practices to evaluate ADP-generated policies; advantages of designing policies that are incremental versus complete overhauls when compared to currently practiced policies; the robustness of policies as scenarios change, such as a shift from high to low intensity conflict; and sequential decision problems not yet studied within defence that may benefit from ADP.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2021.08.095,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,SODA: A real-time simulation framework for object detection and analysis in smart manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85116946450,"For modern manufacturing firms, automation has already become a norm but constantly needs to be improved as firms still face strong demand to increase their productivity. This can be achieved by reducing dependability on manpower, reaching lean and even unmanned production and this is where some of the standards of Industry 4.0 come in useful, not to mention: Machine Vision, Image Recognition or Machine Learning. In our paper, we present SODA – our approach to build a flexible ML and AI enabled framework for object detection, analysis, and simulation. The framework is designed to support a development process of solutions requiring real-time analysis of images of different types of moving objects on a conveyor belt. In our work we discuss architectural challenges of the developed framework as well as the basic components of the system. We do also provide information on how to use the framework and present a sample implementation of an actual system employing some of the machine learning methods.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2021.09.013,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Exploiting supervised machine learning for driver detection in a real-world environment,https://api.elsevier.com/content/abstract/scopus_id/85116888149,"The proliferation of info-entertainment systems in today’s vehicles has provided a really cheap and easy-to-deploy platform with the ability to gather information about the vehicle under analysis. Ultra-response connectivity networks with a latency below 10 milliseconds are providing the perfect infrastructure in which this information can be sent to improve safety and security. With the purpose of providing an architecture to increase safety and security in an automotive context, we in this paper propose a method for detecting the driver in real-time exploiting supervised machine learning techniques. The experimental analysis performed on real-world data shows that the proposed method obtains encouraging results.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2021.09.233,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,A note on the applications of artificial intelligence in the hospitality industry: Preliminary results of a survey,https://api.elsevier.com/content/abstract/scopus_id/85116885410,"Intelligent technologies are widely implemented in different areas of modern society but specific approaches should be applied in services. Basic relationships refer to supporting customers and people responsible for services offering for these customers. The aim of the paper is to analyze and evaluate the state-of-the art of artificial intelligence (AI) applications in the hospitality industry. Our findings show that the major deployments concern in-person customer services, chatbots and messaging tools, business intelligence tools powered by machine learning, and virtual reality & augmented reality. Moreover, we performed a survey (n = 178), asking respondents about their perceptions and attitudes toward AI, including its implementation within a hotel space. The paper attempts to discuss how the hotel industry can be motivated by potential customers to apply selected AI solutions. In our opinion, these results provide useful insights for understanding the phenomenon under investigation. Nevertheless, since the results are not conclusive, more research is still needed on this topic. Future studies may concern both qualitative and quantitative methods, devoted to developing models that: a) quantify the potential benefits and risks of AI implementations, b) determine and evaluate the factors affecting the AI adoption by the customers, and c) measure the user (guest) experience of the hotel services, fueled by AI-based technologies.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.matpr.2021.03.109,Conference Proceeding,Materials Today: Proceedings,scopus,2021-01-01,sciencedirect,Real-time applications and novel manufacturing strategies of incremental forming: An industrial perspective,https://api.elsevier.com/content/abstract/scopus_id/85114180763,"Incremental Sheet-Metal Forming (ISMF) is a flexible and evolving metal forming technology for rapid free-form prototyping and small-batch metal components manufacturing. The end product has evolved by means of localized deformation in addition bi-axial stretching during that deforming tool squeezed on blank with predefined process variables. Owing to a unique process advantages and low manufacturing cost, its market requirement continuous enlargement and the process gradually transforms from prototyping to real-time manufacturing perspective. Over the preceding decades, ISMF technology has been adequately established in research and development, although it is less explored in the real-time industrial environment. The main intention of this exploration is to bring-forth insight into potential applications such as aviation, automotive, bio-medical, research and concept development through implementation of ISMF. Further, component evaluation performed to establish a convenient and feasible solution from deep-drawing and hydro-forming. For customized part forming, conventional forming process seems to be insufficient. Due to industrial transformation, dependent on cost-effectiveness, even prototyping and low-volume manufactured components relying on superior quality. Although, understanding the effect and influence of process variable, which needs the data analysis with implementing the optimization models and Artificial neural-network (ANN) model. These types of analysis majorly focus on monitoring and predict target values at each cycle and also reconfigure to optimistic or organize the iterative method for describing the appropriate process guidelines. Further, recent advances in ISMF process variants are explored, while looking at the benefits of ISMF for real-time part production. ISMF continues to mature into technology for production applications, while exploring the potential field to transform the way sheet components are fabricated in the new-era of digital manufacturing. This study will, in turn, enhance the capabilities of ISMF technology, which has grown significantly over the preceding decades, allowing technology adopters to innovate new design principle and achieve greater production flexibility.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-323-88506-5.50132-7,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Implementation of first-principles surface interactions in a hybrid machine learning assisted modelling of flocculation,https://api.elsevier.com/content/abstract/scopus_id/85110537894,"Machine learning algorithms are drawing attention for modelling processes in the chemical and biochemical industries. Due to a lack of fundamental understanding of complex processes and a lack of reliable real-time measurement methods in bio-based manufacturing, machine learning approaches have become more important. Hybrid modelling approaches that combine detailed process understanding with machine learning can provide an opportunity to integrate prior process knowledge with various measurement data for efficient modelling of the (bio) chemical processes. In this study, the application of a hybrid modelling framework that combines various first-principles models with machine learning algorithms is demonstrated through a laboratory-scale case of flocculation of silica particles in water. Since flocculation is a process that occurs across length- and time scales, an integrated hybrid multi-scale modelling framework can improve the phenomenological understanding of the process. The first-principles models utilized in this study are molecular scale particle surface interaction models such as combined with a larger-scale population balance model.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-323-88506-5.50161-3,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Artificial Intelligence Based Prediction of Exergetic Efficiency of a Blast Furnace,https://api.elsevier.com/content/abstract/scopus_id/85110444162,"The iron melting furnaces are the most energy-consuming equipment of the iron and steel industry. The energy efficiency of the furnace is affected by process conditions such as the inlet temperature, velocity of the charge, and its composition. Hence, optimum values of these process conditions are vital in the efficient operation of the furnace. Computational methods have been very helpful in the optimum design and operation of process equipment. In this study, a first principle (FP) model was developed for an iron-making furnace to visualize its internal dynamics. To minimize the large computational time required for the FP-based analysis, a data-based model, i.e., Artificial Neural Networks (ANN), is developed using data extracted from the FP model. The ANN model was developed using data sets comprised of the values of temperature of the charge and gasses, velocity, concentration of the oxygen, pressure, airflow directions, energy and exergy profiles, and overall exergy efficiency of the furnace along with its height. The ANN model was highly accurate in prediction and is suitable for real-time implementation in a steel manufacturing plant.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-323-88506-5.50144-3,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Machine learning-based approach to identify the optimal design and operation condition of organic solvent nanofiltration (OSN),https://api.elsevier.com/content/abstract/scopus_id/85110354404,"Organic solvent nanofiltration (OSN) is one of the most anticipated separation technologies that provides wide-ranged industrial applications such as solvent recovery, solute concentration, and diluent separation. Despite of technical merits of the OSN technology, the numerous characteristics and perplexing nonlinearity on the OSN system have been a critical obstacle for understanding the governing principles, thereby prohibiting practical deployments. Recently, machine learning (ML) based approaches have been widely used for the modelling, discovery and optimization of complex design problems in chemical engineering area such as catalysis, electrochemistry and physicochemical systems. Therefore, this study aims to develop a new ML-based approach for modelling and optimizing the design scheme and operating condition of the OSN system. By collecting commercial OSN data through literatures reviews, the major descriptors for the prediction of the OSN membrane, such as MWCO, solute mole weight, solute concentration, solvent parameter, temperature, pressure, flux, were defined. We then screened noises and outliers of the collected data to ensure a high and consistent density and uniqueness. Support vector machine (SVM) was implemented as a prediction models to simulate the OSN performance and identify the optimal conditions as well as the process scheme. As a result, the optimal operation strategies (i.e., pressure, temperature and solvent and solvent types) were analyzed to meet the targeted specification of the OSN system (mass flux and rejection rate). The proposed ML-based approach can promote a real-world OSN application by reducing a number of time-consuming and expensive experiments for establishing OSN design and operation strategy.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-323-88506-5.50194-7,Book Series,Computer Aided Chemical Engineering,scopus,2021-01-01,sciencedirect,Attack Detection Using Unsupervised Learning Algorithms in Cyber-Physical Systems,https://api.elsevier.com/content/abstract/scopus_id/85110277992,"Cyber-Physical Systems (CPS) are collections of physical and computer components that are integrated with each other to operate a process safely and efficiently. Examples of CPS include industrial control systems, water systems, robotics systems, smart grid, etc. However, the security aspect of CPS is still a concern that makes them vulnerable to cyber attacks on the control elements, network or physical systems. The work reported here is an attempt towards detecting cyber attacks and improving process monitoring in CPS; using unsupervised machine learning anomaly detection algorithms such as one-class SVM, isolation forest, elliptic envelope. These algorithms are evaluated using the dataset of a real Water Distribution Plant (WADI) built at the iTrust centre at Singapore University of Technology and Design for cyber security research. For modelling purposes, process 1 and 2 of the aforementioned plant were taken into consideration because the implemented attacks were closely related to only these sub-processes. The result of the experiment shows that one-class SVM is found to be the most effective algorithm in determining anomalies for this particular dataset.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2021.11.076,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Regularization-based Continual Learning for Anomaly Detection in Discrete Manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85109178001,"The early and robust detection of anomalies occurring in discrete manufacturing processes allows operators to prevent harm, e.g. defects in production machinery or products. While current approaches for data-driven anomaly detection provide good results on the exact processes they were trained on, they often lack the ability to flexibly adapt to changes, e.g. in products. Continual learning promises such flexibility, allowing for an automatic adaption of previously learnt knowledge to new tasks. Therefore, this article discusses different continual learning approaches from the group of regularization strategies, which are implemented, evaluated, and compared based on a real industrial metal forming dataset.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cirp.2021.04.046,Journal,CIRP Annals,scopus,2021-01-01,sciencedirect,Semi-Double-loop machine learning based CPS approach for predictive maintenance in manufacturing system based on machine status indications,https://api.elsevier.com/content/abstract/scopus_id/85108064671,"The paper presents two original and innovative contributions: 1) the model of machine learning (ML) based approach for predictive maintenance in manufacturing system based on machine status indications only, and 2) semi-Double-loop machine learning based intelligent Cyber-Physical System (I-CPS) architecture as a higher-level environment for ML based predictive maintenance execution. Considering only the machine status information provides rapid and very low investment-based implementation of an advanced predictive maintenance paradigm, especially important for SMEs. The model is validated in real-life situations, exploring different learning algorithms and strategies for learning maintenance predictive models. The findings show very high level of prediction accuracy.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2021.05.031,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Artificial intelligence enhanced interaction in digital twin shop-floor,https://api.elsevier.com/content/abstract/scopus_id/85107885361,"As an enabling technology for smart manufacturing, digital twin has been widely applied in manufacturing shop-floor. A great deal of research focuses on the key issues in implementing digital twin shop-floor (DTS), including scheduling, production planning, fault diagnosis and prognostics. However, DTS puts forward higher requirements in terms of real-time interaction. Artificial intelligence (AI), as an effective approach to improve the intelligence of the physical shop-floor, provides a new method to meet the above requirements. In this paper, a framework of AI-enhanced DTS in interaction is proposed. AI-enhanced DTS improves the real-time interaction through predictive control. The implementation mechanism of AI-enhanced interaction in DTS is also presented in detail. Enabling technologies for interaction in DTS are introduced at last.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2021.03.074,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Requirements towards optimizing analytics in industrial processes,https://api.elsevier.com/content/abstract/scopus_id/85106735396,"Modern production systems are composed of complex manufacturing processes with highly technology specific cause-effect relationships. Developments in sensor technology and computational science allow for data-driven decision making that facilitate effcient and objective production management. However, process data may only be beneficial if it is enriched with meta information and process expertise, reduced to relevant information and modelling results interpreted correctly. The importance of data integration in the heterogeneous industrial environment rises at the same momentum as new metrology techniques are deployed. In this paper, we focus on optimizing analytics, containing data-driven decision making for predictive quality and maintenance. We summarize key requirements for data analytics and machine learning application in industrial processes. With a use case from automotive component manufacturing we characterize industrial production, categorize process data and put requirements in context to a real-world example.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2021.02.026,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,DDNN based data allocation method for IIoT,https://api.elsevier.com/content/abstract/scopus_id/85104871428,"With the complete application of artificial intelligence in the field of industrial production and manufacturing and the rapid development of edge computing, industrial processing sites often need to deploy machine learning tasks at edges and terminals. We propose a data allocation method based on Distributed Deep Neural Networks (DDNN) framework, which allocates data to edge servers or stays locally for processing. DDNN divides deep learning tasks and deploys pre-trained shallow neural networks and deep neural networks at local or edges, respectively. However, all data is processed locally, and the failure is sent to the edge server or the cloud. It will lead to excessive pressure on local terminal equipment and long-term idle edge servers, which cannot meet industrial production’s real-time requirements on user privacy and time-sensitive tasks. In this paper, the complexity and inference error rate of machine learning model, the data processing speed of local equipment and edge server, and the transmission time are comprehensively considered to establish the system model. A joint optimization problem is proposed to minimize the total data processing delay. The optimal solution is derived analytically, and the optimal data allocation methhod is given. Simulation experiments are designed to verify the method’s effectiveness and study the influence of key parameters on the allocation method.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2021.03.075,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Input doubling method based on SVR with RBF kernel in clinical practice: Focus on small data,https://api.elsevier.com/content/abstract/scopus_id/85104314419,"In recent years, machine-learning-based approaches have become of considerable interest to the efficient processing of short or limited data samples. Its so-called small data approach. This is due to the significant growth of new intellectual analysis tasks in various industries, which are characterized by limited historical data. These include Materials Science, Economics, Medicine, and so on. An effective processing of short datasets is especially acute in medicine. Insufficient number of vectors, significant gaps in the data collected during the supervision of patient’s treatment or rehabilitation, reduces the effectiveness or prevents effective intellectual analysis based on them. This paper presents a new approach to processing short medical data samples. The basis of the developed method is SVR with RBF kernel. The algorithmic implementation of the method in both operation modes is described. Experimental modeling on a real short data set (Trabecular bone data) is conducted. It contained only 35 observations. A comparison of the method with a number of existing machine learning methods is conducted. It is experimental established the highest accuracy of the method among those considered. The developed method has potential opportunities for wide application in various fields of medicine.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2021.01.128,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,A Machine Vision-based Cyber-Physical Production System for Energy Efficiency and Enhanced Teaching-Learning Using a Learning Factory,https://api.elsevier.com/content/abstract/scopus_id/85102656008,"Machine vision (MV) can help in achieving real-time data analysis in a manufacturing environment. This can be implemented in any industry to achieve real-time monitoring of workpieces for geometric defects and material irregularities. Identification of defects, sorting of workpieces based on their physical parameters, and analysis of process abnormalities can be achieved by using the real-time data from simple and cost-effective raspberry pi with camera and open source machine learning platform TensorFlow to run convolutional neural network (CNN) model. The proposed cyber-physical production system enables to develop a MV based system for data acquisition integrating physical entities of learning factory (LF) with the cyber world. Nowadays, LFs are widely used to train the workforce for developing competencies for emerging technologies and challenges faced due to technological advancements in Industry 4.0. This paper demonstrates the application of a cost-effective MV system in a learning factory environment to achieve real-time data acquisition and energy efficiency. The proposed low-cost machine vision is found to detect geometric irregularities, colours and surface defects. The simple cost effective MV system has enhanced the energy efficiency and reduced the total carbon footprint by 18.37 % and 78.83 % depending upon the location of MV system along the flow. The teaching-learning experience is also enhanced through action-based learning strategies. This not only ensures less rework, better control, unbiased decisions, 100% quality assurance but also the need of workers/operators can be reduced.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2021.01.115,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Development of a Decision Support System for 3D Printing Processes based on Cyber Physical Production Systems,https://api.elsevier.com/content/abstract/scopus_id/85102637852,"3D printing, an additive manufacturing (AM) technology, potentially provides sustainability advantages such as less waste generation, lightweight geometries, reduced material and energy consumption, lower inventory waste, etc. This paper proposes a decision support system for the 3D printing process based on Cyber Physical Production System (CPPS). The user is enabled to dynamically assess the carbon footprint based on the energy and material usage for their 3D printed object. A CPPS framework for the environmental sustainability of the 3D printing process is presented, which supports the derivation of improved strategies for product design and production. A physical world for 3D printing is used with the internet of things (IoT) devices like sensor node, webcam, smart plugs, and raspberry pi to host printer Management Software (PMS) for real-time monitoring and control of material and energy consumption during the printing process. Experiments have been conducted based on Taguchi L9 orthogonal array with polylactic Acid (PLA) as a filament material to estimate the product-related manufacturing energy consumption with the carbon footprint. The proposed framework can be effectively used by the users to supports the decision-making process for saving resources and energy; and minimizing the effect on the environment.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2021.01.010,Conference Proceeding,Procedia CIRP,scopus,2021-01-01,sciencedirect,Analysis of Barriers to Industry 4.0 adoption in Manufacturing Organizations: An ISM Approach,https://api.elsevier.com/content/abstract/scopus_id/85102622489,"Industry 4.0 has enabled technological integration of cyber physical systems and internet based communication in manufacturing value creation processes. As of now, many people use it as a collective term for advanced technologies, i.e. advanced robotics, artificial intelligence, machine learning, big data analytics, cloud computing, smart sensors, internet of things, augmented reality, etc. This substantially improves flexibility, quality, productivity, cost, and customer satisfaction by transforming existing centralized manufacturing systems towards digital and decentralized one. Despite having potential benefits of industry 4.0, the organizations are facing typical obstacles and challenges in adopting new technologies and successful implementation in their business models. This paper aims to identify potential barriers which may hinder the implementation of industry 4.0 in manufacturing organizations. The identified barriers, through comprehensive literature review and on the basis of opinions collected from industry experts, are: poor value-chain integration, cyber-security challenges, uncertainty about economic benefits, lack of adequate skills in workforce, high investment requirements, lack of infrastructure, jobs disruptions, challenges in data management and data quality, lack of secure standards and norms, and resistance to change. Interpretive Structural Modeling (ISM) is used to establish relationships among these barriers to develop a hierarchical model and MICMAC analysis for further classification of identified barriers for better understanding. An analysis of driving and dependence of the barriers may help in clear understanding of these for successful implementation of Industry 4.0 practices in the organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2021.01.348,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,Procedure model for the development and launch of intelligent assistance systems,https://api.elsevier.com/content/abstract/scopus_id/85101779152,"The paper analyses the current state of knowledge on approaches for the practical implementation of machine learning based assistance systems for production planning and control.
                  A concept of a procedure model for application-oriented projects in the field of industrial series production is proposed. It focusses on order sequencing and machine allocation in a real time production environment. As part of an application-oriented research project, a use case is referenced. In this paper, a first conceptual approach is presented, using the example of an industrial production of printed circuit boards.
                  In the following steps, practical suitability is checked on the basis of the practical reference, conclusions are drawn and the methodology will be developed further. The aim is a generally valid procedure model for industrial series production.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jtice.2021.01.007,Journal,Journal of the Taiwan Institute of Chemical Engineers,scopus,2021-01-01,sciencedirect,On the evaluation of solubility of hydrogen sulfide in ionic liquids using advanced committee machine intelligent systems,https://api.elsevier.com/content/abstract/scopus_id/85099564514,"Ionic Liquids (ILs) are increasingly emerging as new innovating green solvents with great importance from academic, industrial, and environmental perspectives. This surge of interest in considering ILs in various applications is owed to their attractive properties. Involvements in the gas sweetening and the reduction of the amounts of sour and acid gasses are among the most promising applications of ILs. In this study, new advanced committee machine intelligent systems (CMIS) were introduced for predicting the solubility of hydrogen sulfide (H2S) in various ILs. The implemented CMIS models were gained by linking robust data-driven techniques, namely multilayer perceptron (MLP) and cascaded forward neural network (CFNN) beneath rigorous schemes using group method of data handling (GMDH) and genetic programming (GP). The proposed paradigms were developed using an extensive database encompassing 1243 measurements of H2S solubility in 33 ILs. The performed comprehensive error investigation revealed that the newly implemented paradigms yielded very satisfactory prediction performance. Besides, it was found that CMIS-GP provided more accurate estimations of H2S solubility in ILs compared with both the other intelligent models and the best-prior paradigms. In this regard, the developed CMIS-GP exhibited overall average absolute relative deviation (AARD) and coefficient of determination (R2) values of 2.3767% and 0.9990, respectively. Lastly, the trend analyses demonstrated that the tendencies of CMIS-GP predictions were in excellent accordance with the real variations of H2S solubility in ILs with respect to pressure and temperature.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aei.2021.101246,Journal,Advanced Engineering Informatics,scopus,2021-01-01,sciencedirect,"A systematic literature review on intelligent automation: Aligning concepts from theory, practice, and future perspectives",https://api.elsevier.com/content/abstract/scopus_id/85099458674,"With the recent developments in robotic process automation (RPA) and artificial intelligence (AI), academics and industrial practitioners are now pursuing robust and adaptive decision making (DM) in real-life engineering applications and automated business workflows and processes to accommodate context awareness, adaptation to environment and customisation. The emerging research via RPA, AI and soft computing offers sophisticated decision analysis methods, data-driven DM and scenario analysis with regard to the consideration of decision choices and provides benefits in numerous engineering applications. The emerging intelligent automation (IA) – the combination of RPA, AI and soft computing – can further transcend traditional DM to achieve unprecedented levels of operational efficiency, decision quality and system reliability. RPA allows an intelligent agent to eliminate operational errors and mimic manual routine decisions, including rule-based, well-structured and repetitive decisions involving enormous data, in a digital system, while AI has the cognitive capabilities to emulate the actions of human behaviour and process unstructured data via machine learning, natural language processing and image processing. Insights from IA drive new opportunities in providing automated DM processes, fault diagnosis, knowledge elicitation and solutions under complex decision environments with the presence of context-aware data, uncertainty and customer preferences. This sophisticated review attempts to deliver the relevant research directions and applications from the selected literature to the readers and address the key contributions of the selected literature, IA’s benefits, implementation considerations, challenges and potential IA applications to foster the relevant research development in the domain.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.matdes.2020.109201,Journal,Materials and Design,scopus,2021-01-01,sciencedirect,Online prediction of mechanical properties of hot rolled steel plate using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85092064894,"In industrial steel plate production, process parameters and steel grade composition significantly influence the microstructure and mechanical properties of the steel produced. But determining the exact relationship between process parameters and mechanical properties is a challenging process. This work aimed to devise a deep learning model, to predict mechanical properties of industrial steel plate including yield strength (YS), ultimate tensile strength (UTS), elongation (EL), and impact energy (Akv); based on the process parameters as well as composition of raw steel, and apply it online to a real steel manufacturing plant. An optimal deep neural network (DNN) model was formulated with 27 inputs parameters, 2 hidden layers each having 200 nodes and 4 output parameters (27 × 200 × 200 × 4) with an initial learning rate 0.0001, using Adam optimizer and subjected to Z pre-processing method, to yield an accurate model with R2 = 0.907. The tuned DNN model, had a root mean square error of 21.06 MPa, 16.67 MPa, 2.36%, and 39.33 J, and root mean square percentage error of 4.7%, 2.9%, 7.7%, and 16.2%, for YS, UTS, EL and Akv respectively. Through comparative analysis, it was found that the accuracy of DNN model was higher than other classic machine learning algorithms. To interpret the model assumptions and findings, several local linear models were devised and analyzed to establish the link between process parameters and mechanical properties. Finally the tuned DNN model was deployed in the real-steel plant for online monitoring and control of steel mechanical properties, and to guide the production of targeted steel plates with tailored mechanical properties.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/bs.adcom.2020.08.013,Book Series,Advances in Computers,scopus,2021-01-01,sciencedirect,Empowering digital twins with blockchain,https://api.elsevier.com/content/abstract/scopus_id/85090745248,"A digital twin is an exact digital/logical/cyber/virtual representation/replica of any tangible physical system or process. And the digital twin runs on a competent IT infrastructure (say, cloud centers). In essence, a digital twin is typically a software program that takes various real-world data about a ground-level physical system as prospective inputs and produces useful outputs in the form of insights. The outputs generally are the value-adding and decision-enabling predictions or simulations of how that physical system will act on those inputs. These help in quickly and easily realizing highly optimized and organized products with less cost and risk.
                  The manufacturing industry had embraced the digital twin technology long time back to be modern in their operations, outputs, and offerings. The distinct contributions of the digital twin paradigm, since then, have gone up significantly with the seamless synchronization with a number of pioneering technologies such as the Internet of Things (IoT), artificial intelligence (AI), big and streaming data analytics, data lakes, software-defined cloud environments, blockchain, etc. With the concept of cyber physical systems (CPS) is being adopted and adapted widely and wisely, complicated yet sophisticated electronics devices at the ground level are being blessed with their corresponding digital twins. The digital twins enable data scientists and system designers to optimize a number of things including process excellence, knowledge discovery and dissemination in time, better system design, robust verification and validation, etc. In the recent past, with the flourishing of the blockchain technology, the scope for digital twins has gone up remarkably. This unique combination is bound to produce additional competencies and fresh use cases for enterprises. This chapter is to explain how they integrate and initiate newer opportunities to be grabbed and gained for a better tomorrow.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jclepro.2020.124022,Journal,Journal of Cleaner Production,scopus,2021-01-01,sciencedirect,Artificial intelligence in nuclear industry: Chimera or solution?,https://api.elsevier.com/content/abstract/scopus_id/85090601822,"Nuclear industry is in crisis and innovation is the central theme of its survival in future. Artificial intelligence has made a quantum leap in last few years. This paper comprehensively analyses recent advancement in artificial intelligence for its applications in nuclear power industry. A brief background of machine learning techniques researched and proposed in this domain is outlined. A critical assessment of various nuances of artificial intelligence for nuclear industry is provided. Lack of operational data from real power plant especially for transients and accident scenario is a major concern regarding the accuracy of intelligent systems. There is no universally agreed opinion among researchers for selecting the best artificial intelligence techniques for a specific purpose as intelligent systems developed by various researchers are based on different data set. Interlaboratory work frame or round-robin programme to develop the artificial intelligent tool for any specific purpose, based on the same data base, can be crucial in claiming the accuracy and thus the best technique. The black box nature of artificial techniques also poses a serious challenge for its implementation in nuclear industry, as it makes them prone to fooling.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jobe.2020.101601,Journal,Journal of Building Engineering,scopus,2021-01-01,sciencedirect,Trainingless multi-objective evolutionary computing-based nonintrusive load monitoring: Part of smart-home energy management for demand-side management,https://api.elsevier.com/content/abstract/scopus_id/85087827958,"Electricity is the most widely used form of energy in modern society. One method of satisfying the continuously increasing industrial, commercial, and residential electrical-energy demands of consumers in smart grids is to use an Internet-of-things (IoT) service-oriented electrical-energy management system (EMS) to intrusively monitor and manage electrical loads, which can effectively react to demand-response schemes for demand-side management (DSM). Nonintrusive load monitoring (NILM), a viable cost-effective load disaggregation technique, has recently gained considerable attention as a nonintrusive alternative to EMS in the research field of smart grids. This paper presents a smart IoT-oriented home EMS founded on trainingless multi-objective evolutionary computing-based NILM for DSM in a smart grid. Evolutionary computing-based NILM is considered and addressed as a multi-objective combinatorial optimization problem. The proposed NILM technique can determine the electrical appliances based on their individual electrical characteristics extracted from composite electrical-load consumption with no intrusive deployment of smart plugs or power meters. A fully nonintrusive NILM alternative is considered and proposed. In addition, this alternative is different from conventional NILM because conventional NILM considers artificial intelligence including artificial neural networks (NNs) and deep NN as load classifiers of NILM where training and retraining stages and a hyperparameter tuning procedure are required. The proposed smart IoT-oriented home EMS was experimentally investigated with the trainingless multi-objective evolutionary computing-based NILM in a real house environment. The experimental results confirm that the proposed methodology is feasible.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmsy.2020.06.012,Journal,Journal of Manufacturing Systems,scopus,2021-01-01,sciencedirect,"A digital twin to train deep reinforcement learning agent for smart manufacturing plants: Environment, interfaces and intelligence",https://api.elsevier.com/content/abstract/scopus_id/85087690907,"Filling the gaps between virtual and physical systems will open new doors in Smart Manufacturing. This work proposes a data-driven approach to utilize digital transformation methods to automate smart manufacturing systems. This is fundamentally enabled by using a digital twin to represent manufacturing cells, simulate system behaviors, predict process faults, and adaptively control manipulated variables. First, the manufacturing cell is accommodated to environments such as computer-aided applications, industrial Product Lifecycle Management solutions, and control platforms for automation systems. Second, a network of interfaces between the environments is designed and implemented to enable communication between the digital world and physical manufacturing plant, so that near-synchronous controls can be achieved. Third, capabilities of some members in the family of Deep Reinforcement Learning (DRL) are discussed with manufacturing features within the context of Smart Manufacturing. Trained results for Deep Q Learning algorithms are finally presented in this work as a case study to incorporate DRL-based artificial intelligence to the industrial control process. As a result, developed control methodology, named Digital Engine, is expected to acquire process knowledges, schedule manufacturing tasks, identify optimal actions, and demonstrate control robustness. The authors show that integrating a smart agent into the industrial platforms further expands the usage of the system-level digital twin, where intelligent control algorithms are trained and verified upfront before deployed to the physical world for implementation. Moreover, DRL approach to automated manufacturing control problems under facile optimization environments will be a novel combination between data science and manufacturing industries.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jclepro.2020.123365,Journal,Journal of Cleaner Production,scopus,2020-12-20,sciencedirect,An active preventive maintenance approach of complex equipment based on a novel product-service system operation mode,https://api.elsevier.com/content/abstract/scopus_id/85089891280,"The product-service system (PSS) business model has received increasing attention in equipment maintenance studies, as it has the potential to provide high value-added services for equipment users and construct ethical principles for equipment providers to support the implementation of circular economy. However, the PSS providers in equipment industry are facing many challenges when implementing Industry 4.0 technologies. One important challenge is how to fully collect and analyse the operational data of different equipment and diverse users in widely varied conditions to make the PSS providers create innovative equipment management services for their customers. To address this challenge, an active preventive maintenance approach for complex equipment is proposed. Firstly, a novel PSS operation mode was developed, where complex equipment is offered as a part of PSS and under exclusive control by the providers. Then, a solution of equipment preventive maintenance based on the operation mode was designed. A deep neural network was trained to predict the remaining effective life of the key components and thereby, it can pre-emptively assess the health status of equipment. Finally, a real-world industrial case of a leading CNC machine provider was developed to illustrate the feasibility and effectiveness of the proposed approach. Higher accuracy for predicting the remaining effective life was achieved, which resulted in predictive identification of the fault features, proactive implementation of the preventive maintenance, and reduction of the PSS providers’ maintenance costs and resource consumption. Consequently, the result shows that it can help PSS providers move towards more ethical and sustainable directions.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.oceaneng.2020.108261,Journal,Ocean Engineering,scopus,2020-12-15,sciencedirect,Real-time data-driven missing data imputation for short-term sensor data of marine systems. A comparative study,https://api.elsevier.com/content/abstract/scopus_id/85093700362,"In the maritime industry, sensors are utilised to implement condition-based maintenance (CBM) to assist decision-making processes for energy efficient operations of marine machinery. However, the employment of sensors presents several challenges including the imputation of missing values. Data imputation is a crucial pre-processing step, the aim of which is the estimation of identified missing values to avoid under-utilisation of data that can lead to biased results. Although various studies have been developed on this topic, none of the studies so far have considered the option of imputing incomplete values in real-time to assist instant data-driven decision-making strategies. Hence, a methodological comparative study has been developed that examines a total of 20 widely implemented machine learning and time series forecasting algorithms. Moreover, a case study on a total of 7 machinery system parameters obtained from sensors installed on a cargo vessel is utilised to highlight the implementation of the proposed methodology. To assess the models’ performance seven metrics are estimated (Execution time, MSE, MSLE, RMSE, MAPE, MedAE, Max Error). In all cases, ARIMA outperforms the remaining models, yielding a MedAE of 0.08 r/min and a Max Error of 2.4 r/min regarding the main engine rotational speed parameter.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2020.113653,Journal,Expert Systems with Applications,scopus,2020-12-15,sciencedirect,Cost-sensitive learning classification strategy for predicting product failures,https://api.elsevier.com/content/abstract/scopus_id/85088008188,"In the current era of Industry 4.0, sensor data used in connection with machine learning algorithms can help manufacturing industries to reduce costs and to predict failures in advance. This paper addresses a binary classification problem found in manufacturing engineering, which focuses on how to ensure product quality delivery and at the same time to reduce production costs. The aim behind this problem is to predict the number of faulty products, which in this case is extremely low. As a result of this characteristic, the problem is reduced to an imbalanced binary classification problem. The authors contribute to imbalanced classification research in three important ways. First, the industrial application coming from the electronic manufacturing industry is presented in detail, along with its data and modelling challenges. Second, a modified cost-sensitive classification strategy based on a combination of Voronoi diagrams and genetic algorithm is applied to tackle this problem and is compared to several base classifiers. The results obtained are promising for this specific application. Third, in order to evaluate the flexibility of the strategy, and to demonstrate its wide range of applicability, 25 real-world data sets are selected from the KEEL repository with different imbalance ratios and number of features. The strategy, in this case implemented without a predefined cost, is compared with the same base classifiers as those used for the industrial problem.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpx.2020.100058,Journal,International Journal of Pharmaceutics: X,scopus,2020-12-01,sciencedirect,Deep convolutional neural networks: Outperforming established algorithms in the evaluation of industrial optical coherence tomography (OCT) images of pharmaceutical coatings,https://api.elsevier.com/content/abstract/scopus_id/85096171040,"This paper presents a novel evaluation approach for optical coherence tomography (OCT) image analysis of pharmaceutical solid dosage forms based on deep convolutional neural networks (CNNs). As a proof of concept, CNNs were applied to image data from both, in- and at-line OCT implementations, monitoring film-coated tablets as well as single- and multi-layered pellets. CNN results were compared against results from established algorithms based on ellipse-fitting, as well as to human-annotated ground truth data. Performance benchmarks used include, efficiency (computation speed), sensitivity (number of detections from a defined test set) and accuracy (deviation from the reference method). The results were validated by comparing the output of several algorithms to data manually annotated by human experts and microscopy images of cross-sectional cuts of the same dosage forms as a reference method. In order to guarantee comparability for all results, the algorithms were executed on the same hardware. Since modern OCT systems must operate under real-time conditions in order to be implemented in-line into manufacturing lines, the necessary steps are discussed on how to achieve this goal without sacrificing the algorithmic performance and how to tailor a deep CNN to cope with the high amount of image noise and alterations in object appearance. The developed deep learning approach outperforms static algorithms currently available in pharma applications with respect to performance benchmarks, and represents the next level in real time evaluation of challenging industrial OCT image data.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compind.2020.103329,Journal,Computers in Industry,scopus,2020-12-01,sciencedirect,A Middleware Platform for Intelligent Automation: An Industrial Prototype Implementation,https://api.elsevier.com/content/abstract/scopus_id/85092922057,"The development of dynamic data-based Decision Support Systems (DSSs) along with the increasing availability of data in the industry, makes real-time data acquisition and management a challenge. Intelligent automation appears as a holistic combination of automation with analytics and decisions made by artificial intelligence, delivering smart manufacturing and mass customization while improving resource efficiency. However, challenges towards the development of intelligent automation architectures include the lack of interoperability between systems, complex data preparation steps, and the inability to deal with both high-frequency and high-volume data in a timely fashion. This paper contributes to industrial frameworks focused on the development of standardized system architectures for Industry 4.0, closing the gap between generic architectures and physical realizations. It proposes a platform for intelligent automation relying on a gateway or middleware between field devices, enterprise databases, and DSSs in real-time scenarios. This is achieved by providing the middleware interoperability, determinism, and automatic data structuring over an industrial communication infrastructure such as the OPC UA Standard over Time Sensitive Networks (TSN). Cloud services and database warehousing used to address some of the challenges are handled using fog computing and a multi-workload database. This paper presents an implementation of the platform in the pharmaceutical industry, providing interoperability and real-time reaction capability to changes to an industrial prototype using dynamic scheduling algorithms.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.autcon.2020.103354,Journal,Automation in Construction,scopus,2020-12-01,sciencedirect,Real-time online detection of trucks loading via genetic neural network,https://api.elsevier.com/content/abstract/scopus_id/85091689126,"This article focuses on real-time online detection of trucks loading via genetic neural network. Firstly, according to the state structure of the truck and the deployment of the sensor in the monitoring system, a mathematical model that magnetic sensors detecting the weight of the truck is established, it provides a theoretical basis for the calculation of the compensator deviation. Secondly, a feedback compensator for disturbance signals is designed by genetic neural network in the load monitoring system. Thirdly, the stability of the control system is analyzed by the Lyapunov stability theory. Fourthly, a real-time monitoring system is proposed for the loading of trucks. Finally, a complete experiment is processed to in-depth discussion and analysis. Field experiments showed that this scheme solves the problem of real-time load detection of trucks, it proposes a monitoring system for transportation in the construction industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.autcon.2020.103387,Journal,Automation in Construction,scopus,2020-12-01,sciencedirect,Virtual prototyping- and transfer learning-enabled module detection for modular integrated construction,https://api.elsevier.com/content/abstract/scopus_id/85090569290,"Modular integrated construction is one of the most advanced off-site construction technologies and involves the repetitive process of installing prefabricated prefinished volumetric modules. Automatic detection of location and movement of modules should facilitate progress monitoring and safety management. However, automatic module detection has not been implemented previously. Hence, virtual prototyping and transfer-learning techniques were combined in this study to develop a module-detection model based on mask regions with convolutional neural network (Mask R-CNN). The developed model was trained with datasets comprising both virtual and real images, and it was applied to two modular construction projects for automatic progress monitoring. The results indicate the effectiveness of the developed model in module detection. The proposed method using virtual prototyping and transfer learning not only facilitates the development of automation in modular construction, but also provides a new approach for deep learning in the construction industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ssci.2020.104967,Journal,Safety Science,scopus,2020-12-01,sciencedirect,Risk assessment by failure mode and effects analysis (FMEA) using an interval number based logistic regression model,https://api.elsevier.com/content/abstract/scopus_id/85090003051,"In order to reduce risks of failure, industries use a methodology called Failure Mode and Effects Analysis (FMEA) in terms of the Risk Priority Number (RPN). The RPN number is a product of ordinal scale variables, severity (S), occurrence (O) and detection (D) and product of such ordinal variables is debatable. The three risk attributes (S, O, and D) are generally given equal weightage, but this assumption may not be suitable for real-world applications. Apart from severity, occurrence, and detection, the presence of other risk attributes may also influence the risk of failure and hence should be considered for achieving a holistic approach towards mitigating failure modes. This paper proposes a systematic approach for developing a standard equation for RPN measure, using the methodology of interval number based logistic regression. Instead of utilizing RPN in product form for each failure, this method is benefited from decisions based on probability of risk of failure, 
                        
                           '
                           P
                           '
                        
                      which is more realistic in practical applications. A case study is presented to illustrate the application of the proposed methodology in finding the risk of failure of high capacity submersible pumps in the power plant. The developed logistic regression model (logit model) using R software helped in generating the probability of risk of failure equation for predicting the failures. The model showed the correct classification rate to be 77.47%. The Receiver Operating Characteristic (ROC) curve showed the logit-model to be 81.98% accurate with an optimal cut-off value of 0.56.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.epsr.2020.106742,Journal,Electric Power Systems Research,scopus,2020-12-01,sciencedirect,Learning model of generator from terminal data,https://api.elsevier.com/content/abstract/scopus_id/85089545570,"Assuming that a generator is monitored by the system operator via a PMU device positioned at the generator’s terminal bus, we pose and resolve the question of the real-time, data-driven and automatic monitoring of the generator’s performance. We establish regimes of optimal performance for four complementary techniques ranging from the computationally light (a) Vector Auto-Regressive Model, suitable for normal, linear or almost linear regime, via (b) Long-Short-Term-Memory and (c) Neural ODE Deep Learning models, appropriate to monitor mildly nonlinear regimes, and finally to the (d) physics-informed model. For example, the physics-informed model is capable of fast identification of nonlinear transients and providing interpretable results, suitable, in particular, for corrective actions. The conclusions are reached in the result of validating the models on synthetic data generated in a realistic setting from an open-source, state-of-the-art modeling software. Advanced analysis is followed by a summary and conclusion suitable for the next step - validation of the hierarchy of the suggested data-driven schemes in the industry setting.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jclepro.2020.123125,Journal,Journal of Cleaner Production,scopus,2020-12-01,sciencedirect,A systematic literature review on machine tool energy consumption,https://api.elsevier.com/content/abstract/scopus_id/85088635681,"Energy efficiency has become an integral part of the metal manufacturing industries as a means to improve economic and environmental performance, and increase competitiveness. Machine tools are not only the major energy consumer in the manufacturing industry but also have very low efficiency. Therefore, the analysis of energy consumption by the machine tools is primarily important to understand their complex and dynamic energy consumption behavior. This will lead to the development of better corrective measures. Literature review helps in identifying and assessing the existing knowledge to recognize the future research areas for fostering the research interest on the specific topic. In this review article, the reference literature is identified using a systematic methodology followed by descriptive and content analysis to understand the evolution of research in machining energy. The review focuses on four machining energy aspects – classification, modelling, improvement strategies, and efficiency evaluation. A six level hierarchical model is proposed for better understanding of machining energy classification. The literature review shows that the research in this field intensified after 2009. It is observed that the research focus has shifted towards micro level classification of machining energy including transient states. More detailed and accurate energy consumption models are developed in recent years with increased use of soft computational methods. Real time energy data monitoring and its use for online optimization of machining processes is witnessed. The use of micro analysis, energy benchmarking and standardization of energy assessment indices require more research. Deployment of machining energy models for improving the sustainability of machine tools; data analytics and AI applications; and integration with industry 4.0 are new research opportunities in the field.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.joes.2020.03.003,Journal,Journal of Ocean Engineering and Science,scopus,2020-12-01,sciencedirect,Developing a predictive maintenance model for vessel machinery,https://api.elsevier.com/content/abstract/scopus_id/85086839612,"The aim of maintenance is to reduce the number of failures in equipment and to avoid breakdowns that may lead to disruptions during operations. The objective of this study is to initiate the development of a predictive maintenance solution in the shipping industry based on a computational artificial intelligence model using real-time monitoring data. The data analysed originates from the historical values from sensors measuring the vessel´s engines and compressors health and the software used to analyse these data was R. The results demonstrated key parameters held a stronger influence in the overall state of the components and proved in most cases strong correlations amongst sensor data from the same equipment. The results also showed a great potential to serve as inputs for developing a predictive model, yet further elements including failure modes identification, detection of potential failures and asset criticality are some of the issues required to define prior designing the algorithms and a solution based on artificial intelligence. A systematic approach using big data and machine learning as techniques to create predictive maintenance strategies is already creating disruption within the shipping industry, and maritime organizations need to consider how to implement these new technologies into their business operations and to improve the speed and accuracy in their maintenance decision making.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ejor.2020.05.010,Journal,European Journal of Operational Research,scopus,2020-12-01,sciencedirect,Data-driven optimization model customization,https://api.elsevier.com/content/abstract/scopus_id/85086372620,"When embedded in software-based decision support systems, optimization models can greatly improve organizational planning. In many industries, there are classical models that capture the fundamentals of general planning decisions (e.g., designing a delivery route). However, these models are generic and often require customization to truly reflect the realities of specific operational settings. Yet, such customization can be an expensive and time-consuming process. At the same time, popular cloud computing software platforms such as Software as a Service (SaaS) are not amenable to customized software applications. We present a framework that has the potential to autonomously customize optimization models by learning mathematical representations of customer-specific business rules from historical data derived from model solutions and implemented plans. Because of the wide-spread use in practice of mixed integer linear programs (MILP) and the power of MILP solvers, the framework is designed for MILP models. It uses a common mathematical representation for different optimization models and business rules, which it encodes in a standard data structure. As a result, a software provider employing this framework can develop and maintain a single code-base while meeting the needs of different customers. We assess the effectiveness of this framework on multiple classical MILPs used in the planning of logistics and supply chain operations and with different business rules that must be observed by implementable plans. Computational experiments based on synthetic data indicate that solutions to the customized optimization models produced by the framework are regularly of high-quality.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1177/2472555220923332,Journal,SLAS Discovery,scopus,2020-12-01,sciencedirect,"Recommended Guidelines for Developing, Qualifying, and Implementing Complex In Vitro Models (CIVMs) for Drug Discovery",https://api.elsevier.com/content/abstract/scopus_id/85085937026,"The pharmaceutical industry is continuing to face high research and development (R&D) costs and low overall success rates of clinical compounds during drug development. There is an increasing demand for development and validation of healthy or disease-relevant and physiological human cellular models that can be implemented in early-stage discovery, thereby shifting attrition of future therapeutics to a point in discovery at which the costs are significantly lower. There needs to be a paradigm shift in the early drug discovery phase (which is lengthy and costly), away from simplistic cellular models that show an inability to effectively and efficiently reproduce healthy or human disease-relevant states to steer target and compound selection for safety, pharmacology, and efficacy questions. This perspective article covers the various stages of early drug discovery from target identification (ID) and validation to the hit/lead discovery phase, lead optimization, and preclinical safety. We outline key aspects that should be considered when developing, qualifying, and implementing complex in vitro models (CIVMs) during these phases, because criteria such as cell types (e.g., cell lines, primary cells, stem cells, and tissue), platform (e.g., spheroids, scaffolds or hydrogels, organoids, microphysiological systems, and bioprinting), throughput, automation, and single and multiplexing endpoints will vary. The article emphasizes the need to adequately qualify these CIVMs such that they are suitable for various applications (e.g., context of use) of drug discovery and translational research. The article ends looking to the future, in which there is an increase in combining computational modeling, artificial intelligence and machine learning (AI/ML), and CIVMs.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.patter.2020.100107,Journal,Patterns,scopus,2020-11-13,sciencedirect,Wiz: A Web-Based Tool for Interactive Visualization of Big Data,https://api.elsevier.com/content/abstract/scopus_id/85097417500,"In an age of information, visualizing and discerning meaning from data is as important as its collection. Interactive data visualization addresses both fronts by allowing researchers to explore data beyond what static images can offer. Here, we present Wiz, a web-based application for handling and visualizing large amounts of data. Wiz does not require programming or downloadable software for its use and allows scientists and non-scientists to unravel the complexity of data by splitting their relationships through 5D visual analytics, performing multivariate data analysis, such as principal component and linear discriminant analyses, all in vivid, publication-ready figures. With the explosion of high-throughput practices for materials discovery, information streaming capabilities, and the emphasis on industrial digitalization and artificial intelligence, we expect Wiz to serve as an invaluable tool to have a broad impact in our world of big data.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jclepro.2020.122870,Journal,Journal of Cleaner Production,scopus,2020-11-10,sciencedirect,Enhancing the adaptability: Lean and green strategy towards the Industry Revolution 4.0,https://api.elsevier.com/content/abstract/scopus_id/85088397153,"Industry 4.0 has brought forth many advantages and challenges for the industry players. Many organizations are strategizing to take advantage of this industrial paradigm shift, thus improving the sustainability of the enterprise. However, there are many factors such as talent development, machinery advancement and infrastructure development which involve huge investment that need to be considered. This paper presents an enhanced adaptive model for the implementation of the lean and green (L&G) strategy in processing sectors to solve dynamic industry problems associated with Industry 4.0. A feature of this enhanced adaptive model is that it combines experts’ experience and operational data as input in dealing with real industry application. A lean and green index is coupled in the model to serve as a benchmark and process improvement tracking indicator. This allows the industrialists to set a lean and green index (LGI) target for effective process improvement. From this integrated model, an ensemble of backpropagation optimizers is then used to identify the best-optimized strategy. This ensemble optimizer is formulated to perform operation improvement and update the targeted LGI automatically when a higher index is achieved for continuous improvement. A case study on a combined heat and power plant is performed and reflects an improvement of 18.25% on the LGI. This work serves as a practical transition strategy for the industrialist desiring to improve the sustainability of the facility with Industry 4.0 elements at minimum investment cost.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2020.106868,Journal,Computers and Industrial Engineering,scopus,2020-11-01,sciencedirect,Simulation in industry 4.0: A state-of-the-art review,https://api.elsevier.com/content/abstract/scopus_id/85091194972,"Simulation is a key technology for developing planning and exploratory models to optimize decision making as well as the design and operations of complex and smart production systems. It could also aid companies to evaluate the risks, costs, implementation barriers, impact on operational performance, and roadmap toward Industry 4.0. Although several advances have been made in this domain, studies that systematically characterize and analyze the development of simulation-based research in Industry 4.0 are scarce. Therefore, this study aims to investigate the state-of-the-art research performed on the intersecting area of simulation and the field of Industry 4.0. Initially, a conceptual framework describing Industry 4.0 in terms of enabling technologies and design principles for modeling and simulation of Industry 4.0 scenarios is proposed. Thereafter, literature on simulation technologies and Industry 4.0 design principles is systematically reviewed using the preferred reporting items for systematic reviews and meta-analyses (PRISMA) methodology. This study reveals an increasing trend in the number of publications on simulation in Industry 4.0 within the last four years. In total, 10 simulation-based approaches and 17 Industry 4.0 design principles were identified. A cross-analysis of concepts and evaluation of models’ development suggest that simulation can capture the design principles of Industry 4.0 and support the investigation of the Industry 4.0 phenomenon from different perspectives. Finally, the results of this study indicate hybrid simulation and digital twin as the primary simulation-based approaches in the context of Industry 4.0.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2020.106368,Journal,Information and Software Technology,scopus,2020-11-01,sciencedirect,Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions,https://api.elsevier.com/content/abstract/scopus_id/85087690796,"Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems.
                  
                     Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges.
                  
                     Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment.
                  
                     Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions.
                  
                     Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.petrol.2020.107509,Journal,Journal of Petroleum Science and Engineering,scopus,2020-11-01,sciencedirect,"Design and construction of the knowledge base system for geological outfield cavities classifications: An example of the fracture-cavity reservoir outfield in Tarim basin, NW China",https://api.elsevier.com/content/abstract/scopus_id/85087076723,"Tahe oilfield, located in NW Tarim Basin, is one of the largest and most difficult fracture cavity reservoirs in the world. Different fracture cavities, different generated mechanisms, and different oil production capacities. In order to study the significant parameters that can characterize the categories of facture-cavity. This research adopted outfield manual measurement, 3D digital modeling technique to obtain characterization parameters. According to experienced geological survey, typical outcrops were selected, then scanned by UAV (Unmanned aerial vehicle). Consequently, 3D digital models, including real coordinates and parameter information, were established by Agisoft Photoscan. Through geological testing results, various combination characteristic patterns of relative categories were analyzed. By using digital measure tool, combined with manually measured data, the parameters were extracted from the 3D digital model (DM). Then an initial geological database was established. For furtherly analyzing the database, the mathematic statistics methods of multiple linear regression (MLR), neural network technique (NNT) and discriminative classification technique (DCT) were applied. Using software of SPSS statistics 17.0, more than 200 groups of geological data (various categories of fracture-cavity) were optimally processed. Consequently, the significant characteristic parameters were interpreted to determine diverse categories. The results showed that: (1) cavity width, height, fracture length and cavity aspect ratio were significant parameters to classify runoff cavity categories. (2) Fault-controlled cavities could be accurately classified by fracture length and fracture density. (3) The main cavity categories could be distinguished by cavity width, cavity height and fracture density. Performances of the approach have been examined with 10 percentages of the samples, and a good agreement performed in the simulated results, and anastomosis rate was more than 80%. The researched results have critical guiding significance to evaluate types of fracture-cavity, develop and explore of fracture-cavity reservoirs. The construction technique of knowledge base can be applied for diverse fracture-cavity reservoirs in the various formations in different areas in the world.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.measurement.2020.108043,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2020-11-01,sciencedirect,Smart frost measurement for anti-disaster intelligent control in greenhouses via embedding IoT and hybrid AI methods,https://api.elsevier.com/content/abstract/scopus_id/85086577761,"A novel Agro-industrial IoT (AIIoT) technology and architecture for intelligent frost forecasting in greenhouses via hybrid Artificial Intelligence (AI), is reported. The Internet of Things (IoT) allows the objects interconnection on the physical world using sensors and actuators via the Internet. The smart system was designed and implemented through a climatological station equipped with Artificial Neural Networks (ANN) and a fuzzy associative memory (FAM) for ecological control of the anti-frost disaster irrigation. The ANN forecasts the inside temperature of the greenhouses and the fuzzy control predicts the cropland temperatures for the activation of five output levels of the water pump. The results were compared to a Fourier-statistical analysis of hourly data, showing that the ANN models provide a temperature prediction with effectiveness higher than 90%, as compared to monthly data model. Moreover, results of this process were validated through the determination of the coefficient of variance analysis method (
                        
                           
                              
                                 R
                              
                              2
                           
                        
                     ).",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.measurement.2020.108052,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2020-11-01,sciencedirect,Deep learning-based prognostic approach for lithium-ion batteries with adaptive time-series prediction and on-line validation,https://api.elsevier.com/content/abstract/scopus_id/85086367941,"Prognostics for lithium-ion batteries is very critical in many industrial applications, and accurate prediction of battery state of health (SOH) is of great importance for health management. This paper proposes a novel deep learning-based prognostic method for lithium-ion batteries with on-line validation. An effective variant of recurrent neural network, i.e. long short-term memory structure, is used with variable input dimension, that facilitates network training with additional labeled samples. Adaptive time-series predictions are carried out for prognostics. An on-line validation method is further proposed for parameter optimization in real time based on the available system information, which allows for continuous model improvement. Experiments on a popular lithium-ion battery dataset are implemented to validate the effectiveness and superiority of the proposed method. The experimental results show the prognostic performances are promising both for the multi-steps-ahead predictions and long-horizon SOH estimations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.micpro.2020.103227,Journal,Microprocessors and Microsystems,scopus,2020-10-01,sciencedirect,Fog Computing-inspired Smart Home Framework for Predictive Veterinary Healthcare,https://api.elsevier.com/content/abstract/scopus_id/85089574391,"Domestic Pet Care has been an important domain in the healthcare industry. In the presented study, a comprehensive framework of the Smart VetCare system for the health monitoring of domestic pets has been presented. The work is focused on the remote surveillance of domestic animals’ health conditions inside the home environment using IoMT Technology. Specifically, pet health is analyzed for vulnerability in the ambient home environment and ubiquitous activities over a fog computing platform of FogBus. Furthermore, a temporal data granule is formulated and the Probability of Health Vulnerability (PoHV) is defined for determining the health severity of the animal. Additionally, the Temporal Sensitivity Measure (TSM) is defined for real-time pet healthcare analysis, which is visualized using the Self Organized Mapping (SOM) Technique. For validation purposes, the framework is deployed in the smart home environment using 12 IoMT WiSense Nodes and Health Sensor belt for monitoring a domestic dog of American Bully breed over the dynamic resource management platform of FogBus and iFogSim simulator. Based on the comparison with numerous state-of-the-art techniques, the proposed framework can register a better precision value (94.78%), accuracy value (95.38%), sensitivity value (93.71%), and f-measure value (94.41%).",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.nucengdes.2020.110817,Journal,Nuclear Engineering and Design,scopus,2020-10-01,sciencedirect,Machine learning enabled advanced manufacturing in nuclear engineering applications,https://api.elsevier.com/content/abstract/scopus_id/85089553568,"Advanced manufacturing has gained tremendous interest in both research and industry in the past few years. Over nearly the same period of time, machine learning (ML) has made phenomenal advancements, finding its way into many aspects of manufacturing. For the nuclear engineering field, the adoption of advanced manufacturing is a compelling argument due to the ambitious challenges the field faces. The combination of advanced manufacturing with ML holds great potential in the nuclear engineering field, and even further development is needed to accelerate their deployment towards real-world applications. This review paper seeks to detail several key aspects of ML enabled advanced manufacturing that are used or could prove useful to nuclear applications ranging from radiation detector materials to reactor parts fabrication. The applications covered here include new material extrapolation, manufacturing defect detection, and additive manufacturing parameters’ optimization.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aei.2020.101136,Journal,Advanced Engineering Informatics,scopus,2020-10-01,sciencedirect,Ensemble deep learning based semi-supervised soft sensor modeling method and its application on quality prediction for coal preparation process,https://api.elsevier.com/content/abstract/scopus_id/85087393963,"Coal preparation is the most effective and economical technique to reduce impurities and improve the product quality for run-of-mine coal. The timely and accurate prediction for key quality characteristics of separated coal plays a significant role in condition monitoring and production control. However, these quality characteristics are usually difficult to directly measure online in industrial practices Although some computation intelligence based soft sensor modeling methods have been developed and reported in existing research for these quality variables estimation, some problems still exist, i.e., manual feature extraction, considerable unlabeled data, temporal dynamic behavior in data, which will influence the accuracy and efficiency for established soft sensor model. To address above-mentioned problem and develop an more excellent quality prediction model for coal preparation process, a novel deep learning based semi-supervised soft sensor modeling approach is proposed which combining the advantage of unsupervised deep learning technique (i.e., Stacked Auto-Encoder (SAE)) with the advantage of supervised deep bidirectional recurrent learner (i.e., Bidirectional Long Short-Term Memory (BLSTM)). More specifically, the unsupervised SAE networks are implemented to learn the representative features hidden in all available input data (labeled and unlabeled samples) and store them as context vector. Then, partial context vector with corresponding labels and the quality variable measure value at previous time are concatenated to form a new merged input feature vector. After that, the temporal and dynamic features are further extracted from the new merged input feature vector via BLSTM networks. Subsequently, the fully connected layers (FCs) are exploited to learn the higher-level features from the last hidden layer of the BLSTM. Lastly, the learned output features by FCs are fed into a supervised liner regression layer to predict the coal quality metrics. Meanwhile, to avoid over-fitting, some regularization techniques are utilized and discussed in proposed network. The application in ash content estimation for a real dense medium coal preparation process and some comparison experiment result demonstrate that the effectiveness and priority of proposed soft sensor modeling approach.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.patrec.2020.06.028,Journal,Pattern Recognition Letters,scopus,2020-10-01,sciencedirect,On the use of a full stack hardware/software infrastructure for sensor data fusion and fault prediction in industry 4.0,https://api.elsevier.com/content/abstract/scopus_id/85087339064,"Aspects related to prognostics are becoming a crucial part in the industrial sector. In this sense, Industry 4.0 is considered as a new paradigm that leverages on the IoT to propose increasingly more solutions to provide an estimate on the working conditions of an industrial plant. However, in context like the industrial sector where the number and heterogeneity of sensors can be very large, and the time requirements are very stringent, emerges the challenge to design effective infrastructures to interact with these complex systems. In this paper, we propose a full stack hardware/software infrastructure to collect, manage, and analyze the data gathered from a set of heterogeneous sensors attached to a real scale replica industrial plant available in our laboratory. On top of the proposed infrastructure we designed and implemented a fault prediction algorithm which exploits sensors data fusion with the aim to assess the working conditions of the industrial plant. The result section shows the obtained results in terms of accuracy from testing our proposed model and provides a comparison with a traditional Deep Neural Network (DNN) topology.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ins.2020.05.028,Journal,Information Sciences,scopus,2020-10-01,sciencedirect,Input selection methods for data-driven Soft sensors design: Application to an industrial process,https://api.elsevier.com/content/abstract/scopus_id/85086080455,"Soft Sensors (SSs) are inferential models which are widely used in industry. They are generally built through data-driven approaches that exploit industry historical databases. Selection of input variables is one of the most critical issues in SSs design. This paper aims at highlighting difficulties arising from the implementation of data-driven input selection methods when solving real-world case studies. A procedure is, therefore, proposed for input selection, based on both data-driven and expert-driven input selection methods. The procedure allows designing SSs with good prediction accuracy and a low number of inputs.
                  The design of an SS for a real-world industrial process is used. The results reported show that the selection methods proposed in literature do not give consistent results when applied to the considered case study. The key role for plant expert knowledge emerges, outlining the opportunity of judicious use of automatic data-driven procedures.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.knosys.2020.106178,Journal,Knowledge-Based Systems,scopus,2020-09-27,sciencedirect,Deep learning-based unsupervised representation clustering methodology for automatic nuclear reactor operating transient identification,https://api.elsevier.com/content/abstract/scopus_id/85087409980,"Transient identification of condition monitoring data in nuclear reactor is important for system health assessment. Conventionally, the operating transients are correlated with the pre-designed ones by human operators during operations. However, due to necessary conservatism and significant differences between the operating and pre-designed transients, it has been less effective to manually identify transients, that usually contribute to different system degradation modes. This paper proposes a deep learning-based unsupervised representation clustering method for automatic transient pattern recognition based on the on-site condition monitoring data. Sample entropy is used as indicator for transient extraction, and a pre-training stage is implemented using an auto-encoder architecture for learning high-level features. An iterative representation clustering algorithm is further proposed to enhance the clustering effects, where a novel distance metric learning strategy is integrated. Experiments on a real-world nuclear reactor condition monitoring dataset validate the effectiveness and superiority of the proposed method, which provides a promising tool for transient identification in the real industrial scenarios. This study offers a new perspective in exploring unlabeled data with deep learning, and the end-to-end implementation scheme facilitates applications in the real nuclear industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cjche.2020.06.015,Journal,Chinese Journal of Chemical Engineering,scopus,2020-09-01,sciencedirect,Deep learning technique for process fault detection and diagnosis in the presence of incomplete data,https://api.elsevier.com/content/abstract/scopus_id/85089986909,"In modern industrial processes, timely detection and diagnosis of process abnormalities are critical for monitoring process operations. Various fault detection and diagnosis (FDD) methods have been proposed and implemented, the performance of which, however, could be drastically influenced by the common presence of incomplete or missing data in real industrial scenarios. This paper presents a new FDD approach based on an incomplete data imputation technique for process fault recognition. It employs the modified stacked autoencoder, a deep learning structure, in the phase of incomplete data treatment, and classifies data representations rather than the imputed complete data in the phase of fault identification. A benchmark process, the Tennessee Eastman process, is employed to illustrate the effectiveness and applicability of the proposed method.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.robot.2020.103578,Journal,Robotics and Autonomous Systems,scopus,2020-09-01,sciencedirect,Real-time topological localization using structured-view ConvNet with expectation rules and training renewal,https://api.elsevier.com/content/abstract/scopus_id/85086575996,"Mobile service robots possess high potential of providing numerous assistances in the working areas. In an attempt to develop a mobile service robot which is dynamically balanced for faster movement and taller manipulation capability, we designed and prototyped J4.alpha, which is intended for swift navigation and nimble manipulation. Previously, we devised a pure visual method based on a supervised deep learning model for real-time recognition of nodal locations. Four low-resolution RGB cameras are installed around J4.alpha to capture the surrounding visual features for training and detection. As the method is developed for ease of implementation, fast real-time application, accurate detection, and low cost, we further improve the accuracy and the practicality of the method in this study. Specifically, a set of expectation rules are introduced to reject outlier detections, and a scheme of training renewal is devised to effectively react to environmental modifications. In our previous tests, precision and recall rates of the location coordinate detection by the ConvNet models were generally between 0.78 and 0.91; by introducing the expectation rules, precision and recall are improved by approximately 10%. A large scale field test is also carried out here for both corridor and factory scenarios; the performance of the proposed method was tested for detection accuracy and verified for 2 m and 0.5 m nodal intervals. The scheme of training renewal designed for capturing and reflecting environmental modifications was also proved to be effective.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.scs.2020.102252,Journal,Sustainable Cities and Society,scopus,2020-09-01,sciencedirect,A deep learning-based IoT-oriented infrastructure for secure smart City,https://api.elsevier.com/content/abstract/scopus_id/85085594643,"In recent years, the Internet of Things (IoT) infrastructures are developing in various industrial applications in sustainable smart cities and societies such as smart manufacturing, smart industries. The Cyber-Physical System (CPS) is also part of IoT-oriented infrastructure. CPS has gained considerable success in industrial applications and critical infrastructure with a distributed environment. This system aims to integrate the physical world to computational facilities as cyberspace. However, there are many challenges, such as security and privacy, centralization, communication latency, scalability in such an environment. To mitigate these challenges, we propose a Deep Learning-based IoT-oriented infrastructure for a secure smart city where Blockchain provides a distributed environment at the communication phase of CPS, and Software-Defined Networking (SDN) establishes the protocols for data forwarding in the network. A deep learning-based cloud is utilized at the application layer of the proposed infrastructure to resolve communication latency and centralization, scalability. It enables cost-effective, high-performance computing resources for smart city applications such as the smart industry, smart transportation. Finally, we evaluated the performance of our proposed infrastructure. We compared it with existing methods using quantitative analysis and security and privacy analysis with different measures such as scalability and latency. The evaluation of our implementation results shows that performance is improved.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compind.2020.103226,Journal,Computers in Industry,scopus,2020-09-01,sciencedirect,Perspective on holonic manufacturing systems: PROSA becomes ARTI,https://api.elsevier.com/content/abstract/scopus_id/85085261123,"Looking back at 30 years of research into holonic manufacturing systems, these explorations made a lasting scientific contribution to the overall architecture of intelligent manufacturing systems. Most notably, holonic architectures are defined in terms of their world-of-interest (Van Brussel et al., 1998). They do not have an information layer, a communication layer, etc. Instead, they have components that relate to real-world assets (e.g. machine tools) and activities (e.g. assembly). And, they mirror and track the structure of their world-of-interest, which allows them to scale and adapt accordingly.
                  This research has wandered around, at times learning from its mistakes, and progressively carved out an invariant structure while it translated and applied scientific insights from complex-adaptive systems theory (e.g. autocatalytic sets) and from bounded rationality (e.g. holons). This paper presents and discusses the outcome of these research efforts.
                  At the top level, the holonic structure distinguishes intelligent beings (or digital twins) from intelligent agents. These digital twins inherit the consistency from reality, which they mirror. They are intelligent beings when they reflect what exists in the world without imposing artificial limitations in this reality. Consequently, a conflict with a digital twin is a conflict with reality.
                  In contrast, intelligent agents typically transform NP-hard challenges into computations with low-polynomial complexity. Unavoidably, this involves arbitrariness (e.g. don’t care choices). Likewise, relying on case-specific properties, to ensure an outcome in polynomial time, usually renders the validity of an agent’s choices both short-lived and situation-dependent. Here, intelligent agents create conflicts by imposing limitations of their own making in their world-of-interest.
                  Real-world smart systems are aggregates comprising both intelligent beings and intelligent agents. They are performers. Inside these performers, digital twins may constitute the foundations, supporting walls, support beams and pillars because these intelligent beings are protected by their real-world counterpart. Further refining the top-level of this architecture, a holonic structure enables these digital twins to shadow their real-world counterpart whenever it changes, adapts and evolves.
                  In contrast, the artificial limitations, imposed by the intelligent agents, cannot be allowed to build up inertia, which would hamper the undoing of arbitrary or case-specific limitations. To this end, performers explicitly manage the rights over their assets. Revoking such rights from a limitation-imposing agent will free the assets. This will be at the cost of reduced services from the agent. When other service providers rely on this agent, their services may be affected as well; that’s how the inertia builds up and how harmful legacy is created. Thus, the services of digital twins are to be preferred over the services of an intelligent agent by developers of holonic manufacturing systems.
                  Finally, digital twins corresponding to the decision making in the world-of-interest (a non-physical asset) allow to mirror the world-of-interest in a predictive mode (in addition to track and trace). It allows to generate short-term forecasts while preserving the benefits of intelligent beings. These twins are the intentions of the decision-making intelligent agents. Evidently, when intentions change, the forecasts needs to be regenerated (i.e. tracking the corresponding reality by the twin). This advanced feature can be deployed in a number of configurations (cf. annex).",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compind.2020.103244,Journal,Computers in Industry,scopus,2020-09-01,sciencedirect,Machine learning for predictive scheduling and resource allocation in large scale manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85084401966,"The digitalization processes in manufacturing enterprises and the integration of increasingly smart shop floor devices and software control systems caused an explosion in the data points available in Manufacturing Execution Systems. The degree in which enterprises can capture value from big data processing and extract useful insights represents a differentiating factor in developing controls that optimize production and protect resources. Machine learning and Big Data technologies have gained increased traction being adopted in some critical areas of planning and control. Cloud manufacturing allows using these technologies in real time, lowering the cost of implementing and deployment. In this context, the paper offers a machine learning approach for reality awareness and optimization in cloud.
                  Specifically, the paper focuses on predictive production planning (operation scheduling, resource allocation) and predictive maintenance. The main contribution of this research consists in developing a hybrid control solution that uses Big Data techniques and machine learning algorithms to process in real time information streams in large scale manufacturing systems, focusing on energy consumptions that are aggregated at various layers. The control architecture is distributed at the edge of the shop floor for data collecting and format transformation, and then centralized at the cloud computing platform for data aggregation, machine learning and intelligent decisions. The information is aggregated in logical streams and consolidated based on relevant metadata; a neural network is trained and used to determine possible anomalies or variations relative to the normal patterns of energy consumption at each layer. This novel approach allows for accurate forecasting of energy consumption patterns during production by using Long Short-term Memory neural networks and deep learning in real time to re-assign resources (for batch cost optimization) and detect anomalies (for robustness) based on predicted energy data.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2020.02.109,Journal,Neurocomputing,scopus,2020-08-04,sciencedirect,Tracking control of redundant mobile manipulator: An RNN based metaheuristic approach,https://api.elsevier.com/content/abstract/scopus_id/85082490397,"In this paper, we propose a topology of Recurrent Neural Network (RNN) based on a metaheuristic optimization algorithm for the tracking control of mobile-manipulator while enforcing nonholonomic constraints. Traditional approaches for tracking control of mobile robots usually require the computation of Jacobian-inverse or linearization of its mathematical model. The proposed algorithm uses a nature-inspired optimization approach to directly solve the nonlinear optimization problem without any further transformation. First, we formulate the tracking control as a constrained optimization problem. The optimization problem is formulated on position-level to avoid the computationally expensive Jacobian-inversion. The nonholonomic limitation is ensured by adding equality constraints to the formulated optimization problem. We then present the Beetle Antennae Olfactory Recurrent Neural Network (BAORNN) algorithm to solve the optimization problem efficiently using very few mathematical operations. We present a theoretical analysis of the proposed algorithm and show that its computational cost is linear with respect to the degree of freedoms (DOFs), i.e., O(m). Additionally, we also prove its stability and convergence. Extensive simulation results are prepared using a simulated model of IIWA14, a 7-DOF industrial-manipulator, mounted on a differentially driven cart. Comparison results with particle swarm optimization (PSO) algorithm are also presented to prove the accuracy and numerical efficiency of the proposed controller. The results demonstrate that the proposed algorithm is several times (around 75 in the worst case) faster in execution as compared to PSO, and suitable for real-time implementation. The tracking results for three different trajectories; circular, rectangular, and rhodonea paths are presented.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.heliyon.2020.e04667,Journal,Heliyon,scopus,2020-08-01,sciencedirect,Effects of mobile augmented reality apps on impulse buying behavior: An investigation in the tourism field,https://api.elsevier.com/content/abstract/scopus_id/85089806662,"Many of today's online services are designed specifically to encourage impulse buying. Moreover, many studies have shown that with the assistance of Mobile Augmented Reality, retailers have the potential to significantly improve their sales. However, the effects of Mobile AR on consumer impulse buying behavior have yet to be examined, particularly in the tourism field. Consequently, the present study integrates the Technology Acceptance Model (TAM), Stimulus-Organism-Response (SOR) framework, and flow theory to examine the effects of Mobile AR apps on tourist impulse buyingbehavior. The research model is implemented using an online questionnaire, with the results analyzed by Partial-Least-Squares Structural Equation Modeling (PLS-SEM) approach. The results obtained from 479 valid samples show that the characteristics of Mobile AR apps play an important role in governing tourist behavior in making unplanned purchases. In particular, as the utility, ease-of-use, and interactivity of the apps increase, the perceived enjoyment and satisfaction of the user also increase and give rise to a stronger impulse buying behavior. The results also reveal a mediating effect of the flow experience on the relationship between the perceived ease of use of the Mobile AR app and the user satisfaction in using the app. Overall, the findings presented in this study provide a useful source of reference for Mobile AR app developers, retailers, and tourism marketers in better understanding users' preferences for Mobile AR apps and strengthening their impulse buying behavior in the tourism context as a result.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aei.2020.101101,Journal,Advanced Engineering Informatics,scopus,2020-08-01,sciencedirect,Predictive model-based quality inspection using Machine Learning and Edge Cloud Computing,https://api.elsevier.com/content/abstract/scopus_id/85084733420,"The supply of defect-free, high-quality products is an important success factor for the long-term competitiveness of manufacturing companies. Despite the increasing challenges of rising product variety and complexity and the necessity of economic manufacturing, a comprehensive and reliable quality inspection is often indispensable. In consequence, high inspection volumes turn inspection processes into manufacturing bottlenecks.
                  In this contribution, we investigate a new integrated solution of predictive model-based quality inspection in industrial manufacturing by utilizing Machine Learning techniques and Edge Cloud Computing technology. In contrast to state-of-the-art contributions, we propose a holistic approach comprising the target-oriented data acquisition and processing, modelling and model deployment as well as the technological implementation in the existing IT plant infrastructure. A real industrial use case in SMT manufacturing is presented to underline the procedure and benefits of the proposed method. The results show that by employing the proposed method, inspection volumes can be reduced significantly and thus economic advantages can be generated.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.physa.2019.124049,Journal,Physica A: Statistical Mechanics and its Applications,scopus,2020-08-01,sciencedirect,Fast Super-Paramagnetic Clustering,https://api.elsevier.com/content/abstract/scopus_id/85078038012,"We map stock market interactions to spin models to recover their hierarchical structure using a simulated annealing based Super-Paramagnetic Clustering (SPC) algorithm. This is directly compared to a modified implementation of a maximum likelihood approach we call fast Super-Paramagnetic Clustering (f-SPC). The methods are first applied to standard toy test-case problems, and then to a data-set of 447 stocks traded on the New York Stock Exchange (NYSE) over 1249 days. The signal to noise ratio of stock market correlation matrices is briefly considered. Our result recover approximately clusters representative of standard economic sectors and mixed ones whose dynamics shine light on the adaptive nature of financial markets and raise concerns relating to the effectiveness of industry based static financial market classification in the world of real-time data analytics. A key result is that we show that f-SPC maximum likelihood solutions converge to ones found within the Super-Paramagnetic Phase where the entropy is maximum, and those solutions are qualitatively better for high dimensionality data-sets.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.measurement.2020.107768,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2020-07-15,sciencedirect,"Intelligent fault diagnosis of rotating machinery via wavelet transform, generative adversarial nets and convolutional neural network",https://api.elsevier.com/content/abstract/scopus_id/85082880587,"The fault detection of rotating machinery systems especially its typical components such as bearings and gears is of special importance for maintaining machine systems working normally and safely. However, due to the change of working conditions, the disturbance of environment noise, the weakness of early features and various unseen compound failure modes, it is quite hard to achieve high-accuracy intelligent failure monitoring task of rotating machinery using existing intelligent fault diagnosis approaches in real industrial applications. In the paper, a novel and high-accuracy fault detection approach named WT-GAN-CNN for rotating machinery is presented based on Wavelet Transform (WT), Generative Adversarial Nets (GANs) and convolutional neural network (CNN). The proposed WT-GAN-CNN approach includes three parts. To begin with, WT is employed for extracting time-frequency image features from one-dimension raw time domain signals. Secondly, GANs are used to generate more training image samples. Finally, the built CNN model is used to accomplish the fault detection of rotating machinery by the original training time-frequency images and the generated fake training time-frequency images. Two experiment studies are implemented to assess the effectiveness of our proposed approach and the results demonstrate it is higher in testing accuracy than other intelligent failure detection approaches in the literatures even in the interference of strong environment noise or when working conditions are changed. Furthermore, its result in the stability of testing accuracy is also quite excellent.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.telpol.2020.101960,Journal,Telecommunications Policy,scopus,2020-07-01,sciencedirect,Innovation ecosystems theory revisited: The case of artificial intelligence in China,https://api.elsevier.com/content/abstract/scopus_id/85083340447,"Beyond the mainstream discussion on the key role of China in the global AI landscape, the knowledge about the real performance and future perspectives of the AI ecosystem in China is still limited. This paper evaluates the status and prospects of China's AI innovation ecosystem by developing a Triple Helix framework particularized for this case. Based on an in-depth qualitative study and on interviews with experts, the analysis section summarizes the way in which the AI innovation ecosystem in China is being built, which are the key features of the three spheres of the Triple Helix -governments, industry and academic/research institutions-as well as the dynamic context of the ecosystem through the identification of main aspects related to the flows of skills, knowledge and funding and the interactions among them. Using this approach, the discussion section illustrates the specificities of the AI innovation ecosystem in China, its strengths and its gaps, and which are its prospects. Overall, this revisited ecosystem approach permits the authors to address the complexity of emerging environments of innovation to draw meaningful conclusions which are not possible with mere observation. The results show how a favourable context, the broad adoption rate and the competition for talent and capital among regional-specialized clusters are boosting the advance of AI in China, mainly in the business to customer arena. Finally, the paper highlights the challenges ahead in the current implementation of the ecosystem that will largely determine the potential global leadership of China in this domain.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ins.2020.03.063,Journal,Information Sciences,scopus,2020-07-01,sciencedirect,Generating behavior features for cold-start spam review detection with adversarial learning,https://api.elsevier.com/content/abstract/scopus_id/85083304717,"Due to the wide applications, spam detection has long been a hot research topic in both academia and industry. Existing studies show that behavior features are effective in distinguishing the spam and legitimate reviews. However, it usually takes a long time to collect such features and thus is hard to apply them to cold-start spam review detection tasks. Recent advances leveraged the neural network to encode the various types of textual, behavior, and attribute information for this task. However, the inherent problem, i.e., lack of effective behavior features for new users who post just one review, is still unsolved.
                  In this paper, we exploit the generative adversarial network (GAN) for addressing this problem. The key idea is to generate synthetic behavior features (SBFs) for new users from their easily accessible features (EAFs). Specifically, we first select six well recognized real behavior features (RBFs) existing for regular users. We then train a GAN framework including a generator to generate SBFs from their EAFs including text, rating, and attribute features, and a discriminator to discriminate RBFs and SBFs. We design a new implementation of generator and discriminator for effective training. The trained GAN is finally applied to new users for generating synthetic behavior features. We conduct extensive experiments on two Yelp datasets. Experimental results demonstrate that our proposed framework significantly outperforms the state-of-the-art methods.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.petrol.2020.107087,Journal,Journal of Petroleum Science and Engineering,scopus,2020-07-01,sciencedirect,Transformation of academic teaching and research: Development of a highly automated experimental sucker rod pumping unit,https://api.elsevier.com/content/abstract/scopus_id/85079611752,"Sucker rod pumps are one of the most popular solutions for artificial lift since their inception in the 19th century with minimum changes in design. Presently, companies are deploying digital technology in the field and, there has been a big push for a networked oilfield in recent years. This means technology is now able to control machines in remote places, evaluate their performances and control safety operating parameters. But these digital solutions are still not available in universities, causing a technological and technical gap for students and researchers.
                  This study presents a prototype of a new dedicated Interactive Digital Sucker Rod Pumping Unit (ID-SRP) system at the University of Oklahoma with representative operating conditions. The prototype mimics sucker rod pump working principles and also imitates different realistic rod string motions. The application and solutions are focused on providing authentic learning experiences for petroleum engineers. The system is also designed to address and optimize SRP well performance and safety through Model Predictive Controller (MPC) implementation and meeting industrial requirements. It connects the physical and virtual interaction with learning technologies. The objective is to bridge the tangible and the abstract for a better understanding of sucker rod concept and implement existing theories into the digital system. Additionally, it aids our future petroleum engineers on how to apply basic industry principles and upsurge their problem-solving skills.
                  The developed unit is capable of simulating any situations in real time and using Internet of Things (IoT) for data acquisition to create tailored diagnostic tools that students and laboratory staff can utilize. The software selected for the system is LabVIEW, which controls all the necessary equipment. This system can build personalized dynocard graphs, intake live data and export them to other programs live Excel, MATLAB, Python or any other programming languages.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2020.113251,Journal,Expert Systems with Applications,scopus,2020-07-01,sciencedirect,Integrating complex event processing and machine learning: An intelligent architecture for detecting IoT security attacks,https://api.elsevier.com/content/abstract/scopus_id/85079340111,"The Internet of Things (IoT) is growing globally at a fast pace: people now find themselves surrounded by a variety of IoT devices such as smartphones and wearables in their everyday lives. Additionally, smart environments, such as smart healthcare systems, smart industries and smart cities, benefit from sensors and actuators interconnected through the IoT. However, the increase in IoT devices has brought with it the challenge of promptly detecting and combating the cybersecurity attacks and threats that target them, including malware, privacy breaches and denial of service attacks, among others. To tackle this challenge, this paper proposes an intelligent architecture that integrates Complex Event Processing (CEP) technology and the Machine Learning (ML) paradigm in order to detect different types of IoT security attacks in real time. In particular, such an architecture is capable of easily managing event patterns whose conditions depend on values obtained by ML algorithms. Additionally, a model-driven graphical tool for security attack pattern definition and automatic code generation is provided, hiding all the complexity derived from implementation details from domain experts. The proposed architecture has been applied in the case of a healthcare IoT network to validate its ability to detect attacks made by malicious devices. The results obtained demonstrate that this architecture satisfactorily fulfils its objectives.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2020.01.083,Journal,Neurocomputing,scopus,2020-06-07,sciencedirect,Integrating adaptive moving window and just-in-time learning paradigms for soft-sensor design,https://api.elsevier.com/content/abstract/scopus_id/85079267624,"Most applications of soft sensors in process industries require learning from a stream of data, which may exhibit nonstationary dynamics, or concept drift. In this study, we develop a relevance vector machine (RVM) based novel adaptive learning algorithm called MWAdp-JITL, to meet the demands of continuous processes. The resulting algorithm combines active and passive learning: A moving window (MW) algorithm, which adapts the window size against virtual/real concept drifts, is coupled with a just-in-time learning (JITL) model, constructed using an appropriate region of historical data, and the ensemble weights of the MW and JITL models are adjusted for each query point. Tests on four real industrial datasets and a synthetic data, comprising various concept drift scenarios, show that MWAdp-JITL yields superior prediction accuracy and is generally more robust to changes in algorithm parameters compared to conventional adaptive learning methods and state-of-the-art algorithms from the literature. MWAdp-JITL complies with time limits of online prediction, and is applicable for high dimensional processes under various types of concept drifts. It is seen that MWAdp-JITL can successfully achieve a good balance in bias-variance tradeoff, justifying the use of only two exquisitely selected learners in ensemble learning.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cola.2020.100970,Journal,Journal of Computer Languages,scopus,2020-06-01,sciencedirect,"Visual Programming Environments for End-User Development of intelligent and social robots, a systematic review",https://api.elsevier.com/content/abstract/scopus_id/85085272330,"Robots are becoming interactive and robust enough to be adopted outside laboratories and in industrial scenarios as well as interacting with humans in social activities. However, the design of engaging robot-based applications requires the availability of usable, flexible and accessible development frameworks, which can be adopted and mastered by researchers and practitioners in social sciences and adult end users as a whole. This paper surveys Visual Programming Environments aimed at enabling a paradigm fostering the so-called End-User Development of applications involving robots with social capabilities. The focus of this article is on those Visual Programming Environments that are designed to support social research goals as well as to cater for professional needs of people not trained in more traditional text-based computer programming languages. This survey excludes interfaces aimed at supporting expert programmers, at allowing industrial robots to perform typical industrial tasks (such as pick and place operations), and at teaching children how to code. After having performed a systematic search, sixteen programming environments have been included in this survey. Our goal is two-fold: first, to present these software tools with their technical features and Authoring Artificial Intelligence modeling approaches, and second, to present open challenges in the development of Visual Programming Environments for end users and social researchers, which can be informative and valuable to the community. The results show that the most recent such tools are adopting distributed and Component-Based Software Engineering approaches and web technologies. However, few of them have been designed to enable the independence of end users from high-tech scribes. Moreover, findings indicate the need for (i) more objective and comparative evaluations, as well as usability and user experience studies with real end users; and (ii) validations of these tools for designing applications aimed at working “in-the-wild” rather than only in laboratories and structured settings.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2020.106208,Journal,Applied Soft Computing Journal,scopus,2020-06-01,sciencedirect,Dynamic scheduling for flexible job shop with new job insertions by deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85081140568,"In modern manufacturing industry, dynamic scheduling methods are urgently needed with the sharp increase of uncertainty and complexity in production process. To this end, this paper addresses the dynamic flexible job shop scheduling problem (DFJSP) under new job insertions aiming at minimizing the total tardiness. Without lose of generality, the DFJSP can be modeled as a Markov decision process (MDP) where an intelligent agent should successively determine which operation to process next and which machine to assign it on according to the production status of current decision point, making it particularly feasible to be solved by reinforcement learning (RL) methods. In order to cope with continuous production states and learn the most suitable action (i.e. dispatching rule) at each rescheduling point, a deep Q-network (DQN) is developed to address this problem. Six composite dispatching rules are proposed to simultaneously select an operation and assign it on a feasible machine every time an operation is completed or a new job arrives. Seven generic state features are extracted to represent the production status at a rescheduling point. By taking the continuous state features as input to the DQN, the state–action value (Q-value) of each dispatching rule can be obtained. The proposed DQN is trained using deep Q-learning (DQL) enhanced by two improvements namely double DQN and soft target weight update. Moreover, a “softmax” action selection policy is utilized in real implementation of the trained DQN so as to promote the rules with higher Q-values while maintaining the policy entropy. Numerical experiments are conducted on a large number of instances with different production configurations. The results have confirmed both the superiority and generality of DQN compared to each composite rule, other well-known dispatching rules as well as the stand Q-learning-based agent.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rcim.2019.101887,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2020-06-01,sciencedirect,Deep learning-based smart task assistance in wearable augmented reality,https://api.elsevier.com/content/abstract/scopus_id/85074770255,"Wearable augmented reality (AR) smart glasses have been utilized in various applications such as training, maintenance, and collaboration. However, most previous research on wearable AR technology did not effectively supported situation-aware task assistance because of AR marker-based static visualization and registration. In this study, a smart and user-centric task assistance method is proposed, which combines deep learning-based object detection and instance segmentation with wearable AR technology to provide more effective visual guidance with less cognitive load. In particular, instance segmentation using the Mask R-CNN and markerless AR are combined to overlay the 3D spatial mapping of an actual object onto its surrounding real environment. In addition, 3D spatial information with instance segmentation is used to provide 3D task guidance and navigation, which helps the user to more easily identify and understand physical objects while moving around in the physical environment. Furthermore, 2.5D or 3D replicas support the 3D annotation and collaboration between different workers without predefined 3D models. Therefore, the user can perform more realistic manufacturing tasks in dynamic environments. To verify the usability and usefulness of the proposed method, we performed quantitative and qualitative analyses by conducting two user studies: 1) matching a virtual object to a real object in a real environment, and 2) performing a realistic task, that is, the maintenance and inspection of a 3D printer. We also implemented several viable applications supporting task assistance using the proposed deep learning-based task assistance in wearable AR.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.comcom.2020.04.053,Journal,Computer Communications,scopus,2020-05-15,sciencedirect,IOT and cloud computing based parallel implementation of optimized RBF neural network for loader automatic shift control,https://api.elsevier.com/content/abstract/scopus_id/85089243295,"One of the key issues in automatic shift control of V-type cyclical loaders is determining how to find the best gear for the current conditions according to a certain mapping relation, but this complex and nonlinear mapping is difficult to express by a mathematical relation. However, to solve such nonlinear problems, a radial basis function (RBF) neural network is the best choice. In this paper, a certain type of wheel loader is taken as the research object, and an RBF neural network algorithm based on an improved genetic algorithm (GA) optimization is proposed. The global search ability of the GA is improved by adaptively adjusting the crossover probability and mutation probability. The RBF neural network expansion coefficient is optimized by an improved GA. Using industrial IOT technology, an optimized RBF neural network based on Map-Reduce on a cloud computing cluster is designed. The diesel engine computer and transmission computer on the loader are integrated to achieve dual-processor distributed parallel data processing and calculation. Then the loader automatic variable speed control algorithm model of improved GA optimized RBF neural network based on IOT cloud computing is established. The network model is trained and simulated using real vehicle automatic shift test data. The simulation results show that the improved GA-RBF neural network algorithm can achieve a correct recognition rate of 97.92%. The error matrix norm reaches the minimum value when the algorithm is iterated to the 17th generation. The improved algorithm has the advantages of a high gear recognition rate, fast convergence speed and strong real-time shift performance and is an effective new shift control method. The test results show that the shift boost time is less than 0.15 s and has a certain gradient. Compared with the manual shift process performed in the past, some improvements are achieved in the optimal shift time, shift response speed and shift quality. Compared with the traditional single computer based on serial training RBF neural network learning algorithm, whether it is Great progress has been made in convergence speed, training time, recognition rate, and data processing capabilities. Through the simulation and test, the validity of the intelligent shift control method of the improved GA optimized RBF neural network based on IOT cloud computing is verified. It has better engineering application value.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.microrel.2020.113640,Journal,Microelectronics Reliability,scopus,2020-05-01,sciencedirect,Two phase cooling with nano-fluid for highly dense electronic systems-on-chip – A pilot study,https://api.elsevier.com/content/abstract/scopus_id/85083093178,"In recent days, electronics gadgets need to design for higher functionalities with dense populated systems in order to meet the demands like lower in size, weight and power consumption. Even industrial electronic component and system design also prefer same slim fashion. On other hand the overheating of electronic components reduces its performance, life and by the way the reliability of such electronic product/system is greatly affected due to overheating. The conventional cooling methods failed to offer best performances. Hence this part of research proposed a effective two phase cooling technique with nano-fluid. The objective of this research is to maintain the maximum temperature at the junction and hot spots in order to break a new ground in the cooling of electronic systems. The Maximum permissible operating temperature for any commercial electronic applications is only upto 70 °C (equal to 343.15 K) and above which most of the inherent electronic circuits may malfunction and destroy the entire application.The HotSpot Simulator-6.0 software employed for establish, verify the simulated model and trial runs to answer many ‘what if’ questions. In the simulation, hottest spot has been found in Int_Reg region, where the steady temperature grows beyond the threshold temperature level. The temperature has to be decreased in order to provide reliable working environment. Hence, HFO 1234ze nano-fluid employed with flow rate of 1100 ml per minute. The nanofliuid minimizes the temperature of the simulated electronic circuit from 351.80 K to 326.86 K in UUT. The proposed two phase nano-fluid cooling system for 3-D Unit-Under Test (UUT) was verified and validated with real system and simulated for experiments. Thus, a high range of temperature difference from the initial and final steady state temperature has been evidently shown in the proposed two phase nano fluid cooling method. The system found outperforms as best of both the worlds. The nanofluid cooling system can be used in thermal-aware systems and highly dense systems to maintain the temperature not much than 343.15 K, even at full load conditions.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2020.103589,Journal,Engineering Applications of Artificial Intelligence,scopus,2020-05-01,sciencedirect,Deep reinforcement one-shot learning for artificially intelligent classification in expert aided systems,https://api.elsevier.com/content/abstract/scopus_id/85081990367,"In recent years there has been a sharp rise in applications, in which significant events need to be classified but only a few training instances are available. These are known as cases of one-shot learning. To handle this challenging task, organizations often use human analysts to classify events under high uncertainty. Existing algorithms use a threshold-based mechanism to decide whether to classify an object automatically or send it to an analyst for deeper inspection. However, this approach leads to a significant waste of resources since it does not take the practical temporal constraints of system resources into account. By contrast, the focus in this paper is on rigorously optimizing the resource consumption in the system which applies to broad application domains, and is of a significant interest for academic research, industrial developments, as well as society and citizens benefit. The contribution of this paper is threefold. First, a novel Deep Reinforcement One-shot Learning (DeROL) framework is developed to address this challenge. The basic idea of the DeROL algorithm is to train a deep-Q network to obtain a policy which is oblivious to the unseen classes in the testing data. Then, in real-time, DeROL maps the current state of the one-shot learning process to operational actions based on the trained deep-Q network, to maximize the objective function. Second, the first open-source software for practical artificially intelligent one-shot classification systems with limited resources is developed for the benefit of researchers and developers in related fields. Third, an extensive experimental study is presented using the OMNIGLOT dataset for computer vision tasks, the UNSW-NB15 dataset for intrusion detection tasks, and the Cleveland Heart Disease Dataset for medical monitoring tasks that demonstrates the versatility and efficiency of the DeROL framework.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijmultiphaseflow.2019.103194,Journal,International Journal of Multiphase Flow,scopus,2020-05-01,sciencedirect,Bubble patterns recognition using neural networks: Application to the analysis of a two-phase bubbly jet,https://api.elsevier.com/content/abstract/scopus_id/85079560188,"Gas-liquid two-phase bubbly flows are found in different areas of science and technology such as nuclear energy, chemical industry, or piping systems. Optical diagnostics of two-phase bubbly flows with modern panoramic techniques makes it possible to capture simultaneously instantaneous characteristics of both continuous and dispersed phases with a high spatial resolution. In this paper, we introduce a novel approach based on neural networks to recognize bubble patterns in images and identify their geometric parameters. The originality of the proposed method consists in training of a neural network ensemble using synthetic images that resemble real photographs gathered in experiment. The use of neural networks in combination with automatically generated data allowed us to detect overlapping, blurred, and non-spherical bubbles in a broad range of volume gas fractions. Experiments on a turbulent bubbly jet proved that the implemented method increases the identification accuracy, reducing errors of various kinds, and lowers the processing time compared to conventional recognition methods. Furthermore, utilizing the new method of bubbles recognition, the primary physical parameters of a dispersed phase, such as bubble size distribution and local gas content, were calculated in a near-to-nozzle region of the bubbly jet. The obtained results and integral experimental parameters, especially volume gas fraction, are in good agreement with each other.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.marpol.2020.103829,Journal,Marine Policy,scopus,2020-05-01,sciencedirect,Analyzing gaps in policy: Evaluation of the effectiveness of minimum landing size (MLS) regulations in Turkey,https://api.elsevier.com/content/abstract/scopus_id/85079518573,"The Mediterranean and Black Sea host the most intense overfishing and Turkey has the largest commercial fisheries in them (when both seas considered). However, the state of the Turkish fisheries is in critical condition as both the quality (i.e, in number of caught species, value and sizes of fish) and quantity of fisheries catches have been rapidly declining in recent decades. One pioneer fisheries management initiative thoroughly evaluated here pertains to minimum landing size (MLS) regulations for commercial taxa, with the aim of promoting stock sustainability by ensuring fish reproduce before they are caught. This study examines 29 taxa in relation to MLS by analyzing changes in catch per unit effort trends pre-and post MLS to gauge regulation effectiveness, changes to MLS regulations since implementation, and finally evaluates the Turkish MLS sizes in relation to Turkish maturity sizes, to provide advice for taxa requiring changes. It seems intensive fishing may have reduced the size at maturity for many species in Turkey, as they mature smaller here than the Mediterranean and global averages. Eleven taxa listed in MLS regulations are under the lengths of first maturity (Lmat) sizes in Turkish waters and need to be increased, especially that of bonito, hake, swordfish and bluefish (by 18 cm, 10 cm, 10 cm and 8 cm, respectively), while 16 taxa still require national studies to determine their Lmat sizes in Turkish waters. In conclusion, in Turkey, MLS regulations are completely ineffective due to a lack of monitoring and control for juvenile fish at landing sites, markets and processing plants, along with insufficient penalties for such infractions, yet, there remains plenty of room for improvement. To improve the state of the fisheries, MLS measures could be improved by increasing fines, monitoring and control, making some gear types more selective and use of real-time closures and no fishing zones to protect spawning and nursery habitats.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ress.2020.106821,Journal,Reliability Engineering and System Safety,scopus,2020-05-01,sciencedirect,Towards Efficient Robust Optimization using Data based Optimal Segmentation of Uncertain Space,https://api.elsevier.com/content/abstract/scopus_id/85078707908,"Performing multi-objective optimization under uncertainty is a common requirement in industries and academia. Robust optimization (RO) is considered as an efficient and tractable approach provided one has access to behavioral data for the uncertain parameters. However, solutions of RO may be far from the real solution and less reliable due to inability to map the uncertain space accurately, especially when the data appears discontinuous and scattered in the uncertain domain. Amalgamating machine learning algorithms with RO, this paper proposes a data-driven methodology, where a novel fuzzy clustering mechanism is implemented along-with boundary construction, to transcript the uncertain space such that the specific regions of uncertainty are identified. Subsequently, using intelligent Sobol sampling, samples are generated in the mapped uncertain regions. Results of two test cases are presented along with a comprehensive comparison study. Considered case-studies include highly nonlinear model for continuous casting process from steelmaking industries, where a multi-objective optimization problem under uncertainty is solved to balance the conflict between productivity and energy consumption. The Pareto-optimal solutions of the resulting RO problem are obtained through Non-Dominated Sorting Genetic Algorithm – II, and ~23–29% improvement is observed in the uncertain objective function. Further, the spread and diversity metrics are enhanced by ~10–95% as compared to those obtained using other standard uncertainty sets.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jss.2020.110519,Journal,Journal of Systems and Software,scopus,2020-05-01,sciencedirect,Traceability Link Recovery between Requirements and Models using an Evolutionary Algorithm Guided by a Learning to Rank Algorithm: Train control and management case,https://api.elsevier.com/content/abstract/scopus_id/85078162306,"Traceability Link Recovery (TLR) has been a topic of interest for many years within the software engineering community. In recent years, TLR has been attracting more attention, becoming the subject of both fundamental and applied research. However, there still exists a large gap between the actual needs of industry on one hand and the solutions published through academic research on the other.
                  In this work, we propose a novel approach, named Evolutionary Learning to Rank for Traceability Link Recovery (TLR-ELtoR). TLR-ELtoR recovers traceability links between a requirement and a model through the combination of evolutionary computation and machine learning techniques, generating as a result a ranking of model fragments that can realize the requirement.
                  TLR-ELtoR was evaluated in a real-world case study in the railway domain, comparing its outcomes with five TLR approaches (Information Retrieval, Linguistic Rule-based, Feedforward Neural Network, Recurrent Neural Network, and Learning to Rank). The results show that TLR-ELtoR achieved the best results for most performance indicators, providing a mean precision value of 59.91%, a recall value of 78.95%, a combined F-measure of 62.50%, and a MCC value of 0.64. The statistical analysis of the results assesses the magnitude of the improvement, and the discussion presents why TLR-ELtoR achieves better results than the baselines.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ejor.2019.10.015,Journal,European Journal of Operational Research,scopus,2020-05-01,sciencedirect,From one-class to two-class classification by incorporating expert knowledge: Novelty detection in human behaviour,https://api.elsevier.com/content/abstract/scopus_id/85074814083,"One-class classification is the standard procedure for novelty detection. Novelty detection aims to identify observations that deviate from a determined normal behaviour. Only instances of one class are known, whereas so called novelties are unlabelled. Traditional novelty detection applies methods from the field of outlier detection. These standard one-class classification approaches have limited performance in many real business cases. The traditional techniques are mainly developed for industrial problems such as machine condition monitoring. When applying these to human behaviour, the performance drops significantly. This paper proposes a method that improves existing approaches by creating semi-synthetic novelties in order to have labelled data for the two classes. Expert knowledge is incorporated in the initial phase of this data generation process. The method was deployed on a real-life test case where the goal was to detect fraudulent subscriptions to a telecom family plan. This research demonstrates that the two-class expert model outperforms a one-class model on the semi-synthetic dataset. In a next step the model was validated on a real dataset. A fraud detection team of the company manually checked the top predicted novelties. The results show that incorporating expert knowledge to transform a one-class problem into a two-class problem is a valuable method.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.dss.2020.113266,Journal,Decision Support Systems,scopus,2020-04-01,sciencedirect,ForeSim-BI: A predictive analytics decision support tool for capacity planning,https://api.elsevier.com/content/abstract/scopus_id/85079423691,"This paper proposes a decision support tool for maintenance capacity planning of complex product systems. The tool – ForeSim-BI – addresses the problem faced by maintenance organizations in forecasting the workload of future maintenance interventions and in planning an adequate capacity to face that expected workload. Developed and implemented from a predictive analytics perspective in the particular context of a Portuguese aircraft maintenance organization, the tool integrates four main modules: (1) a forecasting module used to predict future and unprecedented maintenance workloads from historical data; (2) a Bayesian inference module used to transform prior workload forecasts, resulting from the forecasting module, into predictive forecasts after observations on the maintenance interventions being predicted become available; (3) a simulation module used to characterize the forecasted total workloads through sets of random variables, including maintenance work types, maintenance work phases, and maintenance work skills; and (4) a Bayesian network module used to combine the simulated workloads with historical data through probabilistic inference. A linear programming model is also developed to improve the efficiency of the decision-making process supported by Bayesian networks. The tool uses real industrial data, comprising 171 aircraft maintenance projects collected at the host organization, and is validated by comparing its results with real observations of a given maintenance intervention to which predictions were made and with a model simulating current forecasting practices employed in industry. Significantly more accurate forecasts have been obtained with the proposed tool, resulting in an important cost saving potential for maintenance organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aei.2020.101052,Journal,Advanced Engineering Informatics,scopus,2020-04-01,sciencedirect,Deep learning-based method for vision-guided robotic grasping of unknown objects,https://api.elsevier.com/content/abstract/scopus_id/85079340469,"Nowadays, robots are heavily used in factories for different tasks, most of them including grasping and manipulation of generic objects in unstructured scenarios. In order to better mimic a human operator involved in a grasping action, where he/she needs to identify the object and detect an optimal grasp by means of visual information, a widely adopted sensing solution is Artificial Vision. Nonetheless, state-of-art applications need long training and fine-tuning for manually build the object’s model that is used at run-time during the normal operations, which reduce the overall operational throughput of the robotic system. To overcome such limits, the paper presents a framework based on Deep Convolutional Neural Networks (DCNN) to predict both single and multiple grasp poses for multiple objects all at once, using a single RGB image as input. Thanks to a novel loss function, our framework is trained in an end-to-end fashion and matches state-of-art accuracy with a substantially smaller architecture, which gives unprecedented real-time performances during experimental tests, and makes the application reliable for working on real robots. The system has been implemented using the ROS framework and tested on a Baxter collaborative robot.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.autcon.2019.103062,Journal,Automation in Construction,scopus,2020-04-01,sciencedirect,Towards automated clash resolution of reinforcing steel design in reinforced concrete frames via Q-learning and building information modeling,https://api.elsevier.com/content/abstract/scopus_id/85078657649,"The design of reinforcing steel bars (rebars) is critical to reinforced concrete (RC) structures. Generally, a good number of rebars are required by a design code, particularly at member connections. As such, rebar clashes (i.e., collisions and congestions) would be inevitable. It would be impractical, labor-intensive, and error-prone to avoid all possible clashes manually or even using standard design software. The building information modeling (BIM) technology has been utilized by the present architecture, engineering, and construction (ACE) industry for clash-free rebar designs. However, most existing BIM-based approaches offer the clash resolution strategy for moving components with an optimization algorithm, and are only applicable to the RC structures with regular shapes. In particular, the optimized path of rebars cannot be adjusted to avoid the obstacles, thus limiting the practical applications. Furthermore, most existing studies lack the learning from design code and constructibility constraints to realize automatic and intelligent arrangement and adjustment of rebars for avoiding the obstacles encountered in complex RC joints and frame structures. Considering these shortcomings, the authors have recently proposed an immediate reward-based multi-agent reinforcement learning (MARL) system with BIM, towards automatic clash-free rebar designs of RC joints without clashes. However, as the immediate reward is required in the MARL system for guiding the learning of a rebar design, it will not succeed in clash-free rebar designs of complex RC structures where immediate reward is often unavailable. In this study, this study further extends the previous work with Q-learning (a model-free reinforcement learning algorithm) for more realistic path planning considering both immediate and delayed rewards in clash-free rebar designs for real-world RC structures. In particular, the rebar design problem is treated as a path-planning problem of multi-agent system, where each rebar is deemed as an intelligence reinforcement learning agent. Next, by employing the Q-learning as the reinforcement learning engine, the particular form of state, action, and immediate and delayed rewards for the reinforcement MARL for automatic rebar designs considering more actual constructible constraints and design codes can be developed. Comprehensive experiments on three typical beam-column joints and a two-story RC building frame were conducted to evaluate the efficiency of the proposed method. The study results of paths of rebar designs, success rates, and average time confirm that the proposed framework with MARL and BIM is effective and efficient.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.talanta.2019.120664,Journal,Talanta,scopus,2020-04-01,sciencedirect,Modelling of bioprocess non-linear fluorescence data for at-line prediction of etanercept based on artificial neural networks optimized by response surface methodology,https://api.elsevier.com/content/abstract/scopus_id/85076829838,"In the last years, regulatory agencies in biopharmaceutical industry have promoted the design and implementation of Process Analytical Technology (PAT), which aims to develop rapid and high-throughput strategies for real-time monitoring of bioprocesses key variables, in order to improve their quality control lines. In this context, spectroscopic techniques for data generation in combination with chemometrics represent alternative analytical methods for on-line critical process variables prediction. In this work, a novel multivariate calibration strategy for the at-line prediction of etanercept, a recombinant protein produced in a mammalian cells-based perfusion process, is presented. For data generation, samples from etanercept processes were daily obtained, from which fluorescence excitation-emission matrices were generated in the spectral ranges of 225.0 and 495.0 nm and 250.0 and 599.5 nm for excitation and emission modes, respectively. These data were correlated with etanercept concentration in supernatant (measured by an off-line HPLC-based reference univariate technique) by implementing different chemometric strategies, in order to build predictive models. Partial least squares (PLS) regression evidenced a non-linear relation between signal and concentration when observing actual vs. predicted concentrations. Hence, a non-parametric approach was implemented, based on a multilayer perceptron artificial neural network (MLP). The MLP topology was optimized by means of the response surface methodology. The prediction performance of MLP model was superior to PLS, since the first is able to cope with non-linearity in calibration models, reaching percentage mean relative error in predictions of about 7.0% (against 12.6% for PLS). This strategy represents a fast and inexpensive approach for etanercept monitoring, which conforms the principles of PAT.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compag.2020.105284,Journal,Computers and Electronics in Agriculture,scopus,2020-03-01,sciencedirect,An experimental study of stunned state detection for broiler chickens using an improved convolution neural network algorithm,https://api.elsevier.com/content/abstract/scopus_id/85079902403,"Effective recognition method of broiler stunned state has always been an important issue in real industries. In recent years, recognition methods such as neural networks have been receiving increasing attention due to their great merits of high diagnostic accuracy and easy implementation. To improve the accuracy and efficiency of broiler stunned state recognition, an improved fast region-based convolutional neural network (You Only Look Once + Multilayer Residual Module (YOLO + MRM)) algorithm was proposed and applied to the recognition of three broiler stunned states: insufficient, appropriate and excessive stuns. The images were collected from a broiler-slaughtering line using a complementary metal-oxide semiconductor (CMOS) camera. The area of the head and wings of a broiler in the original image was marked according to the PASCAL VOC data format and the dataset of each broiler stunned state was obtained. The results showed that the YOLO + MRM algorithm achieved good performance with an accuracy of 96.77%. To compare YOLO + MRM with other models, similar experiments were conducted using a conventional back propagation neural network (BP-NN) classifier, as well as YOLO, and the recognition accuracies were 90.11% and 94.74%, respectively. YOLO + MRM can complete the detection task of more than 180,000 broilers per hour. Compared with the traditional method, little prior expertise on image recognition is required, the recognition accuracy and speed are improved obviously. This study has provided a foundation and highlighted the potential for automatically detecting the stunned state of broiler chickens, which is crucial for the success of an automatic electric stunning process in the poultry industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.adhoc.2019.102047,Journal,Ad Hoc Networks,scopus,2020-03-01,sciencedirect,An intelligent Edge-IoT platform for monitoring livestock and crops in a dairy farming scenario,https://api.elsevier.com/content/abstract/scopus_id/85076174369,"Today’s globalized and highly competitive world market has broadened the spectrum of requirements in all the sectors of the agri-food industry. This paper focuses on the dairy industry, on its need to adapt to the current market by becoming more resource efficient, environment-friendly, transparent and secure. The Internet of Things (IoT), Edge Computing (EC) and Distributed Ledger Technologies (DLT) are all crucial to the achievement of those improvements because they allow to digitize all parts of the value chain, providing detailed information to the consumer on the final product and ensuring its safety and quality. In Smart Farming environments, IoT and DLT enable resource monitoring and traceability in the value chain, allowing producers to optimize processes, provide the origin of the produce and guarantee its quality to consumers. In comparison to a centralized cloud, EC manages the Big Data generated by IoT devices by processing them at the network edge, allowing for the implementation of services with shorter response times, and a higher Quality of Service (QoS) and security. This work presents a platform oriented to the application of IoT, Edge Computing, Artificial Intelligence and Blockchain techniques in Smart Farming environments, by means of the novel Global Edge Computing Architecture, and designed to monitor the state of dairy cattle and feed grain in real time, as well as ensure the traceability and sustainability of the different processes involved in production. The platform is deployed and tested in a real scenario on a dairy farm, demonstrating that the implementation of EC contributes to a reduction in data traffic and an improvement in the reliability in communications between the IoT-Edge layers and the Cloud.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.future.2019.10.043,Journal,Future Generation Computer Systems,scopus,2020-03-01,sciencedirect,HealthFog: An ensemble deep learning based Smart Healthcare System for Automatic Diagnosis of Heart Diseases in integrated IoT and fog computing environments,https://api.elsevier.com/content/abstract/scopus_id/85074613864,"Cloud computing provides resources over the Internet and allows a plethora of applications to be deployed to provide services for different industries. The major bottleneck being faced currently in these cloud frameworks is their limited scalability and hence inability to cater to the requirements of centralized Internet of Things (IoT) based compute environments. The main reason for this is that latency-sensitive applications like health monitoring and surveillance systems now require computation over large amounts of data (Big Data) transferred to centralized database and from database to cloud data centers which leads to drop in performance of such systems. The new paradigms of fog and edge computing provide innovative solutions by bringing resources closer to the user and provide low latency and energy efficient solutions for data processing compared to cloud domains. Still, the current fog models have many limitations and focus from a limited perspective on either accuracy of results or reduced response time but not both. We proposed a novel framework called HealthFog for integrating ensemble deep learning in Edge computing devices and deployed it for a real-life application of automatic Heart Disease analysis. HealthFog delivers healthcare as a fog service using IoT devices and efficiently manages the data of heart patients, which comes as user requests. Fog-enabled cloud framework, FogBus is used to deploy and test the performance of the proposed model in terms of power consumption, network bandwidth, latency, jitter, accuracy and execution time. HealthFog is configurable to various operation modes which provide the best Quality of Service or prediction accuracy, as required, in diverse fog computation scenarios and for different user requirements.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compind.2019.103164,Journal,Computers in Industry,scopus,2020-02-01,sciencedirect,Integrating artificial intelligent techniques and continuous time simulation modelling. Practical predictive analytics for energy efficiency and failure detection,https://api.elsevier.com/content/abstract/scopus_id/85099790267,"Energy efficiency and reliability needs are growing in many economic sectors, where predictive analytics are becoming essential tools for these key variables forecasting.
                  When predicting these variables, in many occasions, the problem to simplify the prediction model format when dealing with similar systems, which are placed in different functional locations, is a very complex problem due to model unavoidable dependency on changing operating conditions (per time and location). So effort is placed in this paper to develop tools that can easily adapt prediction models’ structure to existing operating conditions, for a given time period and place where the asset is located. Furthermore, these tools may allow the model to be easily trained and tested for automated implementation within the plant’s remote surveillance system.
                  To this end, Artificial Intelligence (AI) techniques, and in particular artificial neural network (ANN) models, have been selected in this paper as prediction models, since their structure can be adapted to improve predictions accuracy and they can also learn from dynamic changes in environmental conditions.
                  To demonstrate the adaptability for prediction accuracy and self-learning capabilities of the model, we have implemented an ANN with a backpropagation algorithm as a continuous time simulation model, which is then implemented using Vensim simulation environment, to benefit of the outstanding software optimization features for fast training.
                  Using this model we provide predictions of asset degradation and operational risk under existing real time internal and locational variables. We can also dynamically release preventive maintenance activities. This prediction model is exemplified in an industrial case for failures in cryogenic pumps of LNG tanks.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.comcom.2020.01.018,Journal,Computer Communications,scopus,2020-02-01,sciencedirect,Enhanced resource allocation in mobile edge computing using reinforcement learning based MOACO algorithm for IIOT,https://api.elsevier.com/content/abstract/scopus_id/85077781443,"The Mobile networks deploy and offers a multiaspective approach for various resource allocation paradigms and the service based options in the computing segments with its implication in the Industrial Internet of Things (IIOT) and the virtual reality. The Mobile edge computing (MEC) paradigm runs the virtual source with the edge communication between data terminals and the execution in the core network with a high pressure load. The demand to meet all the customer requirements is a better way for planning the execution with the support of cognitive agent. The user data with its behavioral approach is clubbed together to fulfill the service type for IIOT. The swarm intelligence based and reinforcement learning techniques provide a neural caching for the memory within the task execution, the prediction provides the caching strategy and cache business that delay the execution. The factors affecting this delay are predicted with mobile edge computing resources and to assess the performance in the neighboring user equipment. The effectiveness builds a cognitive agent model to assess the resource allocation and the communication network is established to enhance the quality of service. The Reinforcement Learning techniques Multi Objective Ant Colony Optimization (MOACO) algorithms has been applied to deal with the accurate resource allocation between the end users in the way of creating the cost mapping tables creations and optimal allocation in MEC.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2019.106225,Journal,Computers and Industrial Engineering,scopus,2020-02-01,sciencedirect,Fuzzy possibility regression integrated with fuzzy adaptive neural network for predicting and optimizing electrical discharge machining parameters,https://api.elsevier.com/content/abstract/scopus_id/85076689961,"An electrical discharge machining (EDM) is one of the special production methods that are widely used in moldings, repairs and production of specific industrial components. Due to extensive production costs, optimal machining specifications are significant. Machining specifications are effective on output quality and thus attract more customers leading to higher profits. In this study, the impact of EDM parameters on surface roughness, material removal rate and electrode corrosion percentage have been investigated. In order to consider uncertainty of real production environments, the fuzzy theory is employed. Also, using the design of experiment (DOE) parameters calibration is performed and mathematical programming approach is applied for optimization purpose. The relationship between the machining parameters and the output process specification is examined by a fuzzy possibility regression model. Then, the mathematical relation of exact inputs and fuzzy outputs of the EDM process are extracted. The effectiveness of the three outputs is evaluated by interfacing models and fuzzy hypothesis testing. To determine the optimal levels of each output, a fuzzy adaptive neural network is used and appropriate models are prepared to be adapted with a fitted model of fuzzy possibility regression for comparison purposes. Validation tests imply the effectiveness of the proposed method. The integrated model is implemented in real case study. The results show that, fitted models can predict the material removal rate, surface fineness, and corrosion percentage of the electrode. The prediction accuracy of the proposed method is shown in comparison with the optimal fuzzy adaptive neural network outputs considering error value. Also, the proposed method is successful in identifying the optimal process parameters for EDM with reliable accuracy. The proposed integrated prediction and optimization model can be used as a calibration decision support in production systems to handle dynamic data structures and provide real time machining specifications to increase the output quality.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.micpro.2019.102906,Journal,Microprocessors and Microsystems,scopus,2020-02-01,sciencedirect,Area and power efficient pipelined hybrid merged adders for customized deep learning framework for FPGA implementation,https://api.elsevier.com/content/abstract/scopus_id/85073599282,"With the rapid growth of deep learning and neural network algorithms, various fields such as communication, Industrial automation, computer vision system and medical applications have seen the drastic improvements in recent years. However, deep learning and neural network models are increasing day by day, while model parameters are used for representing the models. Although the existing models use efficient GPU for accommodating these models, their implementation in the dedicated embedded devices needs more optimization which remains a real challenge for researchers. Thus paper, carries an investigation of deep learning frameworks, more particularly as review of adders implemented in the deep learning framework. A new pipelined hybrid merged adders (PHMAC) optimized for FPGA architecture which has more efficient in terms of area and power is presented. The proposed adders represent the integration of the principle of carry select and carry look ahead principle of adders in which LUT is re-used for the different inputs which consume less power and provide effective area utilization. The proposed adders were investigated on different FPGA architectures in which the power and area were analyzed. Comparison of the proposed adders with the other adders such as carry select adders (CSA), carry look ahead adder (CLA), Carry skip adders and Koggle Stone adders has been made and results have proved to be highly vital into a 50% reduction in the area, power and 45% when compared with above mentioned traditional adders.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rcim.2019.101847,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2020-02-01,sciencedirect,Trajectory smoothing method using reinforcement learning for computer numerical control machine tools,https://api.elsevier.com/content/abstract/scopus_id/85070511935,"Tool-path codes output by computer-aided manufacturing software for high-speed machining are composed of discontinuous G01 line segments. The discontinuity of these tool movements causes computer numerical control (CNC) inefficiency. To achieve high-speed continuous motion, corner smoothing algorithms based on pre-planning methods are widely used. However, it is difficult to optimize smoothing trajectories in real-time systems. To obtain smooth trajectories efficiently, this paper proposes a neural network-based direct trajectory smoothing method. An intelligent neural network agent outputs servo commands directly based on the current tool path and running state in every cycle. To achieve direct control, motion feature and reward models were built, and reinforcement learning was used to train the neural network parameters without additional experimental data. The proposed method provides higher cutting efficiency than the local and global smoothing algorithms. Given its simple structure and low computational demands, it can easily be applied to real-time CNC systems.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ejor.2019.07.057,Journal,European Journal of Operational Research,scopus,2020-02-01,sciencedirect,Automating the planning of container loading for Atlas Copco: Coping with real-life stacking and stability constraints,https://api.elsevier.com/content/abstract/scopus_id/85070381903,"The Atlas Copco☆ distribution center in Allen, TX, supplies spare parts and consumables to mining and construction companies across the world. For some customers, packages are shipped in sea containers. Planning how to load the containers is difficult due to several factors: heterogeneity of the packages with respect to size, weight, stackability, positioning and orientation; the set of packages differs vastly between shipments; it is crucial to avoid cargo damage. Load plan quality is ultimately judged by shipping operators.
                  This container loading problem is thus rich with respect to practical considerations. These are posed by the operators and include cargo and container stability as well as stacking and positioning constraints. To avoid cargo damage, the stacking restrictions are modeled in detail. For solving the problem, we developed a two-level metaheuristic approach and implemented it in a decision support system. The upper level is a genetic algorithm which tunes the objective function for a lower level greedy-type constructive placement heuristic, to optimize the quality of the load plan obtained.
                  The decision support system shows load plans on the forklift laptops and has been used for over two years. Management has recognized benefits including reduction of labour usage, lead time, and cargo damage risk.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2019.09.082,Journal,Neurocomputing,scopus,2020-01-29,sciencedirect,A scalable and reconfigurable in-memory architecture for ternary deep spiking neural network with ReRAM based neurons,https://api.elsevier.com/content/abstract/scopus_id/85073152550,"Neuromorphic computing using post-CMOS technologies is gaining increasing popularity due to its promising potential to resolve the power constraints in Von-Neumann machine and its similarity to the operation of the real human brain. In this work, we propose a scalable and reconfigurable architecture that exploits the ReRAM-based neurons for deep Spiking Neural Networks (SNNs). In prior publications, neurons were implemented using dedicated analog or digital circuits that are not area and energy efficient. In our work, for the first time, we address the scaling and power bottlenecks of neuromorphic architecture by utilizing a single one-transistor-one-ReRAM (1T1R) cell to emulate the neuron. We show that the ReRAM-based neurons can be integrated within the synaptic crossbar to build extremely dense Process Element (PE)–spiking neural network in memory array–with high throughput. We provide microarchitecture and circuit designs to enable the deep spiking neural network computing in memory with an insignificant area overhead. Simulation results on MNIST and CIFAR-10 datasets with spiking Resnet (SResnet) and spiking Squeezenet (SSqueez) show that compared to the baseline CPU only solution, our proposed architecture achieves energy saving between 1222 ×  and 1853 ×  and speed improvement between 791 ×  to 1120 ×.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jclepro.2019.118788,Journal,Journal of Cleaner Production,scopus,2020-01-20,sciencedirect,Rapid evaluation of micro-scale photovoltaic solar energy systems using empirical methods combined with deep learning neural networks to support systems’ manufacturers,https://api.elsevier.com/content/abstract/scopus_id/85073926377,"Solar energy is becoming one of the most attractive renewable sources. In many cases, due to a wide range of financial or installation limitations, off-grid small scale micro power panels are favoured as modular systems to power lighting in gardens or to be integrated together to power small devices such as mobile phone chargers and distributed smart city facilities and services. Manufacturers and systems’ integrators have a wide range of options of micro-scale photo voltaic panels to choose from. This makes the selection of the right panel a challenging task and risky investment. To address this and to help manufacturers, this paper suggests and evaluates a novel approach based on integrating empirical lab-testing with short-term real data and neural networks to assess the performance of micro-scale photovoltaic panels and their suitability for a specific application in specific environment. The paper outlines the combination of lab testing power output under seasonal and hourly conditions during the year combined with environmental and operating conditions such as temperature, dust accumulation and tilt angle performance. Based on the lab results, a short in-situ experimental work is implemented and the performance over the year in the selected location in Kuwait is evaluated using deep learning neural networks. The findings of this approach are compared with simulation and long-term real data. The results show a maximum error of 23% of the neural network output when compared with the actual data, and a correlation values with previous work within 87.3% and 91.9% which indicate that the proposed approach could provide an experimental rapid and accurate assessment of the expected power output. Hence, supporting the rapid decision-making process for manufacturers and reducing investment risks.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.fuel.2019.116250,Journal,Fuel,scopus,2020-01-15,sciencedirect,"On the development of experimental methods to determine the rates of asphaltene precipitation, aggregation, and deposition",https://api.elsevier.com/content/abstract/scopus_id/85072862208,"Despite the efforts throughout the last few decades, asphaltene deposition remains as one of the greatest challenges in the petroleum industry. In this work, we present a comprehensive series of experimental studies to better understand the asphaltene precipitation, aggregation, and deposition mechanisms. Here, we introduce a simple method to determine the amount of precipitated asphaltene using NIR spectroscopy measurements without the implementation of calibration curves. Moreover, the kinetics of asphaltene precipitation and aggregation is simultaneously investigated by a newly developed, fast, and reliable NIR spectroscopy technique. In the new method, only less than 2 ml of sample is required for each experiment. In addition, unlike gravimetric techniques, less time consuming and labor-intensive measurements can be performed. In addition, the temperature can be controlled; hence, experiments can be conducted to evaluate the effects of temperature and the driving force on the kinetics of asphaltene precipitation and aggregation. Subsequently, the quantified precipitated asphaltene amount can be used to calibrate the precipitation and aggregation kinetic parameters of the asphaltene deposition model. The results obtained from the kinetics experiments facilitate in establishing a function to scale the precipitation kinetic parameter from laboratory-scale experiments to real field high-pressure high-temperature conditions. Additionally, a multi-section stainless steel packed bed column is proposed to study asphaltene deposition at high temperature and under dynamic conditions. In these experiments, the amount of deposited asphaltene is directly quantified. The results from the packed bed column deposition tests can be used to calibrate the deposition kinetic parameter of the asphaltene deposition model.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2019.09.004,Journal,Neurocomputing,scopus,2020-01-02,sciencedirect,Digital neuromorphic real-time platform,https://api.elsevier.com/content/abstract/scopus_id/85072526243,"Hardware implementations of spiking neural networks in portable devices can improve many applications of robotics, neurorobotics or prosthetic fields in terms of power consumption, high-speed processing and learning mechanisms. Analog and digital platforms have been previously proposed to run these networks. Analog designs are closer to biology since they implement the original mathematical model. However, digital platforms are, to some extent, abstractions of this model so far. In this paper, a full digital platform to design, implement and run real-time analog-like spiking neural networks is presented. Specifically, we present the design and implementation of digital circuits to run real-time biologically plausible spiking neural networks on a Field Programmable Gate Array (FPGA). The circuit designed for the neuron implements the Leaky Integrate and Fire (LIF) model. The synapsis implemented is a bi-exponential current-based one. The synaptic circuit design consists of one static memory with the baseline current and a dynamic memory which stores the updated contribution over time of each pre-synaptic connection. All the parameters of both the neuron and the synapse are configurable. The results of the circuits are validated by running the same experiments on the Brian simulator. The circuits, which are totally original and independent of the technology, use only 136 slice registers of hardware resources. Thus, these designs allow the scale of the network. These circuits aim to be the basis of the spiking neural networks on digital devices. This platform allows the user to first simulate their network within the Brian simulator and then, confidently, move to the hardware platform replicating the same performance or even replace their analog platform with the digital one.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-818576-6.00004-6,Book,Artificial Intelligence to Solve Pervasive Internet of Things Issues,scopus,2020-01-01,sciencedirect,"AI and IoT Capabilities: Standards, Procedures, Applications, and Protocols",https://api.elsevier.com/content/abstract/scopus_id/85125968641,"With the growing technology of hardware components like sensors, actuators, networking devices, and networks media with efficient software for data gathering and analysis, the Internet of Things (IoT) became more effective in real-world applications like healthcare, item tracking in supply chain, military, and prediction of seismic activity in volcanoes. Artificial intelligence (AI) is a branch of engineering based on mathematical techniques, which has potential to enhance many real-world application domains like healthcare, industrial automation, service sector, and so on. Machine learning (ML) is a form of thin AI, and is used to help or automate decision-making. The process of automated decision-making and making predictions is based on gathered data. This data gathering may be performed through IoT. The AI and IoT are clearly intersecting each other as AI simulates intelligent behavior through different kind of machines whereas IoT interconnects these machines and help in collecting data. This chapter will discuss state-of-the-art methods, standards, protocols, and applications of IoT and making smart IoT with AI. The chapter also discusses AI approaches to make Intelligent IoT.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-820027-8.00006-X,Book,Smart Manufacturing: Concepts and Methods,scopus,2020-01-01,sciencedirect,"Measuring, managing, and transforming data for operational insights",https://api.elsevier.com/content/abstract/scopus_id/85124660944,"The process industries face many pressures, such as meeting production targets, minimizing costs, and maximizing product quality. In addition, manufacturers (process industries such as refineries, processing plants, metal producers, paper mills, pharmaceutical plants, or oil and gas producers) must adhere to increasing governmental safety and environmental compliance regulations. This chapter discusses how most of these issues can be addressed through intelligent use of real-time operating data continuously collected from their production facilities. It advocates the use of a data-driven strategy to optimize operational efficiency and maintenance and enable business personnel to quickly and easily take corrective action when abnormal conditions occur. By taking corrective actions in real time, unnecessary costs can be avoided, bottom line profitability is increased, employee safety is maintained, and responsible environmental stewardship is achieved. The strategy uses real-time data infrastructure software as the integration layer between production process control systems (operational technology), business systems, and advanced offline analytics (information technology).",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2020.12.1173,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Brewery wastewater treatment plant key-component estimation using moving-window recurrent neural networks,https://api.elsevier.com/content/abstract/scopus_id/85119620811,"This work proposes an experimental validation of software sensors for advanced on-line anaerobic digester process monitoring. The considered strategy is based on cheap available measurements (conductivity, temperature, pH, redox potential, etc) to reconstruct key component trajectories such as volatile fatty acid, carbonate and alkalinity concentrations, as well as biogas composition (methane, carbon dioxide, etc). The proposed solution considers a radial basis function artificial neural network (RBF -ANN) structure, using data processing (principal component analysis) and an efficient and fast sequential learning algorithm. In order to better reproduce unknown and complex process dynamics, the combination of a moving-window technique with a simple Jordan recurrent ANN structure (MW - RBF - RNN) is proposed. Comparative results based on real industrial data illustrate the estimation improvements provided by the MW - RBF - RNN with respect to the classical RBF - ANN structure.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2020.12.751,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Intelligent comprehensive Occupational health monitoring system for mine workers,https://api.elsevier.com/content/abstract/scopus_id/85119604308,The objective of this work is to present a comprehensive occupational health monitoring system which provides the current state of the occupational health for mine workers. The hearing threshold shift and dust exposure of each individual mine worker is monitored using this system. The data obtained from the system is transmitted via Internet of Things to storage which may be cloud or a server. The novelty of this model lies in its dual ability to monitor both Noise Induced Hearing Loss and Pneumoconiousis which is caused by inhalation of dust particles. The output of this dual system is further processed using Machine learning and artificial intelligence techniques. Recommendations are then provided to the mine worker with regards to their state of health. This system forms part of an early intervention system in the mines. The model was validated using real data from a Platinum mine in South Africa. Future improvement to this work would entail refinement of the current preliminary implementation plan and carrying out the first phase of the implementation.,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2020.12.1251,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,A Framework for Ethics in Cyber-Physical-Human Systems,https://api.elsevier.com/content/abstract/scopus_id/85113481995,"This paper proposes a conceptual framework for consideration of ethical issues in the emerging category of smart cyber-physical systems. Cyber-physical systems (CPS) that bring together controls, communications, computing, and physical systems are being developed in a wide variety of application domains ranging from transportation, energy, and manufacturing, to biomedical and agriculture. Smart CPS are already being and will increasingly be deployed to work with humans, in workplaces, homes, or public spaces, resulting in the creation of cyber-physical human systems (CPHS). Ethical issues in smart CPS and CPHS can be examined within the larger frameworks of ethics of technology and ethics of artificial intelligence. We begin with a description of trends and visions for the future development of smart CPS. We next outline fundamental theories of ethics that offer foundations for thinking about ethical issues in smart CPHS. We argue that it is necessary to fight the tendency toward technological determinism. We argue that in analyzing ethics of smart CPHS, we need to anticipate increasing capabilities and the future deployment of such systems. Ultimately, if these systems are widely deployed in society, they will have a very significant impact, including possible negative consequences, on individuals, communities, nations, and the world. Our framework has two main dimensions: (i) stage of development of CPHS domain from early stage research to mature technologies; and (ii) locus of decision making: individual, corporate, and government settings. We illustrate the framework with some specific examples.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2020.12.2866,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Reinforcement learning for dual-resource constrained scheduling,https://api.elsevier.com/content/abstract/scopus_id/85107805245,"This paper proposes using reinforcement learning to solve scheduling problems where two types of resources of limited availability must be allocated. The goal is to minimize the makespan of a dual-resource constrained flexible job shop scheduling problem. Efficient practical implementation is very valuable to industry, yet it is often only solved combining heuristics and expert knowledge. A framework for training a reinforcement learning agent to schedule diverse dual-resource constrained job shops is presented. Comparison with other state-of-the-art approaches is done on both simpler and more complex instances that the ones used for training. Results show the agent produces competitive solutions for small instances that can outperform the implemented heuristic if given enough time. Other extensions are needed before real-world deployment, such as deadlines and constraining resources to work shifts.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2020.12.2856,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,A deep learning unsupervised approach for fault diagnosis of household appliances,https://api.elsevier.com/content/abstract/scopus_id/85107800132,Fault detection and fault diagnosis are crucial subsystems to be integrated within the control architecture of modern industrial processes to ensure high quality standards. In this paper we present a two-stage unsupervised approach for fault detection and diagnosis in household appliances. In particular a suitable testing procedure has been implemented on a real industrial production line in order to extract the most meaningful features that allow to efficiently classify different types of fault by consecutively exploiting deep autoencoder neural network and k-means or hierarchical clustering techniques.,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2020.12.2855,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Fault prediction as a service in the smart factory: Addressing common challenges for an effective implementation,https://api.elsevier.com/content/abstract/scopus_id/85107753365,"Fault prediction in manufacturing systems has consistently been an important theme in engineering research. Data-driven methods to deliver this service are gaining momentum due to developments regarding information and communication technologies. Particularly, fault prediction may be interpreted as a supervised learning classification problem, in which algorithms trained by operational data gathered from the shop-floor are capable of informing managers whether a machine might enter in a failure state or not. Despite the relevance of this approach, implementations are hindered by several challenges. In this work, we review approaches aimed to deal with four of these challenges, namely: limited amount of training data, unbalanced training data sets, uncertainty regarding which variables should be monitored, and uncertainty regarding how exactly historical data should be employed in the algorithm’s training. To deal with training sets with limited size, learning procedures observed to perform well with a lower volume of training data can be used, such as the Random Forests technique. Alternatively, transfer learning techniques can be utilized to adapt models trained in a virtual domain with abundant synthetic data to the real manufacturing system domain. To deal with unbalance among classification classes, cost-sensitive learning methods can be employed to alter the penalties incurred when misclassifications occurs in the minority class. Alternatively, resampling methods can be applied before learning occurs. Lastly, both the decisions regarding which variables to track, and to what extent historical data should be included in the training process, can be addressed through the use of specific feature selection methods.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-821326-1.00007-3,Book,An Industrial IoT Approach for Pharmaceutical Industry Growth: Volume 2,scopus,2020-01-01,sciencedirect,Internet of Things: From hype to reality,https://api.elsevier.com/content/abstract/scopus_id/85105522320,"The era of the Internet of Things (IoT) is sweeping over and replacing the Internet creating a world where smart things exist connected to each other intelligently. This was predicted by Eric Emerson Schmidt, the former C.E.O. of Google over 20 years ago. The physical world is now connecting to the digital world so quickly with the emergence of the IoT that it seems the Internet will become invisible soon, meaning the physical world will be connecting to the digital world seamlessly. The world will enjoy smart connectivity in the same way that the city of Barcelona has emerged to be the smartest city in the world. We are moving toward system-to-system connection, with smart networking reaching its peak. The idea of software-defined autonomous machines is about to become hugely important, which will become ubiquitous. With the advent of the IoT, we explore how it is becoming a reality and whether it has any limits. Maciej Kranz in his book on the IoT explains the very essential detailed and inclusive idea of the IoT, with IoT expanding to businesses, and covering and impacting on a variety of technology areas. Artificial intelligence and machine learning have a huge scope because of the enormous data generated by sensors and devices connected through the IoT. We will explore in this chapter the hype around the IoT and the reality. We will also discover improved metrics in the IoT that is allowing it to be a leader in the technological world. We are witnessing the fourth revolution in the digitization world and discuss the reasons behind its exponential growth. The protocols that differentiate them from others have evolved for IOT in a new set of patterns. This also creates security concerns and data are described as the new oil, raising further challenges of data privacy.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2020.12.299,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Artificial intelligence platform proposal for paint structure quality prediction within the industry 4.0 concept,https://api.elsevier.com/content/abstract/scopus_id/85103128034,"This article provides an artificial intelligence platform proposal for paint structure quality prediction using Big Data analytics methodologies. The whole proposal fits into the current trends that are outlined in the Industry 4.0 concept. The painting process is very complex, producing huge volumes of data, but the main problem is that the data comes from different data sources, often heterogeneous, and it is necessary to propose a way to collect and integrate them into a common repository. The motivation for this work were the industry requirements to solve specific problems that cannot be solved by standard methods but require a sophisticated and holistic approach. It is the application of artificial intelligence that suggests a solution that is not otherwise visible, and the use of standard methods would not give any satisfactory results. The result is the design of an artificial intelligence platform that has been deployed in a real manufacturing process, and the initial results confirm the correctness and validity of this step. We also present a data collection and integration architecture, which is an integral part of every big data analytics solution, and a principal component analysis that was used to reduce the dimensionality of the large number of production process data.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.matpr.2020.08.718,Conference Proceeding,Materials Today: Proceedings,scopus,2020-01-01,sciencedirect,Hybrid clustering algorithm for an efficient brain tumor segmentation,https://api.elsevier.com/content/abstract/scopus_id/85102451494,"This work describes the data mining methods, techniques and algorithms used for implementation. It is an emerging field of IT industry and research. There are many other fields such as Artificial Intelligence, Machine Learning, Deep Learning, Virtualization, Visualization, Parallel Computing and Image Processing. The human internal Brain can be seen or visualized by the Magnetic Resonance Imaging scan or Computerized Tomography scan. The MRI image is scanned and will be taken as input for processing. The MRI scan is more advantageous and more comfortable than CT scan for diagnosis. MRI scan provides detailed picture of organs. It does not affect the human health and body condition. It doesn't use any radiation. It is purely based on the magnetic field and radio waves. LIPC technique makes the training samples from the patients and arranges them into different group of classes used to construct different dictionaries. Image segmentation is a technique of dividing an image into different multiple portions, which is used to spot out objects and boundaries in images. There are many image segmentation techniques applicable for image processing. No acceptable method is available for solving all kinds of segmentation problem. Every method has merits and demerits. So, choosing good method is the challenging task. The hybrid clustering method is proposed in this work. The k-means algorithm and fuzzy c-means algorithm is proposed for brain tumor segmentation. The algorithm is implemented in synthetic and real time dataset. From the experimental results, this method provides better results in the form of accuracy.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2020.03.134,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Reconstructing CNC platform for EDM machines towards smart manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85102047186,"CNC (computer numerical control) systems play an ultimately important role for controlling EDM (electrical discharge machining) machine tools and their machining processes. Till now, existing CNC systems do not offer sufficient openness that supports researchers and engineers to expend its capabilities and functionalities in response to the increasing demands of smart manufacturing; on the other hand, transforming an EDM machine made by small and medium-sized machine tool builders, into a smart manufacturing system has never been an easy job. To address the issues and overcome the difficulties which block the way towards smart manufacturing, this paper proposes an open architecture CNC platform for EDM machine tools. This platform utilizes the state-of-the-art technologies in implementation of the hardware and software without compromising with the constraints of obsolete techniques. For demonstrating the unique capabilities, the generalized unit arc length increment (GUALI) interpolation method and the Digitizer/Player system architecture are adopted. To exhibit the feasibilities of the newly developed platform, three kinds of EDM machine tools are applied associated with advanced functionalities such as machining process adaptive control, applications of machine learning, 6-axis EDM of shrouded turbine blisks etc. In addition, a small-scale smart manufacturing unit for drilling film cooling holes of turbine blades is built up into real production by applying the new CNC system and related software applications. From the practitioner’s viewpoint, openness and standardization are the keys that enable the people from academia and industry bringing in their domain knowledge to enrich the smart manufacturing ecosystem.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2020.07.006,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Operator support in human-robot collaborative environments using AI enhanced wearable devices,https://api.elsevier.com/content/abstract/scopus_id/85100836551,"Nowadays, in order to cover the needs of market for product mass customization, industries have started to move to hybrid production cells, involving both robots and human operators. Research has been done during previous years to promote and improve the collaboration between humans and robots, trying to address topics such as safety, awareness and cognitive support in form of Augmented Reality based instructions. Results of previous research show bottlenecks related to the way of interaction of the operators with such supportive systems though. Direct interaction approach with the use of push buttons or indirect-gesture based interaction, which are most often adopted by the researchers, require operators to constantly occupy their hands performing the relevant button presses or gestures. Moreover, previous approaches are hardware dependent and need a lot of customization to work with different hardware. This work tries to address these bottlenecks proposing the usage of wearable devices enhanced with AI in order to support the interaction of human operators with robots in human-robot collaborative environments in a seamless and non-intrusive way, wrapped around a framework called “Operator Support Module” (OSM). Among others, OSM supports a variety of hardware to easily fit in various industrial scenarios. Two case studies will be presented to demonstrate the approach.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.11.012,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Application of machine learning and vision for real-time condition monitoring and acceleration of product development cycles,https://api.elsevier.com/content/abstract/scopus_id/85100766330,"Development work within an experimental environment, in which certain properties are investigated and optimized, requires many test runs and is therefore often associated with long execution times, costs and risks. This can affect product, material and technology development in industry and research. New digital driver technologies offer the possibility to automate complex manual work steps in a cost-effective way, to increase the relevance of the results and to accelerate the processes many times over. In this context, this article presents a low-cost, modular and open-source machine vision system for test execution and evaluates it on the basis of a real industrial application. For this purpose a methodology for the automated execution of the load intervals, the process documentation and for the evaluation of the generated data by means of machine learning to classify wear levels. The software and the mechanical structure are designed to be adaptable to different conditions, components and for a variety of tasks in industry and research. The mechanical structure is required for tracking the test object and represents a motion platform with independent positioning by machine vision operators or machine learning. An evaluation of the state of the test object is performed by the transfer learning after the initial documentation run. The manual procedure for classifying the visually recorded data on the state of the test object is described for the training material. This leads to an increased resource efficiency on the material as well as on the personnel side since on the one hand the significance of the tests performed is increased by the continuous documentation and on the other hand the responsible experts can be assigned time efficiently. The presence and know-how of the experts are therefore only required for defined and decisive events during the execution of the experiments. Furthermore, the generated data are suitable for later use as an additional source of data for predictive maintenance of the developed object.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2020.10.091,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Towards smart manufucturing: Implementation and benefits,https://api.elsevier.com/content/abstract/scopus_id/85099879698,"Production activities are generating a large amount of data in different types (i.e., text, images), that is not well exploited. This data can be translated easily to knowledge that can help to predict all the risks that can impact the business, solve problems, promote efficiency of the manufacturing to the maximum, make the production more flexible and improving the quality of making smart decisions, however, implementing the Smart Manufacturing(SM) concept provides this opportunity supported by the new generation of the technologies. Internet Of Things (IoT) for more connectivity and getting data in real time, Big Data to store the huge volume of data and Deep Learning algorithms(DL) to learn from the historical and real time data to generate knowledge, that can be used, predict all the risks, problem solving, and better decision-making.
                  In this paper, we will introduce SM and the main technologies to success the implementation, the benefits, and the challenges.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.10.053,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Enabling real-time quality inspection in smart manufacturing through wearable smart devices and deep learning,https://api.elsevier.com/content/abstract/scopus_id/85099870958,"In this paper, we present a novel method for utilising wearable devices with Convolutional Neural Networks (CNN) trained on acoustic and accelerometer signals in smart manufacturing environments in order to provide real-time quality inspection during manual operations. We show through our framework how recorded or streamed sound and accelerometer data gathered from a wrist-attached device can classify certain user actions as successful or unsuccessful. The classification is designed with a Deep CNN model trained on Mel-frequency Cepstral Coefficients (MFCC) from the acoustic input signals. The wearable device provides feedback on three different modalities: audio, visual and haptic; thus ensuring the worker’s awareness at all time. We validate our findings through deployments of the complete AI-enabled device in production facilities of Mercedes-Benz AG. From the conducted experiments it is concluded that the use of acoustic and accelerometer data is valuable to train a classifier with the purpose of action examination during industrial assembly operations, and provides an intuitive interface for ensuring continued and improved quality inspection.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.10.126,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Simulation-as-a-service for reinforcement learning applications by example of heavy plate rolling processes,https://api.elsevier.com/content/abstract/scopus_id/85099821374,"In the production industry, the digital transformation enables a significant optimization potential. The concept of reinforcement learning offers a suitable approach to train agents on learning control strategies, further advancing automation. While applications training directly on real-world processes are rare due to economical and safety constraints, simulations offer a way to develop and evaluate agents prior to deployment. With the rise of service-based business models, the simulation owner and the machine learning expert are likely to be different stakeholders in a joint project. Due to different requirements for both simulations and reinforcement-learning agents, the stakeholders may be reluctant or unable to grant full access to the respective software. This poses a serious impediment to the potential of the digital transformation. In this paper, a distributed architecture is proposed, which allows the remote training of reinforcement learning agents on a simulation. It is shown that this architecture allows the cooperation between two stakeholders by exposing a suitable technical interface to the simulation. The proposed architecture is implemented for a simulation of the multi-step metal forming process of heavy plate rolling. Furthermore, the implemented architecture is used to successfully train a reinforcement-learning agent on the task of designing optimal parameter schedules.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.05.123,Conference Proceeding,,scopus,2020-01-01,sciencedirect,Integrated tool condition monitoring systems and their applications: A comprehensive review,https://api.elsevier.com/content/abstract/scopus_id/85095576577,"In conventional metal cutting, different tool wear modes, and their individual deterioration rates play vital roles in overall production performance. For a given tool (i.e., geometry or materials), many shop floors still follow a standard rule by pre-setting a tool life, which is conservative but not realistic. Premature failure of a tool can cause unexpected machine downtime and material losses, while another tool could serve beyond that pre-set life. As a result, optimized tool life and productivity cannot be achieved. Moreover, nowadays, there is an increased demand of process monitoring and optimization on the unmanned and the semi-automated shop floors.
                  Tool condition monitoring (TCM) systems for process improvement and optimization have been in research for several decades. Both offline and online TCM systems are invented and discussed. A wide range of original publications are reported focusing on different sub-topics, e.g., specific machining process-based TCM methods, measurement or signal acquisition methods, processing methods, and classifiers. With the recent evolution of smart sensors in the era of Industry 4.0, development of online TCM systems received much attention to the researchers. Accordingly, research on some sub-topics also gets motivated into different directions, such as, feasibility of power or current sensors, machine vision technique, and combination of multi-sensors. Thus, from the industrial viewpoint, the current state of implementation of the proposed TCM systems for (near) real-time process monitoring and control needs to be clear. This paper presents the state-of-the-art of the TCM systems covering three major machining operations, discusses their application feasibility in industry environments, and states some current TCMS implementations. Challenges being faced by the industry are concluded, along with direction and suggestions for future researches.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.05.140,Conference Proceeding,,scopus,2020-01-01,sciencedirect,Development of real-time diagnosis framework for angular misalignment of robot spot-welding system based on machine learning,https://api.elsevier.com/content/abstract/scopus_id/85095131276,"This paper focuses on the real-time online monitoring and diagnosis framework for the angular misalignment of the robot spot-welding system, which can result in significant quality degradation of a weld nugget such as porosity. The data-driven approach is applied by installing the voltage and current sensors, collecting the associated mass data and processing them under normal and abnormal (angular misalignment) conditions. Two categories of features are extracted from the dynamic resistance (DR) and the voltage and current ones that are decomposed by wavelet transform (WT). The DR features are extracted from the DR profile and some critical features are selected by a t-test methodology. In the case of the WT-based features, the critical ones are selected by a max-relevance and min-redundancy (mRMR) and a sequential backward selection (SBS) wrapper. Consequently, three types of critical feature sets, such as DR features, WT features, and hybrid features combining those, are prepared to train machine learning-based models. Support vector machine (SVM) and probabilistic neural network (PNN) are applied to establish the diagnosis models, and the diagnostic accuracy and robustness are evaluated. Finally, the software for the on-line monitoring and diagnosis for angular misalignment of robot spot-welding system is developed and demonstrates its real-time applicability in an industrial site.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.05.146,Conference Proceeding,,scopus,2020-01-01,sciencedirect,One-shot recognition of manufacturing defects in steel surfaces,https://api.elsevier.com/content/abstract/scopus_id/85095111982,"Quality control is an essential process in manufacturing to make the product defect-free as well as to meet customer needs. The automation of this process is important to maintain high quality along with the high manufacturing throughput. With recent developments in deep learning and computer vision technologies, it has become possible to detect various features from the images with near-human accuracy. However, many of these approaches are data intensive. Training and deployment of such a system on manufacturing floors may become expensive and time-consuming. The need for large amounts of training data is one of the limitations of the applicability of these approaches in real-world manufacturing systems. In this work, we propose the application of a Siamese convolutional neural network to do one-shot recognition for such a task. Our results demonstrate how one-shot learning can be used in quality control of steel by identification of defects on the steel surface. This method can significantly reduce the requirements of training data and can also be run in real-time.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2020.04.158,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Image processing based on deep neural networks for detecting quality problems in paper bag production,https://api.elsevier.com/content/abstract/scopus_id/85092428222,"It is critical for manufacturers to identify quality issues in production and prevent defective products being delivered to customers. We investigate the use of deep neural networks to perform automatic quality inspections based on image processing to eliminate the current manual inspection. A deep neural network was implemented in a real-world industrial case study, and its ability to detect quality problems was evaluated and analyzed. The results show that the network has an accuracy of 94.5%, which is considered good in comparison to the 70–80% accuracy of a trained human inspector.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2020.04.135,Conference Proceeding,Procedia CIRP,scopus,2020-01-01,sciencedirect,Application of Artificial Intelligence to an Electrical Rewinding Factory Shop,https://api.elsevier.com/content/abstract/scopus_id/85091693237,"The evolution of artificial intelligence (AI) and big data resulted in the full potential realization of technologies through convergence. Tremendous acceptance, adoption and implementation of the United Nations Sustainable Development Goals (SDG) Agenda 2030, has resulted in original equipment manufacturers (OEM) developing various designs of rotary machines in a bid to improve energy efficiency, with more improvements expected in the coming decade. An effective technique to manage energy efficiency in the smart grid is through integration of demand side management, inclusive of optimization of rewinding of an electric motor in a machine shop. This paper aims to conceptualize application of AI and augmented reality (AR) towards process visibility of remanufacturing rotary machine stators by robotic vision. SLT is the triangulation methodology used in laser scanning for 3D modelling, and instantaneous condition assessment of the core. A pre-defined robotic path is used towards identification of features for range image acquisition. Therefore, the potential of industry 4.0 in resuscitation of end-of-life products through service remanufacturers by RE in a rewinding shop are presented.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2020.05.068,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Adoption of the conceive-design-implement-operate approach to the third year project in a team-based design-build environment,https://api.elsevier.com/content/abstract/scopus_id/85089026312,"In the current day scenario, teaching takes place either on the blackboard or on a power point presentation projected on the wall. Traditional methods of teaching are going to become a thing of the past. Instead, imagine students focusing their mobile screens in the middle of the classroom and they will be all viewing holographic 3D objects surfacing from a table and the teacher explaining the visuals. This is going to be a reality in the near future and this technological innovation will transform the teaching-learning process. The objective of this paper is to propose a framework for using Augmented Reality in mobile phones for engineering education. Use of mobile devices in education provide a new educational paradigm, called M-Learning, It offers many opportunities for students to work on their creativity, while at same time it becomes an element of motivation and collaboration. This paper proposes the modern way of AR technology in engineering education, a handy tool to make teaching and learning effective and also proposes a experimental setup to convert a topic to an Android App.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2020.04.199,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Perspective Vehicle License Plate Transformation using Deep Neural Network on Genesis of CPNet,https://api.elsevier.com/content/abstract/scopus_id/85086630682,"Recent development in vehicular industries and increased number of cars in modern society leads the people to pay more attention on Vehicle License Plate Recognition (V-LPR). V-LPR plays a major role in traffic related application such as road traffic monitoring, vehicle parking lots access control etc. Existing state of the art V-LPR systems in real world deployment works under restricted conditions, such as static illumination, fixed background etc. Most of them fails to work when any of the above given conditions are violated. Hence to address this issue, a novel V-LPR system is designed using modern deep learning framework called ""Capsule Network"". The proposed system is robust and works fine in any condition. Further, the proposed method aims to improve the processing time by integrating the segmentation process within the CN framework which involves the training and recognizing of entire license plate cropped region. Moreover, the feature extraction is performed by CN framework over a segmented alphanumeric character. Finally, Data augmentation technique is also used as a supplement to the CN framework to strengthen the process of training with various orientations like rotation, shift and flip for improving the recognition task.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2020.03.027,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Strategic zoning approach for urban areas: Towards a shared transportation system,https://api.elsevier.com/content/abstract/scopus_id/85085571988,"Investigating downstream freight demand is a prerequisite to accomplishing the overall strategic implementation of transportation systems. Machine learning has recently become widely applied in order to support decision-making in several logistic operational levels: travel/arrival time prediction, occupancy forecasting of logistic spaces, route optimization and so on. Nevertheless, strategic decision-making often overlooks flow tendencies forecasting. Targeting this perspective, the present paper aims at proposing an urban zoning approach based on time series forecasting of supply chain demand through clustering customers. To conduct our approach, we have selected a set of machine learning algorithms that are believed to be robust according to the literature and the achieved accuracy benchmarks. Considering real-life data-based computational results, a number of analytical insights are illustrated.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2020.03.044,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,An artificial intelligence based crowdsensing solution for on-demand accident scene monitoring,https://api.elsevier.com/content/abstract/scopus_id/85085566175,"Road traffic crashes have a devastating impact on societies by claiming more than 1.35 million lives each year and causing up to 50 million injuries. Improving the efficiency of emergency management systems constitutes a key measure to reduce road traffic deaths and injuries. In this work, we propose a comprehensive crowdsensing-based solution for the real-time collection and the analysis of accident scene intelligence as a means to improve the efficiency of the emergency response process and help reduce road fatalities. The solution leverages sensory, mobile, and web technologies for the real-time monitoring of accident scenes, and employs Artificial Intelligence for the automatic analysis of the accident scene data, to allow the automatic generation of accident intelligence reports. Police officers and rescue teams can use those reports for fast and accurate situational assessment and effective response to emergencies. The proposed system was fully implemented and its operation was successfully tested using a variety of scenarios. This work gives interesting insights into the possibility of leveraging crowdsensing and artificial intelligence for offering emergency situational awareness and improving the efficiency of emergency response operations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2020.03.004,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Activity Recognition in Smart Homes using UWB Radars,https://api.elsevier.com/content/abstract/scopus_id/85085563629,"In the last decade, smart homes have transitioned from a potential solution for aging-in-place to a real set of technologies being deployed in the real-world. This technological transfer has been mostly supported by simple, commercially available sensors such as passive infrared and electromagnetic contacts. On the other hand, many teams of research claim that the sensing capabilities are still too low to offer accurate, robust health-related monitoring and services. In this paper, we investigate the possibility of using Ultra-wideband (UWB) Doppler radars for the purpose of recognizing the ongoing ADLs in smart homes. Our team found out that with simple configuration and classical features engineering, a small set of UWB radars could reasonably be used to recognize ADLs in a realistic home environment. A dataset was built from 10 persons performing 15 different ADLs in a 40 square meters apartment with movement on the other side of the wall. Random Forest was able to attain 80% accuracy with an F1-Score of 79%, and a Kappa of 77%. Those results indicate the use of Doppler radars can be a good research avenue for smart homes.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2020.03.036,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Air Quality Forecasting using LSTM RNN and Wireless Sensor Networks,https://api.elsevier.com/content/abstract/scopus_id/85085553433,"In the past few decades, many urban areas around the world have suffered from severe air pollution and the health hazards that come with it, making gathering real-time air quality and air quality forecasting very important to take preventive and corrective measures. This paper proposes a scalable architecture to monitor and gather real-time air pollutant concentration data from various places and to use this data to forecast future air pollutant concentrations. Two sources are used to collect air quality data. The first being a wireless sensor network that gathers and sends pollutant concentrations to a server, with its sensor nodes deployed in various locations in Bengaluru city in South India. The second source is the real-time air quality data gathered and made available by the Government of India as a part of its Open Data initiative. Both sources provide average concentrations of various air pollutants on an hourly basis. Due to its proven track record of success with time-series data, a Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) model was chosen to perform the task of air quality forecasting. This paper critically analyses the performance of the model in two regions that exhibit a significant difference in temporal variations in air quality. As these variations increase, the model suffers performance degradation necessitating adaptive modelling.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.04.017,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,PPE compliance detection using artificial intelligence in learning factories,https://api.elsevier.com/content/abstract/scopus_id/85085527469,"This project demonstrates the application of Artificial Intelligence (AI) and machine vision for the identification of Personal Protective Equipment (PPE), particularly safety glasses in zones of the Learning Factory, where safety risks exist. The objective is to design and implement an automated system for ensuring the safety of personnel when they are in the vicinity of machinery that presents potential risks to the eyes. Microsoft Azure Custom Vision AI and Intelligent AI Services, in conjunction with low-cost vision devices with lightweight onboard AI capability, provide a platform for a deep learning neural network model using publicly available images under the Creative Commons License. A combination of cloud-based and on-premises AI is used in this proof of concept system to provide a real-time vision-based safety system capable of detecting and recording potential safety breaches, promoting compliance, and ultimately preventing accidents before they happen. This system can be used to initiate different control actions in the event of safety violations and can detect multiple forms of protective wear. The flexibility of the system offers multiple benefits to learning factories and manufacturing organizations such as improved user safety, reduced insurance costs, and better detection and recording of safety violations. The hybrid AI architecture approach allows for flexibility in training and deployment based on the capability of local computing resources.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.04.082,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Integrating virtual and physical production processes in learning factories,https://api.elsevier.com/content/abstract/scopus_id/85085519100,"Scaled learning factories are industrial learning environments that provide production systems and processes for learners on a model scale rather than using actual productive machines. This approach has benefits as for instance lower invest, increased approachability and higher safety levels. At the same time, constraints for implementation of actual production processes and required abstraction levels from industrial processes are limitations. To bridge the gap between benefits and limitation we propose the integration of virtual production processes in a prevalent physical learning factories. Resulting mixed reality solutions bear the potential to combine real and virtual objects at the same time and thus extend the physical model environment with virtually represented processes. Based on an initial analysis we develop a concept using spatial augmented reality and a game engine based simulation to realize a virtual integrated production process. The theoretical concept as well as the technical implementation is described. A first evaluation indicates a high rate of acceptance by trainees and illustrates the benefits for learning performance.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.04.055,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,From digital shop floor to real-time reporting: An IIoT based educational use case,https://api.elsevier.com/content/abstract/scopus_id/85085498833,"The Smart Production Lab (Lab)at the FH JOANNEUM in Kapfenberg, Austria, is a digital learning and research factory with an interdisciplinary focus on vertical and horizontal IT-integration. It is aiming at a higher transparency and productivity by applying latest digital technologies. The key technology is the Industrial Internet of Things (IIoT). Therefore, research driven IoT use cases are further developed such as hybrid IoT-concepts and architectures involving edge and cloud computing. State-of-the-Art use cases apply of-the-shelf technologies for ready-to-use implementations and teaching purposes. This paper introduces a case-based teaching concept in the area of IIoT. It provides students with a hands-on experience as well as deep insights in what is meant by modelling and implementing an IoT data flow from the shop floor to real-time reporting. For this purpose, on the operational technology (OT) layer IoT nodes were attached to the machinery in the Lab gathering and providing data for the IoT middleware layer, based on Open Platform Communication Unified Architecture (OPC UA). This central middleware-layer is represented by the open source platform Node-RED. In the respective use case the data is transformed in order to be stored in a NoSQL database, from where it can be accessed for real-time reporting either by cloud or on premise applications. The interdisciplinary nature of these use case consists of integrating the different aspects of a digital production, involving disciplines such as automation, digital retrofitting, operational technology, and informational technology. Thus, it provides students with a comprehensive understanding of the benefits and limitations of IIoT.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.04.037,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Implementing AR/MR - Learning factories as protected learning space to rise the acceptance for mixed and augmented reality devices in production,https://api.elsevier.com/content/abstract/scopus_id/85085498037,"When talking about digitization, changes in the way of working are inevitable: The implementation of intelligent machines or dealing with real-time data lead to new tasks supported by new technology. Also digital technologies such as Augmented and Mixed Reality (AR/MR) are pushing the market and setting new standards in collaboration, prototyping or maintenance. The correct handling of AR/MR devices requires a change in the employees’ behavior; changing working routines are followed by a new skill set and a change in the culture. The acceptance of employees can therefore be regarded as a critical success factor for the implementation of such technologies. Thus, the present paper answers the research question ‘what factors influence the employee’s acceptance of AR and MR data glasses in industry’. On the basis of a comprehensive literature analysis, an implementation workshop was developed and validated in cooperation with an industrial partner. The results were transformed into a workshop within the learning and research factory ‘Smart Production Lab’ to give employees and students the opportunity to train the handling of data glasses in a protected learning space in order to increase the acceptance for the technology.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.04.066,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,5G and AI technology application in the AMTC learning factory,https://api.elsevier.com/content/abstract/scopus_id/85085489730,"5G and AI (Artificial Intelligence) are changing industrial production and offer great potential for manufacturing enterprises. One of the effects resulting from the increasing quantity of production data is the increasing demands of transmission of large amounts of data, fast transmission speed, and rapid data analysis. However, merely relying on traditional communication technology and manual data processing does not lead to high transmission performance and low analysis time. It is essential to integrate 5G and AI technology to flexibly transmit large amounts of data and real-time data. To demonstrate the feasibility and potential of these two technologies, a concept was developed at the Advanced Manufacturing Technology Center (AMTC) at the Tongji University (Shanghai, China) and further implemented in the AMTC learning factory in cooperation with wbk of Karlsruhe Institute of Technology (Karlsruhe, Germany) and Ruhr-University Bochum (Bochum, Germany). This paper presents the learning factory design in detail, describing the concept design, training environment and training phases in the AMTC learning factory. It is followed by a case study consisting of specific examples of 5G and AI, implemented in the AMTC learning factory. The importance of integrated 5G and AI applications is pointed out and discussed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.04.038,Conference Proceeding,Procedia Manufacturing,scopus,2020-01-01,sciencedirect,Seamless data integration in the CAM-NC process chain in a learning factory,https://api.elsevier.com/content/abstract/scopus_id/85085473050,"The seamless data integration of different components in the CAM-NC process chain (tool management software, tool dispensing system, presetting machine and machine tool) is essential for maximizing the efficiency and minimizing the total error rate. This is done by entering the data into the system of the network only once. Then this data is available for all participants in this network at any time. This paper describes the aforementioned integration by using the example of creating a digital tool, which is used in a CAM simulation afterwards. Then the real set-up and machining process is discussed. The process chain explained in this paper was implemented at the smartfactory@tugraz - the Learning Factory at Graz University of Technology.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.softx.2020.100419,Journal,SoftwareX,scopus,2020-01-01,sciencedirect,TWINKLE: A digital-twin-building kernel for real-time computer-aided engineering,https://api.elsevier.com/content/abstract/scopus_id/85079158568,"TWINKLE is a library for building families of solvers to perform Canonical Polyadic Decomposition (CPD) of tensors. The common characteristic of these solvers is that the data structure supporting the tuneable solution strategy is based on a Galerkin projection of the phase space. This allows processing and recovering tensors described by highly sparse and unstructured data. For achieving high performance, TWINKLE is written in C++ and uses the Armadillo open source library for linear algebra and scientific computing, based on LAPACK (Linear Algebra PACKage) and BLAS (Basic Linear Algebra Subprograms) routines. The library has been implemented keeping in mind its future extensibility and adaptability to fulfil the different users’ needs in academia and industry regarding Reduced Order Modelling (ROM) and data analysis by means of tensor decomposition. It is especially focused on post-processing data from Computer-Aided-Engineering (CAE) simulation tools.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aei.2020.101044,Journal,Advanced Engineering Informatics,scopus,2020-01-01,sciencedirect,IoT edge computing-enabled collaborative tracking system for manufacturing resources in industrial park,https://api.elsevier.com/content/abstract/scopus_id/85078852726,"In manufacturing industry, the movement of manufacturing resources in production logistics often affects the overall efficiency. This research is motivated by a world-leading air-conditioner manufacturer. In order to provide the right manufacturing resources for subsequent production steps, excessive time and human effort has been consumed in locating the manufacturing resources in a huge industrial park. The development of Internet of Things (IoT) has made a profound impact on establish smart manufacturing workshop and tracking applications, however a growing trend of data quantity that generated from massive, heterogeneous and bottomed manufacturing resources objects pose challenge to centralized decision. In this study, the concept of edge-computing deeply integrated in collaborative tracking purpose in virtue of IoT technology. An IoT edge computing enabled collaborative tracking architecture is developed to offload the computation pressure and realize distributed decision making. A supervised learning of genetic tracking method is innovatively presented to ensure tracking accuracy and effectiveness. Finally, the research output is developed and implemented in a real-life industrial park for verification. The results show that the proposed tracking method not only performs constant improving accuracy up to 96.14% after learning compared to other tracking method, but also ensure quick responsiveness and scalability.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aei.2020.101037,Journal,Advanced Engineering Informatics,scopus,2020-01-01,sciencedirect,A smart surface inspection system using faster R-CNN in cloud-edge computing environment,https://api.elsevier.com/content/abstract/scopus_id/85078666892,"Automated surface inspection has become a hot topic with the rapid development of machine vision technologies. Traditional machine vision methods need experts to carefully craft image features for defect detection. This limits their applications to wider areas. The emerging convolutional neural networks (CNN) can automatically extract features and yield good results in many cases. However, the CNN-based image classification methods are more suitable for flat surface texture inspection. It is difficult to accurately locate small defects in geometrically complex products. Furthermore, the computational power required in CNN algorithms is usually high and it is not efficient to be implemented on embedded hardware. To solve these problems, a smart surface inspection system is proposed using faster R-CNN algorithm in the cloud-edge computing environment. The faster R-CNN as a CNN-based object detection method can efficiently identify defects in complex product images and the cloud-edge computing framework can provide fast computation speed and evolving algorithm models. A real industrial case study is presented to illustrate the effectiveness of the proposed method. The results show that the proposed method can provide high detection accuracy within a short time.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aei.2019.101013,Journal,Advanced Engineering Informatics,scopus,2020-01-01,sciencedirect,Guidelines for applied machine learning in construction industry—A case of profit margins estimation,https://api.elsevier.com/content/abstract/scopus_id/85075778987,"The progress in the field of Machine Learning (ML) has enabled the automation of tasks that were considered impossible to program until recently. These advancements today have incited firms to seek intelligent solutions as part of their enterprise software stack. Even governments across the globe are motivating firms through policies to tape into ML arena as it promises opportunities for growth, productivity and efficiency. In reflex, many firms embark on ML without knowing what it entails. The outcomes so far are not as expected because the ML, as hyped by tech firms, is not the silver bullet. However, whatever ML offers, firms urge to capitalise it for their competitive advantage. Applying ML to real-life construction industry problems goes beyond just prototyping predictive models. It entails intensive activities which, in addition to training robust ML models, provides a comprehensive framework for answering questions asked by construction folks when intelligent solutions are getting deployed at their premises to substitute or facilitate their decision-making tasks. Existing ML guidelines used in the IT industry are vastly restricted to training ML models. This paper presents guidelines for Applied Machine Learning (AML) in the construction industry from training to operationalising models, which are drawn from our experience of working with construction folks to deliver Construction Simulation Tool (CST). The unique aspect of these guidelines lies not only in providing a novel framework for training models but also answering critical questions related to model confidence, trust, interpretability, bias, feature importance and model extrapolation capabilities. Generally, ML models are presumed black boxes; hence argued that nobody knows what a model learns and how it generates predictions. Even very few ML folks barely know approaches to answer questions asked by the end users. Without explaining the competence of ML, the broader adoption of intelligent solutions in the construction industry cannot be attained. This paper proposed a detailed process for AML to develop intelligent solutions in the construction industry. Most discussions in the study are elaborated in the context of profit margin estimation for new projects.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/bs.adcom.2019.09.005,Book Series,Advances in Computers,scopus,2020-01-01,sciencedirect,Impact of cloud security in digital twin,https://api.elsevier.com/content/abstract/scopus_id/85073737509,"Digital Twin is a way to virtually represent or model a physical object using the real time data. This innovation sets up a way to deal with industries and organizations to supervise their products, consequently bridging the gap between design and implementations. As the name suggests, “Digital Twin” infers that a reproduction of the product is made in order to have a nearby relationship with the live item. The procedure of computerized twin begins by gathering real time data, processed data, and operational data and performs distinctive investigation which helps in anticipating the future. This additionally enhances the customer experiences by giving a digital feel of their product. The objective behind all these is the job of gathering information and putting them in a place, i.e., the cloud which could store exorbitant data. The user experience gets enhanced by the intervention of digital twin technology which could help in the successful working of the products geographically distributed. The impact of Internet of Things and Cloud Computing lifts up the digital twin.
                  The information gathered from the sources can be arranged in terms of utilization and prospect to change on a timely basis. These data, as they are stored require proper coordination and a legitimate use.
                  Digital Twin innovation assumes incredible opportunities in the field of manufacturing, healthcare, smart cities, automobile and so on. The effect of having a digital twin for the product makes it simple for activities and recognize the blemishes, if any happened. This approach can help reduce the workload and furthermore can get trained on the virtual machine without the need of a specific training.
                  With the most prevailing technologies of today, like Artificial Intelligence, Machine Learning and Internet of Things more prominent approach to train and monitor products, taking care of its own execution, collaborating to different frameworks, performing self-repairs are made possible. Hence the future is getting unfolded with the emerging DIGITAL TWIN era. The massive data utilized in the field of digital twin is prone to severe security breaches. Thus digital twin technology should be handled with extreme care so as to protect the data. Hence, this chapter identifies the ways and means of collecting, organizing and storing the data in a secured cloud environment. The data is filtered according to the use and priority and pushed into the cloud. It is determined to implement an exclusive algorithm for a secured cloud which would greatly benefit the users and the providers to handle and process it effectively.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.vehcom.2019.100198,Journal,Vehicular Communications,scopus,2020-01-01,sciencedirect,In-vehicle network intrusion detection using deep convolutional neural network,https://api.elsevier.com/content/abstract/scopus_id/85073150001,"The implementation of electronics in modern vehicles has resulted in an increase in attacks targeting in-vehicle networks; thus, attack detection models have caught the attention of the automotive industry and its researchers. Vehicle network security is an urgent and significant problem because the malfunctioning of vehicles can directly affect human and road safety. The controller area network (CAN), which is used as a de facto standard for in-vehicle networks, does not have sufficient security features, such as message encryption and sender authentication, to protect the network from cyber-attacks. In this paper, we propose an intrusion detection system (IDS) based on a deep convolutional neural network (DCNN) to protect the CAN bus of the vehicle. The DCNN learns the network traffic patterns and detects malicious traffic without hand-designed features. We designed the DCNN model, which was optimized for the data traffic of the CAN bus, to achieve high detection performance while reducing the unnecessary complexity in the architecture of the Inception-ResNet model. We performed an experimental study using the datasets we built with a real vehicle to evaluate our detection system. The experimental results demonstrate that the proposed IDS has significantly low false negative rates and error rates when compared to the conventional machine-learning algorithms.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eng.2019.02.013,Journal,Engineering,scopus,2019-12-01,sciencedirect,Artificial Intelligence in Steam Cracking Modeling: A Deep Learning Algorithm for Detailed Effluent Prediction,https://api.elsevier.com/content/abstract/scopus_id/85074530083,"Chemical processes can benefit tremendously from fast and accurate effluent composition prediction for plant design, control, and optimization. The Industry 4.0 revolution claims that by introducing machine learning into these fields, substantial economic and environmental gains can be achieved. The bottleneck for high-frequency optimization and process control is often the time necessary to perform the required detailed analyses of, for example, feed and product. To resolve these issues, a framework of four deep learning artificial neural networks (DL ANNs) has been developed for the largest chemicals production process—steam cracking. The proposed methodology allows both a detailed characterization of a naphtha feedstock and a detailed composition of the steam cracker effluent to be determined, based on a limited number of commercial naphtha indices and rapidly accessible process characteristics. The detailed characterization of a naphtha is predicted from three points on the boiling curve and paraffins, iso-paraffins, olefins, naphthenes, and aronatics (PIONA) characterization. If unavailable, the boiling points are also estimated. Even with estimated boiling points, the developed DL ANN outperforms several established methods such as maximization of Shannon entropy and traditional ANNs. For feedstock reconstruction, a mean absolute error (MAE) of 0.3 wt% is achieved on the test set, while the MAE of the effluent prediction is 0.1 wt%. When combining all networks—using the output of the previous as input to the next—the effluent MAE increases to 0.19 wt%. In addition to the high accuracy of the networks, a major benefit is the negligible computational cost required to obtain the predictions. On a standard Intel i7 processor, predictions are made in the order of milliseconds. Commercial software such as COILSIM1D performs slightly better in terms of accuracy, but the required central processing unit time per reaction is in the order of seconds. This tremendous speed-up and minimal accuracy loss make the presented framework highly suitable for the continuous monitoring of difficult-to-access process parameters and for the envisioned, high-frequency real-time optimization (RTO) strategy or process control. Nevertheless, the lack of a fundamental basis implies that fundamental understanding is almost completely lost, which is not always well-accepted by the engineering community. In addition, the performance of the developed networks drops significantly for naphthas that are highly dissimilar to those in the training set.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmapro.2019.10.020,Journal,Journal of Manufacturing Processes,scopus,2019-12-01,sciencedirect,Data-driven smart manufacturing: Tool wear monitoring with audio signals and machine learning,https://api.elsevier.com/content/abstract/scopus_id/85074281429,"Tool wear in machining could result in poor surface finish, excessive vibration and energy consumption. Monitoring tool wear in real-time is crucial to improve manufacturing productivity and quality. While numerous sensor-based tool wear monitoring techniques have been demonstrated in laboratory environments, few tool wear monitoring systems have been deployed in factories because it is not realistic to install some of the important sensors such as dynamometers on manufacturing machines. To address this issue, a novel audio signal processing approach is introduced. This technique does not require expensive sensors but audio sensors only. A blind source separation method is used to separate source signals from noise. An extended principal component analysis is used for dimensionality reduction. Real-time multi-channel audio signals are collected during a set of milling tests under varying cutting conditions. The experimental data are used to develop and validate a predictive model. Experimental results have shown that the predictive model is capable of classifying tool wear conditions with high accuracy.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.petrol.2019.106332,Journal,Journal of Petroleum Science and Engineering,scopus,2019-12-01,sciencedirect,Machine learning methods applied to drilling rate of penetration prediction and optimization - A review,https://api.elsevier.com/content/abstract/scopus_id/85070879413,"Drilling wells in challenging oil/gas environments implies in large capital expenditure on wellbore's construction. In order to optimize the drilling related operation, real-time decisions making have been put in place, so that prediction of rate of penetration (ROP) with accuracy is essential. Despite many efforts (theoretical and experimental) throughout the years, modeling the ROP as a mathematical function of some key variables is not so trivial, due to the highly non-linearity behavior experienced. Therefore, several researches in the recent years have been proposing to use data-driven models from artificial intelligence field for ROP prediction and optimization.
                  This paper presents an extensive review of the literature on ROP prediction, especially, with machine learning techniques, as well as how these models can be used to optimize the drilling activities. The ROP models are classified as traditional models (based on physics-models), statistical models (e.g. multiple regression), or machine learning methods. This review enables to see that machine learning techniques can potentially outperform in terms of ROP-prediction accuracy on top of traditional or statistical models. Throughout this work, an extensive analysis of different ways of obtaining ROP models is carried out, concluding with different strategies adopted in literature to perform data-driven model optimization.
                  Despite the saving potential which can be achieved with real-time optimization based on data-driven ROP models, it is noticeable that there is a lack of implementation of those techniques in the industry, as per literature review. To take a step forward in real implementations, the petroleum industry must be aware that yet no rule of thumb already exists on this specific area, but still, good and very reasonable results can be achieved by following the best practices identified in this review. In addition, the modern practices of machine learning provide promising guidelines for implementing projects in oil and gas industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.sigpro.2019.06.019,Journal,Signal Processing,scopus,2019-12-01,sciencedirect,A generic parallel computational framework of lifting wavelet transform for online engineering surface filtration,https://api.elsevier.com/content/abstract/scopus_id/85068175044,"Nowadays, complex geometrical surface texture measurement and evaluation require advanced filtration techniques. Discrete wavelet transform (DWT), especially the second-generation wavelet (Lifting Wavelet Transform – LWT), is the most adopted one due to its unified and abundant characteristics in measured data processing, geometrical feature extraction, manufacturing process planning, and production monitoring. However, when dealing with varied complex functional surfaces, the computational payload for performing DWT in real-time often becomes a core bottleneck in the context of massive measured data and limited computational capacities. It is a more prominent problem for the areal surface texture filtration by using 2D DWT. To address the issue, this paper presents a generic parallel computational framework for lifting wavelet transform (GPCF-LWT) based on Graphics Process Unit (GPU) clusters and the Compute Unified Device Architecture (CUDA). Due to its cost-effective hardware design and the powerful parallel computing capacity, the proposed framework can support online (or near real-time) engineering surface filtration for micro- and nano-scale surface metrology through exploring a novel parallel method named LBB model, the improved algorithms of lifting scheme and three implementation optimizations on the heterogeneous multi-GPU systems. The innovative approach enables optimizations on individual GPU node through an overarching framework that is capable of data-oriented dynamic load balancing (DLB) driven by a fuzzy neural network (FNN). The paper concludes with a case study on filtering and extracting manufactured surface topographical characteristics from real surfaces. The experimental results have demonstrated substantial improvements on the GPCF-LWT implementation in terms of computational efficiency, operational robustness, and task generalization.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rcim.2019.05.008,Journal,Robotics and Computer-Integrated Manufacturing,scopus,2019-12-01,sciencedirect,A real-time human-robot interaction framework with robust background invariant hand gesture detection,https://api.elsevier.com/content/abstract/scopus_id/85066259834,"In the light of factories of the future, to ensure productive and safe interaction between robot and human coworkers, it is imperative that the robot extracts the essential information of the coworker. We address this by designing a reliable framework for real-time safe human-robot collaboration, using static hand gestures and 3D skeleton extraction. OpenPose library is integrated with Microsoft Kinect V2, to obtain a 3D estimation of the human skeleton. With the help of 10 volunteers, we recorded an image dataset of alpha-numeric static hand gestures, taken from the American Sign Language. We named our dataset OpenSign and released it to the community for benchmarking. Inception V3 convolutional neural network is adapted and trained to detect the hand gestures. To augment the data for training the hand gesture detector, we use OpenPose to localize the hands in the dataset images and segment the backgrounds of hand images, by exploiting the Kinect V2 depth map. Then, the backgrounds are substituted with random patterns and indoor architecture templates. Fine-tuning of Inception V3 is performed in three phases, to achieve validation accuracy of 99.1% and test accuracy of 98.9%. An asynchronous integration of image acquisition and hand gesture detection is performed to ensure real-time detection of hand gestures. Finally, the proposed framework is integrated in our physical human-robot interaction library OpenPHRI. This integration complements OpenPHRI by providing successful implementation of the ISO/TS 15066 safety standards for “safety rated monitored stop” and “speed and separation monitoring” collaborative modes. We validate the performance of the proposed framework through a complete teaching by demonstration experiment with a robotic manipulator.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.isatra.2018.12.025,Journal,ISA Transactions,scopus,2019-12-01,sciencedirect,Deep residual learning-based fault diagnosis method for rotating machinery,https://api.elsevier.com/content/abstract/scopus_id/85059116434,"Effective fault diagnosis of rotating machinery has always been an important issue in real industries. In the recent years, data-driven fault diagnosis methods such as neural networks have been receiving increasing attention due to their great merits of high diagnosis accuracy and easy implementation. However, it is mostly difficult to fully train a deep neural network since gradients in optimization may vanish or explode during back-propagation, which results in deterioration and noticeable variance in model performance. In fault diagnosis researches, larger data sequence of machinery vibration signal containing sufficient information is usually preferred and consequently, deep models with large capacity are generally adopted. In order to improve network training, a residual learning algorithm is proposed in this paper. The proposed architecture significantly improves the information flow throughout the network, which is well suited for processing machinery vibration signal with variable sequential length. Little prior expertise on fault diagnosis and signal processing is required, that facilitates industrial applications of the proposed method. Experiments on a popular rolling bearing dataset are implemented to validate the proposed method. The results of this study suggest that the proposed intelligent fault diagnosis method for rotating machinery offers a new and promising approach.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.inffus.2018.11.020,Journal,Information Fusion,scopus,2019-12-01,sciencedirect,Data fusion based coverage optimization in heterogeneous sensor networks: A survey,https://api.elsevier.com/content/abstract/scopus_id/85059069923,"Sensor networks, as a promising network paradigm, have been widely applied in a great deal of critical real-world applications. A key challenge in sensor networks is how to improve and optimize coverage quality which is a fundamental metric to characterize how well a point or a region or a barrier can be sensed by the geographically deployed heterogeneous sensors. Because of the resource-limited, battery-powered and type-diverse features of the sensors, maintaining and optimizing coverage quality includes a significant amount of challenges in heterogeneous sensor networks. Many researchers from both academic and industrial communities have performed numerous significant works on coverage optimization problem in the past decades. Some of them also have surveyed the current models, theories and solutions on the problem of coverage optimization. However, most of the existing surveys and analytical studies ignore how to exploit data fusion and cooperation of the deployed sensors to enhance coverage performance. In this paper, we provide an insightful and comprehensive summarization and classification on the data fusion based coverage optimization problem and techniques. Aiming at overcoming the shortcomings existed in current solutions, we also discuss the future issues and challenges in this area and sketch a general research framework in the context of reinforcement learning.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2019.05.052,Journal,Expert Systems with Applications,scopus,2019-11-30,sciencedirect,Unsupervised collective-based framework for dynamic retraining of supervised real-time spam tweets detection model,https://api.elsevier.com/content/abstract/scopus_id/85067174995,"Twitter is one of the most popular social platforms. It has changed the way of communication and information dissemination through its real-time messaging mechanism. Recently, it has been used by researchers and industries as a new source of data for various intelligent systems, such as tweet sentiment analysis and recommendation systems, which require high data quality. However, due to its flexibility and popularity, Twitter has become the main target for spamming activities such as phishing legitimate users or spreading malicious software, which introduces new security issues and waste resources. Therefore, researchers have developed various machine-learning algorithms to reveal Twitter spam. However, as spammers have become smarter and more crafty, the characteristics of the spam tweets are varying over time making these methods inefficient to detect new spammers tricks and strategies. In addition, some of the employed methods (e.g. blacklisting) or spammer features (e.g. graph-based features) are extremely time-consuming, which hinders the ability to detect spammer activities in real-time. In this paper, we introduce a framework to deal with the volatility of the spam contents and new spamming patterns, called the spam drift. The framework combines the strength of unsupervised machine learning approach, which learns from unlabeled tweets, to retrain a real-time supervised tweet-level spam detection model in a batch mode. A set of experiments on a large-scale data set show the effectiveness of the proposed online unsupervised method in adaptively discovers and learns the patterns of new spam activities and achieve stable recall values reaching more than 95%. Although the average spam precision of our method is around 60%, the high spam recall values show the ability of our proposed method in reducing spam drift problems compared to traditional machine learning algorithms.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jclepro.2019.117870,Journal,Journal of Cleaner Production,scopus,2019-11-20,sciencedirect,"Digestate evaporation treatment in biogas plants: A techno-economic assessment by Monte Carlo, neural networks and decision trees",https://api.elsevier.com/content/abstract/scopus_id/85070258305,"Biogas production is one of the most promising pathways toward fully utilizing green energy within a circular economy. The anaerobic digestion process is the industry standard technology for biogas production due to its lowered energy consumption and its reliance on microbiology. Even in such an environmental-friendly process, liquid digestate is still produced from the remains of digested bio-feedstock and will require treatment. With unsuitable treatment procedure for liquid digestate, the mass of bio-feedstock can potentially escape the circular supply chain within the economy. This paper recommends the implementation of evaporator systems to provide a sustainable liquid digestate treating mechanism within the economy. Studied evaporator systems are represented by vacuum evaporation in combination with ammonia scrubber, stripping and reverse osmosis. Nevertheless, complex multi-dimensional decisions should be made by stakeholders before implementing such systems. Our work utilizes a novel techno-economics model to study the techno-economics robustness in implementing recent state-of-art vacuum evaporation systems with exploitation of waste heat from combined heat and power (CHP) units in biogas plants (BGP). To take into the account the stochasticity of the real world and robustness of the analysis, we used the Monte-Carlo simulation technique to generate more than 20,000 of different possibilities for the implementation of the evaporation system. Favourable decision pathways are then selected using a novel methodology which utilizes the artificial neural network and a hyper-optimized decision tree classifier. Two pathways that give the highest probability of providing a fast payback period are identified. Descriptive statistics are also used to analyse the distributions of decision parameters that lead to success in implementing the evaporator system. The results highlighted that integration of evaporation system are favourable when transport costs and incentives for CHP units are large and while feed-in tariffs for electricity production and specific investment costs are low. The result of this work is expected to pave the way for BGP stakeholders and decision makers in implementing liquid digestate treating technologies within the currently existing infrastructure.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compchemeng.2019.05.037,Journal,Computers and Chemical Engineering,scopus,2019-11-02,sciencedirect,Modern day monitoring and control challenges outlined on an industrial-scale benchmark fermentation process,https://api.elsevier.com/content/abstract/scopus_id/85071606321,"This paper outlines real-world control challenges faced by modern-day biopharmaceutical facilities through the extension of a previously developed industrial-scale penicillin fermentation simulation (IndPenSim). The extensions include the addition of a simulated Raman spectroscopy device for the purpose of developing, evaluating and implementation of advanced and innovative control solutions applicable to biotechnology facilities. IndPenSim can be operated in fixed or operator controlled mode and generates all the available on-line, off-line and Raman spectra for each batch. The capabilities of IndPenSim were initially demonstrated through the implementation of a QbD methodology utilising the three stages of the PAT framework. Furthermore, IndPenSim evaluated a fault detection algorithm to detect process faults occurring on different batches recorded throughout a yearly campaign. The simulator and all data presented here are available to download at www.industrialpenicillinsimulation.com and acts as a benchmark for researchers to analyse, improve and optimise the current control strategy implemented on this facility. Additionally, a highly valuable data resource containing 100 batches with all available process and Raman spectroscopy measurements is freely available to download. This data is highly suitable for the development of big data analytics, machine learning (ML) or artificial intelligence (AI) algorithms applicable to the biopharmaceutical industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2019.106031,Journal,Computers and Industrial Engineering,scopus,2019-11-01,sciencedirect,Machine learning based concept drift detection for predictive maintenance,https://api.elsevier.com/content/abstract/scopus_id/85071975175,"In this work we present a machine learning based approach for detecting drifting behavior – so-called concept drifts – in continuous data streams. The motivation for this contribution originates from the currently intensively investigated topic Predictive Maintenance (PdM), which refers to a proactive way of triggering servicing actions for industrial machinery. The aim of this maintenance strategy is to identify wear and tear, and consequent malfunctioning by analyzing condition monitoring data, recorded by sensor equipped machinery, in real-time. Recent developments in this area have shown potential to save time and material by preventing breakdowns and improving the overall predictability of industrial processes. However, due to the lack of high quality monitoring data and only little experience concerning the applicability of analysis methods, real-world implementations of Predictive Maintenance are still rare. Within this contribution, we present a method, to detect concept drift in data streams as potential indication for defective system behavior and depict initial tests on synthetic data sets. Further on, we present a real-world case study with industrial radial fans and discuss promising results gained from applying the detailed approach in this scope.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.enconman.2019.111932,Journal,Energy Conversion and Management,scopus,2019-11-01,sciencedirect,Cultural coyote optimization algorithm applied to a heavy duty gas turbine operation,https://api.elsevier.com/content/abstract/scopus_id/85070893013,"In the past decades, the quantity of researches regarding industrial gas turbines (GT) has increased exponentially in terms of number of publications and diversity of applications. The GTs offer high power output along with a high combined cycle efficiency and high fuel flexibility. As consequence, the energy efficiency, the pressure oscillations, the pollutant emissions and the fault diagnosis have become some of the recent concerns related to this type of equipment. In order to solve these GTs related problems and many other real-world engineering and industry 4.0 issues, a set of new technological approaches have been tested, such as the combination of Artificial Neural Networks (ANN) and metaheuristics for global optimization. In this paper, the recently proposed metaheuristic denoted Coyote Optimization Algorithm (COA) is applied to the operation optimization of a heavy duty gas turbine placed in Brazil and used in power generation. The global goal is to find the best valves setup to reduce the fuel consumption while coping with environmental and physical constraints from its operation. In order to treat it as an optimization problem, an integrated simulation model is implemented from original data-driven models and others previously proposed in literature. Moreover, a new version of the COA that links some concepts from Cultural Algorithms (CA) is proposed, which is validated under a set of benchmarks functions from the Institute of Electrical and Electronics Engineers (IEEE) Congress on Evolutionary Computation (CEC) 2017 and tested to the GT problem. The results show that the proposed Cultural Coyote Optimization Algorithm (CCOA) outperforms its counterpart for benchmark functions. Further, non-parametric statistical significance tests prove that the CCOA’s performance is competitive when compared to other state-of-the-art metaheuristics after a set of experiments for five case studies. In addition, the convergence analysis shows that the cultural mechanism employed in the CCOA has improved the COA balance between exploration and exploitation. As a result, the CCOA can improve the current GT operation significantly, reducing the fuel consumption up to 
                        
                           3.6
                           %
                        
                      meanwhile all constraints are accomplished.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engfracmech.2019.106642,Journal,Engineering Fracture Mechanics,scopus,2019-10-01,sciencedirect,Necking-induced fracture prediction using an artificial neural network trained on virtual test data,https://api.elsevier.com/content/abstract/scopus_id/85071523401,"The imperfection-based necking model by Marciniak and Kuczyński (MK) is frequently used for predicting the onset of localized necking under proportional and non-proportional loading, which can be considered a lower limit for the occurrence of fracture in a vehicle body structure subjected to crash loading. A large number of virtual imperfection lines at different orientation angles have to be analysed simultaneously in order to find the critical imperfection causing necking under arbitrary loading. This, and the continuous computation of a “distance to necking” quantity, representing a crucial output quantity for the simulation engineer, makes the model computationally expensive and limits industrial use in full-scale vehicle crash simulations.
                  In this work, an extended MK model is used for creating a virtual test data base under proportional and non-proportional loading for training of a computationally more efficient simple feed-forward neural network (NN). Both models are implemented in a User Material routine of an explicit crash code, where the predictions of the NN are in good agreement with the predictions of the MK reference model, however at a significantly reduced computational cost. Besides a pure numerical validation study, an experimental validation study has been performed, imposing biaxial tension loading followed by plane strain tension loading until necking using a special punch test apparatus. Whereas MK and NN are in good agreement with the experimental observations, the agreement of classical necking models, applied in conjunction with a linear damage accumulation (forming severity) concept was less accurate.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ibiod.2019.104744,Journal,International Biodeterioration and Biodegradation,scopus,2019-10-01,sciencedirect,Comparative evaluation of Pseudomonas species in single chamber microbial fuel cell with manganese coated cathode for reactive azo dye removal,https://api.elsevier.com/content/abstract/scopus_id/85069842075,"Microbial fuel cell (MFCs), distinguished by different strains of Pseudomonas species; Pseudomonas aeruginosa (MPEM-MFC I) and Pseudomonas fluorescens (MPEM-MFC II), was analyzed. Results have shown that, over a period of 360 h in the presence of 0.5 mM of model dye, MPEM MFC I produced the maximum power density of 2887 ± 13 μW m−2 (RO-16) and 1906 ± 7 μW m−2 (RB-5) compared with MPEM-MFC II with 1896 ± 15 μW m−2 (RO-16) and 1028 ± 9 μW m−2 (RB-5). Decolorization efficiency of MPEM-MFC I was 98 ± 1.2% (RO-16) and 95 ± 2% (RB-5). Total phenazine production in MPEM-MFC I was 12.3 ± 0.5 μg mL−1 higher than that of 8.9 ± 0.05 μg mL−1 (MPEM-MFC II) and its production have positive influence of electron shuttling that brought out high power output. Addition of phenazine externally reduced the dye degradation. Bioadhesion capability of P. aeruginosa on the anode reduced the internal resistance in MFCs. Thus the implementation of MFC is a most promising technology for the complete decolorization of reactive azo dyes and it has potential economic benefits in real-life industrial application.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.future.2019.04.014,Journal,Future Generation Computer Systems,scopus,2019-10-01,sciencedirect,TIDE: Time-relevant deep reinforcement learning for routing optimization,https://api.elsevier.com/content/abstract/scopus_id/85065443852,"Routing optimization has been researched in network design for a long time, and plenty of optimization schemes have been proposed from both academia and industry. However, such schemes are either too complicated in applications or far from the optimal performance. In recent years, with the development of Software-defined Networking (SDN) and Artificial Intelligence (AI), AI-based methods of routing strategy are being considered. In this paper, we propose TIDE, an intelligent network control architecture based on deep reinforcement learning that can dynamically optimize routing strategies in an SDN network without human experience. TIDE is implemented and validated on a real network environment. Experiment result shows that TIDE can adjust the routing strategy dynamically according to the network condition and can improve the overall network transmitting delay by about 9% compared with traditional algorithms.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.enbuild.2019.07.029,Journal,Energy and Buildings,scopus,2019-09-15,sciencedirect,Whole building energy model for HVAC optimal control: A practical framework based on deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85069552761,"Whole building energy model (BEM) is a physics-based modeling method for building energy simulation. It has been widely used in the building industry for code compliance, building design optimization, retrofit analysis, and other uses. Recent research also indicates its strong potential for the control of heating, ventilation and air-conditioning (HVAC) systems. However, its high-order nature and slow computational speed limit its practical application in real-time HVAC optimal control. Therefore, this study proposes a practical control framework (named BEM-DRL) that is based on deep reinforcement learning. The framework is implemented and assessed in a novel radiant heating system in an existing office building as a case study. The complete implementation process is presented in this study, including: building energy modeling for the novel heating system, multi-objective BEM calibration using the Bayesian method and the Genetic Algorithm, deep reinforcement learning training and simulation results evaluation, and control deployment. By analyzing the real-life control deployment data, it is found that BEM-DRL achieves 16.7% heating demand reduction with more than 95% probability compared to the old rule-based control. However, the framework still faces the practical challenges including building energy modeling of novel HVAC systems and multi-objective model calibration. Systematic study is also needed for the design of deep reinforcement learning training to provide a guideline for practitioners.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jnca.2019.06.003,Journal,Journal of Network and Computer Applications,scopus,2019-09-15,sciencedirect,MAPLE: A Machine Learning Approach for Efficient Placement and Adjustment of Virtual Network Functions,https://api.elsevier.com/content/abstract/scopus_id/85067443855,"As one of the many advantages of cloud computing, Network Function Virtualization (NFV) has revolutionized the network and telecommunication industry through enabling the migration of network functions from expensive dedicated hardware to software-defined components that run in the form of Virtual Network Functions (VNFs). However, with NFV comes numerous challenges related mainly to the complexity of deploying and adjusting VNFs in the physical networks, owing to the huge number of nodes and links in today's datacenters, and the inter-dependency among VNFs forming a certain network service. Several contributions have been made in an attempt to answer these challenges, where most of the existing solutions focus on the static placement of VNFs and overlook the dynamic aspect of the problem, which arises mainly due to the ever-changing resource availability in the cloud datacenters and the continuous mobility of the users. Few attempts have been lately made to incorporate the dynamic aspect to the VNF deployment solutions. The main problem of these approaches lies in their reactive readjustment scheme which determines the placement/migration strategy upon the receipt of a new request or the happening of a certain event, thus resulting in high setup latencies. In this paper, we take advantage of machine learning to reduce the complexity of the placement and readjustment processes through designing a cluster-based proactive solution. The solution consists of (1) an Integer Linear Programming (ILP) model that considers a tradeoff between the minimization of the latency, Service-Level Objective (SLO) violation cost, hardware utilization, and VNF readjustment cost, (2) an optimized k-medoids clustering approach which proactively partitions the substrate network into a set of disjoint on-demand clusters and (3) data-driven cluster-based placement and readjustment algorithms that capitalize on machine learning to intelligently eliminate some cost functions from the optimization problem to boost its feasibility in large-scale networks. Simulation results show that the proposed solution considerably reduces the readjustment time and decrease the hardware utilization compared to the K-means, original k-medoids and migration without clustering approaches.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2019.11.102,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Sustainable operations management for industry 4.0 and its social return,https://api.elsevier.com/content/abstract/scopus_id/85078948022,"In today’s industrial environment, where concepts of smart factories are consolidating their application in companies, it is still necessary to approach management decision making from a perspective that encompasses all aspects of sustainability without losing sight of the social return to which they must contribute. In order to obtain a reliable prediction, of the operation of a Sustainable Manufacturing System (SMS) and its Social Return (SR), this paper develops a methodology and procedures that allow predicting the system performance as a whole. This will allow us to assist management decision making in industries 4.0, supported by multi-criteria methods in knowledge management, simulation, value analysis and operational research by means of:
                  a) Study the economic, social and environmental impacts in the organization and management of the efficient operation of an SMS with the selection of strategies and alternatives in production chains to minimize and / or mitigate environmental and labor risks.
                  b) Encourage of industrial symbiosis or eco-industries networks that create opportunities increasing eco-efficiency and the positive social return of production systems.
                  This proposed methodology will facilitate changes in the structure of production systems in order to implement industry 4.0 paradigms through facilitator technologies such as simulation and virtual reality. This framework will allow Small and Medium Enterprises (SMEs) and other companies to address the decision-making activities that improve the economic-functional efficiency, which will lead to reduce the environmental impact and increase the positive social return of certain production strategies, considering working conditions.
                  The proposed approach went validated, in the area of the Euroregion Galicia North of Portugal, to favour the implementation of the decision-making through the Industry 4.0 Technologies.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2019.11.172,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Machine learning framework for predictive maintenance in milling,https://api.elsevier.com/content/abstract/scopus_id/85078904429,"In the Industry 4.0 era, artificial intelligence is transforming the manufacturing industry. With the advent of Internet of Things (IoT) and machine learning methods, manufacturing systems are able to monitor physical processes and make smart decisions through realtime communication and cooperation with humans, machines, sensors, and so forth. Artificial intelligence enables manufacturers to reduce equipment downtime, spot production defects, improve the supply chain, and shorten design times by using machine learning technologies which learn from experiences. One of the last application of these technologies is the development of Predictive Maintenance systems. Predictive maintenance combines Industrial IoT technologies with machine learning to forecast the exact time in which manufacturing equipment will need maintenance, allowing problems to be solved and adaptive decisions to be made in a timely fashion. This study will discuss the implementation of a milling Cutting-tool Predictive Maintenance solution (including Wear Monitoring), applied to a real milling data set as validation of the framework. More generally, this work provides a basic framework for creating a tool to monitor the wear level, preventing the breakdown, of a generic manufacturing tool, in order to improve human-machine interaction and optimize the production process.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2019.11.385,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,Towards a data-driven predictive-reactive production scheduling approach based on inventory availability,https://api.elsevier.com/content/abstract/scopus_id/85078884096,"To survive in a competitive business environment, manufacturing systems require the proper deployment of advanced technologies coming from Industry 4.0. These technologies allow access to quasi-real-time data that provide a continuously updated picture of the production system, including the state of available inventory. Data-driven predictive-reactive production scheduling has the potential to support the anticipation and prompt reaction to overcome different kinds of disruptions that occur in production execution nowadays. This research paper aims to propose a conceptual model for a data-driven predictive-reactive production scheduling approach combining machine learning and simulation-based optimization, considering current inventory of raw material, work in process and final products inventory to characterize a job-shop production execution state. The approach supports decision-making in dynamic situations related to inventory availability that can affect production schedules.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2019.11.465,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-09-01,sciencedirect,"Integration of automatic generated simulation models, machine control projects and management tools to support whole life cycle of industrial digital twins",https://api.elsevier.com/content/abstract/scopus_id/85078871061,"The paper presents a framework of automatic generation of industrial digital twins. These digital twins will be suitable to support preliminary design phases of systems development, but also to support next phases of detailed designs implementation and systems running phases. These digital twin allow, from the preliminary designing phase, to generate a complete simulation of the target industrial system. But, at the same time, and without the need to develop and add any subsequent code, they should be a valuable support for the phases and tasks of exploitation: maintenance, machine or system learning, etc. The problem is that the requirements for first development phases are much more generic than those for later phases. For this reason, instead of incorporating specificities in the simulation system, the framework takes advantage of the applications which are being developed for the implementation of the real system. In these applications (the control program and the decisions and the high level management system), the specificities have had to be taken into account. The system has been specialized in industrial transportation and warehouse systems which, although have a finite number or building objects, they have an infinite set of final configurations, very different one from each other. The paper presents an evaluation of current simulation platforms suitable to be used as part of the framework, and the digital twin industrial system generation framework itself. An example of application is as well presented.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.trip.2019.100028,Journal,Transportation Research Interdisciplinary Perspectives,scopus,2019-09-01,sciencedirect,Translation software: An alternative to transit data standards,https://api.elsevier.com/content/abstract/scopus_id/85075931935,"Data standardization is recognized in many disciplines as a critical aspect of data stewardship. Establishing and implementing data specifications increases the usefulness of data collection efforts and facilitates analysis techniques. With the advent of large quantities of machine-generated data, the use of standardized data formats feeds opportunities for visualization and advanced applications with machine-learning and Artificial Intelligence (AI). The transportation industry made substantial progress with data format specifications in the late 1990s, primarily for highway traffic. Unfortunately, establishing data standards has been an on-going challenge for the transit community. Archived Intelligent Transportation Systems (ITS) transit data (e.g., Automatic Vehicle Location (AVL), Automatic Passenger Counters (APCs), Automatic Fare Card (AFC)) still lack industry standards for data formats. Recent advancements in electronic transit scheduling (e.g., General Transit Feed Specifications (GTFS)) met a portion of this challenge with Open Data specifications. Now GTFS provides transit riders with agile information on services available at any location where the data is provided to developers of mobile device application (apps). Due to system and vendor limitations, the Metropolitan Transportation Authority (MTA), serving the New York City region, publishes its real-time subway system data in GTFS-R and its bus data in SIRI. This research develops an Application Programming Interface (API) to translate GTFS-R into SIRI to overcome the lack of standards making it possible to harmonize the subway and bus systems for the New York region. This solution offers the opportunity to develop a novel set of analytical tools, including pseudo-surveillance data for performance metrics.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jngse.2019.102933,Journal,Journal of Natural Gas Science and Engineering,scopus,2019-09-01,sciencedirect,Machine learning for surveillance of fluid leakage from reservoir using only injection rates and bottomhole pressures,https://api.elsevier.com/content/abstract/scopus_id/85068973220,"Carbon-neutral economies would require preventing the release of industrial-scale CO2 into the atmosphere by injecting into geologic formations. Large-scale injection of CO2 into deep reservoirs carries a potential for its undesired leakage into above zones, which can act as an obstacle to its large-scale implementation. Current methods for surveillance of CO2 leaks are costly and not very robust, especially the methods that simulate expected pressure behavior based on an assumed reservoir model.
                  This study proposes a machine learning method for surveillance of fluid leakage using deconvolution response function (a non-linear function of time varying bottomhole pressure and injection rates) from injection and monitoring wells as a measure of leakage that is simulated via multivariate linear regression of all the wells present in the reservoir. Leakage is detected by comparing “expected” (baseline without leaks) deconvolution response of all monitoring wells with their “observed” deconvolution response. Three key advantages of the proposed method are that it i) uses only injection rates and bottomhole pressure data (with no reservoir or geological model), ii) is independent of physical process parameterization uncertainties, and iii) applicable to both conventional and unconventional (e.g. fractured tight formations) reservoirs with any fluid (e.g. compressible, incompressible). The proposed method is first trained to learn well history with no leakage, followed by its validation after which it can be used to detect leakage by tracking a meaningful deviation error (at least twenty times the error of no leakage base scenario over same time period) between expected well response and observed well response at all monitoring wells. The well history required for the proposed method comes directly from measurements made at wells in a real field, but in absence of field data the proposed method is illustrated through well history simulated by reservoir simulations; no such numerical simulations are required for application of this method in a real world scenario with well measurements.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jss.2019.05.026,Journal,Journal of Systems and Software,scopus,2019-09-01,sciencedirect,Sentiment based approval prediction for enhancement reports,https://api.elsevier.com/content/abstract/scopus_id/85065795226,"The maintenance and evolution of the software application is a continuous phase in the industry. Users are frequently proposing enhancement requests for further functionalities. However, although only a small part of these requests are finally adopted, developers have to go through all of such requests manually, which is tedious and time consuming. To this end, in this paper we propose a sentiment based approach to predict how likely enhancement reports would be approved or rejected so that developers can first handle likely-to-be-approved requests. This could help the software applications to compete in the industry by upgrading their features in time as per user’s requirements. First, we preprocess enhancement reports using natural language preprocessing techniques. Second, we identify the words having positive and negative sentiments in the summary attribute of the enhancements reports and calculate the sentiment of each enhancement report. Finally, with the history data of real software application, we train a machine learning based classifier to predict whether a given enhancement report would be approved. The proposed approach has been evaluated with the history data from real software applications. The cross-application validation suggests that the proposed approach outperforms the state-of-the-art. The evaluation results suggest that the proposed approach increases the accuracy from 70.94% to 77.90% and improves the F-measure significantly from 48.50% to 74.53%.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compind.2019.04.016,Journal,Computers in Industry,scopus,2019-09-01,sciencedirect,A comparison of fog and cloud computing cyber-physical interfaces for Industry 4.0 real-time embedded machine learning engineering applications,https://api.elsevier.com/content/abstract/scopus_id/85065718296,"Industrial cyber-physical systems are the primary enabling technology for Industry 4.0, which combine legacy industrial and control engineering, with emerging technology paradigms (e.g. big data, internet-of-things, artificial intelligence, and machine learning), to derive self-aware and self-configuring factories capable of delivering major production innovations. However, the technologies and architectures needed to connect and extend physical factory operations to the cyber world have not been fully resolved. Although cloud computing and service-oriented architectures demonstrate strong adoption, such implementations are commonly produced using information technology perspectives, which can overlook engineering, control and Industry 4.0 design concerns relating to real-time performance, reliability or resilience. Hence, this research compares the latency and reliability performance of cyber-physical interfaces implemented using traditional cloud computing (i.e. centralised), and emerging fog computing (i.e. decentralised) paradigms, to deliver real-time embedded machine learning engineering applications for Industry 4.0. The findings highlight that despite the cloud’s highly scalable processing capacity, the fog’s decentralised, localised and autonomous topology may provide greater consistency, reliability, privacy and security for Industry 4.0 engineering applications, with the difference in observed maximum latency ranging from 67.7%–99.4%. In addition, communication failures rates highlighted differences in both consistency and reliability, with the fog interface successfully responding to 900,000 communication requests (i.e. 0% failure rate), and the cloud interface recording failure rates of 0.11%, 1.42%, and 6.6% under varying levels of stress.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2019.03.011,Journal,Expert Systems with Applications,scopus,2019-08-15,sciencedirect,Constraint learning based gradient boosting trees,https://api.elsevier.com/content/abstract/scopus_id/85063576343,"Predictive regression models aim to find the most accurate solution to a given problem, often without any constraints related to the model’s predicted values. Such constraints have been used in prior research where they have been applied to a subpopulation within the training dataset which is of greater interest and importance. In this research we introduce a new setting of regression problems, in which each instance can be assigned a different constraint, defined based on the value of the target (predicted) attribute. The new use of constraints is taken into account and incorporated into the learning process, and is also considered when evaluating the induced model. We propose two algorithms which are modifications to the regression boosting method. There are two advantages of the proposed algorithms: they are not dependent on the base learner used during the learning process, and they can be adopted by any boosting technique. We implemented the algorithms by modifying the gradient boosting trees (GBT) model, and we also introduced two measures for evaluating the models that were trained to solve the constraint problems. We compared the proposed algorithms to three baseline algorithms using four real-life datasets. Due to the algorithms’ focus on satisfying the constraints, in most cases the results showed significant improvement in the constraint-related measures, with just a minimal effect on the general prediction error. The main impact of the proposed approach is in its ability to derive a model with a higher level of assurance for specific cases of interest (i.e., the constrained cases). This is extremely important and has great significance in various use cases and expert and intelligent systems, particularly critical systems, such as critical healthcare systems (e.g., when predicting blood pressure or blood sugar level), safety systems (e.g., when aiming to estimate the distance of cars or airplanes from other objects), or critical industrial systems (e.g., require to estimate their usability along time). In each of these cases, there is a subpopulation of all instances that is of greater interest to the expert or system, and the sensitivity of the model’s error changes according to the real value of the predicted feature. For example, for a subpopulation of patients (e.g., patients under the age of eight, or patients known to be at risk), physicians often require a sensitive model that accurately predicts blood pressure values.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.mfglet.2019.08.003,Journal,Manufacturing Letters,scopus,2019-08-01,sciencedirect,A self-aware and active-guiding training &amp; assistant system for worker-centered intelligent manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85070738030,"Training and on-site assistance is critical to help workers master required skills, improve worker productivity, and guarantee the product quality. Traditional training methods lack worker-centered considerations that are particularly in need when workers are facing ever-changing demands. In this study, we propose a worker-centered training & assistant system for intelligent manufacturing, which is featured with self-awareness and active-guidance. Multi-modal sensing techniques are applied to perceive each individual worker and a deep learning approach is developed to understand the worker’s behavior and intention. Moreover, an object detection algorithm is implemented to identify the parts/tools the worker is interacting with. Then the worker’s current state is inferred and used for quantifying and assessing the worker performance, from which the worker’s potential guidance demands are analyzed. Furthermore, onsite guidance with multi-modal augmented reality is provided actively and continuously during the operational process. Two case studies are used to demonstrate the feasibility and great potential of our proposed approach and system for applying to the manufacturing industry for frontline workers.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compind.2019.04.010,Journal,Computers in Industry,scopus,2019-08-01,sciencedirect,Managing workflow of customer requirements using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85065732680,"Customer requirements – product specifications issued by the customer – organize the dialog between suppliers and customers and, hence, affect the dynamics of supply networks. These large and complex documents are frequently updated over time, while changes are seldom marked by the customers who issue the requirements. The lack of structure and defined responsibilities, thus, demands an expert to manually process the requirements. Here, the possibility to improve the usual workflow with machine learning algorithms is explored.
                  The whole requirements management process has two major bottlenecks, which can be automatized. The first one, detecting changes, can be accomplished via a document comparison tool. The second one, recognizing the responsibilities and assigning them to the right department, can be solved with standard machine learning algorithms. Here, such algorithms are applied to a dataset obtained from a global automotive industry supplier.
                  The proposed method improves the requirements management process by reducing an expert’s workload and thus decreasing the time for processing one document was reduced from 2 weeks to 1 h. Moreover, the method gives a high accuracy of department assignment and can self-improve once implemented into a requirements management system.
                  Although the machine learning methods are very popular nowadays, they are seldom used to improve business processes in real companies, especially in the case of processes that did not require digitalization in the past. Here we show, how such methods can solve some of the management problems and improve their workflow.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compind.2019.05.001,Journal,Computers in Industry,scopus,2019-08-01,sciencedirect,Industrial robot control and operator training using virtual reality interfaces,https://api.elsevier.com/content/abstract/scopus_id/85065132267,"Nowadays, we are involved in the fourth industrial revolution, commonly referred to as “Industry 4.0,” where cyber-physical systems and intelligent automation, including robotics, are the keys. Traditionally, the use of robots has been limited by safety and, in addition, some manufacturing tasks are too complex to be fully automated. Thus, human-robot collaborative applications, where robots are not isolated, are necessary in order to increase the productivity ensuring the safety of the operators with new perception systems for the robot and new interaction interfaces for the human. Moreover, virtual reality has been extended to the industry in the last years, but most of its applications are not related to robots. In this context, this paper works on the synergies between virtual reality and robotics, presenting the use of commercial gaming technologies to create a totally immersive environment based on virtual reality. This environment includes an interface connected to the robot controller, where the necessary mathematical models have been implemented for the control of the virtual robot. The proposed system can be used for training, simulation, and what is more innovative, for robot controlling in an integrated, non-expensive and unique application. Results show that the immersive experience increments the efficiency of the training and simulation processes, offering a cost-effective solution.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ece.2019.05.003,Journal,Education for Chemical Engineers,scopus,2019-07-01,sciencedirect,"Learning distillation by a combined experimental and simulation approach in a three steps laboratory: Vapor pressure, vapor-liquid equilibria and distillation column",https://api.elsevier.com/content/abstract/scopus_id/85066038830,"Distillation is one of the most important separation process in industrial chemistry. This operation is based on a deep knowledge of the fluid phase equilibria involved in the mixture to be separated. In particular, the most important aspects are the determination of the vapor pressures of the single compounds and the correct representation of the eventual not ideality of the mixture. Simulation science is a fundamental tool for managing these complex topics and chemical engineers students have to learn and to use it on real case-studies. To give to the students a complete overview of these complex aspects, a laboratory experience is proposed. Three different work stations were set up: i) determination of vapor pressure of two pure compounds; ii) the study of vapor-liquid equilibria of a binary mixture; iii) the use of a continuous multistage distillation column in dynamic and steady-state conditions. The simulation of all these activities by a commercial software, PRO II by AVEVA, allows to propose and verify the thermodynamic characteristics of the mixture and to correctly interpret the distillation column data. Moreover, the experimental plants and the data elaboration by classical equations are presented. The students are request to prepare a final report in which the description of the experimental plants and experimental procedure, the interpretation of the results and the simulation study are critically discussed in order to encourage them to reason and to acquire the concepts of the course.
                  Two different questionnaires each with 7 questions, for the course and for the laboratory, are proposed and analyzed. The final evaluation of the students was strongly positive both for the course as a whole and for the proposed laboratory activities.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.psep.2019.05.016,Journal,Process Safety and Environmental Protection,scopus,2019-07-01,sciencedirect,An intelligent fire detection approach through cameras based on computer vision methods,https://api.elsevier.com/content/abstract/scopus_id/85065893982,"Fire that is one of the most serious accidents in petroleum and chemical factories, may lead to considerable production losses, equipment damages and casualties. Traditional fire detection was done by operators through video cameras in petroleum and chemical facilities. However, it is an unrealistic job for the operator in a large chemical facility to find out the fire in time because there may be hundreds of video cameras installed and the operator may have multiple tasks during his/her shift. With the rapid development of computer vision, intelligent fire detection has received extensive attention from academia and industry. In this paper, we present a novel intelligent fire detection approach through video cameras for preventing fire hazards from going out of control in chemical factories and other high-fire-risk industries. The approach includes three steps: motion detection, fire detection and region classification. At first, moving objects are detected through cameras by a background subtraction method. Then the frame with moving objects is determined by a fire detection model which can output fire regions and their locations. Since false fire regions (some objects similar with fire) may be generated, a region classification model is used to identify whether it is a fire region or not. Once fire appears in any camera, the approach can detect it and output the coordinates of the fire region. Simultaneously, instant messages will be immediately sent to safety supervisors as a fire alarm. The approach can meet the needs of real-time fire detection on the precision and the speed. Its industrial deployment will help detect fire at the very early stage, facilitate the emergency management and therefore significantly contribute to loss prevention.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2019.04.054,Journal,Computers and Industrial Engineering,scopus,2019-07-01,sciencedirect,Agent-based modelling and heuristic approach for solving complex OEM flow-shop productions under customer disruptions,https://api.elsevier.com/content/abstract/scopus_id/85065083945,"The application of the agent-based simulation approach in the flow-shop production environment has recently gained popularity among researchers. The concept of agent and agent functions can help to automate a variety of difficult tasks and assist decision-making in flow-shop production. This is especially so in the large-scale Original Equipment Manufacturing (OEM) industry, which is associated with many uncertainties. Among these are uncertainties in customer demand requirements that create disruptions that impact production planning and scheduling, hence, making it difficult to satisfy demand in due time, in the right order delivery sequence, and in the right item quantities. It is however important to devise means of adapting to these inevitable disruptive problems by accommodating them while minimising the impact on production performance and customer satisfaction.
                  In this paper, an innovative embedded agent-based Production Disruption Inventory-Replenishment (PDIR) framework, which includes a novel adaptive heuristic algorithm and inventory replenishment strategy which is proposed to tackle the disruption problems. The capabilities and functionalities of agents are utilised to simulate the flow-shop production environment and aid learning and decision making. In practice, the proposed approach is implemented through a set of experiments conducted as a case study of an automobile parts facility for a real-life large-scale OEM. The results are presented in term of Key Performance Indicators (KPIs), such as the number of late/unsatisfied orders, to determine the effectiveness of the proposed approach. The results reveal a minimum number of late/unsatisfied orders, when compared with other approaches.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.est.2019.04.015,Journal,Journal of Energy Storage,scopus,2019-06-01,sciencedirect,Comparison of a physical and a data-driven model of a Packed Bed Regenerator for industrial applications,https://api.elsevier.com/content/abstract/scopus_id/85064907454,"Thermal Energy Storage systems are promising technologies to match intermittent heat supply with demand and improve the energy efficiency of industrial processes. To optimally integrate these energy storage systems in industry, reliable and industrially applicable models are required. This work examines two different modeling approaches for a Sensible Thermal Energy Storage device, namely a Packed Bed Regenerator. A physical 1D-model using finite difference methods and a data-driven grey box model using Recurrent Neural Networks are described. Experimental data from a Packed Bed Regenerator test rig is used to create the data-driven model and to compare the results of both models with real measurements. A quantitative and qualitative comparison of the data-driven and the physical model is conducted. The results of the quantitative investigation show, that both models are able to capture the complex behavior of the Packed Bed Regenerator. With the qualitative analysis, the features of the different models are highlighted and advantages and limitations are discussed. Thus, it provides an orientation in the decision-making process for the choice of an appropriate modeling approach. The findings of this work can support the creation of physical, as well as data-driven models of sensible energy storage systems and strengthen their implementation to industrial processes. The generic grey box modeling approach and the findings of the qualitative comparison of the models can be also applied to other modeling tasks.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.scitotenv.2019.02.213,Journal,Science of the Total Environment,scopus,2019-05-20,sciencedirect,Passive sampling of volatile organic compounds in industrial atmospheres: Uptake rate determinations and application,https://api.elsevier.com/content/abstract/scopus_id/85061829807,"This study describes the implementation of a passive sampling-based method followed by thermal desorption gas-chromatography-mass spectrometry (TD-GC–MS) for the monitoring of volatile organic compounds (VOCs) in industrial atmospheres. However, in order to employ passive sampling as a reliable sampling technique, a specific diffusive uptake rate is required for each compound. Accordingly, the aim of the present study was twofold. First, the experimental diffusive uptake rates of the target VOCs were determined under real industrial air conditions using Carbopack X thermal desorption tubes, and active sampling as reference method. The sampling campaigns carried out between October 2017 and May 2018 provided us of experimental diffusive uptake rates between 0.40 mL min−1 and 0.70 mL min−1 and stable over time (RSD % < 8%) for up to 41 VOCs. Secondly, the uptake rates obtained experimentally were applied for the determination of VOCs concentrations at 16 sampling sites in the North Industrial Complex of Tarragona. The results showed i-pentane, n-pentane and the compounds known as BTEX as the most representative ones. Moreover, some sporadic peaks of 1,3-butadiene, acrylonitrile, ethylbenzene and styrene resulting from certain industrial activities were detected.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2019.03.011,Journal,Engineering Applications of Artificial Intelligence,scopus,2019-05-01,sciencedirect,Distributed parallel deep learning of Hierarchical Extreme Learning Machine for multimode quality prediction with big process data,https://api.elsevier.com/content/abstract/scopus_id/85063385858,"In this work, the distributed and parallel Extreme Learning Machine (dp-ELM) and Hierarchical Extreme Learning Machine (dp-HELM) are proposed for multimode process quality prediction with big data. The efficient ELM algorithm is transformed into the distributed and parallel modeling form according to the MapReduce framework. Since the deep learning network structure of HELM is more accurate than the single layer of ELM in feature representation, the dp-HELM is further developed through decomposing the ELM-based Auto-encoders (ELM-AE) of deep hidden layers into a loop of MapReduce jobs. Additionally, the multimode issue is solved through the “divide and rule” strategy. The distributed and parallel K-means (dp-K-means) is utilized to divide the process modes, which are further trained in a synchronous parallel way by dp-ELM and dp-HELM. Finally, the Bayesian model fusion technique is utilized to integrate the local models for online prediction. The proposed algorithms are deployed on a Hadoop MapReduce computing cluster and the feasibility and efficiency are illustrated through building a real industrial quality prediction model with big process data.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jlp.2019.03.003,Journal,Journal of Loss Prevention in the Process Industries,scopus,2019-05-01,sciencedirect,A fuzzy expert system for mitigation of risks and effective control of gas pressure reduction stations with a real application,https://api.elsevier.com/content/abstract/scopus_id/85063113351,"Environmental changes and increased uncertainty due to technical damage, explosions and large fires have caused the risk of an inevitable element in the gas industry. This study purposes developing a new hybrid fuzzy expert system as a decision support system to mitigate the risk associated with gas transmission stations. The designed knowledge-based system combines the procedural and descriptive rules based on experts’ judgments to analyze the complex relationships between the different components of a gas pressure reduction station. The developed fuzzy expert system is coded in C language integrated production system (CLIPS) and is linked with MATLAB software for calling fuzzy functions. A real case study of gas pressure reduction stations in Iranian gas industry is conducted to validate the proposed expert system model. The expert system provides more than one thousand rules based on expert knowledge to prevent the pressure drop and the quality loss of gas or shutting off gas flow which accordingly increases gas flow stability. The proposed expert system could minimize the risk of hazardous scenarios, such as leakage and corrosion, in the gas industry and provide an acceptable precision in the provision of periodic control strategies and appropriate response under an emergency condition.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2019.02.019,Journal,Engineering Applications of Artificial Intelligence,scopus,2019-05-01,sciencedirect,A new hierarchical approach to requirement analysis of problems in automated planning,https://api.elsevier.com/content/abstract/scopus_id/85062901276,"The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijheatmasstransfer.2018.12.170,Journal,International Journal of Heat and Mass Transfer,scopus,2019-05-01,sciencedirect,Visualization-based nucleate boiling heat flux quantification using machine learning,https://api.elsevier.com/content/abstract/scopus_id/85059864859,"Processes involving complex phenomena are ubiquitous in nature and industry, many of which are difficult to simulate computationally. Nucleate boiling heat transfer, for instance, has numerous practical applications, while the film boiling is an undesirable operation regime. So far, most correlations and computer simulations to quantify boiling heat transfer rely on direct measurement of thermohydraulic data, such as heater temperature, which is often invasive. Here it is demonstrated that neural network-based models can quantify heat transfer using only direct and indirect visual information of the boiling phenomenon, without any prior knowledge of the governing equations, which enables the non-intrusive measurement of heat flux based on boiling process imaging. It is shown that neural networks can encode bubble morphology and its correlation with heat flux returning errors as low as 7% when compared with precise experimental measurements, a significant improvement over current prediction methods of boiling heat transfer. Furthermore, it is shown that these systems may be implemented in inexpensive, compact computers, such as the Raspberry Pi, to infer heat flux in real time from visualization.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.mfglet.2019.05.003,Journal,Manufacturing Letters,scopus,2019-04-01,sciencedirect,A blockchain enabled Cyber-Physical System architecture for Industry 4.0 manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85066168835,"Cyber-Physical Production Systems (CPPSs) are complex manufacturing systems which aim to integrate and synchronize machine world and manufacturing facility to the cyber computational space. However, having intensive interconnectivity and a computational platform is crucial for real-world implementation of CPPSs. In this paper, the potential impacts of blockchain technology in development and realization of real-world CPPSs are discussed. A unified three-level blockchain architecture is proposed as a guideline for researchers and industries to clearly identify the potentials of blockchain and adapt, develop, and incorporate this technology with their manufacturing developments towards Industry 4.0.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compag.2019.02.023,Journal,Computers and Electronics in Agriculture,scopus,2019-04-01,sciencedirect,Real-time nondestructive monitoring of Common Carp Fish freshness using robust vision-based intelligent modeling approaches,https://api.elsevier.com/content/abstract/scopus_id/85062029959,"In the current research, the potential of a novel method based on the artificial neural network was investigated to diagnose the freshness of common carp (Cyprinus carpio) during ice storage. Fish as an aquaculture product has high nutrients and low-fat content. So, people have consumed it as a safe and high-value foodstuff in their daily diet. Investigation of fish freshness is proposed as a significant issue in the aquaculture industry since fish spoils rapidly. The applied system of this study is comprised of the following steps: First, images of samples were captured and the pre-processing operation was done on the images. Then, particular channels including R, G, B, H, S, I, L*, a*, and b* were computed. Next, feature extraction was performed to obtain 6 types of texture features from each channel. Afterward, the hybrid Artificial Bee Colony-Artificial Neural Network (ABC-ANN) algorithm was applied to select the best features. Finally, the Support Vector Machine (SVM), K-Nearest Neighbor (K-NN) and Artificial Neural Network (ANN) algorithms as the most common methods were used to classify fish images. The best performance of the K-NN classifier was calculated in the k = 8 neighborhood size with the accuracy of 90.48. The best kernel function for the SVM algorithm was polynomial with C, sigma, and accuracy of 1, 2 and 91.52 percent, respectively. In this system, the input layer has consisted of 22 neurons based on the feature selection operation and 4 classes including most fresh, fresh, fairly fresh and spoiled have been used as the number of output layer. At the end, the best results of the MLP networks were achieved by LM learning algorithm and 6 neurons in the hidden layer with the 22–10–4 topology and accuracy of 93.01 percent. The achieved results demonstrate the high performance of the ANN classifier for evaluation of common carp freshness during ice storage as a rapid, accurate, non-destructive, real-time and automated method. It shows the potential of computer vision method in combination with artificial neural networks as an intelligent technique for evaluation of fish freshness.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compeleceng.2018.03.015,Journal,Computers and Electrical Engineering,scopus,2019-03-01,sciencedirect,BCI cinematics – A pre-release analyser for movies using H <inf>2</inf> O deep learning platform,https://api.elsevier.com/content/abstract/scopus_id/85046107707,"Entertainment industry has seen a phenomenal growth throughout the globe in recent times and movie industry enjoys a crucial role in the above emergence. A movie can capture the attention of a viewer and can trigger cognitive and emotional processes in the brain. In this article we assess the emotional outcome of the viewer while they watch the movie before its actual release that is, during its preview. Traditionally FMRI was used to assess the activity of brain but proved to be non-feasible and costly so we used EEG Sensors to monitor and record the functioning of the brain of movie viewer for further analysis. The collected data through EEG sensor were analysed using deep learning framework. H2O package of deep learning was employed to find high and low of different brain waves mapping to the emotions depicted in the every scene of the movie. Our proposed system named BCI cinematics obtained 85% accuracy and results were validated by obtaining the feedback from the stake holders. The outcome of this work will assist the creators to understand the emotional impact of movie over a normal viewer impartially thus enable them to modify certain scenes or change sequence of scenes and so on. When deployed in real time our system prove to be a cost saver for movie makers.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.future.2018.02.011,Journal,Future Generation Computer Systems,scopus,2019-03-01,sciencedirect,Collaborative prognostics in Social Asset Networks,https://api.elsevier.com/content/abstract/scopus_id/85042391186,"With the spread of Internet of Things (IoT) technologies, assets have acquired communication, processing and sensing capabilities. In response, the field of Asset Management has moved from fleet-wide failure models to individualised asset prognostics. Individualised models are seldom truly distributed, and often fail to capitalise the processing power of the asset fleet. This leads to hardly scalable machine learning centralised models that often must find a compromise between accuracy and computational power. In order to overcome this, we present a novel theoretical approach to collaborative prognostics within the Social Internet of Things. We introduce the concept of Social Asset Networks, defined as networks of cooperating assets with sensing, communicating and computing capabilities. In the proposed approach, the information obtained from the medium by means of sensors is synthesised into a Health Indicator, which determines the state of the asset. The Health Indicator of each asset evolves according to an equation determined by a triplet of parameters. Assets are given the form of the equation but they ignore their parametric values. To obtain these values, assets use the equation in order to perform a non-linear least squares fit of their Health Indicator data. Using these estimated parameters, they are interconnected to a subset of collaborating assets by means of a similarity metric. We show how by simply interchanging their estimates, networked assets are able to precisely determine their Health Indicator dynamics and reduce maintenance costs. This is done in real time, with no centralised library, and without the need for extensive historical data. We compare Social Asset Networks with the typical self-learning and fleet-wide approaches, and show that Social Asset Networks have a faster convergence and lower cost. This study serves as a conceptual proof for the potential of collaborative prognostics for solving maintenance problems, and can be used to justify the implementation of such a system in a real industrial fleet.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.enbuild.2018.12.034,Journal,Energy and Buildings,scopus,2019-02-15,sciencedirect,"IntelliMaV: A cloud computing measurement and verification 2.0 application for automated, near real-time energy savings quantification and performance deviation detection",https://api.elsevier.com/content/abstract/scopus_id/85059816255,"Energy conservation measures (ECMs) are implemented in all sectors with the objective of improving the efficiency with which energy is consumed. Measurement and verification (M&V) is required to verify the performance of every ECM to ensure its successful implementation and operation. The methodologies implemented to achieve this are currently evolving to a more dynamic state, known as measurement and verification 2.0, through the use of automated and advanced analytics. The primary barrier to the adoption of M&V 2.0 practices are the tools available to practitioners. This paper aims to populate the knowledge gap in the industrial buildings sector by presenting a novel cloud computing-based application, IntelliMaV, that applies advanced machine learning techniques on large datasets to automatically verify the performance of ECMs in near real-time. Additionally, a performance deviation detection system is incorporated, ensuring persistence of savings beyond the typical period of analysis in M&V.
                  IntelliMaV allows M&V practitioners to quantify energy savings with minimum levels of uncertainty by applying powerful analytics to data readily available in industrial facilities. The use of a cloud computing-based architecture reduces the resources required on-site and decreases the time required to train the baseline energy model through the use of parallel processing. The robust nature of the application ensures it is applicable across the broad spectrum of ECMs in the industrial buildings sector. A case study carried out in a large biomedical manufacturing facility demonstrates the ease of use of the application and the benefits realised through its adoption. The energy savings from an ECM were calculated to be 2,353,225 kWh/yr with 25.5% uncertainty at a 90% confidence interval.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.oceaneng.2019.01.003,Journal,Ocean Engineering,scopus,2019-02-01,sciencedirect,Data management for structural integrity assessment of offshore wind turbine support structures: data cleansing and missing data imputation,https://api.elsevier.com/content/abstract/scopus_id/85061324147,"Structural Health Monitoring (SHM) and Condition Monitoring (CM) Systems are currently utilised to collect data from offshore wind turbines (OWTs), to enhance the accurate estimation of their operational performance. However, industry accepted practices for effectively managing the information that these systems provide have not been widely established yet. This paper presents a four-step methodological framework for the effective data management of SHM systems of OWTs and illustrates its applicability in real-time continuous data collected from three operational units, with the aim of utilising more complete and accurate datasets for fatigue life assessment of support structures. Firstly, a time-efficient synchronisation method that enables the continuous monitoring of these systems is presented, followed by a novel approach to noise cleansing and the posterior missing data imputation (MDI). By the implementation of these techniques those data-points containing excessive noise are removed from the dataset (Step 2), advanced numerical tools are employed to regenerate missing data (Step 3) and fatigue is estimated for the results of these two methodologies (Step 4). Results show that after cleansing, missing data can be imputed with an average absolute error of 2.1%, while this error is kept within the [+ 15.2%−11.0%] range in 95% of cases. Furthermore, only 0.15% of the imputed data fell outside the noise thresholds. Fatigue is found to be underestimated both, when data cleansing does not take place and when it takes place but MDI does not. This makes this novel methodology an enhancement to conventional structural integrity assessment techniques that do not employ continuous datasets in their analyses.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.therap.2018.12.002,Journal,Therapie,scopus,2019-02-01,sciencedirect,"Early access to health products in France: Major advances of the French “Conseil stratégique des industries de santé” (CSIS) to be implemented (modalities, regulations, funding)",https://api.elsevier.com/content/abstract/scopus_id/85061149651,"In a context of perpetual evolution of treatments, access to therapeutic innovation is a major challenge for patients and the various players involved in the procedures of access to medicines. The revolutions in genomic and personalized medicine, artificial intelligence and biotechnology will transform the medicine of tomorrow and the organization of our health system. It is therefore fundamental that France prepares for these changes and supports the development of its companies in these new areas. The recent “Conseil stratégique des industries de santé” launched by Matignon makes it possible to propose a regulatory arsenal conducive to the implementation and diffusion of therapeutic innovations. In this workshop, we present a number of proposals, our approach having remained pragmatic with a permanent concern to be effective in the short term for the patients and to simplify the procedures as much as possible. This was achieved thanks to the participation in this workshop of most of the players involved (industrial companies, “Agence nationale de sécurité du médicament et des produits de santé”, “Haute Autorité de santé”, “Institut national du cancer”, “Les entreprises du médicament”, hospitals, “Observatoire du médicament, des dispositifs médicaux et de l’innovation thérapeutique”…). The main proposals tend to favor the implementation of clinical trials on our territory, especially the early phases, a wider access to innovations by favoring early access programs and setting up a process called “autorisation temporaire d’utilisation d’extension” (ATUext) that make it possible to prescribe a medicinal product even if the latter has a marketing authorisation in another indication. In addition, we propose a conditional reimbursement that will be available based on preliminary data but will require re-evaluation based on consolidated data from clinical trials and/or real-life data. Finally, in order to better carry out these assessments, with a view to access or care, we propose the establishment of partnership agreements with health agencies/hospitals in order to encourage the emergence of field experts, in order to prioritize an ascending expertise closer to patients’ needs and to real life.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2018.08.018,Journal,Computers and Industrial Engineering,scopus,2019-02-01,sciencedirect,Ensemble-based big data analytics of lithofacies for automatic development of petroleum reservoirs,https://api.elsevier.com/content/abstract/scopus_id/85052098750,"Big data-driven ensemble learning is explored in this paper for quantitative geological lithofacies modeling, which is an integral and challenging part of petroleum reservoir development and characterization. Quantitative lithofacies modeling involves detection and recognition of underlying subsurface rock’s lithofacies. It requires real-time data acquisition, handling, storage, conditioning, analysis, and interpretation of raw sensory petroleum logging data. The real-time well-logs data collected from the sensor-based tools suffer from complications such as noise, nonlinearity, imbalance, and high-dimensionality which makes the prediction task more challenging. The existing literature on quantitative lithofacies modeling includes several data-driven techniques ranging from conventional well-logs to artificial intelligence (AI). Recently, multiple classifiers based Ensemble learners have been found to be more robust and reliable paradigms for detection and identification tasks in various machine learning applications, however, these are not well embraced in the petroleum industry. Ensemble methodology combines diverse expert’s opinions to obtain overall ensemble decision which in turn reduces the risk of a wrong decision. Thus, the uncertainties associated with complex reservoir data can be better handled by the use of Ensemble learners than the existing single learner based conventional models. Ensemble-based big data analytics, proposed in the paper, includes development and comparative performance testing of five popular ensemble methods (viz. Bagging, AdaBoost, Rotation forest, Random subspace, and DECORATE) for quantitative lithofacies modeling. Seven state-of-the-art base classifiers were used as members of different Ensemble learners for the analysis of Kansas (U.S.A.) oil-field data. The proposed techniques have been implemented on the widely used WEKA platform. The comparative performance analysis of the proposed techniques, presented in the paper, confirms its supremacy over the existing techniques used for quantitative lithofacies modeling.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-817356-5.00012-7,Book,Internet of Things in Biomedical Engineering,scopus,2019-01-01,sciencedirect,Internet of Things Application in Life Sciences,https://api.elsevier.com/content/abstract/scopus_id/85124928251,"Sensors, smart devices, and automated systems have been used in life sciences industries for the benefit of patients and medical personnel for the diagnosis of disease, monitoring of patient conditions, treatment of chronic conditions, and manufacturing and distribution of drugs. The Internet of Things (IoT) has been implemented for connecting sensors and networked devices and collecting and analyzing the experimental data obtained from those sensors and actuators. IoT-based devices and microchips have been used to collect patient health history and records, to understand the functions of the internal organs, and to capture pictures and videos of the human body from the inside. They have proven to be of great benefit in research and to facilitate patient treatment and efficient drug manufacturing. In recent years, inventions such as smart wheelchairs for disabled people to move around more easily; wristbands for analyzing oxygen levels and monitoring heartbeat, temperature, and blood pressure; a pill-shaped camera to capture pictures; and the linking of these devices with smartphone apps have made it possible to monitor, study, and keep track of the conditions of patients and their daily life in real time. With its recent advances, the application of IoT in the life sciences fields has provided better tools and devices to diagnose diseases in their initial stages, to study the effectiveness of a drug in the patient’s body, and to invent and test new drugs efficiently in less time and for less money. The IoT enables remote monitoring of patients and products connecting these sensors and devices. This provides real-time information on patients, which reduces effort and cost and improves treatment outcomes and efficiency. Also, the data collected from these devices have provided information for study and research into the betterment of human health in the field of life sciences. With the application of machine learning and deep learning approaches, vital conclusions can be drawn from big data.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.matpr.2020.03.363,Conference Proceeding,Materials Today: Proceedings,scopus,2019-01-01,sciencedirect,Real-time Thermal Error Compensation Strategy for Precision Machine tools,https://api.elsevier.com/content/abstract/scopus_id/85085555603,"Present manufacturing trend is towards producing precision components with better accuracy. Machine errors like geometrical, thermal and process errors affect the component accuracy. Among these errors, thermal error contributes more than 50-60% of the total machining error. This paper mainly focuses on the development of a real-time thermal error compensation module for precision machine tools and talks about effective modeling of thermal errors, development of thermal error compensation model using feed-forward backpropagation neural network and also simplified model using regression analysis technique, algorithm development for real-time compensation and implementation of module onto the open architecture CNC controller. The developed module has been successfully tested on a Diamond Turning Machine (DTM) by machining the precision component and also verified the effectiveness of the module",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.01.033,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Deep learning-based production forecasting in manufacturing: A packaging equipment case study,https://api.elsevier.com/content/abstract/scopus_id/85083533827,"We propose a Deep Learning (DL)-based approach for production performance forecasting in fresh products packaging. On the one hand, this is a very demanding scenario where high throughput is mandatory; on the other, due to strict hygiene requirements, unexpected downtime caused by packaging machines can lead to huge product waste. Thus, our aim is predicting future values of key performance indexes such as Machine Mechanical Efficiency (MME) and Overall Equipment Effectiveness (OEE). We address this problem by leveraging DL-based approaches and historical production performance data related to measurements, warnings and alarms. Different architectures and prediction horizons are analyzed and compared to identify the most robust and effective solutions. We provide experimental results on a real industrial case, showing advantages with respect to current policies implemented by the industrial partner both in terms of forecasting accuracy and maintenance costs. The proposed architecture is shown to be effective on a real case study and it enables the development of predictive services in the area of Predictive Maintenance and Quality Monitoring for packaging equipment providers.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.01.031,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,A deep learning approach for anomaly detection with industrial time series data: A refrigerators manufacturing case study,https://api.elsevier.com/content/abstract/scopus_id/85083532061,"In refrigerators production, vacuum creation is fundamental to guarantee the correct manufacturing of the product. Before inserting the refrigerant in the refrigerator cabinet, the vacuum is tested through a Pirani gauge that assesses the pressure within the cabinet. Such readings are used to evaluate the vacuum creation process and to verify if leakings are present. In this work, we employ a Deep Learning-based Anomaly Detection approach to associate an Anomaly Score to each pressure profile; this score can be exploited to optimize actions performed by human operators like more detailed inspections or unit exclusion from the downstream production stages. We propose a native time series-based approach based on Deep Learning and compare it with classic ones based on hand-craft features. The proposed approach is designed to be deployed in a Decision Support System for assisting human operators in the following testing operations, helping them in reducing evaluation bias and attention losses that are inevitable in production line environment. Moreover, costs associated with false positives (normally operating units detected as anomalous) and false negatives (undetected anomalies) are considered here to optimize decision making in a cost-reduction perspective. We also describe promising results obtained on real industrial data spanning on a 5-month period and consisting of thousands of tested household units.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.01.333,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Prognostic health management of production systems. New proposed approach and experimental evidences,https://api.elsevier.com/content/abstract/scopus_id/85082764769,"Prognostic Health Management (PHM) is a maintenance policy aimed at predicting the occurrence of a failure in components and consequently minimizing unexpected downtimes of complex systems. Recent developments in condition monitoring (CM) techniques and Artificial Intelligence (AI) tools enabled the collection of a huge amount of data in real-time and its transformation into meaningful information that will support the maintenance decision-making process. The emerging Cyber-Physical Systems (CPS) technologies connect distributed physical systems with their virtual representations in the cyber computational world. The PHM assumes a key role in the implementation of CPS in manufacturing contexts, since it allows to keep CPS and its machines in proper conditions. On the other hand, CPS-based PHM provide an efficient solution to maximize availability of machines and production systems. In this paper, evolving and unsupervised approaches for the implementation of PHM at a component level are described, which are able to process streaming data in real-time and with almost-zero prior knowledge about the monitored component. A case study from a real industrial context is presented. Different unsupervised and online anomaly detection methods are combined with evolving clustering models in order to detect anomalous behaviours in streaming vibration data and integrate the so-generated knowledge into supervised and adaptive models; then, the degradation model for each identified fault is built and the resulting RUL prediction model integrated into the online analysis. Supervised methods are applied to the same dataset, in batch mode, to validate the proposed procedure.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-409547-2.00449-2,Book,Encyclopedia of Analytical Science,scopus,2019-01-01,sciencedirect,Quality assurance | laboratory information management systems,https://api.elsevier.com/content/abstract/scopus_id/85079080236,"In today’s competitive laboratory environment, managers are under increased pressure to deliver high quality data, quickly, and cost effectively, with limited resources. This can only be achieved via laboratory automation. It is more critical than ever to ensure that the modern laboratory is equipped with the latest software Laboratory Information Management Software (LIMS) and tools together with automation technology to ensure that they remain competitive. LIMS together with laboratory automation (positive ID, Robotics, AI, etc.) imparts many benefits that include time savings, resource maximization, efficiency, quality improvements, along with cost reductions. For all businesses that rely on delivering high quality, reliable products, regardless of industry, defects can be responsible for huge losses, from laboratory/company reputation to associated costs of recalls and possibly lawsuits. Compared to manual procedures, automated tasks offer significant benefits which include, reproducibility, increased accuracy, speed (high throughput), enhanced communication, increased responsiveness, automated and effective reporting that results in higher customer satisfaction. Today, LIMS can be delivered on-demand via the Software as a Service (SaaS) model for organizations that either do not have the infrastructure to host the software or who find it more cost effective to utilize the tools hosted in the cloud, eliminating the need for an IT resource. Organizations that employ best practices and implement LIMS have all of their laboratory operational data in a centralized, secure database greatly facilitating access to real-time data, KPIs, document control, quality management as well as regulatory compliance.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2019.05.017,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,The growing path in search of an industrial design identity,https://api.elsevier.com/content/abstract/scopus_id/85076752868,"Knowing that the education system must be reinvented periodically to face the changes of social and cultural paradigm, was reviewed the pedagogical organization of a set of disciplines of an industrial design course that were in operation for a decade. Thus, in view of the objective of restructuring the disciplinary group of industrial design, a new structure has been developed and implemented that could offer students the opportunity to explore problems and challenges that have real applications, increasing the possibility of acquiring competences effectively needed to practice the profession of designer.
                  This restructuring had as its starting point the concept of Project-based learning, which is designated as student-centered pedagogy that involves a dynamic classroom approach in which it is believed that students acquire a deeper knowledge through active exploration of real-world challenges and problems. Consequently, resulting in a learning process organized into levels with increasing degree of complexity. As well, different assimilations of markets and design scenarios.
                  Starting from the first year of the course, where students are still understanding the context of industrial design and its potentialities. At a time when their techniques, principles and methods are still very raw and basic. They are initiated in a LOW-ID and local industry context, to acquire basic skills. The second year allows embark on an intermediate level called MID-ID, with new skills in international brands approach. In the last year of the course the 3rd level is reached, HIGH-ID, with projects with the national industry.
                  The first year of implementation of this curriculum structure showed good results. Thus, favoring a solid interdisciplinary formation with, skills and competences that allow future designers to intervene creatively and competently in a variety of fields. This process allows to progress to the next academic degree to complete and validate the entire formation of the student.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2019.03.212,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,Contribution to the development of a Digital Twin based on product lifecycle to support the manufacturing process,https://api.elsevier.com/content/abstract/scopus_id/85076726437,"The current manufacture challenges are closely linked to the aim of digitalizing the product, the process and the means of production. In such aspects, information about the production processes is available in real-time, allowing managers to act on digital models and, through them, apply decisions in real systems. Thus, having a mirror model or a Digital Twin enables real-time absorption, simulation and implementation of manufacturing variations from the real environment, allowing faster detection of physical problems, and faster production response. The Digital Twin is a virtual representation of the physical system, which is equipped with sensors and actuators and feed the digital system, where the monitoring of data and simulation of variations, for instance, take place. From the synchronized interactions of both components, it is possible to deliver the mentioned faster production responses. Brazilian and German universities joined efforts to develop a Digital Twin based on product lifecycle to support the Manufacturing Process to address these challenges. The proposed Digital Twin seeks to integrate the product twin and the twin of its development process. It shall represent the manufacturing process, enabling the monitoring and optimization of the real production process. The Digital Twin itself is addressed as a product inside the production system and, therefore, its development process will follow the product lifecycle perspective, from the conception and planning to its implementation and usage. The Digital Twin will be further improved with the introduction of Artificial Intelligence tools, characterizing a Smart Digital Twin of the Manufacturing Process. Thus, this paper aims to present the concepts of a research project that is being developed in a joint Brazilian-German Cooperative Research.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2019.09.143,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,Machine Learning approaches for Anomaly Detection in Multiphase Flow Meters,https://api.elsevier.com/content/abstract/scopus_id/85076262725,"Multiphase Flow Meters (MPFM) are important metering tools in the oil and gas industry. A MPFM provides real-time measurements of gas, oil and water flows of a well without the need to separate the phases, a time-consuming procedure that has been classically adopted in the industry. Evaluating the composition of the flow is fundamental for the well management and productivity prediction; therefore, procedures for measuring quality assessment are of crucial importance. In this work we propose an Anomaly Detection approach to MPFM that is effectively able to hand the complexity and variability associated with MPFM data. The proposed approach is designed for embedded implementation and it exploits unsupervised Anomaly Detection approaches like Cluster Based Local Outlier Factor and Isolation Forest.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2019.08.225,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,Curriculum change for graduate-level control engineering education at the Universidad Pontificia Bolivariana,https://api.elsevier.com/content/abstract/scopus_id/85076258553,"This paper addresses the graduate-level control engineering curriculum change performed at the Universidad Pontificia Bolivariana (UPB), Medellin, Colombia. New proposed methodologies include active learning activities using a new multipurpose experimental test bed that was developed with industrial components. The renovated graduate-level control engineering related courses include: Continuous Processes, Discrete Processes, Fuzzy Logic, Neural Networks and Genetic Algorithms, Linear Control, Nonlinear Control, and Optimal Estimation. The new experimental station was developed for teaching, research, and industrial training activities for the School of Engineering at the UPB. In this work, we report the use of the station in an Optimal Estimation course to replace a traditional homework/exams evaluation approach with an applied work that required independent study, the implementation of different observers in a real lab-scale industrial plant, and a paper-style written report. Increasing independent study activities resulted in academic discussions that are valuable for the learning process of the student. The use of the experimental station and the real comparison of estimation algorithms, implemented by using industrial controllers and high-level programming environments, provided the student skills that cannot be acquired by using only simulations in which real implementation restrictions/challenges do not appear. This work represents one of the first approaches for the implementation of the new curriculum model at the UPB for graduate education. The methodology used in the Optimal Estimation class promoted independent learning, critical thinking and writing skills through significant learning activities.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2019.09.169,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,IAssistMe - Adaptable assistant for persons with eye disabilities,https://api.elsevier.com/content/abstract/scopus_id/85076257910,"Visually challenged people may experience certain difficulties in their daily interaction with technology. That is essentially because the main way to exchange and process information is by written text, images or videos. Since the basic purpose of innovation is to improve people’s lifestyle, in this paper we propose a system that can make technology accessible to a broader group. Our prototype is presented as a mobile application based on vocal interaction, which can help people facing visual disorders consult their personal agenda, create an event, invite other friends to attend it, check the weather in certain areas and many other day-to-day tasks. Regarding the implementation, the project consists of a mobile application that interacts with a cloud based system, which makes it reliable and low in latency due to the resource availability in multiple global regions, provided by the newly emerging platform used in building the infrastructure. The novelty of the system lays in the highly flexible serverless architecture [1] that is open to extension and closed to modification through the set of autonomous cloud processing methods that sustain the base of the functionality. This distributed processing approach guarantees that the user always receives a response from his personal assistant, either by using artificial intelligence context generated phrases, by real-time cloud function processing or by fallback to the training answers.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2019.09.069,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,An Innovative Technology: Augmented Reality Based Information Systems,https://api.elsevier.com/content/abstract/scopus_id/85076255225,"In our generation the information systems evolve with new technologies: augmented reality (AR), IoT, artificial intelligence, blockchain etc. Anymore they perform information exchange by sensors. It is estimated that the systems will be in a state of extreme interaction and reach 50 billion devices connected in Internet in 2020. We know that everything around us will be in interaction and they will do everything without any need of human interference. For example, when our dishwasher is full, it will start to wash automatically, or when the run out of the gasoline, our car will drive to the nearest station, or even when a burglar is entered to our house, it will automatically be detected and be announced to the police office. In business life, the processes will be automatical in maximum level and this technology will increase productivity and efficiency. Next to mobile technology, it is thought that these new generation information systems (IS) will take the biggest place in our lives. AR also will be integrated to these systems to augment the information in real world. Humanity will augment its habitat in an innovative way thanks to these AR based IS. This paper surveys the current state-of-the-art AR systems related with aerospace & defense, industry, education, medical and gaming sectors. The connection of AR based IS and innovation is explained with a technological insight. In addition to international use cases HAVELSAN’s use cases are also given that are performed from the aspect of applied open innovation strategy. This strategy is addressed specific to the implemented activities of AR based IS.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2018.12.017,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,AI based injection molding process for consistent product quality,https://api.elsevier.com/content/abstract/scopus_id/85072584818,"In manufacturing processes, Injection Molding is widely used for producing plastic components with large lot size. So, continuous improvements in product quality consistency is crucial to maintaining a competitive edge in the injection molding industry. Various optimization techniques like ANN, GA, Iterative method, and simulation based are being used for optimization of Injection Molding process and obtaining optimal processing conditions. But still due to variation during molding cycles, quality failure occurs. As many constituents like process, Material, machine together yields product quality. This paper is focused on Real time AI based control of process parameters in injection molding cycle. Process parameters and their interrelationship with quality failure has been studied and later supposed to be used to generate algorithm for compensating the deviation of process parameters. Pressure and temperature sensor assisted monitoring system is used to collect data in real time and based on its comparison with the standard values an interrelationship is formed between parameters and plastic material properties. Algorithm generates new process parameter values to compensate the deviation and machine control follows the same. The entire process is supposed to be smart and automatic after being trained with AI and machine learning techniques. Simulation using Moldflow software and real industry collected data has been used for understanding whole molding process establishing relationship between failure and parameters. An automotive product in real industry is chosen for data acquisition, implementation and validation of entire AI based system.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2018.12.026,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Hybrid artificial intelligence system for the design of highly-automated production systems,https://api.elsevier.com/content/abstract/scopus_id/85072561400,"The automated design of production systems is a young field of research which has not been widely explored by industry nor research in recent decades. Currently, the effort spent in production system design is increasing significantly in automotive industry due to the number of product variants and product complexity. Intelligent methods can support engineers in repetitive tasks and give them more opportunity to focus on work which requires their core competencies. This paper presents a novel artificial intelligence methodology that automatically generates initial production system configurations based on real industrial scenarios in the automotive field of body-in-white production. The hybrid methodology reacts flexibly against data sets of different content and has been implemented in a software prototype.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2019.04.090,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,Deep neural network method of recognizing the critical situations for transport systems by video images,https://api.elsevier.com/content/abstract/scopus_id/85071926362,"The deep neural network method of recognizing critical situations for transport systems according to video frames from the intelligent vehicles cameras is offered, that is effective in terms of accuracy and high-speed performance. Unlike the known solutions for the objects and normal or critical situations detection and recognition, it uses the classification with the subsequent reinforcement on the basis of several video stream frames and with the automatic annotation algorithm. The adapted architectures of neural networks are offered: the dual network to identify drivers and passengers according to the face image, the network with independent recurrent layers to classify situations according to the video fragment. The scheme of the intellectual distributed city system of transport safety using the cameras and on-board computers united in a single network is offered. Software modules in Python are developed and natural experiments are made. The possibility of the offered algorithms and programs in UGV or in the driver assistant systems implementation is shown with the illustrating examples in real-time.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2020.01.288,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,Action recognition in manufacturing assembly using multimodal sensor fusion,https://api.elsevier.com/content/abstract/scopus_id/85070765380,"Production innovations are occurring faster than ever. Manufacturing workers thus need to frequently learn new methods and skills. In fast changing, largely uncertain production systems, manufacturers with the ability to comprehend workers’ behavior and assess their operation performance in near real-time will achieve better performance than peers. Action recognition can serve this purpose. Despite that human action recognition has been an active field of study in machine learning, limited work has been done for recognizing worker actions in performing manufacturing tasks that involve complex, intricate operations. Using data captured by one sensor or a single type of sensor to recognize those actions lacks reliability. The limitation can be surpassed by sensor fusion at data, feature, and decision levels. This paper presents a study that developed a multimodal sensor system and used sensor fusion methods to enhance the reliability of action recognition. One step in assembling a Bukito 3D printer, which composed of a sequence of 7 actions, was used to illustrate and assess the proposed method. Two wearable sensors namely Myo-armband captured both Inertial Measurement Unit (IMU) and electromyography (EMG) signals of assembly workers. Microsoft Kinect, a vision based sensor, simultaneously tracked predefined skeleton joints of them. The collected IMU, EMG, and skeleton data were respectively used to train five individual Convolutional Neural Network (CNN) models. Then, various fusion methods were implemented to integrate the prediction results of independent models to yield the final prediction. Reasons for achieving better performance using sensor fusion were identified from this study.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2019.03.041,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,"Design, implementation and evaluation of reinforcement learning for an adaptive order dispatching in job shop manufacturing systems",https://api.elsevier.com/content/abstract/scopus_id/85068485505,"Modern production systems tend to have smaller batch sizes, a larger product variety and more complex material flow systems. Since a human oftentimes can no longer act in a sufficient manner as a decision maker under these circumstances, the demand for efficient and adaptive control systems is rising. This paper introduces a methodical approach as well as guideline for the design, implementation and evaluation of Reinforcement Learning (RL) algorithms for an adaptive order dispatching. Thereby, it addresses production engineers willing to apply RL. Moreover, a real-world use case shows the successful application of the method and remarkable results supporting real-time decision-making. These findings comprehensively illustrate and extend the knowledge on RL.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2019.03.047,Conference Proceeding,Procedia Manufacturing,scopus,2019-01-01,sciencedirect,A Practical Approach of Teaching Digitalization and Safety Strategies in Cyber-Physical Production Systems,https://api.elsevier.com/content/abstract/scopus_id/85065658005,"Digitalization strategies in cyber-physical production systems (CPPS) are one of the key factors of Industry 4.0. The topic not only addresses data preparation, real-time data processing, big data analytics, visualization and machine interface design but also cyber security and safety. Especially, unauthorized access to protected (personal or enterprise) data or unauthorized control of production facilities imply risks when it comes to digitalization. Because of the increased complexity of state-of-the-art technologies, educational institutions need to provide practice-oriented teaching methods in learning factories to help engineers of today understand the impact of those developments.
                  In the light of this fact, this paper presents a practical approach of teaching digitalization strategies in CPPS. Planning, implementing and impacts of digitalization strategies are taught on a use-case with human-robot-collaboration. The objective of the use-case is to realize a real-time obstacle avoidance approach for a collaborative application based on a local positioning system. Here, students not only learn how to model the kinematics of a robot and program a robot but also how to design machine interfaces for real-time data transfer and processing as well as impacts of digitalization on safety and security.
                  The implementation of the use-case is part of the TU Wien teaching portfolio and thus part of its learning factory, where students and apprentices have the possibility to experiment and gain experiences by deliberate error simulations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2019.02.101,Conference Proceeding,Procedia CIRP,scopus,2019-01-01,sciencedirect,Autonomous order dispatching in the semiconductor industry using reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85065424368,"Cyber Physical Production Systems (CPPS) provide a huge amount of data. Simultaneously, operational decisions are getting ever more complex due to smaller batch sizes, a larger product variety and complex processes in production systems. Production engineers struggle to utilize the recorded data to optimize production processes effectively because of a rising level of complexity. This paper shows the successful implementation of an autonomous order dispatching system that is based on a Reinforcement Learning (RL) algorithm. The real-world use case in the semiconductor industry is a highly suitable example of a cyber physical and digitized production system.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cirpj.2018.12.002,Journal,CIRP Journal of Manufacturing Science and Technology,scopus,2019-01-01,sciencedirect,"From factory floor to process models: A data gathering approach to generate, transform, and visualize manufacturing processes",https://api.elsevier.com/content/abstract/scopus_id/85058703955,"The need for tools to help guide decision making is growing within the manufacturing industry. The analysis performed by these tools will help operators and engineers to understand the behaviour of the manufacturing stations better and thereby take data-driven decisions to improve them. The tools use techniques borrowed from fields such as Data Analytics, BigData, Predictive Modelling, and Machine Learning. However, to be able to use these tools efficiently, data from the factory floor is required as input. This data needs to be extracted from two sources, the PLCs, and the robots. In practice, methods to extract usable data from robots are rather scarce. The present work describes an approach to capture data from robots, which can be applied to both legacy and current state-of-the-art manufacturing systems. The described approach is developed using Sequence Planner – a tool for modelling and analyzing production systems – and is currently implemented at an automotive company as a pilot project to visualize and examine the ongoing process. By exploiting the robot code structure, robot actions are converted to event streams that are abstracted into operations. We then demonstrate the applicability of the resulting operations, by visualizing the ongoing process in real-time as Gantt charts, that support the operators performing maintenance. And, the data is also analyzed off-line using process mining techniques to create a general model that describes the underlying behaviour existing in the manufacturing station. Such models are used to derive insights about relationships between different operations, and also between resources.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.impact.2018.12.001,Journal,NanoImpact,scopus,2019-01-01,sciencedirect,SUNDS probabilistic human health risk assessment methodology and its application to organic pigment used in the automotive industry,https://api.elsevier.com/content/abstract/scopus_id/85058641247,"The increasing use of engineered nanomaterials (ENMs) in nano-enabled products (NEPs) has raised societal concerns about their possible health and ecological implications. To ensure a high level of human and environmental protection it is essential to properly estimate the risks of these new materials and to develop adequate risk management strategies. To this end, we propose a quantitative Human Health Risk Assessment (HHRA) methodology, which was developed in the European Seventh Framework research project SUN (Sustainable Nanotechnologies) and implemented in the web-based SUN Decision Support System (SUNDS). One of the major strengths of this probabilistic approach as compared to its deterministic alternatives is its ability to clearly communicate the uncertainties in the estimated risks in order to support better risk communication for more objective decision making by industries and regulators.
                  To demonstrate this methodology, we applied it in a real case study involving a nanoscale organic red pigment used in the automotive industry. Our analysis clearly showed that the main source of uncertainty was the extrapolation from (sub)acute in vivo toxicity data to long-term risk. This extrapolation was necessary due to a lack of (sub)chronic in vivo studies for the investigated nanomaterial. Despite the high uncertainty in the final results due to the conservative assumptions made in the risks assessment, the estimated risks are acceptable for all investigated exposure scenarios along the product lifecycle.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijepes.2018.07.022,Journal,International Journal of Electrical Power and Energy Systems,scopus,2019-01-01,sciencedirect,Design and implementation of flexible Numerical Overcurrent Relay on FPGA,https://api.elsevier.com/content/abstract/scopus_id/85050986246,"This paper presents the contemporary design and implementation of an intelligent revelation in the field of the over-current relay to meet the challenges of the modern grid. The unique three-neuron single layered architecture of Artificial Neural Network (ANN) provides flexibility by exploiting its universal function approximation capabilities. The Unique Selling Proposition (USP) of the present development is the simple design of ANN, suitable for low-end, low-cost Field Programmable Gate Array (FPGA) implementation. The nano-scaled internal processing time for three-phase design, with the provision of remotely controlled adaptive relay settings, would definitely an innovative solution for grid connection of renewable energy sources. The proposed design of the universal over-current relay, confirmed by the real-time testing, is a true fusion of electrical power, communication and information technology to meet the global trend of the electrical power industries.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.powtec.2018.08.064,Journal,Powder Technology,scopus,2018-11-01,sciencedirect,"Settling velocity of drill cuttings in drilling fluids: A review of experimental, numerical simulations and artificial intelligence studies",https://api.elsevier.com/content/abstract/scopus_id/85052516468,"In this paper, a comprehensive review of experimental, numerical and artificial intelligence studies on the subject of cuttings settling velocity in drilling muds made by researchers over the last seven decades is brought to the fore. In this respect, 91 experimental, 13 numerical simulations and 7 artificial intelligence researches were isolated, reviewed, tabulated and discussed. A comparison of the three methods and the challenges facing each of these methods were also reviewed. The major outcomes of this review include: (1) the unanimity among experimental researchers that mud rheology, particle size and shape and wall effect are major parameters affecting the settling velocity of cuttings in wellbores; (2) the prevalence of cuttings settling velocity experiments done with the mud in static conditions and the wellbore in the vertical configuration; (3) the extensive use of rigid particles of spherical shape to represent drill cuttings due to their usefulness in experimental visualization, particle tracking, and numerical implementation; (4) the existence of an artificial intelligence technique - multi-gene genetic programming (MGGP) which can provide an explicit equation that can help in predicting settling velocity; (5) the limited number of experimental studies factoring in the effect of pipe rotation and well inclination effects on the settling velocity of cuttings and (6) the most applied numerical method for determining settling velocity is the finite element method. Despite these facts, there is need to perform more experiments with real drill cuttings and factor in the effects of conditions such as drillstring rotation and well inclination and use data emanating therefrom to develop explicit models that would include the effects of these. It should be noted however, that the aim of this paper is not to create an encyclopaedia of particle settling velocity research, but to provide to the researcher with a basic, theoretical, experimental and numerical overview of what has so far been achieved in the area of cuttings settling velocity in drilling muds.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.petrol.2018.06.072,Journal,Journal of Petroleum Science and Engineering,scopus,2018-11-01,sciencedirect,Data driven model for sonic well log prediction,https://api.elsevier.com/content/abstract/scopus_id/85050476760,"Near wellbore failure during the exploration of hydrocarbon reservoirs presents a serious concern to the oil and gas industry. To predict the probability of these undesirable phenomena, engineers study the mechanical rock properties of the formation such as Young's modulus, Bulk modulus, shear modulus and Poisson's ratio. Conventionally, these are measured indirectly using the established petro physical relationships from sonic wave velocities which can be obtained from sonic well logs. Unfortunately, reliable sonic well logs are not always available, due to poor borehole conditions (wash out), damaged tools and offset well data. Most offset well log data are not acquired with dipole sonic tools; they are acquired with a borehole compensated logging tool. This limits the application of acoustic measurements to estimate the mechanical rock properties.
                  In this study, a three-layer feedforward multilayered perceptron artificial neural network model is presented. This model aims to estimate compressional wave transit time and shear wave transit time using real gamma ray and formation density logs. The validation of the model is confirmed by using an oil and gas offshore shaley sandstone reservoir located in West Africa. The results of the validation show that the model presented in this study can be used to determine the sanding potential of the formation without performing a compressive geoscientific analysis in the absence of sonic well logs. The developed model's effectiveness is tested by comparing the predicted results with results obtained from the measured well log. The paper provides a tool to give preliminary recommendations of the likelihood of the formation to produce sand. Implementation of the proposed model can serve as a cost-effective and reliable alternative for the oil and gas industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.measurement.2018.05.099,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2018-11-01,sciencedirect,Parallel three-dimensional electrical capacitance data imaging using a nonlinear inversion algorithm and L<sup>p</sup> norm-based model regularization,https://api.elsevier.com/content/abstract/scopus_id/85049483981,"In order to improve image reconstructions, different classes of nonlinear inversion algorithms are developed and used in different research topics like imaging processes in oil industry or the characterization of complex porous media or multiphase flows. These algorithms are able to avoid local minima and to reach more adapted minima of a given misfit function between observed/measured and computed data. Techniques as different as electrical, ultrasound or potential methods, are used. We present here a nonlinear algorithm that allows us to produce permittivity images by using electrical capacitance tomography (ECT). ECT is a non-invasive technique to image non-conductive permittivity distributions and is used in many oil industry imaging applications such as multiphase flows in pipelines, fluidized bed reactors, mixing vessels, and tanks of phase separation. Even if the ECT technique provides low resolution reconstructions, it is cheap, robust and very fast when compared to other imaging tools. In this method one or more rings of electrodes excite a medium to be imaged at high frequencies, and more particularly at frequencies for which a static electrical potential field has fully developed. In many studies of other research groups only one ring of sources is introduced but the reconstruction accuracy was not totally satisfactory due to the 3D nature of the problem to be solved. Instead of using nonlinear stochastic algorithms like the simulated annealing (SA) technique that we optimized in previous studies to image permittivity distributions of granular or solid materials as well as real oil–gas or two-phase flows in 2D cylindrical vessel configurations, we propose here a new ECT inversion tool to image permittivities in a 3D cylindrical configuration. 3D stochastic optimization methods such as SA, neural networks, genetic algorithms can become computationally too prohibitive, and classical local or linear inversion methods excessively smooth images in many cases. Therefore, we propose here a 3D parallel inversion procedure with different numbers of rings and different 
                        
                           
                              
                                 L
                              
                              
                                 p
                              
                           
                        
                      norms, with
                        
                           1
                           <
                           p
                           ⩽
                           2
                        
                     , applied to the model regularization of the misfit function to increase the resolution of the models after inversion. We are able to better reconstruct two-phase and three-phase (oil, gas and solids) mixtures by combining 
                        
                           
                              
                                 L
                              
                              
                                 p
                              
                           
                        
                     -norm regularizations of the misfit function to minimize and several rings of electrodes. All these algorithms have been implemented in a more general parallel framework TOMOFAST-X designed for multi-physics joint inversion purposes, and could also be used in other fields of research such as larger-scale geophysical exploration for instance.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ssci.2018.06.012,Journal,Safety Science,scopus,2018-11-01,sciencedirect,Occupational health and safety in the industry 4.0 era: A cause for major concern?,https://api.elsevier.com/content/abstract/scopus_id/85049323662,"Real-time communication, Big Data, human–machine cooperation, remote sensing, monitoring and process control, autonomous equipment and interconnectivity are becoming major assets in modern industry. As the fourth industrial revolution or Industry 4.0 becomes the predominant reality, it will bring new paradigm shifts, which will have an impact on the management of occupational health and safety (OHS).
                  In the midst of this new and accelerating industrial trend, are we giving due consideration to changes in OHS imperatives? Are the OHS consequences of Industry 4.0 being evaluated properly? Do we stand to lose any of the gains made through proactive approaches? Are there rational grounds for major concerns? In this article, we examine these questions in order to raise consciousness with regard to the integration of OHS into Industry4.0.
                  It is clear that if the technologies driving Industry 4.0 develop in silos and manufacturers’ initiatives are isolated and fragmented, the dangers will multiply and the net impact on OHS will be negative. As major changes are implemented, previous gains in preventive management of workplace health and safety will be at risk. If we are to avoid putting technological progress and OHS on a collision course, researchers, field experts and industrialists will have to collaborate on a smooth transition towards Industry 4.0.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.vetmic.2018.08.026,Journal,Veterinary Microbiology,scopus,2018-10-01,sciencedirect,Detection of non-notifiable H4N6 avian influenza virus in poultry in Great Britain,https://api.elsevier.com/content/abstract/scopus_id/85053845094,"A 12-month pilot project for notifiable avian disease (NAD) exclusion testing in chicken and turkey flocks in Great Britain (GB) offered, in partnership with industry, opportunities to carry out differential diagnosis in flocks where NAD was not suspected, and to identify undetected or undiagnosed infections. In May 2014, clinical samples received from a broiler breeder chicken premises that had been experiencing health and production problems for approximately one week tested positive by avian influenza (AI) real-time reverse transcription polymerase chain reaction (RRT-PCR). Following immediate escalation to an official, statutory investigation to rule out the presence of notifiable AI virus (AIV; H5 or H7 subtypes), a non-notifiable H4N6 low pathogenicity (LP) AIV was detected through virus isolation in embryonated specific pathogen free (SPF) fowls’ eggs, neuraminidase inhibition test, cleavage site sequencing and AIV subtype H4-specific serology. Premises movement restrictions were lifted, and no further disease control measures were implemented as per the United Kingdom (UK) legislation. Phylogenetic analysis of the haemagglutinin and neuraminidase genes of the virus revealed closest relationships to viruses from Mallard ducks in Sweden during 2007 and 2009. In June 2014, clinical suspicion of NAD was reported in a flock of free-range laying chickens elsewhere in GB, due to increasing daily mortality and reduced egg production over a five-day period. An H4N6 LPAIV with an intravenous pathogenicity index of 0.50 was isolated. This virus was genetically highly similar, but not identical, to the virus detected during May 2014. Full viral genome analyses showed characteristics of a strain that had not recently transferred from wild birds, implying spread within the poultry sector had occurred. A stalk deletion in the neuraminidase gene sequence indicated an adaptation of the virus to poultry. Furthermore, there was unexpected evidence of systemic spread of the virus on post-mortem. No other cases were reported. Infection with LPAIVs often result in variable clinical presentation in poultry, making detection of disease more difficult.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.mfglet.2018.09.002,Journal,Manufacturing Letters,scopus,2018-10-01,sciencedirect,Industrial Artificial Intelligence for industry 4.0-based manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85053749537,"The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compind.2018.07.004,Journal,Computers in Industry,scopus,2018-10-01,sciencedirect,IDARTS – Towards intelligent data analysis and real-time supervision for industry 4.0,https://api.elsevier.com/content/abstract/scopus_id/85050319341,"The manufacturing industry represents a data rich environment, in which larger and larger volumes of data are constantly being generated by its processes. However, only a relatively small portion of it is actually taken advantage of by manufacturers. As such, the proposed Intelligent Data Analysis and Real-Time Supervision (IDARTS) framework presents the guidelines for the implementation of scalable, flexible and pluggable data analysis and real-time supervision systems for manufacturing environments. IDARTS is aligned with the current Industry 4.0 trend, being aimed at allowing manufacturers to translate their data into a business advantage through the integration of a Cyber-Physical System at the edge with cloud computing. It combines distributed data acquisition, machine learning and run-time reasoning to assist in fields such as predictive maintenance and quality control, reducing the impact of disruptive events in production.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2018.07.016,Journal,Computers and Industrial Engineering,scopus,2018-10-01,sciencedirect,New decision support system for strategic planning in process industries: Computational results,https://api.elsevier.com/content/abstract/scopus_id/85049776857,"The impact of a Stochastic Linear Programming (SLP) based Decision Support System in a manufacturing company, such as an integrated aluminum plant, is measured by two important parameters, the VSS and EVPI. With the real data of an integrated steel plant in India, we demonstrate that SLP based DSS can be very effective in managing demand uncertainty and performing futuristic integrated planning, and their financial impact can be in millions of dollars. A two stage stochastic programming model with recourse is implementedin the DSS here. A set of experiments is conducted. Real data from an aluminum company is used to validate the system. The importance of SLP based DSS can be realized from the fact that the value of the stochastic solution (VSS) is USD 3.58 million with 30% demand variability and equally likely demand distribution. The VSS as a percentage of Expectation of Expected Value (EEV) ranges from 0.90% to 18.93% across experiments.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jpdc.2018.04.005,Journal,Journal of Parallel and Distributed Computing,scopus,2018-10-01,sciencedirect,A malicious threat detection model for cloud assisted internet of things (CoT) based industrial control system (ICS) networks using deep belief network,https://api.elsevier.com/content/abstract/scopus_id/85047404138,"Internet of Things (IoT) devices are extensively used in modern industries combined with the conventional industrial control system (ICS) network through the industrial cloud to make the production data easily available to the corporate business management and easier control for highly profitable production systems. The different devices within the conventional ICS network originally manufactured to run on an isolated network and was not considered for the privacy and security of the control and production/architecture data being trafficked over the manufacturing plant to the corporate. Due to their extensive integration with the industrial cloud network over the internet, these ICS networks are exposed to a significant threat of malicious activities created by malicious software. Protecting ICS from such attacks requires continuous update of their database of anti-malware tools which requires efforts from manual experts on a regular basis. This limits real time protection of ICS.
                  Earlier work by Huda et al. (2017) based on a semi-supervised approach performed well. However training process of the semi-supervised-approach (Huda et al., 2017) is complex procedure which requires a hybridization of feature selection, unsupervised clustering and supervised training techniques. Therefore, it could be time consuming for ICS network for real time protection. In this paper, we propose an adaptive threat detection model for industrial cloud of things (CoT) based on deep learning. Deep learning has been used in many domain of pattern recognition and a popular approach for its simple training procedure. Most importantly, deep learning can learn the hidden patterns of the domain in an unsupervised manner which can avoid the requirements of huge expensive labeled data. We used this particular characteristic of deep learning to design our detection model.
                  Two different types of deep learning based detection models are proposed in this work. The first model uses a disjoint training and testing data for a deep belief network (DBN) and corresponding artificial neural network (ANN). In the second proposed detection model, DBN is trained using new unlabeled data to provide DBN with additional knowledge about the changes in the malicious attack patterns. Novelty of the proposed detection models is that the models are adaptive where training procedures is simpler than earlier work (Huda et al, 2017) and can adapt new malware behaviors from already available and cheap unlabeled data at the same time. This will avoid expensive manual labeling of new attacks and corresponding time complexity making it feasible for ICS networks. Performances of standard DBNs are sensitive to its configurations and values for the hyper-parameters including number of hidden nodes, learning rate and number epochs. Therefore proposed detection models find an optimal configuration by varying the structure of DBNs and other parameters. The proposed detection models are extensively tested on a real malware test bed. Experimental results show that the proposed approaches achieve higher accuracies than standard detection algorithms and obtain similar performances with earlier semi-supervised work (Huda et al., 2017) but provide a comparatively simplified training model.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.apenergy.2018.06.040,Journal,Applied Energy,scopus,2018-09-15,sciencedirect,Optimal scheduling of a microgrid in a volatile electricity market environment: Portfolio optimization approach,https://api.elsevier.com/content/abstract/scopus_id/85048767400,"This paper proposes an optimal scheduling strategy for a microgrid participating in a volatile electricity market. The microgrid system includes photovoltaic generators, a wind turbine, a load, grid connection, and a battery storage system. An optimal microgrid operation is achieved by maximizing the utility function represented by the exponential rate of growth of the electricity market value through electricity transactions between the microgrid and main grid, on the premise of satisfying the power balance and generation limit of system components. The uncertainties occurring during the microgrid operation are represented by generator output, load demand, and electricity price fluctuation. The proposed strategy utilizes the Kelly Criterion, an optimal strategy that maximizes the growth rate of an asset’s net worth over repeated investments, coupled with an artificial neural network forecast of electricity price to deal with the volatile energy market. The proposed algorithm provides significant improvements in microgrid scheduling by eliminating the reliance on renewable generation and load forecasts, which makes it computationally inexpensive and thus feasible for real-time implementation. In representative case scenarios, using real-world tracers, we show that the algorithm has no dependency on meteorological forecasts and performs optimally in a volatile electricity market.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jisa.2018.05.002,Journal,Journal of Information Security and Applications,scopus,2018-08-01,sciencedirect,Identification of malicious activities in industrial internet of things based on deep learning models,https://api.elsevier.com/content/abstract/scopus_id/85047072990,"Internet Industrial Control Systems (IICSs) that connect technological appliances and services with physical systems have become a new direction of research as they face different types of cyber-attacks that threaten their success in providing continuous services to organizations. Such threats cause firms to suffer financial and reputational losses and the stealing of important information. Although Network Intrusion Detection Systems (NIDSs) have been proposed to protect against them, they have the difficult task of collecting information for use in developing an intelligent NIDS which can proficiently detect existing and new attacks. In order to address this challenge, this paper proposes an anomaly detection technique for IICSs based on deep learning models that can learn and validate using information collected from TCP/IP packets. It includes a consecutive training process executed using a deep auto-encoder and deep feedforward neural network architecture which is evaluated using two well-known network datasets, namely, the NSL-KDD and UNSW-NB15. As the experimental results demonstrate that this technique can achieve a higher detection rate and lower false positive rate than eight recently developed techniques, it could be implemented in real IICS environments.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2018.03.014,Journal,Neurocomputing,scopus,2018-06-14,sciencedirect,ACDIN: Bridging the gap between artificial and real bearing damages for bearing fault diagnosis,https://api.elsevier.com/content/abstract/scopus_id/85044327325,"Data-driven algorithms for bearing fault diagnosis have achieved much success. However, it is difficult and even impossible to collect enough data containing real bearing damages to train the classifiers, which hinders the application of these methods in industrial environments. One feasible way to address the problem is training the classifiers with data generated from artificial bearing damages instead of real ones. In this way, the problem changes to how to extract common features shared by both kinds of data because the differences between the artificial one and the natural one always baffle the learning machine. In this paper, a novel model, deep inception net with atrous convolution (ACDIN), is proposed to cope with the problem. The contribution of this paper is threefold. First and foremost, ACDIN improves the accuracy from 75% (best results of conventional data-driven methods) to 95% on diagnosing the real bearing faults when trained with only the data generated from artificial bearing damages. Second, ACDIN takes raw temporal signals as inputs, which means that it is pre-processing free. Last, feature visualization is used to analyze the mechanism behind the high performance of the proposed model.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.enconman.2018.03.044,Journal,Energy Conversion and Management,scopus,2018-06-01,sciencedirect,Adaptive air-fuel ratio control of dual-injection engines under biofuel blends using extreme learning machine,https://api.elsevier.com/content/abstract/scopus_id/85044138298,"Dual-injection engines, which allow real-time control and injection of two different fuels, are capable of varying the ratio of biofuel blends at different engine operating conditions for optimal engine performance. However, while many experiments have been carried out on these engines to demonstrate their advantages, very few studies have focused on the corresponding air–fuel ratio (AFR) control strategy. In order to achieve stable engine operation, it is essential to maintain transient AFR during the change of fuel blend ratio. Therefore, this study proposes an adaptive controller for AFR control of dual-injection engines. The proposed controller is designed based on a recently developed machine learning method called extreme learning machine, and its stability is verified with Lyapunov analysis. Simulations have been performed on an industry-level engine simulation software to verify the controller. Since dual-injection engines are not available in the market, a spark-ignition engine has been retrofitted for dual-injection operation so that the proposed controller can be implemented and evaluated experimentally. Both simulation and experiment results show that the proposed controller can effectively regulate the AFR to desired level. The results also show that the proposed controller outperforms the engine built-in AFR controller, indicating its significance for dual-injection engines.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.autcon.2018.01.003,Journal,Automation in Construction,scopus,2018-05-01,sciencedirect,Transfer learning and deep convolutional neural networks for safety guardrail detection in 2D images,https://api.elsevier.com/content/abstract/scopus_id/85041454603,"Safety has been a concern for the construction industry for decades. Unsafe conditions and behaviors are considered as the major causes of construction accidents. The current safety inspection of conditions and behaviors heavily rely on human efforts which are limited onsite. To improve the safety performance of the industry, a more efficient approach to identify the unsafe conditions on site is required to supplement the current manual inspection practice. A promising way to supplement the current manual safety inspection is automated and intelligent monitoring/inspection through information and sensing technologies, including localization techniques, environment monitoring, image processing and etc. To assess the potential benefits of contemporary technologies for onsite safety inspection, the authors focused on real-time guardrail detection, as unprotected edges are the ones cause for workers falling from heights.
                  In this paper, the authors developed a safety guardrail detection model based on convolutional neural network (CNN). An augmented data set is generated with the addition of background image to guardrail 3D models and used as training set. Transfer learning is utilized and the Visual Geometry Group architecture with 16 layers (VGG-16) model is adopted to construct the basic features extraction for the neural network. In the CNN implementation, 4000 augmented images were used to train the proposed model, while another 2000 images collected from real construction jobsites and 2000 images from Google were used to validate the proposed model. The proposed CNN-based guardrail detection model obtained a high accuracy of 96.5%. In addition, this study indicates that the synthetic images generated by augment technology can be used to create a large training dataset, and CNN-based image detection algorithm is a promising approach in construction jobsite safety monitoring.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2018.01.002,Journal,Neurocomputing,scopus,2018-04-12,sciencedirect,Robot manipulator control using neural networks: A survey,https://api.elsevier.com/content/abstract/scopus_id/85041636063,"Robot manipulators are playing increasingly significant roles in scientific researches and engineering applications in recent years. Using manipulators to save labors and increase accuracies are becoming common practices in industry. Neural networks, which feature high-speed parallel distributed processing, and can be readily implemented by hardware, have been recognized as a powerful tool for real-time processing and successfully applied widely in various control systems. Particularly, using neural networks for the control of robot manipulators have attracted much attention and various related schemes and methods have been proposed and investigated. In this paper, we make a review of research progress about controlling manipulators by means of neural networks. The problem foundation of manipulator control and the theoretical ideas on using neural network to solve this problem are first analyzed and then the latest progresses on this topic in recent years are described and reviewed in detail. Finally, toward practical applications, some potential directions possibly deserving investigation in controlling manipulators by neural networks are pointed out and discussed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.energy.2018.01.159,Journal,Energy,scopus,2018-04-01,sciencedirect,Multiobjective optimization of ethylene cracking furnace system using self-adaptive multiobjective teaching-learning-based optimization,https://api.elsevier.com/content/abstract/scopus_id/85041748366,"The ethylene cracking furnace system is crucial for an olefin plant. Multiple cracking furnaces are used to convert various hydrocarbon feedstocks to smaller hydrocarbon molecules, and the operational conditions of these furnaces significantly influence product yields and fuel consumption. This paper develops a multiobjective operational model for an industrial cracking furnace system that describes the operation of each furnace based on current feedstock allocations, and uses this model to optimize two important and conflicting objectives: maximization of key products yield, and minimization of the fuel consumed per unit ethylene. The model incorporates constraints related to material balance and the outlet temperature of transfer line exchanger. The self-adaptive multiobjective teaching-learning-based optimization algorithm is improved and used to solve the designed multiobjective optimization problem, obtaining a Pareto front with a diverse range of solutions. A real industrial case is investigated to illustrate the performance of the proposed model: the set of solutions returned offers a diverse range of options for possible implementation, including several solutions with both significant improvement in product yields and lower fuel consumption, compared with typical operational conditions.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.measurement.2017.12.026,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2018-03-01,sciencedirect,NARX ANN-based instrument fault detection in motorcycle,https://api.elsevier.com/content/abstract/scopus_id/85039147782,"In the context of motorcycle, we can assist to an increasing interest toward semi-active suspension control systems able to improve both the comfort and the passenger’s safety in both racing and original equipment manufacturer applications. Such systems implement suitable strategies based on the measure of several quantities, among which the relative velocity of the wheels respect to the vehicle body with the aim of regulating in real-time the damping forces. The actual effectiveness of such strategy strongly depends on the reliability and accuracy of the data measured by the sensors involved in the control loop. Due to their simplicity and good performance in terms of linearity, the most used sensors for suspension displacement measurements are based on linear potentiometers but such kind of sensors suffer of wear and tear and aging higher than the other sensors involved in the control loop strategy. As a consequence, the fault detection of such sensor is strongly recommended to avoid wrong and in some cases dangerous suspension behaviors.
                  To this aim, in this paper a Fault Detection scheme for the rear suspension stroke sensor is designed and verified. The residual generation is based on the use of a Nonlinear Auto-Regressive with eXogenous inputs (NARX) network which is able to effectively take into account for the system nonlinearity. Experimental results have proven the good promptness and reliability of the scheme in detecting different kind of faults as “un-calibration faults” (e.g. due to slight variations of the input/output sensor curve), “hold-faults” (e.g. due to the breaking of the potentiometer cursor), “open circuit” and “short circuit” (e.g. due to electrical interruptions and short circuits, respectively).
                  In addition, to verify the feasibility of a real-time implementation on actual processing units employed in such context, the scheme has been successfully implemented on a microcontroller STM32 based on the general-purpose ARM-M4 architecture. The validation tests and analysis have shown that the proposed Instrument Fault Detection scheme could be successfully developed on these kind of architectures by assuring a real-time operating.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cose.2017.11.014,Journal,Computers and Security,scopus,2018-03-01,sciencedirect,Intelligent agents defending for an IoT world: A review,https://api.elsevier.com/content/abstract/scopus_id/85038807783,"Transition to the Internet of Things (IoT) is progressing without realization. In light of this securing traditional systems is still a challenging role requiring a mixture of solutions which may negatively impact, or simply, not scale to a desired operational level. Rule and signature based intruder detection remains prominent in commercial deployments, while the use of machine learning for anomaly detection has been an active research area. Behavior detection means have also benefited from the widespread use of mobile and wireless applications. For the use of smart defense systems we propose that we must widen our perspective to not only security, but also to the domains of artificial intelligence and the IoT in better understanding the challenges that lie ahead in hope of achieving autonomous defense. We investigate how intruder detection fits within these domains, particularly as intelligent agents. How current approaches of intruder detection fulfill their role as intelligent agents, the needs of autonomous action regarding compromised nodes that are intelligent, distributed and data driven. The requirements of detection agents among IoT security are vulnerabilities, challenges and their applicable methodologies. In answering aforementioned questions, a survey of recent research work is presented in avoiding refitting old solutions into new roles. This survey is aimed toward security researchers or academics, IoT developers and information officers concerned with the covered areas. Contributions made within this review are the review of literature of traditional and distributed approaches to intruder detection, modeled as intelligent agents for an IoT perspective; defining a common reference of key terms between fields of intruder detection, artificial intelligence and the IoT, identification of key defense cycle requirements for defensive agents, relevant manufacturing and security challenges; and considerations to future development. As the turn of the decade draws nearer we anticipate 2020 as the turning point where deployments become common, not merely just a topic of conversation but where the need for collective, intelligent detection agents work across all layers of the IoT becomes a reality.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.microrel.2017.11.002,Journal,Microelectronics Reliability,scopus,2018-02-01,sciencedirect,Prognostics of aluminum electrolytic capacitors using artificial neural network approach,https://api.elsevier.com/content/abstract/scopus_id/85033701102,"In this work, an effort is being made to monitor the condition of in-circuit aluminum electrolytic capacitor using artificial neural network (ANN). Recent industrial surveys on the reliability of power electronic systems shows that most of faults occur due to the wear out of aluminum electrolytic capacitors and thermal stress is the major cause for its parametric degradation. The condition of target capacitors can be estimated by monitoring variation in equivalent series resistance (ESR) from the initial pristine state value. ANN is used to estimate ESR of pristine and weak target capacitors at the test conditions. The data set for training and testing of proposed back-propagation trained artificial neural network are experimentally obtained from the developed test bed. Using the test bed, target capacitors are subjected to different operating frequency and temperature in the output section of DC/DC buck converter circuit to determine the effect of variation in electrical and thermal stress on ESR value. After off-line training, the proposed ANN is implemented using National Instruments LabVIEW software. A low cost microcontroller is programmed for real time data acquisition of target capacitors and the serial transmission of acquired dataset to the LabVIEW software installed at host computer. The performance of the proposed method is evaluated in real time by comparing the resulting ESR with the experimental values of in-circuit target capacitors. The proposed ANN, once trained properly, can be used for different circuits and in different operating conditions because of its generalization capability.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2017.08.036,Journal,Neurocomputing,scopus,2018-01-31,sciencedirect,Data-driven model-free slip control of anti-lock braking systems using reinforcement Q-learning,https://api.elsevier.com/content/abstract/scopus_id/85029168035,"This paper proposes the design and implementation of a model-free tire slip control for a fast and highly nonlinear Anti-lock Braking System (ABS). A reinforcement Q-learning optimal control approach is inserted in a batch neural fitted scheme using two neural networks to approximate the value function and the controller, respectively. The transition samples required for learning high performance control can be collected by interacting with the process either by online exploiting the current iteration controller (or policy) under an ε-greedy exploration strategy, or by using data collected under any other controller that is capable of ensuring efficient exploration of the action-state space. Both approaches are highlighted in the paper. Fortunately, the ABS process fits this type of learning-by-interaction because it does not need an initial stabilizing controller. The validation case studies conducted on a real laboratory setup reveal that high control system performance can be achieved using the proposed approaches. Insightful comments on the observed control behavior are offered along with performance comparisons with several types of model-based and model-free controllers including relay, model-based optimal PI, an original model-free neural network state-feedback VRFT controller and a model-free neural network adaptive actor-critic one. With the ability to improve control performance starting from different supervisory controllers or to learn high performance controllers from scratch, the proposed Q-learning optimal control approach proves its performance in a wide operating range and is therefore recommended to its industrial application on ABS.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.chemolab.2017.12.005,Journal,Chemometrics and Intelligent Laboratory Systems,scopus,2018-01-15,sciencedirect,A new reconstruction-based auto-associative neural network for fault diagnosis in nonlinear systems,https://api.elsevier.com/content/abstract/scopus_id/85037701407,"Auto-associative neural network (AANN) is a typical nonlinear principal component analysis method, which is widely used in industry for fault diagnosis purposes, especially in nonlinear systems. However, the basic AANN often suffers from “smearing effects” problems that may lead to misdiagnosis, particularly with regards to the complex faults involving multiple variables. In this work, a new reconstruction-based AANN (RBAANN) method is proposed to enhance the capacity of fault diagnosis. In RBAANN, a generic derivative equation is developed to investigate the effects of AANN model inputs on the prediction error between model inputs and outputs. Based on the derivative equation, the reconstruction-based index for single or multiple variables, which is defined as the minimum prediction error, is obtained by tuning the corresponding model inputs iteratively. However, without the prior knowledge of the real faulty variables, all the possible variable sets need to be evaluated by the reconstruction-based index, and this may result in an exhaustive search and cause a huge computational burden. Thus, a branch and bound algorithm is introduced into RBAANN to solve the variable selection problem. Finally, an efficient fault diagnosis strategy by integrating RBAANN and branch and bound algorithm (BAB-RBAANN) is implemented to further pinpoint the source of the detected faults. This BAB-RBAANN method can handle both single and multiple variable(s) faults for nonlinear systems without prior knowledge efficiently. The effectiveness of the proposed methods is evaluated on a validation example and an industrial example. Comparisons with other methods, including principal component analysis techniques, are also presented.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-813314-9.00011-6,Book,Computational Intelligence for Multimedia Big Data on the Cloud with Engineering Applications,scopus,2018-01-01,sciencedirect,Unsupervised anomaly detection for high dimensional data-An exploratory analysis,https://api.elsevier.com/content/abstract/scopus_id/85081928867,"Context: Anomaly detection is a crucial area engaging the attention of many researchers. It is a process of finding an unusual point or pattern in a given dataset. It is useful in many real time applications such as industry damage detection, detection of fraudulent usage of credit card, detection of failures in sensor nodes, detection of abnormal health and network intrusion detection. Algorithms proposed for anomaly detection in low dimensional data are not suitable for high dimensional data due to the well-known “dimensionality curses”.
               
                  Motivation: To tackle this issue, a plethora of algorithms dedicated to high dimensional data has been proposed. However, unsupervised algorithms have many problems and challenges, as there is no predefined data label to predict anomaly.
               
                  Objective: We aim at providing a complete view of unsupervised anomaly detection for high dimensional data which gives a clear perception of the concept.
               
                  Contribution: In this paper, existing algorithms and real time applications of unsupervised anomaly detection for high dimensional data have been studied. Evaluation measures, datasets and tools used by different authors have been discussed in detail. In addition, a hybrid framework of unsupervised anomaly detection algorithm called DBN–K means applied two different disease dataset is also proposed.
               
                  Future work: As future work, the proposed framework could be implemented and analyzed in other applications. High dimensional streaming data is another interesting area for further investigation, following this research work.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2018.01.036,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,"Intuitive robot programming through environment perception, augmented reality simulation and automated program verification",https://api.elsevier.com/content/abstract/scopus_id/85061975291,"The increasing complexity of products and machines as well as short production cycles with small lot sizes present great challenges to production industry. Both, the programming of industrial robots in online mode using hand-held control devices or in offline mode using text-based programming requires specific knowledge of robotics and manufacturer-dependent robot control systems. In particular for small and medium-sized enterprises the machine control software needs to be easy, intuitive and usable without time-consuming learning steps, even for employees with no in-depth knowledge of information technology. To simplify the programming of application programs for industrial robots, we extended a cloud-based, task-oriented robot control system with environment perception and plausibility check functions. For the environment perception a depth camera and pointcloud processing hardware were installed. We detect objects located in the robot’s workspace by pointcloud processing with ROS and the PCL and add them to the augmented reality user interface of the robot control. The combination of process knowledge from task-oriented application programming and information about available workpieces from automated image processing enables a plausibility check and verification of the robot program before execution. After a robot program has been approved by the plausibility check, it is tested in an augmented reality simulation for collisions with the detected objects before deployment to the physical robot hardware. Experiments were carried out to evaluate the effectiveness of the developed extensions and confirmed their functionality.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2018.09.067,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,A Conceptual Design for Smell Based Augmented Reality: Case Study in Maintenance Diagnosis,https://api.elsevier.com/content/abstract/scopus_id/85059916374,"The trend of Industry 4.0 encourages the next generation of manufacturing to be flexible, intelligent, and interoperable. The implementations of the Artificial Intelligence (AI) technology could potentially enhance maintenance in efficiency, and accuracy. However, it will not be a substitution to the human operator’s flexibility, decision-making and information received by the natural five senses. Augmented reality (AR) is commonly understood as a technology that overlays virtual information onto the existing environment to provide users a new and improved experience to assist their daily activities. However, AR can be used to enhance all human five senses rather than just overlay virtual imagery. In this paper, a design and a practical plan of smell augmentation for diagnosis is initialised, via a case study in maintenance. The aim of this paper is to evaluate the feasibilities, identify challenges, and summarise initial results of overlaying information through smell augmentations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2018.04.009,Conference Proceeding,Procedia Manufacturing,scopus,2018-01-01,sciencedirect,Mixed Reality in Learning Factories,https://api.elsevier.com/content/abstract/scopus_id/85052906978,"Supported by rapid technological development, mixed reality (MR) applications are increasingly deployed in industrial practice. In manufacturing, MR can be utilized for information visualization, remote collaboration, human-machine-interfaces, design tools and education and training. This development makes new demands on learning factories in two major fields: One is the empowerment of users to work with MR in industrial applications. The second field is the utilization of the potential of MR for teaching and learning in learning factories. A great potential lies in the new possibilities of connecting digital content with the physical world. To analyze the potential applications of MR in learning factories in a structured way, an overview of potential MR applications based on the reality-virtuality continuum is presented with an analysis of case studies of applications in a learning factory including a mixed-reality-hackathon.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2018.04.026,Conference Proceeding,Procedia Manufacturing,scopus,2018-01-01,sciencedirect,Design and implementation of a low cost RFID track and trace system in a learning factory,https://api.elsevier.com/content/abstract/scopus_id/85052890798,"The factories of the future will make use of actuators, sensors and cyber-physical systems (CPS) to provide an environment in which human beings, machines, and resources will communicate as in a social network. In such a network, communication between various “objects” relay the current state of the physical world. Business decisions are made using the information and it is therefore critical that this information is accurate and in real-time. Information flow is a key enabler of such future factories. Industrial engineers, as designers and improvement agents of such factories of the future, will need to develop better skills in various aspects of data analytics and information communication technologies. This paper describes the development and implementation of a low cost RFID track and trace system (by students) for application in a Learning Factory for teaching undergraduate industrial engineering students key concepts related to Industry 4.0 and “smart factories”. The benefit of this system is not only a demonstrator to be used in the Learning Factory, but also can be used to teach students in a “learning by doing” fashion critical skills related to real time tracking in a manufacturing environment. The system also demonstrates potential low cost implementation of such technologies in SME’s.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2018.08.421,Conference Proceeding,,scopus,2018-01-01,sciencedirect,A Multi Agent System architecture to implement Collaborative Learning for social industrial assets,https://api.elsevier.com/content/abstract/scopus_id/85052888258,"The ‘Industrial Internet of Things’ aims to connect industrial assets with one another and benefit from the data that is generated, and shared, among these assets. In recent years, the extensive instrumentation of machines and the advancements in Information Communication Technologies are re-shaping the role of assets in our industrial systems. An emerging concept here is that of ‘social assets’: assets that collaborate with each other in order to improve system optimisation. Cyber-Physical Systems (CPSs) are formed by embedding the assets with computers, or microcontrollers, which run real-time decision-making algorithms over the data originating from the asset. These are known as the ‘Digital Twins’ of the assets, and form the backbone of social assets. It is essential to have an architecture which enables a seamless integration of these technological advances for an industry. This paper proposes a Multi Agent System (MAS) architecture for collaborative learning, and presents the findings of an implementation of this architecture for a prognostics problem. Collaboration among assets is performed by calculating inter-asset similarity during operating condition to identify ‘friends’ and sharing operational data within these clusters of friends. The architecture described in this paper also presents a generic model for the Digital Twins of assets. Prognostics is demonstrated for the C-MAPSS turbofan engine degradation simulated data-set (Saxena and Goebel (2008)).",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2018.07.108,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Ambience Inhaling: Speech Noise Inhaler in Mobile Robots using Deep Learning,https://api.elsevier.com/content/abstract/scopus_id/85051344062,"Audio based, machine learning human-computer interface with speech recognition systems performs sensibly well with the human voice under clean ambience, but become frail in applied technological implementation involving real-life interface. In mobile robotic systems, the speech machines are normally retrained with new changing acoustic ambience conditions are to be met. To inhale, classify, and track the real-world ambience noise with the new changing acoustic condition, we introduce an Ambience Inhaling (AI) framework in this article. This framework of an AI is to seek out complete noise information from speech data, in contrast with noise-nature discovery. Our proposed framework uses a deep convolutional neural network (CNN) based learning for classification with speech spectrogram patch segments, including a hybrid Harold Hotelling's T-square algorithm with Bayesian statistics for segmentation analysis. We use a symposium presentation-ambience as a test platform. In the symposium presentation-ambience, noise modeling is done with n-gram language having the parameter of n = 2. The impulsive or short-term noise which is superimposed with long-term noise caused degradation in classification. This degradation caused the classification errors. The provision of decision was made. The Gaussian mixture model and hidden Markova model are used with noise-only and noisy speech respectively. Time and frequency pooling are used with spectrogram also. The classification scores of 62.26%, 65.89%, and 69.12% are achieved with 5, 10 and 15 CNN filters respectively. As a significance, an AI is efficient and innovative.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-444-64241-7.50087-2,Book Series,Computer Aided Chemical Engineering,scopus,2018-01-01,sciencedirect,Reinforcement Learning Applied to Process Control: A Van der Vusse Reactor Case Study,https://api.elsevier.com/content/abstract/scopus_id/85050599810,"With recent advances in industrial automation, data acquisition, and successful applications of Machine Learning methods to real-life problems, data-based methods can be expected to grow in use within the process control community in the near future. Model-based control methods rely on accurate models of the process to be effective. However, such models may be laborious to obtain and, even when available, the optimization problem underlying the online control problem may be too computationally demanding. Furthermore, the process degradation with time imposes that the model should be periodically updated to stay reliable. One way to address these drawbacks is through the merging of Reinforcement Learning (RL) techniques into the classical process control framework. In this work, a methodology to tackle the control of nonlinear chemical processes with RL techniques is proposed and tested on the wellknown benchmark problem of the non-isothermal CSTR with the Van de Vusse reaction. The controller proposed herein is based on the implementation of a policy that associates each state of the process to a certain control action. This policy is directly deduced from a measure of the expected performance gain, given by a value function dependent on the states and actions. In other words, in a given state, the action that provides the highest expected performance gain is chosen and implemented. The value function is approximated by a neural network that can be trained with pre-simulated data and adapted online with the continuous inclusion of new process data through the implementation of an RL algorithm. The results show that the proposed adaptive RLbased controller successfully manages to control and optimize the Van de Vusse reactor against unmeasured disturbances.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2018.06.356,Conference Proceeding,,scopus,2018-01-01,sciencedirect,Design Principles Behind the Construction of an Autonomous Laboratory-Scale Drilling Rig,https://api.elsevier.com/content/abstract/scopus_id/85050080748,"In recent years, hot topics such as digitalization, machine learning, digital twin and big data have evolved from being envisions on the paper to state of art solutions, expected to revolutionize drilling efficiency in the industry. Drilling automation tomorrow is all about exploiting the current state of technologies available to the entire operation of drilling a well. Not only can drilling automation limit costs and reduce the risk to rig personnel and the environment, but they also give access to locations of considerable potential that previously have been regarded unsafe or uneconomical to operate in. There are however some challenges in keeping up with the ever-increasing pace of the development. For one, testing of novel and innovative solutions is often very expensive because of non-productive rig time during implementation, trial runs and data evaluation. Also, the modern technologies require extensive R&D before on-site testing can even commence. While on land-rigs, some of these costs and risks can be greatly minimized, many offshore solutions lack that luxury. This paper presents an overview of the design principles that go into the construction of a fully autonomous laboratory-scale drilling rig at the University of Stavanger. It aims at describing 1) the engineering principles involved to resemble full-scale drilling operations on the laboratory scale, 2) design considerations and components, 3) component requirements for the rig, 4) control system algorithms for real-time optimization of drilling parameters and detection and handling of drilling anomalies, 5) development of drilling models (drill string dynamics, bit-vibration, etc.) and 6) benefits and future work with the laboratory-scale system. Some of the concepts that are presented in this paper have yet to be implemented during 2018.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2018.03.168,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,Reinforcement learning in real-time geometry assurance,https://api.elsevier.com/content/abstract/scopus_id/85049605436,"To improve the assembly quality during production, expert systems are often used. These experts typically use a system model as a basis for identifying improvements. However, since a model uses approximate dynamics or imperfect parameters, the expert advice is bound to be biased. This paper presents a reinforcement learning agent that can identify and limit systematic errors of an expert systems used for geometry assurance. By observing the resulting assembly quality over time, and understanding how different decisions affect the quality, the agent learns when and how to override the biased advice from the expert software.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2018.03.022,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,Fostering Robust Human-Robot Collaboration through AI Task Planning,https://api.elsevier.com/content/abstract/scopus_id/85049587790,"Recent advances in Artificial Intelligence (AI) are facilitating the deployment of intelligent systems in manufacturing. In Human-Robot Collaboration (HRC), industrial robots offer accuracy and efficiency while humans guarantee both experience and specialized and not replaceable skills. The seamless coordination of such different abilities constitutes one of the current challenges. This paper presents a dynamic task sequencing system for robust HRC developed within a EU-funded project. The proposed solution uses AI techniques to deal with the temporal variance entailed by the active presence of humans as well as to dynamically adapt task plans according to actual behavior of the pair human-worker/robot. The tool has been deployed in a real pilot plant.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2018.05.142,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Optimization of Software Testing,https://api.elsevier.com/content/abstract/scopus_id/85049103381,"The goal of any business is to satisfy the needs of its target customers, and IT industry is not an exception from that rule. Thus, the upgraded version of the V-model testing is supposed to deal with the weaknesses of the original version in question by combining it with the method known as agile testing. At the beginning of the report, hypothesis such as the strengths and weaknesses of the existing V-model testing via literature review and interviews with respective specialists in the sphere were analysed. Successively, the possible advantages of agile method of testing were then considered. Moreover, the report comes up with the ways in which the two models could be naturally combined to produce a much more effective one. Once the new model was presented, its strengths and weaknesses were assessed by the means of a case study analysis using metric and a data analysis through a survey were conducted to evaluate the credibility of the futurist model. Promptly, the research found that the suggested testing model provides better results than the common version of V-model testing. Firstly, a real case scenarios under metric evaluation of the models have indicated that the proposed model is better than the V-model, since it can handle the following aspects; reduced testing time, debugging, prioritization of requirements, easy mapping of roles and improved visibility of project resources. Secondly, a survey data analysis highlighted various advantages of the future model. The top priorities of the new model from the respondent’s perception were; the new model manages rapidly changing priorities, it accelerates time to market, it increases productivity and it improves quality.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2018.05.113,Conference Proceeding,Procedia Computer Science,scopus,2018-01-01,sciencedirect,Real Time High Performance of Sliding Mode Controlled Induction Motor Drives,https://api.elsevier.com/content/abstract/scopus_id/85049099142,"Several industrial applications demand high performance speed functioning and require new control techniques so as to ensure a fast dynamic response. The present work investigates real time implementation and experimental sliding mode controlled (SMC) induction motor drives (IM). The strategy of sliding mode control is a powerful tool to ensure robustness. Nevertheless, the chattering phenomenon is a major disadvantage for non linear systems. For this purpose, two different types of analysis such as layer boundary methods are implemented in dSPACE 1104 controller board and compared between them in order to obtain the best method to reduce or eliminate chattering phenomenon. An experimental results using dSPACE 1104 based on TMS320F240 DSP are described in this work.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cirp.2018.04.041,Journal,CIRP Annals,scopus,2018-01-01,sciencedirect,Reinforcement learning for adaptive order dispatching in the semiconductor industry,https://api.elsevier.com/content/abstract/scopus_id/85045954603,"The digitalization of production systems tends to provide a huge amount of data from heterogeneous sources. This is particularly true for the semiconductor industry wherein real time process monitoring is inherently required to achieve a high yield of good parts. An application of data-driven algorithms in production planning to enhance operational excellence for complex semiconductor production systems is currently missing. This paper shows the successful implementation of a reinforcement learning-based adaptive control system for order dispatching in the semiconductor industry. Furthermore, a performance comparison of the learning-based control system with the traditionally used rule-based system shows remarkable results. Since a strict rulebook does not bind the learning-based control system, a flexible adaption to changes in the environment can be achieved through a combination of online and offline learning.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2017.12.230,Conference Proceeding,Procedia CIRP,scopus,2018-01-01,sciencedirect,A Conceptual Model for Developing a Smart Process Control System,https://api.elsevier.com/content/abstract/scopus_id/85044679399,"Current Manufacturing Execution Systems (MES) are not supporting a full integration into overall processes across the supply chain. Thus, optimization is limited to single areas. The SemI40 project is aimed at developing an integrated concept of Smart Process Control System (SPCS), which enhances the overall agility and productivity. The system, therefore, autonomously acquires and interprets process data to allow product individual optimization and enhancing logistics management. It also provides full traceability across the supply chain in real-time and allows model based process simulation and decision making support. The concept is developed based on requirements elicitation in cooperation with industry partners and combines state-of-the-art technologies with current trends, like vertical integration, big data and machine learning.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.mfglet.2017.12.013,Journal,Manufacturing Letters,scopus,2018-01-01,sciencedirect,Artificial neural network based framework for cyber nano manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85042371124,"Nanomanufacturing plays an important role for high performance products in several applications. The challenge for fabricating products with nanomaterials is the inability to interconnect and interface with nano/micro manufacturing equipment. This paper presents a framework for cyber nanomanufacturing. Input part designs of nano/micro scale components are evaluated with an artificial neural network (ANN) based smart agent to predict optimal nanomanufacturing processes. An internet-of-things (IoT) based cyber-interface simulator is implemented to simulate real-time machine availability. Further, an application program interface (API) is developed to integrate the ANN smart agent and IoT simulator outcomes to predict dynamic machine allocations in real-time.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.addma.2017.11.009,Journal,Additive Manufacturing,scopus,2018-01-01,sciencedirect,Anomaly detection and classification in a laser powder bed additive manufacturing process using a trained computer vision algorithm,https://api.elsevier.com/content/abstract/scopus_id/85035797198,"Despite the rapid adoption of laser powder bed fusion (LPBF) Additive Manufacturing by industry, current processes remain largely open-loop, with limited real-time monitoring capabilities. While some machines offer powder bed visualization during builds, they lack automated analysis capability. This work presents an approach for in-situ monitoring and analysis of powder bed images with the potential to become a component of a real-time control system in an LPBF machine. Specifically, a computer vision algorithm is used to automatically detect and classify anomalies that occur during the powder spreading stage of the process. Anomaly detection and classification are implemented using an unsupervised machine learning algorithm, operating on a moderately-sized training database of image patches. The performance of the final algorithm is evaluated, and its usefulness as a standalone software package is demonstrated with several case studies.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ins.2017.09.027,Journal,Information Sciences,scopus,2018-01-01,sciencedirect,Incorporating negative information to process discovery of complex systems,https://api.elsevier.com/content/abstract/scopus_id/85029528097,"The discovery of a formal process model from event logs describing real process executions is a challenging problem that has been studied from several angles. Most of the contributions consider the extraction of a model as a one-class supervised learning problem where only a set of process instances is available. Moreover, the majority of techniques cannot generate complex models, a crucial feature in some areas like manufacturing. In this paper we present a fresh look at process discovery where undesired process behaviors can also be taken into account. This feature may be crucial for deriving process models which are less complex, fitting and precise, but also good on generalizing the right behavior underlying an event log. The technique is based on the theory of convex polyhedra and satisfiability modulo theory (SMT) and can be combined with other process discovery approach as a post processing step to further simplify complex models. We show in detail how to apply the proposed technique in combination with a recent method that uses numerical abstract domains. Experiments performed in a new prototype implementation show the effectiveness of the technique and the ability to be combined with other discovery techniques.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.biosystems.2017.10.001,Journal,BioSystems,scopus,2017-12-01,sciencedirect,Towards a first implementation of the WLIMES approach in living system studies advancing the diagnostics and therapy in personalized medicine,https://api.elsevier.com/content/abstract/scopus_id/85033459793,"The goal of this paper is to advance an extensible theory of living systems using an approach to biomathematics and biocomputation that suitably addresses self-organized, self-referential and anticipatory systems with multi-temporal multi-agents. Our first step is to provide foundations for modelling of emergent and evolving dynamic multi-level organic complexes and their sustentative processes in artificial and natural life systems. Main applications are in life sciences, medicine, ecology and astrobiology, as well as robotics, industrial automation, man-machine interface and creative design. Since 2011 over 100 scientists from a number of disciplines have been exploring a substantial set of theoretical frameworks for a comprehensive theory of life known as Integral Biomathics. That effort identified the need for a robust core model of organisms as dynamic wholes, using advanced and adequately computable mathematics. The work described here for that core combines the advantages of a situation and context aware multivalent computational logic for active self-organizing networks, Wandering Logic Intelligence (WLI), and a multi-scale dynamic category theory, Memory Evolutive Systems (MES), hence WLIMES. This is presented to the modeller via a formal augmented reality language as a first step towards practical modelling and simulation of multi-level living systems. Initial work focuses on the design and implementation of this visual language and calculus (VLC) and its graphical user interface. The results will be integrated within the current methodology and practices of theoretical biology and (personalized) medicine to deepen and to enhance the holistic understanding of life.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.simpat.2015.07.004,Journal,Simulation Modelling Practice and Theory,scopus,2017-12-01,sciencedirect,A reinforcement learning methodology for a human resource planning problem considering knowledge-based promotion,https://api.elsevier.com/content/abstract/scopus_id/84939200331,"This paper addresses a combined problem of human resource planning (HRP) and production-inventory control for a high-tech industry, wherein the human resource plays a critical role. The main characteristics of this resource are the levels of “knowledge” and the learning process. The learning occurs during the production process in which a worker can promote to the upper knowledge level. Workers in upper levels have more productivity in the production. The objective is to maximize the expected profit by deciding on the optimal numbers of workers in various knowledge levels to fulfill both production and training requirement. As taking an action affects next periods’ decisions, the main problem is to find the optimal hiring policy of non-skilled workers in long-time horizon. Thus, we develop a reinforcement learning (RL) model to obtain the optimal decision for hiring workers under the demand uncertainty. The proposed interval-based policy of our RL model, in which for each state there are multiple choices, makes it more flexible. We also embed some managerial issues such as layoff and overtime-working hours into the model. To evaluate the proposed methodology, stochastic dynamic programming (SDP) and a conservative method implemented in a real case study are used. We study all these methods in terms of four criteria: average obtained profit, average obtained cost, the number of new-hired workers, and the standard deviation of hiring policies. The numerical results confirm that our developed method end up with satisfactory results compared to two other approaches.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2017.06.014,Journal,Engineering Applications of Artificial Intelligence,scopus,2017-11-01,sciencedirect,A configurable partial-order planning approach for field level operation strategies of PLC-based industry 4.0 automated manufacturing systems,https://api.elsevier.com/content/abstract/scopus_id/85030713189,"The machine and plant automation domain is faced with an ever increasing demand for ensuring the adaptability of manufacturing facilities in context of Industry 4.0. Field level automation software plays a dominant role in strengthening the overall flexibility of manufacturing resources. Classical programming approaches based typically on signal-oriented languages result in disproportionate effort for ensuring necessary flexibility. To address this challenge, a novel approach based on artificial intelligence planning techniques is presented which is able to handle domain specific requirements while facilitating efficient, scalable problem solving. Throughout this article, a discussion of specific requirements on automated planning techniques for field level automation software in the machine and plant automation domain with respect to Industry 4.0 is provided. An intensive study on existing works and their drawbacks towards addressing these requirements is presented. The proposed configurable partial-order planning approach is based upon a combination of an adapted goal-based planning formulation and its reformulation by means of linear programming techniques. It is shown that the proposed approach is able to efficiently solve large planning problems by exhibiting positive scalability characteristics which indicates its applicability for real-size plants.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2017.09.016,Journal,Computers and Industrial Engineering,scopus,2017-11-01,sciencedirect,Smart operators in industry 4.0: A human-centered approach to enhance operators’ capabilities and competencies within the new smart factory context,https://api.elsevier.com/content/abstract/scopus_id/85029476237,"As the Industry 4.0 takes shape, human operators experience an increased complexity of their daily tasks: they are required to be highly flexible and to demonstrate adaptive capabilities in a very dynamic working environment. It calls for tools and approaches that could be easily embedded into everyday practices and able to combine complex methodologies with high usability requirements. In this perspective, the proposed research work is focused on the design and development of a practical solution, called Sophos-MS, able to integrate augmented reality contents and intelligent tutoring systems with cutting-edge fruition technologies for operators’ support in complex man-machine interactions. After establishing a reference methodological framework for the smart operator concept within the Industry 4.0 paradigm, the proposed solution is presented, along with its functional and non-function requirements. Such requirements are fulfilled through a structured design strategy whose main outcomes include a multi-layered modular solution, Sophos-MS, that relies on Augmented Reality contents and on an intelligent personal digital assistant with vocal interaction capabilities. The proposed approach has been deployed and its training potentials have been investigated with field experiments. The experimental campaign results have been firstly checked to ensure their statistical relevance and then analytically assessed in order to show that the proposed solution has a real impact on operators’ learning curves and can make the difference between who uses it and who does not.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.knosys.2017.07.007,Journal,Knowledge-Based Systems,scopus,2017-10-01,sciencedirect,Fit evaluation of virtual garment try-on by learning from digital pressure data,https://api.elsevier.com/content/abstract/scopus_id/85022033035,"Presently, garment fit evaluation mainly focuses on real try-on, and rarely deals with virtual try-on. With the rapid development of E-commerce, there is a profound growth of garment purchases through the internet. In this context, fit evaluation of virtual garment try-on is vital in the clothing industry. In this paper, we propose a Naive Bayes-based model to evaluate garment fit. The inputs of the proposed model are digital clothing pressures of different body parts, generated from a 3D garment CAD software; while the output is the predicted result of garment fit (fit or unfit). To construct and train the proposed model, data on digital clothing pressures and garment real fit was collected for input and output learning data respectively. By learning from these data, our proposed model can predict garment fit rapidly and automatically without any real try-on; therefore, it can be applied to remote garment fit evaluation in the context of e-shopping. Finally, the effectiveness of our proposed method was validated using a set of test samples. Test results showed that digital clothing pressure is a better index than ease allowance to evaluate garment fit, and machine learning-based garment fit evaluation methods have higher prediction accuracies.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eneco.2017.06.020,Journal,Energy Economics,scopus,2017-08-01,sciencedirect,"Composite forecasting approach, application for next-day electricity price forecasting",https://api.elsevier.com/content/abstract/scopus_id/85024479867,"Accurate forecasting of electricity prices can provide significant benefits to energy suppliers when allocating their assets and to energy consumers for defining an optimal portfolio. There are numerous methods that efficiently support the forecasting of time series, such as electricity prices, which have high volatility. However, the performance of these approaches varies depending on data sets and operational conditions. In this work, the concept of composite forecasting is presented and implemented in a retrospective study, in real industrial forecasting conditions to show the potential of forecast performance improvement and comparable high consistency of a forecast performance across different ‘Day Peak’ and ‘Day Base’ electricity price data sets for different seasons. As individual methods support vector regression, artificial neural networks and ridge regression are implemented. The forecast performances of these methods are evaluated and compared with their forecast combination using different error measures. The results show that composite forecasting processes with ‘inverse root mean squared error’ combination approach can generate, on average, a more accurate and robust forecast than using an individual methods or other combination schemas.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.mineng.2017.02.013,Journal,Minerals Engineering,scopus,2017-08-01,sciencedirect,Designing gold extraction processes: Performance study of a case-based reasoning system,https://api.elsevier.com/content/abstract/scopus_id/85014728806,"This paper presents a method for externalising and formalising knowledge involving the selection of hydrometallurgical process flowsheets for gold extraction from ores. A case-based reasoning (CBR) system was built using an open source software myCBR 3.0. The aim of the systems is to recommend flowsheet alternatives for processing a potential gold ore deposit. Nine attributes: Ore type, Gold ore grade, Gold distribution, Gold grain size, Sulfide present, Arsenic sulfide, Copper sulfide, Iron sulfide and Clay present were modelled and several literature sources of actual gold mines and processes were used for acquiring cases for the system. After preliminary testing, functional evaluation of the built CBR system was carried out by using five real mining projects as test cases. Additionally, human experts in the field of gold hydrometallurgy were interviewed to demonstrate the benefits of the CBR system as it holds no human biases towards any processing techniques. It was found that the suggestions of the CBR system provided useful information and direction for further process design and performed well compared to the interviewed human experts, thus confirming that the system is of practical relevance to the process engineer designing an industrial gold processing plant. The current model was found to be a functioning basis for further development through additional attributes, adjusted attribute weighting and increased number of cases.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2017.08.986,Conference Proceeding,IFAC-PapersOnLine,scopus,2017-07-01,sciencedirect,Using data mining methods for manufacturing process control,https://api.elsevier.com/content/abstract/scopus_id/85031805594,"The Industry 4.0 concept assumes that modern manufacturing systems generate huge amounts of data that must be collected, stored, managed and analysed. The case study is focused on predicting the manufacturing process behaviour according to production data. The paper presents the way of gaining knowledge about the future behaviour of manufacturing system by data mining predictive tasks. The proposed simulation model of the real manufacturing process was designed to obtain the data necessary for the control process. The predictions of the manufacturing process behaviour were implemented varying the input parameters using selected methods and techniques of data mining. The predicted process behaviour was verified using the simulation model.
                  The authors analysed different methods. The neural network method was selected for deploying new data by PMML files in the final phases. The objectives of the research are to design and verify the data mining tools in order to support the manufacturing system control by aiming at improving the decisionmaking process. Based on the prediction of the goal production outcomes, the actual control strategies can be precisely modified. Then they can be used in real manufacturing system without risks.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2017.08.902,Conference Proceeding,IFAC-PapersOnLine,scopus,2017-07-01,sciencedirect,A Networked Production System to Implement Virtual Enterprise and Product Lifecycle Information Loops,https://api.elsevier.com/content/abstract/scopus_id/85031797675,"This paper is aimed at considering supply chain and related data management within an integrated vision of the product lifecycle management (PLM) implemented through the unified approach which is proper to the Industry 4.0 initiative. In particular, with the proposed manufacturing system architecture, decision support tools can use a unified repository fed by a factory replication application, powered by data from the field, even from remote production units. Such data allow to monitor the behaviour of the digital twin of the real machine and produces a digital twin of the real product, incorporating its actual characteristics measured by means of suitable acquiring systems (in the treated example: a 3D laser scanner). Moreover, it is provided a description of the plant technological subsystems that allow to share designing and manufacturing activities across multiple similar units located in remote areas. In this context of virtual enterprise, the supply chain management results as a key factor in enabling a cooperative approach.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2017.03.003,Journal,Information and Software Technology,scopus,2017-07-01,sciencedirect,Uncertainty-wise evolution of test ready models,https://api.elsevier.com/content/abstract/scopus_id/85015382293,"Context
                  Cyber-Physical Systems (CPSs), when deployed for operation, are inherently prone to uncertainty. Considering their applications in critical domains (e.g., healthcare), it is important that such CPSs are tested sufficiently, with the explicit consideration of uncertainty. Model-based testing (MBT) involves creating test ready models capturing the expected behavior of a CPS and its operating environment. These test ready models are then used for generating executable test cases. It is, therefore, necessary to develop methods that can continuously evolve, based on real operational data collected during the operation of CPSs, test ready models and uncertainty captured in them, all together termed as Belief Test Ready Models (BMs)
               
                  Objective
                  Our objective is to propose a model evolution framework that can interactively improve the quality of BMs, based on operational data. Such BMs are developed by one or more test modelers (belief agents) with their assumptions about the expected behavior of a CPS, its expected physical environment, and potential future deployments. Thus, these models explicitly contain subjective uncertainty of the test modelers.
               
                  Method
                  We propose a framework (named as UncerTolve) for interactively evolving BMs (specified with extended UML notations) of CPSs with subjective uncertainty developed by test modelers. The key inputs of UncerTolve include initial BMs of CPSs with known subjective uncertainty and real data collected from the operation of CPSs. UncerTolve has three key features: 1) Validating the syntactic correctness and conformance of BMs against real operational data via model execution, 2) Evolving objective uncertainty measurements of BMs via model execution, and 3) Evolving state invariants (modeling test oracles) and guards of transitions (modeling constraints for test data generation) of BMs with a machine learning technique.
               
                  Results
                  As a proof-of-concept, we evaluated UncerTolve with one industrial CPS case study, i.e., GeoSports from the healthcare domain. Using UncerTolve, we managed to evolve 51% of belief elements, 18% of states, and 21% of transitions as compared to the initial BM developed in an industrial setting.
               
                  Conclusion
                  
                     UncerTolve can successfully evolve model elements of the initial BM, in addition to objective uncertainty measurements using real operational data. The evolved model can be used to generate additional test cases covering evolved model elements and objective uncertainty. These additional test cases can be used to test the current and future deployments of a CPS to ensure that it will handle uncertainty gracefully during its operations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2016.08.019,Journal,Engineering Applications of Artificial Intelligence,scopus,2017-06-01,sciencedirect,GPU-based parallel optimization of immune convolutional neural network and embedded system,https://api.elsevier.com/content/abstract/scopus_id/84995489085,"Up to now, the image recognition system has been utilized more and more widely in the security monitoring, the industrial intelligent monitoring, the unmanned vehicle, and even the space exploration. In designing the image recognition system, the traditional convolutional neural network has some defects such as long training time, easy over-fitting and high misclassification rate. In order to overcome these defects, we firstly used the immune mechanism to improve the convolutional neural network and put forward a novel immune convolutional neural network algorithm, after we analyzed the network structure and parameters of the convolutional neural network. Our algorithm not only integrated the location data of the network nodes and the adjustable parameters, but also dynamically adjusted the smoothing factor of the basis function. In addition, we utilized the NVIDIA GPU (Graphics Processing Unit) to accelerate the new immune convolutional neural network (ICNN) in parallel computing and built a real-time embedded image recognition system for this ICNN. The immune convolutional neural network algorithm was improved with CUDA programming and was tested with the sample data in the GPU-based environment. The GPU-based implementation of the novel immune convolutional neural network algorithm was made with the cuDNN, which was designed by NVIDIA for GPU-based accelerating of DNNs in machine learning. Experimental results show that our new immune convolutional neural network has higher recognition rate, more stable performance and faster computing speed than the traditional convolutional neural network.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2016.09.005,Journal,Neurocomputing,scopus,2017-05-10,sciencedirect,Wood moisture content prediction using feature selection techniques and a kernel method,https://api.elsevier.com/content/abstract/scopus_id/84996497604,"Wood is a renewable, abundant bio-energy and environment friendly resource. Woody biomass Moisture Content (
                        MC
                     ) is a key parameter for controlling the biofuel product qualities and properties. In this paper, we are interested in predicting 
                        MC
                      from data. The input impedance of half-wave dipole antenna when buried in the wood pile varies according to the permittivity of wood. Hence, the measurement of reflection coefficient, that gives information about the input impedance, depends directly on the 
                        MC
                      of wood. The relationship between the reflection coefficient measurements and the 
                        MC
                      is studied. Based upon this relationship, 
                        MC
                      predictive models that use machine learning techniques and feature selection methods are proposed. Numerical experiments using real world data show the relevance of the proposed approach that requires a limited computational power. Therefore, a real-time implementation for industrial processes is feasible.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmsy.2017.02.007,Journal,Journal of Manufacturing Systems,scopus,2017-04-01,sciencedirect,Framework and development of fault detection classification using IoT device and cloud environment,https://api.elsevier.com/content/abstract/scopus_id/85014081409,"While Cyber-physical system (CPS) is considered as a key foundation for cyber manufacturing, many related frameworks and applications have been provided. This research suggests a new and effective CPS architecture for supporting multi-sites and multi-products manufacturing. As target processes, the manufacturing processes for vehicles’ High Intensity Discharge (HID) headlight and cable modules are considered. These modules are manufactured with several multi-manufacturing sites consisting of internal manufacturing tasks and intermediate outsourcing processes. In addition, they produce multiple types of HID cable modules with different components. These issues make it difficult to improve the qualities of the overall processes and to control those considering overall manufacturing plants and processes. In order to overcome these limitations, this research provides an Internet of Things (IoT) embedded cloud control architecture. The mixed flow issues are overcome with the cloud control server with the suggested framework. The developed IoT device detects several system status and transmits the signals. The data is analyzed for the fault detection classification (FDC) mechanism using deep learning based analytics. Then, the cyber manufacturing based simulation is executed using the provided multi-products queueing network model. The estimated simulation results is used for generating dynamic manufacturing decisions reflecting the real-time changes of the production environment. The suggested framework and its implementations can be used for various industrial processes and applications.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmsy.2017.02.011,Journal,Journal of Manufacturing Systems,scopus,2017-04-01,sciencedirect,A fog computing-based framework for process monitoring and prognosis in cyber-manufacturing,https://api.elsevier.com/content/abstract/scopus_id/85013912214,"Small- and medium-sized manufacturers, as well as large original equipment manufacturers (OEMs), have faced an increasing need for the development of intelligent manufacturing machines with affordable sensing technologies and data-driven intelligence. Existing monitoring systems and prognostics approaches are not capable of collecting the large volumes of real-time data or building large-scale predictive models that are essential to achieving significant advances in cyber-manufacturing. The objective of this paper is to introduce a new computational framework that enables remote real-time sensing, monitoring, and scalable high performance computing for diagnosis and prognosis. This framework utilizes wireless sensor networks, cloud computing, and machine learning. A proof-of-concept prototype is developed to demonstrate how the framework can enable manufacturers to monitor machine health conditions and generate predictive analytics. Experimental results are provided to demonstrate capabilities and utility of the framework such as how vibrations and energy consumption of pumps in a power plant and CNC machines in a factory floor can be monitored using a wireless sensor network. In addition, a machine learning algorithm, implemented on a public cloud, is used to predict tool wear in milling operations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.flowmeasinst.2016.10.001,Journal,Flow Measurement and Instrumentation,scopus,2017-04-01,sciencedirect,Intelligent recognition of gas-oil-water three-phase flow regime and determination of volume fraction using radial basis function,https://api.elsevier.com/content/abstract/scopus_id/85000420282,"The problem of how to accurately measure the flow rate of oil–gas–water mixtures in a pipeline remains one of the key challenges in the petroleum industry. This paper proposes a new methodology for identifying flow regimes and predicting volume fractions in gas-oil-water multiphase systems using dual energy fan-beam gamma-ray attenuation technique and artificial neural networks. The novelty of this study in comparison with previous works, is using just 4 extracted features (photo peaks of 241Am and 137Cs in 2 detectors) from the gamma ray spectrums instead of using the whole gamma ray spectrum, which reduces the undesired noises and also improves the speed of recognition in real situations. Radial basis function was used for developing the neural network model in MATLAB software in order to classify the flow patterns (annular, stratified and homogenous) and predict the value of volume fractions. The ideal and static theoretical models for flow regimes have been developed using MCNP-X code. The proposed networks could correctly recognize all the three different flow regimes and also determine volume fractions with mean absolute error of less than 5.68% according to the recognized regime.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compositesb.2016.12.050,Journal,Composites Part B: Engineering,scopus,2017-03-01,sciencedirect,Digitisation of manual composite layup task knowledge using gaming technology,https://api.elsevier.com/content/abstract/scopus_id/85009923562,"Increased market demand for composite products and shortage of expert laminators is compelling the composite industry to explore ways to acquire layup skills from experts and transfer them to novices and eventually to machines. There is a lack of holistic methods in literature for capturing composite layup skills especially involving complex moulds. This research aims to develop an informatics-based method, enabled by consumer-grade gaming technology and machine learning, to capture and digitise manufacturing task knowledge from skill-intensive hand layup. The digitisation is underpinned by the proposed human-workpiece interaction theory and implemented to automatically extract and decode key knowledge constituents such as layup strategies, ply manipulation techniques, motion mechanics and problem-solving during hand layup, collectively categorised as layup skills. The significance of this research is its potential to facilitate cost-effective transfer of skills from experts to novices, real-time automated supervision of hand layup and automation of layup tasks in the future.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jenvman.2016.10.056,Journal,Journal of Environmental Management,scopus,2017-02-01,sciencedirect,Improving the efficiency of dissolved oxygen control using an on-line control system based on a genetic algorithm evolving FWNN software sensor,https://api.elsevier.com/content/abstract/scopus_id/85006851639,"This work proposes an on-line hybrid intelligent control system based on a genetic algorithm (GA) evolving fuzzy wavelet neural network software sensor to control dissolved oxygen (DO) in an anaerobic/anoxic/oxic process for treating papermaking wastewater. With the self-learning and memory abilities of neural network, handling the uncertainty capacity of fuzzy logic, analyzing local detail superiority of wavelet transform and global search of GA, this proposed control system can extract the dynamic behavior and complex interrelationships between various operation variables. The results indicate that the reasonable forecasting and control performances were achieved with optimal DO, and the effluent quality was stable at and below the desired values in real time. Our proposed hybrid approach proved to be a robust and effective DO control tool, attaining not only adequate effluent quality but also minimizing the demand for energy, and is easily integrated into a global monitoring system for purposes of cost management.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.simpat.2016.08.007,Journal,Simulation Modelling Practice and Theory,scopus,2017-02-01,sciencedirect,Intelligent simulation: Integration of SIMIO and MATLAB to deploy decision support systems to simulation environment,https://api.elsevier.com/content/abstract/scopus_id/84996798684,"Discrete-event simulation is a decision support tool which enables practitioners to model and analyze their own system behavior. Although simulation packages are capable of mimicking most tasks in a real-world system, there are some decision-making activities, which are beyond the reach of simulation packages. The Application Programmers Interface (API) of SIMIO provides a wide range of opportunities for researchers to develop their own logic and apply it during the simulation run. This paper illustrates how to deploy MATLAB, as a computational tool coupled with SIMIO, as a simulation package by using a new user-defined step instance named “CallMATLAB”. A manufacturing system case study is introduced where the CallMATLAB step instance is used to create an Iterative Optimization-based Simulation (IOS) model. This model is created to evaluate the performance of different optimizers. The benefits of this hybridization for other industries, including healthcare systems, supply chain management systems, and project management problems are discussed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-409548-9.10355-0,Book,Comprehensive Remote Sensing,scopus,2017-01-01,sciencedirect,Soil moisture retrieval algorithms: The SMOS case,https://api.elsevier.com/content/abstract/scopus_id/85068151872,"After the successful acquisition by a coarse L-band radiometer on board Skylab in the early seventies, the potential of L-band radiometry was made clear in spite of a strict limitation linked to minimum antenna dimensions required for appropriate spatial resolution. More than 20 years later new antenna concepts emerged to mitigate this physical constraint. The first to emerge, in 1997, and to become a reality, was the Soil Moisture and Ocean Salinity (SMOS) mission (Kerr, 1997, 1998). It is European Space Agency’s (ESA’s) second Earth Explorer Opportunity mission (Kerr et al., 2001), launched in November 2009. It is a joint program between ESA, CNES (Centre National d’Etudes Spatiales), and CDTI (Centro para el Desarrollo Tecnologico Industrial). SMOS carries a single payload, an L-band 2D interferometric radiometer in the 1400–1427MHz protected band. This wavelength penetrates well through the atmosphere, and hence, the instrument probes the Earth surface emissivity from space. Surface emissivity can be related to the moisture content in the first few centimeters of soil, and after some surface roughness and temperature corrections, to the sea surface salinity over ocean.
               Soil moisture retrieval from SMOS observations with a required accuracy of 0.04m3/m3 is challenging and involves many steps. The retrieval algorithms are developed and implemented in the ground segment, which processes level 1 and level 2 data. Level 1 consists mainly of directional brightness temperatures, while level 2 consists of geophysical products in swath mode, i.e., for successive imaging snapshots acquired by the sensor during a half orbit from pole to pole. Level 3 consists in composites of brightness temperatures, or geophysical products over time and space, i.e., global maps over given temporal periods from 1day to 1month. In this context, a group of institutes prepared the soil moisture and ocean salinity Algorithm Theoretical Basis Documents (ATBD), used to in operational soil moisture and sea salinity retrieval algorithms (Kerr et al., 2010a).
               The principle of the level 2 soil moisture retrieval algorithm is based on an iterative approach, which aims at minimizing a cost function. The main component of the cost function is given by the sum of the squared weighted differences between measured and modeled brightness temperature (TB) at horizontal and vertical polarizations, for a variety of incidence angles. The algorithm finds the best set of parameters, e.g., soil moisture (SM) and vegetation characteristics, which drive the TB model and minimizes the cost function. From this algorithm, a more sophisticated one was developed to take into account multiorbit retrievals (i.e., level 3). Subsequently, after several years of data acquisition and algorithm improvements, a neural network approach was developed so as to be able to infer soil moisture fields in near-real time. In parallel, several simplified algorithms were tested, the goal being to achieve a seamless transition with other sensors, along with other studies targeted on specific targets such as dense forests, organic rich soils, or frozen and snow-covered grounds. Finally, it may be noted that most of these approaches deliver not only the surface soil moisture but also other quantities of interest such as vegetation optical depth, surface roughness, and surface dielectric constant.
               The goal of this article is to give an overview of these different approaches and corresponding results and adequate references for those wishing to go further. Sea surface salinity is not covered in this article, while the focus is SMOS data.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2017.07.167,Journal,Procedia Manufacturing,scopus,2017-01-01,sciencedirect,Towards Robust Early Stage Data Knowledge-based Inference Engine to Support Zero-defect Strategies in Manufacturing Environment,https://api.elsevier.com/content/abstract/scopus_id/85029884694,"Decision Support Systems are considered as a robust technology able to provide an advantage to several manufacturing companies. As part of the Z-Fact0r EU project, an autonomous and self-adjusted inference engine; namely the Early Stage-Decision Support System (ES-DSS) will be deployed. The scope is to facilitate real-time inspection, condition monitoring and control - diagnosis at the shop-floor, utilizing continuously mine multiple data streams and run the suitable models to monitor operations and quality performance, to classify products on the basis of quality metrics, as well to predict occurrence of defects and deviations from production and quality requirements.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2017.03.093,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Cyber-Physical Manufacturing Metrology Model (CPM<sup>3</sup>) for Sculptured Surfaces - Turbine Blade Application,https://api.elsevier.com/content/abstract/scopus_id/85028681766,"Cyber-Physical Manufacturing (CPM) and digital manufacturing represent the key elements for implementation of Industry 4.0 framework. Worldwide, Industry 4.0 becomes national research strategy in the field of engineering for the following ten years. The International Conference USA-EU-Far East-Serbia Manufacturing Summit was held from 31st May to 2nd June 2016 in Belgrade, Serbia. The result of the conference was the development of Industry 4.0 Model for Serbia as a framework for New Industrial Policy – Horizon 2020/2030.
                  Implementation of CPM in manufacturing systems generates “smart factory”. Products, resources, and processes within smart factory are realized and controlled through CPM model. This leads to significant advantages with respect to high product/process quality, real-time applications, savings in resources consumption, as well as, lower costs in comparison with classical manufacturing systems. Smart factory is designed in accordance with sustainable and service-oriented best business practices/models. It is based on optimization, flexibility, self-adaptability and learning, fault tolerance, and risk management. Complete manufacturing digitalization and digital factory are the key elements of Industry 4.0 Program.
                  In collaborative research, which we carry out in the field of quality control and manufacturing metrology at University of Belgrade, Faculty of Mechanical Engineering in Serbia and at Department of Mechanical Engineering, University of Texas, Austin in USA, three research areas are defined: (а) Digital manufacturing – towards Cloud Manufacturing Systems (as a basis for CPS), in which quality and metrology represent integral parts of process optimization based on Taguchi model, and (б) Cyber-Physical Quality Model (CPQM) – our approach, in which we have developed and tested intelligent model for prismatic parts inspection planning on CMM (Coordinate Measuring Machine). The third research area directs our efforts to the development of framework for Cyber-Physical Manufacturing Metrology Model (CPM3). CPM3 framework will be based on integration of digital product metrology information through metrology features recognition, and generation of global/local inspection plan for free-form surfaces; we will illustrate our approach using turbine blade example. This paper will present recent results of our research on CPM3.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2017.03.115,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Dynamic Analysis of Intelligent Coil Leveling Machine for Cyber-physical Systems Implementation,https://api.elsevier.com/content/abstract/scopus_id/85028650941,"In manufacturing industry, wider range variants and personalized productions are becoming formidable challenges that needed to be for smart manufacturing. In smart manufacturing, machines are connected cooperatively to seamlessly and quickly adjust production setting to reach market requirements. Furthermore, real-time production data visualization and evaluation are the keys to increase manufacturing productivity, efficiency, and flexibility. This integrated research is aimed to develop an intelligent coil leveling machine through dynamic analysis of real-time machine sensors network for cyber-physical systems implementation in smart manufacturing. In this proposed intelligent coil leveling machine, intelligent sensors network is embedded in the machine to allow real-time monitoring of the machine through feedback controlled system and cloud network to ensure optimized production with optimal machine setting instantly. Intelligent sensors network of the proposed coil leveling machine such as leveling roller indentation, leveling force, and coil curvature has been completed. Preliminary real-time dynamic monitoring of the leveling rollers and coil curvature has been accomplished. Following, real-time dynamic analysis is performed to demonstrate the implementation of the cyber-physical systems where machine learning intelligence can be achieved. Lastly, real-time cloud network monitoring are implemented to allow users to collect manufacturing data online. Through this research, conventional leveling machine can be transformed in which machine setting configurations can be adjusted to the production line through virtual cyber-physical system. Production data can be visualized and evaluated in real-time with precise and intelligent production strategies to ensure customer's requirements and to enhance production efficiency and flexibility in smart manufacturing of sheet metal coil.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2017.03.125,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Ant Colony Optimization Algorithms to Enable Dynamic Milkrun Logistics,https://api.elsevier.com/content/abstract/scopus_id/85028644167,"Flexibility in combination with high capacities are the main reasons for milkruns being one of the most popular intralogistics solutions. In most cases they are only used for static routes to always deliver the same material to the same stations. However, in the context of Industry 4.0, milkrun logistic also has become very popular for use cases where different materials have to be delivered to different stations in little time, so routes cannot be planned in advance anymore. As loading and unloading the milkrun requires a significant amount of time, beside the routing problem itself, both driving and loading times have to be taken into account. Especially in scenarios where high flexibility is required those times will vary significantly and thus are a crucial factor for obtaining the optimal solution. Although containing stochastic components, those times can be predicted by considering the optimal point of time for delivery. In consequence, the best tradeoff between short routes and optimal delivery times is in favor of the shortest route. To solve this optimization problem a biology-inspired method – the ant colony optimization algorithm – has been enhanced to obtain the best solution regarding the above-mentioned aspects. A manufacturing scenario was used to prove the ability of the algorithm in real world problems. It also demonstrates the ability to adapt to changes in manufacturing systems very quickly by dynamically modelling and simulating the processes in intralogistics. The paper describes the ant colony optimization algorithm with the necessary extensions to enable it for milkrun logistic problems. Additionally the implemented software environment to apply the algorithm in practice is explained.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.petrol.2016.11.033,Journal,Journal of Petroleum Science and Engineering,scopus,2017-01-01,sciencedirect,A hybrid particle swarm optimization and support vector regression model for modelling permeability prediction of hydrocarbon reservoir,https://api.elsevier.com/content/abstract/scopus_id/85028257367,"The significance of accurate permeability prediction cannot be over-emphasized in oil and gas reservoir characterization. Support vector machine regression (SVR), a computational intelligence technique, has been very successful in the estimation of permeability and has been widely deployed due to its unique features. However, careful selection of SVR hyper-parameters is highly essential to its optimum performance and this task is traditionally done using trial and error approach (TE-SVR) which takes a lot of time and do not guarantee optimal selection of the hyper-parameters. In this work, the performance of particle swarm optimization (PSO) technique, a heuristic optimization technique, is investigated for the optimal selection of SVR hyper-parameters for the first time in modelling and characterization of hydrocarbon reservoir. The technique is capable of automatic selection of the optimum combination of SVR hyper-parameters resulting in higher predictive accuracy and generalization ability of the developed model. The resulting PSO-SVR model is compared to SVR models whose parameters are obtained through random search (RAND-SVR) and trial and error approach (TE-SVR). The comparison is done using real-life industrial datasets obtained during petroleum exploration from four distinct oil wells located in a Middle Eastern oil and gas field. Simulation results indicate that the PSO-SVR model outperforms all the other models. Error reduction of 15.1%, 26.15%, 12.32% and 7.1% are recorded for PSO-SVR model compared to ordinary SVR (TE-SVR) in well-A, well-B, well-C and well-D, respectively. Also, reduction of 12.8%, 23.97%, 2.51% and 0.11 are recorded when PSO-SVR and RAND-SVR results are compared in the respective wells. Furthermore, the results show the potential of the application of heuristics algorithms, such as PSO, in the optimization of computational intelligence techniques employed in hydrocarbon reservoir characterizations. Therefore, PSO technique is proposed for the optimization of SVR hyper-parameters in permeability prediction and reservoir characterization based on its superior performance over the commonly employed optimization techniques.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2017.07.091,Conference Proceeding,Procedia Manufacturing,scopus,2017-01-01,sciencedirect,Machine Learning-based CPS for Clustering High throughput Machining Cycle Conditions,https://api.elsevier.com/content/abstract/scopus_id/85023607399,"Cyber-physical systems (CPS) have opened up a wide range of opportunities in terms of performance analysis that can be applied directly to the machine tool industry and are useful for maintenance systems and machine designers. High-speed communication capabilities enable the data to be gathered, pre-processed and processed for the purpose of machine diagnosis. This paper describes a complete real-world CPS implementation cycle, ranging from machine data acquisition to processing and interpretation. In fact, the aim of this paper is to propose a CPS for machine component knowledge discovery based on clustering algorithms using real data from a machining process. Therefore, it compares three clustering algorithms –k-means, hierarchical agglomerative and Gaussian mixture models– in terms of their contribution to spindle performance knowledge during high throughput machining operation.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2017.04.039,Journal,Procedia Manufacturing,scopus,2017-01-01,sciencedirect,Digital Twin as Enabler for an Innovative Digital Shopfloor Management System in the ESB Logistics Learning Factory at Reutlingen - University,https://api.elsevier.com/content/abstract/scopus_id/85020859111,"Technologies for mapping the “digital twin” have been under development for approximately 20 years. Nowadays increasingly intelligent, individualized products encourages companies to respond innovatively to customer requirements and to handle the rising product variations quickly.
                  An integrated engineering network, spanning across the entire value chain, is operated to intelligently connect various company divisions, and to generate a business ecosystem for products, services and communities. The conditions for the digital twin are thereby determined in which the digital world can be fed into the real, and the real world back into the digital to deal such intelligent products with rising variations.
                  The term digital twin can be described as a digital copy of a real factory, machine, worker etc., that is created and can be independently expanded, automatically updated as well as being globally available in real time. Every real product and production site is permanently accompanied by a digital twin. First prototypes of such digital twins already exist in the ESB Logistics Learning Factory on a cloud- and app-based software that builds on a dynamic, multidimensional data and information model. A standardized language of the robot control systems via software agents and positioning systems has to be integrated. The aspect of the continuity of the real factory in the digital factory as an economical means of ensuring continuous actuality of digital models looks as the basis of changeability.
                  For the indoor localization sensor combinations that in addition to the hardware already contain the software required for the sensor data fusion should be used. Processing systems, scenario-live-simulations and digital shop floor management results in a mandatory procedural combination. Essential to the digital twin is the ability to consistently provide all subsystems with the latest state of all required information, methods and algorithms.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2017.01.047,Conference Proceeding,Procedia CIRP,scopus,2017-01-01,sciencedirect,Using Graph-based Design Languages to Enhance the Creation of Virtual Commissioning Models,https://api.elsevier.com/content/abstract/scopus_id/85020027557,"‘Industrie 4.0’ based production systems are likely to change the way future products are manufactured. As information technology gains influence on these systems there is a chance of higher flexibility due to decentralized logic and artificial intelligence. All this leads to a higher complexity and also indeterminism is feasible. Therefore simulation technologies will become a mandatory requirement, especially virtual commissioning will get necessary as the amount of software is rising.
                  A lot of manpower is required to establish and maintain a virtual commissioning system as it needs a large database of standard components. Therefore in most cases small- and mid-sized companies are forced to avoid such technologies. Using graph-based design languages to create virtual commissioning models can help to solve this problem. The basic principle is to shape an abstract model of a production system which will then be individually built within the domain specific tools. One of these should be a virtual commissioning tool to evaluate the functionality of the built model. If a change in the design is necessary, the new virtual commissioning model can be regenerated automatically. This approach is even more reasonable, if graph-based design languages are used throughout the whole product life cycle.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2017.01.213,Conference Proceeding,Procedia Computer Science,scopus,2017-01-01,sciencedirect,Hybrid Agents Implementation for the Control of the Construction Company,https://api.elsevier.com/content/abstract/scopus_id/85016095509,"Planning the project duration together with separate works is an essential element of managing the construction. The final duration depends on multiple factors, including the funds, customer requests, and capabilities of the construction company. In order to avoid additional costs in penalties or additional expenses, the management needs to estimate the real construction duration in advance, before the contract is signed. Further on, these terms need to be monitored both in whole and for the specific jobs in order to be able to edit further stages with regard of the remaining time, resources and used resources ratio. The development of a decision support system for the construction company is a pressing problem due to the growing demand in decision making persons’ labor automation in planning and monitoring the construction processes. The paper presents the model and the application experience for such a system.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rser.2016.11.046,Journal,Renewable and Sustainable Energy Reviews,scopus,2017-01-01,sciencedirect,Comparison of different discharge strategies of grid-connected residential PV systems with energy storage in perspective of optimal battery energy storage system sizing,https://api.elsevier.com/content/abstract/scopus_id/85006456716,"The paper presents a yearly comparison of different residential self-consumption-reducing discharge strategies for grid connected residential PV systems with the Battery Energy Storage System (BESS). Altogether, three discharge strategies are taken into consideration; base case, adaptive algorithm and an energy-market-oriented remote-controlled strategy. All of the presented strategies are strictly limited to available residential load reduction based on the current BESS regulations. Furthermore, the simulations are run on real-world measurement data. The adaptive “grid-friendly” discharge algorithm utilizes global optima and a moving-average calculation to maximize the self-energy consumption in regards to the peak grid load periods. On the other hand a remote-controlled discharge scenario is taken into consideration to maximize self-consumption and to decrease the grid load when the intraday energy prices are the highest, if there is opportunity to do so. The system size optimization equations are based on a hidden layer feedforward neural network system. The main goal of the network is to predict the corresponding equations for the optimization software. Based on the neural network results, an “easy-to-use” BESS-size economic optimization web-based application has been developed to demonstrate the feasibility of the different discharge methodologies and to make easier different manufactures’ BESS system comparisons.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.paerosci.2016.10.001,Journal,Progress in Aerospace Sciences,scopus,2017-01-01,sciencedirect,An evolutionary outlook of air traffic flow management techniques,https://api.elsevier.com/content/abstract/scopus_id/85006413338,"In recent years Air Traffic Flow Management (ATFM) has become pertinent even in regions without sustained overload conditions caused by dense traffic operations. Increasing traffic volumes in the face of constrained resources has created peak congestion at specific locations and times in many areas of the world. Increased environmental awareness and economic drivers have combined to create a resurgent interest in ATFM as evidenced by a spate of recent ATFM conferences and workshops mediated by official bodies such as ICAO, IATA, CANSO the FAA and Eurocontrol. Significant ATFM acquisitions in the last 5 years include South Africa, Australia and India. Singapore, Thailand and Korea are all expected to procure ATFM systems within a year while China is expected to develop a bespoke system. Asia-Pacific nations are particularly pro-active given the traffic growth projections for the region (by 2050 half of all air traffic will be to, from or within the Asia-Pacific region). National authorities now have access to recently published international standards to guide the development of national and regional operational concepts for ATFM, geared to Communications, Navigation, Surveillance/Air Traffic Management and Avionics (CNS+A) evolutions. This paper critically reviews the field to determine which ATFM research and development efforts hold the best promise for practical technological implementations, offering clear benefits both in terms of enhanced safety and efficiency in times of growing air traffic. An evolutionary approach is adopted starting from an ontology of current ATFM techniques and proceeding to identify the technological and regulatory evolutions required in the future CNS+A context, as the aviation industry moves forward with a clearer understanding of emerging operational needs, the geo-political realities of regional collaboration and the impending needs of global harmonisation.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2016.10.021,Journal,International Journal of Production Economics,scopus,2017-01-01,sciencedirect,Single-hidden layer neural networks for forecasting intermittent demand,https://api.elsevier.com/content/abstract/scopus_id/84994731834,"Managing intermittent demand is a vital task in several industrial contexts, and good forecasting ability is a fundamental prerequisite for an efficient inventory control system in stochastic environments. In recent years, research has been conducted on single-hidden layer feedforward neural networks, with promising results. In particular, back-propagation has been adopted as a gradient descent-based algorithm for training networks. However, when managing a large number of items, it is not feasible to optimize networks at item level, due to the effort required for tuning the parameters during the training stage. A simpler and faster learning algorithm, called the extreme learning machine, has been therefore proposed in the literature to address this issue, but it has never been tried for forecasting intermittent demand. On the one hand, an extensive comparison of single-hidden layer networks trained by back-propagation is required to improve our understanding of them as predictors of intermittent demand. On the other hand, it is also worth testing extreme learning machines in this context, because of their lower computational complexity and good generalisation ability.
                  In this paper, neural networks trained by back-propagation and extreme learning machines are compared with benchmark neural networks, as well as standard forecasting methods for intermittent demand on real-time series, by combining different input patterns and architectures. A statistical analysis is then conducted to validate the best performance through different aggregation levels. Finally, some insights for practitioners are presented to improve the potential of neural networks for implementation in real environments.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.energy.2016.09.096,Journal,Energy,scopus,2016-12-01,sciencedirect,Generation of realistic scenarios for multi-agent simulation of electricity markets,https://api.elsevier.com/content/abstract/scopus_id/84988734772,"Most market operators provide daily data on several market processes, including the results of all market transactions. The use of such data by electricity market simulators is essential for simulations quality, enabling the modelling of market behaviour in a much more realistic and efficient way. RealScen (Realistic Scenarios Generator) is a tool that creates realistic scenarios according to the purpose of the simulation: representing reality as it is, or on a smaller scale but still as representative as possible. This paper presents a novel methodology that enables RealScen to collect real electricity markets information and using it to represent market participants, as well as modelling their characteristics and behaviours. This is done using data analysis combined with artificial intelligence. This paper analyses the way players' characteristics are modelled, particularly in their representation in a smaller scale, simplifying the simulation while maintaining the quality of results. A study is also conducted, comparing real electricity market values with the market results achieved using the generated scenarios. The conducted study shows that the scenarios can fully represent the reality, or approximate it through a reduced number of representative software agents. As a result, the proposed methodology enables RealScen to represent markets behaviour, allowing the study and understanding of the interactions between market entities, and the study of new markets by assuring the realism of simulations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.epsr.2016.07.018,Journal,Electric Power Systems Research,scopus,2016-12-01,sciencedirect,Classification for consumption data in smart grid based on forecasting time series,https://api.elsevier.com/content/abstract/scopus_id/84981303127,"One of the most important tasks of present day smart grid implementations is to classify different types of consumers (households, office buildings and industrial plants) because they may be served by the power supplier with different parameters, rates, contracts.
                  In this paper, we propose a novel classification scheme for smart grid systems where the collected data are processed in order to increase the efficiency of electricity transportation as well as demand-supply management. The new scheme is based on forecasting the consumption time series obtained from a smart meter. Class assignment is determined using the forecast error. Different linear and nonlinear methods were tested based on the corresponding assumptions on the statistical behavior of the underlying consumption time series.
                  Performance tests were carried out with simulations in order to demonstrate the capabilities and to compare the achieved performance of the proposed scheme with existing solutions. The simulations have been executed using (i) artificially generated consumption data, which data came from a bottom-up semi-Markov model and (ii) real, measured power consumption data as well. The parameters of the model have been validated on real data. The numerical results have demonstrated that our method can better model and classify the consumption patterns of office-buildings than the existing methods. As a result, the proposed method may prove to be a promising classification tool.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2016.06.043,Journal,Expert Systems with Applications,scopus,2016-11-30,sciencedirect,Data quality assessment of maintenance reporting procedures,https://api.elsevier.com/content/abstract/scopus_id/84977274884,"Today’s largest and fastest growing companies’ assets are no longer physical, but rather digital (software, algorithms...). This is all the more true in the manufacturing, and particularly in the maintenance sector where quality of enterprise maintenance services are closely linked to the quality of maintenance data reporting procedures. If quality of the reported data is too low, it can results in wrong decision-making and loss of money. Furthermore, various maintenance experts are involved and directly concerned about the quality of enterprises’ daily maintenance data reporting (e.g., maintenance planners, plant managers...), each one having specific needs and responsibilities. To address this Multi-Criteria Decision Making (MCDM) problem, and since data quality is hardly considered in existing expert maintenance systems, this paper develops a maintenance reporting quality assessment (MRQA) dashboard that enables any company stakeholder to easily – and in real-time – assess/rank company branch offices in terms of maintenance reporting quality. From a theoretical standpoint, AHP is used to integrate various data quality dimensions as well as expert preferences. A use case describes how the proposed MRQA dashboard is being used by a Finnish multinational equipment manufacturer to assess and enhance reporting practices in a specific or a group of branch offices.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.resconrec.2016.03.012,Journal,"Resources, Conservation and Recycling",scopus,2016-11-01,sciencedirect,Implementation of OPTIMASS to optimise municipal wastewater sludge processing chains: Proof of concept,https://api.elsevier.com/content/abstract/scopus_id/85028239611,"In sludge management, sludge is increasingly perceived as a marketable product rather than as a waste material. This awareness in combination with the variety of factors influencing the optimal management strategy and disposal route, introduces the need to optimise the sludge treatment throughout the whole chain instead of only minimising its production. In this paper, OPTIMASS, a mixed integer linear programming model to optimise strategic and tactical decisions in biomass-based supply chains, is proposed in order to meet this need. The applicability of OPTIMASS is illustrated through its implementation with a view to minimise the overall global warming impact of a real municipal wastewater sludge processing chain in “region X”. A first scenario addresses the optimisation of the allocation and treatment of municipal wastewater sludge within the current network. Second, OPTIMASS is used to identify the optimal location(s) for new drying facilities in this chain. Finally, the effect on the optimal chain of changes in municipal wastewater sludge production and of changes in global warming impact of the cement industry as a disposal route is evaluated.
                  The analysis reveals that municipal wastewater sludge processing chains can be considered to be instances of the generic biomass-based supply chain and that the OPTIMASS tool can be applied to support strategic and tactical decisions for optimising sludge management in case new technologies, new treatment facility locations, new disposal options, etc. are at stake. The validity of the OPTIMASS approach is confirmed by the close correspondence between its outcome and the results of a decision support system, specifically developed for the municipal wastewater sludge processing chain.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cie.2016.07.031,Journal,Computers and Industrial Engineering,scopus,2016-11-01,sciencedirect,TIMSPAT – Reachability graph search-based optimization tool for colored Petri net-based scheduling,https://api.elsevier.com/content/abstract/scopus_id/84991205081,"The combination of Petri net (PN) modeling with AI-based heuristic search (HS) algorithms (PNHS) has been successfully applied as an integrated approach to deal with scheduling problems that can be transformed into a search problem in the reachability graph. While several efficient HS algorithms have been proposed albeit using timed PN, the practical application of these algorithms requires an appropriate tool to facilitate its development and analysis. However, there is a lack of tool support for the optimization of timed colored PN (TCPN) models based on the PNHS approach for schedule generation. Because of its complex data structure, TCPN-based scheduling has often been limited to simulation-based performance analysis only. Also, it is quite difficult to evaluate the strength and tractability of algorithms for different scheduling scenarios due to the different computing platforms, programming languages and data structures employed. In this light, this paper presents a new tool called TIMSPAT, developed to overcome the shortcomings of existing tools. Some features that distinguish this tool are the collection of several HS algorithms, XML-based model integration, the event-driven exploration of the timed state space including its condensed variant, localized enabling of transitions, the introduction of static place, and the easy-to-use syntax statements. The tool is easily extensible and can be integrated as a component into existing PN simulators and software environments. A comparative study is performed on a real-world eyeglass production system to demonstrate the application of the tool for scheduling purposes.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.csi.2016.03.003,Journal,Computer Standards and Interfaces,scopus,2016-11-01,sciencedirect,Intelligent software product line configurations: A literature review,https://api.elsevier.com/content/abstract/scopus_id/84964669931,"A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.knosys.2016.07.022,Journal,Knowledge-Based Systems,scopus,2016-10-15,sciencedirect,Software test quality rating: A paradigm shift in swarm computing for software certification,https://api.elsevier.com/content/abstract/scopus_id/84979704090,"Recently, software quality issues have emerged to be recognized as a fundamental point as we actualize an extensive growth of organizations involved in software industries. Still, these organizations cannot ensure the quality of their products; therefore abandoning customers in uncertainties. Software certification is the branch of quality by means that quality requires to be measured prior to certification admitting process. However, creating an official certification model is difficult due to the deficiency of data in the domain of software engineering. This research participates in solving the problem of assessing software quality by introducing a model that handles a fuzzy inference engine to mix both of the processes–driven and application-driven quality assurance procedures. The fundamental purpose of the suggested model is to enhance the compactness and the interpretability of the system's fuzzy rules via engaging an ant colony optimization algorithm (ACO), which attempts to discover a good rule description by a set of compound rules initially represented with traditional single rules. The proposed model is a fitting one that can be seen as practicing certification models that have already been created from software quality domain data and modifying them to a context-specific data. The model has been tested by a case study and the results have confirmed feasibility and practicality of the model in a real environment.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jclepro.2016.05.091,Journal,Journal of Cleaner Production,scopus,2016-10-01,sciencedirect,Developing an ant colony approach for green closed-loop supply chain network design: a case study in gold industry,https://api.elsevier.com/content/abstract/scopus_id/84988844589,"The forward/reverse logistics network design is an important and strategic issue due to its effects on efficiency and responsiveness of a supply chain. In practice, it is needed to formulate and solve real problems through efficient algorithms in a reasonable time. Hence, this paper tries to cover real case problem with a multi-objective model and an integrated forward/reverse logistics network design. Further, the model is customized and implemented for a case study in gold industry where the reverse logistics play crucial role. A new solution approach is applied for the proposed 7-layer network of the case study and the solutions are achieved in order solve the current difficulties of the investigated supply chain. This paper seeks to address how a multi objective logistics model in the gold industry can be created and solved through an efficient meta-heuristic algorithm. A green approach based on the CO2 emission is considered in the network design approach. The developed model includes four echelons in the forward direction and three echelons in the reverse. First, an integer linear programming model is developed to minimize costs and emissions. Then, in order to solve the model, an algorithm based on ant colony optimization is developed. The performance of the proposed algorithm has been compared with the optimum solutions of the LINGO software through various numerical examples based on the random data and real-world instances. The evaluation studies demonstrate that the proposed model is practical and applicable and the developed algorithm is reliable and efficient. The results prove the managerial implications of the model and the solution approach in terms of presenting appropriate modifications to the mangers of the selected supply chain. Further, a Taguchi-based parameter setting is undertaken to ensure using the appropriate parameters for the algorithm.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.adhoc.2016.06.011,Journal,Ad Hoc Networks,scopus,2016-10-01,sciencedirect,Feature selection for performance characterization in multi-hop wireless sensor networks,https://api.elsevier.com/content/abstract/scopus_id/84977657790,"Current trends in Wireless Sensor Networks are faced with the challenge of shifting from testbeds in controlled environments to real-life deployments, characterized by unattended and long-term operation. The network performance in such settings depends on various factors, ranging from the operational space, the behavior of the protocol stack, the intra-network dynamics, and the status of each individual node. As such, characterizing the network’s high-level performance based exclusively on link-quality estimation, can yield episodic snapshots on the performance of specific, point-to-point links. The objective of this work is to provide an integrated framework for the unsupervised selection of the dominant features that have crucial impact on the performance of end-to-end links, established over a multi-hop topology. Our focus is on compressing the original feature vector of network parameters, by eliminating redundant network attributes with predictable behavior. The proposed approach is implemented alongside different cases of protocol stacks and evaluated on data collected from real-life deployments in rural and industrial environments. Discussions on the efficacy of the proposed scheme, and the dominant network characteristics per deployment are offered.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2016.06.005,Journal,International Journal of Production Economics,scopus,2016-09-01,sciencedirect,Hybrid flow shop batching and scheduling with a bi-criteria objective,https://api.elsevier.com/content/abstract/scopus_id/84975859979,"This paper addresses the hybrid flow shop batching and scheduling problem where sequence-dependent family setup times are present and the objective is to simultaneously minimize the weighted sum of the total weighted completion time and total weighted tardiness. In particular, it disregards the group technology assumptions by allowing for the possibility of splitting pre-determined groups of jobs into inconsistent batches in order to improve the operational efficiency. A benchmark of small size problems is considered to show the benefits of batching on group scheduling. Since the problem is strongly NP-hard, several algorithms based upon tabu search are developed at three levels, which move back and forth between batching and scheduling phases. Two algorithms incorporate tabu search into the framework of path-relinking to exploit the information on good solutions. These tabu search/path-relinking algorithms comprise several distinguishing features including two relinking procedures to effectively construct paths and the stage-based improvement procedure to consider the move interdependency. The best tabu search algorithm as a local search algorithm is compared to a population-based algorithm, and the superiority of the former over the latter is shown using a statistical experiment. The initial solution finding mechanism is implemented to trigger the search into the solution space. The efficiency and effectiveness of the best algorithm is verified with the help of the results found by CPLEX. The results show that the best algorithm, based on tabu search/path relinking and the stage-based improvement procedure, could find solutions at least as good as CPLEX, but in drastically shorter computational time. In order to reflect the real industry requirements, dynamic machine availability times, dynamic job release times, machine eligibility and machine capability for processing jobs, desired lower bounds on batch sizes, and job skipping are considered.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jpowsour.2016.05.092,Journal,Journal of Power Sources,scopus,2016-08-30,sciencedirect,Prognostics of Proton Exchange Membrane Fuel Cells stack using an ensemble of constraints based connectionist networks,https://api.elsevier.com/content/abstract/scopus_id/84975104897,"Proton Exchange Membrane Fuel Cell (PEMFC) is considered the most versatile among available fuel cell technologies, which qualify for diverse applications. However, the large-scale industrial deployment of PEMFCs is limited due to their short life span and high exploitation costs. Therefore, ensuring fuel cell service for a long duration is of vital importance, which has led to Prognostics and Health Management of fuel cells. More precisely, prognostics of PEMFC is major area of focus nowadays, which aims at identifying degradation of PEMFC stack at early stages and estimating its Remaining Useful Life (RUL) for life cycle management. This paper presents a data-driven approach for prognostics of PEMFC stack using an ensemble of constraint based Summation Wavelet- Extreme Learning Machine (SW-ELM) models. This development aim at improving the robustness and applicability of prognostics of PEMFC for an online application, with limited learning data. The proposed approach is applied to real data from two different PEMFC stacks and compared with ensembles of well known connectionist algorithms. The results comparison on long-term prognostics of both PEMFC stacks validates our proposition.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cviu.2016.03.018,Journal,Computer Vision and Image Understanding,scopus,2016-07-01,sciencedirect,"Wize Mirror-a smart, multisensory cardio-metabolic risk monitoring system",https://api.elsevier.com/content/abstract/scopus_id/84963813154,"In the recent years personal health monitoring systems have been gaining popularity, both as a result of the pull from the general population, keen to improve well-being and early detection of possibly serious health conditions and the push from the industry eager to translate the current significant progress in computer vision and machine learning into commercial products. One of such systems is the Wize Mirror, built as a result of the FP7 funded SEMEOTICONS (SEMEiotic Oriented Technology for Individuals CardiOmetabolic risk self-assessmeNt and Self-monitoring) project. The project aims to translate the semeiotic code of the human face into computational descriptors and measures, automatically extracted from videos, multispectral images, and 3D scans of the face. The multisensory platform, being developed as the result of that project, in the form of a smart mirror, looks for signs related to cardio-metabolic risks. The goal is to enable users to self-monitor their well-being status over time and improve their life-style via tailored user guidance. This paper is focused on the description of the part of that system, utilising computer vision and machine learning techniques to perform 3D morphological analysis of the face and recognition of psycho-somatic status both linked with cardio-metabolic risks. The paper describes the concepts, methods and the developed implementations as well as reports on the results obtained on both real and synthetic datasets.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jobe.2016.04.010,Journal,Journal of Building Engineering,scopus,2016-06-01,sciencedirect,"Modeling and simulation controlling system of HVAC using fuzzy and predictive (radial basis function, RBF) controllers",https://api.elsevier.com/content/abstract/scopus_id/84965095816,"Heating, ventilating and air conditioning (HVAC) systems are used in buildings, industry and agriculture to provide thermal and humidity comfort. Modeling of HVAC system can help to design precise controlling systems. In this study, a HVAC system had been modeled using MATLAB simulation software that had been developed using a fuzzy controlling system and radial basis function (RBF) model of artificial neural network (ANN) as a predictive control system. Results of the modeled systems were extracted and compared with actual system. In order to compare results of the modeled and actual systems, comparing parameters, such as mean absolute error (MAE), root mean square error (RMSE), mean absolute percentage/relative error (MAPE) and coefficient of Pearson correlation (r) were applied. The results indicated that, the modeled systems was accurately controlling the system and the difference between real and modeled system was also close. In the results as a whole, the predictive controller (RBF network) has the best performance compared to fuzzy model.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.epsr.2016.03.012,Journal,Electric Power Systems Research,scopus,2016-06-01,sciencedirect,Metalearning to support competitive electricity market players' strategic bidding,https://api.elsevier.com/content/abstract/scopus_id/84962522494,"Electricity markets are becoming more competitive, to some extent due to the increasing number of players that have moved from other sectors to the power industry. This is essentially resulting from incentives provided to distributed generation. Relevant changes in this domain are still occurring, such as the extension of national and regional markets to continental scales. Decision support tools have thereby become essential to help electricity market players in their negotiation process. This paper presents a metalearner to support electricity market players in bidding definition. The proposed metalearner uses a dynamic artificial neural network to create its own output, taking advantage on several learning algorithms already implemented in ALBidS (Adaptive Learning strategic Bidding System). The proposed metalearner considers different weights for each strategy, based on their individual performance. The metalearner's performance is analysed in scenarios based on real electricity markets data using MASCEM (Multi-Agent Simulator for Competitive Electricity Markets). Results show that the proposed metalearner is able to provide higher profits to market players when compared to other current methodologies and that results improve over time, as consequence of its learning process.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ins.2016.01.001,Journal,Information Sciences,scopus,2016-05-01,sciencedirect,Solving integrated process planning and scheduling problem with constructive meta-heuristics,https://api.elsevier.com/content/abstract/scopus_id/84957879888,"For product manufacturing, process planning is to select a series of manufacturing processes according to the product design specification, and scheduling is to allocate manufacturing resources such as machines and tools to these processes. It is a common problem that the process plan and the schedule are not able to cope with the changes in real time manufacturing. Integrated process planning and scheduling (IPPS) is to conduct the process planning and scheduling functions concurrently, with the aim to improve the dynamic responsiveness of the production schedule. This paper investigates the formulation and implementation of constructive meta-heuristics for solving IPPS problems. To begin with, a model representation is established to express IPPS problems with AND/OR graphs. With this model representation, a generic framework is proposed for implementing constructive meta-heuristics in the solution model. The generic framework provides a common procedure for the constructive meta-heuristics, which encapsulates the calculation of the search frontier and state transitions, and provides two interfaces for accommodating different constructive search algorithms. Ant colony optimization (ACO), a commonly-used algorithm which possesses all typical characteristics of constructive meta-heuristics, is adopted as a representative example for illustrating the implementation. Experiments and tests are conducted to validate the proposed system. The single objective minimizing the makespan is set for evaluating the performance of the proposed system. Experimental results of the benchmark problems have shown the effectiveness and high performance of the proposed approach based on the integration of the generic framework and ACO strategy.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-08-100571-2.00008-7,Book,Information Systems for the Fashion and Apparel Industry,scopus,2016-04-08,sciencedirect,Intelligent demand forecasting systems for fast fashion,https://api.elsevier.com/content/abstract/scopus_id/84979900385,"Sales forecasting in the fashion industry has been a very challenging issue for decades. Recently, the concept of fast fashion has emerged as a successful strategy for retailers. In terms of sales forecasting, this concept involves new approaches to deal with specific features such as the limited amount of historical data and shortened time for the computation. The literature review of existing methods in this domain shows that many models have been proposed. They are mainly based on artificial intelligence techniques. Among these techniques, we focus on the two models that arise as references for long-term and short-term forecasts to develop a two-stage sales forecasting system. Associated with a store replenishment model, which is inspired from a method implemented in a famous fast fashion brand, we propose to simulate our two-stage forecasting system on real data to evaluate the real benefits of advanced forecasting techniques for fast fashion retailing.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ymssp.2015.09.025,Journal,Mechanical Systems and Signal Processing,scopus,2016-03-01,sciencedirect,Classification of acoustic emission signals using wavelets and Random Forests: Application to localized corrosion,https://api.elsevier.com/content/abstract/scopus_id/84961163563,"This paper aims to propose a novel approach to classify acoustic emission (AE) signals deriving from corrosion experiments, even if embedded into a noisy environment. To validate this new methodology, synthetic data are first used throughout an in-depth analysis, comparing Random Forests (RF) to the k-Nearest Neighbor (k-NN) algorithm. Moreover, a new evaluation tool called the alter-class matrix (ACM) is introduced to simulate different degrees of uncertainty on labeled data for supervised classification. Then, tests on real cases involving noise and crevice corrosion are conducted, by preprocessing the waveforms including wavelet denoising and extracting a rich set of features as input of the RF algorithm. To this end, a software called RF-CAM has been developed. Results show that this approach is very efficient on ground truth data and is also very promising on real data, especially for its reliability, performance and speed, which are serious criteria for the chemical industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.mechatronics.2015.06.015,Journal,Mechatronics,scopus,2016-03-01,sciencedirect,Symbolic discrete-time planning with continuous numeric action parameters for agent-controlled processes,https://api.elsevier.com/content/abstract/scopus_id/84939200231,"In industrial domains such as manufacturing control, a trend away from centralized planning and scheduling towards more flexible distributed agent-based approaches could be observed over recent years. To be of practical relevance, the local control mechanisms of the autonomous agents must be able to dependably adhere and dynamically adjust to complex numeric goal systems like business key performance indicators in an economically beneficial way. However, planning with numeric state variables and objectives still poses a challenging task within the field of artificial intelligence (AI).
                  In this article, a new general-purpose AI planning approach is presented that operates in two stages and extends existing domain-independent modeling formalisms like PDDL with continuous (i.e., infinite-domain) numeric action parameters, which are currently still unsupported by state-of-the-art AI planners. In doing so, it enables the solution of mathematical optimization problems at the action level of the planning tasks, which are inherent to many real-world control problems. To deal with certain difficulties concerning reliable and fast detection of action applicability that arise when planning with real-valued action parameters, the implemented planner allows resorting to an adjustable “satisficing” strategy by means of partial execution and subsequent repair of infeasible plans over the course of time. The functioning of the system is evaluated in a multi-agent simulation of a shop floor control scenario with focus on the effects the possible problem cases and different degrees of satisficing have on attained plan quality and total planning time. As the results demonstrate the basic practicability of the approach for the given setting, this contribution constitutes an important step towards the effective and dependable integration of complex numeric goal systems and non-linear multi-criteria optimization tasks into autonomous agent-controlled industrial processes in a reusable, domain-independent way.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.seppur.2015.12.056,Journal,Separation and Purification Technology,scopus,2016-02-29,sciencedirect,Rapid cultivation of aerobic granule for the treatment of solvent recovery raffinate in a bench scale sequencing batch reactor,https://api.elsevier.com/content/abstract/scopus_id/84954169665,"Aerobic granular sludge (AGS) was cultivated in a bench scale sequencing batch reactor within 21days. Strategy of the rapid startup was inoculated with part of mature AGS during cultivation, while aerobic biological selector was implemented for the inhibiting outgrowth of filamentous bacteria and fast selection of zoogloea bacteria. Then, the cultivated AGS was employed for the treatment of solvent recovery raffinate. Stable AGS was successfully domesticated after 55days under strategy of gradually increase the proportion of real wastewater in influent. The domesticated AGS was orange, irregular shape, smooth and compact. SVI, SV30/SV5, MLVSS/MLSS, EPS, PN/PS, average particle size, granulation rate, (SOUR)H and (SOUR)N of AGS were 19.06mL/g, 0.97, 0.55, 30.05mg/g MLVSS, 1.10, 1.28mm, 98.87%, 32.47 and 7.97mg O2/hgVSS respectively. Finally, COD, TIN, NH4
                     +–N and TP of the effluent were lower than 25.9mg/L, 1.64mg/L, 1.13mg/L and 0.21mg/L, and their removal rate was more than 98.43%, 97.12%, 98.02% and 98.09% respectively. Thus, COD, TP removal, nitrification and denitrification were realized in a single bioreactor. The result indicated that the feasibility of AGS for high C/N ratio industrial wastewater treatment.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2016.11.160,Conference Proceeding,IFAC-PapersOnLine,scopus,2016-01-01,sciencedirect,Neural networks as a diagnosing tool for industrial level measurement through non-contacting radar type and support to the decision for its better application,https://api.elsevier.com/content/abstract/scopus_id/85006454620,"The aim of this study was to develop an analysis tool based on artificial neural networks (ANN) to detect level measurement problems with free wave propagation radars. The trend of using this type of radar has been growing in the last ten years mainly because of its easy installation on the top of tanks and reservoirs, and for its low rate maintenance comparing to other level measurement technologies. For the experiments, a Rosemount radar was used and the training of the neural network was based on the data from the software Radar Master. Therefore, some network topologies in different scenarios were tested and it was possible to demonstrate the efficiency of the ANN with accuracy rate between 94.44 to 100% for the first experiment with networks using 10, 20 or 50 neurons in the hidden layer. This technique was applied in a real industrial application, a sugar and ethanol mill, and accuracy rate was about 87,0 to 96,1%. This methodology can be applied to asset management software for diagnosis report or troubleshooting which would increase the level measurement reliability and plant safety.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2016.06.096,Conference Proceeding,Procedia CIRP,scopus,2016-01-01,sciencedirect,Transfer of Model of Innovative Smart Factory to Croatian Economy Using Lean Learning Factory,https://api.elsevier.com/content/abstract/scopus_id/84999791899,"Croatia's manufacturing industry faces many problems and obstacles that have a large impact on its competitiveness. Insufficiently educated and unskilled personnel, particularly in the production and management fields, are decreasing competitiveness that is necessary for survival in the global market. Objective of project Innovative Smart Enterprise is to establish a special learning environment in one Laboratory asLean Learning Factory, i.e. simulation of a real factory through specialized equipment. The Lean Learning Factory's mission is to integrate needed knowledge into the engineering curriculum. Therefore, Lean Learning Factory at University of Split is in continuous developing process to support practice-based engineering curriculum with possibility of learning necessary tools and methods, using didactic games or real life products and equipment. Solution proposal for best balance between toys and real products consider design and production line development for product Karet. It is a traditional and original product from Croatia, so it will raise enthusiasmin learning process in both students and industry employees. Two assembly lines will be developed, one traditionally equipped and one intelligent, networked, flexible, and fully improved by Lean tools. By deeper analysis of both assembly lines, hybrid assembly lines could be designed, to balance on one side assembly tact time according to customer demand and total cost of installation and running on the other side. Methods and tools adapted and implemented, in both design and analysis process for optimization of this hybrid assembly line would be scaled and adjusted for industry use as part as knowledge transfer from university to enterprises.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.proeng.2016.07.416,Conference Proceeding,Procedia Engineering,scopus,2016-01-01,sciencedirect,Automated Detection of Faults in Wastewater Pipes from CCTV Footage by Using Random Forests,https://api.elsevier.com/content/abstract/scopus_id/84997822163,"Sewer systems require regular inspection in order to ensure their satisfactory condition. As most sewer networks consist of pipes too small for engineers to traverse, CCTV footage is used to record the interior of these pipes. This footage is manually analysed by qualified engineers, to determine the condition of the pipe and the presence of any faults. We propose a methodology, which automatically detects faults within the CCTV footage. This has the potential to dramatically reduce the time required to process the large volume of CCTV footage produced during a survey. The proposed methodology first characterises localised regions of each video frame using multiscale GIST features. Extremely randomised trees are then used to learn a classifier that distinguishes between frames showing a fault and normal frames. The technique is tested on 670 video segments from real sewer inspections of a variety of pipes, supplied by Wessex Water. Detection performance is assessed by plotting receiver operating characteristics and quantifying the area under the curve. Preliminary results indicate high detection accuracy of 88% and an area under the ROC curve of 96%. The machine learning used reduces the footage to a selection of frames containing faults, which can be quickly identified (whether by an engineer or another piece of software), showing promise for use in industrial wastewater network surveys.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2016.07.167,Conference Proceeding,IFAC-PapersOnLine,scopus,2016-01-01,sciencedirect,A proposal for teaching SCADA systems using Virtual Industrial Plants in Engineering Education,https://api.elsevier.com/content/abstract/scopus_id/84994803392,"The main objectives of SCADA (Supervisory Control And Data Acquisition) systems are the supervisory analysis of the system, control algorithms validation, and data acquisition. These systems are normally implemented according to the international standards: UNE-EN ISO 9241, ISAIOI-Human-Machine Interfaces, ISA S5, and in the case treated in this paper The Spanish Royal Decree 488/1997. This paper presents a software architecture for the development of educational laboratories, through industrial virtual plants which models and logic are implemented in Matlab® and used within LabVIEW® through an appropriate protocol. Lab VIEW® from National Instruments, a specific purpose software for this kind of applications, was used, since it allows us to provide a friendly interface, to perform communications, data acquisition and the information management. In addition, to illustrate the use of the proposed architecture, different virtual industrial plants for students of different Bachelor and Master degrees in engineering at the University of Almeria have been developed. This paper shows the different virtual industrial plants that have been developed using SCADA systems to facilitate students’ learning of basic concepts and techniques for an Industrial Informatics course.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-444-63428-3.50407-0,Book Series,Computer Aided Chemical Engineering,scopus,2016-01-01,sciencedirect,Process Integration: Pinch Analysis and Mathematical Programming - Directions for Future Development,https://api.elsevier.com/content/abstract/scopus_id/84994259521,"Numerous studies have been performed process systems engineering field for improving the efficiency of supplying and using energy, water and other resources and consequently for reducing the emissions of greenhouse gases, volatile organic compounds and other pollutants, accumulating a significant body of methods, applications and results. It has become apparent that the resource inputs and effluents of industrial processes and the other units including the business centres, civic objects and even agricultural plants can and are often connected with each other. Most industrial plants and the other units throughout the world still use more energy and water than necessary, they are proven cases in the range 20 – 30 %, emitting too large volumes of Greenhouse Gases and other pollutants.
                  Water-saving measures and the reuse of water may reduce groundwater consumption by as much as 25 – 30 %. Usually reducing resource consumption is achieved by increasing internal recycling and the reuse of energy and material streams. Projects for improving process resource efficiencies can be very beneficial and also potentially improve the public perception of the companies.
                  Motivating, launching and carrying out such projects, however, involve appropriate optimisation, based on adequate process models, applied within the framework of appropriate resource minimisation strategies and procedures. Process Integration supporting process design, integration and optimisation has been around for nearly 45 years. It has been closely related to the development of process systems engineering, as well as utilising mathematical modelling and information technology.
                  In the broader sense Process Integration methods can be classified into those relying on process based insight and targeting on the one hand, mainly employing targeting, heuristics and artificial intelligence—AI. On the other hand are the methods employing detailed mathematical models usually implemented as algebraic models with embedded superstructures in the case of process network synthesis. The methods relying on thermodynamic insights have been first published in the early 1980-s (Linnhoff and Flower, 1978) as well as those using mathematical programming—MP (Papoulias and Grossmann, 1983). There can also be a combined approach (Klemeš and Kravanja, 2013).
                  On the one hand, the concept relying on thermodynamic and/or physical insights using the well-known Pinch Analysis has been the more widely accepted in both academia and industry. Process Integration has thus converged towards two schools of thought, the thermodynamic based (Pinch) and the mathematically based MP, each having its own advantages and drawbacks. The thermodynamic school has mostly preceded that of the MP in generating ideas based on engineering creativity. The MP school has enacted its ideas and described them as explicit mathematical models for solving advanced PI problems.
                  The collaboration between both approaches has been widening, taking from each other the more applicable parts. Its development has been accelerating as the combined methodology has been able to provide answers and support for important issues regarding economic development—energy, water and resources better utilisation and savings. This contribution is targeted towards a short overview of recent achievements and future challenges.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2015.12.071,Conference Proceeding,Procedia CIRP,scopus,2016-01-01,sciencedirect,Enhancing Constraint Propagation in ACO-based Schedulers for Solving the Job Shop Scheduling Problem,https://api.elsevier.com/content/abstract/scopus_id/84968773482,"Increasing number of variants lead to growing complexity in planning processes in production. Not only is the initial planning a tremendous task if there is a huge variety of products but also reacting to changes becomes more frequent and more demanding. Many algorithms being able to solve the static problem need to perform a full recalculation if there is disturbance in production which makes them too time consuming for instant reactions to changes in production.
                  Ant Colony Optimization (ACO) has proven its potentials in solving the theoretical Job Shop Scheduling Problem offering the advantage of not needing an entire recalculation in the case of changes. But when using the algorithm for calculation in real time scenarios with returning data from production plants several restrictions have to be fulfilled. The reaction to those restriction is currently not sufficiently provided by implementations of the ACO which prevents the use in practical applications. These restrictions are modelled as constraints that can for example involve the reaction to disturbances like failures or manual changes. But also considering transportation times or providing the possibility to realize batch processes is discussed. There are different possibilities to realize the reactions to restrictions in ACO, but in this paper they are modelled as constraints affecting the ACO during optimization. The constraint propagation is implemented by restricting the selection of succeeding edges, an approach that only has little impact on computational performance.
                  In this paper the concept of constraining the Ant Colony Optimization in Job Shop Scheduling is being introduced and explained. Subsequently the demand for additional constraints is presented and enhancements to the existing approach are defined and commented theoretically.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2015.07.016,Journal,Expert Systems with Applications,scopus,2016-01-01,sciencedirect,Applying supplier selection methodologies in a multi-stakeholder environment: A case study and a critical assessment,https://api.elsevier.com/content/abstract/scopus_id/84944345232,"In the contemporary global market, supplier selection represents a crucial process for enhancing firms’ competitiveness. In firms operating in low-complexity sectors, supplier selection generally leverages on few significant variables (price, delivery time, quality) and it is often left to the buyers’ experience. On the other hand, in industries characterised by remarkable product complexity, supplier selection systems gain the characteristics of a multi-stakeholder and multi-criteria problem, which needs to be theoretically formalised and realistically adapted to specific contexts.
                  An increasing number of researches have been devoted to the development of different methodologies to cope with this problem. Nevertheless, while the number of applications is growing, there is little empirical evidence of the practical usefulness of such tools, that are mainly tested on numerical examples or computational experiments and focused on a dyadic version of the problem, overlooking the wider set of actors involved in the decision-making problem. The result is a clear dichotomy between academic theory and business practice.
                  Therefore, the paper contributes to understand the above dichotomy by evaluating the applicability to real-world multi-stakeholder problems of the two main approaches proposed in the literature to deal with supplier selection, the analytic hierarchic process (AHP) and the fuzzy set theory (FST). Based on an industrial case study, a thorough discussion is developed, dealing with the issues arising during the implementation and practical functioning of such decision support systems, also providing provide practical guidelines and managerial implications.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ultsonch.2015.07.022,Journal,Ultrasonics Sonochemistry,scopus,2016-01-01,sciencedirect,"Impact of ultrasound on solid-liquid extraction of phenolic compounds from maritime pine sawdust waste. Kinetics, optimization and large scale experiments",https://api.elsevier.com/content/abstract/scopus_id/84938390300,"Maritime pine sawdust, a by-product from industry of wood transformation, has been investigated as a potential source of polyphenols which were extracted by ultrasound-assisted maceration (UAM). UAM was optimized for enhancing extraction efficiency of polyphenols and reducing time-consuming. In a first time, a preliminary study was carried out to optimize the solid/liquid ratio (6g of dry material per mL) and the particle size (0.26cm2) by conventional maceration (CVM). Under these conditions, the optimum conditions for polyphenols extraction by UAM, obtained by response surface methodology, were 0.67W/cm2 for the ultrasonic intensity (UI), 40°C for the processing temperature (T) and 43min for the sonication time (t). UAM was compared with CVM, the results showed that the quantity of polyphenols was improved by 40% (342.4 and 233.5mg of catechin equivalent per 100g of dry basis, respectively for UAM and CVM). A multistage cross-current extraction procedure allowed evaluating the real impact of UAM on the solid–liquid extraction enhancement. The potential industrialization of this procedure was implemented through a transition from a lab sonicated reactor (3L) to a large scale one with 30L volume.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.simpat.2015.05.011,Journal,Simulation Modelling Practice and Theory,scopus,2015-11-01,sciencedirect,A flexible framework for accurate simulation of cloud in-memory data stores,https://api.elsevier.com/content/abstract/scopus_id/84947019341,"In-memory (transactional) data stores, also referred to as data grids, are recognized as a first-class data management technology for cloud platforms, thanks to their ability to match the elasticity requirements imposed by the pay-as-you-go cost model. On the other hand, determining how performance and reliability/availability of these systems vary as a function of configuration parameters, such as the amount of cache servers to be deployed, and the degree of in-memory replication of slices of data, is far from being a trivial task. Yet, it is an essential aspect of the provisioning process of cloud platforms, given that it has an impact on the amount of cloud resources that are planned for usage. To cope with the issue of predicting/analysing the behavior of different configurations of cloud in-memory data stores, in this article we present a flexible simulation framework offering skeleton simulation models that can be easily specialized in order to capture the dynamics of diverse data grid systems, such as those related to the specific (distributed) protocol used to provide data consistency and/or transactional guarantees. Besides its flexibility, another peculiar aspect of the framework lies in that it integrates simulation and machine-learning (black-box) techniques, the latter being used to capture the dynamics of the data-exchange layer (e.g. the message passing layer) across the cache servers. This is a relevant aspect when considering that the actual data-transport/networking infrastructure on top of which the data grid is deployed might be unknown, hence being not feasible to be modeled via white-box (namely purely simulative) approaches. We also provide an extended experimental study aimed at validating instances of simulation models supported by our framework against execution dynamics of real data grid systems deployed on top of either private or public cloud infrastructures. Particularly, our validation test-bed has been based on an industrial-grade open-source data grid, namely Infinispan by JBoss/Red-Hat, and a de-facto standard benchmark for NoSQL platforms, namely YCSB by Yahoo. The validation study has been conducted by relying on both public and private cloud systems, scaling the underlying infrastructure up to 100 (resp. 140) Virtual Machines for the public (resp. private) cloud case. Further, we provide some experimental data related to a scenario where our framework is used for on-line capacity planning and reconfiguration of the data grid system.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compbiomed.2015.07.015,Journal,Computers in Biology and Medicine,scopus,2015-11-01,sciencedirect,"Implementation of a web based universal exchange and inference language for medicine: Sparse data, probabilities and inference in data mining of clinical data repositories",https://api.elsevier.com/content/abstract/scopus_id/84941884468,"We extend Q-UEL, our universal exchange language for interoperability and inference in healthcare and biomedicine, to the more traditional fields of public health surveys. These are the type associated with screening, epidemiological and cross-sectional studies, and cohort studies in some cases similar to clinical trials. There is the challenge that there is some degree of split between frequentist notions of probability as (a) classical measures based only on the idea of counting and proportion and on classical biostatistics as used in the above conservative disciplines, and (b) more subjectivist notions of uncertainty, belief, reliability, or confidence often used in automated inference and decision support systems. Samples in the above kind of public health survey are typically small compared with our earlier “Big Data” mining efforts. An issue addressed here is how much impact on decisions should sparse data have. We describe a new Q-UEL compatible toolkit including a data analytics application DiracMiner that also delivers more standard biostatistical results, DiracBuilder that uses its output to build Hyperbolic Dirac Nets (HDN) for decision support, and HDNcoherer that ensures that probabilities are mutually consistent. Use is exemplified by participating in a real word health-screening project, and also by deployment in a industrial platform called the BioIngine, a cognitive computing platform for health management.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2015.04.036,Journal,Expert Systems with Applications,scopus,2015-07-28,sciencedirect,Clustering and visualization of failure modes using an evolving tree,https://api.elsevier.com/content/abstract/scopus_id/84937967581,"Despite the popularity of Failure Mode and Effect Analysis (FMEA) in a wide range of industries, two well-known shortcomings are the complexity of the FMEA worksheet and its intricacy of use. To the best of our knowledge, the use of computation techniques for solving the aforementioned shortcomings is limited. As such, the idea of clustering and visualization pertaining to the failure modes in FMEA is proposed in this paper. A neural network visualization model with an incremental learning feature, i.e., the evolving tree (ETree), is adopted to allow the failure modes in FMEA to be clustered and visualized as a tree structure. In addition, the ideas of risk interval and risk ordering for different groups of failure modes are proposed to allow the failure modes to be ordered, analyzed, and evaluated in groups. The main advantages of the proposed method lie in its ability to transform failure modes in a complex FMEA worksheet to a tree structure for better visualization, while maintaining the risk evaluation and ordering features. It can be applied to the conventional FMEA methodology without requiring additional information or data. A real world case study in the edible bird nest industry in Sarawak (Borneo Island) is used to evaluate the usefulness of the proposed method. The experiments show that the failure modes in FMEA can be effectively visualized through the tree structure. A discussion with FMEA users engaged in the case study indicates that such visualization is helpful in comprehending and analyzing the respective failure modes, as compared with those in an FMEA table. The resulting tree structure, together with risk interval and risk ordering, provides a quick and easily understandable framework to elucidate important information from complex FMEA forms; therefore facilitating the decision-making tasks by FMEA users. The significance of this study is twofold, viz., the use of a computational visualization approach to tackling two well-known shortcomings of FMEA; and the use of ETree as an effective neural network learning paradigm to facilitate FMEA implementations. These findings aim to spearhead the potential adoption of FMEA as a useful and usable risk evaluation and management tool by the wider community.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.measurement.2015.06.004,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2015-07-06,sciencedirect,A signal pre-processing algorithm designed for the needs of hardware implementation of neural classifiers used in condition monitoring,https://api.elsevier.com/content/abstract/scopus_id/84934767009,"Gearboxes have a significant influence on the durability and reliability of a power transmission system. Currently, extensive research studies are being carried out to increase the reliability of gearboxes working in the energy industry, especially with a focus on planetary gears in wind turbines and bucket wheel excavators. In this paper, a signal pre-processing algorithm designed for condition monitoring of planetary gears working in non-stationary operation is presented. The algorithm is dedicated for hardware implementation on Field Programmable Gate Arrays (FPGAs). The purpose of the algorithm is to estimate the features of a vibration signal that are related to failures, e.g. misalignment and unbalance. These features can serve as the components of an input vector for a neural classifier. The approach proposed here has several important benefits: it is resistant to small speed fluctuations up to 7%, it can be performed in real-time conditions and its implementation does not require many resources of FPGAs.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijfoodmicro.2015.03.010,Journal,International Journal of Food Microbiology,scopus,2015-07-02,sciencedirect,A strategy to establish food safety model repositories,https://api.elsevier.com/content/abstract/scopus_id/84926308733,"Transferring the knowledge of predictive microbiology into real world food manufacturing applications is still a major challenge for the whole food safety modelling community. To facilitate this process, a strategy for creating open, community driven and web-based predictive microbial model repositories is proposed. These collaborative model resources could significantly improve the transfer of knowledge from research into commercial and governmental applications and also increase efficiency, transparency and usability of predictive models. To demonstrate the feasibility, predictive models of Salmonella in beef previously published in the scientific literature were re-implemented using an open source software tool called PMM-Lab. The models were made publicly available in a Food Safety Model Repository within the OpenML for Predictive Modelling in Food community project. Three different approaches were used to create new models in the model repositories: (1) all information relevant for model re-implementation is available in a scientific publication, (2) model parameters can be imported from tabular parameter collections and (3) models have to be generated from experimental data or primary model parameters. All three approaches were demonstrated in the paper. The sample Food Safety Model Repository is available via: http://sourceforge.net/projects/microbialmodelingexchange/files/models and the PMM-Lab software can be downloaded from http://sourceforge.net/projects/pmmlab/. This work also illustrates that a standardized information exchange format for predictive microbial models, as the key component of this strategy, could be established by adoption of resources from the Systems Biology domain.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2015.08.045,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-07-01,sciencedirect,Downhole pressure estimation using committee machines and neural networks,https://api.elsevier.com/content/abstract/scopus_id/84992511260,"In gas-lifted oil wells the monitoring of downhole pressure plays an important role. However, the permanent downhole gauge (PDG) sensor often fails. Because maintenance or replacement of PDGs is usually unfeasible, soft-sensors are promising alternatives to monitor the downhole pressure in the case of sensor failure. In this paper, a data-driven soft-sensor is implemented to estimate the downhole pressure using committee machines composed by finite impulse response (FIR) neural networks. Experimental results in three real datasets of the same oil well indicate that the identified soft-sensor is able to predict the downhole pressure with satisfactory accuracy. The model input variables were selected by statistical tests which increased insight concerning such variables. Committee machines outperformed single-model soft-sensors on experimental data.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jal.2014.11.005,Journal,Journal of Applied Logic,scopus,2015-06-01,sciencedirect,Implementation and testing of a soft computing based model predictive control on an industrial controller,https://api.elsevier.com/content/abstract/scopus_id/84922903117,"This work presents a real time testing approach of an Intelligent Multiobjective Nonlinear-Model Predictive Control Strategy (iMO-NMPC). The goal is the testing and analysis of the feasibility and reliability of some Soft Computing (SC) techniques running on a real time industrial controller. In this predictive control strategy, a Multiobjective Genetic Algorithm is used together with a Recurrent Artificial Neural Network in order to obtain the control action at each sampling time. The entire development process, from the numeric simulation of the control scheme to its implementation and testing on a PC-based industrial controller, is also presented in this paper. The computational time requirements are discussed as well. The obtained results show that the SC techniques can be considered also to tackle highly nonlinear and coupled complex control problems in real time, thus optimising and enhancing the response of the control loop. Therefore this work is a contribution to spread the SC techniques in on-line control applications, where currently they are relegated mainly to be used off-line, as is the case of optimal tuning of control strategies.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.bios.2014.07.084,Journal,Biosensors and Bioelectronics,scopus,2015-05-05,sciencedirect,"Lytic enzymes as selectivity means for label-free, microfluidic and impedimetric detection of whole-cell bacteria using ALD-Al<inf>2</inf>O<inf>3</inf> passivated microelectrodes",https://api.elsevier.com/content/abstract/scopus_id/84922271489,"Point-of-care (PoC) diagnostics for bacterial detection offer tremendous prospects for public health care improvement. However, such tools require the complex combination of the following performances: rapidity, selectivity, sensitivity, miniaturization and affordability. To meet these specifications, this paper presents a new selectivity method involving lysostaphin together with a CMOS-compatible impedance sensor for genus-specific bacterial detection. The method enables the sample matrix to be directly flown on the polydopamine-covered sensor surface without any pre-treatment, and considerably reduces the background noise. Experimental proof-of-concept, explored by simulations and confirmed through a setup combining simultaneous optical and electrical real-time monitoring, illustrates the selective and capacitive detection of Staphylococcus epidermidis in synthetic urine also containing Enterococcus faecium. While providing capabilities for miniaturization and system integration thanks to CMOS compatibility, the sensors show a detection limit of ca. 108 (CFU/mL).min in a 1.5μL microfluidic chamber with an additional setup time of 50min. The potentials, advantages and limitations of the method are also discussed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compind.2015.05.001,Journal,Computers in Industry,scopus,2015-05-04,sciencedirect,Artificial cognitive control with self-x capabilities: A case study of a micro-manufacturing process,https://api.elsevier.com/content/abstract/scopus_id/84955410573,"Nowadays, even though cognitive control architectures form an important area of research, there are many constraints on the broad application of cognitive control at an industrial level and very few systematic approaches truly inspired by biological processes, from the perspective of control engineering. Thus, our main purpose here is the emulation of human socio-cognitive skills, so as to approach control engineering problems in an effective way at an industrial level. The artificial cognitive control architecture that we propose, based on the shared circuits model of socio-cognitive skills, seeks to overcome limitations from the perspectives of computer science, neuroscience and systems engineering. The design and implementation of artificial cognitive control architecture is focused on four key areas: (i) self-optimization and self-leaning capabilities by estimation of distribution and reinforcement-learning mechanisms; (ii) portability and scalability based on low-cost computing platforms; (iii) connectivity based on middleware; and (iv) model-driven approaches. The results of simulation and real-time application to force control of micro-manufacturing processes are presented as a proof of concept. The proof of concept of force control yields good transient responses, short settling times and acceptable steady-state error. The artificial cognitive control architecture built into a low-cost computing platform demonstrates the suitability of its implementation in an industrial setup.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2015.06.036,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,Dexrov: Dexterous undersea inspection and maintenance in presence of communication latencies,https://api.elsevier.com/content/abstract/scopus_id/84992521813,"Underwater inspection and maintenance (e.g. in the oil & gas industry) are demanding and costly activities for which ROV based setups are often deployed in addition or in substitution to deep divers - contributing to operations risks and costs cutting. However the operation of a ROV requires significant off-shore dedicated manpower to handle and operate the robotic platform. In order to reduce the burden of operations, DexROV proposes to work out more cost effective and time efficient ROV operations, where manned support is in a large extent delocalized onshore (i.e. from a ROV control center), possibly at a large distance from the actual operations, relying on satellite communications. The proposed scheme also makes provision for advanced dexterous manipulation capabilities, exploiting human expertise when deemed useful. The outcomes of the project will be integrated and evaluated in a series of tests and evaluation campaigns, culminating with a realistic deep sea (1,300 meters) trial.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2015.06.159,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,A benchmark dataset for depth sensor based activity recognition in a manufacturing process,https://api.elsevier.com/content/abstract/scopus_id/84953879013,Algorithms for automated recognition of human activities are crucial for supporting the next generation of process measures in manufacturing. While there is active research underway for many sensor systems and algorithms they will need to be tested in real-world conditions in order to mature and become robust or generalized enough for broad deployment in industry. In this paper we present a case study and dataset from a real-world setting along with three performance measures for six common classifiers. The intent is to provide a dataset and baseline performance level metrics so that others may compare their activity recognition algorithms to a common standard.,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2015.06.228,Conference Proceeding,IFAC-PapersOnLine,scopus,2015-05-01,sciencedirect,Multicast dataset synchronization and agent negotiation in distributed manufacturing control systems,https://api.elsevier.com/content/abstract/scopus_id/84953870369,"Multi agent systems represent an elegant approach for the control architecture of manufacturing systems. Distributed control architectures have the potential to achieve greater flexibility by being capable of local decision making based on real time reasoning. One of the main challenges of these distributed architectures is represented by the capability to synchronize the production data across all execution points in a reliable and consistent fashion. In this context, this paper aims to resolve the problems associated with real time production data synchronization in distributed multi-agent control systems by proposing a common dataset synchronized across all agent entities using multicast network communication. On top of this common dataset approach, an agent negotiation mechanism is proposed that addresses the operation sequencing and resource allocation in decentralized operation model. The pilot implementation is using JADE multi agent platform and JGroups for real time data synchronization and NetLogo for abstract representation of the simulation system. Experimental results gathered from the pilot implementation are discussed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.isatra.2014.11.011,Journal,ISA Transactions,scopus,2015-05-01,sciencedirect,Online monitoring and control of particle size in the grinding process using least square support vector regression and resilient back propagation neural network,https://api.elsevier.com/content/abstract/scopus_id/84929271125,"Particle size soft sensing in cement mills will be largely helpful in maintaining desired cement fineness or Blaine. Despite the growing use of vertical roller mills (VRM) for clinker grinding, very few research work is available on VRM modeling. This article reports the design of three types of feed forward neural network models and least square support vector regression (LS-SVR) model of a VRM for online monitoring of cement fineness based on mill data collected from a cement plant. In the data pre-processing step, a comparative study of the various outlier detection algorithms has been performed. Subsequently, for model development, the advantage of algorithm based data splitting over random selection is presented. The training data set obtained by use of Kennard–Stone maximal intra distance criterion (CADEX algorithm) was used for development of LS-SVR, back propagation neural network, radial basis function neural network and generalized regression neural network models. Simulation results show that resilient back propagation model performs better than RBF network, regression network and LS-SVR model. Model implementation has been done in SIMULINK platform showing the online detection of abnormal data and real time estimation of cement Blaine from the knowledge of the input variables. Finally, closed loop study shows how the model can be effectively utilized for maintaining cement fineness at desired value.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2015.03.034,Journal,Applied Soft Computing Journal,scopus,2015-04-30,sciencedirect,A genetic algorithm based decision support system for the multi-objective node placement problem in next wireless generation network,https://api.elsevier.com/content/abstract/scopus_id/84929176273,"The node placement problem involves positioning and configuring infrastructure for wireless networks. Applied to next generation networks, it establishes a new wireless architecture able to integrate heterogeneous components that can collaborate and exchange data. Furthermore, the heterogeneity of wireless networks makes the problem more intractable. This paper presents a novel multi-objective node placement problem that optimizes concurrently four objectives: maximizing communication coverage, minimizing the active structures’ costs, maximizing of the total capacity bandwidth and minimizing the noise level in the network. Known to be 
                        NP
                     -hard, the problem can be approached by applying heuristics mainly for large problem instances. As the number of nodes to place is not determined beforehand; we propose to apply a multi-objective variable-length genetic algorithm (VLGA) that simultaneously searches for the optimal number, positions and nature of heterogeneous nodes and communication devices. The performance of the VLGA is highlighted through the implementation of a decision support system (DSS) applied to the surveillance maritime problem using real data instances. We compare the ability of the proposed algorithm with an existing multi-objective model from the literature in order to validate its effectiveness in dealing with heterogeneous components. The results show that the proposed model well fits the network architecture constraints with a better balance between the objectives applied to the surveillance problem.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.mee.2015.01.018,Journal,Microelectronic Engineering,scopus,2015-04-20,sciencedirect,An FPGA based human detection system with embedded platform,https://api.elsevier.com/content/abstract/scopus_id/84922572817,"Focusing on the computing speed of the practical machine learning based human detection system at the testing (detecting) stage to reach the real-time requirement in an embedded platform, the idea of iterative computing HOG with FPGA circuit design is proposed. The completed HOG accelerator contains gradient calculation circuit module and histogram accumulation circuit module. The linear SVM classification algorithm producing a number of necessary weak classifiers is combined with Adaboost algorithm to establish a strong classifier. The human detection is successfully implemented on a portable embedded platform to reduce the system cost and size. Experimental result shows that the performance error of accuracy appears merely about 0.1–0.4% in comparison between the presented FPGA based HW/SW co-design and the PC based pure software. Meanwhile, the computing speed achieves the requirement of a real-time embedded system, 15fps.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-800341-1.00001-2,Book,Industrial Agents: Emerging Applications of Software Agents in Industry,scopus,2015-03-12,sciencedirect,Software Agent Systems,https://api.elsevier.com/content/abstract/scopus_id/84944408386,"Agents and multi-agent systems are one of the most fascinating topics in computer science. They attracted and unified not only researchers from nearly all computer science areas but also researchers from other core disciplines such as psychology, sociology, biology, or control engineering. In the meantime, agent-based systems successfully prove their usefulness in many different real-life application areas, especially industrial ones. This is a clear sign that this discipline has become mature. This chapter presents a comprehensive state-of-the-art introduction into advanced software agents and multi-agent systems. Properties and types of agents and multi-agent systems are discussed, which include precise definitions of both. A successful cooperation between agents is only possible if they can communicate in an efficient and semantically meaningful way. Thus, relevant communication strategies are discussed. Agent-based applications can be very powerful, complex systems. Their development can profit a lot from adequate support tools. Different development support options and environments are discussed in some detail. Due to their nature, multi-agent systems are excellent candidates for the realization of comprehensive simulations, especially if the individuality and uniqueness of components of the simulation environment play an important role. The second part of the chapter addresses supporting technologies and concepts. Ontologies, self-organization and emergence, and swarm intelligence and stigmergy are introduced and discussed in some detail.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.isatra.2014.09.019,Journal,ISA Transactions,scopus,2015-03-01,sciencedirect,Soft sensor for real-time cement fineness estimation,https://api.elsevier.com/content/abstract/scopus_id/84926259783,"This paper describes the design and implementation of soft sensors to estimate cement fineness. Soft sensors are mathematical models that use available data to provide real-time information on process variables when the information, for whatever reason, is not available by direct measurement. In this application, soft sensors are used to provide information on process variable normally provided by off-line laboratory tests performed at large time intervals. Cement fineness is one of the crucial parameters that define the quality of produced cement. Providing real-time information on cement fineness using soft sensors can overcome limitations and problems that originate from a lack of information between two laboratory tests. The model inputs were selected from candidate process variables using an information theoretic approach. Models based on multi-layer perceptrons were developed, and their ability to estimate cement fineness of laboratory samples was analyzed. Models that had the best performance, and capacity to adopt changes in the cement grinding circuit were selected to implement soft sensors. Soft sensors were tested using data from a continuous cement production to demonstrate their use in real-time fineness estimation. Their performance was highly satisfactory, and the sensors proved to be capable of providing valuable information on cement grinding circuit performance. After successful off-line tests, soft sensors were implemented and installed in the control room of a cement factory. Results on the site confirm results obtained by tests conducted during soft sensor development.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compag.2014.12.010,Journal,Computers and Electronics in Agriculture,scopus,2015-02-01,sciencedirect,A Decision Support System to design modified atmosphere packaging for fresh produce based on a bipolar flexible querying approach,https://api.elsevier.com/content/abstract/scopus_id/84921031926,"To design new packaging for fresh food, stakeholders of the food chain express their needs and requirements, according to some goals and objectives. These requirements can be gathered into two groups: (i) fresh food related characteristics and (ii) packaging intrinsic characteristics. Modified Atmosphere Packaging (MAP) is an efficient way to delay senescence and spoilage and thus to extend the very short shelf life of respiring products such as fresh fruits and vegetables. Consequently, packaging O2/CO2 permeabilities must fit the requirements of fresh fruits and vegetable as predicted by virtual MAP simulating tools. Beyond gas permeabilities, the choice of a packaging material for fresh produce includes numerous other factors such as the cost, availability, potential contaminants of raw materials, process ability, and waste management constraints. For instance, the user may have the following multi-criteria query for his/her product asking for a packaging with optimal gas permeabilities that guarantee product quality and optionally a transparent packaging material made from renewable resources with a cost for raw material less than 3€/kg. To help stakeholders taking a rational decision based on the expressed needs, a new multi-criteria Decision Support System (DSS) for designing biodegradable packaging for fresh produce has been built. In this paper we present the functional specification, the software architecture and the implementation of the developed tool. This tool includes (i) a MAP simulation module combining mass transfer models and respiration of the food, (ii) a multi-criteria flexible querying module which handles imprecise, uncertain and missing data stored in the database. We detail its operational functioning through a real life case study to determine the most satisfactory materials for apricots packaging.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.promfg.2015.07.372,Journal,Procedia Manufacturing,scopus,2015-01-01,sciencedirect,Case Study: Use of Online Tools in the Classroom and their Impact on Industrial Design Pedagogy,https://api.elsevier.com/content/abstract/scopus_id/85009959445,"Industrial Design education is going through a rapid evolution with more Design students making use of internet resources and tools such as crowdsourcing, 3D printing services, and other web-based tools to validate their ideas more quickly. The popularity of online 3D printing services such as Shapeways and Sculpteo accelerate the design process and learning. These services allow the designer to “print” virtually in any material such as plastics or metals. The impact of this new technology and other new web-based tools is significant not only in the industry but in the classroom as well. Current Industrial Design pedagogy is still partially based on technology, materials and processes that were developed a century ago. For example, pencils and paper are still the primary idea development tool. Books and magazines used to be the primary research tool but already have been surpassed by the Internet. Computer technology has improved significantly since the appearance of the first PCs, Macs and CNC machines. With all these advances in technology, one aspect of Industrial Design education that needs to be re-visited is the pedagogy of Design Drafting in this new age of online 3D printing services. The traditional Design pedagogy that was based on the development of different skills or competencies in separate courses or classes have not changed significantly in the last 40 years, 3D printing technology could potentially change this situation. Some new academic papers discuss this newer trend in Industrial Design schools but very few provide examples on how they implemented the new Internet-based 3D printing services in their curriculum. Industrial Design schools need to adapt quickly to the new reality, embracing Internet resources and online tools as core skills that every designer must have. This paper will discuss one case in particular where student projects were developed using online 3D printing services.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.bica.2015.04.008,Journal,Biologically Inspired Cognitive Architectures,scopus,2015-01-01,sciencedirect,Automatic navigation of wall following mobile robot using Adaptive Resonance Theory of Type-1,https://api.elsevier.com/content/abstract/scopus_id/84960798237,"The automatic navigation of wall following robot is playing important role in various real world tasks such as underwater exploration, unmanned flight, and in automotive industries based on its computational complexity. In this work, a novel navigation approach based on biologically inspired neural network, known as “Adaptive Resonance Theory-1” which was proposed by Carpenter and Grossberg, has been implemented and investigated for navigation of wall following mobile robots. The proposed navigation algorithm is successfully tested with three sensor reading datasets obtained from clockwise navigation of SCITOS G5 mobile robot. Test decision accuracy (%), and simulation time were used as performance analysis parameters for the proposed algorithm and it has been found that the present work can achieve 99.59% of maximum decision accuracy.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2015.07.575,Conference Proceeding,Procedia Computer Science,scopus,2015-01-01,sciencedirect,A Decision Support System for Estimating Growth Parameters of Commercial Fish Stock in Fisheries Industries,https://api.elsevier.com/content/abstract/scopus_id/84948408653,"In this paper we present a Decision Support System (DSS) to estimate the values of the growth parameters from the yield effort data of a harvested population. The software is developed using Visual C# programming language in windows form application. Several softwares are used to check the validity of the multiple linear regression computational output in the DSS. The multiple linear regression computation is done using Ordinary Least Square (OLS) method and its computation comes from the discretization of two most known growth models, namely Logistic growth model and Gompertz growth model. We discretize the models in terms of yield effort variables and used the resulting equations as the bases for computing the intrinsic growth rate r and the carrying capacity K parameters which are the main ingredients in the MSY formula. Those models are also used as two selection tools placed in the main menu of the software. We use an existing published data as an example to estimate the growth parameters. We also compute the Maximum Sustainable Yield (MSY) for each model which represents the maximum amount of allowable biomass extracted from the fish population without harming the sustainability of the fisheries. Technically it suggests the value of the maximum sustainable yield as a decision to the harvester in managing the population and is often represented in a function of the growth parameters of the harvested population. Knowing the right growth parameters of the population is critical in determining the MSY, since the MSY is not stable. Hence, using an inaccurate values of growth parameters is likely detrimental in applying the MSY to the real fisheries. Our DSS has a contribution as a tool in reducing the error in calculating the growth parameters and the MSY. We found that the results are in agreement with the known literatures.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmsy.2014.06.004,Journal,Journal of Manufacturing Systems,scopus,2015-01-01,sciencedirect,"A toolbox for the design, planning and operation of manufacturing networks in a mass customisation environment",https://api.elsevier.com/content/abstract/scopus_id/84942294486,"The task of design, planning and operation of manufacturing networks is becoming more and more challenging for companies, as globalisation, mass customisation and the turbulent economic landscape create demand volatility, uncertainties and high complexity. In this context, this paper investigates the performance of decentralised manufacturing networks through a set of methods developed into a software framework in a toolbox approach. The Tabu Search and Simulated Annealing metaheuristic methods are used together with an Artificial Intelligence method, called Intelligent Search Algorithm. A multi-criteria decision making procedure is carried out for the evaluation of the quality of alternative manufacturing network configurations using multiple conflicting criteria including dynamic complexity, reliability, cost, time, quality and environmental footprint. A comparison of the performance of each method based on the quality of the solutions that it provided is carried out. The statistical design of experiments robust engineering technique is used for the calibration of the adjustable parameters of the methods. Moreover, the impact of demand fluctuation to the operational performance of the alternative networks, expressed thorough a dynamic complexity indicator, is investigated through simulation. The developed framework is validated through a real life case, with data coming from the CNC machine building industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-444-63578-5.50097-9,Book Series,Computer Aided Chemical Engineering,scopus,2015-01-01,sciencedirect,Data Analysis and Modelling of a Fluid Catalytic Cracking Unit (FCCU) for an Implementation of Real Time Optimization,https://api.elsevier.com/content/abstract/scopus_id/84940474774,"In the Fluid Catalytic Cracking Units (FCCU) large hydrocarbon molecules are cracked into smaller molecules, generating high value products such as diesel, gasoline and useful petrochemical olefins. The control of these units is fundamental to maintain a satisfactory operation. Hence, the Real Time Optimization has proved an interesting strategy. A dynamic simulation of a FCCU was developed using a phenomenological industrial validated model. A Dynamic Neural Network (DNN) was trained with data from the FCCU model and gross and systematic errors were added to employ this system as a virtual plant. Data from this virtual plant were used to study strategies of online data processing, considering steady state identification (SSI) and gross error detection (GED), in order to eliminate measurement noise, as the initial steps into an RTO implementation.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.future.2014.11.015,Journal,Future Generation Computer Systems,scopus,2015-01-01,sciencedirect,On-line failure prediction in safety-critical systems,https://api.elsevier.com/content/abstract/scopus_id/84917709364,"In safety-critical systems such as Air Traffic Control system, SCADA systems, Railways Control Systems, there has been a rapid transition from monolithic systems to highly modular ones, using off-the-shelf hardware and software applications possibly developed by different manufactures. This shift increased the probability that a fault occurring in an application propagates to others with the risk of a failure of the entire safety-critical system. This calls for new tools for the on-line detection of anomalous behaviors of the system, predicting thus a system failure before it happens, allowing the deployment of appropriate mitigation policies.
                  The paper proposes a novel architecture, namely CASPER, for online failure prediction that has the distinctive features to be (i) black-box: no knowledge of applications internals and logic of the system is required (ii) non-intrusive: no status information of the components is used such as CPU or memory usage; The architecture has been implemented to predict failures in a real Air Traffic Control System. CASPER exhibits high degree of accuracy in predicting failures with low false positive rate. The experimental validation shows how operators are provided with predictions issued a few hundred of seconds before the occurrence of the failure.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2014.09.004,Journal,International Journal of Production Economics,scopus,2015-01-01,sciencedirect,An RFID-based intelligent decision support system architecture for production monitoring and scheduling in a distributed manufacturing environment,https://api.elsevier.com/content/abstract/scopus_id/84915733989,"Global manufacturing companies have some pressing needs to improve production visibility and decision-making performance by implementing effective production monitoring and scheduling. This paper proposes a radio frequency identification (RFID)-based intelligent decision support system architecture to handle production monitoring and scheduling in a distributed manufacturing environment. A pilot implementation of the architecture is reported in a distributed clothing manufacturing environment. RFID and cloud technologies were integrated for real-time and remote production capture and monitoring. Intelligent optimization techniques were also implemented to generate effective production scheduling solutions. A prototype system with remote monitoring and production scheduling functions was developed and implemented in a distributed manufacturing environment, which demonstrated the effectiveness of the architecture. The proposed architecture has good extensibility and scalability, which can easily be integrated with production decision-making as well as production and logistics operations in the supply chain. Lastly, this paper discusses the difficulties encountered and lessons learned during system implementation and the managerial implications of the proposed architecture.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2014.10.018,Journal,Applied Soft Computing Journal,scopus,2015-01-01,sciencedirect,Performance assessment of heat exchanger using intelligent decision making tools,https://api.elsevier.com/content/abstract/scopus_id/84912132496,"Process and manufacturing industries today are under pressure to deliver high quality outputs at lowest cost. The need for industry is therefore to implement cost savings measures immediately, in order to remain competitive. Organizations are making strenuous efforts to conserve energy and explore alternatives. This paper explores the development of an intelligent system to identify the degradation of heat exchanger system and to improve the energy performance through online monitoring system. The various stages adopted to achieve energy performance assessment are through experimentation, design of experiments and online monitoring system. Experiments are conducted as per full factorial design of experiments and the results are used to develop artificial neural network models. The predictive models are used to predict the overall heat transfer coefficient of clean/design heat exchanger. Fouled/real system value is computed with online measured data. Overall heat transfer coefficient of clean/design system is compared with the fouled/real system and reported. It is found that neural net work model trained with particle swarm optimization technique performs better comparable to other developed neural network models. The developed model is used to assess the performance of heat exchanger with the real/fouled system. The performance degradation is expressed using fouling factor, which is derived from the overall heat transfer coefficient of design system and real system. It supports the system to improve the performance by asset utilization, energy efficient and cost reduction in terms of production loss. This proposed online energy performance system is implemented into the real system and the adoptability is validated.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jss.2014.07.038,Journal,Journal of Systems and Software,scopus,2014-11-01,sciencedirect,A learning-based module extraction method for object-oriented systems,https://api.elsevier.com/content/abstract/scopus_id/84908163044,"Developers apply object-oriented (OO) design principles to produce modular, reusable software. Therefore, service-specific groups of related software classes called modules arise in OO systems. Extracting the modules is critical for better software comprehension, efficient architecture recovery, determination of service candidates to migrate legacy software to a service-oriented architecture, and transportation of such services to cloud-based distributed systems. In this study, we propose a novel approach to automatic module extraction to identify services in OO software systems. In our approach, first we create a weighted and directed graph of the software system in which vertices and edges represent the classes and their relations, respectively. Then, we apply a clustering algorithm over the graph to extract the modules. We calculate the weight of an edge by considering its probability of being within a module or between modules. To estimate these positional probabilities, we propose a machine-learning-based classification system that we train with data gathered from a real-world OO reference system. We have implemented an automatic module extraction tool and evaluated the proposed approach on several open-source and industrial projects. The experimental results show that the proposed approach generates highly accurate decompositions that are close to authoritative module structures and outperforms existing methods.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.mejo.2013.12.006,Journal,Microelectronics Journal,scopus,2014-03-01,sciencedirect,Enhancing confidence in indirect analog/RF testing against the lack of correlation between regular parameters and indirect measurements,https://api.elsevier.com/content/abstract/scopus_id/84897670549,"The greedy specification testing remains mandatory for analog and radio frequency (RF) integrated circuits because of the accuracy of the sorting based on these measurements. Unfortunately, to be implemented, this kind of testing method often incurs very high costs (expensive instruments, long test time…). A common approach, in the literature, is the so-called indirect/alternate test strategy. This strategy consists in deriving targeted specifications from low-cost Indirect Measurements (IMs). During the industrial test phase, the estimation of regular specifications using IMs is based on a correlation model that has been built previously, during a training phase. Despite the substantial test cost reduction offered by this strategy, its deployment in industry is limited, mainly because of a lack of confidence in the accuracy of estimations made by the correlation model. A solution to increase the confidence in the estimation of specifications using the indirect approach is to implement redundancy in the prediction phase. In this paper, we demonstrate that the redundancy implementation brings more than identifying rare misjudged circuits from a high-correlated model. Indeed redundancy massively increases the accuracy despite of the lack of accurate models that have been assumed in previous implementations of redundant indirect testing. This approach is illustrated on a real case study for which we have experimental measurements on a set of 10,000 devices.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.patcog.2013.09.007,Journal,Pattern Recognition,scopus,2014-03-01,sciencedirect,The cluster assessment of facial attractiveness using fuzzy neural network classifier based on 3D Moiré features,https://api.elsevier.com/content/abstract/scopus_id/84888386508,"Facial attractiveness has long been argued upon varied emphases by philosophers, artists, psychologists and biologists. A number of studies empirically investigated how facial attractiveness was influenced by 2D facial characteristics, such as symmetry, averageness and golden ratio. However, few implementations of facial beauty assessment were based on 3D facial features. The purpose of this paper is to propose a novel cluster assessment system for facial attractiveness that is characterized by the incorporation of 3D geometric Moiré features with an adjusted fuzzy neural network (FNN). We first extract 3D facial features from images acquired by a 3dMD scanner. Seven Moiré features are employed to represent a 3D facial image. The FNN classifier, taking the Moiré features as the parameters, is then trained and validated against independently conducted attractiveness ratings. A number of diverse referees were invited and offered their attractiveness ratings over a five-item Likert scale for 100 female facial images. The proposed assessment presents a high accuracy rate of 90%, and the area under curve (AUC) computed from the receiver operating characteristic (ROC) curve is 0.95. The results show that the perceptions of facial attractiveness are essentially consensus among raters, and can be mathematically modeled through supervised learning techniques. The high accuracy achieved proves that the proposed FNN classifier can serve as a general, automated and human-like judgment tool for objective classification of female facial attractiveness, and thus has potential applications to the entertainment industry, cosmetic industry, virtual media, and plastic surgery.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.enbuild.2014.08.004,Journal,Energy and Buildings,scopus,2014-01-01,sciencedirect,Neural network model ensembles for building-level electricity load forecasts,https://api.elsevier.com/content/abstract/scopus_id/84907570987,"The future power grid is expected to provide unprecedented flexibility in how energy is generated, distributed, and managed, which increasingly necessitates an ability to perform accurate short-term small-scale electricity load and generation forecasting, e.g., at the level of individual buildings or sites. In this paper, we present a novel building-level neural network-based ensemble model for day-ahead electricity load forecasting and show that it outperforms the previously established best performing model, SARIMA, by up to 50%, in the context of load data from half a dozen operational commercial and industrial sites. In addition, we show a straightforward, automated way to select model parameters, making our model practical for use in real deployments.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.anifeedsci.2014.04.017,Journal,Animal Feed Science and Technology,scopus,2014-01-01,sciencedirect,Keeping under control a liquid feed fermentation process for pigs: A reality scale pilot based study,https://api.elsevier.com/content/abstract/scopus_id/84903816625,"An original and fully automated liquid feeding pilot has been designed and implemented to monitor and optimize the fermentation process of liquid feed for pigs at a pre-industrial scale. The installation was designed and instrumented to continuously record the temperature, pH and redox potential (E
                     
                        h
                     ) during the fermentation course of wheat flour based feed mixed with water in a 1:2.5 (w:w) ratio. Single and multiple batches experiments were carried-out with feed inoculation achieved by leftover or with a selected culture of lactic acid producing bacteria (LAB). Physicochemical and microbiological characteristics of the fermentation process which include lactic and acetic acids and ethanol concentrations, enumerations of lactic acid producing bacteria, yeasts, total coliforms and Escherichia coli, were monitored and analyzed as a function of the main feed control factors: incubation time, operating temperature, feed time schedule and percentage of leftover. From batch experiments, it was observed that increasing the operating temperature from 15 to 30°C, accelerates the rate of fermentation by reducing about 5–6-folds the process latency and the duration to reach a pH value of 4.0 which is considered as optimal to achieve biosafety. Nevertheless, this does not prevent the blooming of coliforms as their counts increases from 4 to 6 log10
                     CFU/mL within 24h. In opposite, multiple batches are proved to be effective in both accelerating the fermentation rates and reducing the survival of Coliform bacteria in fermented liquid feed (FLF). Feed fermented at 25°C during 24-h cycles with a 22% leftover ensures the prominence of LAB strains over yeasts with a population level that stabilizes at around 9 log10
                     CFU/mL (vs. 7 log10
                     CFU/mL for single batches), a lactic acid production up to 35g/kg dry matter (DM) and a pH value between 5 and 3.5 throughout the period. Concomitantly, total Coliforms number decreases from 7.5 to 2.2 log10
                     CFU/mL within 72h whereas E. coli became undetectable beyond 48h. Addition of a starter culture (Pediococcus acidilactici, Bactocell®) at 9 log10
                     CFU/kg DM at the initial stage of FLF production reduces 25–35 times the total coliforms and E. coli counts. No significant differences in the amounts of organic compounds produced by the microflora as compared to the control FLF after 80h nor in the microbial levels are observed. It is concluded that sequences of fermentation cycles allows, in a given temperature range, establishing a positive, robust, microbial ecosystem.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.powtec.2014.05.051,Journal,Powder Technology,scopus,2014-01-01,sciencedirect,"Soft sensing of particle size in a grinding process: Application of support vector regression, fuzzy inference and adaptive neuro fuzzy inference techniques for online monitoring of cement fineness",https://api.elsevier.com/content/abstract/scopus_id/84902965121,"Use of soft sensors for online particle size monitoring in a grinding process is a viable alternative since physical sensors for the same are not available for many such processes. Cement fineness is an important quality parameter in the cement grinding process. However, very few studies have been done for soft sensing of cement fineness in the grinding process. Moreover, most of the grinding process modeling approaches have been reported for ball mills and rarely any modeling of vertical roller mill is available. In this research, modeling of vertical roller mill used for clinker grinding has been done using support vector regression (SVR), fuzzy inference and adaptive neuro fuzzy inference(ANFIS) techniques since these techniques have not yet been largely explored for particle size soft sensing. The modeling has been done by collection of the real industrial data from a cement grinding process followed by data cleaning and a structured method of dividing the data into training and validation data sets using the Kennard–Stone subset selection algorithm. Optimum SVR hyper parameters were determined using a combined approach of analytical method and grid search plus cross validation. The models were developed using MATLAB from the training data and were tested with the validation data. Results reveal that the proposed ANFIS model of the clinker grinding process shows much superior performance compared with the other types of model. The ANFIS model was implemented in the SIMULINK environment for real-time monitoring of cement fineness from the knowledge of input variables and the model computation time was determined. It is observed that the model holds good promise to be implemented online for real-time estimation of cement fineness which will certainly help the plant operators in maintaining proper cement quality and in reducing losses.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2013.08.003,Journal,Expert Systems with Applications,scopus,2014-01-01,sciencedirect,Intelligent business processes composition based on multi-agent systems,https://api.elsevier.com/content/abstract/scopus_id/84888360250,"This paper proposes a novel model for automatic construction of business processes called IPCASCI (Intelligent business Processes Composition based on multi-Agent systems, Semantics and Cloud Integration). The software development industry requires agile construction of new products able to adapt to the emerging needs of a changing market. In this context, we present a method of software component reuse as a model (or methodology), which facilitates the semi-automatic reuse of web services on a cloud computing environment, leading to business process composition. The proposal is based on web service technology, including: (i) Automatic discovery of web services; (ii) Semantics description of web services; (iii) Automatic composition of existing web services to generate new ones; (iv) Automatic invocation of web services. As a result of this proposal, we have presented its implementation (as a tool) on a real case study. The evaluation of the case study and its results are proof of the reliability of IPCASCI.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2013.05.017,Journal,Applied Soft Computing Journal,scopus,2014-01-01,sciencedirect,A hybrid noise suppression filter for accuracy enhancement of commercial speech recognizers in varying noisy conditions,https://api.elsevier.com/content/abstract/scopus_id/84888294149,"Commercial speech recognizers have made possible many speech control applications such as wheelchair, tone-phone, multifunctional robotic arms and remote controls, for the disabled and paraplegic. However, they have a limitation in common in that recognition errors are likely to be produced when background noise surrounds the spoken command, thereby creating potential dangers for the disabled if recognition errors exist in the control systems. In this paper, a hybrid noise suppression filter is proposed to interface with the commercial speech recognizers in order to enhance the recognition accuracy under variant noisy conditions. It intends to decrease the recognition errors when the commercial speech recognizers are working under a noisy environment. It is based on a sigmoid function which can effectively enhance noisy speech using simple computational operations, while a robust estimator based on an adaptive-network-based fuzzy inference system is used to determine the appropriate operational parameters for the sigmoid function in order to produce effective speech enhancement under variant noisy conditions. The proposed hybrid noise suppression filter has the following advantages for commercial speech recognizers: (i) it is not possible to tune the inbuilt parameters on the commercial speech recognizers in order to obtain better accuracy; (ii) existing noise suppression filters are too complicated to be implemented for real-time speech recognition; and (iii) existing sigmoid function based filters can operate only in a single-noisy condition, but not under varying noisy conditions. The performance of the hybrid noise suppression filter was evaluated by interfacing it with a commercial speech recognizer, commonly used in electronic products. Experimental results show that improvement in terms of recognition accuracy and computational time can be achieved by the hybrid noise suppression filter when the commercial recognizer is working under various noisy environments in factories.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.advengsoft.2013.09.003,Journal,Advances in Engineering Software,scopus,2014-01-01,sciencedirect,Software architecture knowledge for intelligent light maintenance,https://api.elsevier.com/content/abstract/scopus_id/84885359031,"The maintenance management plays an important role in the monitoring of business activities. It ensures a certain level of services in industrial systems by improving the ability to function in accordance with prescribed procedures. This has a decisive impact on the performance of these systems in terms of operational efficiency, reliability and associated intervention costs. To support the maintenance processes of a wide range of industrial services, a knowledge-based component is useful to perform the intelligent monitoring. In this context we propose a generic model for supporting and generating industrial lights maintenance processes. The modeled intelligent approach involves information structuring and knowledge sharing in the industrial setting and the implementation of specialized maintenance management software in the target information system. As a first step we defined computerized procedures from the conceptual structure of industrial data to ensure their interoperability and effective use of information and communication technologies in the software dedicated to the management of maintenance (E-candela). The second step is the implementation of this software architecture with specification of business rules, especially by organizing taxonomical information of the lighting systems, and applying intelligence-based operations and analysis to capitalize knowledge from maintenance experiences. Finally, the third step is the deployment of the software with contextual adaptation of the user interface to allow the management of operations, editions of the balance sheets and real-time location obtained through geolocation data. In practice, these computational intelligence-based modes of reasoning involve an engineering framework that facilitates the continuous improvement of a comprehensive maintenance regime.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ultras.2013.07.018,Journal,Ultrasonics,scopus,2014-01-01,sciencedirect,Ultrasonic sensor based defect detection and characterisation of ceramics,https://api.elsevier.com/content/abstract/scopus_id/84884211045,"Ceramic tiles, used in body armour systems, are currently inspected visually offline using an X-ray technique that is both time consuming and very expensive. The aim of this research is to develop a methodology to detect, locate and classify various manufacturing defects in Reaction Sintered Silicon Carbide (RSSC) ceramic tiles, using an ultrasonic sensing technique. Defects such as free silicon, un-sintered silicon carbide material and conventional porosity are often difficult to detect using conventional X-radiography. An alternative inspection system was developed to detect defects in ceramic components using an Artificial Neural Network (ANN) based signal processing technique. The inspection methodology proposed focuses on pre-processing of signals, de-noising, wavelet decomposition, feature extraction and post-processing of the signals for classification purposes. This research contributes to developing an on-line inspection system that would be far more cost effective than present methods and, moreover, assist manufacturers in checking the location of high density areas, defects and enable real time quality control, including the implementation of accept/reject criteria.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.sna.2013.09.021,Journal,"Sensors and Actuators, A: Physical",scopus,2013-11-14,sciencedirect,Feasibility study of Hierarchical Temporal Memories applied to welding diagnostics,https://api.elsevier.com/content/abstract/scopus_id/84887294247,"Defect classification in on-line welding quality monitoring systems is an active area of research with a significant relevance to several industrial sectors where welding processes are extensively employed. Approaches based on some artificial intelligence implementations, like Artificial Neural Networks or Fuzzy Logic have been attempted, but their impact in real industrial scenarios is nowadays rather modest. In this paper a new approach based on Hierarchical Temporal Memories and the acquired plasma spectra is explored and analyzed by means of several arc-welding experimental tests. Results show the ability of the proposed solution to perform a suitable classification among several weld perturbations. The search for an optimal configuration of the algorithm and the usefulness of both spatial (spectral) and temporal identification of patterns will be also discussed, and the results will be compared with those provided by a solution based on feature selection and neural networks, exhibiting the better performance of the HTM model in terms of performance and handling of the input data.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.fluid.2013.08.018,Journal,Fluid Phase Equilibria,scopus,2013-11-05,sciencedirect,Utilization of support vector machine to calculate gas compressibility factor,https://api.elsevier.com/content/abstract/scopus_id/84884180985,"The compressibility factor (Z-factor) is considered as a very important parameter in the petroleum industry because of its broad applications in PVT characteristics. In this study, a meta-learning algorithm called Least Square Support Vector Machine (LSSVM) was developed to predict the compressibility factor. In addition, the proposed technique was examined with previous models, exhibiting an R
                     2 and an MSE of 0.999 and 0.000014, respectively. A significant drawback in the conventional LSSVM is the determination of optimal parameters to attain desired output with a reasonable accuracy. To eliminate this problem, the current study introduced coupled simulated annealing (CSA) algorithm to develop a new model, known as CSA-LSSVM. The proposed algorithm included 4756 datasets to validate the effectiveness of the CSA-LSSVM model using statistical criteria. The new technique can be utilized in chemical and petroleum engineering software packages where the most accurate value of Z-factor is required to predict the behavior of real gas, significantly affecting design aspects of equipment involved in gas processing plants.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jprocont.2013.09.014,Journal,Journal of Process Control,scopus,2013-10-28,sciencedirect,A multilayer-perceptron based method for variable selection in soft sensor design,https://api.elsevier.com/content/abstract/scopus_id/84886080853,"The paper proposes a new method for variable selection for prediction settings and soft sensors applications. The new variable selection method is based on the multi-layer perceptron (MLP) neural network model, where the network is trained a single time, maintaining low computational cost. The proposed method was successfully applied, and compared with four state-of-the-art methods in one artificial dataset and three real-world datasets, two publicly available datasets (Box–Jenkins gas furnace and gas mileage), and a dataset of a problem where the objective is to estimate the fluoride concentration in the effluent of a real urban water treatment plant (WTP). The proposed method presents similar or better approximation performance when compared to the other four methods. In the experiments, among all the five methods, the proposed method selects the lowest number of variables and variables-delays pairs to achieve the best solution. In soft sensors applications having a lower number of variables is a positive factor for decreasing implementation costs, or even making the soft sensor feasible at all.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2013.02.039,Journal,Neurocomputing,scopus,2013-10-22,sciencedirect,Point and prediction interval estimation for electricity markets with machine learning techniques and wavelet transforms,https://api.elsevier.com/content/abstract/scopus_id/84881221196,"A growing number of countries all over the world are switching over to deregulated or the market structure of electricity sector with a view to enhance productivity, efficiency and to lower the prices. Barring a few cases, the deregulated structure is doing quite well in most of the countries. However a persistent issue that plagues the involved parties such as producers, traders, retailers etc., is the uncertainty that prevails in the system. Due to a number of known, unknown factors, the electricity prices exhibit fluctuating characteristics which is difficult to control as well as predict. Several forecasting techniques have been developed and successfully implemented for existing markets around the world with comparable performance. However, the uncertainty aspect of the point forecasts has not been analyzed significantly. In this work, an attempt is made to quantify such uncertainties existing in the market using statistical techniques like prediction intervals. Hybrid models using neural networks and Extreme Learning machines with wavelets as preprocessors are developed and applied for point as well as prediction interval forecasting for Ontario Electricity Market, PJM Day-Ahead and Real time markets.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2013.04.006,Journal,Engineering Applications of Artificial Intelligence,scopus,2013-09-01,sciencedirect,Post-design analysis for building and refining AI planning systems,https://api.elsevier.com/content/abstract/scopus_id/84880771590,"The growth of industrial applications of artificial intelligence has raised the need for design tools to aid in the conception and implementation of such complex systems. The design of automated planning systems faces several engineering challenges including the proper modeling of the domain knowledge: the creation of a model that represents the problem to be solved, the world that surrounds the system, and the ways the system can interact with and change the world in order to solve the problem. Knowledge modeling in AI planning is a hard task that involves acquiring the system requirements and making design decisions that can determine the behavior and performance of the resulting system. In this paper we investigate how knowledge acquired during a post-design phase of modeling can be used to improve the prospective model. A post-design framework is introduced which combines a knowledge engineering tool and a virtual prototyping environment for the analysis and simulation of plans. This framework demonstrates that post-design analysis supports the discovery of missing requirements and can guide the model refinement cycle. We present three case studies using benchmark domains and eight state-of-the-art planners. Our results demonstrate that significant improvements in plan quality and an increase in planning speed of up to three orders of magnitude can be achieved through a careful post-design process. We argue that such a process is critical for the deployment of AI planning technology in real-world engineering applications.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jsr.2013.04.005,Journal,Journal of Safety Research,scopus,2013-06-24,sciencedirect,Transferability and robustness of real-time freeway crash risk assessment,https://api.elsevier.com/content/abstract/scopus_id/84879088842,"Introduction
                  This study examines the data from single loop detectors on northbound (NB) US-101 in San Jose, California to estimate real-time crash risk assessment models.
               
                  Method
                  The classification tree and neural network based crash risk assessment models developed with data from NB US-101 are applied to data from the same freeway, as well as to the data from nearby segments of the SB US-101, NB I-880, and SB I-880 corridors. The performance of crash risk assessment models on these nearby segments is the focus of this research.
               
                  Results
                  The model applications show that it is in fact possible to use the same model for multiple freeways, as the underlying relationships between traffic data and crash risk remain similar.
               
                  Impact on Industry
                  The framework provided here may be helpful to authorities for freeway segments with newly installed traffic surveillance apparatuses, since the real-time crash risk assessment models from nearby freeways with existing infrastructure would be able to provide a reasonable estimate of crash risk. The robustness of the model output is also assessed by location, time of day, and day of week. The analysis shows that on some locations the models may require further learning due to higher than expected false positive (e.g., the I-680/I-280 interchange on US-101 NB) or false negative rates. The approach for post-processing the results from the model provides ideas to refine the model prior to or during the implementation.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2012.04.033,Journal,Neurocomputing,scopus,2013-06-03,sciencedirect,Applying soft computing techniques to optimise a dental milling process,https://api.elsevier.com/content/abstract/scopus_id/84875966713,"This study presents a novel soft computing procedure based on the application of artificial neural networks, genetic algorithms and identification systems, which makes it possible to optimise the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving both time and financial costs and/or energy. This novel intelligent procedure is based on the following phases. Firstly, a neural model extracts the internal structure and the relevant features of the data set representing the system. Secondly, the dynamic system performance of different variables is specifically modelled using a supervised neural model and identification techniques. This constitutes the model for the fitness function of the production process, using relevant features of the data set. Finally, a genetic algorithm is used to optimise the machine parameters from a non parametric fitness function. The proposed novel approach was tested under real dental milling processes using a high-precision machining centre with five axes, requiring high finishing precision of measures in micrometres with a large number of process factors to analyse. The results of the experiment, which validate the performance of the proposed approach, are presented in this study.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2012.11.009,Journal,Engineering Applications of Artificial Intelligence,scopus,2013-05-01,sciencedirect,An intelligent system for wafer bin map defect diagnosis: An empirical study for semiconductor manufacturing,https://api.elsevier.com/content/abstract/scopus_id/84876945059,"Wafer bin maps (WBMs) that show specific spatial patterns can provide clue to identify process failures in the semiconductor manufacturing. In practice, most companies rely on experienced engineers to visually find the specific WBM patterns. However, as wafer size is enlarged and integrated circuit (IC) feature size is continuously shrinking, WBM patterns become complicated due to the differences of die size, wafer rotation, the density of failed dies and thus human judgments become inconsistent and unreliable. To fill the gaps, this study aims to develop a knowledge-based intelligent system for WBMs defect diagnosis for yield enhancement in wafer fabrication. The proposed system consisted of three parts: graphical user interface, the WBM clustering solution, and the knowledge database. In particular, the developed WBM clustering approach integrates spatial statistics test, cellular neural network (CNN), adaptive resonance theory (ART) neural network, and moment invariant (MI) to cluster different patterns effectively. In addition, an interactive converse interface is developed to present the possible root causes in the order of similarity matching and record the diagnosis know-how from the domain experts into the knowledge database. To validate the proposed WBM clustering solution, twelve different WBM patterns collected in real settings are used to demonstrate the performance of the proposed method in terms of purity, diversity, specificity, and efficiency. The results have shown the validity and practical viability of the proposed system. Indeed, the developed solution has been implemented in a leading semiconductor manufacturing company in Taiwan. The proposed WBM intelligent system can recognize specific failure patterns efficiently and also record the assignable root causes verified by the domain experts to enhance troubleshooting effectively.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.robot.2012.12.005,Journal,Robotics and Autonomous Systems,scopus,2013-05-01,sciencedirect,A survey of bio-inspired robotics hands implementation: New directions in dexterous manipulation,https://api.elsevier.com/content/abstract/scopus_id/84875695547,"Recently, significant advances have been made in ROBOTICS, ARTIFICIAL INTELLIGENCE and other COGNITIVE related fields, allowing to make much sophisticated biomimetic robotics systems. In addition, enormous number of robots have been designed and assembled, explicitly realize biological oriented behaviors. Towards much skill behaviors and adequate grasping abilities (i.e. ARTICULATION and DEXTEROUS MANIPULATION), a new phase of dexterous hands have been developed recently with biomimetically oriented and bio-inspired functionalities. In this respect, this manuscript brings a detailed survey of biomimetic based dexterous robotics multi-fingered hands. The aim of this survey, is to find out the state of the art on dexterous robotics end-effectors, known in literature as (ROBOTIC HANDS) or (DEXTEROUS MULTI-FINGERED) robot hands. Hence, this review finds such biomimetic approaches using a framework that permits for a common description of biological and technical based hand manipulation behavior. In particular, the manuscript focuses on a number of developments that have been taking place over the past two decades, and some recent developments related to this biomimetic field of research. In conclusions, the study found that, there are rich research efforts in terms of KINEMATICS, DYNAMICS, MODELING and CONTROL methodologies. The survey is also indicating that, the topic of biomimetic inspired robotics systems make significant contributions to robotics hand design, in four main directions for future research. First, they provide a genuine world test of models of biologically inspired hand designs and dexterous manipulation behaviors. Second, they provide novel manipulation articulations and mechanisms available for industrial and domestic uses, most notably in the field of human like hand design and real world applications. Third, this survey has also indicated that, there are quite large number of attempts to acquire biologically inspired hands. These attempts were almost successful, where they exposed more novel ideas for further developments. Such inspirations were directed towards a number of topics related (HAND MECHANICS AND DESIGN), (HAND TACTILE SENSING), (HAND FORCE SENSING), (HAND SOFT ACTUATION) and (HAND CONFIGURATION AND TOPOLOGY). FOURTH, in terms of employing AI related sciences and cognitive thinking, it was also found that, rare and exceptional research attempts were directed towards the employment of biologically inspired thinking, i.e. (AI, BRAIN AND COGNITIVE SCIENCES) for hand upper control and towards much sophisticated dexterous movements. Throughout the study, it has been found there are number of efforts in terms of mechanics and hand designs, tactical sensing, however, for hand soft actuation, it seems this area of research is still far away from having a realistic muscular type fingers and hand movements.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compind.2012.11.005,Journal,Computers in Industry,scopus,2013-04-01,sciencedirect,IMAQCS: Design and implementation of an intelligent multi-agent system for monitoring and controlling quality of cement production processes,https://api.elsevier.com/content/abstract/scopus_id/84875245342,"In cement plant, since all processes are chemical and irreversible, monitoring and control is a critical factor. If the process is not controlled at any stage, the final product can be damaged or lost. Thus, in such environments, considering the quality of the product at each state is essential. Also, to control the process, communication among different parts of production line is essential. The wasted time in production line has a direct effect on process correction time and cement production performance. Here, a model of a new intelligent multi-agent quality control system (IMAQCS) for controlling the quality of cement production processes is suggested. This model, using of rule-based artificial intelligence technique, concentrates on relationship between departments in cement production line to monitor multi-attribute quality factors. With the presence of agents for controlling the quality of cement processes, real-time analyzing and decision making in a fault condition will be provided. In order to validate the proposed model, IMAQCS is deployed in real plants of a cement industries complex in Iran. The ability of the system in the process production environment is assessed. The effectiveness and efficiency of the system are demonstrated by reducing the process correction time and increasing the cement production performance. Finally, this system can effectively impact on factory resources and cost saving.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.epsr.2013.01.011,Journal,Electric Power Systems Research,scopus,2013-02-21,sciencedirect,FPGA-based neural network harmonic estimation for continuous monitoring of the power line in industrial applications,https://api.elsevier.com/content/abstract/scopus_id/84873945333,"Manufacturing cells are present in almost all the industrial sector. Unfortunately, all machine tools into the manufacturing cell are connected to the same power line, implying that their operation adds nonlinear loads such as harmonics and interharmonics that affect the general machine-tool condition. A novel NN-based methodology for harmonic monitoring through time in transient and stationary signals that satisfies the IEC61000-4-7 standard is presented, as well as its implementation into a field programmable gate array (FPGA) for continuous and online monitoring. The proposed method and the developed instrument have been tested in a real manufacturing cell.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1533/9780857093967.1.208,Book,Joining Textiles: Principles and Applications,scopus,2013-01-01,sciencedirect,Intelligent sewing systems for garment automation and robotics,https://api.elsevier.com/content/abstract/scopus_id/84903011824,"Sewing machine interactions at different speeds have been used to construct qualitative rules mapping fabric properties to optimum sewing machine settings for intelligent sewing machines. the inference procedures of fuzzy logic have been implemented in a neural network to allow for optimisation of output membership functions and, subsequently, self-learning. the technique is successfully applied to develop intelligent sewing machines and further implemented in textile and garment manufacturing. An intelligent manufacturing environment has been put forward in which fabric properties predict the sewability of any fabric, determine the minimum change of fabric properties required, and control in real time the stitching of a garment by using the feedback closed loop of the Neuro-Fuzzy model. the system has been successfully tried in an industrial setting. optimum settings were achieved under static and dynamic machine conditions, including for the properties of difficult fabrics and compensation for mishandling by the operator over the speed range of the sewing machine.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procir.2013.09.042,Conference Proceeding,Procedia CIRP,scopus,2013-01-01,sciencedirect,An enabling digital foundation towards smart machining,https://api.elsevier.com/content/abstract/scopus_id/84886789550,"Today's major challenges for manufacturing companies in the aerospace and automotive industries are clear: global cooperation with multiple supply chain partners, production optimization, management and tracking of information so as to meet new requirements in terms of traceability, security and sustainability. The need for a data exchange standard that allows disparate entities and their associated devices in a manufacturing system to share data seamlessly is clearly obvious. And the first expected impact is the ‘next generation’ smart controller that could really enable an intelligent machining process based on real-time monitoring and diagnosis, self-learning decision and adaptive optimization. The four-year project titled FoFdation envisions a ‘Digital and Smart Factory’ architecture and implementation. This has the potential to achieve significant benefits in earlier visibility of manufacturing issues, faster production ramp-up time, faster time to volume production and subsequently shorter time to market, reduced manufacturing costs and improved product quality, as well as sustainability objectives like low energy consumption and waste reduction. The present paper describes the on-going work with specific focus on the definition and implementation of the FoFdation Smart Machine Controller (SMC) in an adaptable architecture that satisfies both commercial and open source CNC controllers. It highlights the project's end use validation framework as well as sets a strong Manufacturing Information System foundation on which process optimization and control as well as sustainable practices can be based. It presents the general vision of the target solution for the SMC developed in the FoFdation project. It is based on efforts past and present both by academia and industry in various capacities and proposes tentative implementations based on the STEP-NC standard to define the machine controller of the future.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.3182/20130825-4-US-2038.00105,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2013-01-01,sciencedirect,An on-line training simulator built on dynamic simulations of crushing plants,https://api.elsevier.com/content/abstract/scopus_id/84885800887,"Crushing plants are widely used around the world as a pre-processing step in the mineral and mining industries or as standalone processing plants for final products in the aggregates industry. Despite automation and different types of advanced model predictive control, many the processes are still managed by operators. The skill of the operators influences the process performance and thus production yield. Therefore, it is important to train the operators so they know how to behave in different situations and to make them able to operate the process in the best possible way.
                  Different types of models for crushers and other production units have been developed during the years and the latest improvement is the addition of dynamic behavior which gives the crushing plants a time dependent behavior and performance. This can be used as a simulator for operators training. By connecting an Internet based Human Machine Interface (WebHMI) to a dynamic simulator with the models incorporated, an on-line training environment for operators can be achieved.
                  In this paper, a dynamic crushing plant simulator implemented in MATLAB/SIMULINK has been connected to a WebHMI. The WebHMI is accessible via the Internet, thus creating a realistic control room for operators’ training. In the created training environment, the operators can be trained under realistic conditions. Simple training scenarios and how they could be simulated are discussed. Apart from the increased level of knowledge and experience among the operators, the time aspect is an important factor. While a real crushing plant is still being built, the operators to be can already be trained, saving a lot of the commissioning and ramp up time.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.3182/20130828-3-UK-2039.00025,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2013-01-01,sciencedirect,A new extensive source for web-based control education - Contlab.eu,https://api.elsevier.com/content/abstract/scopus_id/84885206555,"Modern technologies allow to create a networked control system with off-the-shelf mobile devices. As such, there is the possibility of having the role of who offers and who uses a “remote” laboratory played by the same people. Extending recently published ideas, the paper presents a first nucleus of functionalities allowing one to create process simulators and controllers which run on a mobile application, and then share them with others. Some words are also spent on some of the possibilities opened by the proposal, sketching out some interesting didactic activities to propose to the students.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2012.05.031,Journal,Applied Soft Computing Journal,scopus,2013-01-01,sciencedirect,Optimal design of laser solid freeform fabrication system and real-time prediction of melt pool geometry using intelligent evolutionary algorithms,https://api.elsevier.com/content/abstract/scopus_id/84881665462,"With the rapid growth of laser applications and the introduction of high efficiency lasers (e.g. fiber lasers), laser material processing has gained increasing importance in a variety of industries. Among the applications of laser technology, laser cladding has received significant attention due to its high potential for material processing such as metallic coating, high value component repair, prototyping, and even low-volume manufacturing. In this paper, two optimization methods have been applied to obtain optimal operating parameters of Laser Solid Freeform Fabrication Process (LSFF) as a real world engineering problem. First, Particle Swarm Optimization (PSO) algorithm was implemented for real-time prediction of melt pool geometry. Then, a hybrid evolutionary algorithm called Self-organizing Pareto based Evolutionary Algorithm (SOPEA) was proposed to find the optimal process parameters. For further assurance on the performance of the proposed optimization technique, it was compared to some well-known vector optimization algorithms such as Non-dominated Sorting Genetic Algorithm (NSGA-II) and Strength Pareto Evolutionary Algorithm (SPEA 2). Thereafter, it was applied for simultaneous optimization of clad height and melt pool depth in LSFF process. Since there is no exact mathematical model for the clad height (deposited layer thickness) and the melt pool depth, the authors developed two Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to estimate these two process parameters. Optimization procedure being done, the archived non-dominated solutions were surveyed to find the appropriate ranges of process parameters with acceptable dilutions. Finally, the selected optimal ranges were used to find a case with the minimum rapid prototyping time. The results indicate the acceptable potential of evolutionary strategies for controlling and optimization of LSFF process as a complicated engineering problem.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.riai.2013.05.002,Journal,RIAI - Revista Iberoamericana de Automatica e Informatica Industrial,scopus,2013-01-01,sciencedirect,Identification and wavenet control of AC motor,https://api.elsevier.com/content/abstract/scopus_id/84880211449,"En el presente artículo se muestra un esquema de identificación y control que sintoniza en línea las ganancias proporcional, integral y derivativa de un controlador PID discreto aplicado a un sistema dinámico SISO. Esto se logra empleando una red neuronal de base radial con funciones de activación wavelet hijas Morlet (wavenet) adicionalmente en cascada un filtro de respuesta infinita al impulso (IIR). Dicho esquema es aplicado en tiempo real para controlar la velocidad de un motor de inducción de CA trifásico del tipo jaula de ardilla (MIJA) alimentado con un variador de frecuencia trifásico, de esta forma se muestra cómo este esquema de identificación y control en línea, puede ser implementado en este tipo de plantas que son ampliamente utilizadas en la industria, sin la necesidad de obtener los parámetros del modelo matemático del conjunto variador de frecuencia-motor de inducción trifásico. Se presentan los resultados obtenidos en simulación numérica y experimentales, empleando para esto la plataforma de LabVIEW.
               
                  This paper presents a control scheme to tune online the proportional, integral and derivative gains of a discrete PID controller, through the identification and control of a SISO stable and minimum phase dynamic system. This is accomplished using a radial basis network neural with daughter Morlet wavelets activation functions in cascaded with an infinite impulse response (IIR) filter. This scheme is applied in real time to control the speed of an AC three-phase induction motor supplied with a three-phase inverter. So in this way we show how the identification and control scheme can be implemented in this type of plants that are widely used in industry, without the need of mathematical model parameters of the induction motor. We present numerical simulation and experimental results.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compchemeng.2012.06.021,Journal,Computers and Chemical Engineering,scopus,2012-12-20,sciencedirect,SmartGantt - An interactive system for generating and updating rescheduling knowledge using relational abstractions,https://api.elsevier.com/content/abstract/scopus_id/84869501412,"Generating and updating rescheduling knowledge that can be used in real time has become a key issue in reactive scheduling due to the dynamic and uncertain nature of industrial environments and the emergent trend towards cognitive systems in production planning and execution control. Disruptive events have a significant impact on the feasibility of plans and schedules. In this work, the automatic generation and update through learning of rescheduling knowledge using simulated transitions of abstract schedule states is proposed. An industrial example where a current schedule must be repaired in response to unplanned events such as the arrival of a rush order, raw material delay, or an equipment failure which gives rise to the need for rescheduling is discussed. A software prototype (SmartGantt) for interactive schedule repair in real-time is presented. Results demonstrate that responsiveness is dramatically improved by using relational reinforcement learning and relational abstractions to develop a repair policy.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.dss.2012.08.006,Journal,Decision Support Systems,scopus,2012-12-01,sciencedirect,Sales forecasting for computer wholesalers: A comparison of multivariate adaptive regression splines and artificial neural networks,https://api.elsevier.com/content/abstract/scopus_id/84868667879,"Artificial neural networks (ANNs) have been found to be useful for sales/demand forecasting. However, one of the main shortcomings of ANNs is their inability to identify important forecasting variables. This study uses multivariate adaptive regression splines (MARS), a nonlinear and non-parametric regression methodology, to construct sales forecasting models for computer wholesalers. Through the outstanding variable screening ability of MARS, important sales forecasting variables for computer wholesalers can be obtained to enable them to make better sales management decisions. Two sets of real sales data collected from Taiwanese computer wholesalers are used to evaluate the performance of MARS. The experimental results show that the MARS model outperforms backpropagation neural networks, a support vector machine, a cerebellar model articulation controller neural network, an extreme learning machine, an ARIMA model, a multivariate linear regression model, and four two-stage forecasting schemes across various performance criteria. Moreover, the MARS forecasting results provide useful information about the relationships between the forecasting variables selected and sales amounts through the basis functions, important predictor variables, and the MARS prediction function obtained, and hence they have important implications for the implementation of appropriate sales decisions or strategies.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ymssp.2012.01.021,Journal,Mechanical Systems and Signal Processing,scopus,2012-07-01,sciencedirect,FPGA-based entropy neural processor for online detection of multiple combined faults on induction motors,https://api.elsevier.com/content/abstract/scopus_id/84860217701,"For industry, a faulty induction motor signifies production reduction and cost increase. Real-world induction motors can have one or more faults present at the same time that can mislead to a wrong decision about its operational condition. The detection of multiple combined faults is a demanding task, difficult to accomplish even with computing intensive techniques. This work introduces information entropy and artificial neural networks for detecting multiple combined faults by analyzing the 3-axis startup vibration signals of the rotating machine. A field programmable gate array implementation is developed for automatic online detection of single and combined faults in real time.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cirp.2012.03.065,Journal,CIRP Annals - Manufacturing Technology,scopus,2012-04-23,sciencedirect,Decision support systems for effective maintenance operations,https://api.elsevier.com/content/abstract/scopus_id/84861592241,"To compete successfully in the market place, leading manufacturing companies are pursuing effective maintenance operations. Existing computerized maintenance management systems (CMMS) can no longer meet the needs of dynamic maintenance operations. This paper describes newly developed decision support tools for effective maintenance operations: (1) data-driven short-term throughput bottleneck identification, (2) estimation of maintenance windows of opportunity, (3) prioritization of maintenance tasks, (4) joint production and maintenance scheduling systems, and (5) maintenance staff management. Mathematical algorithms and simulation tools are utilized to illustrate the concepts of these decision support systems. Results from real implementations in automotive manufacturing are presented to demonstrate the effectiveness of these tools.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmsy.2011.09.002,Journal,Journal of Manufacturing Systems,scopus,2012-04-01,sciencedirect,Intelligent evaluation of supplier bids using a hybrid technique in distributed supply chains,https://api.elsevier.com/content/abstract/scopus_id/84858340427,"The main idea of this research is to devise the smart module to pick the best supplier bid(s) automatically. The hybrid model is composed of three useful tools: fuzzy logic, AHP, and QFD. The approach has been carefully implemented and verified via a real-world case study in a medium-to-large industry manufacturing vehicle tires and other rubber products. A collection of 12 assessment criteria classified into two categories have been considered. Eight factors are derived from customer suggestions and the other four are design specifications required to manufacture the product. The main outcomes are: a hybrid autonomous model to evaluate supplier bids without direct human intervention; devising a hybrid three-module method and overcoming complexity of computations in resulting algorithm by means of agents; outlining the best criteria to assess suppliers; evaluating the suppliers based on voice of customer during all stages of the process; and discussing analysis, design, and implementation issues of the evaluation agent. The paper includes implications for development of an integrated total system for supply chain coordination. The most important advantages of this work over earlier researches on supplier selection are: implementation of an autonomous assessment mechanism using intelligent agents for the first time, making the best out of three widely applied methodologies all at once, evaluation process mainly based on features of customer order, coordination of supply job based on a bidding system, and portal-mediated operation and control.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.conengprac.2011.06.009,Journal,Control Engineering Practice,scopus,2012-04-01,sciencedirect,Data reconciliation and optimal management of hydrogen networks in a petrol refinery,https://api.elsevier.com/content/abstract/scopus_id/84857191932,"This paper describes the main problems associated to the management of hydrogen networks in petrol refineries and presents an approach to deal with them with the aim of operating the installation in the most profitable way. In particular, the problems of data reconciliation, economic optimization and interaction with the underlying basic control system are reviewed. The paper provides also a proposal for the implementation of the system and illustrates the approach with results obtained using real data from an industrial site.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2011.11.112,Journal,Expert Systems with Applications,scopus,2012-04-01,sciencedirect,A multi-agent-based decision support system for bankruptcy contagion effects,https://api.elsevier.com/content/abstract/scopus_id/84855900478,"With the increasing interdependence of marketing participants, distress experienced by a specific entity may cause other connecting firms to encounter financial difficulties, leading to a negative impact on their stock valuations. At the same time, individual investors have a great need to gain relevant information for portfolio risk management. The monitoring vision cannot be limited to investors’ portfolios but must take into account any potential candidates affected. Based on the ontological knowledge model of inter-firm relationships, the proposed multi-agent decision support system continuously observes real-time news reports and forecasts their potential impact on the corresponding stock price. After identifying relating companies for which significant market reactions can be expected, a wireless push-based message service promptly supplies information. A case study is used to illustrate the multi-agent-based decision support system (MAB-DSS) implementation and its use. The example shows that the MAB-DSS can automate the solution for intricate and dynamic valuation effects among interdependent firms and provide constructive advice for individual investors.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.proeng.2012.07.193,Conference Proceeding,Procedia Engineering,scopus,2012-01-01,sciencedirect,Modelling and simulation for Industrial DC Motor using Intelligent control,https://api.elsevier.com/content/abstract/scopus_id/84877113112,"This paper presents an overview of Proportional Integral control (PI) and Artificial Intelligent control (AI) algorithms. AI and PI controller are analyzed using Matlab [Simulink] software. The DC motor is an attractive piece of equipment in many industrial applications requiring variable speed and load characteristics due to its ease of controllability. The main objective of this paper illustrates how the speed of the DC motor can be controlled using different controllers. The simulation results demonstrate that the responses of DC motor with an AI control which is Fuzzy Logic Control shows satisfactory well damped control performance. The results shows that Industrial DC Motor model develop using its physical parameters and controlled with an AI controller give better response, it means it can used as a controller to the real time DC Motor",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compeleceng.2012.05.013,Journal,Computers and Electrical Engineering,scopus,2012-01-01,sciencedirect,Automatic network intrusion detection: Current techniques and open issues,https://api.elsevier.com/content/abstract/scopus_id/84866355973,"Automatic network intrusion detection has been an important research topic for the last 20years. In that time, approaches based on signatures describing intrusive behavior have become the de-facto industry standard. Alternatively, other novel techniques have been used for improving automation of the intrusion detection process. In this regard, statistical methods, machine learning and data mining techniques have been proposed arguing higher automation capabilities than signature-based approaches. However, the majority of these novel techniques have never been deployed on real-life scenarios. The fact is that signature-based still is the most widely used strategy for automatic intrusion detection. In the present article we survey the most relevant works in the field of automatic network intrusion detection. In contrast to previous surveys, our analysis considers several features required for truly deploying each one of the reviewed approaches. This wider perspective can help us to identify the possible causes behind the lack of acceptance of novel techniques by network security experts.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.3182/20120403-3-DE-3010.00066,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,A model-based approach for timing analysis of industrial automation systems,https://api.elsevier.com/content/abstract/scopus_id/84866095348,"This paper presents a temporal characterization for automation systems. The final goal is to achieve a whole model in which a schedulability analysis could be applied in order to assure that this kind of systems meet timing non-functional requirements of the application. This work is performed in the context of a Model Driven Development approach. The definition of three Domain Specific Models: control specification, and hardware and software architectures, is the base for the whole model. In particular, the software domain model uses the XML interface defined by PLCopen for expressing IEC 61131-3 automation projects. The information contained in the model of the application is processed to generate the temporal model of the automation system. A specific transformation of this model makes possible to carry out a schedulability analysis of the system. In particular, this is achieved by generating the specific input model of the well-known Modelling and Analysis Suite for Real-Time Applications, MAST.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.3182/20120403-3-DE-3010.00010,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,RodosVisor - An object-oriented and customizable hypervisor: The CPU virtualization,https://api.elsevier.com/content/abstract/scopus_id/84866091325,"RodosVisor is an object-oriented and bare-metal virtual machine monitor (VMM) or hypervisor designed for the aerospace industry, mainly to provide time and spatial separation to the NetworkCentric core avionics machine, Montenegro and Dittrich (2009). The NetworkCentric core avionics machine consists of several harmonized components working together to implement dependable computing in a simple way, with computing units managed by the local real-time operating system RODOS. To support partitioned software architectures such as AIR, Rufino et al. (2009), and MILS, DeLong, R. (2007), RodosVisor adapted the Popek and Goldberg's fidelity, efficiency and resource control virtualization requirements, Popek and Goldberg (1974), to the space application domain by extending them with extra ones, like timing determinism, reactivity and improved dependability. Another distinctive RodosVisor feature is the customized design based on generative programming techniques, such as aspect oriented programming and template meta-programming.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.3182/20120403-3-DE-3010.00082,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,Web monitoring system and gateway for serial communication PLC,https://api.elsevier.com/content/abstract/scopus_id/84866084543,"An industrial process requires interacting with the rest of the plant, being able to exchange data with other devices and monitoring systems in order to optimize production, reporting information and providing control capabilities to distant users. Internet, and, especially web browsers are an excellent tool to provide information for remote users, allowing not only monitoring but also controlling the industrial process as an SCADA software or HMI system. The proposed system does not need specific proprietary software and its associated license costs. In this work, a webserver system is implemented under a Freescale microcontroller, acting as a gateway for a simple PLC with single RS232 communication capabilities. The webserver is modular, providing independent access to single I/O PLC locations. Different webpage design can offer different monitoring capabilities by combining the required I/O modules according to the required application without any change in the microcontroller programming. This capability is close to SCADA software or industrial HMI systems where custom screens can be made. This proposal offers a low cost and flexible monitoring solution to old or basic industrial processes controlled by PLC with low communication capabilities. Using a web browser, the system can be monitored from any internet capable device: PC, tablet, smartphone, etc. An example for a pneumatic PID levitation control system is given.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.autcon.2011.05.018,Journal,Automation in Construction,scopus,2012-01-01,sciencedirect,Simulation and analytical techniques for construction resource planning and scheduling,https://api.elsevier.com/content/abstract/scopus_id/81355138778,"To date, few construction methods or models in the literature have discussed about helping the project managers decide the near-optimum distributions of manpower, material, equipment and space according to their project objectives and project constraints. Thus, the traditional scheduling methods or models often result in a “seat-of-the-pants” style of management, rather than decision making based on an analysis of real data. This paper presents an intelligent scheduling system (ISS) that can help the project managers to find the near-optimum schedule plan according to their project objectives and project constraints. ISS uses simulation techniques to distribute resources and assign different levels of priorities to different activities in each simulation cycle to find the near-optimal solution. ISS considers and integrates most of the important construction factors (schedule, cost, space, manpower, equipment and material) simultaneously in a unified environment, which makes the resulting schedule that will be closer to optimal. Furthermore, ISS allows for what-if analyses of possible scenarios, and schedule adjustments based on unforeseen conditions (change orders, late material delivery, etc.). Finally, two sample applications and one real-world construction project are utilized to illustrate and compare the effectiveness of ISS with two widely used software packages, Primavera Project Planner and Microsoft Project.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.wasman.2011.07.018,Journal,Waste Management,scopus,2011-12-01,sciencedirect,A web-based Decision Support System for the optimal management of construction and demolition waste,https://api.elsevier.com/content/abstract/scopus_id/80054853534,"Wastes from construction activities constitute nowadays the largest by quantity fraction of solid wastes in urban areas. In addition, it is widely accepted that the particular waste stream contains hazardous materials, such as insulating materials, plastic frames of doors, windows, etc. Their uncontrolled disposal result to long-term pollution costs, resource overuse and wasted energy. Within the framework of the DEWAM project, a web-based Decision Support System (DSS) application – namely DeconRCM – has been developed, aiming towards the identification of the optimal construction and demolition waste (CDW) management strategy that minimises end-of-life costs and maximises the recovery of salvaged building materials. This paper addresses both technical and functional structure of the developed web-based application. The web-based DSS provides an accurate estimation of the generated CDW quantities of twenty-one different waste streams (e.g. concrete, bricks, glass, etc.) for four different types of buildings (residential, office, commercial and industrial). With the use of mathematical programming, the DeconRCM provides also the user with the optimal end-of-life management alternative, taking into consideration both economic and environmental criteria. The DSS’s capabilities are illustrated through a real world case study of a typical five floor apartment building in Thessaloniki, Greece.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2011.05.011,Journal,Applied Soft Computing Journal,scopus,2011-12-01,sciencedirect,Credit risk evaluation using neural networks: Emotional versus conventional models,https://api.elsevier.com/content/abstract/scopus_id/80053571498,"Credit scoring and evaluation is one of the key analytical techniques in credit risk evaluation which has been an active research area in financial risk management. Artificial neural networks (NNs) have been considered to be accurate tools for credit analysis among others in the credit industry. Lately, emotional neural networks (EmNNs) have been suggested and applied successfully for pattern recognition. In this paper we investigate the efficiency of EmNNs and compare their performance to conventional NNs when applied to credit risk evaluation. In total 12 neural networks; based equally on emotional and conventional neural models; are arbitrated under three learning schemes to classify whether a credit application is approved or declined. The learning schemes differ in the ratio of training-to-validation data used during training and testing the neural networks. The emotional and conventional neural models are trained using real world credit application cases from the Australian credit approval datasets which has 690 cases; each case with 14 numerical attributes; based on which an application is accepted or rejected. The performance of the 12 neural networks will be evaluated using certain criteria. Experimental results suggest that both emotional and conventional neural models can be used effectively for credit risk evaluations, however the emotional models outperform their conventional counterparts in decision making speed and accuracy, thus, making them ideal for implementation in fast automatic processing of credit applications.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijthermalsci.2011.06.020,Journal,International Journal of Thermal Sciences,scopus,2011-12-01,sciencedirect,A simplified method to evaluate the energy performance of CO<inf>2</inf> heat pump units,https://api.elsevier.com/content/abstract/scopus_id/80052736833,"The prediction of the performances of CO2 transcritical heat pumps demands accurate calculation methods, where a particular effort is devoted to the gas cooler modelling, as the correlation between high pressure and gas cooler outlet temperature strongly affects the cycle performance. The above-mentioned methods require a large amount of input data and calculation power. As a consequence they are often useless for the full characterisation of heat pumps which are sold on the market.
                  A simplified numerical method for the performance prediction of vapour compression heat pumps working in a transcritical cycle is presented, based only on performance data at the nominal rating conditions. The proposed procedure was validated against experimental data of two different tap water heat pumps. For the considered units, simulation results are in good agreement with the experimental ones. The deviations range from −6.4% to +1.7% and from −3.8% to +5.8% for the COPH
                      of the air/water heat pump and the water/water heat pump, respectively. The heating capacity deviations stayed within −5.5% and +1.7% range and within −5.0% and +7.9% range for the same units.
                  The proposed mathematical model appears to be a reliable tool to be used by the refrigeration industry or to be implemented into dynamic building-plant energy simulation codes. Finally, it represents a useful instrument for the definition of tailored approximated optimal high pressure curve considering the operating characteristics of the specific CO2 transcritical unit. It could also be implemented on board of a real unit control system where it could be used as model coupled to computational intelligence algorithms for pressure optimisation.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2011.06.027,Journal,Neurocomputing,scopus,2011-11-01,sciencedirect,Neural network based controller for Cr<sup>6+</sup>-Fe<sup>2+</sup> batch reduction process,https://api.elsevier.com/content/abstract/scopus_id/80053311549,An automated pilot plant has been designed and commissioned to carry out online/real-time data acquisition and control for the Cr6+–Fe2+ reduction process. Simulated data from the Cr6+–Fe2+ model derived are validated with online data and laboratory analysis using ICP-AES analysis method. The distinctive trend or patterns exhibited in the ORP profiles for the non-equilibrium model derived have been utilized to train neural network-based controllers for the process. The implementation of this process control is to ensure sufficient Fe2+ solution is dosed into the wastewater sample in order to reduce all Cr6+–Cr3+. The neural network controller has been utilized to compare the capability of set-point tracking with a PID controller in this process. For this process neural network-based controller dosed in less Fe2+ solution compared to the PID controller which hence reduces wastage of chemicals. Industrial Cr6+ wastewater samples obtained from an electro-plating factory has also been tested on the pilot plant using the neural network-based controller to determine its effectiveness to control the reduction process for a real plant. The results indicate the proposed controller is capable of fully reducing the Cr6+–Cr3+ in the batch treatment process with minimal dosage of Fe2+.,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.envsoft.2011.04.002,Journal,Environmental Modelling and Software,scopus,2011-10-01,sciencedirect,Application of the Analytic Hierarchy Process and the Analytic Network Process for the assessment of different wastewater treatment systems,https://api.elsevier.com/content/abstract/scopus_id/79957865817,"Multicriteria analyses (MCAs) are used to make comparative assessments of alternative projects or heterogeneous measures and allow several criteria to be taken into account simultaneously in a complex situation. The paper shows the application of different MCA techniques to a real decision problem concerning the choice of the most sustainable wastewater treatment (WWT) technology, namely Anaerobic digestion, Phytoremediation and Composting, for small cheese factories. Particularly, the Analytic Hierarchy Process (AHP) and its recent implementation, the Analytic Network Process (ANP), have been considered for prioritizing the different technologies. The models enable all the elements of the decision process to be considered, namely environmental aspects, technological factors and economic costs, and to compare them to find the best alternative. The AHP and ANP techniques are applied through specific software packages with user-friendly interfaces called Expertchoice and Superdecision, respectively. A comparison of the merits obtained from the different models shows that Phytoremediation results as the most sustainable WWT technology for small cheese factories and that the use of the ANP method, which allows more sophisticated analysis to be made, succeeds in offering better results.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2011.04.012,Journal,Expert Systems with Applications,scopus,2011-09-15,sciencedirect,Fast defect detection in homogeneous flat surface products,https://api.elsevier.com/content/abstract/scopus_id/79958015915,"This paper introduces a novel hybrid approach for both defect detection and localization in homogeneous flat surface products. Real time defect detection in industrial products is a challenging problem. Fast production speeds and the variable nature of production defects complicate the process of automating the defect detection task. Speeding up the detection process is achieved in this paper by implementing a hybrid approach that is based on the statistical decision theory, multi-scale and multi-directional analysis and a neural network implementation of the optimal Bayesian classifier. The coefficient of variation is first used as a homogeneity measure for approximate defect localization. Second, features are extracted from the log Gabor filter bank response to accurately localize and detect the defect while reducing the complexity of Gabor based inspection approaches. A probabilistic neural network (PNN) is used for fast defect classification based on the maximum posterior probability of the Log-Gabor based statistical features. Experimental results show a major performance enhancement over existing defect detection approaches.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ces.2011.03.041,Journal,Chemical Engineering Science,scopus,2011-08-01,sciencedirect,Successive approximate model based multi-objective optimization for an industrial straight grate iron ore induration process using evolutionary algorithm,https://api.elsevier.com/content/abstract/scopus_id/79958723405,"Multi-objective optimization of any complex industrial process using first principle computationally expensive models often demands a substantially higher computation time for evolutionary algorithms making it less amenable for real time implementation. A combination of the above-mentioned first principle model and approximate models based on artificial neural network (ANN) successively learnt in due course of optimization using the data obtained from first principle models can be intelligently used for function evaluation and thereby reduce the aforementioned computational burden to a large extent. In this work, a multi-objective optimization task (simultaneous maximization of throughput and Tumble index) of an industrial iron ore induration process has been studied to improve the operation of the process using the above-mentioned metamodeling approach. Different pressure and temperature values at different points of the furnace bed, grate speed and bed height have been used as decision variables whereas the bounds on cold compression strength, abrasion index, maximum pellet temperature and burn-through point temperature have been treated as constraints. A popular evolutionary multi-objective algorithm, NSGA II, amalgamated with the first principle model of the induration process and its successively improving approximation model based on ANN, has been adopted to carry out the task. The optimization results show that as compared to the PO solutions obtained using only the first principle model, (i) similar or better quality PO solutions can be achieved by this metamodeling procedure with a close to 50% savings in function evaluation and thereby computation time and (ii) by keeping the total number of function evaluations same, better quality PO solutions can be obtained.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2011.01.051,Journal,Expert Systems with Applications,scopus,2011-08-01,sciencedirect,Recommendation system for localized products in vending machines,https://api.elsevier.com/content/abstract/scopus_id/79953690190,"This paper proposes a framework of localized product recommendation system for automatic vending machines applications. The goal is to offer suitable recommendations of localized products to customers in distinct locations. We develop a hybrid technique that combines a meta-heuristic approach, clustering technique, classification, and statistical method. In the approach, an intelligent system is implemented to analyze product attributes and determine localized products based on the transaction data. To prove the feasibility and effectiveness of proposed approach, we implemented the system in several automatic vending machines owned by an information service company of Taiwan. Nine machines were selected and compared from two locations: living lab by Institute for Information Industry of Taiwan at Song-shan District and business office building at Nei-hu District in Taipei. The real life experiments showed that the profit of vending machine increases after applying our system.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2011.01.081,Journal,Expert Systems with Applications,scopus,2011-07-01,sciencedirect,Expert system for analysis of quality in production of electronics,https://api.elsevier.com/content/abstract/scopus_id/79952444562,"Quality issues have become increasingly important in the production of electronics, especially when dealing with electronic products not assimilated to the mainstream of consumer electronics, but rather to the group of industrial electronic devices and machinery designed to last for years or even decades. In this paper, an intelligent optimization and modeling system for electronics production is demonstrated. The system exploits real production data and can be used to diagnose and optimize the manufacturing processes. It contains three modules consisting of appropriate mathematical tools specifically tailored to each task: (1) preprocessing, (2) variable selection, and (3) optimization modules. Moreover, concrete examples are presented from the latter two modules, by using a wave soldering process as a case study. Currently, the system works on the Matlab platform, but can be programmed into standalone software and automated in the future. The results illustrate that the system can offer an efficient tool for diagnostics and process optimization in the electronics industry.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jchromb.2011.03.059,Journal,Journal of Chromatography B: Analytical Technologies in the Biomedical and Life Sciences,scopus,2011-06-01,sciencedirect,Influence of different spacer arms on Mimetic Ligand <sup>™</sup> A2P and B14 membranes for human IgG purification,https://api.elsevier.com/content/abstract/scopus_id/79955910197,"Microporous membranes are an attractive alternative to circumvent the typical drawbacks associated to bead-based chromatography. In particular, the present work intends to evaluate different affinity membranes for antibody capture, to be used as an alternative to Protein A resins. To this aim, two Mimetic Ligands™ A2P and B14, were coupled onto different epoxide and azide group activated membrane supports using different spacer arms and immobilization chemistries. The spacer chemistries investigated were 1,2-diaminoethane (2LP), 3,6-dioxa-1,8-octanedithiol (DES) and [1,2,3] triazole (TRZ). These new mimetic membrane materials were investigated by static and by dynamic binding capacity studies, using pure polyclonal human immunoglobulin G (IgG) solutions as well as a real cell culture supernatant containing monoclonal IgG1. The best results were obtained by combining the new B14 ligand with a TRZ-spacer and an improved Epoxy 2 membrane support material. The new B14-TRZ-Epoxy 2 membrane adsorbent provided binding capacities of approximately 3.1mg/mL, besides (i) a good selectivity towards IgG, (ii) high IgG recoveries of above 90%, (iii) a high Pluronic-F68 tolerance and (iv) no B14-ligand leakage under harsh cleaning-in-place conditions (0.6M sodium hydroxide). Furthermore, foreseeable improvements in binding capacity will promote the implementation of membrane adsorbers in antibody manufacturing.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.matdes.2011.01.058,Journal,Materials and Design,scopus,2011-06-01,sciencedirect,A hybrid of back propagation neural network and genetic algorithm for optimization of injection molding process parameters,https://api.elsevier.com/content/abstract/scopus_id/79953161387,"This paper presents a hybrid optimization method for optimizing the process parameters during plastic injection molding (PIM). This proposed method combines a back propagation (BP) neural network method with an intelligence global optimization algorithm, i.e. genetic algorithm (GA). A multi-objective optimization model is established to optimize the process parameters during PIM on the basis of the finite element simulation software Moldflow, Orthogonal experiment method, BP neural network as well as Genetic algorithm. Optimization goals and design variables (process parameters during PIM) are specified by the requirement of manufacture. A BP artificial neural network model is developed to obtain the mathematical relationship between the optimization goals and process parameters. Genetic algorithm is applied to optimize the process parameters that would result in optimal solution of the optimization goals. A case study of a plastic article is presented. Warpage as well as clamp force during PIM are investigated as the optimization objectives. Mold temperature, melt temperature, packing pressure, packing time and cooling time are considered to be the design variables. The case study demonstrates that the proposed optimization method can adjust the process parameters accurately and effectively to satisfy the demand of real manufacture.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aca.2011.01.041,Journal,Analytica Chimica Acta,scopus,2011-03-18,sciencedirect,Biodiesel classification by base stock type (vegetable oil) using near infrared spectroscopy data,https://api.elsevier.com/content/abstract/scopus_id/79952487365,"The use of biofuels, such as bioethanol or biodiesel, has rapidly increased in the last few years. Near infrared (near-IR, NIR, or NIRS) spectroscopy (>4000cm−1) has previously been reported as a cheap and fast alternative for biodiesel quality control when compared with infrared, Raman, or nuclear magnetic resonance (NMR) methods; in addition, NIR can easily be done in real time (on-line). In this proof-of-principle paper, we attempt to find a correlation between the near infrared spectrum of a biodiesel sample and its base stock. This correlation is used to classify fuel samples into 10 groups according to their origin (vegetable oil): sunflower, coconut, palm, soy/soya, cottonseed, castor, Jatropha, etc. Principal component analysis (PCA) is used for outlier detection and dimensionality reduction of the NIR spectral data. Four different multivariate data analysis techniques are used to solve the classification problem, including regularized discriminant analysis (RDA), partial least squares method/projection on latent structures (PLS-DA), K-nearest neighbors (KNN) technique, and support vector machines (SVMs). Classifying biodiesel by feedstock (base stock) type can be successfully solved with modern machine learning techniques and NIR spectroscopy data. KNN and SVM methods were found to be highly effective for biodiesel classification by feedstock oil type. A classification error (E) of less than 5% can be reached using an SVM-based approach. If computational time is an important consideration, the KNN technique (E
                     =6.2%) can be recommended for practical (industrial) implementation. Comparison with gasoline and motor oil data shows the relative simplicity of this methodology for biodiesel classification.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2010.09.007,Journal,Applied Soft Computing Journal,scopus,2011-03-01,sciencedirect,Forecasting stock markets using wavelet transforms and recurrent neural networks: An integrated system based on artificial bee colony algorithm,https://api.elsevier.com/content/abstract/scopus_id/78751613501,"This study presents an integrated system where wavelet transforms and recurrent neural network (RNN) based on artificial bee colony (abc) algorithm (called ABC-RNN) are combined for stock price forecasting. The system comprises three stages. First, the wavelet transform using the Haar wavelet is applied to decompose the stock price time series and thus eliminate noise. Second, the RNN, which has a simple architecture and uses numerous fundamental and technical indicators, is applied to construct the input features chosen via Stepwise Regression-Correlation Selection (SRCS). Third, the Artificial Bee Colony algorithm (ABC) is utilized to optimize the RNN weights and biases under a parameter space design. For illustration and evaluation purposes, this study refers to the simulation results of several international stock markets, including the Dow Jones Industrial Average Index (DJIA), London FTSE-100 Index (FTSE), Tokyo Nikkei-225 Index (Nikkei), and Taiwan Stock Exchange Capitalization Weighted Stock Index (TAIEX). As these simulation results demonstrate, the proposed system is highly promising and can be implemented in a real-time trading system for forecasting stock prices and maximizing profits.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.cma.2010.11.014,Journal,Computer Methods in Applied Mechanics and Engineering,scopus,2011-02-01,sciencedirect,Multi-objective optimization of turbomachinery using improved NSGA-II and approximation model,https://api.elsevier.com/content/abstract/scopus_id/78650612499,"Coupled optimization methods based on multi-objective genetic algorithms and approximation models are widely used in engineering optimizations. In the present paper, a similar framework is proposed for the aerodynamic optimization of turbomachinery by coupling the well known multi-objective genetic algorithm—NSGA-II and back propagation neural network. The verification results of mathematical problems show that the coupled method with the origin NSGA-II cannot get the real Pareto front due to the prediction error of BPNN. A modified crowding distance is proposed in cooperation with a coarse-to-fine approaching strategy based on the iterations between NSGA-II and BPNN. The results of mathematical model problems show the effect of these improving strategies. An industrial application case is implemented on a transonic axial compressor. The optimization objectives are to maximize efficiencies of two working points and to minimize the variation of the choked mass flow. CFD simulation is employed to provide the performance evaluation of initial training samples for BPNN. The optimized results are compared with optimization results of a single objective optimization based on weighting function. The comparison shows that the present framework can provide not only better solutions than the single objective optimization, but also various alternative solutions. The increase of computational costs is acceptable especially when approximation models are used.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2011.08.020,Conference Proceeding,Procedia Computer Science,scopus,2011-01-01,sciencedirect,Model development of a virtual learning environment to enhance lean education,https://api.elsevier.com/content/abstract/scopus_id/84856459118,"Modern day industry is becoming leaner by the day. This demands engineers with an in-depth understanding of lean philosophies. Current methods for teaching lean include hands-on projects and simulation. However, simulation games available in the market lack simplicity, ability to store the results, and modeling power. The goal of this research is to develop a virtual simulation platform which would enable students to perform various experiments by applying lean concepts. The design addresses these deficiencies through the use of VE-Suite, a virtual engineering software. The design includes user-friendly dialogue boxes, graphical models of machines, performance display gauges, and an editable layout. The platform uses laws of operations management such as Little's law, economic order quantity (EOQ) models, and cycle time. These laws enable students to implement various lean concepts such as pull system, just-in-time (JIT), single piece flow, single minute exchange of dies (SMED), kaizen, kanban, U-layout, by modifying the process parameters such as process times, setup times, layout, number, and placement of machines. The simulation begins with a traditional push type mass production line and the students improve the line by implementing lean techniques. Thus, students experience the advantages of lean real time while facing the real life problems encountered in implementing it.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jngse.2011.08.002,Journal,Journal of Natural Gas Science and Engineering,scopus,2011-01-01,sciencedirect,An implementation of a distributed artificial intelligence architecture to the integrated production management,https://api.elsevier.com/content/abstract/scopus_id/84855498917,"The oil production process is highly complex and requires the combination of several disciplines and technological tools for its management. System integration and the automation of the workflows required to develop oil production operations are two main problems nowadays at the oil and gas production industry. This work approaches these problems through the implementation of distributed artificial intelligence architecture, designed for the automated production management. The architecture comprises a standardized schema to access information sources, a production ontological framework and an intelligent workflow mechanism based on multi-agent systems and electronic institution. Our architecture present several novelties: the incorporation of the semantic integration, the extension of the agents theory through the electronic institutions paradigm to solve the real-time decision problems typical in the industry, the Holon-Agent hybrid model used to make more feasible its implementation, among others. An oil production management case study is presented in order to demonstrate the applicability of the proposed architecture.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.isatra.2011.06.006,Journal,ISA Transactions,scopus,2011-01-01,sciencedirect,A hybrid intelligent controller for a twin rotor MIMO system and its hardware implementation,https://api.elsevier.com/content/abstract/scopus_id/80052415887,"This paper presents a fuzzy PID control scheme with a real-valued genetic algorithm (RGA) to a setpoint control problem. The objective of this paper is to control a twin rotor MIMO system (TRMS) to move quickly and accurately to the desired attitudes, both the pitch angle and the azimuth angle in a cross-coupled condition. A fuzzy compensator is applied to the PID controller. The proposed control structure includes four PID controllers with independent inputs in 2-DOF. In order to reduce total error and control energy, all parameters of the controller are obtained by a RGA with the system performance index as a fitness function. The system performance index utilized the integral of time multiplied by the square error criterion (ITSE) to build a suitable fitness function in the RGA. A new method for RGA to solve more than 10 parameters in the control scheme is investigated. For real-time control, Xilinx Spartan II SP200 FPGA (Field Programmable Gate Array) is employed to construct a hardware-in-the-loop system through writing VHDL on this FPGA.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.csda.2010.06.014,Journal,Computational Statistics and Data Analysis,scopus,2011-01-01,sciencedirect,Robust weighted kernel logistic regression in imbalanced and rare events data,https://api.elsevier.com/content/abstract/scopus_id/77956394683,"Recent developments in computing and technology, along with the availability of large amounts of raw data, have contributed to the creation of many effective techniques and algorithms in the fields of pattern recognition and machine learning. The main objectives for developing these algorithms include identifying patterns within the available data or making predictions, or both. Great success has been achieved with many classification techniques in real-life applications. With regard to binary data classification in particular, analysis of data containing rare events or disproportionate class distributions poses a great challenge to industry and to the machine learning community. This study examines rare events (REs) with binary dependent variables containing many more non-events (zeros) than events (ones). These variables are difficult to predict and to explain as has been evidenced in the literature. This research combines rare events corrections to Logistic Regression (LR) with truncated Newton methods and applies these techniques to Kernel Logistic Regression (KLR). The resulting model, Rare Event Weighted Kernel Logistic Regression (RE-WKLR), is a combination of weighting, regularization, approximate numerical methods, kernelization, bias correction, and efficient implementation, all of which are critical to enabling RE-WKLR to be an effective and powerful method for predicting rare events. Comparing RE-WKLR to SVM and TR-KLR, using non-linearly separable, small and large binary rare event datasets, we find that RE-WKLR is as fast as TR-KLR and much faster than SVM. In addition, according to the statistical significance test, RE-WKLR is more accurate than both SVM and TR-KLR.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.robot.2010.08.004,Journal,Robotics and Autonomous Systems,scopus,2010-12-31,sciencedirect,Open-ended evolution as a means to self-organize heterogeneous multi-robot systems in real time,https://api.elsevier.com/content/abstract/scopus_id/78649913375,"This work deals with the application of multi-robot systems to real tasks and, in particular, their coordination through interaction based control systems. Within this field, the practical solutions that have been implemented in real robots mainly use strongly coordinated architectures and assignment strategies because of reliability and fault tolerance issues when addressing problems in reality. Emergent approaches have also been proposed with limited success, basically due to the unpredictability of the behaviors obtained. Here, an emergent approach, called r-ASiCo, is presented containing a procedure to produce predictable solutions and thus avoiding the typical problems associated with these techniques. The r-ASico algorithm is the real time version of the Asynchronous Situated Co-evolution algorithm (ASiCo), which exploits natural open-ended evolution to generate emergent complex collective behaviors and deals with systems made up of a huge number of elements and nonlinear interactions. The goal of r-ASiCo is to design the global behavior desired for the robot team as a collective entity and allow the emergence of behaviors through the interaction of the team members using social rules they learn to implement. To this end, r-ASiCo manages a series of features that are inherent to natural evolution based methods such as energy exchange and mating selection procedures, together with a technique to guide the evolution towards a design objective, the principled evaluation function selection procedure. Hence, this paper presents the components and operation of r-ASiCo and illustrates its application through a collective cleaning task example. It was implemented using 8 e-puck robots in two different real scenarios and its results complemented with those of a 30 e-puck case. The results show the capabilities of r-ASiCo to create a self-organized and adaptive multi-robot system configuration that is tolerant to environmental changes and to failures within the robot team.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ijpe.2010.07.018,Journal,International Journal of Production Economics,scopus,2010-12-01,sciencedirect,Sales forecasts in clothing industry: The key success factor of the supply chain management,https://api.elsevier.com/content/abstract/scopus_id/78049311675,"Like many others, Textile–apparel companies have to deal with a very competitive environment and have to manage consumers which become more demanding. Thus, to stay competitive, companies rely on sophisticated information systems and logistic skills, and especially accurate and reliable forecasting systems.
                  However, forecasters have to deal with some singular constraints of the textile–apparel market such as for instance the volatile demand, the strong seasonality of sales, the wide number of items with short life cycle or the lack of historical data. To respond to these constraints, companies have implemented specific forecasting systems often simple but robust.
                  After the study of existing practices in the clothing industry, we propose different forecasting models which perform more accurate and more reliable sales forecasts. These models rely on advanced methods such as fuzzy logic, neural networks and data mining. In order to evaluate the benefits of these methods for the supply chain and more especially for the reduction of the bullwhip effect, a simulation based on real data of sourcing and forecasting processes is performed and analyzed.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.infsof.2010.06.006,Journal,Information and Software Technology,scopus,2010-11-01,sciencedirect,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,https://api.elsevier.com/content/abstract/scopus_id/77956418030,"Context
                  Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results.
               
                  Objective
                  In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software.
               
                  Method
                  We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost–benefit analysis to show that our approach can be efficiently put into practice.
               
                  Results
                  Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold.
               
                  Conclusion
                  Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations.",industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
