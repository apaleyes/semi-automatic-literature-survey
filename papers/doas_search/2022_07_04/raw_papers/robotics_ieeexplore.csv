doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database,query_name,query_value
10.1109/ICRA.2019.8793690,A Fog Robotics Approach to Deep Robot Learning: Application to Object Recognition and Grasp Planning in Surface Decluttering,IEEE,Conferences,"The growing demand of industrial, automotive and service robots presents a challenge to the centralized Cloud Robotics model in terms of privacy, security, latency, bandwidth, and reliability. In this paper, we present a `Fog Robotics' approach to deep robot learning that distributes compute, storage and networking resources between the Cloud and the Edge in a federated manner. Deep models are trained on non-private (public) synthetic images in the Cloud; the models are adapted to the private real images of the environment at the Edge within a trusted network and subsequently, deployed as a service for low-latency and secure inference/prediction for other robots in the network. We apply this approach to surface decluttering, where a mobile robot picks and sorts objects from a cluttered floor by learning a deep object recognition and a grasp planning model. Experiments suggest that Fog Robotics can improve performance by sim-to-real domain adaptation in comparison to exclusively using Cloud or Edge resources, while reducing the inference cycle time by 4× to successfully declutter 86% of objects over 213 attempts.",https://ieeexplore.ieee.org/document/8793690/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRAE53653.2021.9657761,A ROS Based Open Source Simulation Environment for Robotics Beginners,IEEE,Conferences,"This paper presents an open source robot simulation environment based on the robot operating system (ROS). To help novice to learn robotics, we have designed several important experiments that most robotics beginners will need to practice. Our simulation environment provides an efficient, safe, and transferable testing and operating environment for the rapid verification of robot algorithms. Through practicing the experiments in this paper, robotics beginners can learn robotics knowledge faster, better and at a lower cost, which should great facilitate them to operate robots in real-world applications. Source code, simulation video and detailed wiki for the simulation environment are available online at https://github.com/Suyixiu/robot_sim.",https://ieeexplore.ieee.org/document/9657761/,2021 6th International Conference on Robotics and Automation Engineering (ICRAE),19-22 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACTE.2008.125,A Simple and Robust Persian Speech Recognition System and Its Application to Robotics,IEEE,Conferences,"In this paper, a Persian speech recognition system is proposed to recognize Persian isolated spoken words. The main contribution of this work in comparison with the previous ones is simplicity and generality. The proposed system can be widely used in various real world applications when the designer does not need so much expertise in pattern and speech recognition. Due to generality, robustness, computational simplicity and reachability, general frequency domain feature as well as multi-layer percepron neural network as classifier are considered. To demonstrate the efficiency of the proposed speech recognition system, a wheeled mobile robot is navigated in a real domestic environment via Persian spoken commands. The results indicate the high potential of the proposed system to deal with real world applications.",https://ieeexplore.ieee.org/document/4736958/,2008 International Conference on Advanced Computer Theory and Engineering,20-22 Dec. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2009.4913063,A bio-inspired haptic interface for tele-robotics applications,IEEE,Conferences,"This paper presents the design concept for a bio-inspired exoskeleton intended for applications in tele-robotics and virtual reality. We based the development on an attentive analysis of the human arm anatomy with the intent to synthesize a system that will be able to interface with the human limb in a natural way. Our main goal is to develop a multi contact-point haptic interface that does not restrict the arm mobility and therefore increases the operational workspace. We propose a simplified kinematic model of the human arm using a notation coming from the robotics field. To figure out the best kinematic architecture we employed real movement data, measured from a human subject, and integrated them with the kinematic model of the exoskeleton. This allows us to test the system before its construction and to formalize specific requirements. We also implemented and tested a first passive version of the shoulder joint.",https://ieeexplore.ieee.org/document/4913063/,2008 IEEE International Conference on Robotics and Biomimetics,22-25 Feb. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2014.7090308,A chaotic neural network as motor path generator for mobile robotics,IEEE,Conferences,This work aims at developing a motor path generator for applications in mobile robotics based on a chaotic neural network. The computational paradigm inspired by the neural structure of microcircuits located in the human prefrontal cortex is adapted to work in real-time and used to generate the joints trajectories of a lightweight quadruped robot. The recurrent neural network was implemented in Matlab and a software framework was developed to test the performances of the system with the robot dynamic model. Preliminary results demonstrate the capability of the neural controller to learn period signals in a short period of time allowing adaptation during the robot operation.,https://ieeexplore.ieee.org/document/7090308/,2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014),5-10 Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.1999.815411,Advanced control techniques based in artificial intelligence for robotics manipulators,IEEE,Conferences,"The performance quality in nonlinear model based control of mechanical manipulators is conditioned to the reliability of the mathematical model and precision in the knowledge of all the involved parameters. Control methods based on artificial intelligence techniques (learning algorithms, system identification and neural networks) can be applied to improve its performance. A neural control scheme is proposed, consisting basically of a neural network for learning the robot inverse dynamics and online generating the control signal. Also an online supervision based on optimisation techniques is designed and implemented for such neural control. Simulation results are provided to evaluate the alternative variations to the proposed central scheme.",https://ieeexplore.ieee.org/document/815411/,1999 7th IEEE International Conference on Emerging Technologies and Factory Automation. Proceedings ETFA '99 (Cat. No.99TH8467),18-21 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEKIM52309.2021.00040,Application of Teaching Innovation Based on robotics engineering,IEEE,Conferences,"As the core major of “Internet + Industrial Intelligence”, robotics engineering is an upgrade and reconstruction of traditional engineering major. The industrial robot course is the professional core course of the Robotics Engineering. It is also a comprehensive course of multi-discipline integration, which involved mechanical engineering, automatic control, computer, sensor, electronic technology, artificial intelligence and other multi-disciplinary content. Robotics Engineering is characterized by broad foundation, great difficulty, emphasis on practice, rapid development and application of new knowledge. In the process of implementation of the teaching innovation, the new concept of engineering education was applied to propose a new form of curriculum system. Taking the projects of engineering as the study objects, disassemble the knowledge points involved in industrial robots, break the course boundaries, reshape the knowledge system, draw knowledge maps and then design teaching activities. In teaching innovation, teachers extend classroom through formation of subject competition teams, promote teaching and promote learning by competition, realize the integration of “teaching, class and competition”, build a bridge between theory and practice, then complete the transformation from knowledge learning to ability training. Besides, they also keep contact with intelligent manufacturing enterprises in Zhuhai and the Bay Area to obtain real-time new developments in enterprises. Thus, the latest information was introduced into classroom. Therefore, the meaning of “production, teaching, research and application” has been deepened. According to the characteristics of the knowledge points of the course, experts were invited to make special lectures for students which can bring them with international perspective and frontier knowledge.",https://ieeexplore.ieee.org/document/9479656/,"2021 2nd International Conference on Education, Knowledge and Information Management (ICEKIM)",29-31 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2000.844768,Application of automatic action planning for several work cells to the German ETS-VII space robotics experiments,IEEE,Conferences,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces.",https://ieeexplore.ieee.org/document/844768/,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),24-28 April 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE51222.2021.9404749,Artificial Intelligence and Robotics: Impact & Open issues of automation in Workplace,IEEE,Conferences,"In engineering province robotics is one of the cognitive perspective to human communication or it concern with synod of perception of action. In Today's Tech World Artificial Intelligence is an essential tool which provides effective analytical business solutions & plays significant role in the domain of robotics and have several similarities like human behavior which may drive the real world. This paper shows the significant blend of Artificial Intelligence and robotics which transform entire industries, technological improvement of robotics application & utilization. It also focuses on different aspects of targets like marketing, home appliances, medical science, Smart agriculture and many more which includes open issues and technological challenges arises by this combination and conclude that robotics with AI can work in real world with real objects. Further AI based robotics are very important area in economics and organizational consequence, implementation of automation in any organizational design give impact on overall economy and infrastructure provide a wider direction for further research on Robotics and IoT are two terms each covering a myriad of technologies and concepts.",https://ieeexplore.ieee.org/document/9404749/,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),4-5 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECMR.2019.8870908,Autonomous Robots as Actors in Robotics Theatre - Tribute to the Centenary of R.U.R.,IEEE,Conferences,"In the eyes of the roboticists, the play R.U.R. (Rossum's Universal Robots) of Czech writer Karel Čapek is seen as the messenger of the new robot age. R.U.R. is renown for the first mentioning of the word robot for a humanoid machine that looks, moves, feels, thinks and works like a human. Inspired by the 100th anniversary of R.U.R. in 2020, we have decided to make a performance with Pepper and NAO humanoid robots acting together with human actors. Performing in a theatrical performance is very demanding even for human actors, so we see the implementation of R.U.R. with robotic co-actors as a real challenge. For this purpose, we have analyzed human-robot and robot-robot interaction in the R.U.R. script to evaluate whether NAO and Pepper robots that we have are apt to act autonomously. Due to specific robot deficiencies that we found, we have made the robot casting first and then adapted the R.U.R. script to enable Pepper and NAO robots to perform their roles.",https://ieeexplore.ieee.org/document/8870908/,2019 European Conference on Mobile Robots (ECMR),4-6 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDL-EpiRob48136.2020.9278071,Bayesian Optimization for Developmental Robotics with Meta-Learning by Parameters Bounds Reduction,IEEE,Conferences,"In robotics, methods and softwares usually require optimizations of hyperparameters in order to be efficient for specific tasks, for instance industrial bin-picking from homogeneous heaps of different objects. We present a developmental framework based on long-term memory and reasoning modules (Bayesian Optimisation, visual similarity and parameters bounds reduction) allowing a robot to use meta-learning mechanism increasing the efficiency of such continuous and constrained parameters optimizations. The new optimization, viewed as a learning for the robot, can take advantage of past experiences (stored in the episodic and procedural memories) to shrink the search space by using reduced parameters bounds computed from the best optimizations realized by the robot with similar tasks of the new one (e.g. bin-picking from an homogenous heap of a similar object, based on visual similarity of objects stored in the semantic memory). As example, we have confronted the system to the constrained optimizations of 9 continuous hyperparameters for a professional software (Kamido) in industrial robotic arm bin-picking tasks, a step that is needed each time to handle correctly new object. We used a simulator to create bin-picking tasks for 8 different objects (7 in simulation and one with real setup, without and with meta-learning with experiences coming from other similar objects) achieving goods results despite a very small optimization budget, with a better performance reached when meta-learning is used (84.3 % vs 78.9 % of success overall, with a small budget of 30 iterations for each optimization) for every object tested (p-value=0.036).",https://ieeexplore.ieee.org/document/9278071/,2020 Joint IEEE 10th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob),26-30 Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCS52396.2021.00073,Bluetooth Communications in Educational Robotics,IEEE,Conferences,"In a world in a continuous and rapid change, it is absolutely necessary for our students to keep up with the rapid progress of new technologies: Internet of Things (IoT), Robotics, Artificial Intelligence (AI), Virtual Reality (VR), Augmented Reality (AR) etc. The rapid evolution and diversification of these emerging technologies has recently led to their introduction into the educational offer of the school curriculum for the gymnasium. The discipline of Information and Communication Technology (ICT) has already been implemented, a discipline that involves both the formation of skills to use new technologies and the formation of computational thinking necessary for the efficient and intelligent use of these technologies. In order to teach and learn Physics from a STEM (Science, Technology, Engineering and Mathematics) educational perspective, we initiated optional school courses of IoT, Robotics and AI (approached through Machine Learning). These courses stimulate, at the level of students, computational thinking, creativity and innovation and lead, from an interdisciplinary perspective, to the development of emerging specializations such as Mathematics-Physics-Automation, Mathematics-Physics-Electronics, Mathematics-Physics-Informatics-Robotics etc. In this paper we presented a method of approaching, in the school educational space, the study of wireless communication technologies between smart devices, through an Educational Robotics project. The project consisted of creating a wireless controlled mobile robotic platform (robot car) via a Bluetooth module connected to an Arduino Uno board.",https://ieeexplore.ieee.org/document/9481012/,2021 23rd International Conference on Control Systems and Computer Science (CSCS),26-28 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793510,Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs,IEEE,Conferences,"The ability to interpret a scene is an important capability for a robot that is supposed to interact with its environment. The knowledge of what is in front of the robot is, for example, relevant for navigation, manipulation, or planning. Semantic segmentation labels each pixel of an image with a class label and thus provides a detailed semantic annotation of the surroundings to the robot. Convolutional neural networks (CNNs) are popular methods for addressing this type of problem. The available software for training and the integration of CNNs for real robots, however, is quite fragmented and often difficult to use for non-experts, despite the availability of several high-quality open-source frameworks for neural network implementation and training. In this paper, we propose a tool called Bonnet, which addresses this fragmentation problem by building a higher abstraction that is specific for the semantic segmentation task. It provides a modular approach to simplify the training of a semantic segmentation CNN independently of the used dataset and the intended task. Furthermore, we also address the deployment on a real robotic platform. Thus, we do not propose a new CNN approach in this paper. Instead, we provide a stable and easy-to-use tool to make this technology more approachable in the context of autonomous systems. In this sense, we aim at closing a gap between computer vision research and its use in robotics research. We provide an open-source codebase for training and deployment. The training interface is implemented in Python using TensorFlow and the deployment interface provides C++ library that can be easily integrated in an existing robotics codebase, a ROS node, and two standalone applications for label prediction in images and videos.",https://ieeexplore.ieee.org/document/8793510/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE49439.2021.9551562,Building Skill Learning Systems for Robotics,IEEE,Conferences,"Skill-generating policies have enabled robots to perform a wide range of applications as for example assembly tasks. However, the manual engineering effort for such policies is fairly high and the environment is frequently required to be rather deterministic. For expanding robot deployment to low-volume manufacturing two challenges need to be addressed. First, the robot should acquire the skill-generating policy not from a robot programmer but rather from an expert on the task and second, the robot needs to be able to operate in unstructured environments. In this paper we present a learning approach that combines imitation learning and reinforcement learning to provide a tool for intuitive task teaching followed by self-optimization of the system. The presented approach is applied to a dual-arm assembly task using a real robot and appropriate simulation models. Whereas pure imitation learning does not result in an acceptable success rate for the considered example, after 400 episodes of reinforcement learning the robot can successfully solve the assembly task.",https://ieeexplore.ieee.org/document/9551562/,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),23-27 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1993.292250,Cellular robotics: simulation and HW implementation,IEEE,Conferences,"Aspects of self-organization are presented in this paper. Computer simulations as well as a real prototypical implementation are used to illustrate the proposed approach. Results of simulations are presented to compare different strategies of self-organization enabling a system of autonomous robots to form a chain between two landmarks in a completely unknown environment. This chain implicitly represents a path between any two points of the environment without an explicit representation of free space (no single robot has a global map of the environment). The experimental part, even if restricted to a few robots, demonstrates that the set of stimuli-action processes used in the simulations are indeed feasible on real systems.<>",https://ieeexplore.ieee.org/document/292250/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2001.977213,Computation principles for the development of visual skills in robotics,IEEE,Conferences,"Different working principles are often considered when different visual behaviors are implemented in an agent. This occurs basically because the physical interaction between the behavior and the environment is not studied in depth. The paper shows how apparently different visual behaviors share common theoretical principles for their working mechanism. In particular properties related to the navigation vector field they compute in the environment, provide a base to explain visual learning, guidance, topological navigation, sub goal placement, obstacle avoidance and navigation enhancement. To handle the mathematics of a vector field robust tools are needed. Techniques borrowed from computer vision literature provide the necessary mathematical tools. All behaviors described have been tested in real robots. On going research is still in progress for topological navigation and subgoal placement.",https://ieeexplore.ieee.org/document/977213/,Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180),29 Oct.-3 Nov. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO49542.2019.8961433,CyberEarth: a Virtual Simulation Platform for Robotics and Cyber-Physical Systems,IEEE,Conferences,"The increasing sophisticated robot and intelligent system applications require universal visualization platforms which can guarantee the security and efficiency of task process execution in the situation of user-programming and using different kinds of automated equipment. In this paper, we present a universal visualization framework to build up program-driven simulation software of complex robots and intelligent systems by integrating several open-source technical modules, including Ubuntu Linux operation-system, QT Creator IDE environment, ROS robot operation system, OSG(OpenSceneGraph) 3D scene, osgEarth GIS(Geographic Information System)-based 3D scene, and also Python based user-programing robotic script language. Many complex visualization simulation systems of complex tasks in wide area and dynamic scenarios are realized by using this framework. Based on this framework, we built a virtual simulation platform CyberEarth for robotics and Cyber-Physical systems. The typical robotic simulation task, which is a visual coverage task for Multi-Agent/UAV, is also introduced to demonstration the universality of this platform.",https://ieeexplore.ieee.org/document/8961433/,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),6-8 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRW55335.2022.00036,Developing a VR Simulator for Robotics Navigation and Human Robot Interactions employing Digital Twins,IEEE,Conferences,"Providing care to seniors and adults with Developmental Disabilities (DD) has seen increased use and development of assistive technologies including service robots. Such robots ease the challenges associated with care, companionship, medication intake, and fall prevention, among others. Research and development in this field rely on in-person data collection to ensure proper robot navigation, interactions, and service. However, the current COVID-19 pandemic has caused the implementation of physical distancing and access restrictions to long-term care facilities, thus making data collection very difficult. This traditional method poses numerous challenges as videos may not be representative of the population in terms of how people move, interact with the environment, or fall. In this paper, we present the development of a VR simulator for robotics navigation and fall detection with digital twins as a solution to test the virtual robot without having access to the real physical location, or real people. The development process required the development of virtual sensors that are able to create LIDAR data for the virtual robot to navigate and detect obstacles. Preliminary testing has allowed us to obtain promising results for the virtual simulator to train a service robot to navigate and detect falls. Our results include virtual maps, robot navigation, and fall detection.",https://ieeexplore.ieee.org/document/9757529/,2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),12-16 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MWSCAS.2018.8624056,EMG-based hand gesture control system for robotics,IEEE,Conferences,"In this paper, a Electromyogram (EMG) based hand gesture control system is developed. A wearable human machine interface (HMI) device is designed for an in-home assistance service robot. An EMG-based control system utilizes MyoWave muscle sensor to acquire and amplify EMG signal. A microcontroller system is used to an artificial neural network (ANN) to classify the EMG signal. Based on different hand movements, commands are sent through WiFi to control the motor in a service robot. The on-board Camera system mounted the robot can capture video real-time. In addition, a web server is implemented to provide live video feedback for robot navigation and user instructions.",https://ieeexplore.ieee.org/document/8624056/,2018 IEEE 61st International Midwest Symposium on Circuits and Systems (MWSCAS),5-8 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
,Engineering Safety in Swarm Robotics,IEEE,Conferences,"Robotics, artificial intelligence, and the Internet-of-Things are driving current research and development for the technology sector. Robotic and multi-robot systems are becoming pervasive and more and more lives rely on their proper functioning in transportation, medical systems, personal robotics, and manufacturing. Assuring the security and safety of these systems is of primary importance to guarantee the real-world applicability of current research, and we argue that it should be an integral part of system design. Current software standards for safety and security for critical systems (e.g. industrial and aerospace) are not directly applicable to the large distributed systems that are envisioned for the near future. In this paper, we propose to address safety and security of swarm robotics systems at the programming language level. We propose to extend the Buzz multi-robot scripting language with constructs and code analysis that allow the verification of safety and security during development. We believe that detecting and correcting issues with what are inherently emergent systems, i.e. where collective behavior might not be immediately apparent from a single robot's code, during development would allow for a more effective advancement of swarm robotics.",https://ieeexplore.ieee.org/document/8445818/,2018 IEEE/ACM 1st International Workshop on Robotics Software Engineering (RoSE),27 May-3 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON.2010.5453897,Enhancing student learning in artificial intelligence using robotics,IEEE,Conferences,"Artificial intelligence (AI) techniques may be applied to a variety of real-world problems. At Embry-Riddle Aeronautical University, CS 455: Artificial Intelligence was offered during the Spring 2008 semester in which students from all disciplines were invited to attend. Robot kits are incorporated into the course as a pedagogical tool to motivate and encourage learning by applying theoretically abstract algorithms to concrete real-world problems. This paper discusses the approach to incorporating robotics in the AI classroom. A set of commercial off-the-shelf robot kits are discussed and analyzed with respect to the students' work during the semester. Finally, recommendations for improvements on teaching AI to a multi-disciplinary audience with the help of robot kits will be discussed.",https://ieeexplore.ieee.org/document/5453897/,Proceedings of the IEEE SoutheastCon 2010 (SoutheastCon),18-21 March 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2019.8852425,Exploring Deep Models for Comprehension of Deictic Gesture-Word Combinations in Cognitive Robotics,IEEE,Conferences,"In the early stages of infant development, gestures and speech are integrated during language acquisition. Such a natural combination is therefore a desirable, yet challenging, goal for fluid human-robot interaction. To achieve this, we propose a multimodal deep learning architecture, for comprehension of complementary gesture-word combinations, implemented on an iCub humanoid robot. This enables human-assisted language learning, with interactions like pointing at a cup and labelling it with a vocal utterance. We evaluate various depths of the Mask Regional Convolutional Neural Network (for object and wrist detection) and the Residual Network (for gesture classification). Validation is carried out with two deictic gestures across ten real-world objects on frames recorded directly from the iCub's cameras. Results further strengthen the potential of gesture-word combinations for robot language acquisition.",https://ieeexplore.ieee.org/document/8852425/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793593,"Fast Instance and Semantic Segmentation Exploiting Local Connectivity, Metric Learning, and One-Shot Detection for Robotics",IEEE,Conferences,"Semantic scene understanding is important for autonomous robots that aim to navigate dynamic environments, manipulate objects, or interact with humans in a natural way. In this paper, we address the problem of jointly performing semantic segmentation as well as instance segmentation in an online fashion, so that autonomous robots can use this information on-the-go and without sacrificing accuracy. We achieve this by exploiting a local connectivity prior of objects in the real world and a multi-task convolutional neural network architecture. The network identifies the individual object instances and their classes without region proposals or pre-segmentation of the images into individual classes. We implemented and thoroughly evaluated our approach, and our experiments suggest that our method can be used to accurately segment instance masks of objects and identify their class in an online fashion.",https://ieeexplore.ieee.org/document/8793593/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SPCA.2006.297452,From Robotics to Pervasive Computing Environments,IEEE,Conferences,"This talk proposes that the digital home is virtually identical to the software and hardware architecture used to construct mobile robots leading to the proposition that ""pervasive computing environments can be regarded as robots that we live inside"". The author argue that it is possible and rational to apply robotic techniques to pervasive computing problems in the digital home",https://ieeexplore.ieee.org/document/4079023/,2006 First International Symposium on Pervasive Computing and Applications,3-5 Aug. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2011.6181717,Human-like gradual multi-agent Q-learning using the concept of behavior-based robotics for autonomous exploration,IEEE,Conferences,"In the last few years, the field of mobile robotics has made lots of advancements. These advancements are due to the extensive application of mobile robots for autonomous exploration. Mobile robots are being popularly used for applications in space, underwater explorations, underground coal mines monitoring, inspection in chemical/toxic/ nuclear factories etc. But if these environments are unknown/unpredictable, conventional/ classical robotics may not serve the purpose. In such cases robot learning is the best option. Learning from the past experiences, is one such way for real time application of robots for completely unknown environments. Reinforcement learning is one of the best learning methods for robots using a constant system-environment interaction. Both single and multi-agent concepts are available for implementation of learning. The current research work describes a multi-agent based reinforcement learning using the concept of behaviour-based robotics for autonomous exploration of mobile robots. The concept has also been tested both in indoor and outdoor environments using real-time robots.",https://ieeexplore.ieee.org/document/6181717/,2011 IEEE International Conference on Robotics and Biomimetics,7-11 Dec. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.2006.322407,Incorporating an Affective Model to an Intelligent Tutor for Mobile Robotics,IEEE,Conferences,"Emotions have been identified as important players in motivation, and motivation is very important for learning. When a tutor recognizes the affective state of the student and responds accordingly, the tutor may be able to motivate students and improve the learning process. We propose a general affective behavior model which integrates information from the student's pedagogical state, affective state, and the tutorial situation, to decide the best tutorial action, considering the tutor preferences from a pedagogical and affective point of view. Our proposal is based on emotions models, personality theories and teachers' expertise. The affective model is implemented as a dynamic decision network, with utility measures on both learning and motivation, and is being incorporated to an intelligent tutor within a virtual laboratory for learning mobile robotics. This paper presents preliminary results in the construction of the affective behavior model",https://ieeexplore.ieee.org/document/4116913/,Proceedings. Frontiers in Education. 36th Annual Conference,27-31 Oct. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.2007.4374932,Learning Wall Following Behaviour in Robotics through Reinforcement and Image-based States,IEEE,Conferences,"In this work, a visual and reactive wall following behaviour is learned by reinforcement. With artificial vision the environment is perceived in 3D, and it is possible to avoid obstacles that are invisible to other sensors that are more common in mobile robotics. Reinforcement learning reduces the need for intervention in behaviour design, and simplifies its adjustment to the environment, the robot and the task. In order to facilitate its generalization to other behaviours and to reduce the role of the designer, we propose a regular image-based codification of states. Even though this is much more difficult, our implementation converges and is robust. Results are presented with a Pioneer 2 AT. Learning phase has been realized on the Gazebo 3D simulator and the test phase has been proved in simulated and real environments to demonstrate the correct design and robustness of our algorithms.",https://ieeexplore.ieee.org/document/4374932/,2007 IEEE International Symposium on Industrial Electronics,4-7 June 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2019.8702186,"Live Demonstration: Neuromorphic Robotics, from Audio to Locomotion Through Spiking CPG on SpiNNaker",IEEE,Conferences,"This live demonstration presents an audio-guided neuromorphic robot: from a Neuromorphic Auditory Sensor (NAS) to locomotion using Spiking Central Pattern Generators (sCPGs). Several gaits are generated by sCPGs implemented on a SpiNNaker board. The output of these sCPGs is sent in a real-time manner to an Field Programmable Gate Array (FPGA) board using an AER-to-SpiNN interface. The control of the hexapod robot joints is performed by the FPGA board. The robot behavior can be changed in real-time by means of the NAS. The audio information is sent to the SpiNNaker board which classifies it using a Spiking Neural Network (SNN). Thus, the input sound will activate a specific gait pattern which will eventually modify the behavior of the robot.",https://ieeexplore.ieee.org/document/8702186/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2014.35,Mobile Robot Performance in Robotics Challenges: Analyzing a Simulated Indoor Scenario and Its Translation to Real-World,IEEE,Conferences,"This paper discusses the pros and cons of using 3D simulators for testing the autonomous behavior of mobile robots in indoor environments. Major contribution of the paper is the discussion about which problems that can be faced using the simulator and those that cannot. We present the integration and calibration of a real non-commercial robot in a simulator, the characterization of the errors in sensing, navigation, and manipulation, and how these errors would impact in the real performance of the robot. The experimental support of the claims made in the paper has been developed using the gazebo simulator. RoCKIn competition rulebook defined the indoor restrictions.",https://ieeexplore.ieee.org/document/7102451/,"2014 2nd International Conference on Artificial Intelligence, Modelling and Simulation",18-20 Nov. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON.2014.6950737,Modified reinforcement learning for sequential action behaviors and its application to robotics,IEEE,Conferences,"When developing a robot or other automaton, the efficacy of the agent is highly dependent on the performance of the behaviors which underpin the control system. Especially in the case of agents which must act in real world or disorganized environments, the design of robust behaviors can be both difficult and time consuming, and often requires the use of sensitive tuning. In response to this need, we present a behavioral, goal-oriented, reinforcement-based machine learning strategy which is flexible, simple to implement, and designed for application in real-world environments, but with the capability of software-based training. In this paper, we will explain our design paradigms, the formal implementation thereof, and the algorithm proper. We will show that the algorithm is able to emulate standard reinforcement learning within comparable training time, and to extend the capabilities thereof as well. We also demonstrate extension of learning beyond the scope of training examples, and present an example of a physical robot which learns a sequential action behavior by experimentation.",https://ieeexplore.ieee.org/document/6950737/,IEEE SOUTHEASTCON 2014,13-16 March 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITNG.2011.116,Nerve: A Lightweight Middleware for Quality-of-service Networked Robotics,IEEE,Conferences,"Social robots must adapt to dynamic environments, human interaction partners and challenging new stringent tasks. Their inner software should be designed and deployed carefully because slight changes in the robot's requirements can have an important impact in the existing code. This paper focus on the design and implementation of a lightweight middleware for networked robotics called \textit{Nerve}, which guarantees the scalability and quality-of-service requirements for this kind of real-time software. Its benefits have been proved through its use in a Robot Learning by Imitation control architecture, but its design guidelines are general enough to be also applied with common distributed and real-time embedded applications.",https://ieeexplore.ieee.org/document/5945314/,2011 Eighth International Conference on Information Technology: New Generations,11-13 April 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.2005.1554245,Plenary talk June 29; The 3rdGeneration of Robotics: Ubiquitous Robot,IEEE,Conferences,"This talk shows its possibility of implementation in real life through demonstrations using a Sobot, Rity: i) continuous interface between physical and virtual worlds ii) seamless transmission of Sobot between a PC and a Mobot, and iii) omnipresence of Sobot. Rity, developed at the Robot Intelligence Technology (RIT) Laboratory, KAIST, is a Sobot implemented as a 12 DOF artificial creature in the virtual 3D world created in a PC. It has virtual sensors to survive in the virtual world and physical sensors attached to the PC to interact with the real world. Based on sensor information it can express its emotion, and interact with human beings through a web camera in the real world. It can generate behaviors autonomously and has its own IP. This means that it can be accessed through a network at anywhere and anytime using any device. With this technique omnipresence of Sobot can be realized in a ubiquitous space. The eventual goal of this research is to integrate Sobot, Embot, and Mobot to build up a Ubibot so that ubiquitous services through it can be available in a ubiquitous era",https://ieeexplore.ieee.org/document/1554245/,2005 International Symposium on Computational Intelligence in Robotics and Automation,27-30 June 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACIIW.2019.8925192,Real-time pain detection in facial expressions for health robotics,IEEE,Conferences,"Automatic pain detection is an important challenge in health computing. In this paper we report on our efforts to develop a real-time, real-world pain detection system from human facial expressions. Although many studies addressed this challenge, most of them use the same dataset for training and testing. There is no cross-check with other datasets or implementation in real-time to check performance on new data. This is problematic, as evidenced in this paper, because the classifiers overtrain on dataset-specific features. This limits realtime, real-world usage. In this paper, we investigate different methods of real-time pain detection. The training data uses a combination of pain and emotion datasets, unlike other papers. The best model shows an accuracy of 88.4% on a dataset including pain and 7 non-pain emotional expressions. Results suggest that convolutional neural networks (CNN) are not the best methods in some cases as they easily overtrain if the dataset is biased. Finally we implemented our pain detection method on a humanoid robot for physiotherapy. Our work highlights the importance of cross-corpus evaluation & real-time testing, as well as the need for a well balanced and ecologically valid pain dataset.",https://ieeexplore.ieee.org/document/8925192/,2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW),3-6 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.1999.781948,Realization of robust controllers in evolutionary robotics: a dynamically-rearranging neural network approach,IEEE,Conferences,"The evolutionary robotics approach has been attracting a lot of attention in the field of robotics and artificial life. In this approach, neural networks are widely used to construct controllers for autonomous mobile agents, since they intrinsically have generalization, noise-tolerant abilities and so on. However, there are still open questions: (1) the gap between simulated and real environments, (2) the evolutionary and learning phase are completely separated, and (3) the conflict between stability and evolvability/adaptability. In this paper, we try to overcome these problems by incorporating the concept of dynamic rearrangement function of biological neural networks with the use of neuromodulators. Simulation results show that the proposed approach is highly promising.",https://ieeexplore.ieee.org/document/781948/,Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406),6-9 July 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2009.5286456,Realtime execution of automated plans using evolutionary robotics,IEEE,Conferences,"Applying neural networks to generate robust agent controllers is now a seasoned practice, with time needed only to isolate particulars of domain and execution. However we are often constrained to local problems due to an agents inability to reason in an abstract manner. While there are suitable approaches for abstract reasoning and search, there is often the issues that arise in using offline processes in real-time situations. In this paper we explore the feasibility of creating a decentralised architecture that combines these approaches. The approach in this paper explores utilising a classical automated planner that interfaces with a library of neural network actuators through the use of a Prolog rule base. We explore the validity of solving a variety of goals with and without additional hostile entities as well as added uncertainty in the the world. The end results providing a goal driven agent that adapts to situations and reacts accordingly.",https://ieeexplore.ieee.org/document/5286456/,2009 IEEE Symposium on Computational Intelligence and Games,7-10 Sept. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIS.2017.7959982,Robotics data real-time management based on NoSQL solution,IEEE,Conferences,"In nowadays, robotics database management systems are increasing. These systems ensure good storage of data and with big data analytic, a new approach demands new structures and methods for collecting, recording, and analyzing enterprise data. This paper work deals with the NoSQL databases which are the secret of the continual progression data that new data management solutions have been emerged. They crossed several areas as personalization, profile management, big data in real-time, content management, catalogue, view of customers, mobile applications, internet of things, digital communication and fraud detection. Machine learning, for example, thrives on more data, so smart machines can learn more and faster, the Robotics are our use of case to focus on our Test. The implementation of NoSQL for Robotics wrestle all the data they acquire into usable form because with the ordinary type of Robotics we are facing very big limits to manage and find the exact information in real-time. Our original proposed approach was demonstrated by experimental studies and running example used as a use case.",https://ieeexplore.ieee.org/document/7959982/,2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS),24-26 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSN.2019.00027,SOTER: A Runtime Assurance Framework for Programming Safe Robotics Systems,IEEE,Conferences,"The recent drive towards achieving greater autonomy and intelligence in robotics has led to high levels of complexity. Autonomous robots increasingly depend on third-party off-the-shelf components and complex machine-learning techniques. This trend makes it challenging to provide strong design-time certification of correct operation. To address these challenges, we present SOTER, a robotics programming framework with two key components: (1) a programming language for implementing and testing high-level reactive robotics software, and (2) an integrated runtime assurance (RTA) system that helps enable the use of uncertified components, while still providing safety guarantees. SOTER provides language primitives to declaratively construct a RTA module consisting of an advanced, high-performance controller (uncertified), a safe, lower-performance controller (certified), and the desired safety specification. The framework provides a formal guarantee that a well-formed RTA module always satisfies the safety specification, without completely sacrificing performance by using higher performance uncertified components whenever safe. SOTER allows the complex robotics software stack to be constructed as a composition of RTA modules, where each uncertified component is protected using a RTA module. To demonstrate the efficacy of our framework, we consider a real-world case-study of building a safe drone surveillance system. Our experiments both in simulation and on actual drones show that the SOTER-enabled RTA ensures the safety of the system, including when untrusted third-party components have bugs or deviate from the desired behavior.",https://ieeexplore.ieee.org/document/8809550/,2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),24-27 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/URAI.2016.7734049,Secure robotics,IEEE,Conferences,"Security is an under-studied problem within robotics and Internet of Things. Part of the reason for this is that currently most robots and IoT devices remain in the lab at all times. Recent trends show more robots and IoT devices moving “out into the wild” with no humans to protect them. This creates vulnerabilities beyond the well known and well studied network/internet based threat. These threats include external network, local network, software, physical access, tricking the artificial intelligence, and intellectual property theft. This document discribes the above and shows our current work towards detection and mitigation.",https://ieeexplore.ieee.org/document/7734049/,2016 13th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),19-22 Aug. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCVW.2017.84,SkiMap++: Real-Time Mapping and Object Recognition for Robotics,IEEE,Conferences,"We introduce SkiMap++, an extension to the recently proposed SkiMap mapping framework for robot navigation [1]. The extension deals with enriching the map with semantic information concerning the presence in the environment of certain objects that may be usefully recognized by the robot, e.g. for the sake of grasping them. More precisely, the map can accommodate information about the spatial locations of certain 3D object features, as determined by matching the visual features extracted from the incoming frames through a random forest learned off-line from a set of object models. Thereby, evidence about the presence of object features is gathered from multiple vantage points alongside with the standard geometric mapping task, so to enable recognizing the objects and estimating their 6 DOF poses. As a result, SkiMap++ can reconstruct the geometry of large scale environments as well as localize some relevant objects therein (Fig.1) in real-time on CPU. As an additional contribution, we present an RGB-D dataset featuring ground-truth camera and object poses, which may be deployed by researchers interested in pursuing SLAM alongside with object recognition, a topic often referred to as Semantic SLAM1.",https://ieeexplore.ieee.org/document/8265293/,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),22-29 Oct. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISRIMT53730.2021.9596792,Smart Seeing Eye Dog Wheeled Assistive Robotics,IEEE,Conferences,"To date, approximately 12 million people over the age of 40 in the United States are visually impaired, 3 million of whom are blind. Meanwhile, many people are blinded by accidents. Guide dogs can assist blind people in their daily life. However, the cost to breed, raise, train, and match a Guiding Eyes dog with a person with vision loss is estimated to be approximately $50,000. Another weakness is that the life expectancy of a human is 73.4 years around the world and the average working life of a guide dog is around eight years. As a result, people with visual impairments need more guide dogs throughout their lives. If our design is put into use, we can reduce the cost significantly. In this paper, we are going to build a smart seeing eye dog, which is a kind of wheeled Assistive Robotics that could guide the blind people walk on the road like but perform better than a real seeing eye dog. We started this paper from four aspects. They are the structure of the mechanical dog, TensorFlow-based image recognition, ROS-based motion simulation and human-computer interaction. Because of the cost of training an assistive dog is extremely high and the dogs are all red-green color-blind, which means that they cannot recognize the traffic lights and road sign. In our paper, these problems can be solved very well. Soon, between this mechanical dog paper may help blind people to better integrate into society.",https://ieeexplore.ieee.org/document/9596792/,2021 3rd International Symposium on Robotics & Intelligent Manufacturing Technology (ISRIMT),24-26 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMSOFT.2018.8537236,Special Session: Embedded Software for Robotics: Challenges and Future Directions,IEEE,Conferences,"This paper surveys recent challenges and solutions in the design, implementation, and verification of embedded software for robotics. Emphasis is placed on mobile robots, like self-driving cars. In design, it addresses programming support for robotic systems, secure state estimation, and ROS-based monitor generation. In the implementation phase, it describes the synthesis of control software using finite precision arithmetic, real-time platforms and architectures for safety-critical robotics, efficient implementation of neural network based-controllers, and standards for computer vision applications. The issues in verification include verification of neural network-based robotic controllers, and falsification of closed-loop control systems. The paper also describes notable open-source robotic platforms. Along the way, we highlight important research problems for developing the next generation of high-performance, low-resource-usage, correct embedded software.",https://ieeexplore.ieee.org/document/8537236/,2018 International Conference on Embedded Software (EMSOFT),30 Sept.-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2004.1308800,The artificial ecosystem: a distributed approach to service robotics,IEEE,Conferences,"We propose a multiagent, distributed approach to autonomous mobile robotics which is an alternative to most existing systems in literature: robots are thought of as mobile units within an intelligent environment where they coexist and co-operate with fixed, intelligent devices that are assigned different roles: helping the robot to localize itself, controlling automated doors and elevators, detecting emergency situations, etc. To achieve this, intelligent sensors and actuators (i.e. physical agents) are distributed both onboard the robot and throughout the environment, and they are handled by Real-Time software agents which exchange information on a distributed message board. The paper outlines the benefits of the approach in terms of efficiency and Real-Time responsiveness.",https://ieeexplore.ieee.org/document/1308800/,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",26 April-1 May 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCI46756.2018.00293,"Timing and its Implementation in a Language, Communication, and Systems Integration SDK and Platform for Intelligent Entities and Robotics",IEEE,Conferences,"A framework to integrate different artificial intelligence and machine learning algorithms is combined with an execution framework to create a powerful cloud computing system development platform. By providing an execution framework and control software that is native to cloud architectures and supports interactivity and time synchronization, the true utility of cloud computing and Big Data systems can be increased. Many Big Data software systems are not interactive, automated, or able to run in real-time. An integration example is provided.",https://ieeexplore.ieee.org/document/8947742/,2018 International Conference on Computational Science and Computational Intelligence (CSCI),12-14 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSRR53300.2021.9597686,Towards Ethics Training in Disaster Robotics: Design and Usability Testing of a Text-Based Simulation,IEEE,Conferences,"Rescue robots are expected to soon become commonplace at disaster sites, where they are increasingly being deployed to provide rescuers with improved access and intervention capabilities while mitigating risks. The presence of robots in operation areas, however, is likely to carry a layer of additional ethical complexity to situations that are already ethically challenging. In addition, limited guidance is available for ethically informed, practical decision-making in real-life disaster settings, and specific ethics training programs are lacking. The contribution of this paper is thus to propose a tool aimed at supporting ethics training for rescuers operating with rescue robots. To this end, we have designed an interactive text-based simulation. The simulation was developed in Python, using Tkinter, Python's de-facto standard GUI. It is designed in accordance with the Case-Based Learning approach, a widely used instructional method that has been found to work well for ethics training. The simulation revolves around a case grounded in ethical themes we identified in previous work on ethical issues in rescue robotics: fairness and discrimination, false or excessive expectations, labor replacement, safety, and trust. Here we present the design of the simulation and the results of usability testing.",https://ieeexplore.ieee.org/document/9597686/,"2021 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)",25-27 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2017.8172430,"Towards the use of consumer-grade electromyographic armbands for interactive, artistic robotics performances",IEEE,Conferences,"In recent years, gesture-based interfaces have been explored in order to control robots in non-traditional ways. These require the use of systems that are able to track human body movements in 3D space. Deploying Mo-cap or camera systems to perform this tracking tend to be costly, intrusive, or require a clear line of sight, making them ill-adapted for artistic performances. In this paper, we explore the use of consumer-grade armbands (Myo armband) which capture orientation information (via an inertial measurement unit) and muscle activity (via electromyography) to ultimately guide a robotic device during live performances. To compensate for the drop in information quality, our approach rely heavily on machine learning and leverage the multimodality of the sensors. In order to speed-up classification, dimensionality reduction was performed automatically via a method based on Random Forests (RF). Online classification results achieved 88% accuracy over nine movements created by a dancer during a live performance, demonstrating the viability of our approach. The nine movements are then grouped into three semantically-meaningful moods by the dancer for the purpose of an artistic performance achieving 94% accuracy in real-time. We believe that our technique opens the door to aesthetically-pleasing sequences of body motions as gestural interface, instead of traditional static arm poses.",https://ieeexplore.ieee.org/document/8172430/,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),28 Aug.-1 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593799,Utility Model Re-description within a Motivational System for Cognitive Robotics,IEEE,Conferences,"This paper describes a re-descriptive approach to the efficient acquisition of ever higher level and more precise utility models within the motivational system (MotivEn) of a cognitive architecture. The approach is based on a two-step process whereby, as a first step, simple imprecise sensor correlation related utility models are obtained from the interaction traces of the robot. These utility models allow the robot to increase the frequency of achieving goals, and thus, provide lots of traces that can be used to try to train precise value functions implemented as artificial neural networks. The approach is tested experimentally on a real robotic setup that involves the coordination of two robots.",https://ieeexplore.ieee.org/document/8593799/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3007064,Drive Through Robotics: Robotic Automation for Last Mile Distribution of Food and Essentials During Pandemics,IEEE,Journals,"The COVID-19 pandemic unraveled the weak points in the global supply chain for goods. Specifically, people all over the world, including those in the most advanced nations have had to go without medical supplies and personal protective equipment. Scarcity of essentials increases anxiety and uncertainty exacerbating unproductive behaviors like hoarding and price gouging. Left to market forces, such unfair practices are likely to aggravate hardships and increase the loss of lives. Thus, there is a critical need to ensure safe distribution of food and essential supplies to all citizens to sustain them through challenging times. To this end, we propose a simple, affordable and contact-less robotic system for preparing and dispensing food and survival-kits at community scale. The system has provisions to prevent hoarding and price gouging. Design, simulation, and, validation of the system has been completed to ensure readiness for real world implementation. This project is part of an open-source program and detailed designs are available upon request to entities interested in using it to serve their communities.",https://ieeexplore.ieee.org/document/9133423/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2938366,Grasping Objects From the Floor in Assistive Robotics: Real World Implications and Lessons Learned,IEEE,Journals,"This paper presents a system enabling a mobile robot to autonomously pick-up objects a human is pointing at from the floor. The system does not require object models and is designed to grasp unknown objects. The robot decides by itself if an object is suitable for grasping by considering measures of size, position and the environment suitability. The implementation is built on the second prototype of the home care robot Hobbit, thereby verifying that complex robotic manipulation tasks can be performed with economical hardware. The presented system was already tested in real apartments with elderly people. We highlight this by discussing the additional complexity for complete autonomous behavior in apartments compared with tests in labs.",https://ieeexplore.ieee.org/document/8819885/,IEEE Access,2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2018.2873597,Hierarchical Semantic Mapping Using Convolutional Neural Networks for Intelligent Service Robotics,IEEE,Journals,"The introduction of service robots in the public domain has introduced a paradigm shift in how robots are interacting with people, where robots must learn to autonomously interact with the untrained public instead of being directed by trained personnel. As an example, a hospital service robot is told to deliver medicine to Patient Two in Ward Three. Without awareness of what “Patient Two” or “Ward Three” is, a service robot must systematically explore the environment to perform this task, which requires a long time. The implementation of a Semantic Map allows for robots to perceive the environment similar to people by associating semantic information with spatial information found in geometric maps. Currently, many semantic mapping works provide insufficient or incorrect semantic-metric information to allow a service robot to function dynamically in human-centric environments. This paper proposes a semantic map with a hierarchical semantic organization structure based on a hybrid metric-topological map leveraging convolutional neural networks and spatial room segmentation methods. Our results are validated using multiple simulated and real environments on our lab's custom developed mobile service robot and demonstrate an application of semantic maps by providing only vocal commands. We show that this proposed method provides better capabilities in terms of semantic map labeling and retain multiple levels of semantic information.",https://ieeexplore.ieee.org/document/8490234/,IEEE Access,2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TLT.2018.2833111,Inquiry-Based Learning With RoboGen: An Open-Source Software and Hardware Platform for Robotics and Artificial Intelligence,IEEE,Journals,"It has often been found that students appreciate hands-on work, and find that they learn more with courses that include a project than those relying solely on conventional lectures and tests. This type of project driven learning is a key component of “Inquiry-based learning” (IBL), which aims at teaching methodology as well as content by incorporating the student as an actor rather than a spectator. Robotics applications are especially well-suited for IBL due to the value of trial and error experience, the multiple possibilities for students to implement their own ideas, and the importance of programming, problem-solving, and electro-mechanical skills in real world engineering and science jobs. Furthermore, robotics platforms can be useful teaching media and learning tools for a variety of topics. Here, we present RoboGen: an open-source, web-based, software, and hardware platform for Robotics and Artificial Intelligence with a particular focus on Evolutionary Robotics. We describe the platform in detail, compare it to existing alternatives, and present results of its use as a platform for Inquiry-based learning within a master's level course at the Ecole Polytechnique Fédérale de Lausanne.",https://ieeexplore.ieee.org/document/8354804/,IEEE Transactions on Learning Technologies,1 July-Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2016.2597119,Multidimensional Modeling of Physiological Tremor for Active Compensation in Handheld Surgical Robotics,IEEE,Journals,"Precision, robustness, dexterity, and intelligence are the design indices for current generation surgical robotics. To augment the required precision and dexterity into normal microsurgical work-flow, handheld robotic instruments are developed to compensate physiological tremor in real time. The hardware (sensors and actuators) and software (causal linear filters) employed for tremor identification and filtering introduces time-varying unknown phase delay that adversely affects the device performance. The current techniques that focus on three-dimensions (3-D) tip position control involves modeling and canceling the tremor in three axes (x-, y-, and z -axes) separately. Our analysis with the tremor recorded from surgeons and novice subjects shows that there exists significant correlation in tremor across the dimensions. Based on this, a new multidimensional modeling approach based on extreme learning machines is proposed in this paper to correct the phase delay and to accurately model 3-D tremor simultaneously. Proposed method is evaluated through both simulations and experiments. Comparison with the state-of-the art techniques highlight the suitability and better performance of the proposed approach for tremor compensation in handheld surgical robotics.",https://ieeexplore.ieee.org/document/7529172/,IEEE Transactions on Industrial Electronics,Feb. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMCB.2008.920227,Multihierarchical Interactive Task Planning: Application to Mobile Robotics,IEEE,Journals,"To date, no solution has been proposed to human-machine interactive task planning that deals simultaneously with two important issues: 1) the capability of processing large amounts of information in planning (as it is needed in any real application) and 2) being efficient in human-machine communication (a proper set of symbols for human-machine interaction may not be suitable for efficient automatic planning and vice versa). In this paper, we formalize a symbolic model of the environment to solve these issues in a natural form through a human-inspired mechanism that structures knowledge in multiple hierarchies. Planning with a hierarchical model may be efficient even in cases where the lack of hierarchical information would make it intractable. However, in addition, our multihierarchical model is able to use the symbols that are most familiar to each human user for interaction, thus achieving efficiency in human-machine communication without compromising the task-planning performance. We formalize here a general interactive task-planning process which is then particularized to be applied to a mobile robotic application. The suitability of our approach has been demonstrated with examples and experiments.",https://ieeexplore.ieee.org/document/4505426/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",June 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JPROC.2018.2856739,Navigating the Landscape for Real-Time Localization and Mapping for Robotics and Virtual and Augmented Reality,IEEE,Journals,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",https://ieeexplore.ieee.org/document/8436423/,Proceedings of the IEEE,Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JSSC.2020.3028298,NeuroSLAM: A 65-nm 7.25-to-8.79-TOPS/W Mixed-Signal Oscillator-Based SLAM Accelerator for Edge Robotics,IEEE,Journals,"Simultaneous localization and mapping (SLAM) is a quintessential problem in autonomous navigation, augmented reality, and virtual reality. In particular, low-power SLAM has gained increasing importance for its applications in power-limited edge devices such as unmanned aerial vehicles (UAVs) and small-sized cars that constitute devices with edge intelligence. This article presents a 7.25-to-8.79-TOPS/W mixed-signal oscillator-based SLAM accelerator for applications in edge robotics. This study proposes a neuromorphic SLAM IC, called NeuroSLAM, employing oscillator-based pose-cells and a digital head direction cell to mimic place cells and head direction cells that have been discovered in a rodent brain. The oscillatory network emulates a spiking neural network and its continuous attractor property achieves spatial cognition with a sparse energy distribution, similar to the brains of rodents. Furthermore, a lightweight vision system with a max-pooling is implemented to support low-power visual odometry and re-localization. The test chip fabricated in a 65-nm CMOS exhibits a peak energy efficiency of 8.79 TOPS/W with a power consumption of 23.82 mW.",https://ieeexplore.ieee.org/document/9222208/,IEEE Journal of Solid-State Circuits,Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/13.485240,Robotics laboratory exercises,IEEE,Journals,"The authors report new laboratory exercises in robotic manipulation, computer vision, artificial intelligence, and mechatronics, four areas that are central to any robotics curriculum. The laboratory exercises supply the student with hands-on experience that complements classroom lectures and software development. Through this experience, the student confronts the hard realities of robot systems and learns to deal with them. Such hands-on experience is essential for a sound robotics education, because many critical lessons about the real world can only be learned through personal experience.",https://ieeexplore.ieee.org/document/485240/,IEEE Transactions on Education,Feb. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TETC.2017.2769705,Robust Robot Tracking for Next-Generation Collaborative Robotics-Based Gaming Environments,IEEE,Journals,"The collaboration between humans and robots is one of the most disruptive and challenging research areas. Even considering advances in design and artificial intelligence, humans and robots could soon ally to perform together a number of different tasks. Robots could also became new playmates. In fact, an emerging trend is associated with the so-called phygital gaming, which builds upon the idea of merging the physical world with a virtual one in order to let physical and virtual entities, such as players, robots, animated characters and other game objects interact seamlessly as if they were all part of the same reality. This paper specifically focuses on mixed reality gaming environments that can be created by using floor projection, and tackles the issue of enabling accurate and robust tracking of off-the-shelf robots endowed with limited sensing capabilities. The proposed solution is implemented by fusing visual tracking data gathered via a fixed camera in a smart environment with odometry data obtained from robot's on-board sensors. The solution has been tested within a phygital gaming platform in a real usage scenario, by experimenting with a robotic game that exhibits many challenging situations which would be hard to manage using conventional tracking techniques.",https://ieeexplore.ieee.org/document/8094867/,IEEE Transactions on Emerging Topics in Computing,1 July-Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASE.2019.2938316,Semiautomatic Labeling for Deep Learning in Robotics,IEEE,Journals,"In this article, we propose an augmented reality semiautomatic labeling (ARS), a semiautomatic method which leverages on moving a 2-D camera by means of a robot, proving precise camera tracking, and an augmented reality pen (ARP) to define initial object bounding box, to create large labeled data sets with minimal human intervention. By removing the burden of generating annotated data from humans, we make the deep learning technique applied to computer vision, which typically requires very large data sets, truly automated and reliable. With the ARS pipeline, we created two novel data sets effortlessly, one on electromechanical components (industrial scenario) and other on fruits (daily-living scenario) and trained two state-of-the-art object detectors robustly, based on convolutional neural networks, such as you only look once (YOLO) and single shot detector (SSD). With respect to conventional manual annotation of 1000 frames that takes us slightly more than 10 h, the proposed approach based on ARS allows to annotate 9 sequences of about 35 000 frames in less than 1 h, with a gain factor of about 450. Moreover, both the precision and recall of object detection is increased by about 15% with respect to manual labeling. All our software is available as a robot operating system (ROS) package in a public repository alongside with the novel annotated data sets. Note to Practitioners-This article was motivated by the lack of a simple and effective solution for the generation of data sets usable to train a data-driven model, such as a modern deep neural network, so as to make them accessible in an industrial environment. Specifically, a deep learning robot guidance vision system would require such a large amount of manually labeled images that it would be too expensive and impractical for a real use case, where system reconfigurability is a fundamental requirement. With our system, on the other hand, especially in the field of industrial robotics, the cost of image labeling can be reduced, for the first time, to nearly zero, thus paving the way for self-reconfiguring systems with very high performance (as demonstrated by our experimental results). One of the limitations of this approach is the need to use a manual method for the detection of objects of interest in the preliminary stages of the pipeline (ARP or graphical interface). A feasible extension, related to the field of collaborative robotics, could be used to exploit the robot itself, manually moved by the user, even for this preliminary stage, so as to eliminate any source of inaccuracy.",https://ieeexplore.ieee.org/document/8844069/,IEEE Transactions on Automation Science and Engineering,April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TE.2012.2224867,SyRoTek—Distance Teaching of Mobile Robotics,IEEE,Journals,"E-learning is a modern and effective approach for training in various areas and at different levels of education. This paper gives an overview of SyRoTek, an e-learning platform for mobile robotics, artificial intelligence, control engineering, and related domains. SyRoTek provides remote access to a set of fully autonomous mobile robots placed in a restricted area with dynamically reconfigurable obstacles, which enables solving a huge variety of problems. A user is able to control the robots in real time by their own developed algorithms as well as being able to analyze gathered data and observe activity of the robots by provided interfaces. The system is currently used for education at the Czech Technical University in Prague, Prague, Czech Republic, and at the University of Buenos Aires, Buenos, Aires, Argentina, and it is freely accessible to other institutions. In addition to the system overview, this paper presents the experience gained from the actual deployment of the system in teaching activities.",https://ieeexplore.ieee.org/document/6341862/,IEEE Transactions on Education,Feb. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1992.601935,"""Arnie P."" - A Robot Golfing System Using Binocular And A Heuristic Feedback Mechanism",IEEE,Conferences,"This paper describes a robot vision golfing system. The Automated Robotic Navigational unit with Intelligent Eye and Putter (ARNIE P)τproject was initiated to investigate the problems and develop software solutions for robotic tasks that require good hand-eye coordination and an intelligent sensor feedback mechanism. This system has only one frame buffer and no specialized hardware, so quasi-real time 3D tracking is accomplished in software using the Unix Spline facility. The single frame buffer and digitizer, stores and retains the location of the ball from two separate cameras during the time interval between the golf ball initially crossing a trigger scan line and the ball coming to a complete stop. The most novel aspect of this study is that by attempting to build or model a difficult perceptory task such as golf, which requires integrating many complicated computational pieces (binocular stereo vision, robot arm motion, heuristic feedback, learning), it appears to be a good plarform to experiment with artificial intelligence techniques and robotics.",https://ieeexplore.ieee.org/document/601935/,Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems,7-10 July 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCYB.2004.1437628,2004 International Conference on Computational Cybernetics Proceedings,IEEE,Conferences,The following topics are dealt with: cybernetics; control; hardware architectures; vision systems; genetic algorithms; evolutionary computing; sensors; real-time systems; multiagents systems; robotics; software engineering; intelligent systems; learning; fractional-order systems; and engineering systems,https://ieeexplore.ieee.org/document/1437628/,"Second IEEE International Conference on Computational Cybernetics, 2004. ICCC 2004.",30 Aug.-1 Sept. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS52415.2021.9466061,3D Control System of Arm Robot Prototype for Skin Cancer Detection,IEEE,Conferences,"Arm robot has a lack of control systems that depend on desired control for assistive medical. Our laboratory robotics & artificial intelligent at Padjadjaran University created skin cancer detection of arm robot with dark flow framework to identify skin cancer in real-time. The implementation of the arm robot was for increasing the accuracy, precision, and stability. The main purpose of this paper was to control an arm robot for skin cancer detection that is capable to scan the whole body skin to localize the skin cancers by driving the manipulator in circular or elliptical skimming. To initiate the communication with the arm robot which used Dynamixel as the actuators, we applied USB2Dynamixel as the communicator. SMPS2Dynamixel was used to supply the power into servo motors. 3D Control system software has designed, and it had some features such as; forward kinematic movement, inverse kinematic movement, and 3D simulation to help user visualize the position of the arm robot. Control software was built in MATLAB GUI environment and 3D simulation adapted Peter Corke Robotics Toolbox.",https://ieeexplore.ieee.org/document/9466061/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2006.372290,3D Modelling from Multi-view Registered Range Images Using K-means Clustering,IEEE,Conferences,"3D modelling from range images captured using laser scanning systems finds a wide range of applications in computer vision and industrial robotics. However due to the presence of scanning noise, accumulative registration errors, and improper data fusion, the reconstructed surfaces from multiple registered range images captured from different viewpoints are often distorted with thick patches, false connections and blurred features. Moreover, the existing integration methods are often expensive in the sense of computational time and data storage. These shortcomings will hinder the wide applications of 3D modelling using the latest laser scanning systems. In this paper, the k-means clustering approach from the pattern recognition and machine learning literatures is employed to optimally fuse the overlapping areas between two range images captured from two neighbouring viewpoints and to iteratively minimize the integration error. The final fused point set is then triangulated using an improved Delaunay method, guaranteeing a watertight surface. The new method is theoretically guaranteed to converge. A comparative study based on real images shows that the proposed algorithm is computationally efficient and significantly reduces the integration error, while desirably retaining geometric details of object surface.",https://ieeexplore.ieee.org/document/4237612/,2006 IEEE International Conference on Industrial Technology,15-17 Dec. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3DV50981.2020.00038,3D-Aware Ellipse Prediction for Object-Based Camera Pose Estimation,IEEE,Conferences,"In this paper, we propose a method for coarse camera pose computation which is robust to viewing conditions and does not require a detailed model of the scene. This method meets the growing need of easy deployment of robotics or augmented reality applications in any environments, especially those for which no accurate 3D model nor huge amount of ground truth data are available. It exploits the ability of deep learning techniques to reliably detect objects regardless of viewing conditions. Previous works have also shown that abstracting the geometry of a scene of objects by an ellipsoid cloud allows to compute the camera pose accurately enough for various application needs. Though promising, these approaches use the ellipses fitted to the detection bounding boxes as an approximation of the imaged objects. In this paper, we go one step further and propose a learning-based method which detects improved elliptic approximations of objects which are coherent with the 3D ellipsoid in terms of perspective projection. Experiments prove that the accuracy of the computed pose significantly increases thanks to our method and is more robust to the variability of the boundaries of the detection boxes. This is achieved with very little effort in terms of training data acquisition - a few hundred calibrated images of which only three need manual object annotation.",https://ieeexplore.ieee.org/document/9320405/,2020 International Conference on 3D Vision (3DV),25-28 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECTI-CON54298.2022.9795616,6D Valves Pose Estimation based on YOLACT and DenseFusion for the Offshore Robot Application,IEEE,Conferences,"Offshore oil and gas operations are most concerned with safety, it often requires workers to be stationed on the platform to control ongoing process. The cost of transportation can be expensive, and the working environment is dangerous, so it is natural for certain operations to be automated away by autonomous robots. One such operation is maneuvering valves, which is a common and important task most suitable for robotics. To achieve this goal, the robot itself need the ability to perceive the location and orientation of the target valve. In this paper, we proposed a computer vision methods based on the RGB-D image inputs for guiding the real-world spatial information of valves to the robot. In detail, we first exploited the YOLACT as an instance segmentation method to precisely extract the valves from the background then pass the segmented valve pixel to the 6D pose estimation algorithm. The 2D pixels and 3D points generated are then utilized by the DenseFusion algorithm to predict the valve&#x2019;s 6D pose composed of position and orientation based on fusion of RGB and depth features. In order to evaluate the validity of the proposed method, these algorithms were then implemented in ROS2 and tested on edge device which embedded in the real offshore robot system. The results show that our proposed method is promising and could be effectively utilized by the offshore autonomous robot to operate the valves.",https://ieeexplore.ieee.org/document/9795616/,"2022 19th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)",24-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/A-SSCC53895.2021.9634782,A 40nm Embedded SG-MONOS Flash Macro for High-end MCU Achieving 200MHz Random Read Operation and 7.91Mb/mm2 Density with Charge Assisted Offset Cancellation Sense Amplifier,IEEE,Conferences,"The fusion of AI and IoT technologies in conjunction with 5G mobile communication is strongly driving the realization of smarter societies and factories. In particular, endpoint devices for home automation, machine vision, robotics, etc. need higher performance and higher intelligence. That is why some of these devices are moving to so-called “crossover area”, which is located at the boundary between high-end MCU area and low-end MPU area. In this area, it is essential to achieve both high performance and low cost as shown in Fig. 1. Users familiar with conventional MCUs prefer to continue the approach with MCUs from the viewpoints of reusability of existing software assets, real-time operations and BOM costs (no need of external Flash memories). To meet their expectations for performance and cost, embedded Flash memory (eFlash), one of the key components in an MCU, should achieve high-speed random read operation corresponding to CPU operations at higher frequency (600MHz-1GHz) while reducing its macro size which occupies a significant part of MCU die size.",https://ieeexplore.ieee.org/document/9634782/,2021 IEEE Asian Solid-State Circuits Conference (A-SSCC),7-10 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00028,A Benchmark of Four Methods for Generating 360° Saliency Maps from Eye Tracking Data,IEEE,Conferences,"Modeling and visualization of user attention in Virtual Reality is important for many applications, such as gaze prediction, robotics, retargeting, video compression, and rendering. Several methods have been proposed to model eye tracking data as saliency maps. We benchmark the performance of four such methods for 360° images. We provide a comprehensive analysis and implementations of these methods to assist researchers and practitioners. Finally, we make recommendations based on our benchmark analyses and the ease of implementation.",https://ieeexplore.ieee.org/document/8613647/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCC51160.2020.9347897,A Comparative Analysis of Kinematics of Industrial Robot KUKA KR 60–3 Using Scientific Computing Languages,IEEE,Conferences,"In the field of robotics, there are kinematic analysis methods that are responsible for describing the positions and orientations of the end effectors, as well as the angles, velocities and trajectories of industrial robots; such techniques are: forward kinematics, inverse kinematics and velocity kinematics. For the solutions of these complex mathematical calculations, the use of scientific computing languages or programs is required; which more and more algorithms, libraries and complements are implemented, that achieve a reduction in programming hours and result in the creation of better solutions in areas of all kinds. For this reason, the kinematics of the Industrial Robot KUKA KR 60-3 was programmed in the languages and programs most used in scientific computing, with the aim of comparing the performance (real time) when carrying out symbolic and numerical analysis in said studies.",https://ieeexplore.ieee.org/document/9347897/,2020 Asia Conference on Computers and Communications (ACCC),18-20 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOEI53556.2022.9777153,A Comparative Study of Machine Learning Based Image Captioning Models,IEEE,Conferences,"Automated image captioning is a crucial concept for numerous real-world applications as it is useful in robotics, image indexing, self-driving vehicles and greatly helpful for impaired eyesight people. An image provided in real-time can be converted into text using image captioning models developed by machine learning algorithms. Understanding an image mostly depends on the features of the image. Machine learning techniques are widely used for image captioning tasks. This research study has performed a comparative analysis on three Machine Learning (ML) algorithms, i.e. k-Nearest Neighbor (KNN), Convolution Neural Network (CNN) with Long Short Term Memory (LSTM) and Attention Based LSTM. In addition, an improved KNN algorithm with reduced time complexity and an improved CNN with LSTM and Attention Based LSTM model with an added beam search method is proposed to improve the underlying approaches further. The performance of the three selected models are empirically evaluated using BLEU, ROUGE and METEOR scores on the widely used flickr8k dataset, and the experimental results demonstrate the supremacy of the Attention Based LSTM over the other two approaches. Finally, the current study&#x0027;s findings help guide the researchers and practitioners in selecting the appropriate approach for Image Captioning with empirical evidence in terms of standard evaluation metrics.",https://ieeexplore.ieee.org/document/9777153/,2022 6th International Conference on Trends in Electronics and Informatics (ICOEI),28-30 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KSE.2018.8573394,A Comparative Study on Detection and Estimation of a 3-D Object Model in a Complex Scene,IEEE,Conferences,"In this paper, we tackle the approaches to detect and estimate 3-D object model in a complex scene. Although it is fundamental research in the computer vision and robotics community, this task still has many challenges especially when the scene is complex with contaminated or occluded data. To do this, we compare three common approaches including two conventional ways (e.g., geometrical and appearance-based techniques) and the proposed scheme. While geometrical approaches tend to directly detect and estimate objects without any learning procedure, the appearance-based required a training process to model the interested object. We show that a combination of recent advantages of deep learning (e.g., RCNN, Yolo) could resolve the detection task, while the geometrical based approaches estimate full 3-D model. The evaluation utilizes two different dataset. One from a public available, second one is our self-prepared dataset. Difference scenarios are considered in the evaluation. The results confirmed that the proposed technique achieves the best performances. As a consequence, it suggests to deploy real application supporting visually impaired people in detecting and grasping common objects in their activities of daily living.",https://ieeexplore.ieee.org/document/8573394/,2018 10th International Conference on Knowledge and Systems Engineering (KSE),1-3 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CODESISSS51650.2020.9244038,A Fast Design Space Exploration Framework for the Deep Learning Accelerators: Work-in-Progress,IEEE,Conferences,"The Capsule Networks (CapsNets) is an advanced form of Convolutional Neural Network (CNN), capable of learning spatial relations and being invariant to transformations. CapsNets requires complex matrix operations which current accelerators are not optimized for, concerning both training and inference passes. Current state-of-the-art simulators and design space exploration (DSE) tools for DNN hardware neglect the modeling of training operations, while requiring long exploration times that slow down the complete design flow. These impediments restrict the real-world applications of CapsNets (e.g., autonomous driving and robotics) as well as the further development of DNNs in life-long learning scenarios that require training on low-power embedded devices. Towards this, we present XploreDL, a novel framework to perform fast yet high-fidelity DSE for both inference and training accelerators, supporting both CNNs and CapsNets operations. XploreDL enables a resource-efficient DSE for accelerators, focusing on power, area, and latency, highlighting Pareto-optimal solutions which can be a green-lit to expedite the design flow. XploreDL can reach the same fidelity as ARM's SCALE-sim, while providing 600x speedup and having a 50x lower memory-footprint. Preliminary results with a deep CapsNet model on MNIST for training accelerators show promising Pareto-optimal architectures with up to 0.4 TOPS/squared-mm and 800 fJ/op efficiency. With inference accelerators for AlexNet the Pareto-optimal solutions reach up to 1.8 TOPS/squared-mm and 200 fJ/op efficiency.",https://ieeexplore.ieee.org/document/9244038/,2020 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),20-25 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAD51958.2021.9643557,A General Hardware and Software Co-Design Framework for Energy-Efficient Edge AI,IEEE,Conferences,"A huge number of edge applications including self-driving cars, mobile health, robotics, and augmented reality / virtual reality are enabled by deep neural networks (DNNs). Currently, much of this computation for these applications happens in the cloud, but there are several good reasons to perform the processing on local edge platforms such as smartphones: improved accessibility to different parts of the world, low latency, and data privacy. In this paper, we present a general hardware and software co-design framework for energy-efficient edge AI for both simple classification and structured output prediction tasks (e.g., 3D shapes from images). This framework relies on two key ideas. First, we design a space of DNNs of increasing complexity (coarse to fine) and perform input-specific adaptive inference by selecting a DNN of appropriate complexity depending on the hardness of input examples. Second, we execute the selected DNN on the target edge platform using a resource management policy to save energy. We also provide instantiations of our co-design framework for three qualitatively different problem settings: convolutional neural networks for image classification, graph convolutional networks for predicting 3D shapes from images, and generative adversarial networks on photo-realistic unconditional image generation. Our experiments on real-world benchmarks and mobile platforms show the effectiveness of our co-design framework in achieving significant gain in energy with little to no loss in accuracy of predictions.",https://ieeexplore.ieee.org/document/9643557/,2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD),1-4 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN52387.2021.9534180,A Lightweight sequence-based Unsupervised Loop Closure Detection,IEEE,Conferences,"Stable, effective and lightweight loop closure detection is an always pursued goal in real-time SLAM systems, that can be ported on embedded processors and deployed on autonomous robotics. Deep learning methods have extended the expressive ability and adaptability of the descriptor, and sequence-based methods can greatly improve the matching accuracy. However, the increased computation complexity and storage bandwidth requirements of matching calculations for high-dimensional descriptor make it infeasible for real-time deployment, especially for robots that navigate in relatively big maps. To address this challenge, we propose a lightweight sequence-based unsupervised loop closure detection scheme. To be specific, Principal Component Analysis (PCA) is applied to squeeze the descriptor dimensions while maintaining sufficient expressive ability. Additionally, with the consideration of the image sequence and combining linear query with fast approximate nearest neighbor search to further reduce the execution time and improve the efficiency of sequence matching. We implement our method on CALC, a state-of-the-art unsupervised solution, and conduct experiments on NVIDIA TX2, results demonstrate that the accuracy has been improved by 5%, while the execution speed is 2× faster. Source code is available at https://github.com/Mingrui-Yu/Seq-CALC.",https://ieeexplore.ieee.org/document/9534180/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UEMCON47517.2019.8993080,A Low-Cost Arm Robotic Platform based on Myoelectric Control for Rehabilitation Engineering,IEEE,Conferences,"Rehabilitation robotics is a recent kind of service robot that include devices such as robotic prosthesis and exoskeletons. These devices could help motor disabled people to rehabilitate their motor functions, and could provide functional compensation to accomplish motor activities. In order to control robotic prosthesis and exoskeletons it is required to identify human movement intention, to be converted into commands for the device. Motor impaired people may use surface electromyography (sEMG) signals to control these devices, taking into account that sEMG signals directly reflects the human motion intention. Myoelectric control is an advanced technique related with the detection, processing, classification, and application of sEMG signals to control human-assisting robots or rehabilitation devices. Despite recent advances with myoelectric control algorithms, currently there is still an important need to develop suitable methods involving usability, for controlling prosthesis and exoskeletons in a natural way. Traditionally, acquiring EMG signals and developing myoelectric control algorithms require expensive hardware. With the advent of low-cost technologies (i.e. sensors, actuators, controllers) and hardware support of simulation software packages as Matlab, affordable research tools could be used to develop novel myoelectric control algorithms. This work describes the implementation and validation of a Matlab-based robotic arm using low-cost technologies such as Arduino commanded using myoelectric control. The platform permits implementation of a variety of EMG-based algorithms. It was carried out a set of experiments aimed to evaluate the platform, through an application of pattern recognition based myoelectric control to identify and execute seven movements of the robotic upper limb: 1-forearm pronation; 2- forearm supination; 3-wrist flexion; 4-wrist extension; 5- elbow flexion; 6- elbow extension; 7-resting. The algorithm use a feature extraction stage based on a combination of time and frequency domain features (mean absolute value, waveform length, root mean square) and a widely used k-NN classifier. Obtained mean classification errors were 5.9%. As future work, additional features in the myoelectric control algorithm will be evaluated, for real-time applications.",https://ieeexplore.ieee.org/document/8993080/,"2019 IEEE 10th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",10-12 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2019.8914519,A Multimodal Perception System for Detection of Human Operators in Robotic Work Cells,IEEE,Conferences,"Workspace monitoring is a critical hw/sw component of modern industrial work cells or in service robotics scenarios, where human operators share their workspace with robots. Reliability of human detection is a major requirement not only for safety purposes but also to avoid unnecessary robot stops or slowdowns in case of false positives. The present paper introduces a novel multimodal perception system for human tracking in shared workspaces based on the fusion of depth and thermal images. A machine learning approach is pursued to achieve reliable detection performance in multi-robot collaborative systems. Robust experimental results are finally demonstrated on a real robotic work cell.",https://ieeexplore.ieee.org/document/8914519/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTSS52674.2021.00016,A ROS 2 Response-Time Analysis Exploiting Starvation Freedom and Execution-Time Variance,IEEE,Conferences,"Robots are commonly subject to real-time constraints. To ensure that such constraints are met, recent work has analyzed the response times of processing chains under ROS 2, a popular robotics framework. However, prior work supports only scalar worst-case execution time bounds and does not exploit that the ROS 2 scheduling mechanism is starvation-free. This paper proposes a novel response-time analysis for ROS 2 processing chains that accounts for both the high execution-time variance typically encountered in robotics workloads and the starvation freedom of the default ROS 2 callback scheduler. Experimental results from both synthetic callback graphs and a real ROS 2 workload empirically show the proposed analysis to be much more accurate (often by a factor of 2x or more).",https://ieeexplore.ieee.org/document/9622336/,2021 IEEE Real-Time Systems Symposium (RTSS),7-10 Dec 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635878,A Registration-aided Domain Adaptation Network for 3D Point Cloud Based Place Recognition,IEEE,Conferences,"In the field of large-scale SLAM for autonomous driving and mobile robotics, 3D point cloud based place recognition has aroused significant research interest due to its robustness to changing environments with drastic daytime and weather variance. However, it is time-consuming and effort-costly to obtain high-quality point cloud data for place recognition model training and ground truth for registration in the real world. To this end, a novel registration-aided 3D domain adaptation network for point cloud based place recognition is proposed. A structure-aware registration network is introduced to help to learn features with geometric information and a 6-DoFs pose between two point clouds with partial overlap can be estimated. The model is trained through a synthetic virtual LiDAR dataset through GTA-V with diverse weather and daytime conditions and domain adaptation is implemented to the real-world domain by aligning the global features. Our results outperform state-of-the-art 3D place recognition baselines or achieve comparable on the real-world Oxford RobotCar dataset with the visualization of registration on the virtual dataset.",https://ieeexplore.ieee.org/document/9635878/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARM52023.2021.9536056,A Review of Bilateral Teleoperation Control Strategies with Soft Environment,IEEE,Conferences,"In the past two decades, bilateral teleoperation with haptic feedback has attracted great research and application interests in both robotics and other areas. Initially triggered by the need to handle dangerous and remote distance tasks such as nuclear materials manipulation and space exploration, bilateral teleoperation has found its way into other applications as a result of development of control theory, robotic technology (both hardware and software) and latest breakthrough in artificial intelligence and machine learning. Consequently, bilateral teleoperation is found facing new challenges brought by these new applications. One major and obvious change is the working environment for the slave manipulator: different from rigid or solid contact environments which are reasonably assumed in early applications in industrial, nuclear and aerospace applications, the slave environment is now more complex and often the objects in contact are much softer in term of stiffness and can not be described by simple elastic model if good teleoperation performance (accurate and transparent) is expected. In this paper, the research of bilateral teleoperation system considering soft environment in recent 20 years has been surveyed for the first time in literature, to the knowledge of the authors. Following the difference in real applications, in this review the definition of soft environment covers linear elastic environment with much lower stiffness than conventional industrial environment and nonlinear complex soft environment with/out time-varying characteristics. Accordingly, the surveyed control strategies and structures in recent literature to improve the stability and accuracy of bilateral teleoperation with soft environment are classified and explained. Finally, the main applications, current challenges and future perspectives of bilateral teleoperation with soft environment are discussed.",https://ieeexplore.ieee.org/document/9536056/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561941,A Robot Walks into a Bar: Automatic Robot Joke Success Assessment,IEEE,Conferences,"Effective social robots should leverage humor’s unique ability to improve relationship connections and dispel stress, but current robots possess limited (if any) humorous abilities. In this paper, we aim to supplement one aspect of autonomous robots by giving robotic systems the ability to ""read the room"" to assess how their humorous statements are received by nearby people in real time. Using a dataset of the audio of crowd responses to a robotic comedian over multiple performances (first presented in past work), we establish human-labeled joke success ground truths and compare individual human rater accuracy against the outputs of lightweight Machine Learning (ML) approaches that are easy to deploy in real-time joke assessment. Our results indicate that all three ML approaches (naïve Bayes, support vector machines, and single-hidden-layer feedforward neural networks) performed significantly better than the baseline approach used in our past work. In particular, support vector machines and neural network approaches are comparable to a human rater in the task of assessing if a joke failed or not in certain cases. The products of this work will inform self-assessment techniques for robots and help social robotics researchers test their own assessment methods on realistic data from human crowds.",https://ieeexplore.ieee.org/document/9561941/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561722,A Scavenger Hunt for Service Robots,IEEE,Conferences,"Creating robots that can perform general-purpose service tasks in a human-populated environment has been a longstanding grand challenge for AI and Robotics research. One particularly valuable skill that is relevant to a wide variety of tasks is the ability to locate and retrieve objects upon request. This paper models this skill as a Scavenger Hunt (SH) game, which we formulate as a variation of the NP-hard stochastic traveling purchaser problem. In this problem, the goal is to find a set of objects as quickly as possible, given probability distributions of where they may be found. We investigate the performance of several solution algorithms for the SH problem, both in simulation and on a real mobile robot. We use Reinforcement Learning (RL) to train an agent to plan a minimal cost path, and show that the RL agent can outperform a range of heuristic algorithms, achieving near optimal performance. In order to stimulate research on this problem, we introduce a publicly available software stack and associated website that enable users to upload scavenger hunts which robots can download, perform, and learn from to continually improve their performance on future hunts.",https://ieeexplore.ieee.org/document/9561722/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593714,A Software Framework for Planning Under Partial Observability,IEEE,Conferences,"Planning under partial observability is both challenging and critical for reliable robot operation. The past decade has seen substantial advances in this domain: The mathematically principled approach for addressing such problems, namely the Partially Observable Markov Decision Process (POMDP), has started to become practical for various robotics tasks. Good approximate solutions for problems framed as POMDPs can now be computed on-line, with a few classes of problems being solved in near real-time. However, applications of these more recent advances are often hindered by the lack of easy-to-use software tools. Implementation of state of the art algorithms exist, but most (if not all)require the POMDP model to be hard-coded inside the program, increasing the difficulty of applying them. To alleviate this problem, we propose a software toolkit, called On-line POMDP Planning Toolkit (OPPT)(downloadable from http://robotics.itee.uq.edu.au/~oppt). By providing a well-defined and general abstract solver API, OPPT enables the user to quickly implement new POMDP solvers. Furthermore, OPPT provides an easy-to-use plug-in architecture with interfaces to the high-fidelity simulator Gazebo that, in conjunction with user-friendly configuration files, allows users to specify POMDP models of a standard class of robot motion planning under partial observability problems with no additional coding effort.",https://ieeexplore.ieee.org/document/8593714/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1998.727477,A constraint-based controller for soccer-playing robots,IEEE,Conferences,"Soccer meets the requirements of the situated agent approach and as a task domain is sufficiently rich to support research integrating many branches of robotics and AI. A robot is an integrated system, with a controller embedded in its plant. A robotic system is the coupling of a robot to its environment. Robotic systems are, in general, hybrid dynamic systems, consisting of continuous, discrete and event-driven components. Constraint nets provide a semantic model for modeling hybrid dynamic systems. Controllers are embedded constraint solvers that solve constraints in real-time. A controller for our new softbot soccer team, UBC Dynamo98, has been modeled in constraint nets, and implemented in Java, using the Java Beans architecture. The paper demonstrates that the formal constraint net approach is a practical tool for designing and implementing controllers for robots in multi-agent real-time environments.",https://ieeexplore.ieee.org/document/727477/,"Proceedings. 1998 IEEE/RSJ International Conference on Intelligent Robots and Systems. Innovations in Theory, Practice and Applications (Cat. No.98CH36190)",17-17 Oct. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759384,A convolutional neural network for robotic arm guidance using sEMG based frequency-features,IEEE,Conferences,"Recently, robotics has been seen as a key solution to improve the quality of life of amputees. In order to create smarter robotic prosthetic devices to be used in an everyday context, one must be able to interface them seamlessly with the end-user in an inexpensive, yet reliable way. In this paper, we are looking at guiding a robotic device by detecting gestures through measurement of the electrical activity of muscles captured by surface electromyography (sEMG). Reliable sEMG-based gesture classifiers for end-users are challenging to design, as they must be extremely robust to signal drift, muscle fatigue and small electrode displacement without the need for constant recalibration. In spite of extensive research, sophisticated sEMG classifiers for prostheses guidance are not yet widely used, as systems often fail to solve these issues simultaneously. We propose to address these problems by employing Convolutional Neural Networks. Specifically as a first step, we demonstrate their viability to the problem of gesture recognition for a low-cost, low-sampling rate (200Hz) consumer-grade, 8-channel, dry electrodes sEMG device called Myo armband (Thalmic Labs) on able-bodied subjects. To this effect, we assessed the robustness of this machine learning oriented approach by classifying a combination of 7 hand/wrist gestures with an accuracy of ∼97.9% in real-time, over a period of 6 consecutive days with no recalibration. In addition, we used the classifier (in conjunction with orientation data) to guide a 6DoF robotic arm, using the armband with the same speed and precision as with a joystick. We also show that the classifier is able to generalize to different users by testing it on 18 participants.",https://ieeexplore.ieee.org/document/7759384/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MWSCAS.2016.7870095,A framework for teaching robotic control using a novel visual programming language,IEEE,Conferences,"This paper proposes an education robotic platform that aims to improve teaching methods of programming and robotics skills, for both beginner and advanced users. We propose an innovative platform that consists of a versatile set of sensors and actuators, controlled by utilizing a user-friendly visual programming language through a mobile phone interface, or by utilizing a representational state transfer application programming interface for more advanced users. Suggested methods form the foundation of problem-based learning, by emphasizing hands-on experimental assignments and activities, and collaborative learning. We present the software and hardware architecture of the system, and case studies for utilizing different control modes. Consequently, students can improve their understanding of basic robotic concepts by observing real-time response and feedback of the actuator and sensor modules integrated in the robotic platform.",https://ieeexplore.ieee.org/document/7870095/,2016 IEEE 59th International Midwest Symposium on Circuits and Systems (MWSCAS),16-19 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARSC.2018.8374189,A generic visual perception domain randomisation framework for Gazebo,IEEE,Conferences,"The impressive results of applying deep neural networks in tasks such as object recognition, language translation, and solving digital games are largely attributed to the availability of massive amounts of high quality labelled data. However, despite numerous promising steps in incorporating these techniques in robotic tasks, the cost of gathering data with a real robot has halted the proliferation of deep learning in robotics. In this work, a plugin for the Gazebo simulator is presented, which allows rapid generation of synthetic data. By introducing variations in simulator-specific or irrelevant aspects of a task, one can train a model which exhibits some degree of robustness against those aspects, and ultimately narrow the reality gap between simulated and real-world data. To show a use-case of the developed software, we build a new dataset for detection and localisation of three object classes: box, cylinder and sphere. Our results in the object detection and localisation task demonstrate that with small datasets generated only in simulation, one can achieve comparable performance to that achieved when training on real-world images.",https://ieeexplore.ieee.org/document/8374189/,2018 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC),25-27 April 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2014.6942859,A machine learning approach for real-time reachability analysis,IEEE,Conferences,"Assessing reachability for a dynamical system, that is deciding whether a certain state is reachable from a given initial state within a given cost threshold, is a central concept in controls, robotics, and optimization. Direct approaches to assess reachability involve the solution to a two-point boundary value problem (2PBVP) between a pair of states. Alternative, indirect approaches involve the characterization of reachable sets as level sets of the value function of an appropriate optimal control problem. Both methods solve the problem accurately, but are computationally intensive and do no appear amenable to real-time implementation for all but the simplest cases. In this work, we leverage machine learning techniques to devise query-based algorithms for the approximate, yet real-time solution of the reachability problem. Specifically, we show that with a training set of pre-solved 2PBVP problems, one can accurately classify the cost-reachable sets of a differentially-constrained system using either (1) locally-weighted linear regression or (2) support vector machines. This novel, query-based approach is demonstrated on two systems: the Dubins car and a deep-space spacecraft. Classification errors on the order of 10% (and often significantly less) are achieved with average execution times on the order of milliseconds, representing 4 orders-of-magnitude improvement over exact methods. The proposed algorithms could find application in a variety of time-critical robotic applications, where the driving factor is computation time rather than optimality.",https://ieeexplore.ieee.org/document/6942859/,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,14-18 Sept. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISTI.2015.7170600,A mixed reality game using 3Pi robots — “PiTanks”,IEEE,Conferences,"In the growing field of Robotics, one of the many possible paths to explore is the social aspect that it can influence upon the present society. The combination of the goal-oriented development of robots with the interactivity used in games while employing mixed reality is a promising route to take in regard to designing user-friendly robots and improving problem solving featured in artificial intelligence software. In this paper, we present a competitive team-based game using Pololu's 3Pi robots moving in a projected map, capable of human interaction via game controllers. The game engine was developed utilizing the framework Qt Creator with C++ and OpenCV for the image processing tasks. The technical framework uses the ROS framework for communications that may be, in the future, used to connect different modules. Various parameters of the implementation are tested, such as position tracking errors.",https://ieeexplore.ieee.org/document/7170600/,2015 10th Iberian Conference on Information Systems and Technologies (CISTI),17-20 June 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SACI.2013.6608963,A new NIR camera for gesture control of electronic devices,IEEE,Conferences,"Since the introduction Gesture Control technology in the electronic gaming technology a series of attempts have been made to deploy it also on other domains such as robotics, teaching, medical, automotive and many others. Human gesture used for Man-Machine Interaction became attractive as it offers a simpler way of controlling sophisticated devices, in a sci-fi-like scenario, in return of an increasingly computational power required by the artificial intelligence algorithms needed to detect, track and recognize them. There have been attempts to bring a solution to it by using 2D or 3D based image processing methods. There is a clear balance incline towards 3D methods in the consumer product as besides the almost insurmountable difficulties for producing robust and stable results, the price constraint added supplementary hurdles. As perfect illumination conditions are core factors in obtaining the above results, the infrared light was unanimously adopted by the domain technologies. In this paper, a novel real-time depth-mapping principle and a corresponding hardware solution for an IR depth-mapping camera is introduced. The new IR camera architecture comprises an illuminator module which is pulsed and modulated via a monotonic function using a phase-locked loop control for the laser intensity, while the reflected infrared light is captured during the increasing and decreasing monotonic function. A reconfigurable hardware architecture (RHA) unit calculates the depth and controls the IR waves in synchronism with the infrared sensor. The resolution of the depth map is variable depending on the resolution and gating possibilities of the image sensor. A sensor of 1 megapixel is used, providing a resolution of 1024×1024. Images of real objects are reconstructed in 3D based on the data obtained by the laser controlled by the RHA. A corresponding image processing algorithm builds the 3D map of the object in real-time. In this paper the camera is used to control consumer electronic products such as TV sets, laptops and others.",https://ieeexplore.ieee.org/document/6608963/,2013 IEEE 8th International Symposium on Applied Computational Intelligence and Informatics (SACI),23-25 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2000.856157,A new board for CNN stereo vision algorithm,IEEE,Conferences,Artificial vision for environment recognition is a very useful tool in autonomous robotics. Specifically the use of stereo vision algorithms implemented via a hardware neural architecture allows real time scene reconstruction. In this paper the follow-on of previous work on an analogue hardware Cellular Neural Network implementation of the algorithm is presented. In this paper a new CNN based PCI electronic board is presented.,https://ieeexplore.ieee.org/document/856157/,2000 IEEE International Symposium on Circuits and Systems (ISCAS),28-31 May 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FTC.2016.7821768,A non-biological AI approach towards natural language understanding,IEEE,Conferences,"The problem being addressed in this paper is that using brute force in Natural Language Processing and Machine Learning combined with advanced statistics will only approximate meaning and thus will not deliver in terms of real text understanding. Counting words and tracking word order or parsing by syntax will also result in probability and guesswork at best. Their vendors struggle in delivering accurate quality and this results in ill-functioning applications. The newer generation methodologies like Deep Learning and Cognitive Computing are breaking barriers in the (Big Data) fields of Internet of Things, Robotics and Image/Video Recognition but cannot be successfully deployed for text without huge amounts of training and sample data. In the short term, we believe non-biological Artificial Intelligence will produce the best results for text understanding. Miia applied advanced Linguistic and Semantic Technologies combined with ConceptNet modeling and Machine Learning to successfully cater deep intelligent and cross-language quality to several industries.",https://ieeexplore.ieee.org/document/7821768/,2016 Future Technologies Conference (FTC),6-7 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SICE.2002.1195611,A reinforcement learning using adaptive state space construction strategy for real autonomous mobile robots,IEEE,Conferences,"In the recent robotics, much attention has been focused on utilizing reinforcement learning for designing robot controllers. However, there still exists difficulties, one of them is well known as state space explosion problem. As the state space for a learning system becomes continuous and high dimensional, its combinational state space exponentially explodes and the learning process is time consuming. In this paper, we propose an adaptive state space recruitment strategy for reinforcement learning, which enables the system to divide state space gradually according to task complexity and progress of learning. Some simulation results and real robot implementation show the validity of the method.",https://ieeexplore.ieee.org/document/1195611/,Proceedings of the 41st SICE Annual Conference. SICE 2002.,5-7 Aug. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRDS.2002.1041504,A reinforcement learning with adaptive state space recruitment strategy for real autonomous mobile robots,IEEE,Conferences,"In the recent robotics, much attention has been focused on utilizing reinforcement learning for designing robot controllers. However, there still exists difficulties, one of them is well known as state space explosion problem. As the state space for learning system becomes continuous and high dimensional, the learning process results in time-consuming since its combinational states explodes exponentially. In order to adopt reinforcement learning for such complicated systems, it should be taken not only ""adaptability"" but ""computational efficiencies"" into account. In the paper, we propose an adaptive state space recruitment strategy for reinforcement learning, which enables the system to divide state space gradually according to task complexity and progress of learning. Some simulation results and real robot implementation show the validity of the method.",https://ieeexplore.ieee.org/document/1041504/,IEEE/RSJ International Conference on Intelligent Robots and Systems,30 Sept.-4 Oct. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2012.6485305,A robust real-time tracking system based on an adaptive selection mechanism for mobile robots,IEEE,Conferences,"Extensive research has been conducted in the domain of object tracking. Among the existing tracking methods, most of them mainly focus on using various cues such as color, texture, contour, features, motion as well as depth information to achieve a robust tracking performance. The tracking methods themselves are highly emphasized while properties of the objects to be tracked are usually not exploited enough. In this paper, we first propose a novel adaptive tracking selection mechanism dependent on the properties of the objects. The system will automatically choose the optimal tracking algorithm after examining the textureness of the object. In addition, we propose a robust tracking algorithm for uniform objects based on color information which can cope with real world constraints. In the mean time, we deployed a textured object tracking algorithm which combines the Lucas-Kanade tracker and a model based tracker using the Random Forests classifier. The whole system was tested and the experimental results on a variety of objects show the effectiveness of the adaptive tracking selection mechanism. Moreover, the promising tracking performance shows the robustness of the proposed tracking algorithm. The computation cost of the algorithm is very low, which proves that it can be further used in various real-time robotics applications.",https://ieeexplore.ieee.org/document/6485305/,2012 12th International Conference on Control Automation Robotics & Vision (ICARCV),5-7 Dec. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2017.7965912,A self-driving robot using deep convolutional neural networks on neuromorphic hardware,IEEE,Conferences,"Neuromorphic computing is a promising solution for reducing the size, weight and power of mobile embedded systems. In this paper, we introduce a realization of such a system by creating the first closed-loop battery-powered communication system between an IBM Neurosynaptic System (IBM TrueNorth chip) and an autonomous Android-Based Robotics platform. Using this system, we constructed a dataset of path following behavior by manually driving the Android-Based robot along steep mountain trails and recording video frames from the camera mounted on the robot along with the corresponding motor commands. We used this dataset to train a deep convolutional neural network implemented on the IBM NS1e board containing a TrueNorth chip of 4096 cores. The NS1e, which was mounted on the robot and powered by the robot's battery, resulted in a self-driving robot that could successfully traverse a steep mountain path in real time. To our knowledge, this represents the first time the IBM TrueNorth has been embedded on a mobile platform under closed-loop control.",https://ieeexplore.ieee.org/document/7965912/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2010.5509238,A voice-commandable robotic forklift working alongside humans in minimally-prepared outdoor environments,IEEE,Conferences,"One long-standing challenge in robotics is the realization of mobile autonomous robots able to operate safely in existing human workplaces in a way that their presence is accepted by the human occupants. We describe the development of a multi-ton robotic forklift intended to operate alongside human personnel, handling palletized materials within existing, busy, semi-structured outdoor storage facilities. The system has three principal novel characteristics. The first is a multimodal tablet that enables human supervisors to use speech and pen-based gestures to assign tasks to the forklift, including manipulation, transport, and placement of palletized cargo. Second, the robot operates in minimally-prepared, semi-structured environments, in which the forklift handles variable palletized cargo using only local sensing (and no reliance on GPS), and transports it while interacting with other moving vehicles. Third, the robot operates in close proximity to people, including its human supervisor, other pedestrians who may cross or block its path, and forklift operators who may climb inside the robot and operate it manually. This is made possible by novel interaction mechanisms that facilitate safe, effective operation around people. We describe the architecture and implementation of the system, indicating how real-world operational requirements motivated the development of the key subsystems, and provide qualitative and quantitative descriptions of the robot operating in real settings.",https://ieeexplore.ieee.org/document/5509238/,2010 IEEE International Conference on Robotics and Automation,3-7 May 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIRCA48905.2020.9182995,An Approach for Digital Farming using Mobile Robot,IEEE,Conferences,"Farming is the backbone of the Indian economy and it has been unchartered territory for a technological solution. As of late developments in Artificial Intelligence technology combined with Robotics has paved the way for an option of digital farming. As a matter of fact, Indian farming has been facing various challenges that include abrupt change in climatic conditions, spoiling of yields, soil nutrient requirement, pests/weed control and so forth. Robotics and Artificial Intelligence (AI) along with the integration of various sensors ensures the possibility of better outcome. In this work the simulation of Mobile robot for the purpose of seed sowing along with its movement has been presented. The implementation comprises of the Motor schema for the navigation of robot and Gale Shapley (GS) algorithm for stable match of seed and yield combination. Such a robotic system combined with AI in real time will form excellent means of farming in terms of yield.",https://ieeexplore.ieee.org/document/9182995/,2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA),15-17 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISEC52395.2021.9763986,An Autonomous Driving Simulation Platform as a Virtual HSAVC Competition Environment,IEEE,Conferences,"At ISEC 2018, Professor Marc E. Herniter presented the High School Autonomous Vehicle Competition (HSAVC), which introduces autonomous driving to high school students. The competition promotes STEM education by challenging participants to use MATLAB to create a vision-based track detection algorithm and Simulink to build a motor controller model. Following the COVID-19 global pandemic, many in-person STEM competitions were canceled, including HSAVC. The goal of the Autonomous Driving Simulation Platform is to replicate the physical conditions of HSAVC using simulation to allow students to continue the activity virtually. Using MATLAB, the Simulation Platform creates a real-time virtual environment for students to test their HSAVC track detection algorithms and motor controller models. The Simulation Platform consists of two MATLAB apps: a Track Generator and a Driving Simulator. The Track Generator application can create fixed tracks based on user inputs or randomized tracks based on user-defined lengths. The Track Generator utilizes a growth and mutation algorithm to create a track with three distinct track sections: straight, left curve, and right curve. The Track Generator&#x2019;s randomized track replicates the HSAVC&#x2019;s physical track, and the Driving Simulator replicates the HSAVC&#x2019;s 1:18 scale autonomous vehicle equipped with a linescan camera, two drive motors, and a servo motor with a vehicle model and a camera model. The Track Generator and Driving Simulator have been successfully designed and implemented with MATLAB App Designer. Users can create a track and test their algorithms and models through an intuitive interface, making it an effective tool for STEM education in any classroom. The Autonomous Driving Simulation Platform holds potential as a solution to continue the HSAVC during the pandemic and can increase student engagement in the HSAVC from high schools around the world like Amazon Web Services DeepRacer. Another benefit of the Simulation Platform is convenient and controlled virtual algorithm testing, which allows for repetitive experimentation to be simulated without risk of damaged materials. The simulation platform has broad potential as an educational tool, such as complementing high school robotics curriculums to teach motor control algorithms and training reinforced learning racing models. The successful virtual adaptation of HSAVC demonstrates how simulation can provide many educational benefits when borrowing the framework of STEM competition.",https://ieeexplore.ieee.org/document/9763986/,2021 IEEE Integrated STEM Education Conference (ISEC),13-13 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2010.5509310,An Inertia-Based Surface Identification System,IEEE,Conferences,"In many robotics applications, knowing the material properties around a robot is often critical for the robot's successful performance. For example, in mobility, knowledge about the ground surface may determine the success of a robot's gait. In manipulation, the physical properties of an object may dictate the results of a grasping strategy. Thus, a reliable surface identification system would be invaluable for these applications. This paper presents an Inertia-Based Surface Identification System (ISIS) based on accelerometer sensor data. Using this system, a robot actively “knocks” on a surface with an accelerometer-equipped device (e.g., hand or leg), collects the accelerometer data in real-time, and then analyzes and extracts three critical physical properties, the hardness, the elasticity, and the stiffness, of the surface. A lookup table and k-nearest neighbors techniques are used to classify the surface material based on a database of previously known materials. This technique is low-cost and efficient in computation. It has been implemented on the modular and self-reconfigurable SuperBot and has achieved high accuracy (95% and 85%) in several identification experiments with real-world material.",https://ieeexplore.ieee.org/document/5509310/,2010 IEEE International Conference on Robotics and Automation,3-7 May 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSE46696.2019.8984462,An IoT Reconfigurable SoC Platform for Computer Vision Applications,IEEE,Conferences,"The field of Internet of Things (IoT) and smart sensors has expanded rapidly in various fields of research and industrial applications. The area of IoT robotics has become a critical component in the evolution of Industry 4.0 standard. In this paper, we developed an IoT based reconfigurable System on Chip (SoC) robot that is fast and efficient for computer vision applications. It can be deployed in other IoT robotics applications and achieve its intended function. A Terasic Hexapod Spider Robot (TSR) was used with its DE0-Nano SoC board to implement our IoT robotics system. The TSR was designed to provide a competent computer vision application to recognize different shapes using a machine learning classifier. The data processing for image detection was divided into two parts, the first part involves hardware implementation on the SoC board and to provide real-time interaction of the robot with the surrounding environment. The second part of implementation is based on the cloud processing technique, where further data analysis was performed. The image detection algorithm for the computer vision component was tested and successfully implemented to recognize shapes. The TSR moves or reacts based on the detected image. The Field Programmable Gate Array (FPGA) part is programmed to handle the movement of the robot and the Hard Processor System (HPS) handles the shape recognition, Wi-Fi connectivity, and Bluetooth communication. This design is implemented, tested and can be used in real-time applications in harsh environments where movements of other robots are restricted.",https://ieeexplore.ieee.org/document/8984462/,2019 International Symposium on Systems Engineering (ISSE),1-3 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DCOSS.2019.00111,An Open Source and Open Hardware Deep Learning-Powered Visual Navigation Engine for Autonomous Nano-UAVs,IEEE,Conferences,"Nano-size unmanned aerial vehicles (UAVs), with few centimeters of diameter and sub-10 Watts of total power budget, have so far been considered incapable of running sophisticated visual-based autonomous navigation software without external aid from base-stations, ad-hoc local positioning infrastructure, and powerful external computation servers. In this work, we present what is, to the best of our knowledge, the first 27g nano-UAV system able to run aboard an end-to-end, closed-loop visual pipeline for autonomous navigation based on a state-of-the-art deep-learning algorithm, built upon the open-source CrazyFlie 2.0 nano-quadrotor. Our visual navigation engine is enabled by the combination of an ultra-low power computing device (the GAP8 system-on-chip) with a novel methodology for the deployment of deep convolutional neural networks (CNNs). We enable onboard real-time execution of a state-of-the-art deep CNN at up to 18Hz. Field experiments demonstrate that the system's high responsiveness prevents collisions with unexpected dynamic obstacles up to a flight speed of 1.5m/s. In addition, we also demonstrate the capability of our visual navigation engine of fully autonomous indoor navigation on a 113m previously unseen path. To share our key findings with the embedded and robotics communities and foster further developments in autonomous nano-UAVs, we publicly release all our code, datasets, and trained networks.",https://ieeexplore.ieee.org/document/8804776/,2019 15th International Conference on Distributed Computing in Sensor Systems (DCOSS),29-31 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.859945,An agent team for RoboCup simulator league,IEEE,Conferences,"RoboCup is an attempt to promote AI and robotics research by providing a common task, soccer playing, for evaluation of various theories, algorithms and agent architectures. RoboCup consists of both the real robot league and the simulator league, where the soccer server is a standard software platform. A wide range of key issues on AI research emerges when designing simulator teams, such as the agent architecture, multi-agent teamwork, machine learning, etc. These issues are what we concerned most when developing our simulator team. Our team participated the first RoboCup tournament in China, and won the second place in the competition.",https://ieeexplore.ieee.org/document/859945/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2002.1235010,An autonomous mobile robot with fuzzy obstacle avoidance behaviors and a visual landmark recognition system,IEEE,Conferences,"Multi-sensor fusion has been a hot topic in the field of robotics. Inspired by the modern philosophy's spirit, the behavior-based systems interact with the real world directly. In this study, a fully autonomous mobile robot is developed that extracts all its knowledge from physical sensors and expresses all its goals and desires as physical action to affect its environment. The control software implements behavior-based artificial intelligence, where the coordination between various sensors are realized by layers of several simple and primitive behaviors similar to those observed in animals. In the developed mobile robot, each module itself generates behaviors. Behaviors corresponding to different sensors have different priorities, where the vision system has the lowest priority, and the ultrasonic sensors and bumper sensors have higher priority. The effectiveness of the developed system is demonstrated by experimental studies.",https://ieeexplore.ieee.org/document/1235010/,"7th International Conference on Control, Automation, Robotics and Vision, 2002. ICARCV 2002.",2-5 Dec. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2010.5509935,An insect-based method for learning landmark reliability using expectation reinforcement in dynamic environments,IEEE,Conferences,"Navigation in unknown dynamic environments still remains a major challenge in robotics. Whereas insects like the desert ant with very limited computing and memory capacities solve this task with great efficiency. Thus, the understanding of the underlying neural mechanisms of insect navigation can inform us on how to build simpler yet robust autonomous robots. Based on recent developments in insect neuroethology and cognitive psychology, we propose a method for landmark navigation in dynamic environments. Our method enables the navigator to learn the reliability of landmarks using an expectation reinforcement method. For that end, we implemented a real-time neuronal model based on the Distributed Adaptive Control framework. The results demonstrate that our model is capable of learning the stability of landmarks by reinforcing its expectations. Also, the proposed mechanism allows the navigator to optimally restore its confidence when its expectations are violated. We also perform navigational experiments with real ants to compare with the results of our model. The behavior of the proposed autonomous navigator closely resembles real ant navigational behavior. Moreover, our model explains navigation in dynamic environments as a memory consolidation process, harnessing expectations and their violations.",https://ieeexplore.ieee.org/document/5509935/,2010 IEEE International Conference on Robotics and Automation,3-7 May 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.1992.201634,An intelligent mobile robot golfing system using binocular stereo vision,IEEE,Conferences,"This paper describes a robot vision golfing system. The ARNIE P/sup tau / (Automated Robotic Navigational unit with Intelligent Eye and Putter) project was initiated to investigate the problems and develop software solutions for robotic tasks that require good hand-eye coordination and an intelligent feedback mechanism. This system has only one frame buffer and no specialized hardware, so quasi-real-time 3D tracking is accomplished in software using the unix spline facility. Golf is a difficult perceptory task which requires the integration of many complicated computational tasks. It is therefore a good platform to experiment with artificial intelligence techniques and robotics.<>",https://ieeexplore.ieee.org/document/201634/,[1992] Proceedings. 11th IAPR International Conference on Pattern Recognition,30 Aug.-3 Sept. 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2004.1398386,Ant colony optimization based swarms: implementation for the mine detection application,IEEE,Conferences,"Mine detection is a sensitive task confronting the battlefield strategists. There is an ever-increasing demand for proper and sophisticated resources for many issues involved in the task. Traditional practices still involve human force directly in executing the tasks in spite of the advances in technology for tools and implements for the operation [GAO, 2001]. The problem includes various facets inherently: two of the prominent issues are location of mines over a minefield and secondly removal of the mines once located [GAO, 2001]. These two issues are not totally independent as technology used for one can directly or indirectly affect the other. Developments in artificial intelligence, natural heuristics, computational optimization and robotics have endowed us with the ability to realize unmanned robots (or robot like vehicles) that work intelligently on a real time basis in attempting at the problem of mine detection. In this paper we focus on the algorithms developed using ant colony optimization based approaches to the mine detection application and its implementation on a real-time basis. We focus on certain optimization techniques that could be used for effective realization of the algorithm. Generic groundscout robots had been already built at the MABL, RIT [Sahin F. et al., 2003]. These robots have been used to demonstrate the implementation",https://ieeexplore.ieee.org/document/1398386/,"2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)",10-13 Oct. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASA54658.2022.9765294,"Application of AI, IOT and ML for Business Transformation of The Automotive Sector",IEEE,Conferences,"Automotive industry is essential in human lives. It is not possible to imagine a day without driving or some public transport. Today, digital technologies are making motor vehicles and the industry more intelligent. The entire value chain of automotive business is transforming. A better connect with customers is needed. All this is possible through advanced digital technologies. Automotive companies are overhauling business processes and relationships. Legacy IT systems for manufacturing, engineering, supply chain etc. are being reinvented. This transformation encompasses software, robotics, connected devices, and artificial intelligence. Artificial intelligence (AI) made the dream of self-driving cars possible. AI will soon transform every device. Tesla, Google Waymo, and Nvidia are examples of machine learning algorithms used to detect how far different objects are, from the car. Augmented reality (AR) and virtual reality (VR) analysis enables users to watch blind spots. AI enhances security by simultaneous coordination with many sensors. With AR, VR and mixed reality (MR), automotive companies have a personalized retail platform and a competitive edge. This paper studies AI applications in the automotive sector. It studies the recent developments, and applications of AI. It discusses how companies use AI for cost reduction, market strategies, sales promotion, and even funding.",https://ieeexplore.ieee.org/document/9765294/,2022 International Conference on Decision Aid Sciences and Applications (DASA),23-25 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1996.549172,Applying self-organizing networks to recognizing rooms with behavior sequences of a mobile robot,IEEE,Conferences,"We describe the application of a self-organizing network to the robot which learns to recognize rooms (enclosures) using behavior sequences. In robotics research, most studies on recognizing environments have tried to build the precise geometric map with highly sensitive sensors. However many natural agents like animals recognize the environments with low sensitivity sensors, and a geometric map may not be necessary. Thus we attempt to build a mobile robot using a self-organizing network to recognize the enclosures, in which it acts, with low sensitivity and local sensors. The mobile robot is behavior-based and does wall-following in an enclosure. Then the sequences of behaviors executed in each enclosure are obtained. The sequences are transformed into real-value vectors, and inputted to the Kohonen self-organizing network. Unsupervised learning is done and a mobile robot becomes able to distinguish and identify enclosures. We fully implemented the system using a real mobile robot and made experiments for evaluating the ability. Consequently we found out the recognition of enclosures was done well and our method was robust against small obstacles in an enclosure.",https://ieeexplore.ieee.org/document/549172/,Proceedings of International Conference on Neural Networks (ICNN'96),3-6 June 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2018.8500457,Artificial Intelligence Course Design: iSTREAM-based Visual Cognitive Smart Vehicles,IEEE,Conferences,"New intelligent era calls for new learners and thus urgently needs a series of artificial intelligence. As a good educational platform for teaching artificial intelligence, smart cars have aroused concern and practices of all parties. However, at present, most courses and training pay more attention to basic knowledge and technology of smart cars, seldom to training based on artificial intelligence curriculum system and comprehensive competency integrating science, technology, art and management. Therefore, based on concept of iSTREAM (intelligence for Science, Technology, Robotics, Engineering, Art, and Management) and Raspberry intelligent vehicle teaching platform, this paper introduced a smart car-themed artificial intelligence courses including basic courses, specialized courses, specialized technical courses and elective courses. This course can guide learners to develop smart cars based on visual cognition, in-depth learning, VR and 3D printing integrated artistic creativity. It combines disciplines such as science, technology, art, games and management to upgrade a single knowledge and technology course into a comprehensive competency course that integrates knowledge, skills, emotion and management. Practice in Beijing NO.13 and NO.101 High School shows that this course allows students to experience scientific research process, learn artificial intelligence related knowledge and skills, understand scientific way of thinking and scientific research methods, stimulate learners' responsibility and scientific passion, and cultivate leadership skills through self-learning and partly project management.",https://ieeexplore.ieee.org/document/8500457/,2018 IEEE Intelligent Vehicles Symposium (IV),26-30 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIBCON50419.2021.9438884,Assessment of Map Construction in vSLAM,IEEE,Conferences,"Vision-based Simultaneous Localization and Mapping (vSLAM) is a challenging task in modern computer vision. vSLAM is particularly important as mobile robotics application. It allows to localize the robot and build the map of unknown environment in 3D in real-time. During research and development of new methods, it needs extensive evaluation on trajectory and map quality compared to known methods. In this work we focus on map quality estimation. We develop the simulated ground-truth data in photo-realistic environment and introduce new metrics in order to estimate map quality. We evaluate neural network based vSLAM methods with our framework in order to show that it fits map quality estimation more than standard approaches. Open-source implementation of our map metrics is available at https://github.com/CnnDepth/slam_comparison.",https://ieeexplore.ieee.org/document/9438884/,2021 International Siberian Conference on Control and Communications (SIBCON),13-15 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ic-ETITE47903.2020.203,Automated Detection Of Driving Pathway Using Image Processing,IEEE,Conferences,"Image data is one of the most popular real world input data that can be used for variety of applications ranging from robotics and computer vision to security systems. In combination with other methods such as neural network, Artificial neural network and image processing techniques, manipulation of image data can lead to applications such as detection of objects, tracking, identification and vision based robotics and so on. Advanced Driver Assistance System (ADAS) also use image for camera based driver assistance systems.The report covers a hardware model system that tests the software work of detection of traffic signs and path for it own ADAS systems. Different problems were tackled, including the choice of OS, and additional hardware components needed to tackle. The choice of programming languages, equipment, OS and methods were based on simplicity and practicality. Artificial neural network in combination with Open CV libraries were used for stop sign, traffic light and path road detection. The hardware model consisted of RC Car attached to raspberry pi board with a mounted pi camera for video streaming and an arduino controller attached to a radio transmitter for controlling through Open CV running in windows PC.",https://ieeexplore.ieee.org/document/9077722/,2020 International Conference on Emerging Trends in Information Technology and Engineering (ic-ETITE),24-25 Feb. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2018.00655,Automated Training Plan Generation for Athletes,IEEE,Conferences,"In sports, athletes need detailed and individualised training plans for maintaining and improving their skills in order to achieve their best performance in competitions. This presents a considerable workload for coaches, who besides setting objectives have to formulate extremely detailed training plans. Automated Planning, which has already been successfully deployed in many real-world applications such as space exploration, robotics, and manufacturing processes, embodies a useful mechanism that can be exploited for generating training plans for athletes. In this paper, we propose the use of Automated Planning techniques for generating individual training plans, which consist of exercises the athlete has to perform during training, given the athlete's current performance, period of time, and target performance that should be achieved. Our experimental analysis, which considers general training of kickboxers, shows that apart of considerable less planning time, training plans automatically generated by the proposed approach are more detailed and individualised than plans prepared manually by an expert coach.",https://ieeexplore.ieee.org/document/8616652/,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",7-10 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACTA54488.2022.9753620,Automatic Traffic Sign Detection System With Voice Assistant,IEEE,Conferences,"In the arena of artificial intelligence, the world is revolutionizing with many technological applications being incorporated with Artificial Intelligence due to improved efficiency and performance. AI has penetrated drastically, delving deep into locker room decisions in many fields like agriculture, healthcare, military, manufacturing, robotics, transportation and so on. AI does a lot more than improving our lives, in most cases, it saves our lives too. Autonomous vehicles, the so-called self-driving cars, are one of the greatest applications of AI and are very instrumental in making the machine work autonomously by observing and interpreting the real-life scenario of the environment. This paper deals with the deployment of an Automatic Traffic sign detection System with voice assistant, which is one of the applications of autonomous vehicles, which can tone down the driver from puzzling traffic conditions significantly increasing driving safety and comfort. This will require an appropriate database and algorithm for improved accuracy in performance. This paper, therefore, compares the features, accuracy, and efficiency of various deep learning algorithms and comes up with a varied model thus saving computational resources.",https://ieeexplore.ieee.org/document/9753620/,2022 International Conference on Advanced Computing Technologies and Applications (ICACTA),4-5 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAC.2004.1301379,Autonomic systems for mobile robots,IEEE,Conferences,"Mobile robots are an excellent testbed for autonomic computing research. The ultimate goal of robotics research is to develop a platform that can function autonomously in the face of hardware and software failures. This goal is becoming more important as robots are increasingly being deployed outside of controlled environments. In this paper, we discuss our work toward implementing an autonomic system for a mobile robot. This work is motivated by our experiences with existing mobile robot control software during real-world deployments.",https://ieeexplore.ieee.org/document/1301379/,"International Conference on Autonomic Computing, 2004. Proceedings.",17-18 May 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAMIMIA47173.2019.9223365,Autonomous Car Simulation Using Evolutionary Neural Network Algorithm,IEEE,Conferences,"Automation with artificial intelligence (AI) has widely implemented in robotics, transportation and manufacture. AI has become a powerful technology that change human life and help human more flexible doing something. In this paper, it will show a result of simulation from an autonomous car using the evolutionary neural network algorithm which combines genetic algorithm and neural network. The purpose of the simulation is to test the model that we develop to know the right direction based on the track, so the evolutionary neural network that implemented to the autonomous car be able to deliver the best solution before it implements in the real machine or car technology. Genetic algorithm combines with a neural network to reach an evolution condition. The evolution process is achieved through crossover, mutation and selection process, so the algorithm will give the best result from the iteration of the experiment. The result of our experiment shows that evolutionary neural network algorithm give the best result within 3 layer architecture, with iteration average is 14.5 reach finish point (check point) 3 in the track simulation. Based on the simulation, our car model can find out the right direction.",https://ieeexplore.ieee.org/document/9223365/,"2019 International Conference on Advanced Mechatronics, Intelligent Manufacture and Industrial Automation (ICAMIMIA)",9-10 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8202143,Autonomous skill-centric testing using deep learning,IEEE,Conferences,Software testing is an important tool to ensure software quality. This is a hard task in robotics due to dynamic environments and the expensive development and time-consuming execution of test cases. Most testing approaches use model-based and/or simulation-based testing to overcome these problems. We propose model-free skill-centric testing in which a robot autonomously executes skills in the real world and compares it to previous experiences. The skills are selected by maximising the expected information gain on the distribution of erroneous software functions. We use deep learning to model the sensor data observed during previous successful skill executions and to detect irregularities. Sensor data is connected to function call profiles such that certain misbehaviour can be related to specific functions. We evaluate our approach in simulation and in experiments with a KUKA LWR 4+ robot by purposefully introducing bugs to the software. We demonstrate that these bugs can be detected with high accuracy and without the need for the implementation of specific tests or task-specific models.,https://ieeexplore.ieee.org/document/8202143/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR53236.2021.9659396,Autonomy in robotic prostate biopsy through AI-assisted fusion,IEEE,Conferences,"Artificial intelligence (AI) and especially deep learning (DL) are applied in many fields and are the leading technologies towards robot autonomy. Proper DL algorithms and sufficient amount of medical data may be applied in medical robotics to increase autonomy, safety, accuracy and precision of a medical procedure. We investigate in this paper the implementation and the integration of DL techniques in a new robotic system for prostate biopsy-PROST. The DL algorithm, named PROST-Net, is a convolutional neural network (CNN) employed for the segmentation of the prostate in different types of medical images: pre-operative magnetic resonance (MRI) and intra-operative ultrasound (US). The US images come from different acquisition planes (axial and sagittal) with different alignments of the sensors (convex and linear) making the design of the CNN challenging. Tests on patient data produced an accuracy of 86% in US images and 77% in MRI and were estimated by using Dice similarity Coefficient (DC). The biopsy robot will use the output of PROST-Net for the initial fusion of pre-operative and intra-operative images to define the biopsy targets and the planning of the procedure. Real-time processing of the data with PROST-Net will empower dynamic update of the initial fusion by following the current position of the prostate with respect to the robotic reference frame.",https://ieeexplore.ieee.org/document/9659396/,2021 20th International Conference on Advanced Robotics (ICAR),6-10 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigComp48618.2020.00-21,Benchmarking Jetson Platform for 3D Point-Cloud and Hyper-Spectral Image Classification,IEEE,Conferences,"Modern innovations of embedded system platforms (hardware accelerations) play a vital role in revolutionizing deep learning into practical scenarios, transforming human efforts into an automated intelligent system such as autonomous driving, robotics, IoT (Internet-of-Things) and many other useful applications. NVIDIA Jetson platform provides promising performance in terms of energy efficiency, favorable accuracy, and throughput for running deep learning algorithms. In this paper, we present benchmarking of Jetson platforms (Nano, TX1, and Xavier) by evaluating its performance based on computationally expensive deep learning algorithms. Previously, most of the benchmark results were based on 2-D images with conventional deep learning models for image processing. However, the implementation of many other complex data types at Jetson platform has remained a challenge. We also showed the practical impact of optimizing the algorithm vs improving the hardware accelerations by deploying a diverse range of dense and intensive deep learning architectures at all three aforementioned Jetson platforms, to make a better comparison of performance. In this regard, we have used two entirely different data-types, namely (i) ModelNet-40(Princeton-3D point-cloud) data-set along with PointNet deep learning architecture for classification of 3D point-cloud, and (ii) hyperspectral images (HSI) datasets (KSC and Pavia) alongside stacked autoencoders(SAE) to classify HSI correspondingly. This will broaden the scope of edge-devices to handle 3-D and HSI data whilst real-time classification will be processed at edge-server under the umbrella of edge-computing. The selection of (i) was made to exploit GPU heavily as the code uses TensorFlowgpu whereas (ii) was chosen to challenge the CPU cores of each platform as the code is based on Theano and may suffer from under-utilizing the GPU cores. We have presented the detailed evaluation exclusively in term of performance indices as inference time, the maximum number of concurrent processes, resource utilization per process and efficiency",https://ieeexplore.ieee.org/document/9070378/,2020 IEEE International Conference on Big Data and Smart Computing (BigComp),19-22 Feb. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967694,Benchmarking and Workload Analysis of Robot Dynamics Algorithms,IEEE,Conferences,"Rigid body dynamics calculations are needed for many tasks in robotics, including online control. While there currently exist several competing software implementations that are sufficient for use in traditional control approaches, emerging sophisticated motion control techniques such as nonlinear model predictive control demand orders of magnitude more frequent dynamics calculations. Current software solutions are not fast enough to meet that demand for complex robots. The goal of this work is to examine the performance of current dynamics software libraries in detail. In this paper, we (i) survey current state-of-the-art software implementations of the key rigid body dynamics algorithms (RBDL, Pinocchio, Rigid-BodyDynamics.jl, and RobCoGen), (ii) establish a methodology for benchmarking these algorithms, and (iii) characterize their performance through real measurements taken on a modern hardware platform. With this analysis, we aim to provide direction for future improvements that will need to be made to enable emerging techniques for real-time robot motion control. To this end, we are also releasing our suite of benchmarks to enable others to help contribute to this important task.",https://ieeexplore.ieee.org/document/8967694/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2005.1545040,Broker: an interprocess communication solution for multi-robot systems,IEEE,Conferences,"We describe in this paper a novel implementation of the interprocess communication (IPC) technology, called Broker, in support of the development and the operation of a complex robot system. We view each robot system as a collection of processes that need to exchange information, e.g. motion commands and sensory data, in a flexible and convenient fashion, without affecting each other's operations in case of a process's scheduled termination or unexpected failure. We argue that the IPC technology provides an ideal framework for this purpose, and we carefully make our design decisions about its implementation based on the needs of robotics applications. Broker is programming language, operating system, and hardware platform independent and has served us well in a RoboCup project and collective robotics experiments, in both simulation and real-world environments.",https://ieeexplore.ieee.org/document/1545040/,2005 IEEE/RSJ International Conference on Intelligent Robots and Systems,2-6 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDA.2010.5687045,Bézier curve based dynamic obstacle avoidance and trajectory learning for autonomous mobile robots,IEEE,Conferences,"This paper addresses the problem of avoiding dynamic obstacles while following the learned trajectory through non-point based maps directly through laser data. The geometric representation of free configuration area changes while a moving obstacle enters into the safety region of autonomous mobile robot. We have applied the Bézier curve properties to the free configuration eigenspaces to satisfy the dynamic obstacle avoidance path constraints. The algorithm is designed to accurately represent the mobile robot's characteristics while avoiding obstacle such as minimum turning radius. Moreover, we also discuss the obstacle avoided path feasibility as a vectorial combination of free configuration eigen-vectors at discrete time scan-frames to manifest a trajectory, which once followed and mapped onto the two control signals of mobile robot will enable it to build an efficient and accurate online environment map. Preliminary results in Matlab have been shown to validate the idea, while the same has been implemented in Player/stage (robotics real-time software) to analyze the performance of the proposed system.",https://ieeexplore.ieee.org/document/5687045/,2010 10th International Conference on Intelligent Systems Design and Applications,29 Nov.-1 Dec. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-Berlin.2018.8576251,CNN Inference: Dynamic and Predictive Quantization,IEEE,Conferences,"Deep Learning techniques like Convolutional Neural Networks (CNN) are the de-facto method for image classification with broad usage spanning across automotive, industrial, medicine, robotics etc. Efficient implementation of CNN inference on embedded device requires a quantization method, which minimizes the accuracy loss, ability to generalize across deployment scenarios as well as real-time processing. Existing literature doesn't address all these three requirements simultaneously. In this paper, we propose a novel quantization algorithm to overcome above mentioned challenges. The proposed solution dynamically selects the scale for quantizing activations and uses Kalman filter to predict quantization scale to reduce accuracy loss. The proposed solution exploits the range statistics from previous inference processes to estimate quantization scale, enabling real-time solution. The proposed solution is implemented on TI's TDA family of embedded automotive processors. The proposed solution is running real time semantic segmentation on TDA2x processor within 0.1% accuracy loss compared floating point algorithm. The solution performs well across multiple deployment scenarios (e.g. rain, snow, night etc) demonstrating generalization capability of the solution.",https://ieeexplore.ieee.org/document/8576251/,2018 IEEE 8th International Conference on Consumer Electronics - Berlin (ICCE-Berlin),2-5 Sept. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC.1989.4790240,CONDOR: A Coarse-grained Parallel Architecture for Robot Control,IEEE,Conferences,"This paper presents an overview of the CONDOR, a real-time development environment designed for robotics research applications. The architecture is based on standard hardware components consisting of upto eight microprocessors interconnected through a shared memory bus, and is coupled with a powerful software environment based on message passing that enables the development of control programs for complicated robots. The hardware is extremely easy to set up since it uses standard components. Besides program libraries tuned for real-time control, the software utilities include a multi-processor pseudo-terminal emulator, a file-server and a flexible symbolic debugger that greatly enhance programmer productivity.",https://ieeexplore.ieee.org/document/4790240/,1989 American Control Conference,21-23 June 1989,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCME52200.2021.9591113,Cobots for FinTech,IEEE,Conferences,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested & validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space.",https://ieeexplore.ieee.org/document/9591113/,"2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",7-8 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MSM49833.2020.9202398,Collaborative Robot System for Playing Chess,IEEE,Conferences,"In recent years, number of collaborative robots industrial applications has made a significant increasment. Implementation of collaborative robots is a safe and effective way for designing robot-human cooperation systems. Combined with constantly developing artificial intelligence, collaborative systems are actually able to solve complex problems that require some sort of intelligence. For humans, board games are a good example of the visualization of robot intelligence. Such systems require estimation and detection of board and pieces in manipulator workspace, some kind of decision-making algorithms and robot control system to move pieces. The flagship of such systems are chess playing robots. The chess game has a defined and easy to understand set of rules which makes it interesting example of intelligent robotics systems application. In this paper, we present an implementation of collaborative robots for chess playing system which was designed to play against human or another robot. The system is able to track state of the game via camera, calculate the optimal move using implemented decision-making algorithm, detect illegal moves and execute pick-and-place task to physically move pieces. We test the developed system in a real-world setup and provide experimental results documenting the performance of proposed approach.",https://ieeexplore.ieee.org/document/9202398/,2020 International Conference Mechatronic Systems and Materials (MSM),1-3 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISM52913.2021.00039,Combining Linked Open Data and Multimedia Knowledge Base for Digital Cultural Heritage Robotic Applications,IEEE,Conferences,"The current trends in society evolution are showing rapid changes in our habitual environments and consequently affecting human interactions with them. Among the key factors of this process we can certainly cite the growth of BigData favored by the knowledge digitalization, the dissemination of sensors in environments and the advancements of connectivity capabilities. At the same time, the progress in artificial intelligence and cognitive robotics has lead to the production of sophisticated humanoid robots, which are progressively spreading to a wide public. In this way, research efforts are needed for a proper knowledge management and knowledge acquisition by machines, in order to have more natural and friendly human-robot interactions in daily tasks, usually performed by human beings. In this paper we show an approach related to a human-robot interaction in cultural heritage context, simulating a digital ecosystem where a robot plays the role of a guide for tourists and it is able to proactively interact with its interlocutors by combining both semantic and visual information. The proposed approach, its implementation and experimental results on a real robotic platform are shown and discussed.",https://ieeexplore.ieee.org/document/9666077/,2021 IEEE International Symposium on Multimedia (ISM),29 Nov.-1 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVCNZ51579.2020.9290542,Comparison of Face Detection Algorithms on Mobile Devices,IEEE,Conferences,"Face detection is a fundamental task for many computer vision applications such as access control, security, advertisement, automatic payment, and healthcare. Due to technological advances mobile robots are becoming increasingly common in such applications (e.g. healthcare and security robots) and consequently there is a need for efficient and effective face detection methods on such platforms. Mobile robots have different hardware configurations and operating conditions from desktop applications, e.g. unreliable network connections and the need for lower power consumption. Hence results for face detection methods on desktop platforms cannot be directly translated to mobile platforms.We compare four common face detection algorithms, Viola-Jones, HOG, MTCNN and MobileNet-SSD, for use in mobile robotics using different face data bases. Our results show that for a typical mobile configuration (Nvidia Jetson TX2) Mobile-NetSSD performed best with 90% detection accuracy for the AFW data set and a frame rate of almost 10 fps with GPU acceleration. MTCNN had the highest precision and was superior for more difficult face data sets, but did not achieve real-time performance with the given implementation and hardware configuration.",https://ieeexplore.ieee.org/document/9290542/,2020 35th International Conference on Image and Vision Computing New Zealand (IVCNZ),25-27 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MI-STA52233.2021.9464484,Comparison of PID and Artificial Neural Network Controller in on line of Real Time Industrial Temperature Process Control System,IEEE,Conferences,"Due to its simple structure and robustness, the traditional proportional-integral-derivative (PID) controller is commonly used in the field of industrial automation and process control, but it does not function well with nonlinear systems, time-delayed linear systems and time-varying systems. A new type of PID controller based on artificial neural networks and evolutionary algorithms is presented in this paper. An powerful instrument for a highly nonlinear system is the Artificial Neural Network. The interest in the study of the nonlinear system has increased through the implementation of a high-speed computer system,. In complex systems such as robotics and process control systems, the Neuro Control Algorithm is often applied. Systems of process management is also nonlinear and hard to control consistently.. This paper presents a comprehensive analysis in Which is offline trained by a multilayered feed forward back propagation neural network to act as a process control system controller, That is to say, a temperature control device without prior knowledge of its dynamics. Via the implementation of a range of input vectors to the neural network, the inverse dynamics model is developed. Based on these input vectors, the output of the neural network It is being studied by explicitly configuring it to monitor the operation. In this paper, based on set-point adjustment, impact of disturbances in load and variable dead time, compassion between the PID controller and ANN is conducted. The outcome shows that ANN outperforms the controller of the PID.",https://ieeexplore.ieee.org/document/9464484/,2021 IEEE 1st International Maghreb Meeting of the Conference on Sciences and Techniques of Automatic Control and Computer Engineering MI-STA,25-27 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2002.1189341,Computer based robot training in a virtual environment,IEEE,Conferences,"As more market segments are welcoming automation, the robotic field continues to expand. With the accepted breadth of viable industrial robotic applications increasing, the need for flexible robotic training also grows. In the area of simulation and offline programming there have been innovative developments to Computer Aided Robotics (CAR) Systems. New and notable releases have been introduced to the public, especially among the small, affordable, and easy to use systems. These CAR-Systems are mainly aimed at system integrators in general industry business fields to whom the complex, powerful software tools used by the automotive industry (and its suppliers) are oversized. In general, CAR-Systems are used to design robot cells and to create the offline programs necessary to reduce start-up time and to achieve a considerable degree of planning reliability. Another potential yet to be fully considered, is the use of such CAR-Systems as an inexpensive and user-friendly tool for robotics training. This paper will show the educational potential and possibility inherent in simulation and introduce a successful example of this new method of training. Finally, this presentation should be seen as an attempt to outline novel methods for future education in an industrial environment characterized by the increased occurrence and implementation of the virtual factory.",https://ieeexplore.ieee.org/document/1189341/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2000.870793,Cultural algorithms: concepts and experiments,IEEE,Conferences,"Evolutionary computation is a generic name given to the resolution of computational problems that are planned and implemented based on models of the evolutionary process. Most of the evolutionary algorithms that have been proposed follow biological paradigms and the concepts of natural selection, mutation and reproduction. There are, however, other paradigms which may be adopted in the creation of evolutionary algorithms. Several problems involving unstructured environments may be addressed from the point of view of cultural paradigms, which offer plenty of categories of models where one does not know all possible solutions to a problem - a very common situation in real life. This work applies the computational properties of cultural technology to the solution of a specific problem, adapted from the robotics literature. A test environment denoted the ""Cultural Algorithms Simulator"" was developed to allow anyone to learn more about the rather unconventional characteristics of a cultural technology.",https://ieeexplore.ieee.org/document/870793/,Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512),16-19 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECS46596.2019.8964645,Data-Driven Video Grasping Classification for Low-Power Embedded System,IEEE,Conferences,"Video-based hand grasp analysis can support both robotics and prosthetics. Indeed, computational aspects represent a major issue, as hand grasp analysis is expected to support grasping systems that are hosted on low-power embedded systems. This paper proposes a framework for video-based grasping classification that is designed for implementation on resource-constrained devices. The framework adopts a fully data-driven strategy and relies on deep learning to deal with advanced analysis of video signals. Nonetheless, the overall design takes advantage of CNN architectures that can cope with the constraints imposed by embedded systems. The experimental session involved a real-world dataset containing daily life activities collected using egocentric perspective. In addition, the complete inference system is implemented on a NVIDIA Jetson-TX2 obtaining real time performances. The results confirm that the proposed system can suitably balance the trade off between accuracy and computational costs.",https://ieeexplore.ieee.org/document/8964645/,"2019 26th IEEE International Conference on Electronics, Circuits and Systems (ICECS)",27-29 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.2018.8396547,Data-driven quality prognostics for automated riveting processes,IEEE,Conferences,"Technologies based in robotics and automatics are reshaping the aerospace industry. Aircraft manufacturers and top-tier suppliers now rely on robotics to perform most of its operational tasks. Over the years, a succession of implemented mobile robots has been developed with the mission of automating important industrial processes such as welding, material handling or assembly procedures. However, despite the progress achieved, a major limitation is that the process still requires human supervision and an extensive quality control process. An approach to address this limitation is to integrate machine learning methods within the quality control process. The idea is to develop algorithms that can direct manufacturing experts towards critical areas requiring human supervision and quality control. In this paper we present an application of machine learning to a concrete industrial problem involving the quality control of a riveting machine. The proposal consists of an intelligent predictive model that can be integrated within the existing real time sensing and pre-processing sub-systems at the equipment level. The framework makes use of several data-driven techniques for pre-processing and feature engineering, combined with the most accurate algorithms, validated through k-folds cross validation technique which also estimates prediction errors. The model is able to classify the manufacturing process of the machine as nominal or anomalous according to a real-world data set of design requirements and operational data. Several machine learning algorithms are compared such as linear regression, nearest neighbor, support vector machines, decision trees, random forests and extreme gradient boost. Results obtained from the case study suggest that the proposed model produces accurate predictions which meet industrial standards.",https://ieeexplore.ieee.org/document/8396547/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MSM49833.2020.9201666,Deep Learning-based Algorithm for Mobile Robot Control in Textureless Environment,IEEE,Conferences,"For the implementation of stereo image-based visual servoing algorithm in the eye-in-hand robotics applications, one of the main concerns is the accurate point feature detection and matching algorithm. Since the visual servoing is carried out in the textureless environment, the feature detection process is even more challenging. To fulfill the requirement of a robust and reliable point feature detection process, in this paper we present the novel deep learning-based algorithm. The approach based on convolutional neural networks and algorithm for detection of manufacturing entities is proposed and detected regions of interest are utilized for the improvement of the point feature detection algorithm. The proposed algorithm is experimentally evaluated in real-world settings by using wheeled nonholonomic mobile robot RAICO equipped with stereo vision system. The experimental results show the improvement of 58% in the accuracy of matched point features in the images obtained during the visual servoing process. Moreover, with the implementation of the proposed deep learning-based approach, the number of successful experimental runs has increased by 80%.",https://ieeexplore.ieee.org/document/9201666/,2020 International Conference Mechatronic Systems and Materials (MSM),1-3 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561145,Deep Reinforcement Learning Framework for Underwater Locomotion of Soft Robot,IEEE,Conferences,"Soft robotics is an emerging technology with excellent application prospects. However, due to the inherent compliance of the materials used to build soft robots, it is extremely complicated to control soft robots accurately. In this paper, we introduce a data-based control framework for solving the soft robot underwater locomotion problem using deep reinforcement learning (DRL). We first built a soft robot that can swim based on the dielectric elastomer actuator (DEA). We then modeled it in a simulation for the purpose of training the neural network and tested the performance of the control framework through real experiments on the robot. The framework includes the following: a simulation method for the soft robot that can be used to collect data for training the neural network, the neural network controller of the swimming robot trained in the simulation environment, and the computer vision method to collect the observation space from the real robot using a camera. We confirmed the effectiveness of the learning method for the soft swimming robot in the simulation environment by allowing the robot to learn how to move from a random initial state to a specific direction. After obtaining the trained neural network through the simulation, we deployed it on the real robot and tested the performance of the control framework. The soft robot successfully achieved the goal of moving in a straight line in disturbed water. The experimental results suggest the potential of using deep reinforcement learning to improve the locomotion ability of mobile soft robots.",https://ieeexplore.ieee.org/document/9561145/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV48863.2021.9575616,Deep Reinforcement Learning based control algorithms: Training and validation using the ROS Framework in CARLA Simulator for Self-Driving applications,IEEE,Conferences,"This paper presents a Deep Reinforcement Learning (DRL) framework adapted and trained for Autonomous Vehicles (AVs) purposes. To do that, we propose a novel software architecture for training and validating DRL based control algorithms that exploits the concepts of standard communication in robotics using the Robot Operating System (ROS), the Docker approach to provide the system with portability, isolation and flexibility, and CARLA (CAR Learning to Act) as our hyper-realistic open-source simulation platform. First, the algorithm is introduced in the context of Self-Driving and DRL tasks. Second, we highlight the steps to merge the proposed algorithm with ROS, Docker and the CARLA simulator, as well as how the training stage is carried out to generate our own model, specifically designed for the AV paradigm. Finally, regarding our proposed validation architecture, the paper compares the trained model with other state-of-the-art traditional control approaches, demonstrating the full strength of our DL based control algorithm, as a preliminary stage before implementing it in our real-world autonomous electric car.",https://ieeexplore.ieee.org/document/9575616/,2021 IEEE Intelligent Vehicles Symposium (IV),11-17 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SoftCOM50211.2020.9238313,Deep Semantic Image Segmentation for UAV-UGV Cooperative Path Planning: A Car Park Use Case,IEEE,Conferences,"Navigation of Unmanned Ground Vehicles (UGV) in unknown environments is an active area of research for mobile robotics. A main hindering factor for UGV navigation is the limited range of the on-board sensors that process only restricted areas of the environment at a time. In addition, most existing approaches process sensor information under the assumption of a static environment. This restrains the exploration capability of the UGV especially in time-critical applications such as search and rescue. The cooperation with an Unmanned Aerial Vehicle (UAV) can provide the UGV with an extended perspective of the environment which enables a better-suited path planning solution that can be adjusted on demand. In this work, we propose a UAV-UGV cooperative path planning approach for dynamic environments by performing semantic segmentation on images acquired from the UAV’s view via a deep neural network. The approach is evaluated in a car park scenario, with the goal of providing a path plan to an empty parking space for a ground-based vehicle. The experiments were performed on a created dataset of real-world car park images located in Croatia and Germany, in addition to images from a simulated environment. The segmentation results demonstrate the viability of the proposed approach in producing maps of the dynamic environment on demand and accordingly generating path plans for ground-based vehicles.",https://ieeexplore.ieee.org/document/9238313/,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",17-19 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSP.2017.8286790,Design framework for general purpose object recognition on a robotic platform,IEEE,Conferences,"The advancement in the broader field of Computer Vision is consequential, through past few decades. Therefore, a considerable improvement in object detection and tagging using convolutional neural networks has given way to accurate yet complex methods, which can identify objects in real-time. However, the growth in the area of implementing the algorithms on low powered portable devices has been relatively slow. This paper aims to converge the fields of computer vision and robotics, focusing on implementation of image description applications on an embedded system platform. We aim to integrate Neural Network powered object recognition system ‘YOLO v2’ with a robotic platform to explore the potential applications in the advancing domain of service and personal robotics.",https://ieeexplore.ieee.org/document/8286790/,2017 International Conference on Communication and Signal Processing (ICCSP),6-8 April 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.2000.876849,Design of a dedicated CNN chip for autonomous robot navigation,IEEE,Conferences,"Obstacle avoidance is the main issue in autonomous robotics. It requires a three-dimensional effective environment sensing in real time. Among the others, the stereo vision approach to environmental information extraction seems to be very appealing, even if it leads an extremely high computational cost. However, a high performance implementation of this algorithm on a cellular neural network is able to overcome these difficulties. In the paper, the design of a CNN chip well suited for this algorithm is presented. This chip, performing a real time processing of the stereo vision data, will improve the cruising speed of a robotic platform.",https://ieeexplore.ieee.org/document/876849/,Proceedings of the 2000 6th IEEE International Workshop on Cellular Neural Networks and their Applications (CNNA 2000) (Cat. No.00TH8509),25-25 May 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON.2008.4494306,Design of an integrated environment for operation and control of robotic arms (non-reviewed),IEEE,Conferences,"As more advanced control algorithms are becoming available for the control of robotic arms, traditional fixed controller boards and associated code generators are becoming less convenient way to test such control algorithms in real-time. The process of using such boards is complex, time consuming, and inflexible. In this work, an integrated hardware-software environment was developed and presented where researchers can simply use any Matlab/Simulink basic function block and/or toolbox, such as fuzzy logic or neural network, to design, implement, and test different controller algorithms in real-time for robotic arm operations. The hardware includes a computer, the dSPACE-ds1103 digital processing board, an amplifier board, and the Zebra-ZERO robotics arm as a test-bed. Also, Matlab GUI, m-file, Matlab/Simulink blocks, and dSPACE interface functions are combined together to form the software environment. Control algorithms can be designed in the Matlab/Simulink then converted to c-code and download to the dSPACE processing board. The Matlab m-file are used to code the arm inverse kinematics model and the path planning to calculate the joint angles then send them to the dSPACE processing board using the dSPACE interface functions. Finally, the dSPACE processing board generates physical signal to control the robot arm in real-time. The proposed hardware-software components are developed and integrated together, and several control algorithms can be tested on it. The development steps and some of the real-time testing results conducted on the hardware are explained next in this extended abstract.",https://ieeexplore.ieee.org/document/4494306/,IEEE SoutheastCon 2008,3-6 April 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONIELECOMP.2017.7891823,Detecting falling people by autonomous service robots: A ROS module integration approach,IEEE,Conferences,"In this paper is presented the integration of diverse modules for people fallen detection by a mobile service robot. This integration has been achieved in the middleware ROS (Robotics Operation System). The proposed implementation are arranged over an modular architecture of three layers: Hardware, Processing and Decision. The modules implemented are on the processing layer. The first module uses an RGB-D camera to detect and track a person in the environment. This module calculate features to detect the fallen pose. In the second module, a PID controller in a pan/tilt unit is used, in order to track the person with a minimum error and soft movement. For this purpose the centroid of the person is located at the center of the plane image. The main characteristics in our architecture are: 1) Segmentation in depth is used, because 3D information is required for detecting the fallen pose; 2) The parameters of PID control are tuned using a manual method and a genetic algorithm, to compare and improve the performance of the tracking person module. Once the PID controller was optimized, the architecture to follow the person and detect the fallen pose, is probed in real time.",https://ieeexplore.ieee.org/document/7891823/,"2017 International Conference on Electronics, Communications and Computers (CONIELECOMP)",22-24 Feb. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSPN46366.2019.9150190,Developing A Framework for A Tactile Internet Enabled Robot Assisted Real-Time Interactive Medical System,IEEE,Conferences,"In this paper we outline a high-level framework and architecture for a robotic assisted real time interactive medical system for use in developing countries to help cure the acute shortage of qualified skill medical personnel in the health sector in of an internet of skills domain. We explore the application of new and innovative advancements in technological areas such as AI, 5G mobile networks, the tactile internet networks, robotics and haptic technology to aid in the digital transfer of medical expertise over a wide geographical area. We describe and propose technical specifications of such systems and review existing literature and current technologies in these areas. We interrogate the potential benefits and challenges facing the deployment of these technologies.",https://ieeexplore.ieee.org/document/9150190/,"2019 International Conference on Communications, Signal Processing and Networks (ICCSPN)",29-31 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MHS.2000.903293,Developing Khepera robot applications in a Webots environment,IEEE,Conferences,"Khepera is a high performance mini-robot. Its compact power allows an efficient experimentation using a real robot and applying the basic simulation tools. Webots is a high quality Khepera simulator used in the fields of autonomous systems, intelligent robotics, evolutionary robotics, machine learning, computer vision, and artificial intelligence. The simulation program can be transferred to the real robots easily. The aim of this article is to support the development of Khepera applications in the Webots environment. Starting from the introduction of Khepera robot and its development methodologies, the paper presents and analyses an application example of Khepera robot in the Webots environment. Finally, current applications and future research directions are presented.",https://ieeexplore.ieee.org/document/903293/,MHS2000. Proceedings of 2000 International Symposium on Micromechatronics and Human Science (Cat. No.00TH8530),22-25 Oct. 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INTERCON.2019.8853573,Development of a hand pose recognition system on an embedded computer using Artificial Intelligence,IEEE,Conferences,"The recognition of hand gestures is a very interesting research topic due to the growing demand in recent years in robotics, virtual reality, autonomous driving systems, human-machine interfaces and in other new technologies. Despite several approaches for a robust recognition system, gesture recognition based on visual perception has many advantages over devices such as sensors, or electronic gloves. This paper describes the implementation of a visual-based recognition system on a embedded computer for 10 hand poses recognition. Hand detection is achieved using a tracking algorithm and classification by a light convolutional neural network. Results show an accuracy of 94.50%, a low power consumption and a near real-time response. Thereby, the proposed system could be applied in a large range of applications, from robotics to entertainment.",https://ieeexplore.ieee.org/document/8853573/,"2019 IEEE XXVI International Conference on Electronics, Electrical Engineering and Computing (INTERCON)",12-14 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MetroAgriFor52389.2021.9628544,Digital Technologies and Automation in Livestock Production Systems: a Digital Footprint from Multisource Data,IEEE,Conferences,"In the last years, dairy production is changing fast towards smart livestock systems, driven by the rapid pace of technological advancements as Internet of Things, big data, machine learning, augmented reality and robotics. These technologies are pushing widespread collection, implementation, transmission and use of digitized information in livestock farms. Currently, this an uncontrolled process that poses questions on sustainability of the virtual environment where such processes take place. The aim of the present paper is to introduce a preliminary digitization footprint approach, which parameterizes the amount of digital information so as to quantify these data in terms of volumes invested for data storage, processing or transfer.",https://ieeexplore.ieee.org/document/9628544/,2021 IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor),3-5 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVES.2009.5400189,Digital implementation of fuzzy logic controller for wide range speed control of brushless DC motor,IEEE,Conferences,"The brushless DC motors find wide applications such as in battery operated vehicles, wheel chairs, automotive fuel pumps, robotics, machine tools, aerospace and in many industrial applications due to their superior electrical and mechanical characteristics and its capability to operate in hazardous environment. Conventional controllers fail to yield desired performance in BLDC motor control systems due to the non-linearity arising out of variation in the system parameters and change in load. The main focus is now on the application of artificial intelligent techniques such as fuzzy logic to solve this problem. Another great challenge is to reduce the size and cost of the drive system without compromising the performance. In this paper, the design and digital implementation of fuzzy logic controller using a versatile ADUC812 microcontroller, and low-cost, compact, superior performance components are used in order to reduce the cost and size of the drive system. The experimental results are presented to prove the flexibility of the control scheme in real time.",https://ieeexplore.ieee.org/document/5400189/,2009 IEEE International Conference on Vehicular Electronics and Safety (ICVES),11-12 Nov. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV50864.2020.00024,Domain Generalization via Optical Flow: Training a CNN in a Low-Quality Simulation to Detect Obstacles in the Real World,IEEE,Conferences,"Many applications in robotics and autonomous systems benefit from machine learning applied to computer vision, but often the acquisition and preparation of data for training is complex and time-consuming. Simulation can significantly reduce the effort and potential risk of data collection, thereby allowing faster prototyping. However, the ability of a data-driven system to generalize from simulated data to the real world is far from obvious and often leading to inconsistent real-world results. This paper demonstrates that some properties of optical flow can be exploited to address this generalization problem. In this work, we train a neural network to detect collisions with simulated optical flow data. Our network, FlowDroNet, is able to correctly predict up to 89 percent of the collisions of a realworld dataset and easily achieves a higher detection accuracy when compared to a network trained on a similar dataset of realworld collisions. We release our code, models and a real-world dataset for collision avoidance as open-source. We also explore the relationship between the complexity of the input information and the ability to generalize to unseen environments, and show that in some situations, optical flow is an interesting tool to bridge the reality gap.",https://ieeexplore.ieee.org/document/9108671/,2020 17th Conference on Computer and Robot Vision (CRV),13-15 May 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SCCC.2001.972633,Domain-dependent option policies in autonomous robot learning,IEEE,Conferences,"In control-related applications such as robotics, determination of optimal solutions is made very difficult for many reasons. Among these stands the difficulty in finding out an appropriate model of the domain, as defined by the control agent (robot), environment where it acts and their interaction. Reinforcement learning is a theory which defines a collection of algorithms for determination of control actions under model-free assumptions, which allows control agents to learn optimal actions in an autonomous way. In reinforcement learning, a cost functional to be optimised is determined in advance. The agent then learns how to perform this optimisation via trial and error on its environment. A trial corresponds to execution of actions chosen by the agent, and the error is the immediate result (a real-valued reinforcement) of this action. In the work reported, we consider trials by a learning robotic agent which are not based on low level actions, but instead on sequences of actions (options or macro-operators). We analysed the performance both in terms of learning speed and quality of learned control-for options that correspond to mappings from states to action policies (O/sub /spl Pi// options). Experimental results show that careful (domain-dependent) selection of options (via methods such as discretised potential fields) produce much faster learning for option-based robots when compared to their action-based counterparts. Of critical importance, however, is the option mapping in regions of the state space where the options are not assumed to be necessary: as performance of reinforcement learning algorithms is strongly dependent on sufficient exploration of the state space, even in such regions a careful, ad-hoc selection of actions is of foremost importance.",https://ieeexplore.ieee.org/document/972633/,SCCC 2001. 21st International Conference of the Chilean Computer Science Society,9-9 Nov. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FCCM51124.2021.00046,Edge Accelerator for Lifelong Deep Learning using Streaming Linear Discriminant Analysis,IEEE,Conferences,"Lifelong deep learning models are expected to continuously adapt and acquire new knowledge in dynamic environments. This capability is essential for numerous vision tasks in robotics and drones, and the models must be deployed on the edge to achieve real-time performance. We propose a FPGA accelerator of a streaming classifier for lifelong deep learning, which is based on streaming linear discriminant analysis (SLDA). When combined with a frozen Convolutional Neural Network (CNN) model, the proposed system is capable of class incremental lifelong learning for object classification.",https://ieeexplore.ieee.org/document/9444094/,2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM),9-12 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WiSPNET51692.2021.9419475,Emotion based Media Playback System using PPG Signal,IEEE,Conferences,"The study involved identifying human emotions and integrates the identified emotion with the music system. The idea is to develop a complete product to utilize the detected emotion in a real-time application and also to achieve more accuracy and less memory. Human emotions are identified using physiological signals such as electrocardiography, electromyography, photoplethysmography, respiration, skin temperature, etc. Obtaining photoplethysmography (PPG) from the sensor is a simple, cost-effective, and non-invasive method. PPG sensors are capable of providing accurate heart-rate (HR) by detecting the variations in the blood flow. Signals are acquired using wearable technology from a personal whereas not compromising comfort and privacy. The attributes of Heart Rate Variability are analyzed to describe emotions, namely happy, calm, unhappy (sad), and fear using Machine learning technology. We deployed this recognized emotion to automate the music system associated with its emotion. To bring this, we built an Android app to communicate with the smart wearable utilized. Totally 150 members from both genders have participated. The accuracy of 91.81% is achieved. This emotion recognition system can be used in various fields like robotics, medicine, virtual reality, and gaming, advertising, education, automotive working conditions and safety, home appliances.",https://ieeexplore.ieee.org/document/9419475/,"2021 Sixth International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)",25-27 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2013.6628436,Emotional evaluation of bandit problems,IEEE,Conferences,"In this paper, we discuss an approach to evaluate decisions made during a multi-armed bandit learning experiment. Usually, the results of machine learning algorithms applied on multi-armed bandit scenarios are rated in terms of earned reward and optimal decisions taken. These criteria are valuable for objective comparison in finite experiments. But learning algorithms used in real scenarios, for example in robotics, need to have instantaneous criteria to evaluate their actual decisions taken. To overcome this problem, in our approach each decision updates the Zürich model which emulates the human sense of feeling secure and aroused. Combining these two feelings results in an emotional evaluation of decision policies and could be used to model the emotional state of an intelligent agent.",https://ieeexplore.ieee.org/document/6628436/,2013 IEEE RO-MAN,26-29 Aug. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561889,End-to-End Semi-supervised Learning for Differentiable Particle Filters,IEEE,Conferences,"Recent advances in incorporating neural networks into particle filters provide the desired flexibility to apply particle filters in large-scale real-world applications. The dynamic and measurement models in this framework are learnable through the differentiable implementation of particle filters. Past efforts in optimising such models often require the knowledge of true states which can be expensive to obtain or even unavailable in practice. In this paper, in order to reduce the demand for annotated data, we present an end-to-end learning objective based upon the maximisation of a pseudo-likelihood function which can improve the estimation of states when large portion of true states are unknown. We assess performance of the proposed method in state estimation tasks in robotics with simulated and real-world datasets.",https://ieeexplore.ieee.org/document/9561889/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EAEEIE54893.2022.9820462,Ethics in Engineering Education,IEEE,Conferences,"Although ethics was introduced into the engineering curricula in the 1970s for the first time, it is not yet a standard and integral part of engineering education. As a research discipline, engineering ethics education appeared in the late 1990s and early 2000s.Ethics was introduced to biomedical engineering curricula at the Czech Technical University in Prague already in the 1990s and since then it became an integral part of biomedical engineering study programs. However, until now we do not find ethics, for example, in electrical engineering or information technology study programs. Currently, ethics is intensively discussed in relation to artificial intelligence, robotics, and human-machine interaction.Artificial intelligence and robotics are not purely theoretical research areas anymore. They already entered our daily life. The development of autonomous vehicles opened many questions that were previously touched only on a high theoretical level. Nowadays, artificial intelligence and robotics constitute frequently independent study programs and it is highly desirable to introduce the ethical and social issues directly into the curricula. We are well aware of the fact that the curricula have a limited number of courses, should be studied in a limited time and it is impossible to think that students become experts in all studied fields. The conclusion we agreed on is that ethics should become integrated into engineering courses that illustrate the necessity of ethical aspects in real-life applications and examples of practical solutions.In the paper, we present a brief description of our approach to the integration of ethical questions in specific courses in biomedical engineering and biomedical informatics study programs.",https://ieeexplore.ieee.org/document/9820462/,2022 31st Annual Conference of the European Association for Education in Electrical and Information Engineering (EAEEIE),29 June-1 July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RVSP.2013.73,Evolution of Neural Controllers for Simulated and Real Quadruped Robots,IEEE,Conferences,"Evolutionary robotics is an approach that employs evolutionary computation to develop a controller for an autonomous robotic system. Evolutionary computing usually operates depending on a population of candidate controllers, initially selected from a random distribution. The population is iteratively modified according to the fitness function. In this paper, an automatic control system is designed for quadruped robots using an Evolutionary Neural Network (ENN) and the performance is measured in terms of the distance travelled by the robot from its origin. The evolved neural controllers are analyzed in the simulation environment and the results are implemented in a real quadruped robot. The comparison between the simulated and real robot shows the performance of the quadruped robot in terms of number of iterations over the distance covered in the desired direction. The developed ENN helps the robot to choose the best possible solution to achieve the maximum distance.",https://ieeexplore.ieee.org/document/6830033/,"2013 Second International Conference on Robot, Vision and Signal Processing",10-12 Dec. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2003.1251857,Exploiting value statistics for similar continuing tasks,IEEE,Conferences,"In this paper, we try to consider interaction design for adaptation from the viewpoint of transfer of knowledge. Advancements in robotics are amazing, and their interaction processes with outside world (including human) are getting to be longer in time scale. We investigate these matters in an abstract agent that faces multiple learning tasks within its lifetime, transferring past learning experiences to improve its performance. We formulize the multitask reinforcement learning problem at first, and then we present two ways of incorporating past learning experiences into the agent's learning algorithm.",https://ieeexplore.ieee.org/document/1251857/,"The 12th IEEE International Workshop on Robot and Human Interactive Communication, 2003. Proceedings. ROMAN 2003.",2-2 Nov. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIEDS49339.2020.9106581,"Explorer51 – Indoor Mapping, Discovery, and Navigation for an Autonomous Mobile Robot",IEEE,Conferences,"The nexus of robotics, autonomous systems, and artificial intelligence (AI) has the potential to change the nature of human guided exploration of indoor and outdoor spaces. Such autonomous mobile robots can be incorporated into a variety of applications, ranging from logistics and maintenance, to intelligence gathering, surveillance, and reconnaissance (ISR). One such example is that of a tele-operator using the robot to generate a map of the inside of a building while discovering and tagging the objects of interest. During this process, the tele-operator can also assign an area for the robot to navigate autonomously or return to a previously marked area/object of interest. Search and rescue and ISR abilities could be immensely improved with such capabilities. The goal of this research is to prototype and demonstrate the above autonomous capabilities in a mobile ground robot called Explorer51. Objectives include: (i) enabling an operator to drive the robot non-line of sight to explore a space by incorporating a first-person view (FPV) system to stream data from the robot to the base station; (ii) implementing automatic collision avoidance to prevent the operator from running the robot into obstacles; (iii) creating and saving 2D and 3D maps of the space in real time by using a 2D laser scanner, tracking, and depth/RGB cameras; (iv) locating and tagging objects of interest as waypoints within the map; (v) autonomously navigate within the map to reach a chosen waypoint.To accomplish these goals, we are using the AION Robotics R1 Unmanned Ground Vehicle (UGV) rover as the platform for Explorer51 to demonstrate the autonomous features. The rover runs the Robot Operating System (ROS) onboard an NVIDIA Jetson TX2 board, connected to a Pixhawk controller. Sensors include a 2D scanning LiDAR, depth camera, tracking camera, and an IMU. Using existing ROS packages such as Cartographer and TEB planner, we plan to implement ROS nodes for accomplishing these tasks. We plan to extend the mapping ability of the rover using Visual Inertial Odometry (VIO) using the cameras. In addition, we will explore the implementation of additional features such as autonomous target identification, waypoint marking, collision avoidance, and iterative trajectory optimization. The project will culminate in a series of demonstrations to showcase the autonomous navigation, and tele-operation abilities of the robot. Success will be evaluated based on ease of use by the tele-operator, collision avoidance ability, autonomous waypoint navigation accuracy, and robust map creation at high driving speeds.",https://ieeexplore.ieee.org/document/9106581/,2020 Systems and Information Engineering Design Symposium (SIEDS),24-24 April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS48674.2020.9214045,Extensions of the open-source framework Aerostack 3.0 for the development of more interactive flights between UAVs,IEEE,Conferences,"The basis for properly verified R&D works is to provide reliable prototyping tools at three most important stages: computer simulation, laboratory tests and real-world experiments. In the laboratory-limited conditions, particular importance is attributed to the first two stages, especially in the context of the safety development of autonomous flights of unmanned aerial vehicle (UAV) groups in various missions. The open-source framework Aerostack support those needs and its effectiveness has been proven in the International Micro Air Vehicle Indoor Competitions (IMAV 2013, 2016, 2017) and Mohammed Bin Zayed International Robotics Challenge (MBZIRC 2020). In the paper, the exemplary functionalities for the new version of Aerostack Version 3.0 Distribution Sirocco (Aerial robotics framework for the industry), extended additionally with a library of new behaviors, are presented. The mission of UAVs can be developed fast and effectively in order to conduct test flights with real drones in lab, before one will decide to fly autonomously outdoor. The representative results obtained for low-cost AR.Drone 2.0 UAV models in two missions, are presented. The first mission is autonomous patrolling the area by a pair of UAVs, the second - intercepting the intruder in guarded area by the guard UAV.",https://ieeexplore.ieee.org/document/9214045/,2020 International Conference on Unmanned Aircraft Systems (ICUAS),1-4 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECS49266.2020.9294790,FPGA Implementation of Simplified Spiking Neural Network,IEEE,Conferences,"Spiking Neural Networks (SNN) are third generation Artificial Neural Networks (ANN), which are close to the biological neural system. In recent years SNN has become popular in the area of robotics and embedded applications, therefore, it has become imperative to explore its real-time and energy-efficient implementations. SNNs are more powerful than their predecessors because of their ability to encode temporal information and to use biologically plausible plasticity rules. In this paper, a simpler and computationally efficient SNN model is described. The proposed model is implemented and validated utilizing a Xilinx Virtex 6 FPGA. It is demonstrated that the proposed model analyzes a fully connected network consisting of 800 neurons and 12,544 synapses in real-time.",https://ieeexplore.ieee.org/document/9294790/,"2020 27th IEEE International Conference on Electronics, Circuits and Systems (ICECS)",23-25 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SACI.2007.375494,FPGA Parallel Implementation of CMAC Type Neural Network with on Chip Learning,IEEE,Conferences,"The hardware implementation of neural networks is a new step in the evolution and use of neural networks in practical applications. The CMAC cerebellar model articulation controller is intended especially for hardware implementation, and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA's has several benefits, with emphasis on parallelism and the real time capabilities. This paper discusses the hardware implementation of the CMAC type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor.",https://ieeexplore.ieee.org/document/4262496/,2007 4th International Symposium on Applied Computational Intelligence and Informatics,17 Yearly-18 May 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WiSPNET51692.2021.9419438,Faster Training of Edge-attention Aided 6D Pose Estimation Model using Transfer Learning and Small Customized Dataset,IEEE,Conferences,"Computer Vision is the field of machine learning that deals with computers gaining knowledge from digital images/videos and performing tasks that human vision is capable of doing. It is widely used in the field of robotics for designing guidance systems where objects in the robot's field of view are identified and located. This research work is an application-specific project enabling a half-humanoid to find the 6D pose and bounding boxes of its hand and other objects within its field of view. We add an edge prediction head to the NOCS (Normalised Object Coordinate Space) model, which predicts the edges of each object from the predicted instance maps. An additional edge-agreement-loss found from the predicted edges is added to the total loss. This increases the attention to the edges and improves the accuracy of prediction of the instance masks. This edge-attention aided model is initialized with pre-trained weights of CAMERA and REAL dataset using transfer learning. The backbone layers of the model are frozen and the head layers alone are trained using a synthetic dataset (HAND dataset) we created using a software called blender. The model gives promising results when tested with objects kept in varying lighting conditions and at different distances from the camera. The use of transfer learning in models as large as the NOCS model allows us to train the model for a new class by only training the top few layers with a significantly small dataset.",https://ieeexplore.ieee.org/document/9419438/,"2021 Sixth International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)",25-27 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2017.8280891,Fault diagnosis in robot swarms: An adaptive online behaviour characterisation approach,IEEE,Conferences,"The need for an active approach to fault tolerance in swarm robotics systems is well established. This will necessarily include an approach to fault diagnosis if robot swarms are to retain long-term autonomy. This paper proposes a novel method for fault diagnosis, based around behavioural feature vectors, that incorporates real-time learning and memory. Initial results are encouraging, and show that an unsupervised learning approach is able to diagnose common electro-mechanical fault types, and arrive at an appropriate recovery option in the majority of the cases tested.",https://ieeexplore.ieee.org/document/8280891/,2017 IEEE Symposium Series on Computational Intelligence (SSCI),27 Nov.-1 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SBR-LARS.2012.57,Fixed-Point Neural Network Ensembles for Visual Navigation,IEEE,Conferences,"Visual navigation is an important research field in robotics because of the low cost and the high performance that is usually achieved by visual navigation systems. Pixel classification as a road pixel or a non-road pixel is a task that can be well performed by Artificial Neural Networks. In the case of real-time instances of the image classification problem, as when applied to autonomous vehicles navigation, it is interesting to achieve the best possible execution time. Hardware implementations of these systems can achieve fast execution times but the floating-point implementation of Neural Networks are commonly complex and resource intensive. This work presents the implementation and analysis of a fixed-point Neural Network Ensemble for image classification. The system is composed by six fixed-point Neural Networks verified with cross-validation technique, using some proposed voting schemes and analyzed considering the execution time, precision, memory consumption and accuracy for hardware implementation. The results show that the fixed-point implementation is faster, consumes less memory and has an acceptable precision compared to the floating-point implementation. This fact suggests that the fixed point implementation should be used in systems that need a fast execution time. Some questions about ensembles and voting have to be reviewed for fixed-point Neural Network Ensembles.",https://ieeexplore.ieee.org/document/6363361/,2012 Brazilian Robotics Symposium and Latin American Robotics Symposium,16-19 Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/OCEANS44145.2021.9705773,Fusion-UWnet: Multi-channel Fusion-based Deep CNN for Underwater Image Enhancement,IEEE,Conferences,"Underwater image enhancement has been considered as one of the prime research areas due to its massive significance in underwater surveillance and the development of underwater autonomous robotics. Deep learning methods have been used for image processing, where heavy models like GANs and very deep CNNs are being deployed for the task. Due to the bulky nature of the models, they consume significant memory and are numerically expensive in computational tasks, making them inefficient to some degree in underwater exploration tasks. These models are primarily trained over synthetically generated data which makes them less correlative for real-world tasks. This paper proposes a deep network architecture that uses a series of convolutional blocks to fuse significant complementary features of two separate enhanced versions of the input image along with the input one. Further, a combination of perceptual and structural similarity losses is used to find out the error. We have also benchmarked our model on three underwater datasets, highlighting the generalizing capabilities over a mix of real-world and synthetic data.",https://ieeexplore.ieee.org/document/9705773/,OCEANS 2021: San Diego – Porto,20-23 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2009.5195905,GPS and sonar based area mapping and navigation by mobile robots,IEEE,Conferences,"In this paper, we have presented a GPS and sonar based area mapping and navigation scheme for a mobile robot. A mapping is achieved between the GPS space and the world coordinates of the mobile robot which enables us to generate direct motion commands for it. This mapping enables the robot to navigate among different GPS locations within the mapped area. The GPS data is extracted online to get the latitude and longitude information of a particular location. In the training phase, a 2-D axis transformation is used to relate local robot frame with the robot world coordinates and then the actual world coordinates are mapped from the GPS data using a RBFN (radial basis function network) based Neural Network. In the second phase, direct GPS data is used to get the mapping into the world coordinates of mobile robot using the trained network and the motion commands are generated accordingly. The physical placement of sonar devices, their ranging limits and beam opening angles are considered during navigation for possible collision detection and obstacle avoidance. This scheme is successfully implemented in real time with Pioneer mobile robot from ActivMedia Robotics and GPS receiver. The scheme is also tested in the simulation to justify its application in the real world.",https://ieeexplore.ieee.org/document/5195905/,2009 7th IEEE International Conference on Industrial Informatics,23-26 June 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3ICT.2019.8910276,Ground Operations Management using a Data Governance Dashboard,IEEE,Conferences,"An incident involving the use of chemical, biological, radiological, and nuclear (CBRN) materials might represent a significant challenge for crime scene investigators. The paper presents a full system architecture to assess the hazardous situations resulting from CBRN materials. This issue is crucial since there were a number of incidents that occurred in which forensics people could not reach the location either due to being unreachable or due to harmful emissions. The proposed solution integrates various inputs including data from sensors, video streaming, geo-data along with using Artificial Intelligence (AI) for good decision-making and data analysis. A geo-dashboard was also designed to demonstrate, in real-time, the collected data from several angles and according to various queries. It also monitors the performance in real-time. The topic is not new however the novelty of the proposed solution is the integration of multiple sources of data, applying deep neural nets and projecting the data and data analytics in real-time on a dashboard that displays the analysis and data from different perspectives considering the viewpoint of the individuals who will use that system. The paper also presents how the ROCSAFE multidisciplinary research project addresses the identified scenario. The project combines topics from robotics, sensor technology, analytical and situation awareness software, transforming data into knowledgeable insights to support the decision-making process.",https://ieeexplore.ieee.org/document/8910276/,"2019 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)",22-23 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII46433.2020.9025951,Gym-Ignition: Reproducible Robotic Simulations for Reinforcement Learning,IEEE,Conferences,"This paper presents Gym-Ignition, a new framework to create reproducible robotic environments for reinforcement learning research. It interfaces with the new generation of Gazebo, part of the Ignition Robotics suite, which provides three main improvements for reinforcement learning applications compared to the alternatives: 1) the modular architecture enables using the simulator as a C++ library, simplifying the interconnection with external software; 2) multiple physics and rendering engines are supported as plugins, simplifying their selection during the execution; 3) the new distributed simulation capability allows simulating complex scenarios while sharing the load on multiple workers and machines. The core of Gym-Ignition is a component that contains the Ignition Gazebo simulator and exposes a simple interface for its configuration and execution. We provide a Python package that allows developers to create robotic environments simulated in Ignition Gazebo. Environments expose the common OpenAI Gym interface, making them compatible out-of-the-box with third-party frameworks containing reinforcement learning algorithms. Simulations can be executed in both headless and GUI mode, the physics engine can run in accelerated mode, and instances can be parallelized. Furthermore, the Gym-Ignition software architecture provides abstraction of the Robot and the Task, making environments agnostic on the specific runtime. This abstraction allows their execution also in a real-time setting on actual robotic platforms, even if driven by different middlewares.",https://ieeexplore.ieee.org/document/9025951/,2020 IEEE/SICE International Symposium on System Integration (SII),12-15 Jan. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636167,HARL-A: Hardware Agnostic Reinforcement Learning Through Adversarial Selection,IEEE,Conferences,"The use of reinforcement learning (RL) has led to huge advancements in the field of robotics. However data scarcity, brittle convergence and the gap between simulation & real world environments, mean that most common RL approaches are subject to over fitting and fail to generalise to unseen environments. Hardware agnostic policies would mitigate this by allowing a single network to operate in a variety of test domains, where dynamics vary due to changes in robotic morphologies or internal parameters. We utilise the idea that learning to adapt a known and successful control policy is easier and more flexible than jointly learning numerous control policies for different morphologies.This paper presents the idea of Hardware Agnostic Reinforcement Learning using Adversarial selection (HARL-A). In this approach training examples are sampled using a novel adversarial loss function. This is designed to self regulate morphologies based on their learning potential. Simply applying our learning potential based loss function to current state-of-the-art already provides ~ 30% improvement in performance. Meanwhile experiments using the full implementation of HARL-A report an average increase of 70% to a standard RL baseline and 55% compared with current state-of-the-art.",https://ieeexplore.ieee.org/document/9636167/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1992.225127,Hierarchical architecture for multi-sensor robot cell operation,IEEE,Conferences,"The authors describe a hierarchical architecture designed to carry out experiments in multisensor integration and sensor-based control in robotics. The hierarchical model is composed of three major levels: a high-level information processing and planning structure at the top, a logic-branching control structure at the intermediate level, and a real-time continuous sensory feedback loop at the bottom level. The two lower control structures are addressed. The principal submodules of the intermediate structure are described, with particular emphasis on communication issues and on the available software mechanisms for configuration and online maintenance of the robot cell. The architecture of the real-time continuous control structure that composes the bottom level is also described. The application of the adaptive self-tuning scheme in controlling position and force, specified in task-space coordinates, is discussed. Practical issues and experimental results are summarized.<>",https://ieeexplore.ieee.org/document/225127/,Proceedings of the 1992 IEEE International Symposium on Intelligent Control,11-13 Aug. 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1999.770002,High-speed navigation using the global dynamic window approach,IEEE,Conferences,"Many applications in mobile robotics require the safe execution of a collision-free motion to a goal position. Planning approaches are well suited for achieving a goal position in known static environments, while real-time obstacle avoidance methods allow reactive motion behavior in dynamic and unknown environments. This paper proposes the global dynamic window approach as a generalization of the dynamic window approach. It combines methods from motion planning and real-time obstacle avoidance to result in a framework that allows robust execution of high-velocity, goal-directed reactive motion for a mobile robot in unknown and dynamic environments. The global dynamic window approach is applicable to nonholonomic and holonomic mobile robots.",https://ieeexplore.ieee.org/document/770002/,Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No.99CH36288C),10-15 May 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCICC50026.2020.9450269,Human Capability Augmentation through Cognitive and Autonomous Systems,IEEE,Conferences,"The Covid-19 pandemic reminds us again about our limited knowledge and understanding in the nature including both micro and macro worlds. We have been developing a variety of tools such as automation, robotics, internet, and artificial intelligence (AI), etc. to augment human capability for improved safety, quality, and productivity in work and life, but human lives are still vulnerable over 100 years since the last Spanish Flu in 1918. We are even more vulnerable when the tools we developed (e.g., automation and AI) do not understand human intent or follow human instructions. Recent accidents to the Boeing 737 Max passengers ring the alarm again about the imperative needs of appropriate design concepts and scientific methodologies for developing safety critical cognitive and/or autonomous systems or AI functions and collaborative partnership of human and intelligent systems. With AI and its related technologies reach their bottleneck, it is even more vital to follow scientific and systematic methodology to understand well about capacity and limitation of both human intelligence and machine intelligence so that their strengths can be optimized for a collaborative partnership when dealing with safety critical situations. This talk discusses about the needs for the researchers, designers, developers, and all practitioners who are interested in building and using 21st century human-autonomy symbiosis technologies (Why). It touches the topics of proper analytical methodologies for functional requirements of the intelligent systems, design methodologies, implementation strategies, evaluation approaches, and trusted relationships (How). These aspects will be explained with real-world examples when considering contextual constraints of technology, human capability and limitations, and functionalities that AI and autonomous systems should achieve (When). Audience will gain insights of context-based and interaction-centered design approach for developing a safe, trusted, and collaborative partnership between human and technology by optimizing the interaction between human intelligence and AI. The challenges and potential issues will also be discussed for guiding future research and development activities when augmenting human capabilities with AI, and cognitive and/or autonomous systems.",https://ieeexplore.ieee.org/document/9450269/,2020 IEEE 19th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),26-28 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2012.6491182,Human recognition with a hardware-accelerated multi-prototype learning and classification system,IEEE,Conferences,"This paper reports a hardware-accelerated multi-prototype learning and classification system which is suitable for real-time recognition systems. The real-world applicability of robotics or surveillance systems is dependent upon their real-time performance. Hardware based solutions can meet the needs for real-time limited problems; however, hardware-friendly solutions have lacked the flexibility to handle a large range of complex tasks. Software based solutions have been used to tackle complex tasks and allow for greater flexibility but lack the speeds which hardware systems can provide. The developed multi-prototype learning and classification system surmounts these limitations and is applied to the problem of human recognition for demonstrating its capabilities. A fully digital Euclidian distance searching circuit is developed in order to reduce the computational cost within the learning and classification process. The system outperforms other implementations by significantly reducing training times and attains a per sample recognition speed of 1.03 μs.",https://ieeexplore.ieee.org/document/6491182/,2012 IEEE International Conference on Robotics and Biomimetics (ROBIO),11-14 Dec. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.1992.253866,Hybrid architectures for intelligent robotic systems,IEEE,Conferences,"Hybrid architectures, based on combinations of analogic, symbolic, and neural methods, are well suited for real-time applications in advanced robotics. Real-time industrial applications are mainly based on the correction of preplanned programs. So far, the planning and control modules of these kind of applications are often unable to react and/or classify un-expected events. The approach described attempts to integrate the sensor-based analogic method and the neural method into a multiple-level architecture that operates on an analogic world model, so that the action planning can be performed in a smart, reactive way. Given the task, the system builds the world model of the scenario. The reasoning and planning modules act both at the strategic as well as reactive levels, and the activated sensor-based motor strategies handle the sensorial data inputs and drive the robot controller module in the execution of the stream of motor commands. The interaction between the different levels is mainly based on the idea of maintaining and updating in real-time the world model, so that each module can locally operate on specific parts of the whole world model.<>",https://ieeexplore.ieee.org/document/253866/,[1992] Proceedings IEEE International Workshop on Robot and Human Communication, 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICM52667.2021.9664950,Image Inpainting and Classification Agent Training Based on Reinforcement Learning and Generative Models with Attention Mechanism,IEEE,Conferences,"What distinguishes the field of artificial intelligence (AI) from others is to develop fully independent agents that learn optimal behavior, change, and evolve solely through the communication of trial and error with the surrounding environment. Reinforcement learning (RL) can be seen in multiple aspects of Machine Learning (ML), provided the environment, reward, actions, the state will be defined. Agent training in previous years is seen to only relate to robotics, games, and self-driving cars. While trying to divert the focus of researchers from the view of self-driving cars, games, robots, etc. Here, we investigated using reinforcement learning in the aspect of task completion. We deployed our architecture in an inpainting task where the agent generates the distorted or missing image content into an eminent fidelity completed the image by using reinforcement learning to influence the generative model utilized. The Generative Adversary Network (GAN) problem of not being steady and challenging to train was overwhelmed by utilizing latent space representation. The dimension is reduced compared to the distorted or corrupted image in training the GAN. Then reinforcement learning was deployed to pick the correct GAN input to get the image’s latent space representation that is most suitable for the current input of the missing or distorted image region. In this paper, we also learned that the trained agent enhances the accuracy in a classification task of images with missing data. We successfully examined the classification enhancement on images missing 30%, 50%, and 70%.",https://ieeexplore.ieee.org/document/9664950/,2021 International Conference on Microelectronics (ICM),19-22 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCME52200.2021.9590955,Impact of Real-World Market Conditions on Returns of Deep Learning based Trading Strategies,IEEE,Conferences,"Based on recent advancements in natural language processing, computer vision and robotics, a growing number of researchers and traders attempt to predict future asset prices using deep learning techniques. Typically, the goal is to find a profitable and at the same time low-risk trading strategy. However, it is not straightforward to evaluate a found trading strategy. Evaluating solely on historic price data neglects important factors arising in real markets. In this paper, we analyze the impact of real-world market conditions in terms of trading fees, borrow interests, slippage and spreads on trading returns. For that, we propose a deep learning trading bot based on Temporal Convolutional Networks, which is deployed to a real cryptocurrency exchange. We compare the results obtained in the real market with simulated returns and investigate the impact of the different real-world market conditions. Our results show that besides trading fees (which have the biggest impact on returns), factors like slippage and spread also affect the returns of the trading strategy.",https://ieeexplore.ieee.org/document/9590955/,"2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",7-8 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2014.6983001,Incremental training of Restricted Boltzmann Machines using information driven saccades,IEEE,Conferences,"In the context of developmental robotics, a robot has to cope with complex sensorimotor spaces by reducing their dimensionality. In the case of sensor space reduction, classical approaches for pattern recognition use either hardcoded feature detection or supervised learning. We believe supervised learning and hard-coded feature extraction must be extended with unsupervised learning of feature representations. In this paper, we present an approach to learn representations using space-variant images and saccades. The saccades are driven by a measure of quantity of information in the visual scene, emerging from the activations of Restricted Boltzmann Machines (RBMs). The RBM, a generative model, is trained incrementally on locations where the system saccades. Our approach is implemented using real data captured by a NAO robot in indoor conditions.",https://ieeexplore.ieee.org/document/6983001/,4th International Conference on Development and Learning and on Epigenetic Robotics,13-16 Oct. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE48305.2020.9216902,Industrial Robot Grasping with Deep Learning using a Programmable Logic Controller (PLC),IEEE,Conferences,"Universal grasping of a diverse range of previously unseen objects from heaps is a grand challenge in e-commerce order fulfillment, manufacturing, and home service robotics. Recently, deep learning based grasping approaches have demonstrated results that make them increasingly interesting for industrial deployments. This paper explores the problem from an automation systems point-of-view. We develop a robotics grasping system using Dex-Net, which is fully integrated at the controller level. Two neural networks are deployed on a novel industrial AI hardware acceleration module close to a PLC with a power footprint of less than 10 W for the overall system. The software is tightly integrated with the hardware allowing for fast and efficient data processing and real-time communication. The success rate of grasping an object form a bin is up to 95% with more than 350 picks per hour, if object and receptive bins are in close proximity. The system was presented at the Hannover Fair 2019 (world's largest industrial trade fair) and other events, where it performed over 5,000 grasps per event.",https://ieeexplore.ieee.org/document/9216902/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CADCG.2007.4407908,Intelligent Robotic Peg-in-Hole Insertion Learning Based on Haptic Virtual Environment,IEEE,Conferences,A new approach is explored to transfer human manipulation skills to a robotics system. A skill acquisition algorithm utilizes the position and contact force/torque data generated in the virtual environment combined with a priori knowledge about the task to generate the skills required to perform such a task. Such skills are translated into actual robotic trajectories for implementation in real time. The peg-in-hole insertion problem is used as a case study. The results are reported.,https://ieeexplore.ieee.org/document/4407908/,2007 10th IEEE International Conference on Computer-Aided Design and Computer Graphics,15-18 Oct. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDA.2010.5687225,Intelligent online case-based planning agent model for real-time strategy games,IEEE,Conferences,"Research in learning and planning in real-time strategy (RTS) games is very interesting in several industries such as military industry, robotics, and most importantly game industry. A recent published work on online case-based planning in RTS Games does not include the capability of online learning from experience, so the knowledge certainty remains constant, which leads to inefficient decisions. In this paper, an intelligent agent model based on both online case-based planning (OLCBP) and reinforcement learning (RL) techniques is proposed. In addition, the proposed model has been evaluated using empirical simulation on Wargus (an open-source clone of the well known RTS game Warcraft 2). This evaluation shows that the proposed model increases the certainty of the case base by learning from experience, and hence the process of decision making for selecting more efficient, effective and successful plans.",https://ieeexplore.ieee.org/document/5687225/,2010 10th International Conference on Intelligent Systems Design and Applications,29 Nov.-1 Dec. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIMCA.2006.133,International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce - Title,IEEE,Conferences,"The following topics are dealt with: intelligent agents and ontologies; data mining, knowledge discovery and decision making; intelligent systems; Web technologies and Web services; virtual reality and games; image processing and image understanding techniques; adaptive control and automation; modelling, prediction and control; multi-agent systems and computational intelligence; agent systems, personal assistant agents and profiling; fuzzy systems for industrial automation; control strategies; neural network applications; clustering, classification, data mining and risk analysis; dynamics systems; innovative control systems, hardware design and implementation; robotics and automation; e-business, e-commerce, innovative Web applications; Web databases; diagnosis and medical applications; learning systems; optimization, hybrid systems, genetic algorithms and evolutionary computation control applications; online learning and ERP; knowledge acquisition and classification; nanomechatronics; simulation and control; mobile network applications; information retrieval; Bayesian networks; human computer interaction; cognitive science; mobile agents; knowledge management; intelligent control; e-search and navigation; security.",https://ieeexplore.ieee.org/document/4052645/,2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06),28 Nov.-1 Dec. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CogInfoCom47531.2019.9089994,Investigating the Social Robots’ Role in Improving Children Attitudes toward Recycling. The case of PeppeRecycle,IEEE,Conferences,"In this paper we investigate the impact of a social robot in the context of serious games in which the robot plays the role of a game opponent by challenging and, at the same time, teaching the child to correctly recycle waste materials. To this aim we performed a study in which we investigated the dimensions that are used to evaluate serious games integrated with those that are typical of the interaction with a social robot. To endow the robot with the capability to play as a game opponent in a real-world context, we implemented an image recognition module based on a Convolutional Neural Network so that the robot could detect and classify the waste material as a child would do, by seeing it. After a preliminary evaluation of the approach, we started a formal experiment in which we measured the effectiveness of game design, the robot evaluation and the evaluation of cognitive and affective elements that can form the pro-environmental attitude and then the tendency to recycling. A primary school classroom was involved in the study and, results obtained so far, are encouraging and drew promising possibilities for robotics education in changing recycling attitude for children since Pepper is positively evaluated as trustful and believable and this allowed to be concentrated on the `memorization' task during the game.",https://ieeexplore.ieee.org/document/9089994/,2019 10th IEEE International Conference on Cognitive Infocommunications (CogInfoCom),23-25 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCWC51732.2021.9376024,Joint Activity Localization and Recognition with Ultra Wideband based on Machine Learning and Compressed Sensing,IEEE,Conferences,"Joint human activity localization and recognition has broad application prospects in human-computer interaction, virtual reality, smart healthcare system, security monitoring and robotics. Ultra-wideband (UWB) is an emerging technology adopted in real-time location system (RTLS) and has shown satisfactory performance in the task of human activity localization. However, few studies have been carried out to simultaneously recognize human activities based on UWB RTLS, which limits the use of UWB RTLS in many applications. In this study, we develop a RTLS based on UWB for the joint task of activity localization and recognition. A compressed sensing-based activity recognition approach is proposed for the task of activity recognition and several machine learning methods are designed to further improve the activity localization accuracy for the task of activity localization. The experimental results show that our UWB RTLS achieves good performance in this joint task.",https://ieeexplore.ieee.org/document/9376024/,2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC),27-30 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCTA48790.2019.9478839,Keynote Speech II: Readiness for the Impact of Emerging Technologies,IEEE,Conferences,"Summary form only given. The complete presentation was not made available for publication as part of the conference proceedings. The digital world is becoming increasingly intertwined with the physical world of machines, to which it is bringing ubiquitous intelligence and a perpetual flow of information. These trends are driving us towards a very different future. That future has already started. A new wave of social, economic, and psychological changes is expected to abruptly affect almost everything we do. With change, many opportunities come along. Those who anticipate the course of the future, and prepare for it, will be ready to seize these opportunities and will come out winners. Those who chose to ignore the signs of change, will risk losing their livelihood and eventually hurting their families, businesses, and societies. Those who see the storm coming but react by standing still in panic, disgruntlement, and lamentation will be defenseless when the inevitable waves hit their shores. This presentation overviews the trends in technology and applications, including Artificial Intelligence, Big Data Analytics, Robotics, Internet of Things, Industry 4.0, etc. The impact that such advances are likely to have on the high-tech as well as the low-tech job markets is outlined. Some actions and initiatives are proposed and discussed, with the purpose of triggering a larger debate on how individuals, businesses, academic institutions, and governments should prepare for the anticipated massive changes that are already beginning to affect our world.",https://ieeexplore.ieee.org/document/9478839/,2019 29th International Conference on Computer Theory and Applications (ICCTA),29-31 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISORCW.2012.36,Knowledge Representation for Cognitive Robotic Systems,IEEE,Conferences,"Cognitive robotics are autonomous systems capable of artificial reasoning. Such systems can be achieved with a logical approach, but still AI struggles to connect the abstract logic with real-world meanings. Knowledge representation and reasoning help to resolve this problem and to establish the vital connection between knowledge, perception, and action of a robot. Cognitive robots must use their knowledge against the perception of their world and generate appropriate actions in that world in compliance with some goals and beliefs. This paper presents an approach to multi-tier knowledge representation for cognitive robots, where ontologies are integrated with rules and Bayesian networks. The approach allows for efficient and comprehensive knowledge structuring and awareness based on logical and statistical reasoning.",https://ieeexplore.ieee.org/document/6196117/,2012 IEEE 15th International Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing Workshops,11-11 April 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIS.2013.6782155,Knowledge representation with KnowLang the marXbot case study,IEEE,Conferences,"Intelligent systems are capable of AI exhibited via knowledge representation and reasoning, which helps to connect abstract knowledge symbols to real-world meanings. This paper presents a formal language for knowledge representation called KnowLang. The language implies a multi-tier specification model emphasizing knowledge corpuses, knowledge base operators and inference primitives. The approach allows for efficient and comprehensive knowledge structuring where ontologies are integrated with rules and Bayesian networks. The paper presents the KnowLang specification constructs formally along with a case study based on a mobile robotics platform.",https://ieeexplore.ieee.org/document/6782155/,2012 IEEE 11th International Conference on Cybernetic Intelligent Systems (CIS),23-24 Aug. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2010.5649358,LCM: Lightweight Communications and Marshalling,IEEE,Conferences,"We describe the Lightweight Communications and Marshalling (LCM) library for message passing and data marshalling. The primary goal of LCM is to simplify the development of low-latency message passing systems, especially for real-time robotics research applications. Messages can be transmitted between different processes using LCM's publish/subscribe message-passing system. A platformand language-independent type specification language separates message description from implementation. Message specifications are automatically compiled into language-specific bindings, eliminating the need for users to implement marshalling code while guaranteeing run-time type safety. LCM is notable in providing a real-time deep traffic inspection tool that can decode and display message traffic with minimal user effort and no impact on overall system performance. This and other features emphasize LCM's focus on simplifying both the development and debugging of message passing systems. In this paper, we explain the design of LCM, evaluate its performance, and describe its application to a number of autonomous land, underwater, and aerial robots.",https://ieeexplore.ieee.org/document/5649358/,2010 IEEE/RSJ International Conference on Intelligent Robots and Systems,18-22 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8460502,Learning Motion Predictors for Smart Wheelchair Using Autoregressive Sparse Gaussian Process,IEEE,Conferences,"Constructing a smart wheelchair on a commercially available powered wheelchair (PWC) platform avoids a host of seating, mechanical design and reliability issues but requires methods of predicting and controlling the motion of a device never intended for robotics. Analog joystick inputs are subject to black-box transformations which may produce intuitive and adaptable motion control for human operators, but complicate robotic control approaches; furthermore, installation of standard axle mounted odometers on a commercial PWC is difficult. In this work, we present an integrated hardware and software system for predicting the motion of a commercial PWC platform that does not require any physical or electronic modification of the chair beyond plugging into an industry standard auxiliary input port. This system uses an RGB-D camera and an Arduino interface board to capture motion data, including visual odometry and joystick signals, via ROS communication. Future motion is predicted using an autoregressive sparse Gaussian process model. We evaluate the proposed system on real-world short-term path prediction experiments. Experimental results demonstrate the system's efficacy when compared to a baseline neural network model.",https://ieeexplore.ieee.org/document/8460502/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1997.633274,Learning and reasoning method using fuzzy coloured Petri nets under uncertainty,IEEE,Conferences,"Petri nets have been widely used to model computer systems. Manufacturing systems, robotics systems, knowledge-based systems, and other kinds of engineering applications. Further, to present complex real-world knowledge fuzzy Petri net models have been proposed to perform fuzzy reasoning automatically. However, in Petri nets one has to represent all kinds of processes by separate subnets even though the process has the same behavior as another. Real-world knowledge often contains many parts which are similar, but not identical. This means that the total number of Petri nets becomes very large. Therefore, it becomes difficult to see the similarities and the differences among the individual subnets representing similar parts. The problems may be annoying for a small system, and catastrophic for the description of a large-scale system. To avoid this kind of problem the authors propose a learning and reasoning method using fuzzy coloured Petri nets (FCPN) under uncertainty. For the correction of rules of the knowledge-based system a hand-built classifier and empirical learning method based on domain theory have been proposed as machine learning methods, where there is a significant gap between the knowledge-intensive approach in the former and the virtually knowledge-free approach in the letter. To resolve such problems simultaneously they propose a hybrid learning method which is built on top of the knowledge-based fuzzy coloured Petri net and genetic algorithms.",https://ieeexplore.ieee.org/document/633274/,"1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation",12-15 Oct. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC.1992.4792313,Learning for Skill Acquisition and Refinement: Toward Exploring Everyday Physics,IEEE,Conferences,"The present talk claims that ""robotics"" is not a test bed for AI but should involve a research frontier, which attempts to account for intelligibility of everyday physics underlying human activities such as perception, remembrance, planning, practices, and skill. In addition to traditional AI and neuro-network approaches, more of new domains that can account for any aspect of human intellectual behaviors must be exploited, and also more of new tools that actualize real implementation of intelligence in machines need to be devised. To aim at going on an expedition in this direction, this talk introduces one new domain and another new tool. The former is practice-based learning for skill refinement and the latter is a design tool of signal-based structured information base for skill acquirement.",https://ieeexplore.ieee.org/document/4792313/,1992 American Control Conference,24-26 June 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1991.174419,Learning for skill refinement,IEEE,Conferences,"It is claimed that 'robotics' is not a test bed for AI but should involve a research frontier relating to the physics underlying human activities such as perception, remembering, planning, practice, and skill. In addition to traditional AI and neural network approaches, other domains that can account for any aspect of human intellectual behavior must be exploited, and tools that actualize real implementation of intelligence in machines need to be devised. A practice-based learning domain for skill refinement and a design tool for a signal-based structured information base for skill acquisition are presented.<>",https://ieeexplore.ieee.org/document/174419/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCIC.2015.7435795,Learning mechanism for RT task scheduling,IEEE,Conferences,"The fascinations of Internet of Things (IoT) necessitate a large number of devices are to be integrated with the existing IoT. These devices are very difficult to manage in a large distributed environment without a careful management design. These location based devices generate data at fixed intervals of time and need configure these devices to software platform to analyze data and understand environment in better way. So, learning capability should incorporate within the system as the environment of system changes dynamically. As the Internet of Things continues to develop, further potential is estimated by a combination with related technology approaches and concepts such as Cloud Computing, Future Internet, Big Data, Robotics and Semantic Technologies. The idea is becomes now evident as those related concepts have started to reveal synergies by combining them.",https://ieeexplore.ieee.org/document/7435795/,2015 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC),10-12 Dec. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811592,Learning to Detect Slip with Barometric Tactile Sensors and a Temporal Convolutional Neural Network,IEEE,Conferences,"The ability to perceive object slip via tactile feedback enables humans to accomplish complex manipulation tasks including maintaining a stable grasp. Despite the utility of tactile information for many applications, tactile sensors have yet to be widely deployed in industrial robotics settings; part of the challenge lies in identifying slip and other events from the tactile data stream. In this paper, we present a learning-based method to detect slip using barometric tactile sensors. These sensors have many desirable properties including high durability and reliability, and are built from inexpensive, off-the-shelf components. We train a temporal convolution neural network to detect slip, achieving high detection accuracies while displaying robustness to the speed and direction of the slip motion. Further, we test our detector on two manipulation tasks involving a variety of common objects and demonstrate successful generalization to real-world scenarios not seen during training. We argue that barometric tactile sensing technology, combined with data-driven learning, is suitable for many manipulation tasks such as slip compensation.",https://ieeexplore.ieee.org/document/9811592/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812166,Legged Robots that Keep on Learning: Fine-Tuning Locomotion Policies in the Real World,IEEE,Conferences,"Legged robots are physically capable of traversing a wide range of challenging environments, but designing controllers that are sufficiently robust to handle this diversity has been a long-standing challenge in robotics. Reinforcement learning presents an appealing approach for automating the controller design process and has been able to produce remarkably robust controllers when trained in a suitable range of environments. However, it is difficult to predict all likely conditions the robot will encounter during deployment and enumerate them at training-time. What if instead of training controllers that are robust enough to handle any eventuality, we enable the robot to continually learn in any setting it finds itself in? This kind of real-world reinforcement learning poses a number of challenges, including efficiency, safety, and autonomy. To address these challenges, we propose a practical robot reinforcement learning system for fine-tuning locomotion policies in the real world. We demonstrate that a modest amount of real-world training can substantially improve performance during deployment, and this enables a real A1 quadrupedal robot to autonomously fine-tune multiple locomotion skills in a range of environments, including an outdoor lawn and a variety of indoor terrains. (Videos and code<sup>1</sup><sup>1</sup>https://sites.google.com/berkele.edu/fine-tuning-locomotion)",https://ieeexplore.ieee.org/document/9812166/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2019.8803544,Lightweight Monocular Depth Estimation Model by Joint End-to-End Filter Pruning,IEEE,Conferences,"Convolutional neural networks (CNNs) have emerged as the state-of-the-art in multiple vision tasks including depth estimation. However, memory and computing power requirements remain as challenges to be tackled in these models. Monocular depth estimation has significant use in robotics and virtual reality that requires deployment on low-end devices. Training a small model from scratch results in a significant drop in accuracy and it does not benefit from pre-trained large models. Motivated by the literature of model pruning, we propose a lightweight monocular depth model obtained from a large trained model. This is achieved by removing the least important features with a novel joint end-to-end filter pruning. We propose to learn a binary mask for each filter to decide whether to drop the filter or not. These masks are trained jointly to exploit relations between filters at different layers as well as redundancy within the same layer. We show that we can achieve around 5x compression rate with small drop in accuracy on the KITTI driving dataset. We also show that masking can improve accuracy over the baseline with fewer parameters, even without enforcing compression loss.",https://ieeexplore.ieee.org/document/8803544/,2019 IEEE International Conference on Image Processing (ICIP),22-25 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SPA.2019.8936736,Machine Learning for Embodied Agents: From Signals to Symbols and Actions,IEEE,Conferences,"The aim of this tutorial lecture is to show the role of machine learning and some other AI-related techniques in embodied autonomous agents, and autonomous robots in particular. In this tutorial we bring to the forefront the aspects of robotics that are closely related to computer science. We believe that the progress in algorithms and data processing methods together with the rapid increase in the available computing power were the driving forces behind the successes of modern robotics in the last decade. During this period robots of various classes migrated from university laboratories to commercial companies and then to our everyday life, as now everybody can buy an autonomous vacuum cleaner or lawnmower, while self-driving cars and drones for goods delivery are waiting for proper legal regulations to enter the market. Robotics and Artificial Intelligence already went a long path of mutual inspiration and common development, starting from the symbolic AI (aka Good Old-Fashioned Artificial Intelligence) and its extensive use in early autonomous robots, such as Shakey the robot, created in SRI International by Nils Nilsson, considered one of the ""fathers"" of modern AI. We briefly characterize the range of the most important applications of typical AI methods in modern robotics, including motion planning algorithms [2,3], interpretation of sensory data leading to creation of a world model [4 ,5], and classical learning methods, such as reinforcement learning [6]. However, what made robotics a part of the new wave of AI applications was the recent ""revolution"" of machine learning, mostly grounded in the enormous success of the deep learning paradigm and its many variants that proved to outclass classic methods in a broad range of problems related to the processing of images and other types of signals. The quick adoption of the recent advances in Machine Learning (ML) in robotics seems to be motivated by the fact that ML gives the possibility to infer solutions from data, as opposed to the classic model-based paradigm that was for decades used in robotics. Whereas the modelbased solutions are mathematically elegant and theoretically provable (with respect to stability, convergence, etc.) they often fail once confronted with real-world problems and real sensory data, as their underlying mathematical models are only a very rough approximation of the real world. Therefore, a wider adoption of ML in robotics gives a chance to make robots more robust and adaptive. On the other hand, we should try to use the new techniques without discarding the knowledge and expertise we already have - machine learning methods can benefit a lot from the prior knowledge and the known structure of the problem that has to be solved by learning. This knowledge and structure can be adopted from the model-based methods that a re already well-established in robotics. In the lecture robots are understood in a broad sense, as all embodied agents that have means to physically interact with the environment. They can be either manipulators, mobile robots, aerial vehicles, self-driving cars, and various ""smart"" devices and sensors. In the second part of the lecture attention is paid to specific problems that appear in application of machine learning to embodied agents, such as the need to search a for solution in huge, multi-dimensional spaces (""curse of dimensionality""), and the ever-present problem of representation and incorporation of uncertainty in the processing of real-world data. Some examples of applications of autonomous robots are given, which were successful due to the use of AI - in particular the probabilistic representation of knowledge and machine learning. The most prominent examples are the DARPA competitions: ""Grand Challenge"", ""Urban Challenge"" and ""Robotics Challenge"" (DRC), and the ""Amazon Picking Challenge"", which proves the interest of large corporations in the development of AI-based robotics [7]. In the third part of the lecture new research directions offered by machine learning and the increased availability of training data are discussed. An overview of the most popular application areas of ML in robotics and other autonomous systems is presented along with the typical machine learning paradigms applied in these areas. The focus is on deep learning, mostly using convolutional neural networks to process various sensory data. We discuss three aspects of embodied agents that make machine learning in robotics quite specific with respect to other application areas, such as medical images or natural language processing. The first aspect is dealing with the ""open world"", in which autonomous robots usually operate. This situation breaks the assumptions underlying some popular ML methods, and creates the need to face the problem of unknown classes identification [8] incremental learning [9], and the uncertainty of sensory data [10]. We also stress out that an embodied agent has the ability to actively acquire information [11]. The second aspect is the inference about the scene seen by the agent, where in the case of robotics, semantics and geometry intermingle [12], because the robot has to work in a three-dimensional world, although it often perceives it through twodimensional images [13,14]. The third aspect of our analysis is related to the most important feature of robots that distinguishes them from all other learning agents (software-based). Robots are embodied agents, that is they have a physical ""body"", and are subject to physical constraints, such as the maximum speed of motion or maximum range of perception. Therefore, in ML for robots analysis of the spatio-temporal dependencies in data is very important [15]. Robots support advanced learning methods thanks to the possibility of interaction with the environment - a simple example is active vision with moving camera, a much more complex one is manipulation with active testing of the behavior of objects (repositioning, pushing) [16]. At the end of the lecture, in the context of specific needs and limitations characteristic to the applications of ML in robotics, new concepts of machine learning (e.g. deep reinforcement learning [17], interactive perception [18]) are presented. The lecture is summarized with a brief discussion of the most important challenges and open problems of ML applied to embodied agents.",https://ieeexplore.ieee.org/document/8936736/,"2019 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA)",18-20 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPRW53098.2021.00141,MarkerPose: Robust Real-time Planar Target Tracking for Accurate Stereo Pose Estimation,IEEE,Conferences,"Despite the attention marker-less pose estimation has attracted in recent years, marker-based approaches still provide unbeatable accuracy under controlled environmental conditions. Thus, they are used in many fields such as robotics or biomedical applications but are primarily implemented through classical approaches, which require lots of heuristics and parameter tuning for reliable performance under different environments. In this work, we propose MarkerPose, a robust, real-time pose estimation system based on a planar target of three circles and a stereo vision system. MarkerPose is meant for high-accuracy pose estimation applications. Our method consists of two deep neural networks for marker point detection. A SuperPoint-like network for pixel-level accuracy keypoint localization and classification, and we introduce EllipSegNet, a lightweight ellipse segmentation network for sub-pixel-level accuracy keypoint detection. The marker’s pose is estimated through stereo triangulation. The target point detection is robust to low lighting and motion blur conditions. We compared MarkerPose with a detection method based on classical computer vision techniques using a robotic arm for validation. The results show our method provides better accuracy than the classical technique. Finally, we demonstrate the suitability of MarkerPose in a 3D freehand ultrasound system, which is an application where highly accurate pose estimation is required. Code is available in Python and C++ at https://github.com/jhacsonmeza/MarkerPose.",https://ieeexplore.ieee.org/document/9523117/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),19-25 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCI50826.2021.9402304,Maze Solving with humanoid robot NAO using Real-Time object detection,IEEE,Conferences,"In a future that is not too distant from today, humanoids are surely going to be an integral part of both our professional and private lives, assisting us with various tasks. Unlike normal robots that we may encounter in our everyday lives, humanoids are designed in specific manners to give them more human-like capabilities that enable them to perform complex tasks such as climbing a flight of stairs. In this paper, we present a Maze-Solving Algorithm which is a software developed specifically for the humanoid robot, NAO, and gives it the capability to enter and exit a maze autonomously. NAO is a next-gen humanoid bot developed by SoftBank Robotics using the power of AI. The bot is equipped with numerous sensors and cameras. Though various quantitative approaches were considered and experimented with, we stuck onto the one which had the least average time complexity of all after a thorough comparative study. We suggest an approach where the humanoid can detect and localize objects from a distance and take programmable decisions based on them. AI constantly tries to give robots human-thinking capabilities to make their decision-making skills similar to those of humans, if not better than them. This algorithm was developed taking into consideration how a human intellect would react rationally if he is stuck in a maze. The methodology used revolves primarily around the combined use of SONAR(Sound navigation ranging) and tactical sensors, and cameras equipped within the bot. The output values from this hardware were then evaluated to judge the distance from a wall and the reactions from the bot were calculated by the suggested algorithm accordingly.",https://ieeexplore.ieee.org/document/9402304/,2021 International Conference on Computer Communication and Informatics (ICCCI),27-29 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANS.1998.724376,Model development of an underwater manipulator for coordinated arm-vehicle control,IEEE,Conferences,"This paper presents research on the hydrodynamic modeling of a manipulator for an autonomous underwater scientific vehicle. The focus is on improving the modeling accuracy of the in-line hydrodynamic coupling between a two-link manipulator and a small, free-floating vehicle in order to achieve better control for coordinated motion of the combined system. Loads predicted using existing models for underwater arms were determined to be off by as much as 25% when applied to a real, two-link arm in a test tank. In this new approach, an experimentally-determined model has been developed that takes into account the 3D flow effects that have previously not been included. The end result is a model that provides accurate predictions for the joint torques of a two-link arm in a form simple enough to be implemented in algorithms for precision planning and control. This project is part of a joint program between the Aerospace Robotics Laboratory at Stanford University and the Monterey Bay Aquarium Research Institute.",https://ieeexplore.ieee.org/document/724376/,IEEE Oceanic Engineering Society. OCEANS'98. Conference Proceedings (Cat. No.98CH36259),28 Sept.-1 Oct. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1996.503841,Modeling hybrid systems as the limit of discrete computational processes,IEEE,Conferences,This paper outlines a new formalism for hybrid systems in which continuous dynamics are represented as the limit of a discrete computational process. Hybrid systems are systems which are composed of continuous and discrete components. Such systems often arise in robotics where operations in the continuous real world are controlled by a discrete software system. Most existing hybrid formalisms treat their continuous and discrete components as distinct types of process. This new approach brings the two together and provides a platform for the investigation of the transition between continuous change and discrete events.,https://ieeexplore.ieee.org/document/503841/,Proceedings of IEEE International Conference on Robotics and Automation,22-28 April 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EHB52898.2021.9657633,Modular Telepresence Robot for Distance Medical Education,IEEE,Conferences,"In special conditions such as pandemics, wars or any other problem can lead to the physical suspension of the education system including courses in medical schools. Telepresence robotics is a solution through which courses and laboratories can be held remotely where medical students can interact physically, through robots, with teaching or laboratory equipment, thus actively participating in the educational process. Although there are various variants of telepresence robots, this paper presents the study of the possibility to design and make a telepresence robot kit, low cost, containing modules easy to assemble and adapted for various teaching situations, respectively for the development of remote laboratory experiments. The proposed kit has mechanical components that can be printed on a 3D printer. The electronic components are compatible with the Arduino environment and include three modules: sensory, navigation and a communications and artificial intelligence module. The software elements are also modularly designed and allow adaptation to various study and laboratory situations. Robotic elements compatible with the Arduino environment such as arms or lifting or throwing systems can be added to the robot. The results of the experimental measurements showed that the robot has acceptable travel errors for unpretentious applications, but to increase its accuracy in laboratory conditions, the software was programmed so that, with the help of artificial intelligence, the robot can perform real-time measurements and correct automatically. This mechanism is useful in medical applications such as assisting and guiding blind patients.",https://ieeexplore.ieee.org/document/9657633/,2021 International Conference on e-Health and Bioengineering (EHB),18-19 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636224,Moving Forward in Formation: A Decentralized Hierarchical Learning Approach to Multi-Agent Moving Together,IEEE,Conferences,"Multi-agent path finding in formation has many potential real-world applications like mobile warehouse robotics. However, previous multi-agent path finding (MAPF) methods hardly take formation into consideration. Further-more, they are usually centralized planners and require the whole state of the environment. Other decentralized partially observable approaches to MAPF are reinforcement learning (RL) methods. However, these RL methods encounter difficulties when learning path finding and formation problems at the same time. In this paper, we propose a novel decentralized partially observable RL algorithm that uses a hierarchical structure to decompose the multi-objective task into unrelated ones. It also calculates a theoretical weight that makes each tasks reward has equal influence on the final RL value function. Additionally, we introduce a communication method that helps agents cooperate with each other. Experiments in simulation show that our method outperforms other end-to-end RL methods and our method can naturally scale to large world sizes where centralized planner struggles. We also deploy and validate our method in a real-world scenario.",https://ieeexplore.ieee.org/document/9636224/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2016.7850240,Multi-Channel Bayesian ART for robot fusion perception,IEEE,Conferences,"Multiple sensor data fusion is the technique of associate information from a number of different sensors to produce a robust and comprehensive description. Data fusion pose is using in various robotics application such as environment mapping, object recognition and robot localization. Their relation is generally hard coded and difficult to learn incrementally if new objects or events arise. In this paper, we propose a new learning architecture termed as Multi-Channel Bayesian ART which is very flexible can be adapted to new domains or different sensor configurations easily. The other advantages of the proposed method are: (1) it is capable of incremental on-line learning without forgetting previously-learned knowledge (2) It can process data real time and does not require any prior training to make it work in natural environment. The effectiveness of our proposed method is validated by real experimental results implemented on robot.",https://ieeexplore.ieee.org/document/7850240/,2016 IEEE Symposium Series on Computational Intelligence (SSCI),6-9 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI50040.2020.00088,Multi-Robot Collision Avoidance with Map-based Deep Reinforcement Learning,IEEE,Conferences,"Multi-robot collision avoidance in a communication-free environment is one of the key issues for mobile robotics and autonomous driving. In this paper, we propose a map-based deep reinforcement learning (DRL) approach for collision avoidance of multiple robots, where robots do not communicate with each other and only sense other robots' positions and the obstacles around them. We use the egocentric grid map of a robot to represent the environmental information around it, which can be easily generated by using multiple sensors or sensor fusion. The learned policy generated from the DRL model directly maps 3 frames of egocentric grid maps and the robot's relative local goal positions into low-level robot control commands. We first train a convolutional neural network for the navigation policy in a simulator of multiple mobile robots using proximal policy optimization (PPO). Then we deploy the trained model to real robots to perform collision avoidance in their navigation. We evaluate the approach with various scenarios both in the simulator and on three differential-drive mobile robots in the real world. Both qualitative and quantitative experiments show that our approach is efficient with a high success rate. The demonstration video can be found at https://youtu.be/jcLKlEXuFuk.",https://ieeexplore.ieee.org/document/9288300/,2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI),9-11 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO52101.2021.9596665,Multi-Stakeholder Engagement in Agile Service Platform Co-Creation,IEEE,Conferences,"In the recent decade, the concept of Smart Tourism Destination (STD) has emerged to represent both a strategic aim to develop sustainable competitive advantages for tourism destinations or wider regions, and a managerial approach and dimensions to enhance data-driven development to measure and apply smart technologies for competitive and inclusive change in a tourism ecosystem. In tourism destination development globally, the aim for sustainable digital transformation in low-productivity business sectors is one of the key drivers for smart tourism and smart destination development. More recently, the global Covid-19 pandemic has given a boost to touchless, digital and green tourism development initiatives in the EU and worldwide. To advance sustainable digital transformation with agile methods in STD development, a systemic service platform approach and agile prototyping with smart emerging technologies, (eg. with AI, IoT, Augmented & Virtual Reality, 5G and Robotics), and multi-stakeholder co-creation is needed to engage key tourism industry ecosystem representatives. This conceptual paper advocates that open-source solutions combined with comprehensive multi-stakeholder co-creation aid in prototyping a systemic digital platform solution for smart tourism destinations. The paper concludes with an illustration of a conceptual model of service platform development for smart destinations, utilizing network co-creation with quadruple-helix stakeholders for sustainable regional impacts.",https://ieeexplore.ieee.org/document/9596665/,"2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO)",27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341372,Multiplicative Controller Fusion: Leveraging Algorithmic Priors for Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer,IEEE,Conferences,"Learning-based approaches often outperform hand-coded algorithmic solutions for many problems in robotics. However, learning long-horizon tasks on real robot hardware can be intractable, and transferring a learned policy from simulation to reality is still extremely challenging. We present a novel approach to model-free reinforcement learning that can leverage existing sub-optimal solutions as an algorithmic prior during training and deployment. During training, our gated fusion approach enables the prior to guide the initial stages of exploration, increasing sample-efficiency and enabling learning from sparse long-horizon reward signals. Importantly, the policy can learn to improve beyond the performance of the sub-optimal prior since the prior's influence is annealed gradually. During deployment, the policy's uncertainty provides a reliable strategy for transferring a simulation-trained policy to the real world by falling back to the prior controller in uncertain states. We show the efficacy of our Multiplicative Controller Fusion approach on the task of robot navigation and demonstrate safe transfer from simulation to the real world without any fine-tuning. The code for this project is made publicly available at https://sites.google.com/view/mcf-nav/home.",https://ieeexplore.ieee.org/document/9341372/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS.2016.7502665,Natural user interfaces for human-drone multi-modal interaction,IEEE,Conferences,"Personal drones are becoming part of every day life. To fully integrate them into society, it is crucial to design safe and intuitive ways to interact with these aerial systems. The recent advances on User-Centered Design (UCD) applied to Natural User Interfaces (NUIs) intend to make use of human innate features, such as speech, gestures and vision to interact with technology in the way humans would with one another. In this paper, a Graphical User Interface (GUI) and several NUI methods are studied and implemented, along with computer vision techniques, in a single software framework for aerial robotics called Aerostack which allows for intuitive and natural human-quadrotor interaction in indoor GPS-denied environments. These strategies include speech, body position, hand gesture and visual marker interactions used to directly command tasks to the drone. The NUIs presented are based on devices like the Leap Motion Controller, microphones and small size monocular on-board cameras which are unnoticeable to the user. Thanks to this UCD perspective, the users can choose the most intuitive and effective type of interaction for their application. Additionally, the strategies proposed allow for multi-modal interaction between multiple users and the drone by being able to integrate several of these interfaces in one single application as is shown in various real flight experiments performed with non-expert users.",https://ieeexplore.ieee.org/document/7502665/,2016 International Conference on Unmanned Aircraft Systems (ICUAS),7-10 June 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEE.2018.8472657,Neural Control of Mobile Robot Motion Based on Feedback Error Learning and Mimetic Structure,IEEE,Conferences,"Mobile robots motion control is a basic problem in robotics and there are still some control difficulties such as uncertainty in a real implementation which should be considered. This paper is concerned with the neural control of wheeled mobile robots trajectory tracking and posture stabilization. In the trajectory-tracking problem, the Feedback Error Learning (FEL) structure is used and for the posture stabilization problem, the Mimetic structure is employed. These neural based structures use a classic controller, Dynamic Feedback Linearization (DFL), and help to improve the adaptiveness of it. The effectiveness of the proposed controllers is verified by simulation in Webots robotic simulator and on the e-puck which is a differential wheeled mobile robot. The simulation results verify the ability of the proposed methods for controlling the robot and handling uncertainties.",https://ieeexplore.ieee.org/document/8472657/,"Electrical Engineering (ICEE), Iranian Conference on",8-10 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM.2009.5229901,Neural Q-Learning controller for mobile robot,IEEE,Conferences,"In recent years, increasing trend in application of autonomous mobile robot worldwide has highlighted the importance of path planning controller in robotics-related fields, especially where dynamic and unknown environment is involved. Writing a good robot controller program can be a very time consuming process. It is inevitably wasting of resources and efforts if we have to rewrite the controller over and over again whenever there is emergence of changes in the environment. Reinforcement Learning (RL) algorithms and Artificial Neural Network (ANN) are used to assist autonomous mobile robot to learn in an unrecognized environment. This research study is focused on exploring integration of multi-layer neural network and Q-Learning as an online learning controller. Learning process is divided into two stages. In the initial stage the agent will map the environment through collecting state-action information according to the Q-Learning procedure. Second training process involves neural network training which will utilize the state-action information gathered in earlier phase as training samples. During final application of the controller, Q-Learning would be used as the primary navigating tool whereas the trained neural network will be employed when approximation is needed. MATLAB simulation was developed to verify the validity of the algorithm before it is real-time implemented on the real world using Team AmigoBot™ robot. The results obtained from both simulation and actual application confirmed on-spot learning ability of the controller accompanied with certain degree of flexibility and robustness.",https://ieeexplore.ieee.org/document/5229901/,2009 IEEE/ASME International Conference on Advanced Intelligent Mechatronics,14-17 July 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OPTIM.2008.4602496,Neural control based on RBF network implemented on FPGA,IEEE,Conferences,"The RBF radial basis function network is intended especially for hardware implementation and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA has several benefits, with emphasis on parallelism and the real time capabilities. This paper discusses the hardware implementation of the RBF type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor.",https://ieeexplore.ieee.org/document/4602496/,2008 11th International Conference on Optimization of Electrical and Electronic Equipment,22-24 May 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISESD.2016.7886710,Neural network implementation for invers kinematic model of arm drawing robot,IEEE,Conferences,"Nowadays, the research in robotics field is growing. One of the studies in robotics is the control method of the robotic arm movement. In this research, a 3 DOF arm drawing robot was built. An inverse kinematic models of the robot arm is made using artificial neural network method. Artificial neural network model was implemented in a GUI application. The ANN model can work in real-time to control arm robot movement to reach certain coordinates. Based on test results, the inverse kinematic models of the arm drawing robot had an error rate under 2%. It is of 0.16% for X coordinate and 0.46% for Y coordinate.",https://ieeexplore.ieee.org/document/7886710/,2016 International Symposium on Electronics and Smart Devices (ISESD),29-30 Nov. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SOFA.2009.5254883,Neurodynamic optimization with its application for model predictive control,IEEE,Conferences,"Summary form only given. Optimization problems arise in a wide variety of scientific and engineering applications. It is computationally challenging when optimization procedures have to be performed in real time to optimize the performance of dynamical systems. For such applications, classical optimization techniques may not be competent due to the problem dimensionality and stringent requirement on computational time. One very promising approach to dynamic optimization is to apply artificial neural networks. Because of the inherent nature of parallel and distributed information processing in neural networks, the convergence rate of the solution process is not decreasing as the size of the problem increases. Neural networks can be implemented physically in designated hardware such as ASICs where optimization is carried out in a truly parallel and distributed manner. This feature is particularly desirable for dynamic optimization in decentralized decision-making situations arising frequently in control and robotics. In this talk, the author presents the historic review and the state of the art of neurodynamic optimization models and selected applications in robotics and control. Specifically, starting from the motivation of neurodynamic optimization, we will review various recurrent neural network models for optimization. Theoretical results about the stability and optimality of the neurodynamic optimization models will be given along with illustrative examples and simulation results. It will be shown that many problems in control systems, such model predictive control, can be readily solved by using the neurodynamic optimization models. Specifically, linear and nonlinear model predictive control based on neurodynamic optimization will be delineated.",https://ieeexplore.ieee.org/document/5254883/,2009 3rd International Workshop on Soft Computing Applications,29 July-1 Aug. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NGCT.2015.7375178,Neuronal Logic gates realization using Vedic mathematics,IEEE,Conferences,"Gates are the fundamental building block of all logic circuits. Artificial neural networks (ANN) have processing capabilities in a parallel architecture, and due to this they are useful in applications like pattern recognition, system identification, prediction problems, robotics, and control problems. Boolean logic realization using artificial neural network is known as Neuronal Logic. Simple and low precision computations are the basic requirements of ANN which can be performed faster. This can be implemented on cheap and low precision hardware. Neural network involves enormous number of multiplication and addition calculations. It has been already proved that multipliers based on Vedic mathematics are faster in speed than the standard multipliers. In this paper, the possibility of hardware realization of neuronal logic gates using Vedic multipliers herein referred to as Vedic neuron has been explored. This is achieved by performing the neural network computations using Vedic mathematics rather than the conventional multiplication process. Basic logic gates like AND, OR and AND-NOT have been studied and its hardware implementation using neural network has been simulated using VHDL. A comparative study was carried out on the computation speed of neuronal logic gates implemented using conventional multipliers as well as neuronal logic gates implemented using Vedic multipliers. The increase in processing speed with Vedic neuron implementation has been observed which can be of use in several real time operations where speed is critical.",https://ieeexplore.ieee.org/document/7375178/,2015 1st International Conference on Next Generation Computing Technologies (NGCT),4-5 Sept. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNN53494.2021.9580216,Neuropunk revolution and its implementation via real-time neurosimulations and their integrations,IEEE,Conferences,"In this paper I present the perspectives of the “neuropunk revolution” technologies. One could understand the “neuropunk revolution” as the integration of real-time neurosimulations into biological nervous/motor system via neurostimulation or artificial robotic systems via integration with actuators. I see the added value of the real-time neurosimulations as bridge technology for the set of developed technologies: BCI, neuroprosthetics, AI, robotics to provide bio-compatible integration into biological or artificial limbs. Here I present the three types of integration of the “neuropunk revolution” technologies as inbound, outbound and closed-loop in-outbound systems. I see the shift of the perspective how we see now the set of technologies including AI, BCI, neuroprosthetics and robotics due to the proposed concept for example the integration of external to a body simulated part of nervous system back into the biological nervous system or muscles.",https://ieeexplore.ieee.org/document/9580216/,2021 Third International Conference Neurotechnologies and Neurointerfaces (CNN),13-15 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2018.8625038,NimbRo-OP2X: Adult-Sized Open-Source 3D Printed Humanoid Robot,IEEE,Conferences,"Humanoid robotics research depends on capable robot platforms, but recently developed advanced platforms are often not available to other research groups, expensive, dangerous to operate, or closed-source. The lack of available platforms forces researchers to work with smaller robots, which have less strict dynamic constraints or with simulations, which lack many real-world effects. We developed NimbRo-OP2X to address this need. At a height of 135 cm our robot is large enough to interact in a human environment. Its low weight of only 19kg makes the operation of the robot safe and easy, as no special operational equipment is necessary. Our robot is equipped with a fast onboard computer and a GPU to accelerate parallel computations. We extend our already open-source software by a deep-learning based vision system and gait parameter optimisation. The NimbRo-OP2X was evaluated during RoboCup 2018 in Montreál, Canada, where it won all possible awards in the Humanoid AdultSize class.",https://ieeexplore.ieee.org/document/8625038/,2018 IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids),6-9 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CLEI53233.2021.9639989,ODROM: Object Detection and Recognition supported by Ontologies and applied to Museums,IEEE,Conferences,"In robotics, object detection in images or videos, obtained in real-time from sensors of robots can be used to support the implementation of service robot tasks (e.g., navigation, model its social behavior, recognize objects in a specific domain), usually accomplished in indoor environments. However, traditional deep learning based object detection techniques present limitations in such indoor environments, specifically related to the detection of small objects and the management of high density of multiple objects. Coupled with these limitations, for specific domains (e.g., hospitals, museums), it is important that the robot, apart from detecting objects, extracts and knows information of the targeted objects. Ontologies, as a part of the Semantic Web, are presented as a feasible option to formally represent the information related to the objects of a particular domain. In this context, this work proposes an object detection and recognition process based on a Deep Learning algorithm, object descriptors, and an ontology. ODROM, an Object Detection and Recognition algorithm supported by Ontologies and applied to Museums, is an implementation to validate the proposal. Experiments show that the usage of ontologies is a good way of desambiguating the detection, obtained with a and $\mathbf{mAP}{@}0.5=0.88$ and a $\mathbf{mAP}{@}[0.5:0.95]=61\%$.",https://ieeexplore.ieee.org/document/9639989/,2021 XLVII Latin American Computing Conference (CLEI),25-29 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV48922.2021.01064,ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition,IEEE,Conferences,"Object recognition has made great advances in the last decade, but predominately still relies on many high-quality training examples per object category. In contrast, learning new objects from only a few examples could enable many impactful applications from robotics to user personalization. Most few-shot learning research, however, has been driven by benchmark datasets that lack the high variation that these applications will face when deployed in the real-world. To close this gap, we present the ORBIT dataset and benchmark, grounded in the real-world application of teachable object recognizers for people who are blind/low-vision. The dataset contains 3,822 videos of 486 objects recorded by people who are blind/low-vision on their mobile phones. The benchmark reflects a realistic, highly challenging recognition problem, providing a rich playground to drive research in robustness to few-shot, high-variation conditions. We set the benchmark&#x2019;s first state-of-the-art and show there is massive scope for further innovation, holding the potential to impact a broad range of real-world vision applications including tools for the blind/low-vision community. We release the dataset at https://doi.org/10.25383/city.14294597 and benchmark code at https://github.com/microsoft/ORBIT-Dataset.",https://ieeexplore.ieee.org/document/9711109/,2021 IEEE/CVF International Conference on Computer Vision (ICCV),10-17 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRC.2019.00061,"ORC—A Lightweight, Lightning-Fast Middleware",IEEE,Conferences,"Robotic tasks are commonly solved by integrating numerous different software and hardware modules into one working application. The necessary integration work typically contributes a considerable share of the total work required for a project, which is why past research on robotics computing has pushed towards generating higher-level abstraction layers, like middlewares. However, the current state-of-the-art cannot provide reliable, low-latency communication performance as we will show in the experimental evaluation. In this paper we propose the Open Robot Communication framework (ORC). Compared to previous middlewares, ORC is lightweight and geared towards applications with high-performance requirements. We consider ORC especially useful for applications with Human Robot Interaction or collaborative tasks involving multiple robots. In the paper, we compare the runtime performance of ORC to the robot operating system (ROS). We can show that ORC enables message transfer with delays far below one millisecond and we demonstrate the real-time capabilities of ORC in a force-control task implemented in Python.",https://ieeexplore.ieee.org/document/8675625/,2019 Third IEEE International Conference on Robotic Computing (IRC),25-27 Feb. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2016.7487351,Object discovery and grasp detection with a shared convolutional neural network,IEEE,Conferences,"Grasp an object from a stack of objects in real-time is still a challenge in robotics. This requires the robot to have the ability of both fast object discovery and grasp detection: a target object should be picked out from the stack first and then a proper grasp configuration is applied to grasp the object. In this paper, we propose a shared convolutional neural network (CNN) which can simultaneously implement these two tasks in real-time. The processing speed of the model is about 100 frames per second on a GPU which largely satisfies the requirement. Meanwhile, we also establish a labeled RGBD dataset which contains scenes of stacked objects for robotic grasping. At last, we demonstrate the implementation of our shared CNN model on a real robotic platform and show that the robot can accurately discover a target object from the stack and successfully grasp it.",https://ieeexplore.ieee.org/document/7487351/,2016 IEEE International Conference on Robotics and Automation (ICRA),16-21 May 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759720,Object identification from few examples by improving the invariance of a Deep Convolutional Neural Network,IEEE,Conferences,"The development of reliable and robust visual recognition systems is a main challenge towards the deployment of autonomous robotic agents in unconstrained environments. Learning to recognize objects requires image representations that are discriminative to relevant information while being invariant to nuisances, such as scaling, rotations, light and background changes, and so forth. Deep Convolutional Neural Networks can learn such representations from large web-collected image datasets and a natural question is how these systems can be best adapted to the robotics context where little supervision is often available. In this work, we investigate different training strategies for deep architectures on a new dataset collected in a real-world robotic setting. In particular we show how deep networks can be tuned to improve invariance and discriminability properties and perform object identification tasks with minimal supervision.",https://ieeexplore.ieee.org/document/7759720/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2009.4913200,Object orientation recognition based on SIFT and SVM by using stereo camera,IEEE,Conferences,"The goal of this research is to recognize an object and its orientation in space by using stereo camera. The principle of object orientation recognition in this paper was based on the scale invariant feature transform (SIFT) and support vector machine (SVM). SIFT has been successfully implemented on object recognition but it had a problem recognizing the object orientation. For many autonomous robotics applications, such as using a vision-guided industrial robot to grab a product, not only correct object recognition will be needed in this process but also object orientation recognition is required. In this paper we used SVM to recognize object orientation. SVM has been known as a promising method for classification accuracy and its generalization ability. The stereo camera system adopted in this research provided more useful information compared to single camera one. The object orientation recognition technique was implemented on an industrial robot in a real application. The proposed camera system and recognition algorithms were used to recognize a specific object and its orientation and then guide the industrial robot to perform some alignment operations on the object.",https://ieeexplore.ieee.org/document/4913200/,2008 IEEE International Conference on Robotics and Biomimetics,22-25 Feb. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635977,On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents,IEEE,Conferences,"In many situations it is either impossible or impractical to develop and evaluate agents entirely on the target domain on which they will be deployed. This is particularly true in robotics, where doing experiments on hardware is much more arduous than in simulation. This has become arguably more so in the case of learning-based agents. To this end, considerable recent effort has been devoted to developing increasingly realistic and higher fidelity simulators. However, we lack any principled way to evaluate how good a ""proxy domain"" is, specifically in terms of how useful it is in helping us achieve our end objective of building an agent that performs well in the target domain. In this work, we investigate methods to address this need. We begin by clearly separating two uses of proxy domains that are often conflated: 1) their ability to be a faithful predictor of agent performance and 2) their ability to be a useful tool for learning. In this paper, we attempt to clarify the role of proxy domains and establish new proxy usefulness (PU) metrics to compare the usefulness of different proxy domains. We propose the relative predictive PU to assess the predictive ability of a proxy domain and the learning PU to quantify the usefulness of a proxy as a tool to generate learning data. Furthermore, we argue that the value of a proxy is conditioned on the task that it is being used to help solve. We demonstrate how these new metrics can be used to optimize parameters of the proxy domain for which obtaining ground truth via system identification is not trivial.",https://ieeexplore.ieee.org/document/9635977/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAACS48474.2019.8988124,Online Adversarial Planning in μRTS : A Survey,IEEE,Conferences,"Online planning is an important research area focusing on the problem of real-time decision making, using information extracted from the environment. The aim is to compute, at each decision point, the best decision possible that contributes to the realization of a fixed objective. Relevant application domains include robotics, control engineering and computer games. Real-time strategy (RTS) games pose considerable challenges to artificial intelligence techniques, due to their dynamic, complex and adversarial aspects, where online planning plays a prominent role. They also constitute an ideal research platform and test-bed for online planning. μRTS is an open-source AI research platform that features a minimalistic, yet complete RTS implementation, used by AI researchers for developing and testing intelligent RTS game-playing agents. The unique characteristics of μRTS helped for the emergence of interesting online adversarial planning techniques, dealing with multiple levels of abstraction. This paper presents the major μRTS online planning approaches to date, categorized by the degree of abstraction, in fully and partially observable environments.",https://ieeexplore.ieee.org/document/8988124/,2019 International Conference on Theoretical and Applicative Aspects of Computer Science (ICTAACS),15-16 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8202247,Online learning for human classification in 3D LiDAR-based tracking,IEEE,Conferences,"Human detection and tracking are essential aspects to be considered in service robotics, as the robot often shares its workspace and interacts closely with humans. This paper presents an online learning framework for human classification in 3D LiDAR scans, taking advantage of robust multi-target tracking to avoid the need for data annotation by a human expert. The system learns iteratively by retraining a classifier online with the samples collected by the robot over time. A novel aspect of our approach is that errors in training data can be corrected using the information provided by the 3D LiDAR-based tracking. In order to do this, an efficient 3D cluster detector of potential human targets has been implemented. We evaluate the framework using a new 3D LiDAR dataset of people moving in a large indoor public space, which is made available to the research community. The experiments analyse the real-time performance of the cluster detector and show that our online learned human classifier matches and in some cases outperforms its offline version.",https://ieeexplore.ieee.org/document/8202247/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOTEH48170.2020.9066274,Ontology-Driven Generation of Interactive 3D Worlds,IEEE,Conferences,"In this paper, the ontology-driven approach to automated generation of interactive 3D applications is presented. The ontologies describing both static and dynamic application properties combined with domain-specific knowledge are leveraged. The approach is evaluated in two case studies: smart city simulator and virtual robotics testbed. The generated output is JavaScript code using Three.js library. According to the achieved results, the adoption of ontologies dramatically speeds up the development time compared to manual process.",https://ieeexplore.ieee.org/document/9066274/,2020 19th International Symposium INFOTEH-JAHORINA (INFOTEH),18-20 March 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2014.6889837,Optimising the overall power usage on the SpiNNaker neuromimetic platform,IEEE,Conferences,"Simulations of biological tissue have been extensively used to replicate phenomena observed by in-vivo and in-vitro experiments as an alternative methodology for explaining how computations could take place in a brain region. Additional benefits of simulated neural networks over in-vivo experiments include greater observability, experimental control and reproducibility. General-purpose supercomputers provide the computational power and parallelism required to implement highly complex neural models, but this comes at the expense of high power requirements and communication overheads. Moreover, there are certain cases where real-time simulation performance is a desirable feature, for example in the field of cognitive robotics where embodied agents need to interact with their environment through biologically inspired asynchronous sensors. The SpiNNaker neuromimetic platform is a scalable architecture that has been designed to enable energy-efficient, large-scale simulations of spiking neurons in biological realtime. This work is based on a recent study which revealed that while they are generally energy efficient, SpiNNaker chips dissipate significant amount of power whilst in the idle state. In this paper we perform a systematic investigation into the overall energy consumption of a SpiNNaker system and propose a number of optimised suspend modes in order to reduce this. The proposed implementation is 60% more energy efficient in the idle state, 50% in the uploading and 52% in the downloading phases, while the power dissipation of the whole simulation is reduced by 52%. For demonstration purposes, we run a neural network simulation comprising thousands of neurons and millions of complex synapses on a 48-chip SpiNNaker board, generating millions of synaptic events per second.",https://ieeexplore.ieee.org/document/6889837/,2014 International Joint Conference on Neural Networks (IJCNN),6-11 July 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FormaliSE.2019.00012,Parallelizable Reachability Analysis Algorithms for Feed-Forward Neural Networks,IEEE,Conferences,"Artificial neural networks (ANN) have displayed considerable utility in a wide range of applications such as image processing, character and pattern recognition, self-driving cars, evolutionary robotics, and non-linear system identification and control. While ANNs are able to carry out complicated tasks efficiently, they are susceptible to unpredictable and errant behavior due to irregularities that emanate from their complex non-linear structure. As a result, there have been reservations about incorporating them into safety-critical systems. In this paper, we present a reachability analysis method for feed-forward neural networks (FNN) that employ rectified linear units (ReLUs) as activation functions. The crux of our approach relies on three reachable-set computation algorithms, namely exact schemes, lazy-approximate schemes, and mixing schemes. The exact scheme computes an exact reachable set for FNN, while the lazy-approximate and mixing schemes generate an over-approximation of the exact reachable set. All schemes are designed efficiently to run on parallel platforms to reduce the computation time and enhance the scalability. Our methods are implemented in a toolbox called, NNV, and is evaluated using a set of benchmarks that consist of realistic neural networks with sizes that range from tens to a thousand neurons. Notably, NNV successfully computes and visualizes the exact reachable sets of the real world ACAS Xu deep neural networks (DNNs), which are a variant of a family of novel airborne collision detection systems known as the ACAS System X, using a representation of tens to hundreds of polyhedra.",https://ieeexplore.ieee.org/document/8807491/,2019 IEEE/ACM 7th International Conference on Formal Methods in Software Engineering (FormaliSE),27-27 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCID.2016.1055,Path Planning for UUV in Dynamic Environment,IEEE,Conferences,"From naval operations to ocean science missions, the importance of autonomous vehicles is increasing with the advances in underwater robotics technology. Due to the dynamic and intermittent underwater environment and physical limitations of underwater unmanned vehicle (UUV), feasible and optimal path planning is crucial for autonomous underwater operations. According to different mission, the path planning method of UUV is divided into two categories: the point to point path planning and the complete coverage path planning. The objective of this thesis is to develop and demonstrate an efficient underwater path planning method that is adapted to complicated ocean environment. In this thesis, existing path planning method for the fields of ocean science and robotics are first reviewed, and then local dynamic obstacle avoidance method is proposed to avoid dynamic obstacles. Based on this again, the path planning of UUV in local dynamic environment can be efficiently implemented by adopting rolling window path planning method and local dynamic obstacle avoidance method. This method with the guide point strategy combines global path planning with local dynamic path planning, so that not only the requirements of real-time on-line path planning for UUV are met, the global optimality is also considered. A navigation route for UUV is planned in advance by using priori environmental information based on ant colony algorithm, so it provides the reference information for the selection of guide point. In order to solved the problem of area coverage search, a complete coverage path planning method is proposed by combining ant colony algorithm with biologically inspires neural network. In order to demonstrate underwater path planning method, all of the above ideas and methods developed were tested in simulation experiments.",https://ieeexplore.ieee.org/document/7830329/,2016 9th International Symposium on Computational Intelligence and Design (ISCID),10-11 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV52889.2021.00019,PathBench: A Benchmarking Platform for Classical and Learned Path Planning Algorithms,IEEE,Conferences,"Path planning is a key component in mobile robotics. A wide range of path planning algorithms exist, but few attempts have been made to benchmark the algorithms holistically or unify their interface. Moreover, with the recent advances in deep neural networks, there is an urgent need to facilitate the development and benchmarking of such learning-based planning algorithms. This paper presents PathBench, a platform for developing, visualizing, training, testing, and benchmarking of existing and future, classical and learned 2D and 3D path planning algorithms, while offering support for Robot Operating System (ROS). Many existing path planning algorithms are supported; e.g. A*, wavefront, rapidly-exploring random tree, value iteration networks, gated path planning networks; and integrating new algorithms is easy and clearly specified. We demonstrate the benchmarking capability of PathBench by comparing implemented classical and learned algorithms for metrics, such as path length, success rate, computational time and path deviation. These evaluations are done on built-in PathBench maps and external path planning environments from video games and real world databases. PathBench is open source 1.",https://ieeexplore.ieee.org/document/9469507/,2021 18th Conference on Robots and Vision (CRV),26-28 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPA.1994.636094,Perception systems implemented in analog VLSI for real-time applications,IEEE,Conferences,"We point out that analog VLSI can now be considered as the ideal medium to implement computational systems intended to carry out real time perceptive or even cognitive tasks that are not well handled by traditional computers. By exploiting the analog features of the transistors, only a few devices are needed to realise most of the elementary functions required to implement perceptive systems, resulting in very dense, sophisticated circuits and low power consumption. Elementary artificial retinas in silicon based on their biological counterparts have already been successfully used in industrial applications. Artificial cochleas and noses are also under development. This new enabling technology is of great interest over a wide range of industrial sectors, including robotics, automotive, surveillance and food industry.",https://ieeexplore.ieee.org/document/636094/,Proceedings of PerAc '94. From Perception to Action,7-9 Sept. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812287,Prediction of Metacarpophalangeal Joint Angles and Classification of Hand Configurations Based on Ultrasound Imaging of the Forearm,IEEE,Conferences,"With the advancement in computing and robotics, it is necessary to develop fluent and intuitive methods for inter-acting with digital systems, augmented/virtual reality (AR/VR) interfaces, and physical robotic systems. Hand movement recognition is widely used to enable such interaction. Hand configuration classification and metacarpophalangeal (MCP) joint angle detection are important for a comprehensive reconstruction of hand motion. Surface electromyography (sEMG) and other technologies have been used for the detection of hand motions. Ultrasound images of the forearm offer a way to visualize the internal physiology of the hand from a musculoskeletal perspective. Recent works have shown that these images can be classified using machine learning to predict various hand configurations. In this paper, we propose a Convolutional Neu-ral Network (CNN) based deep learning pipeline for predicting the MCP joint angles. We supplement our results by using a Support Vector Classifier (SVC) to classify the ultrasound information into several predefined hand configurations based on activities of daily living (ADL). Ultrasound data from the forearm were obtained from six subjects who were instructed to move their hands according to predefined hand configurations relevant to ADLs. Motion capture data was acquired as the ground truth for hand movements at three speeds (0.5 Hz, 1 Hz, and 2 Hz) for the index, middle, ring, and pinky fingers. We demonstrated the perfect prediction of hand configurations through SVC classification and a correspondence between the predicted MCP joint angles and the actual MCP joint angles for the fingers, with an average root mean square error of 7.35 degrees. A low latency (6.25 &#x2013; 9.10 Hz) pipeline was implemented for the prediction of both MCP joint angles and hand configuration estimation aimed for real-time implementation.",https://ieeexplore.ieee.org/document/9812287/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN46459.2019.8956461,Privacy First: Designing Responsible and Inclusive Social Robot Applications for in the Wild Studies,IEEE,Conferences,"Deploying social robots applications in public spaces for conducting in the wild studies is a significant challenge but critical to the advancement of social robotics. Real world environments are complex, dynamic, and uncertain. Human-Robot interactions can be unstructured and unanticipated. In addition, when the robot is intended to be a shared public resource, management issues such as user access and user privacy arise, leading to design choices that can impact on users' trust and the adoption of the designed system. In this paper we propose a user registration and login system for a social robot and report on people's preferences when registering their personal details with the robot to access services. This study is the first iteration of a larger body of work investigating potential use cases for the Pepper social robot at a government managed centre for startups and innovation. We prototyped and deployed a system for user registration with the robot, which gives users control over registering and accessing services with either face recognition technology or a QR code. The QR code played a critical role in increasing the number of users adopting the technology. We discuss the need to develop social robot applications that responsibly adhere to privacy principles, are inclusive, and cater for a broad spectrum of people.",https://ieeexplore.ieee.org/document/8956461/,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),14-18 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UPCON52273.2021.9667555,Probabilistic Modeling of Human Locomotion for Biped Robot Trajectory Generation,IEEE,Conferences,"The wheel-type robot has found numerous applications in hospitals, restaurants, entertainment, the automation industry, etc., and shows its applicability in solving the tasks efficiently. However, it failed to achieve the same efficiency in an unstructured environment that is mostly found in the real world. Thus, a biped robot can replace the wheel-type robot for better performance. The biped robot has many joints which make it a complex higher degree of freedom system. Hence, the designing of the controller, reference trajectory generation, state estimation and, filter design for feedback signal is a very cumbersome task. This paper focuses on the generation of the reference trajectories. Since human locomotion is optimal naturally, therefore, the human data is used for this study, which is collected at Robotics and Machine ANalytics (RAMAN) Lab, MNIT, Jaipur, India. In the literature, various authors have implemented model-based learning methods to develop a model based on data. However, these models suffer from model bias i.e., it is assumed that learned model accurately define the real system. Therefore, in this paper, the authors have proposed probabilistic models to model the human locomotion data. The reference trajectory is generated using the Bayesian ridge regression, Automatic relevance determination regression, and Gaussian process regression. The performance evaluation of developed models are based on average error, maximum error, root mean square error, and percentage normalized root mean square error.",https://ieeexplore.ieee.org/document/9667555/,"2021 IEEE 8th Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)",11-13 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRC.2016.7738697,Processor-in-memory support for artificial neural networks,IEEE,Conferences,"Hardware acceleration of artificial neural network (ANN) processing has potential for supporting applications benefiting from real time and low power operation, such as autonomous vehicles, robotics, recognition and data mining. Most interest in ANNs targets acceleration of deep multi-layered ANNs that can require days of offline training to converge on a desired network behavior. Interest has grown in ANNs capable of supporting unsupervised training, where networks can learn new information from unlabeled data dynamically without the need for offline training. These ANNs require large memories with bandwidths much higher than supported in modern GPGPUs. Custom hardware acceleration and memory co-design holds the potential to provide real-time performance in cases where the performance requirements cannot be met by modern GPGPUs. This work presents a custom processor solution to accelerate two hetero-associative memories (Sparsey and HTM) capable of unsupervised and one-hot learning. This custom processor is implemented as an expandable ASIP built upon a configurable SIMD engine for exploiting parallelism. Functional specialization is implemented utilizing processor-in-memory techniques, which results in up to a 20× speedup and a 2000× reduction in energy per frame compared to a software implementation operating on a dataset for recognition of human actions.",https://ieeexplore.ieee.org/document/7738697/,2016 IEEE International Conference on Rebooting Computing (ICRC),17-19 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRoM.2015.7367861,ReMoRo; A mobile robot platform based on distributed I/O modules for research and education,IEEE,Conferences,"We present our recent work on the electrical and hardware design of the mobile robot platform ReMoRo that is based on distributed input/output modules. We have designed three generation of this platform with different specifications, which it help us to design more compatible and applied mobile robot. Nevertheless, the goal of this project was to develop a low-cost and robust but extensible modular robot platform for research and educational purposes. In this paper we describe a new affordable robot structure that enables large-scale innovative, new curriculum, multi robot research and multi-robotics outreach to computer and artificial intelligent students. We introduce the ReMoRo platform, which offers a balance between capabilities, accessibility, cost and an opendesign. All of electrical devices like sensors module, motor drivers and device communication manager are designed based on ARM Cortex M3 microcontrollers that runs under Real-Time Operating System (freeRTOS) for manages each modules internal scheduling and activation control in communication bus. With a range of different sensors, cylindrical manipulator and omnidirectional locomotion system, RoMeRo can interact with environment in multiple ways, handle common objects and therefore be used in various service robot scenarios like warehouse robots or multi agent mobile robots. We demonstrate the usability of our concept by quantifying the object-handling task and also briefly describe the software design based on ROS framework for educational usage.",https://ieeexplore.ieee.org/document/7367861/,2015 3rd RSI International Conference on Robotics and Mechatronics (ICROM),7-9 Oct. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9562075,Reaching Pruning Locations in a Vine Using a Deep Reinforcement Learning Policy,IEEE,Conferences,"We outline a neural network-based pipeline for perception, control and planning of a 7 DoF robot for tasks that involve reaching into a dormant grapevine canopy. The proposed system consists of a 6 DoF industrial robot arm and a linear slider that can actuate on an entire grape vine. Our approach uses Convolutional Neural Networks to detect buds in dormant grape vines and a Reinforcement Learning based control strategy to reach desired cut-point locations for pruning tasks. Within this framework, three methodologies are developed and compared to reach the desired locations: the learned policy-based approach (RL), a hybrid method that uses the learned policy and an inverse kinematics solver (RL+IK), and lastly a classical approach commonly used in robotics. We first tested and validated the suitability of the proposed learning methodology in a simulated environment that resembled laboratory conditions. A reaching accuracy of up to 61.90% and 85.71% for the RL and RL+IK approaches respectively was obtained for a vine that the agent observed while learning. When testing in a new vine, the accuracy was up to 66.66% and 76.19% for RL and RL+IK, respectively. The same methods were then deployed on a real system in an end to end procedure: autonomously scan the vine using a vision system, create its model and finally use the learned policy to reach cutting points. The reaching accuracy obtained in these tests was 73.08%.",https://ieeexplore.ieee.org/document/9562075/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CT54291.2022.9825287,Real Time Protein Crystal Monitoring System,IEEE,Conferences,"High-resolution crystal structures of the biological macromolecules (protein, DNA, RNA or their complexes) are essential for understanding their biological functions and causes of diseases at a molecular level. X-ray crystallographic technique gives the atomic positions of these biological macromolecules and decodes the intermolecular interactions between protein-drug, protein-protein, protein-nucleic acids which are crucial for rational drug design. To aid the macromolecular crystallization experiments, we have indigenously developed automated system, called Real Time Protein Crystal Monitoring System (RT-PCMS) for crystal imaging and to monitor the progress of protein crystal growth during crystallization. RT-PCMS images crystallization drops at frequent time intervals in 24- or 96-well crystallization plate formats. The system consists of precise robotics motion and a custom designed motorized microscopic imaging system, capturing multi-focus composite images of protein crystals in droplets in multiple wells. In a typical successful trial, the crystallization drop comprises protein crystals of size 20 to 300 microns located at the different regions of an elliptical droplet. We have implemented powerful image processing and deep learning algorithms in this system, fusing multi-depth crystal images within a crystallization drop and classification into different crystallization stages. The system is controlled through a personal computer that provides a custom developed graphical user interface software for visualization of all captured images at different crystallization stages and record the results of crystal formation.",https://ieeexplore.ieee.org/document/9825287/,2022 IEEE 7th International conference for Convergence in Technology (I2CT),7-9 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.1991.753906,Real Time Video Frame Grabber,IEEE,Conferences,Computer vision forms one of the most fascinating and rapidly developing fields of Artificial Intelligence with major applications in areas like Robotics and Image Processing.,https://ieeexplore.ieee.org/document/753906/,"TENCON '91. Region 10 International Conference on EC3-Energy, Computer, Communication and Control Systems",28-30 Aug. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCE.2008.4580693,Real time implementation of NARMA L2 feedback linearization and smoothed NARMA L2 controls of a single link manipulator,IEEE,Conferences,"Robotics is a field of modern technology which requires knowledge in vast areas such as electrical engineering, mechanical engineering, computer science as well as finance. Nonlinearities and parametric uncertainties are unavoidable problems faced in controlling robots in industrial plants. Tracking control of a single link manipulator driven by a permanent magnet brushed DC motor is a nonlinear dynamics due to effects of gravitational force, mass of the payload, posture of the manipulator and viscous friction coefficient. Furthermore uncertainties arise because of changes of the rotor resistance with temperature and random variations of friction while operating. Due to this fact classical PID controller can not be used effectively since it is developed based on linear system theory. Neural network control schemes for manipulator control problem have been proposed by researchers; in which their competency is validated through simulation studies. On the other hand, actual real time applications are rarely established. Instead of simulation studies, this paper is aimed to implement neural network controller in real time for controlling a DC motor driven single link manipulator. The work presented in this paper is concentrating on neural NARMA L2 control and its improvement called to as Smoothed NARMA L2 control. As proposed by K. S Narendra and Mukhopadhyay, Narma L2 control is one of the popular neural network architectures for prediction and control. The real time experimentation showed that the Smoothed NARMA L2 is effective for controlling the single link manipulator for both point-to-point and continuous path motion control.",https://ieeexplore.ieee.org/document/4580693/,2008 International Conference on Computer and Communication Engineering,13-15 May 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRAI.2012.6413407,Real time localization of mobile robotic platform via fusion of Inertial and Visual Navigation System,IEEE,Conferences,"Inertial Navigation System (INS) is one of the most important component of a mobile robotic platform, be it ground or air based. It is used to localize the mobile robotic platform in the real world and identify its location in terms of latitudes and longitudes or other related coordinate systems. Highly accurate and precise INS is quite expensive and is therefore not suitable for more general purpose applications. It is, therefore, a standard approach in mobile robotics to use a low grade commercial INS coupled with another navigation device to provide a more accurate triangulation. Generally, INS and Global Positioning System (GPS) are integrated using Kalman Filters to provide accurate localization information about the mobile robots. Although, in certain scenarios, the mobile robot is not able to acquire a GPS fix for long durations of time especially when navigating in indoor environments or in areas with inadequate GPS satellite coverage. In such cases, an additional source of location fix is required. This paper describes an accurate and stable data fusion filter which integrates the position of a mobile robot from a Visual Navigation System (VNS) with the position from an INS to accurately localize the robot in absence of GPS data. This research proposes a seven error states model and uses it in Kalman Filter for data fusion. The filter is tuned and tested using dynamic and static data from INS and VNS. Simulation and experimentation results show that the seven error states model based Kalman Filter provides a good balance between accuracy, robustness and processing efficiency for a real time implementation. Experiments also show that in absence of GPS data only a couple of fixes from the VNS are sufficient to quickly correct the position of the mobile robotic platform and three fixes at different times are sufficient for velocity correction of INS.",https://ieeexplore.ieee.org/document/6413407/,2012 International Conference of Robotics and Artificial Intelligence,22-23 Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2008.143,Real-Time Classification of Streaming Sensor Data,IEEE,Conferences,"The last decade has seen a huge interest in classification of time series. Most of this work assumes that the data resides in main memory and is processed offline. However, recent advances in sensor technologies require resource-efficient algorithms that can be implemented directly on the sensors as real-time algorithms. We show how a recently introduced framework for time series classification, time series bitmaps, can be implemented as efficient classifiers which can be updated in constant time and space in the face of very high data arrival rates. We describe results from a case study of an important entomological problem, and further demonstrate the generality of our ideas with an example from robotics.",https://ieeexplore.ieee.org/document/4669683/,2008 20th IEEE International Conference on Tools with Artificial Intelligence,3-5 Nov. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8794220,Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations,IEEE,Conferences,"Deployment of deep learning models in robotics as sensory information extractors can be a daunting task to handle, even using generic GPU cards. Here, we address three of its most prominent hurdles, namely, i) the adaptation of a single model to perform multiple tasks at once (in this work, we consider depth estimation and semantic segmentation crucial for acquiring geometric and semantic understanding of the scene), while ii) doing it in real-time, and iii) using asymmetric datasets with uneven numbers of annotations per each modality. To overcome the first two issues, we adapt a recently proposed real-time semantic segmentation network, making changes to further reduce the number of floating point operations. To approach the third issue, we embrace a simple solution based on hard knowledge distillation under the assumption of having access to a powerful `teacher' network. We showcase how our system can be easily extended to handle more tasks, and more datasets, all at once, performing depth estimation and segmentation both indoors and outdoors with a single model. Quantitatively, we achieve results equivalent to (or better than) current state-of-the-art approaches with one forward pass costing just 13ms and 6.5 GFLOPs on 640×480 inputs. This efficiency allows us to directly incorporate the raw predictions of our network into the SemanticFusion framework [1] for dense 3D semantic reconstruction of the scene.",https://ieeexplore.ieee.org/document/8794220/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341473,Real-World Human-Robot Collaborative Reinforcement Learning,IEEE,Conferences,"The intuitive collaboration of humans and intelligent robots (embodied AI) in the real-world is an essential objective for many desirable applications of robotics. Whilst there is much research regarding explicit communication, we focus on how humans and robots interact implicitly, on motor adaptation level. We present a real-world setup of a human-robot collaborative maze game, designed to be non-trivial and only solvable through collaboration, by limiting the actions to rotations of two orthogonal axes, and assigning each axes to one player. This results in neither the human nor the agent being able to solve the game on their own. We use deep reinforcement learning for the control of the robotic agent, and achieve results within 30 minutes of real-world play, without any type of pre-training. We then use this setup to perform systematic experiments on human/agent behaviour and adaptation when co-learning a policy for the collaborative game. We present results on how co-policy learning occurs over time between the human and the robotic agent resulting in each participant's agent serving as a representation of how they would play the game. This allows us to relate a person's success when playing with different agents than their own, by comparing the policy of the agent with that of their own agent.",https://ieeexplore.ieee.org/document/9341473/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EFTA.2007.4416888,Real-time architecture for mobile assistant robots,IEEE,Conferences,"Mobile robotics is a challenging research area, with produced results that were unthinkable several years ago. There exist algorithms and methods capable of performing difficult tasks such as detect/classify objects, skill learning and SLAM. From the initial design steps, the real-time software architecture of a robotic platform requires great attention. The problem is difficult, because various components, such as sensing, perception, localization, and motor control, are required to operate and interact in real-time. This makes the system a very complex one. This paper presents a real-time control architecture designed for mobile robots and intelligent vehicles. Moreover, an example of application of the control structure consisting on a system for learning to classify places, using laser range data, is reported.",https://ieeexplore.ieee.org/document/4416888/,2007 IEEE Conference on Emerging Technologies and Factory Automation (EFTA 2007),25-28 Sept. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTOSS.1994.292553,Real-time platforms and environments for time constrained flexible manufacturing,IEEE,Conferences,"The Spring Kernel and associated algorithms, languages, and tools provide system support for static or dynamic real-time applications that require predictable operation. Spring currently consists of two major parts: (1) the development environment, where application and target systems are described, preprocessed and downloaded, and (2) the run-time environment, where the operating system, the Spring Kernel, creates and ensures predictable executions of application tasks. We have integrated our real-time systems technology with component technologies from robotics, computer vision, and real-time artificial intelligence, to develop a test platform for flexible manufacturing. The results being produced are generic so that they should be in many other real-time applications such as air traffic control and chemical plants. We describe this platform, identify new features developed, and comment on some lessons learned to date from this experiment.<>",https://ieeexplore.ieee.org/document/292553/,Proceedings of 11th IEEE Workshop on Real-Time Operating Systems and Software,18-19 May 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSCC42614.2022.9731734,ReckOn: A 28nm Sub-mm2 Task-Agnostic Spiking Recurrent Neural Network Processor Enabling On-Chip Learning over Second-Long Timescales,IEEE,Conferences,"The robustness of autonomous inference-only devices deployed in the real world is limited by data distribution changes induced by different users, environments, and task requirements. This challenge calls for the development of edge devices with an always-on adaptation to their target ecosystems. However, the memory requirements of conventional neural-network training algorithms scale with the temporal depth of the data being processed, which is not compatible with the constrained power and area budgets at the edge. For this reason, previous works demonstrating end-to-end on-chip learning without external memory were restricted to the processing of static data such as images [1]&#x2013;[4], or to instantaneous decisions involving no memory of the past, e.g. obstacle avoidance in mobile robots [5]. The ability to learn short-to-long-term temporal dependencies on-chip is a missing enabler for robust autonomous edge devices in applications such as gesture recognition, speech processing, and cognitive robotics.",https://ieeexplore.ieee.org/document/9731734/,2022 IEEE International Solid- State Circuits Conference (ISSCC),20-26 Feb. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCT.2010.5640434,Recognizing & interpreting Indian Sign Language gesture for Human Robot Interaction,IEEE,Conferences,"This paper describes a novel approach towards recognizing of Indian Sign Language (ISL) gestures for Humanoid Robot Interaction (HRI). An extensive approach is being introduced for classification of ISL gesture which imparts an elegant way of interaction between humanoid robot HOAP-2 and human being. ISL gestures are being considered as a communicating agent for humanoid robot which is being used in this context explicitly. It involves different image processing techniques followed by a generic algorithm for feature extraction process. The classification technique deals with the Euclidean distance metric. The concrete HRI system has been established for initiation based learning mechanism. The Real time robotics simulation software, WEBOTS has been adopted to simulate the classified ISL gestures on HOAP-2 robot. The JAVA based software has been developed to deal with the entire HRI process.",https://ieeexplore.ieee.org/document/5640434/,2010 International Conference on Computer and Communication Technology (ICCCT),17-19 Sept. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2017.8022704,Recovering camera motion from points and lines in stereo images: A recursive model-less approach using trifocal tensors,IEEE,Conferences,"Estimating the 3-D motion of a moving camera from images is a common task in robotics and augmented reality. Most existing marker-less approaches make use of either points or lines. Taking the advantages of both kinds of features in an unknown environment is more attractive due to their availability and differences in characteristics. A novel model-less method is presented in this paper to tackle the 3-D motion tracking problem. Two Bayesian filters, one for point measurements while another for line measurements, are embedded in the Interacting Probabilistic Switching (IPS) framework. They compensate for the weaknesses in one another by utilizing both kinds of features in the stereo images. The proposed method is able to obtain the 3D motion given as little as two line or two point correspondences in consecutive images with the use of multiple trifocal tensors. Our method outperformed two recent methods in terms of accuracy and the problem of drifting was very little in real scenarios.",https://ieeexplore.ieee.org/document/8022704/,"2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",26-28 June 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARM52023.2021.9536145,Reducing the Dimension of the Configuration Space with Self Organizing Neural Networks,IEEE,Conferences,"For robotics, especially industrial applications, it is crucial to reactively plan safe motions through efficient algorithms. Planning is more powerful in the configuration space than the task space. However, for robots with many degrees of freedom, this is challenging and computationally expensive. Sophisticated techniques for motion planning such as the Wavefront algorithm are limited by the high dimensionality of the configuration space, especially for robots with many degrees of freedom. For a neural implementation of the Wavefront algorithm in the configuration space, neurons represent discrete configurations and synapses are used for path planning. In order to decrease the complexity, we reduce the search space by pruning superfluous neurons and synapses. We present different models of self-organizing neural networks for this reduction. The approach takes real-life human motion data as input and creates a representation with reduced dimension. We compare six different neural network models and adapt the Wavefront algorithm to the different structures of the reduced output spaces. The method is backed up by an extensive evaluation of the reduced spaces, including their suitability for path planning by the Wavefront algorithm.",https://ieeexplore.ieee.org/document/9536145/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DIS.2006.63,Remote Programming of Multirobot Systems within the UPC-UJI Telelaboratories: System Architecture and Agent-Based Multirobot Control,IEEE,Conferences,"One of the areas that needs more improvement within the E-Learning environments via Internet (in fact they suppose a very big effort to be accomplished) is allowing students to access and practice real experiments is a real laboratory, instead of using simulations [1]. Real laboratories allow students to acquire methods, skills and experience related to real equipment, in a manner that is very close to the way they are being used in industry. The purpose of the project is the study, development and implementation of an E-Learning environment to allow undergraduate students to practice subjects related to Robotics and Artificial Intelligence. The system, which is now at a preliminary stage, will allow the remote experimentation with real robotic devices (i.e. robots, cameras, etc.). It will enable the student to learn in a collaborative manner (remote participation with other students) where it will be possible to combine the onsite activities (performed ""in-situ"" within the real lab during the normal practical sessions), with the ""online"" one (performed remotely from home via the Internet). Moreover, the remote experiments within the E-Laboratory to control the real robots can be performed by both, students and even scientist. This project is under development and it is carried out jointly by two Universities (UPC and UJI). In this article we present the system architecture and the way students and researchers have been able to perform a Remote Programming of Multirobot Systems via web.",https://ieeexplore.ieee.org/document/1633445/,IEEE Workshop on Distributed Intelligent Systems: Collective Intelligence and Its Applications (DIS'06),15-16 June 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812394,RepAr-Net: Re-Parameterized Encoders and Attentive Feature Arsenals for Fast Video Denoising,IEEE,Conferences,"Real-time video denoising finds applications in several fields like mobile robotics, satellite television, and surveillance systems. Traditional denoising approaches are more common in such systems than their deep learning-based counterparts despite their inferior performance. The large size and heavy computational requirements of neural network-based denoising models pose a serious impediment to their deployment in real-time applications. In this paper, we propose RepAr-Net, a simple yet efficient architecture for fast video de noising. We propose to use temporally separable encoders to generate feature maps called arsenals that can be cached for reuse. We also incorporate re-parameterizable blocks that improve the representative power of the network without affecting the run-time. We benchmark our model on the Set-8 and 2017 DAVIS-Test datasets. Our model achieves state-of-the-art results with up to 29.62&#x0025; improvement in PSNR and a 50&#x0025; decrease in run times over existing methods. Our codes are open-sourced at: github.com/spider-tronix/RepAr-Net.",https://ieeexplore.ieee.org/document/9812394/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1997.656813,RoboCup as a research program,IEEE,Conferences,"An overview of RoboCup (The World Cup Robot Soccer), which offers opportunities for AI and robotics research by providing an attractive but formidable challenge. It also provides a range of challenge programs which is designed to evaluate specific technical issues. The challenge program will be up-dated and new challenges will be offered as technology progresses. Along with other programs such as an education program, RoboCup offers a comprehensive research program which promotes AI and robotics.",https://ieeexplore.ieee.org/document/656813/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968306,Robot Learning via Human Adversarial Games,IEEE,Conferences,"Much work in robotics has focused on “humanin-the-loop” learning techniques that improve the efficiency of the learning process. However, these algorithms have made the strong assumption of a cooperating human supervisor that assists the robot. In reality, human observers tend to also act in an adversarial manner towards deployed robotic systems. We show that this can in fact improve the robustness of the learned models by proposing a physical framework that leverages perturbations applied by a human adversary, guiding the robot towards more robust models. In a manipulation task, we show that grasping success improves significantly when the robot trains with a human adversary as compared to training in a self-supervised manner.",https://ieeexplore.ieee.org/document/8968306/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2001.933270,Robotic Antarctic meteorite search: outcomes,IEEE,Conferences,"Automation of the search for and classification of Antarctic meteorites offers a unique case for early demonstration of robotics in a scenario analogous to geological exploratory missions to other planets and to the Earth's extremes. Moreover, the discovery of new meteorite samples is of great value because meteorites are the only significant source of extraterrestrial material available to scientists. In this paper we focus on the primary outcomes and technical lessons learned from the first field demonstration of autonomous search and in situ classification of Antarctic meteorites by a robot. Using a novel autonomous control architecture, specialized science sensing, combined manipulation and visual servoing, and Bayesian classification, the Nomad robot classified five indigenous meteorites during an expedition to the remote site of Elephant Moraine in January 2000. Nomad's expedition proved the rudiments of science autonomy and exemplified the merits of machine learning techniques for autonomous geological classification in real-world settings. On the other hand, the expedition showcased the difficulty in executing reliable robotic deployment of science sensors and a limited performance in the speed and coverage of autonomous search.",https://ieeexplore.ieee.org/document/933270/,Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),21-26 May 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2015.7354310,Robotic agents capable of natural and safe physical interaction with human co-workers,IEEE,Conferences,"Many future application scenarios of robotics envision robotic agents to be in close physical interaction with humans: On the factory floor, robotic agents shall support their human co-workers with the dull and health threatening parts of their jobs. In their homes, robotic agents shall enable people to stay independent, even if they have disabilities that require physical help in their daily life - a pressing need for our aging societies. A key requirement for such robotic agents is that they are safety-aware, that is, that they know when actions may hurt or threaten humans and actively refrain from performing them. Safe robot control systems are a current research focus in control theory. The control system designs, however, are a bit paranoid: programmers build “software fences” around people, effectively preventing physical interactions. To physically interact in a competent manner robotic agents have to reason about the task context, the human, and her intentions. In this paper, we propose to extend cognition-enabled robot control by introducing humans, physical interaction events, and safe movements as first class objects into the plan language. We show the power of the safety-aware control approach in a real-world scenario with a leading-edge autonomous manipulation platform. Finally, we share our experimental recordings through an online knowledge processing system, and invite the reader to explore the data with queries based on the concepts discussed in this paper.",https://ieeexplore.ieee.org/document/7354310/,2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Sept.-2 Oct. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636467,Robust Feedback Motion Policy Design Using Reinforcement Learning on a 3D Digit Bipedal Robot,IEEE,Conferences,"In this paper, a hierarchical and robust framework for learning bipedal locomotion is presented and successfully implemented on the 3D biped robot Digit built by Agility Robotics. We propose a cascade-structure controller that combines the learning process with intuitive feedback regulations. This design allows the framework to realize robust and stable walking with a reduced-dimensional state and action spaces of the policy, significantly simplifying the design and increasing the sampling efficiency of the learning method. The inclusion of feedback regulation into the framework improves the robustness of the learned walking gait and ensures the success of the sim-to-real transfer of the proposed controller with minimal tuning. We specifically present a learning pipeline that considers hardware-feasible initial poses of the robot within the learning process to ensure the initial state of the learning is replicated as close as possible to the initial state of the robot in hardware experiments. Finally, we demonstrate the feasibility of our method by successfully transferring the learned policy in simulation to the Digit robot hardware, realizing sustained walking gaits under external force disturbances and challenging terrains not incurred during the training process. To the best of our knowledge, this is the first time a learning-based policy is transferred successfully to the Digit robot in hardware experiments.",https://ieeexplore.ieee.org/document/9636467/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6631400,Robust real-time visual odometry for dense RGB-D mapping,IEEE,Conferences,"This paper describes extensions to the Kintinuous [1] algorithm for spatially extended KinectFusion, incorporating the following additions: (i) the integration of multiple 6DOF camera odometry estimation methods for robust tracking; (ii) a novel GPU-based implementation of an existing dense RGB-D visual odometry algorithm; (iii) advanced fused realtime surface coloring. These extensions are validated with extensive experimental results, both quantitative and qualitative, demonstrating the ability to build dense fully colored models of spatially extended environments for robotics and virtual reality applications while remaining robust against scenes with challenging sets of geometric and visual features.",https://ieeexplore.ieee.org/document/6631400/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2009.5152197,Robust servo-control for underwater robots using banks of visual filters,IEEE,Conferences,"We present an application of machine learning to the semi-automatic synthesis of robust servo-trackers for underwater robotics. In particular, we investigate an approach based on the use of Boosting for robust visual tracking of color objects in an underwater environment. To this end, we use AdaBoost, the most common variant of the Boosting algorithm, to select a number of low-complexity but moderately accurate color feature trackers and we combine their outputs. The novelty of our approach lies in the design of this family of weak trackers, which enhances a straightforward color segmentation tracker in multiple ways. From a large and diverse family of possible filters, we select a small subset that optimizes the performance of our trackers. The tracking process applies these trackers on the input video frames, and the final tracker output is chosen based on the weights of the final array of trackers. By using computationally inexpensive, but somewhat accurate trackers as members of the ensemble, the system is able to run at quasi real-time, and thus, is deployable on-board our underwater robot. We present quantitative cross-validation results of our spatio-chromatic visual tracker, and conclude by pointing out some difficulties faced and subsequent shortcomings in the experiments we performed, along with directions of future research in the area of ensemble tracking in real-time.",https://ieeexplore.ieee.org/document/5152197/,2009 IEEE International Conference on Robotics and Automation,12-17 May 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FCCM48280.2020.00067,Scalable Full Hardware Logic Architecture for Gradient Boosted Tree Training,IEEE,Conferences,"Gradient Boosted Tree is most effective and standard machine learning algorithm in many fields especially with various type of tabular dataset. Besides, recent industry field and robotics field require high-speed, power efficient and real-time training with enormous data. FPGA is effective device which enable custom domain specific approach to give acceleration as well as power efficiency. We introduce a scalable full hardware implementation of Gradient Boosted Tree training with high performance and flexibility of hyper parameterization. Experimental work shows that our hardware implementation achieved 11–33 times faster than state-of-art GPU acceleration even with small gates and low power FPGA device.",https://ieeexplore.ieee.org/document/9114741/,2020 IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM),3-6 May 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMWRTS.1995.514298,Scheduling algorithms for improving the response in intelligent real-time environments,IEEE,Conferences,"Over the last few years, the usefulness and maturity of artificial intelligence technologies, and in particular of knowledge-based systems, have been demonstrated by the ever growing number of commercial applications using them. In the domain of real-time control systems (process control, avionics, robotics, ...), these techniques have been considered with a renewed interest, as they appear as a promising approach to cope with the increasing complexity of the systems to be controlled. The integration of nonpredictable methods in real time systems is one of the crucial points. A task model allowing the representation of activities with optional parts and several scheduling algorithms to incorporate them into real time systems is described.",https://ieeexplore.ieee.org/document/514298/,Proceedings Seventh Euromicro Workshop on Real-Time Systems,14-16 June 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNNB.2005.1614810,Security Assurance Using Face Recognition & Detection System Based On Neural Networks,IEEE,Conferences,"In this paper, we have proposed a new method of implementing an assurance system using the facial information of the people, this is a different approach to the conventional security system which uses biometric information or cryptography for assurance, here we use an efficient self-scaling face recognition system supported with a face detection system, the system is capable enough to extract the human faces from a real time video and to recognize the people using a face recognition system, we are designing the framework for face recognition system with a hybrid RBF neural network, the real advantage of the system lies in its capability to inculcate some basic features of the self organizing map (SOM) so that the system can scale on its own and it doesn't get outdated with time, for the face detection system we use a content based face detection algorithm, the facial feature so detected is inputted into the face recognition system, if the person's information is already present in the system, authentication can be accomplished, this system can be used in public places like airports and supermarkets, the information of criminals can be stored in the system and in case any of the criminals are detected by the system, the security personnel can be signaled, this system can also be implemented in robotics, the system can help the computer to identify individual users distinctly, our facial recognition system has been tested and found to be persistent in recognizing the individual even if the input facial image is of different gesture or holds some extra lineament like beard, moustache or spectacles so we can definitely state that the system is reliable and efficient",https://ieeexplore.ieee.org/document/1614810/,2005 International Conference on Neural Networks and Brain,13-15 Oct. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793744,Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data,IEEE,Conferences,"The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive hand-labeled datasets are available. As generating these datasets is time-consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any hand-labeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15% in Average Precision and 20% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at https://bit.ly/2letCuE.",https://ieeexplore.ieee.org/document/8793744/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793595,Semi Supervised Deep Quick Instance Detection and Segmentation,IEEE,Conferences,"In this paper, we present a semi supervised deep quick learning framework for instance detection and pixelwise semantic segmentation of images in a dense clutter of items. The framework can quickly and incrementally learn novel items in an online manner by real-time data acquisition and generating corresponding ground truths on its own. To learn various combinations of items, it can synthesize cluttered scenes, in real time. The overall approach is based on the tutor-child analogy in which a deep network (tutor) is pretrained for class-agnostic object detection which generates labeled data for another deep network (child). The child utilizes a customized convolutional neural network head for the purpose of quick learning. There are broadly four key components of the proposed framework: semi supervised labeling, occlusion aware clutter synthesis, a customized convolutional neural network head, and instance detection. The initial version of this framework was implemented during our participation in Amazon Robotics Challenge (ARC), 2017. Our system was ranked 3rd rd, 4th and 5 th worldwide in pick, stow-pick and stow task respectively. The proposed framework is an improved version over ARC'17 where novel features such as instance detection and online learning has been added.",https://ieeexplore.ieee.org/document/8793595/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN50785.2021.9515358,Sequential Prediction with Logic Constraints for Surgical Robotic Activity Recognition,IEEE,Conferences,"Many real-world time-sensitive and high-stake applications (e.g., surgical, rescue, and recovery robotics) exhibit sequential nature; thus, applying Recurrent Neural Network (RNN)-based sequential models is an attractive approach to detect robotic activity. One limitation of such approaches is data scarcity. As a result, limited training samples may lead to over-fitting, producing incorrect predictions during deployment. Nevertheless, abundant domain knowledge may still be available, which may help formulate logic constraints. In this paper, we propose a novel way to integrate domain knowledge into RNN-based sequential prediction. We build a Markov Logic Network (MLN)-based classifier that automatically learns constraint weights from data. We propose two methods to incorporate this MLN-based prediction: (i) PriorLayer, in which the values of the hidden layer of the RNN are combined with weights learned from logic constraints in an additional neural network layer, and (ii) Conflation, in which class probabilities from RNN predictions and constraint weights are combined based on the conflation of class probabilities. We evaluate robotic activity classification methods on a simulated OpenAI Gym environment and a real-world DESK dataset for surgical robotics. We observe that our proposed MLN-based approaches boost the performance of LSTM-based networks. In particular, MLN boosts the accuracy of LSTM from 71% to 84% on the Gym dataset and from 68% to 72% on the Taurus robot dataset. Furthermore, MLN (i.e., PriorLayer) shows regularization capability where it improves accuracy in initial LSTM training while avoiding over-fitting early, thus improves the final classification accuracy on unseen data. The code is available at https://github.com/masud99r/prediction-with-logic-constraints.",https://ieeexplore.ieee.org/document/9515358/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2018.8569569,ShadowCam: Real-Time Detection of Moving Obstacles Behind A Corner For Autonomous Vehicles,IEEE,Conferences,"Moving obstacles occluded by corners are a potential source for collisions in mobile robotics applications such as autonomous vehicles. In this paper, we address the problem of anticipating such collisions by proposing a vision-based detection algorithm for obstacles which are outside of a vehicle's direct line of sight. Our method detects shadows of obstacles hidden around corners and automatically classifies these unseen obstacles as “dynamic” or “static”. We evaluate our proposed detection algorithm on real-world corners and a large variety of simulated environments to assess generalizability in different challenging surface and lighting conditions. The mean classification accuracy on simulated data is around 80% and on real-world corners approximately 70%. Additionally, we integrate our detection system on a full-scale autonomous wheelchair and demonstrate its feasibility as an additional safety mechanism through real-world experiments. We release our real-time-capable implementation of the proposed ShadowCam algorithm and the dataset containing simulated and real-world data under an open-source license.",https://ieeexplore.ieee.org/document/8569569/,2018 21st International Conference on Intelligent Transportation Systems (ITSC),4-7 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DICTA.2018.8615804,Similar Gesture Recognition using Hierarchical Classification Approach in RGB Videos,IEEE,Conferences,"Recognizing human actions from the video streams has become one of the very popular research areas in computer vision and deep learning in the recent years. Action recognition is wildly used in different scenarios in real life, such as surveillance, robotics, healthcare, video indexing and human-computer interaction. The challenges and complexity involved in developing a video-based human action recognition system are manifold. In particular, recognizing actions with similar gestures and describing complex actions is a very challenging problem. To address these issues, we study the problem of classifying human actions using Convolutional Neural Networks (CNN) and develop a hierarchical 3DCNN architecture for similar gesture recognition. The proposed model firstly combines similar gesture pairs into one class, and classify them along with all other class, as a stage-1 classification. In stage-2, similar gesture pairs are classified individually, which reduces the problem to binary classification. We apply and evaluate the developed models to recognize the similar human actions on the HMDB51 dataset. The result shows that the proposed model can achieve high performance in comparison to the state-of-the-art methods.",https://ieeexplore.ieee.org/document/8615804/,2018 Digital Image Computing: Techniques and Applications (DICTA),10-13 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN50785.2021.9515431,Simplifying the A.I. Planning modeling for Human-Robot Collaboration,IEEE,Conferences,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism.",https://ieeexplore.ieee.org/document/9515431/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEM51511.2021.9445285,Simulation based vehicle movement tracking using Kalman Filter algorithm for Autonomous vehicles,IEEE,Conferences,"In the domain of Software automotive industry, one of the most widely used algorithms for performing analysis of driving operations is the Kalman filter algorithm. In today's world of advanced machine learning, the Kalman filter remains an important tool to fuse measurements from several sensors to estimate the real time state of robotics systems such as a self-driving vehicle. Kalman filter is able to update an estimate of evolving nature of continuously changing states of the common filters to take a probabilistic estimate. The driving scenario results are updated in real time using 2-steps update and correction method. In this paper, we have described the process of Kalman filter and its variant to estimate about the detection of moving object in a given traffic scenario using advance toolboxes of MATLAB. Results have been shown for multiple changing parameters.",https://ieeexplore.ieee.org/document/9445285/,2021 2nd International Conference on Intelligent Engineering and Management (ICIEM),28-30 April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593856,Skill-Oriented Designer of Conceptual Robotic Structures,IEEE,Conferences,"This communication presents an application for the use of ontologies in the generation of robot structures. The ontology developed for this app relies on the IEEE Standard Ontologies for Robotics and Automation (ORA) and it incorporates a set of concepts, relations and axioms that link robotic skills with the structural parts needed for their realization. The user can select a base configuration and/or a set of desired skills that the robot should be able to perform. Then, the application evaluates the axioms and returns an abstract structure that can carry out the requested skills. The final implementation of the structure can be achieved with any modular robotic platform that could identify each structural part with a physical device.",https://ieeexplore.ieee.org/document/8593856/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UBMK.2019.8907108,Smart Class Applications for Education,IEEE,Conferences,"We use technological developments in education and adapt technology to education. Educational computers, tablets, smart phones and other technological devices that integrate with these devices aim to increase the quality of education within the scope of computer-aided training. At the same time, with the development of the Internet, it has become much easier to access resources. The environments where the hardware and software technologies used in education are used together can be considered as intelligent classes. Video conferencing and live broadcasts used in the first smart classrooms have now turned into different applications. Intelligent classes that support learning with robotics, mobile learning, virtual reality and augmented reality applications, and different learning environments and materials are used today to improve the quality of education. The Individualized Smart Class (BAS) application proposed in this study is designed in two steps, hardware and software. It is envisaged to use laptop computers, microphones, headphones, tablets and virtual reality glasses in the classroom as hardware. At the same time the broadband of the classes will be ensured to have the internet. Even in this class, wireless network systems such as infrared, Bluetooth, Wi-Fi can be used. As a software, a class that can be included in the virtual classes with cloud architecture and can use the increased reality applications is considered. In addition, mobile applications that provide virtual reality will enrich the course materials. With this application, equality of opportunity will be provided for disadvantaged students in education and training, quality of education will be improved and it will be beneficial for the development of our country in this context.",https://ieeexplore.ieee.org/document/8907108/,2019 4th International Conference on Computer Science and Engineering (UBMK),11-15 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISC2.2016.7580798,SmartSEAL: A ROS based home automation framework for heterogeneous devices interconnection in smart buildings,IEEE,Conferences,"With this paper we present the SmartSEAL inter-connection system developed for the nationally founded SEAL project. SEAL is a research project aimed at developing Home Automation (HA) solutions for building energy management, user customization and improved safety of its inhabitants. One of the main problems of HA systems is the wide range of communication standards that commercial devices use. Usually this forces the designer to choose devices from a few brands, limiting the scope of the system and its capabilities. In this context, SmartSEAL is a framework that aims to integrate heterogeneous devices, such as sensors and actuators from different vendors, providing networking features, protocols and interfaces that are easy to implement and dynamically configurable. The core of our system is a Robotics middleware called Robot Operating System (ROS). We adapted the ROS features to the HA problem, designing the network and protocol architectures for this particular needs. These software infrastructure allows for complex HA functions that could be realized only levering the services provided by different devices. The system has been tested in our laboratory and installed in two real environments, Palazzo Fogazzaro in Schio and “Le Case” childhood school in Malo. Since one of the aim of the SEAL project is the personalization of the building environment according to the user needs, and the learning of their patterns of behaviour, in the final part of this work we also describe the ongoing design and experiments to provide a Machine Learning based re-identification module implemented with Convolutional Neural Networks (CNNs). The description of the adaptation module complements the description of the SmartSEAL system and helps in understanding how to develop complex HA services through it.",https://ieeexplore.ieee.org/document/7580798/,2016 IEEE International Smart Cities Conference (ISC2),12-15 Sept. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197523,SnapNav: Learning Mapless Visual Navigation with Sparse Directional Guidance and Visual Reference,IEEE,Conferences,"Learning-based visual navigation still remains a challenging problem in robotics, with two overarching issues: how to transfer the learnt policy to unseen scenarios, and how to deploy the system on real robots. In this paper, we propose a deep neural network based visual navigation system, SnapNav. Unlike map-based navigation or Visual-Teach-and-Repeat (VT&R), SnapNav only receives a few snapshots of the environment combined with directional guidance to allow it to execute the navigation task. Additionally, SnapNav can be easily deployed on real robots due to a two-level hierarchy: a high level commander that provides directional commands and a low level controller that provides real-time control and obstacle avoidance. This also allows us to effectively use simulated and real data to train the different layers of the hierarchy, facilitating robust control. Extensive experimental results show that SnapNav achieves a highly autonomous navigation ability compared to baseline models, enabling sparse, map-less navigation in previously unseen environments.",https://ieeexplore.ieee.org/document/9197523/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2002.1045681,Socially interactive robots. Why our current beliefs about them still work,IEEE,Conferences,"Discussion about the application of scientific knowledge in robotics in order to build people helpers is widespread. The issue herein addressed is philosophically poignant, that of robots that are 'people'. It is currently popular to speak about robots and the image of Man. Behind this lurks the dialogical mind and the questions on its artificial existence. Without intending to defend or refute the discourse in favour of 'recreating' Man, a lesser familiar question is brought forth: 'Given that we are capable of creating a man (constructing a robot-person), what would the consequences of this be and would we be satisfied with such technology?' Thorny topic; it questions the entire knowledge foundation upon which strong AI/Robotics is positioned. The author argues for improved monitoring of technological progress and thus favours 'soft' (weak) implementation techniques.",https://ieeexplore.ieee.org/document/1045681/,Proceedings. 11th IEEE International Workshop on Robot and Human Interactive Communication,27-27 Sept. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSTW.2019.00028,Software Testing: According to Plan!,IEEE,Conferences,"Automated planning and scheduling represents a branch of classical artificial intelligence (AI) research. Although initially used in robotics and intelligent agents, the use of planning for testing purposes has increased over the years. There sequences of actions representing interactions with the system under test guide the test execution towards reaching a test purpose. A planning problem is formally defined as a model that resembles the interaction with a real system under test (SUT). The obtained solutions are generated, i.e., the plans, directly correspond to test cases. The planning model offers the possibility to generate test cases with a great variety of interactions without the need for an extensive model definition. Until now, planning has proven to be efficient in detecting both functional and non-functional issues. The second play a major role in uncovering vulnerabilities in software. In fact, testing of any domain can be specified as a planning problem. The purpose of this paper is to summarize previous research in the domain of planning for testing including discussing examples from multiple domains.",https://ieeexplore.ieee.org/document/8728938/,"2019 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",22-23 April 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITMS52826.2021.9615342,Speaker Identification using Triplet Loss Function Combined with Clustering Techniques,IEEE,Conferences,"Speaker identification plays a critical role in many applications like robotics specially the applications that focus on humanoid robotics. The speaker identification includes comparing unknown utterances against pre-stored utterances of speakers. In general, the encoded features are stored from the pre-known speakers database and 1:N comparisons between the extracted encoded features of the unknown utterances and the pre-stored N known speakers are implemented. Different techniques can be used for these types of comparisons of which cosine similarity is the most used one. However, the more the number of the pre-stored known speakers, the longer the execution time the model will need to finish these comparisons, and hence it may not be suitable for real-time applications. In this paper, we combined previously published Triple Neural Network for speaker identification with clustering techniques on the speakers dataset. We employed different clustering techniques and presented two different methods for comparing unknown utterances against pre-stored utterances. The obtained results showed a significant enhancement in the comparisons time with a few reductions in the obtained accuracy. The proposed approach provided a framework that can represent a trade-off between execution time and obtained accuracy.",https://ieeexplore.ieee.org/document/9615342/,2021 62nd International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS),14-15 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/EECSI53397.2021.9624311,Strawberry Fruit Quality Assessment for Harvesting Robot using SSD Convolutional Neural Network,IEEE,Conferences,"Strawberry has a tremendous economic value as well as being visually appealing. Therefore, strawberry farmers need to ensure that they only harvest good quality strawberries. However, assessing the quality of strawberries is not an easy problem, especially for local plantations which do not have enough human resources. As robotics becomes accessible and widely used for agriculture work such as harvesting fruit, the real-time embedded system computation power becomes much more powerful nowadays. This paper discusses the harvesting robot's ability to distinguish the quality of strawberries in realtime detection using computer vision technology in the form of object detection by utilizing a deep neural network in a single board computer (SBC). The robot software is built on Robot Operating System (ROS) framework. The proposed method is tested on a robot equipped with a monocular camera. The learning process shows that the robot can detect and differentiate between good and bad quality strawberries with 90% accuracy and maintain a high frame rate.",https://ieeexplore.ieee.org/document/9624311/,"2021 8th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)",20-21 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636743,Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation,IEEE,Conferences,"We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent’s dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exempli-fies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in a real robot to navigate an apartment, and show that they can generalize in a zero-shot manner. A video summary is available here: https://youtu.be/QOQ56XVIYVE",https://ieeexplore.ieee.org/document/9636743/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2013.4,Table of contents,IEEE,Conferences,"The following topics are dealt with: artificial intelligence; neural networks and fuzzy systems; evolutionary computation; bioinformatics and bioengineering; data and semantic mining; games, VR and visualization; intelligent systems and applications; systems intelligence; control intelligence; e-science and e-systems; robotics, cybernetics, engineering, and manufacturing; operations research; discrete-event and real-time systems; image, speech and signal processing; industry, business, management, human factors and social issues; energy, power, transport, logistics, harbour, shipping and marine simulation; parallel, distributed, and software architectures and systems; mobile-ad hoc wireless networks, Mobicast, sensor placement, and target tracking; performance engineering of computer and communications systems; and circuits and devices.",https://ieeexplore.ieee.org/document/6959881/,"2013 1st International Conference on Artificial Intelligence, Modelling and Simulation",3-5 Dec. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCEA.2010.4,Table of contents - Volume 1,IEEE,Conferences,The following deals with the following topics: algorithms; artificial intelligence; software engineering; bioinformatics; computer graphics; computer architecture; information systems; computer aided instruction; computer games; virtual reality; data security; digital simulation; computer aided design; ethical aspects; database systems; digital libraries; signal processing; image processing; logic design; e-commerce; human computer interaction; embedded systems; Internet; mobile computing; multimedia systems; natural language processing; neural networks; programming languages; robotics; control systems; theoretical computer science; and wireless sensor networks.,https://ieeexplore.ieee.org/document/5445635/,2010 Second International Conference on Computer Engineering and Applications,19-21 March 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCRD.2011.5764067,Table of contents vol. 01,IEEE,Conferences,The following topics are dealt with: computer research and development; event driven programming; artificial intelligence; expert systems; algorithm analysis; high performance computing; automated software engineering; human computer interaction; bioinformatics; scientific computing; image processing; information retrieval; compilers; interpreters; computational intelligence; computer architecture; embedded systems; computer animation; Internet; Web applications; communication/networking; knowledge data engineering; computer system implementation; logics; VLSI; mathematical software; information systems; computer based education; mathematical logic; mobile computing; computer games; multimedia applications; computer graphics; virtual reality; natural language processing; neural networks; computer modeling; parallel computing; distributed computing; computer networks; pattern recognition; computer security; computer simulation; computer vision; probability; statistics; performance evaluation; computer aided design/manufacturing; computing ethics; programming languages; problem complexity; control systems; physical sciences; engineering; discrete mathematics; reconfigurable computing systems; data communications; robotics; automation; system security; cryptography; data compression; data encryption; data mining; database systems; document processing; text processing; educational technology; digital library; technology management; digital signal processing; theoretical computer science; digital systems; logic design; ubiquitous computing; and visualizations.,https://ieeexplore.ieee.org/document/5764067/,2011 3rd International Conference on Computer Research and Development,11-13 March 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.2008.4720346,"Teaching concepts in fuzzy logic using low cost robots, PDAs, and custom software",IEEE,Conferences,"Fuzzy logic is a topic traditionally taught in artificial intelligence, machine learning, and robotics courses. Students receive the necessary mathematical and theoretical foundation in lecture format. The final learning experience may require that students create and code their own fuzzy logic application that solves a real world problem. This can be an issue when the target is a bioengineering course that introduces classical control theory, fuzzy logic, neural networks, genetic algorithms and genetic programming through the use of a low cost robot, personal digital assistant (PDA) handheld computer, and custom PDA software. In this course, the concepts and theories discussed in lecture are reinforced and extended in a corresponding laboratory through the use of wireless robots and PDAs. Fuzzy logic libraries and software modules for laptops and desktop computers are readily available, however, when it comes to handheld computers no such libraries exist. Students are able to spend more time experimenting with different fuzzy logic controllers when a custom fuzzy logic library and PDA graphical user interface are utilized. In this paper we introduce and discuss a unique low cost wireless robot, a custom fuzzy logic library, a custom fuzzy logic GUI for the PDA, and the implementation results for the fuzzy logic section in a newly created bioengineering course. Diagnostic and summative assessment in the form of a pre-test and post-test was administered for each section of the course, however, only the results for the fuzzy logic section will be provided.",https://ieeexplore.ieee.org/document/4720346/,2008 38th Annual Frontiers in Education Conference,22-25 Oct. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARSC.2015.19,Testing a Fully Autonomous Robotic Salesman in Real Scenarios,IEEE,Conferences,"Over the past decades, the number of robots deployed in museums, trade shows and exhibitions have grown steadily. This new application domain has become a key research topic in the robotics community. Therefore, new robots are designed to interact with people in these domains, using natural and intuitive channels. Visual perception and speech processing have to be considered for these robots, as they should be able to detect people in their environment, recognize their degree of accessibility and engage them in social conversations. They also need to safely navigate around dynamic, uncontrolled environments. They must be equipped with planning and learning components, that allow them to adapt to different scenarios. Finally, they must attract the attention of the people, be kind and safe to interact with. In this paper, we describe our experience with Gualzru, a salesman robot endowed with the cognitive architecture RoboCog. This architecture synchronizes all previous processes in a social robot, using a common inner representation as the core of the system. The robot has been tested in crowded, public daily life environments, where it interacted with people that had never seen it before nor had a clue about its functionality. Experimental results presented in this paper demonstrate the capabilities of the robot and its limitations in these real scenarios, and define future improvement actions.",https://ieeexplore.ieee.org/document/7101621/,2015 IEEE International Conference on Autonomous Robot Systems and Competitions,8-10 April 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIA.2006.305788,The Design and Implementation of OpenGL-based Comprehensive Educational Robot System,IEEE,Conferences,"In this paper, the authors present the design and implementation of MountTai, a cost effective OpenGL based comprehensive educational robot system for China's primary and high school education. Firstly the system's goal and framework is introduced, then it is described the MountTai robot's functions and construction in hardware. The paper expatiates at length how VR technology is used to implement the system software as well as how the software's functions are designed to illustrate robotics in different perspectives relating to mechanics, electronics, communication, artificial intelligence, language programming. The Web-based teaching course dedicated to robot-DIY tutorials is also shown. Finally, concluding remarks for future works are given.",https://ieeexplore.ieee.org/document/4097992/,2006 IEEE International Conference on Information Acquisition,20-23 Aug. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GHTC.2018.8601597,The EDNA Public Safety Drone: Bullet-Stopping Lifesaving,IEEE,Conferences,"Urban gun violence in cities across the world is a serious issue for public safety agencies and disaster management organizations. This led us to the development of the EDNA drone, an aerial robotics solution designed to equip first responders in high-risk settings with lifesaving-edge tools for situational awareness and non-lethal conflict resolution. The EDNA is an unmanned aerial vehicle (UAV) that delivers the patent-pending “Predictive Probable Cause” technology. The EDNA drone is designed to provide automated real-time analysis to assist teams entering high-risk situations where gun violence may occur. By leveraging machine learning, biometric sensors, and advanced materials in the field and routing feedback to an intuitive augmented-reality interface, the EDNA will provide autonomous threat detection and bullet-stopping capabilities wherever those features are needed--to groups such as Police and Sheriff's Departments, Fire Departments, and EMT and emergency rescue teams. Data from the EDNA drone's sensors is fed to machine learning algorithms running on the drone in real-time. Through a neural network trained on past data, the EDNA is able to detect the presence and location of firearms and explosives, even through walls or other obstacles. Through the use of advanced metal foams and composite materials, the armored drone can even stop bullets-functionality which has obvious benefits for humanitarian deployment.",https://ieeexplore.ieee.org/document/8601597/,2018 IEEE Global Humanitarian Technology Conference (GHTC),18-21 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CT.1997.617707,The Intelligent Room project,IEEE,Conferences,"At the MIT Artificial Intelligence Laboratory, we have been working on technologies for an Intelligent Room. Rather than pull people into the virtual world of the computer, we are trying to pull the computer out into the real world of people. To do this, we are combining robotics and vision technology with speech understanding systems and agent-based architectures to provide ready-at-hand computation and information services for people engaged in day-to-day activities, both on their own and in conjunction with others. We have built a layered architecture where, at the bottom level, vision systems track people and identify their activities and gestures, and, through word spotting, decide whether people in the room are talking to each other or to the room itself. At the next level, an agent architecture provides a uniform interface to such specially-built systems, and to other off-the-shelf software, such as Web browsers, etc. At the highest level, we are able to build application systems that provide occupants of the room with specialized services; examples we have built include systems for command-and-control situations rooms and as a room for giving presentations.",https://ieeexplore.ieee.org/document/617707/,Proceedings Second International Conference on Cognitive Technology Humanizing the Information Age,25-28 Aug. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMC.1998.727776,The importance of artificial intelligence-expert systems in computer integrated manufacturing,IEEE,Conferences,"In order to maintain their competitiveness, companies feel compelled to adopt productivity increasing measures. Yet, they cannot relinquish the flexibility their production cycles need in order to improve their response, and thus, their positioning in the market. To achieve this, companies must combine these two seemingly opposed principles. Thanks to new technological advances, this combination is already a working reality in some companies. It is made possible today by the implementation of computer integrated manufacturing (CIM) and artificial intelligence (AI) techniques, fundamentally by means of expert systems (ES) and robotics. Depending on how these (AI/CIM) techniques contribute to automation, their immediate effects are an increase in productivity and cost reductions. Yet also, the system's flexibility allows for easier adaptation and, as a result, an increased ability to generate value, in other words, competitiveness is improved. The authors have analyzed three studies to identify the possible benefits or advantages, as well as the inconveniences, that this type of technique may bring to companies, specifically in the production field. Although the scope of the studies and their approach differ from one to the other, their joint contribution can be of unquestionable value in order to understand a little better the importance of ES within the production system.",https://ieeexplore.ieee.org/document/727776/,IEMC '98 Proceedings. International Conference on Engineering and Technology Management. Pioneering New Technologies: Management Issues and Challenges in the Third Millennium (Cat. No.98CH36266),11-13 Oct. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2007.364220,Towards Mapping of Cities,IEEE,Conferences,"Map learning is a fundamental task in mobile robotics because maps are required for a series of high level applications. In this paper, we address the problem of building maps of large-scale areas like villages or small cities. We present our modified car-like robot which we use to acquire the data about the environment. We introduce our localization system which is based on an information filter and is able to merge the information obtained by different sensors. We furthermore describe out mapping technique that is able to compactly model three-dimensional scenes and allows us efficient and accurate incremental map learning. We additionally apply a global optimization techniques in order to accurately close loops in the environment. Our approach has been implemented and deeply tested on a real car equipped with a series of sensors. Experiments described in this paper illustrate the accuracy and efficiency of the presented techniques.",https://ieeexplore.ieee.org/document/4209838/,Proceedings 2007 IEEE International Conference on Robotics and Automation,10-14 April 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2005.1490968,Towards Robot Soccer Team Behaviours Through Approximate Simulation,IEEE,Conferences,"Robot soccer is now recognized as one of the most popular and efficient testbeds for intelligent robotics. It involves many challenges for computation, mechanics, control, software engineering, machine learning, and other fields. The international RoboCup initiative supports research into robot soccer and provides an excellent environment to investigate machine learning for robotics in simulation and the real world",https://ieeexplore.ieee.org/document/1490968/,"Proceedings. The 4th International Conference on Development and Learning, 2005",19-21 July 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811766,"Towards Safe, Realistic Testbed for Robotic Systems with Human Interaction",IEEE,Conferences,"Simulation has been a necessary, safe testbed for robotics systems (RS). However, testing in simulation alone is not enough for robotic systems operating in close proximity, or interacting directly with, humans, because simulated humans are very limited. Furthermore, testing with real humans can be unsafe and costly. As recent advances in machine learning are being brought to physical robotic systems, how to collect data as well as evaluate them with human interactions safely yet realistically is a critical question. This paper presents a Mixed-Reality (MR) system toward human-centered development of robotic systems emphasizing benefits as a data collection and testbed tool. MR testbeds allow humans to interact with various levels of virtuality to maintain both realism and safety. We detail the advantages and limitations of these different levels of realism or virtualization, and report our MR-based RS testbed implemented using off-the-shelf MR devices with the Unity game engine and ROS. We demonstrate our testbed in a multi-robot, multi-person tracking and monitoring application. We share our vision and insights earned during the development and data collection.",https://ieeexplore.ieee.org/document/9811766/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2015.7354134,Towards bridging the reality gap between tensegrity simulation and robotic hardware,IEEE,Conferences,"Using a new hardware implementation of our designs for tunably compliant spine-like tensegrity robots, we show that the NASA Tensegrity Robotics Toolkit can effectively generate and predict desirable locomotion strategies for these many degree of freedom systems. Tensegrity, which provides structural integrity through a tension network, shows promise as a design strategy for more compliant robots capable of interaction with rugged environments, such as a tensegrity interplanetary probe prototype surviving multi-story drops. Due to the complexity of tensegrity structures, modeling through physics simulation and machine learning improves our ability to design and evaluate new structures and their controllers in a dynamic environment. The kinematics of our simulator, the open source NASA Tensegrity Robotics Toolkit, have been previously validated within 1.3% error on position through motion capture of the six strut robot ReCTeR. This paper provides additional validation of the dynamics through the direct comparison of the simulator to forces experienced by the latest version of the Tetraspine robot. These results give us confidence in our strategy of using tensegrity to impart future robotic systems with properties similar to biological systems such as increased flexibility, power, and mobility in extreme terrains.",https://ieeexplore.ieee.org/document/7354134/,2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Sept.-2 Oct. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2017.7966054,Towards real-time robot simulation on uneven terrain using neural networks,IEEE,Conferences,"Simulation is a valuable tool for robotics research and development, and various simulation packages have been proposed. However, we are aware of no freely-available packages which implement the required fidelity to accurately model earth-moving robots that manipulate the terrain itself. The software which does exist for this is difficult if not impossible to run in real-time while achieving the desired accuracy. This paper proposes a simulation system in which a neural network is trained using data generated in a 3D high-fidelity, non-real-time simulator. The resulting neural network is used to accurately predict the motion of a robot in a 2D simulator, while also taking into consideration a height-field representing a 3D terrain. Using a trained neural network to drive the new simulation provides considerable speedup over the high-fidelity 3D simulation, allowing behaviour to be simulated in real-time while still capturing the physics of the agents and the environment.",https://ieeexplore.ieee.org/document/7966054/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON.2017.7925321,Towards real-time segmentation of 3D point cloud data into local planar regions,IEEE,Conferences,"This article describes an algorithm for efficient segmentation of point cloud data into local planar surface regions. This is a problem of generic interest to researchers in the computer graphics, computer vision, artificial intelligence and robotics community where it plays an important role in applications such as object recognition, mapping, navigation and conversion from point clouds representations to 3D surface models. Prior work on the subject is either computationally burdensome, precluding real time applications such as robotic navigation and mapping, prone to error for noisy measurements commonly found at long range or requires availability of coregistered color imagery. The approach we describe consists of 3 steps: (1) detect a set of candidate planar surfaces, (2) cluster the planar surfaces merging redundant plane models, and (3) segment the point clouds by imposing a Markov Random Field (MRF) on the data and planar models and computing the Maximum A-Posteriori (MAP) of the segmentation labels using Bayesian Belief Propagation (BBP). In contrast to prior work which relies on color information for geometric segmentation, our implementation performs detection, clustering and estimation using only geometric data. Novelty is found in the fast clustering technique and new MRF clique potentials that are heretofore unexplored in the literature. The clustering procedure removes redundant detections of planes in the scene prior to segmentation using BBP optimization of the MRF to improve performance. The MRF clique potentials dynamically change to encourage distinct labels across depth discontinuities. These modifications provide improved segmentations for geometry-only depth images while simultaneously controlling the computational cost. Algorithm parameters are tunable to enable researchers to strike a compromise between segmentation detail and computational performance. Experimental results apply the algorithm to depth images from the NYU depth dataset which indicate that the algorithm can accurately extract large planar surfaces from depth sensor data.",https://ieeexplore.ieee.org/document/7925321/,SoutheastCon 2017,30 March-2 April 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON52537.2022.9766804,Training of Engineers: Approaches to Customization of Educational Programs,IEEE,Conferences,"Since the field of information technology (IT) is constantly and rapidly developing, the training of engineering personnel couldn&#x2019;t be behind this global digital transformation. The new digital reality focuses on the mandatory formation of competencies which are end-to-end technology-oriented: Big Data, machine learning and artificial intelligence, augmented and virtual reality, robotics, blockchain, Internet of Things, 5G technologies, quantum technologies and others. Considering the speed of the development of these technologies, concerns have arisen about the relevance of the content of educational programs of higher education, as well as the degree of flexibility of educational trajectories of engineering graduates. To ensure the relevance and flexibility, constant revision of training programs regarding the study and development of new paradigms and solutions is required. This paper calls into question how to meet the requirements of the labor market at the time of graduation, considering that the educational programs are compiled at the time of the beginning of education and in accordance with the established federal standards of higher education. The goal is to adjust the results of the educational program or even to introduce completely new results in accordance with the changes that have occurred in the industry at the time of the implementation of the program to ensure students form the most relevant and in-demand competencies. The article describes the approaches of the Institute of Computer Technologies and Information Security (ICTIS) of the Southern Federal University to the creation of flexible, interdisciplinary bachelor&#x2019;s and master&#x2019;s degree educational programs. The content of these programs directly corresponds to the current and promising demands of the labor market. The presented approaches to the customization of educational programs meet the needs of students in personal and professional development. These approaches contribute to the development of a student-centered learning system that is specific for each level of education. Project-based learning is a key tool for the formation of relevant professional competencies within bachelor&#x2019;s degree programs. The master&#x2019;s degree focuses on the formation of unique research competencies in the context of the current agenda. For this purpose, the ICTIS has opened special research programs that involve a grant system for undergraduates. What is also important is the inclusion of undergraduates in research groups led by postdocs of the Institute. The key principle uniting these approaches into a single system is the introduction of its own educational standards in engineering areas in the ICTIS. The standards of the ICTIS regulate the possibility for students to choose variable professional competencies after mastering the basic educational component. These competencies are formed annually based on the analysis of prospective labor market demands and can be included in the program even after the start of its implementation. The analysis on the results of the ICTIS educational standards implementation has shown the effectiveness of the concept of these standards in the field of computer technology and information security. The effectiveness of this concept of the educational standard allows gradually being implemented in other fields of knowledge.",https://ieeexplore.ieee.org/document/9766804/,2022 IEEE Global Engineering Education Conference (EDUCON),28-31 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VLSID.2018.20,Tutorial T2A: Safe Autonomous Systems: Real-Time Error Detection and Correction in Safety-Critical Signal Processing and Control Algorithms,IEEE,Conferences,"While the last two decades have seen revolutions in computing and communications systems, the next few decades will see a revolution in the use of every-day robotics and artificial intelligence in broad societal applications. Examples of such systems include sensor networks, the smart power grid, self-driven cars and autonomous drones. Such systems are driven by signal processing, control and learning algorithms that process sensor data, actuate control functions and learn about the environment in which these systems operate. The trustworthiness and safety of such systems is of paramount importance and has significant impact on the commercial viability of the underlying technology. As a consequence, anomalies in system operation due to computation errors in on-board processors, degradation and failure of embedded sensors, actuators and electro-mechanical subsystems and unforeseen changes in their operation environment need to detected with minimum latency. Such anomalies also need to be mitigated in ways that ensure the safety of such systems under all possible failure scenarios. Many future systems will be selflearning in the field. It is necessary to ensure that such learning does not compromise the safety of all human personnel involved in the operation of such systems. To enable safe operation of such systems, the underlying hardware needs to be tuned in the field to maximize performance, reliability and error-resilience while minimizing power consumption. To enable such dynamic adaptation, device operating conditions and the onset of soft errors are sensed using post-manufacture and real-time checking mechanisms. These mechanisms rely on the use of built-in sensors and/or low-overhead function encoding techniques to detect anomalies in system functions. A key capability is that of being able to deduce multiple performance parameters of the system-under-test using compact optimized stimulus using learning algorithms. The sensors and function encodings assess the loss in performance of the relevant systems due to workload uncertainties, manufacturing process imperfections, soft errors and hardware malfunction and failures induced by electromechanical degradation. These are then mitigated through the use of algorithm-through-circuit level compensation techniques based on pre-deployment simulation and post-deployment self-learning. These techniques continuously trade off performance vs. power of the individual software and hardware modules in such a way as to deliver the end-to-end desired application level Quality of Service (QoS), while minimizing energy/power consumption and maximizing reliability and safety. Applications to signal processing, and control algorithms for example autonomous systems will be discussed.",https://ieeexplore.ieee.org/document/8326883/,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),6-10 Jan. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2016.7733537,UAV degradation identification for pilot notification using machine learning techniques,IEEE,Conferences,"Unmanned Aerial Vehicles are currently investigated as an important sub-domain of robotics, a fast growing and truly multidisciplinary research field. UAVs are increasingly deployed in real-world settings for missions in dangerous environments or in environments which are challenging to access. Combined with autonomous flying capabilities, many new possibilities, but also challenges, open up. To overcome the challenge of early identification of degradation, machine learning based on flight features is a promising direction. Existing approaches build classifiers that consider their features to be correlated. This prevents a fine-grained detection of degradation for the different hardware components. This work presents an approach where the data is considered uncorrelated and, using machine learning techniques, allows the precise identification of UAV's damages.",https://ieeexplore.ieee.org/document/7733537/,2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA),6-9 Sept. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/M2VIP.2018.8600864,Unsupervised Video Prediction Network with Spatio-temporal Deep Features,IEEE,Conferences,"Predicting the future states of things is an important performance form of intelligence and it is also of vital importance in real-time systems such as autonomous cars and robotics. This paper aims to tackle a video prediction task. Previous methods for future frame prediction are always subject to restrictions from environment, leading to poor accuracy and blurry prediction details. In this work, we present an unsupervised video prediction framework which iteratively anticipates the raw RGB pixel values in future video frames. Extensive experiments are implemented on advanced datasets — KTH and KITTI. The results demonstrate that our method achieves a good performance.",https://ieeexplore.ieee.org/document/8600864/,2018 25th International Conference on Mechatronics and Machine Vision in Practice (M2VIP),20-22 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECS53924.2021.9665511,Using Hardware Performance Counters to support infield GPU Testing,IEEE,Conferences,"Graphics Processing Units (GPUs) have gained importance in several domains where a high computational effort is required (e.g., where Artificial Intelligence is used). At the same time, their adoption extended to domains (e.g., automotive, robotics, aerospace) where the effects of possible hardware faults can be extremely serious. Hence, it is crucial to identify methods allowing to quickly detect the occurrence of faults in a GPU while it works in the operational phase. In this paper we propose a method based on the adoption of hardware performance counters. We show that the method is able to detect permanent faults occurring in some critical modules, thus allowing to increase the overall reliability of the system. The method does not involve significant costs, since performance counters already exist in all real GPUs, e.g., to support silicon debug, and can be easily accessed in software.",https://ieeexplore.ieee.org/document/9665511/,"2021 28th IEEE International Conference on Electronics, Circuits, and Systems (ICECS)",28 Nov.-1 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAACS-SWS50031.2020.9297062,Using a COTS Smartphone to Control an Autonomous Self-Driving Platform,IEEE,Conferences,"Recent interest in self-driving cars has boosted related fields like autonomous systems and robotics. This paper describes a simple and inexpensive small-scale self driving platform called ASV, which is based on a lowcost microcontroller and a COTS smartphone connected via WiFi. The camera of the phone, which is fixed to the platform, acquires images which are processed in a Convolutional Neural Network (CNN) inspired by the Nvidia's PilotNet. The network is trained in end-to-end learning to produce steering command to follow highway style lanes with markers on both sides. On the microcontroller, the steering commands are used for motor actuation and control of the physical movement of the platform. This paper presents the structure and implementation of ASV and evaluates its real-time performance and latency. For typical speeds encountered in small-scale systems, the performance is found more than sufficient for lane following with the CNN, leaving plenty of room for extensions. The platform's simplicity allows it to be used in research, education, and to spark interest in self-driving systems and neural networks. It can form the basis for general robot control.",https://ieeexplore.ieee.org/document/9297062/,2020 IEEE 5th International Symposium on Smart and Wireless Systems within the Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS),17-18 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF51394.2020.9443272,VLSI Hardware Architecture for Gaussian Process,IEEE,Conferences,"Gaussian process (GP) is a popular machine learning technique that is widely used in many application domains, especially in robotics. However, GP is very computation intensive and time consuming during the inference phase, thereby bringing severe challenges for its large-scale deployment in real-time applications. In this paper, we propose two efficient hardware architecture for GP accelerator. One architecture targets for general GP inference, and the other architecture is specifically optimized for the scenario when the data point is gradually observed. Evaluation results show that the proposed hardware accelerator provides significant hardware performance improvement than the general-purpose computing platform.",https://ieeexplore.ieee.org/document/9443272/,"2020 54th Asilomar Conference on Signals, Systems, and Computers",1-4 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793556,VPE: Variational Policy Embedding for Transfer Reinforcement Learning,IEEE,Conferences,"Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments.We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.",https://ieeexplore.ieee.org/document/8793556/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EEEI.2014.7005895,Verification of safety for autonomous unmanned ground vehicles,IEEE,Conferences,"The existing tools for hardware and software reliability and safety engineering do not supply sufficient solutions regarding AI (Artificial Intelligent) adaptive and learning algorithms, which are being used in autonomous robotics and massively rely on designer experience and include methods such as Heuristic, Rules based decision, Fuzzy Logic, Neural Networks, and Genetic Algorithms, Bayes Networks, etc. Since it is obvious that only this kind of algorithms can deal with the complexity and the uncertainty of the real world environment, suitable safety validation methodology is required. In this paper we present the limitation of the existing reliability and safety engineering tools in dealing with autonomous systems and propose a novel methodology based on statistical testing in simulated environment.",https://ieeexplore.ieee.org/document/7005895/,2014 IEEE 28th Convention of Electrical & Electronics Engineers in Israel (IEEEI),3-5 Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2019.8798186,Virtual Reality and Photogrammetry for Improved Reproducibility of Human-Robot Interaction Studies,IEEE,Conferences,"Collecting data in robotics, especially human-robot interactions, traditionally requires a physical robot in a prepared environment, that presents substantial scalability challenges. First, robots provide many possible points of system failure, while the availability of human participants is limited. Second, for tasks such as language learning, it is important to create environments that provide interesting' varied use cases. Traditionally, this requires prepared physical spaces for each scenario being studied. Finally, the expense associated with acquiring robots and preparing spaces places serious limitations on the reproducible quality of experiments. We therefore propose a novel mechanism for using virtual reality to simulate robotic sensor data in a series of prepared scenarios. This allows for a reproducible dataset that other labs can recreate using commodity VR hardware. We demonstrate the effectiveness of this approach with an implementation that includes a simulated physical context, a reconstruction of a human actor, and a reconstruction of a robot. This evaluation shows that even a simple “sandbox” environment allows us to simulate robot sensor data, as well as the movement (e.g., view-port) and speech of humans interacting with the robot in a prescribed scenario.",https://ieeexplore.ieee.org/document/8798186/,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),23-27 March 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1999.811679,What we learned from RoboCup-97 and RoboCup-98,IEEE,Conferences,"RoboCup is an increasingly successful attempt to promote the full integration of robotics and AI research. The most prominent feature of RoboCup is that it provides the researchers with the opportunity to demonstrate their research results as a form of competition in a dynamically changing hostile environment, defined as the international standard game definition, in which the gamut of intelligent robotics research issues are naturally involved. The article describes what we have learned from the past RoboCup activities, and overview the future perspectives of RoboCup in the next century, mainly focusing on the real robot leagues. Finally, we introduce the new leagues, one of which will have been held at RoboCup-99 in Stockholm.",https://ieeexplore.ieee.org/document/811679/,Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289),17-21 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2011.6083632,[Copyright notice],IEEE,Conferences,The following topics are dealt with: brain-machine interface; machine learning technology; service systems; homeland security systems; virtual reality; agent-based modeling; human centered transportation systems; awareness science and engineering; soft computing; enterprise information systems; social signal processing; infrastructure system; manufacturing systems; pattern recognition; medical mechatronics; minimally invasive surgery; medical robotics; medical technology; intelligent power systems; discrete event systems; Petri nets; biometrics; bioinformatics; computational intelligence; supply chain management; shared control; fault diagnosis; systems engineering; Internet; support vector machines; knowledge acquisition; cloud computing; grey systems; humanoid robots; redundant manipulators; formal methods; granular computing; wireless sensor networks; nonlinear control systems; gesture-based interaction; software engineering; multi-agent systems; cognitive computing; social robotics; natural language processing; conflict resolution; intelligent transportation systems; human-robot interaction; image processing; medical informatics; decision support systems; assistive technology; human-centered design; data mining; and anti-terrorism applications.,https://ieeexplore.ieee.org/document/6083632/,"2011 IEEE International Conference on Systems, Man, and Cybernetics",9-12 Oct. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UKSim.2012.123,[Cover art],IEEE,Conferences,The following topics are dealt with: neural networks; evolutionary computation; adaptive dynamic programming; re-enforcement learning; bio-informatics; bio-engineering; computational finance; economics; semantic mining; data mining; virtual reality; data visualization; intelligent systems; soft computing; hybrid computing; e-science; e-systems; robotics; cybernetics; manufacturing; engineering; operations research; discrete event systems; real time systems; image processing; speech processing; signal processing; industry; business; social issues; human factors; marine simulation; power systems; logistics;parallel systems; distributed systems; software architectures;Internet modelling; semantic Web; ontologies; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; circuits; sensors and devices.,https://ieeexplore.ieee.org/document/6205540/,2012 UKSim 14th International Conference on Computer Modelling and Simulation,28-30 March 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INES.2015.7329762,[Front cover],IEEE,Conferences,The following topics are dealt with: learning; Web; urban water-supply system; IP Core; DCI approach; real-time sensor network; linked open data source; process mining; image coding; deep neural network architecture; human computer interaction; social human-robot interaction; VANET; authorized V2V communication; MIMO system; surgical robotics; ontologies; genetic algorithm; image reconstruction; mobile robot; artificial neural network; fuzzy reasoning; heat exchanger; fuzzy controller design; mobile device; human machine interface design; decision support system; data mining technique; discrete-time SISO system; augmented reality; visual analysis; content management system; Androids; nonlinear MPC; collaborative filtering; recommendation; wireless sensor networks;; humidity control; temperature control and stability.,https://ieeexplore.ieee.org/document/7329762/,2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES),3-5 Sept. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2015.1,[Title page i],IEEE,Conferences,The following topics are dealt with: artificial intelligence; neural networks; fuzzy systems; evolutionary computation; bioinformatics; bioengineering; data mining; semantic mining; games; VR; visualization; intelligent systems applications; hybrid computing; soft computing; intelligent systems control; control intelligence; e-science; e-systems; robotics; cybernetics; manufacturing system; operations research; discrete event systems; real time systems; signal processing; speech processing; image processing; natural language processing; human factors; social issues; shipping; marine simulation; transport; logistics; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; software architectures; distributed systems; parallel systems; power simulation; performance engineering; communication systems; and circuits.,https://ieeexplore.ieee.org/document/7604531/,"2015 3rd International Conference on Artificial Intelligence, Modelling and Simulation (AIMS)",2-4 Dec. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCET.2010.5486348,[Title page],IEEE,Conferences,The following topics are dealt with: artificial intelligence; automated software engineering; bioinformatics; scientific computing; biomedical engineering; compilers; interpreters; computational intelligence; computer animation; computer architecture; VLSI; information systems; computer based education; computer games; digital systems; distributed systems; parallel processing; e-commerce; e-governance; event driven programming; expert systems; human computer interaction; image processing; information retrieval; embedded systems; Internet; Web application; knowledge data engineering; virtual reality; mobile computing; multimedia applications; computer modeling; natural language processing; computer networks; neural networks; computer security; computer simulation; pattern recognition; computer vision; performance evaluation; computer aided design; programming languages; computing ethics; robotics; control systems; cryptography; data communication; software engineering; system security; data compression; data encryption; data mining; digital library; wireless sensor networks; ubiquitous computing; technology management.,https://ieeexplore.ieee.org/document/5486348/,2010 2nd International Conference on Computer Engineering and Technology,16-18 April 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561446,"droidlet: modular, heterogenous, multi-modal agents",IEEE,Conferences,"In recent years, there have been significant advances in building end-to-end Machine Learning (ML) systems that learn at scale. But most of these systems are: (a) isolated (perception, speech, or language only); (b) trained on static datasets. On the other hand, in the field of robotics, large-scale learning has always been difficult. Supervision is hard to gather and real world physical interactions are expensive.In this work we introduce and open-source droidlet, a modular, heterogeneous agent architecture and platform. It allows us to exploit both large-scale static datasets in perception and language and sophisticated heuristics often used in robotics; and provides tools for interactive annotation. Furthermore, it brings together perception, language and action onto one platform, providing a path towards agents that learn from the richness of real world interactions.",https://ieeexplore.ieee.org/document/9561446/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9340956,robo-gym – An Open Source Toolkit for Distributed Deep Reinforcement Learning on Real and Simulated Robots,IEEE,Conferences,"Applying Deep Reinforcement Learning (DRL) to complex tasks in the field of robotics has proven to be very successful in the recent years. However, most of the publications focus either on applying it to a task in simulation or to a task in a real world setup. Although there are great examples of combining the two worlds with the help of transfer learning, it often requires a lot of additional work and fine-tuning to make the setup work effectively. In order to increase the use of DRL with real robots and reduce the gap between simulation and real world robotics, we propose an open source toolkit: robo-gym1. We demonstrate a unified setup for simulation and real environments which enables a seamless transfer from training in simulation to application on the robot. We showcase the capabilities and the effectiveness of the framework with two real world applications featuring industrial robots: a mobile robot and a robot arm. The distributed capabilities of the framework enable several advantages like using distributed algorithms, separating the workload of simulation and training on different physical machines as well as enabling the future opportunity to train in simulation and real world at the same time. Finally, we offer an overview and comparison of robo-gym with other frequently used state-of-the-art DRL frameworks.",https://ieeexplore.ieee.org/document/9340956/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759250,Efficient learning of stand-up motion for humanoid robots with bilateral symmetry,IEEE,Conferences,"Standing up after falling is an essential ability for humanoid robots in order to resume their tasks without help from humans. Although many humanoid robots, especially small-size humanoid robots, have their own stand-up motions, there has not been a generalized method to automatically learn flexible stand-up motions for humanoid robots which can be applied to various fallen positions. In this research, we propose a method for learning stand-up motions for humanoid robots using Q-learning making use of their bilateral symmetry. We implemented this method on DarwIn-OP humanoid robots and learned an optimal policy in simulation. We compared the resulting stand-up motion with manually designed stand-up motions and with stand-up motions learned without considering bilateral symmetry. Both in simulation and on the real robot, the new stand-up motion was successful in most trials while other motions took longer or were not as robust.",https://ieeexplore.ieee.org/document/7759250/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2012.6385832,Elastic strips: Implementation on a physical humanoid robot,IEEE,Conferences,"For robots to operate in human environments, they are required to react safely to unexpected changes in the work area. However, existing manipulation task planning methods take more than several seconds or minutes to update their solutions when environmental changes are recognized. Furthermore, the computation time exponentially increases in case of highly complex structures such as humanoid robots. Therefore, we propose a reactive system for high d.o.f. robots to perform interactive manipulation tasks under real-time conditions. The paper describes the implementation of the Elastic Strip Framework, a plan modification approach to update initial motion plans. To improve its real-time performance and reliability, the previous geometric approximation is replaced by an implicit method that constructs an elastic tunnel for collision checking. Additionally, in order to maintain a robust system even in exceptional situations, such as undetected obstacles, the force transformer module executes compliant motions, and the current elastic strip adapts the path tracking motion by monitoring tracking errors of the actual motion. The proposed system is applied to a Honda humanoid robot. Real-time performance is successfully demonstrated in real-world experiments.",https://ieeexplore.ieee.org/document/6385832/,2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,7-12 Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2001.976268,Embedding cooperation in robots to play soccer game,IEEE,Conferences,"Robotic soccer provides an opportunity to explore such a challenging research topic that multiple agents (physical robots or sofbots) work together in a realtime, noisy and adversarial environment to obtain specific objectives. It requires each agent can not only deal with infinite unpredictable situations, but also present cooperation with others. The previous researches about cooperation often put emphasis on task decomposition and conflict avoidance among team members. In this paper, we describe a robot architecture, which addresses ""scaling cooperation"" among robots, and meanwhile keeps each robot making decision independently. The architecture is based on ""ideal cooperation"" principle and implemented for Small Robot League in RoboCup Experimental results prove its effectiveness and reveal several primary characteristics of behaviors in robotic soccer. Finally, some important problems of future work are discussed.",https://ieeexplore.ieee.org/document/976268/,Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180),29 Oct.-3 Nov. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII.2019.8700376,Emotion Recognition from Speech for an Interactive Robot Agent,IEEE,Conferences,"Speech is one of the fundamental approaches for human to human interaction. Given this, it should be the main approach for robot human interaction as well. Towards this, the research presented here focuses on emotion recognition from human speech to aid the interaction between humans and robots. There are various steps involved in developing emotion recognition system for an interactive robot agent. The first step is to choose a suitable dataset that is Berlin database for training and testing the models developed. The second important step is extraction and choice of suitable features related to emotions. The third step is to make an appropriate classification scheme. The performance of each classifier is analyzed and acomparison among multiple frameworks of emotion recognition is made. In response to the findings in these preliminary studies, a prototype application was developed to allow the recognition of emotions from speech in real-time for future use on an interactive robot. On a preliminary test set, the application achieved performance levels between 81% and 92%. The approach offers the integration of speech collection hardware, emotion recognition software, mobile devices and robotic systems to aid assist human-robot interaction.",https://ieeexplore.ieee.org/document/8700376/,2019 IEEE/SICE International Symposium on System Integration (SII),14-16 Jan. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN46459.2019.8956327,End-User Programming of Low-and High-Level Actions for Robotic Task Planning,IEEE,Conferences,"Programming robots for general purpose applications is extremely challenging due to the great diversity of end-user tasks ranging from manufacturing environments to personal homes. Recent work has focused on enabling end-users to program robots using Programming by Demonstration. However, teaching robots new actions from scratch that can be reused for unseen tasks remains a difficult challenge and is generally left up to robotic experts. We propose iRoPro, an interactive Robot Programming framework that allows end-users to teach robots new actions from scratch and reuse them with a task planner. In this work we provide a system implementation on a two-armed Baxter robot that (i) allows simultaneous teaching of low-and high-level actions by demonstration, (ii) includes a user interface for action creation with condition inference and modification, and (iii) allows creating and solving previously unseen problems using a task planner for the robot to execute in real-time. We evaluate the generalisation power of the system on six benchmark tasks and show how taught actions can be easily reused for complex tasks. We further demonstrate its usability with a user study (N=21), where users completed eight tasks to teach the robot new actions that are reused with a task planner. The study demonstrates that users with any programming level and educational background can easily learn and use the system.",https://ieeexplore.ieee.org/document/8956327/,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),14-18 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE.2018.8326229,End-to-end deep learning for autonomous navigation of mobile robot,IEEE,Conferences,"This paper proposes an end-to-end method for training convolutional neural networks for autonomous navigation of a mobile robot. Traditional approach for robot navigation consists of three steps. The first step is extracting visual features from the scene using the camera input. The second step is to figure out the current position by using a classifier on the extracted visual features. The last step is making a rule for moving the direction manually or training a model to handle the direction. In contrast to the traditional multi-step method, the proposed visuo-motor navigation system can directly output the linear and angular velocities of the robot from an input image in a single step. The trained model gives wheel velocities for navigation as outputs in real-time making it possible to be implanted on mobile robots such as robotic vacuum cleaner. The experimental results show an average linear velocity error of 2.2 cm/s and average angular velocity error of 3.03 degree/s. The robot deployed with the proposed model can navigate in a real-world environment by only using the camera without relying on any other sensors such as LiDAR, Radar, IR, GPS, IMU.",https://ieeexplore.ieee.org/document/8326229/,2018 IEEE International Conference on Consumer Electronics (ICCE),12-14 Jan. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII52469.2022.9708826,Evaluation of Variable Impedance- and Hybrid Force/MotionControllers for Learning Force Tracking Skills,IEEE,Conferences,"For robots to perform real-world force interaction tasks with human level dexterity, it is crucial to develop adaptable and compliant force controllers. Learning techniques, especially reinforcement learning, provide a platform to develop adaptable controllers for complex robotic tasks. This paper presents an evaluation of two prominent force control methods, variable impedance control and hybrid force-motion control in a robot learning framework. The controllers are evaluated on a Franka Emika Panda robotic manipulator for a robotic interaction task demanding force and motion tracking using a model-based reinforcement learning algorithm, PILCO. Utilizing the learning framework to find the optimal controller parameters has significantly improved the performance of the controllers. The implementation of the controllers integrated with the robot learning framework is available on https://github.com/martihmy/Compliant_control.",https://ieeexplore.ieee.org/document/9708826/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC45853.2021.9504892,Evolutionary Inherited Neuromodulated Neurocontrollers with Objective Weighted Ranking,IEEE,Conferences,"In the physical world, individuals compete against others within their own population or separately evolving populations. Robotic agents will soon face this coevolutionary and adversarial reality. Many challenges not encountered in the evolution of single populations are encountered in adversarial coevolution. These challenges can render single evolutionary approaches either less effective or ineffective. Most problems have a fundamental goal, but also feature secondary desirable objectives. Here, evader agents in the pursuit-evasion game that do not elude capture are effectively useless. Secondly, when applied to evolve real robots online in a coevolutionary context, non-optimal robots can be erratic and cause damage. Objective hierarchy can be used to define the importance of each objective, and promote quicker optimization of primary objectives such as evasion. The Evolutionary Inherited Neuromodulated Neurocontroller (EINN) method incorporates objective weighted ranking (OWR), a novel objective hierarchy method that promotes optimization of the primary objective in simultaneous multi-objective optimization. EINN is compared to the previously demonstrated Lamarckian-inherited Neuromodulated MultiObjective Evolutionary Neurocontroller (LNMOEN), and shown to be effective in a single evolutionary context.",https://ieeexplore.ieee.org/document/9504892/,2021 IEEE Congress on Evolutionary Computation (CEC),28 June-1 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAIA.1989.49141,Experiences with the subsumption architecture,IEEE,Conferences,"A subsumption architecture has been proposed as an effective approach for the construction of robust, real-time control systems for mobile robots. To investigate its strengths and weaknesses, a simulation of the architecture was developed called the Subsumption Architecture Tool (SAT). This simulation allows various models of system behavior to be quickly built and tested. During the building and testing of the SAT, issues related to some architectural features became evident: level of commitment of each layer; code redundancy; problem decomposition and programming style; complexity of large system; and abstract reasoning capabilities. The effects of these issues are presented with respect to the design and implementation choices of two sample layers of behavior. These layers are used to illustrate considerations that need to be taken into account when a project team is considering the use of the subsumption architecture or when a subsumption-architecture-based system is being designed and implemented.<>",https://ieeexplore.ieee.org/document/49141/,[1989] Proceedings. The Fifth Conference on Artificial Intelligence Applications,6-10 March 1989,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CMCSN.2012.100,Experimental Study on Long-Range Navigation Behavior of Agricultural Robots,IEEE,Conferences,"In this paper, we study on the navigating bahevior of CRFNFP[1]-based agricultual robots in real scenes. We designed two sets of experiments and three navigating system with different configuration of software for comparative study. The experimental results indicate that, compared to the traditional local-map-based navigating systems, the CRFNFP-based navigating system does enhance the long-range perception for mobile robots and helps planning more efficient paths for the navigation.",https://ieeexplore.ieee.org/document/6245858/,"2012 International Conference on Computing, Measurement, Control and Sensor Network",7-9 July 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636594,Explaining the Decisions of Deep Policy Networks for Robotic Manipulations,IEEE,Conferences,"Deep policy networks enable robots to learn behaviors to solve various real-world complex tasks in an end-to-end fashion. However, they lack transparency to provide the reasons of actions. Thus, such a black-box model often results in low reliability and disruptive actions during the deployment of the robot in practice. To enhance its transparency, it is important to explain robot behaviors by considering the extent to which each input feature contributes to determining a given action. In this paper, we present an explicit analysis of deep policy models through input attribution methods to explain how and to what extent each input feature affects the decisions of the robot policy models. To this end, we present two methods for applying input attribution methods to robot policy networks: (1) we measure the importance factor of each joint torque to re ect the influence of the motor torque on the end-effector movement, and (2) we modify a relevance propagation method to handle negative inputs and outputs in deep policy networks properly. To the best of our knowledge, this is the first report to identify the dynamic changes of input attributions of multi-modal sensor inputs in deep policy networks online for robotic manipulation.",https://ieeexplore.ieee.org/document/9636594/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793829,Exploiting Trademark Databases for Robotic Object Fetching,IEEE,Conferences,"Service robots require the ability to recognize various household objects in order to carry out certain tasks, such as fetching an object for a person. Manually collecting information on all the objects a robot may encounter in a household is tedious and time-consuming; therefore this paper proposes the use of large-scale data from existing trademark databases. These databases contain logo images and a description of the goods and services the logo was registered under. For example, Pepsi is registered under soft drinks. We extend domain randomization in order to generate synthetic data to train a convolutional neural network logo detector, which outperformed previous logo detectors trained on synthetic data. We also provide a practical implementation for object fetching on a robot, which uses a Kinect and the logo detector to identify the object the human user requested. Tests on this robot indicate promising results, despite not using any real world photos for training.",https://ieeexplore.ieee.org/document/8793829/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AMC.2019.8371065,"Extending the life of legacy robots: MDS-Ach, a real-time, process based, networked, secure middleware based on the x-Ach methodology",IEEE,Conferences,"This work shows how to add modern tools to legacy robots while retaining the original tools and original calibration procedures/utilities through the use of a lightweight middleware connected to the communications level of the robot. MDS-Ach is a middleware made for the Xitome Mobile Dexterous Social (MDS) Robot originally released in 2008. The robot is being actively used at multiple locations including the U.S. Naval Research Laboratory's Laboratory for Autonomous Systems Research (NRL-LASR). The MDS-Ach middleware gives the MDS Robot the software capabilities of modern robot systems using the x- Ach real-time processes based architecture. It controls the MDS Robot directly over the controller area network (CAN) bus via a dedicated real-time daemon. Each process communicates with the others over a network capable shared memory. The shared memory is a ""first-in-last-out"" (i.e. reads the newest data first) non-head-of-line blocking ring buffer which ensures readability of latest data first while retaining the ability to retrieve the older data. When running over a network, UDP or TCP protocol can be utilized depending on the timing and reliability requirements. SSH tunneling is used when secure connections between networked controllers are required. The MDS-Ach middleware is designed to allow for simple and easy development with modern robotic tools while adding accessibility and usability to our non-hardware-focused partners. Real-time collision avoidance and a robust inverse kinematics solution are implemented within the MDS-Ach system. Examples of collision avoidance, inverse kinematics implementation, and the software architecture are given.",https://ieeexplore.ieee.org/document/8371065/,2018 IEEE 15th International Workshop on Advanced Motion Control (AMC),9-11 March 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSyS47076.2019.8982469,FPGA-enabled Binarized Convolutional Neural Networks toward Real-time Embedded Object Recognition System for Service Robots,IEEE,Conferences,"In this presentation, we report the results of applying a binarized Convolutional Neural Network (CNN) and a Field Programmable Gate Array (FPGA) for image-based object recognition. While the demand rises for robots with robust object recognition implemented with Neural Networks, a tradeoff between data processing rate and power consumption persists. Some applications utilise Graphics Processing Units (GPU), which results in high power consumption, thus undesirable for embedded systems, while the others communicate with cloud computers to minimise computational resources at the clients' side, i.e. robots, raising another concern that the robots are unable to perform object recognition without the servers and network connections. To overcome these difficulties, we propose an embedded object recognition system implemented with a binarized CNN and an FPGA. FPGAs consist of a matrix of reconfigurable logic gates allowing parallel computing which befit most image processing algorithms such as the CNN. We train the binarized CNN on one of our datasets that contain images of several kinds of food and beverages. The results of the experiments show that the binarized CNN with an FPGA maintains high accuracy as well as real-time computation, suggesting that the proposed system is suitable for robots to perform their tasks in a real-world environment without needing to communicate with a server.",https://ieeexplore.ieee.org/document/8982469/,2019 IEEE International Circuits and Systems Symposium (ICSyS),18-19 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR52367.2021.9517650,FT-MSTC: An Efficient Fault Tolerance Algorithm for Multi-robot Coverage Path Planning,IEEE,Conferences,"Fault tolerance is very important for multi-robot systems, especially for those operated in remote environments. The ability to tolerate failures, allows robots effectively to continue performing tasks without the need for immediate human intervention. In this paper, we present a new efficient fault tolerance algorithm for multi-robot coverage path planning (mCPP). The entire coverage path is considered as a topological task loop. The ideal mCPP problem is handled by partitioning this task loop and assign each partition to individual robot. When a faulty robot is detected, we use an optimization method to minimize the overall maximum coverage cost while considering both the tasks accomplished before robot failures and the remaining tasks. We perform various experiments for regular grid maps and real field terrains. We compare our algorithm against other coverage path planning algorithms and our algorithm outperforms existing spiral-STC-based methods in terms of the overall maximum coverage cost.",https://ieeexplore.ieee.org/document/9517650/,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),15-19 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCC54389.2021.9674390,Face Recognition System with Feature Fusion for Rehabilitation Robots in Healthcare,IEEE,Conferences,"Face recognition is a research problem across multiple disciplines such as computer vision, pattern recognition, artificial intelligence, psychology, healthcare etc. At present, the rehabilitation centers in hospitals are dependent on the nursing staff with no implementation of rehabilitation automation. In this study of face detection, a facial expression recognition system was developed which can be useful for robots of rehabilitation nursing beds. This article mainly researches two aspects: face image feature extraction and classifier design. Since many low-resolution face images are often encountered in actual environments (such as surveillance), if you use traditional methods to directly extract image features, the results are often not satisfactory. For the influence of face recognition results, this paper proposes a feature fusion extraction method based on KNN with LDA(Fisherfaces) and PCA (eigenfaces). First, the original image (low resolution) is decomposed into three different resolution images, and then the three images are used as the basis for the position to extract facial features, then LDA and PCA are used to perform preliminary analysis to obtain reduced dimensional reduction features. Finally use the K-nearest neighbor classifier to perform classification prediction. Study compares KNN-LDA with KNN-PCA with different distances of KNN algorithm.",https://ieeexplore.ieee.org/document/9674390/,2021 7th International Conference on Computer and Communications (ICCC),10-13 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2004.1380629,Fault tolerance for communication-based multirobot formation,IEEE,Conferences,"This paper investigates the ability of fault tolerance for multirobot formation, which is important for practical formation in complex environment. Our model enables mobile robots group to continue to complete given tasks by reorganizing their formation, when some members are in failure. First, to build such model, a multi-agent architecture is presented, which is implemented through communication. Second, we introduce the hierarchy graph of multirobot formation to be the theoretical foundation of the fault tolerance system. The graph analysis is suitable for general leader-follower formation format. And then, the failure detection mechanism for formation is discussed. Finally, integrated fault tolerance algorithm is investigated, including supplement for faulty robots and formation reconfiguration. The improved agent architecture adding the fault tolerance module is also presented. The experiments on real multiple mobile robots demonstrate our design is feasible.",https://ieeexplore.ieee.org/document/1380629/,Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),26-29 Aug. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CYBER53097.2021.9588329,Fault-Aware Robust Control via Adversarial Reinforcement Learning,IEEE,Conferences,"Robots have limited adaptation ability compared to humans and animals in the case of damage. However, robot damages are prevalent in realworld applications, especially for robots deployed in extreme environments. The fragility of robots greatly limits their widespread application. We propose an adversarial reinforcement learning framework, which significantly increases robot robustness over joint damage cases in both manipulation tasks and locomotion tasks. The agent is trained iteratively under the joint damage cases where it has poor performance. We validate our algorithm on a three-fingered robot hand and a quadruped robot. Our algorithm can be trained only in simulation and directly deployed on a real robot without any fine-tuning. It also demonstrates exceeding success rates over arbitrary joint damage cases.",https://ieeexplore.ieee.org/document/9588329/,"2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARSC52212.2021.9429801,Few-Shot Visual Grounding for Natural Human-Robot Interaction,IEEE,Conferences,"Natural Human-Robot Interaction (HRI) is one of the key components for service robots to be able to work in human-centric environments. In such dynamic environments, the robot needs to understand the intention of the user to accomplish a task successfully. Towards addressing this point, we propose a software architecture that segments a target object from a crowded scene, indicated verbally by a human user. At the core of our system, we employ a multi-modal deep neural network for visual grounding. Unlike most grounding methods that tackle the challenge using pre-trained object detectors via a two-stepped process, we develop a single stage zero-shot model that is able to provide predictions in unseen data. We evaluate the performance of the proposed model on real RGB-D data collected from public scene datasets. Experimental results showed that the proposed model performs well in terms of accuracy and speed, while showcasing robustness to variation in the natural language input.",https://ieeexplore.ieee.org/document/9429801/,2021 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC),28-29 April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RIOS.2013.6595317,Flexible snake robot: Design and implementation,IEEE,Conferences,"This paper presents a snake robot able to pass different and difficult paths because of special physical form and movement joints mechanism. These snake robots have no passive wheels. The robot moves by friction between the robot body and the surface on which it is. The joints have been designed and fabricated in a way that each joint has two freedom grades and it may move 228 degrees in every direction. Each joint has two DC servo motors and the power is transferred from the motors output to the joint shaft through bevel gear. The flexibility of the robot makes possible to move forward, back and laterally by imitating real snake's moves. In this paper different measures have been presented in order to design and assemble the joints, motors driver, different ways to guide the robot and its vision.",https://ieeexplore.ieee.org/document/6595317/,2013 3rd Joint Conference of AI & Robotics and 5th RoboCup Iran Open International Symposium,8-8 April 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2015.7223421,Flying robot manipulation system using a virtual plane,IEEE,Conferences,"The flexible movements of flying robots make it difficult for novices to manipulate them precisely with controllers such as a joystick and a proportional radio system. Moreover, the mapping of instructions between a robot and its reactions is not necessarily intuitive for users. We propose manipulation methods for flying robots using augmented reality technologies. In the proposed system, a virtual plane is superimposed on a flying robot and users control the robot by manipulating the virtual plane and drawing a moving path on it. We present the design and implementation of our system and describe experiments conducted to evaluate our methods.",https://ieeexplore.ieee.org/document/7223421/,2015 IEEE Virtual Reality (VR),23-27 March 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759701,From indoor GIS maps to path planning for autonomous wheelchairs,IEEE,Conferences,"This work focuses on how to compute trajectories for an autonomous wheelchair based on indoor GIS maps, in particular on IndoorGML maps, which set the standard in this context. Good wheelchair trajectories are safe and comfortable for the user and the people sharing the space with him, turn gently, are high legible, and smooth (at least G2 continuos). We derive a navigation graph from a given IndoorGML map. We define and solve an optimization problem to find the desired path: given a succession of cells to traverse, the path corresponds to the best composite Bézier trajectory for the wheelchair. We discuss a related multi-objective path planning problem. Experimental results and an implementation on real robots show the planner performance.",https://ieeexplore.ieee.org/document/7759701/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCI-CC.2014.6921432,From information revolution to intelligence revolution: Big data science vs. intelligence science,IEEE,Conferences,"The hierarchy of human knowledge is categorized at the levels of data, information, knowledge, and intelligence. For instance, given an AND-gate with 1,000-input pins, it may be described very much differently at various levels of perceptions in the knowledge hierarchy. At the data level on the bottom, it represents a 21,000 state space, known as `big data' in recent terms, which appears to be a big issue in engineering. However, at the information level, it just represents 1,000 bit information that is equivalent to the numbers of inputs. Further, at the knowledge level, it expresses only two rules that if all inputs are one, the output is one; and if any input is zero, the output is zero. Ultimately, at the intelligence level, it is simply an instance of the logical model of an AND-gate with arbitrary inputs. This problem reveals that human intelligence and wisdom are an extremely efficient and a fast convergent induction mechanism for knowledge and wisdom elicitation and abstraction where data are merely factual materials and arbitrary instances in the almost infinite state space of the real world. Although data and information processing have been relatively well studied, the nature, theories, and suitable mathematics underpinning knowledge and intelligence are yet to be systematically studied in cognitive informatics and cognitive computing. This will leads to a new era of human intelligence revolution following the industrial, computational, and information revolutions. This is also in accordance with the driving force of the hierarchical human needs from low-level material requirements to high-level ones such as knowledge, wisdom, and intelligence. The trend to the emerging intelligent revolution is to meet the ultimate human needs. The basic approach to intelligent revolution is to invent and embody cognitive computers, cognitive robots, and cognitive systems that extend human memory capacity, learning ability, wisdom, and creativity. Via intelligence revolution, an interconnected cognitive intelligent Internet will enable ordinary people to access highly intelligent systems created based on the latest development of human knowledge and wisdom. Highly professional systems may help people to solve typical everyday problems. Towards these objectives, the latest advances in abstract intelligence and intelligence science investigated in cognitive informatics and cognitive computing are well positioned at the center of intelligence revolution. A wide range of applications of cognitive computers have been developing in ICIC [http://www.ucalgary.ca/icic/] such as, inter alia, cognitive computers, cognitive robots, cognitive learning engines, cognitive Internet, cognitive agents, cognitive search engines, cognitive translators, cognitive control systems, cognitive communications systems, and cognitive automobiles.",https://ieeexplore.ieee.org/document/6921432/,2014 IEEE 13th International Conference on Cognitive Informatics and Cognitive Computing,18-20 Aug. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDC40024.2019.9029916,From self-tuning regulators to reinforcement learning and back again,IEEE,Conferences,"Machine and reinforcement learning (RL) are increasingly being applied to plan and control the behavior of autonomous systems interacting with the physical world. Examples include self-driving vehicles, distributed sensor networks, and agile robots. However, when machine learning is to be applied in these new settings, the algorithms had better come with the same type of reliability, robustness, and safety bounds that are hallmarks of control theory, or failures could be catastrophic. Thus, as learning algorithms are increasingly and more aggressively deployed in safety critical settings, it is imperative that control theorists join the conversation. The goal of this tutorial paper is to provide a starting point for control theorists wishing to work on learning related problems, by covering recent advances bridging learning and control theory, and by placing these results within an appropriate historical context of system identification and adaptive control.",https://ieeexplore.ieee.org/document/9029916/,2019 IEEE 58th Conference on Decision and Control (CDC),11-13 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COASE.2017.8256157,Full automatic path planning of cooperating robots in industrial applications,IEEE,Conferences,"Parts made of carbon fiber reinforced plastics (CFRP) for airplane components can be so huge that a single industrial robot is no longer able to handle them, and cooperating robots are required. Manual programming of cooperating robots is difficult, but with large numbers of different sized and shaped cut-pieces, it is almost impossible. This paper presents an automated production system consisting of a camera for the precise detection of the position of each cut-piece and a collision-free path planner which can dynamically react to different positions for the transfer motions. The path is planned for multiple robots adhering to motion constrains, such as the requirement that the textile cut-piece must form a catenary which can change during transport. Additionally a technique based on machine learning has been implemented which correctly resolves redundancy for a linear axis during planning. Finally, all components are tested on a real robot system in industrial scale.",https://ieeexplore.ieee.org/document/8256157/,2017 13th IEEE Conference on Automation Science and Engineering (CASE),20-23 Aug. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSAI.2017.8248355,Fully convolutional denoising autoencoder for 3D scene reconstruction from a single depth image,IEEE,Conferences,"In this work, we propose a 3D scene reconstruction algorithm based on a fully convolutional 3D denoising autoencoder neural network. The network is capable of reconstructing a full scene from a single depth image by creating a 3D representation of it and automatically filling holes and inserting hidden elements. We exploit the fact that our neural network is capable of generalizing object shapes by inferring similarities in geometry. Our fully convolutional architecture enables the network to be unconstrained by a fixed 3D shape, and so it is capable of successfully reconstructing arbitrary scene sizes. Our algorithm was evaluated on a real word dataset of tabletop scenes acquired using a Kinect and processed using KinectFusion software in order to obtain ground truth for network training and evaluation. Extensive measurements show that our deep neural network architecture outperforms the previous state of the art both in terms of precision and recall for the scene reconstruction task. The network has been broadly profiled in terms of memory footprint, number of floating point operations, inference time and power consumption in CPU, GPU and embedded devices. Its small memory footprint and its low computation requirements enable low power, memory constrained, real time always-on embedded applications such as autonomous vehicles, warehouse robots, interactive gaming controllers and drones.",https://ieeexplore.ieee.org/document/8248355/,2017 4th International Conference on Systems and Informatics (ICSAI),11-13 Nov. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZY.2006.1681996,Fuzzy Logic based Active Map Learning for Autonomous Robot,IEEE,Conferences,"The paper proposes a fast map learning approach for real-time map building and active exploration in unknown indoor environments. This approach includes a map model, a map update method, an exploration method, and a map postprocessing method. The map adopts a grid-based representation and uses frequency value to measure the confidence that a cell is occupied by an obstacle. The exploration method is implemented by coordinating two novel behaviors: path-exploring behavior and environment-detection behavior. Fuzzy logic is used to implement the behavior design and coordination. The fast map update and path planning (i.e. the exploration method) make our approach a candidate for real-time implementation on mobile robots. The results are demonstrated by simulated experiments based on a Pioneer robot with eight forward sonar sensors.",https://ieeexplore.ieee.org/document/1681996/,2006 IEEE International Conference on Fuzzy Systems,16-21 July 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2004.1374845,Fuzzy reinforcement learning for an evolving virtual servant robot,IEEE,Conferences,"This work presents our research in the application of reinforcement learning algorithms for the generation of autonomous intelligent virtual robots, that can learn and enhance their task performance in assisting humans in housekeeping. For the control system architecture of the virtual agents, two algorithms, based on Watkins' Q(/spl lambda/) learning and the zeroth-level classifier system (ZLCS), are incorporated with fuzzy inference systems(FlS). Performance of these algorithms is evaluated and compared. A 3D application of a virtual robot whose task is to interact with virtual humans and offer optimal services on everyday in-house needs is designed and implemented. The learning systems are incorporated in the decision-making process of the virtual robot servant to allow itself to understand and evaluate the fuzzy value requirements and enhance its performance.",https://ieeexplore.ieee.org/document/1374845/,RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No.04TH8759),22-22 Sept. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967785,GQ-STN: Optimizing One-Shot Grasp Detection based on Robustness Classifier,IEEE,Conferences,"Grasping is a fundamental robotic task needed for the deployment of household robots or furthering warehouse automation. However, few approaches are able to perform grasp detection in real time (frame rate). To this effect, we present Grasp Quality Spatial Transformer Network (GQ-STN), a one-shot grasp detection network. Being based on the Spatial Transformer Network (STN), it produces not only a grasp configuration, but also directly outputs a depth image centered at this configuration. By connecting our architecture to an externally-trained grasp robustness evaluation network, we can train efficiently to satisfy a robustness metric via the backpropagation of the gradient emanating from the evaluation network. This removes the difficulty of training detection networks on sparsely annotated databases, a common issue in grasping. We further propose to use this robustness classifier to compare approaches, being more reliable than the traditional rectangle metric. Our GQ-STN is able to detect robust grasps on the depth images of the Dex-Net 2.0 dataset with 92.4 % accuracy in a single pass of the network. We finally demonstrate in a physical benchmark that our method can propose robust grasps more often than previous sampling-based methods, while being more than 60 times faster.",https://ieeexplore.ieee.org/document/8967785/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2007.4303585,Genetic Programming in Robot Exploration,IEEE,Conferences,"Exploration using mobile robots is an active research area. In general, an optimal robot exploration strategy is difficult to obtain. In this paper an investigation is conducted using genetic programming (GP) to solve this problem. GP is a form of artificial intelligence capable of automatically creating and developing computer programs to solve problems using the theory of evolution. However, like many other learning algorithms, GP is a computationally expensive and time-consuming process. This characteristic can impede its application where learning time is limited, such as in real-time robotic control applications. Therefore, this paper further investigates the possibility of developing a time-efficient GP algorithm to reduce evolution time. This is done by directly incorporating the amount of time evolved solutions take to form into the fitness function, in order to encourage time efficient problem solving. Experimental results have shown that while the time efficient aspect of the proposed GP algorithm is not conclusive, the robot exploration using GP produces promising outcomes.",https://ieeexplore.ieee.org/document/4303585/,2007 International Conference on Mechatronics and Automation,5-8 Aug. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CACRE52464.2021.9501291,Give Me a Wrench!: Finding Tools for Human Partners in Human-Robot Collaborative Manufacturing Contexts,IEEE,Conferences,"Manufacturing processes can be optimized by enabling human-robot collaboration. A relevant goal in this area is to create a collaborative solution in which robots can provide assisting actions to humans, thereby, reducing menial labor as well as increasing productivity. The solution is based on implementing efficient hand-over of mechanical tools from robots to humans. Hand-over tasks are inevitable in human-robot collaborative manufacturing contexts. These tasks need three-step mechanism: object identification, object grasping, and the actual hand-over. This paper presents an approach for robots to find tools for human partners in human-robot collaboration via deep learning. This is achieved using the object detection system YOLOv3 for identification of commonly used mechanical tools. By training on a custom dataset of 800 images of mechanical tools created for the study, the tool recognition is implemented in realworld human-robot hand-over tasks. Experimental results show that the proposed approach achieves a high accuracy for identification of tools in real-world human-robot collaboration. Future work of this study is also discussed.",https://ieeexplore.ieee.org/document/9501291/,"2021 6th International Conference on Automation, Control and Robotics Engineering (CACRE)",15-17 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN47096.2020.9223558,HATSUKI : An anime character like robot figure platform with anime-style expressions and imitation learning based action generation,IEEE,Conferences,"Japanese character figurines are popular and have a pivot position in Otaku culture. Although numerous robots have been developed, few have focused on otaku-culture or on embodying anime character figurines. Therefore, we take the first steps to bridge this gap by developing Hatsuki, which is a humanoid robot platform with anime based design. Hatsuki’s novelty lies in its aesthetic design, 2D facial expressions, and anime-style behaviors that allows Hatsuki to deliver rich interaction experiences resembling anime-characters. We explain our design implementation process of Hatsuki, followed by our evaluations. In order to explore user impressions and opinions towards Hatsuki, we conducted a questionnaire in the world’s largest anime-figurine event. The results indicate that participants were generally very satisfied with Hatsuki’s design, and proposed various use case scenarios and deployment contexts for Hatsuki. The second evaluation focused on imitation learning, as such a method can provide better interaction ability in the real world and generate rich, context-adaptive behaviors in different situations. We made Hatsuki learn 11 actions, combining voice, facial expressions and motions, through the neural network based policy model with our proposed interface. Results show our approach was successfully able to generate the actions through self-organized contexts, which shows the potential for generalizing our approach in further actions under different contexts. Lastly, we present our future research direction for Hatsuki and provide our conclusion.",https://ieeexplore.ieee.org/document/9223558/,2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),31 Aug.-4 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEA.2006.257252,Hand Posture Recognition in Gesture-Based Human-Robot Interaction,IEEE,Conferences,"Natural and friendly interface is critical for the development of service robots. Gesture-based interface offers a way to enable untrained users to interact with robots more easily and efficiently. In this paper, we present a posture recognition system implemented on a real humanoid service robot. The system applies the RCE neural network based color segmentation algorithm to separate hand images from complex backgrounds. The topological features of the hand are then extracted from the silhouette of the segmented hand region. Based on the analysis of these simple but distinctive features, hand postures are identified accurately. Experimental results on gesture-based robot programming demonstrated the effectiveness and robustness of the system",https://ieeexplore.ieee.org/document/4025853/,2006 1ST IEEE Conference on Industrial Electronics and Applications,24-26 May 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2008.4651150,High-dimensional underactuated motion planning via task space control,IEEE,Conferences,"Kinodynamic planning algorithms have the potential to find feasible control trajectories which accomplish a task even in very nonlinear or constrained dynamical systems. Underactuation represents a particular form of a dynamic constraint, inherently present in many machines of interest (e.g., walking robots), and necessitates planning for long-term control solutions. A major limitation in motion planning techniques, especially for real-time implementation, is that they are only practical for relatively low degree-of-freedom problems. Here we present a model-based dimensionality reduction technique based on an extension of partial feedback linearization control into a task-space framework. This allows one to plan motions for a complex underactuated robot directly in a low-dimensional task-space, and to resolve redundancy with lower-priority tasks. We illustrate the potential of this approach with an extremely simple motion planning system which solves the swing-up problem for multi-link underactuated pendula, and discuss extensions to the control of walking.",https://ieeexplore.ieee.org/document/4651150/,2008 IEEE/RSJ International Conference on Intelligent Robots and Systems,22-26 Sept. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.2000.889888,History checking of temporal fuzzy logic formulas for monitoring behavior-based mobile robots,IEEE,Conferences,"Behavior-based robot control systems have shown remarkable success for controlling robots evolving in real world environments. However, they can fail in different manners due to their distributed control and their local decision making. In this case, monitoring can be used to detect failures and help to recover from them. In this work, we present an approach for specifying monitoring knowledge and a method for using this knowledge to detect failures. In particular we show how temporal fuzzy logic can be used to represent monitoring knowledge and then utilized to effectively detect runtime failures. New semantics are introduced to take into consideration uncertainty and noisy information. There are numbers of advantages to our approach including a declarative semantics for the monitoring knowledge and an independence of this knowledge from the implementation details of the control system. Moreover we show how our system can deal effectively with noisy information and sensor readings. Experiments with two real world robots and the simulator are used to illustrate failure examples and the benefits of failure detection and noise elimination.",https://ieeexplore.ieee.org/document/889888/,Proceedings 12th IEEE Internationals Conference on Tools with Artificial Intelligence. ICTAI 2000,15-15 Nov. 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACSOS49614.2020.00036,How far should I watch? Quantifying the effect of various observational capabilities on long-range situational awareness in multi-robot teams,IEEE,Conferences,"In our previous work, we showed that individual robots within a multi-robot team can gain long-distance situational awareness from passive observations of a single nearby neighbor without any explicit robot-to-robot communication. However, that prior work was developed only in simulation, and performance was not measured for real robot teams in physical space with realistic hardware limitations. Toward this end, we studied the performance of these methods in real robot scenarios with methods using more sophisticated techniques in machine learning to mitigate practical implementation problems. In this study, we further extend that work by characterizing the effects of changing history length and sensor range. Rather than finding that increasing history length and sensor range always yield better estimation performance, we find that the optimal history length and sensor range varies depending on the distance between the estimating robot and the robot being estimated. For estimation problems where the estimation target is nearby, longer histories actually degrade performance, and so sensor ranges could be increased instead. Conversely, for farther targets, history length is as valuable or more valuable than sensor range. Thus, just as optimal shutter speed varies with light availability and speed of the subject, passive situational awareness in multi-robot teams is best achieved with different strategies depending on proximity to locations of interest. All studies use the teams of Thymio II physical, two-wheeled robots in laboratory environments 1.1Data and models used are available at https://github.com/PavlicLab/ACSOS2020_ReTLo_Extension.git.",https://ieeexplore.ieee.org/document/9196255/,2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS),17-21 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWSSIP.2015.7314233,How humans can help computers to solve an artificial problem?,IEEE,Conferences,"The idea of using CAPTCHA (Completely Automated Public Turing Test to Tell Computers and Humans Apart) was to protect websites from attacks initiated by the automated computer scripts or computer robots (bots). One of the most important issues about CAPTCHAs is that the test has to be designed in a way that makes it too hard or almost impossible for the computer programs to break the test however, at the same time it should be fairly easy for human users to solve. ReCAPTCHA is known as one of the most popular CAPTCHA models which is being used by the majority of well-known websites such as Yahoo!, Google, Facebook and etc. ReCAPTCHA is being used in order to help digitizing old text books and notes. In this paper we investigate the algorithm behind reCAPTCHA more in depth and find out how basically a simple script-based computer program can get use of real human users in order to solve an artificial problem for the machines. We also review some of the most important security accepts of the reCAPTCHA model.",https://ieeexplore.ieee.org/document/7314233/,"2015 International Conference on Systems, Signals and Image Processing (IWSSIP)",10-12 Sept. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6630610,Human-friendly robot navigation in dynamic environments,IEEE,Conferences,"The vision-based mechanisms that pedestrians in social groups use to navigate in dynamic environments, avoiding obstacles and each others, have been subject to a large amount of research in social anthropology and biological sciences. We build on recent results in these fields to develop a novel fully-distributed algorithm for robot local navigation, which implements the same heuristics for mutual avoidance adopted by humans. The resulting trajectories are human-friendly, because they can intuitively be predicted and interpreted by humans, making the algorithm suitable for the use on robots sharing navigation spaces with humans. The algorithm is computationally light and simple to implement. We study its efficiency and safety in presence of sensing uncertainty, and demonstrate its implementation on real robots. Through extensive quantitative simulations we explore various parameters of the system and demonstrate its good properties in scenarios of different complexity. When the algorithm is implemented on robot swarms, we could observe emergent collective behaviors similar to those observed in human crowds.",https://ieeexplore.ieee.org/document/6630610/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6631340,Humanoid robot posture-control learning in real-time based on human sensorimotor learning ability,IEEE,Conferences,"In this paper we propose a system capable of teaching humanoid robots new skills in real-time. The system aims to simplify the robot control and to provide a natural and intuitive interaction between the human and the robot. The key element of the system is exploitation of the human sensorimotor learning ability where a human demonstrator learns how to operate a robot in the same fashion as humans adapt to various everyday tasks. Another key aspect of the proposed system is that the robot learns the task simultaneously while the human is operating the robot. This enables the control of the robot to be gradually transferred from the human to the robot during the demonstration. The control is transferred based on the accuracy of the imitated task. We demonstrated our approach using an experiment where a human demonstrator taught a humanoid robot how to maintain the postural stability in the presence of the perturbations. To provide the appropriate feedback information of the robot's postural stability to the human sensorimotor system, we utilized a custom-built haptic interface. To absorb the demonstrated skill by the robot, we used Locally Weighted Projection Regression machine learning method. A novel approach was implemented to gradually transfer the control responsibility from the human to the incrementally built autonomous robot controller.",https://ieeexplore.ieee.org/document/6631340/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2009.4983067,HyperNEAT controlled robots learn how to drive on roads in simulated environment,IEEE,Conferences,"In this paper we describe simulation of autonomous robots controlled by recurrent neural networks, which are evolved through indirect encoding using HyperNEAT algorithm. The robots utilize 180 degree wide sensor array. Thanks to the scalability of the neural network generated by HyperNEAT, the sensor array can have various resolution. This would allow to use camera as an input for neural network controller used in real robot. The robots were simulated using software simulation environment. In the experiments the robots were trained to drive with imaximum average speed. Such fitness forces them to learn how to drive on roads and avoid collisions. Evolved neural networks show excellent scalability. Scaling of the sensory input breaks performance of the robots, which should be gained back with re-training of the robot with a different sensory input resolution.",https://ieeexplore.ieee.org/document/4983067/,2009 IEEE Congress on Evolutionary Computation,18-21 May 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIIoT52608.2021.9454183,Image Classification with Knowledge-Based Systems on the Edge for Real-Time Danger Avoidance in Robots,IEEE,Conferences,"Mobile robots are increasingly common in society and are increasingly being used for complex and high-stakes tasks such as search and rescue. The growing requirements for these robots demonstrate a need for systems which can review and react in real time to environmental hazards, which will allow robots to handle environments that are both dynamic and dangerous. We propose and test a system which allows mobile robots to reclassify environmental objects during operation in conjunction with an edge system. We train an image classification model with 99 percent accuracy and deploy it in conjunction with an edge server and JSON-based ruleset to allow robots to react to and avoid hazards.",https://ieeexplore.ieee.org/document/9454183/,2021 IEEE World AI IoT Congress (AIIoT),10-13 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ELECTR.1991.718282,Imaging And Controls For Mars Robots With Neural Networks,IEEE,Conferences,"Two aspects of the design of space robots is covered implemented by neural networks and by hybrid approach with artificial intelligence. One is a neurocontroller for a real-time autonomous system. An optical control system developed saves the time for the image processing that analyzes an image sensor through the environment and induces a transformation over the sensor array. A prototype of the neurocontroller is able to learn and control by itself. The second aspect deals with the design of a Servo Control System for a Robot with the capability of ""learning in Unanticipated Situations"" incorporated in the system. The robot is assumed to be employed to perform useful tasks in an alien evironment. The model developed is shown to provide the robot with the capability to recover from unanticipated situations that can lead to the disruption of its normal operation, and to learn to avoid such situations in the future. These two aspects will be integrated for a design of a very intelligent autonomous space robot.",https://ieeexplore.ieee.org/document/718282/,"Electro International, 1991",16-18 April 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSP.2018.8524377,Implementation of Robotic Vision to Perform Threaded Assembly,IEEE,Conferences,"In manufacturing of mechanical parts and assemblies, proper thread-engagement between a bolt and a nut is vital for the performance and reliability of the product. Typically, this is a precision work, requiring repetitive manual operations. In this paper, we explain how such assembly operations can be carried out by collaborative robots (co-bots) by monitoring the position and orientation of the nut and bolt using an image-sensor (camera). The focus of our discussion is the assembly-operation of bolting of a nut by the grippers of a co-bot. Slips and misalignment, leading to wrong positioning of the nut and the bolt, are identified by capturing the images of the two components in real time using Microsoft Kinect camera-sensor. 3D Reconstruction of the image captured by the camera-sensor is carried out using the Kinect Fusion application. The reconstructed image is in the form of a polygonal mesh which is further converted to 3D Point Cloud data which is less sensitive to noise. Thereafter, the Point Cloud is segmented by dividing the entire scene into many clusters in order to distinguish the objects of the scene as grippers and nut and bolt. These clusters can be used for the training of the co-bot for the proposed operation. This method of extracting object-boundaries leading to recognition of objects is a vital operation in the field of robotic vision. We provide baseline description of various machine learning techniques that can be applied to realize proper assembly of a nut and a bolt.",https://ieeexplore.ieee.org/document/8524377/,2018 International Conference on Communication and Signal Processing (ICCSP),3-5 April 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1991.174485,Implementation of an active optical range sensor using laser slit for in-door intelligent mobile robot,IEEE,Conferences,"The sensor with real-time environment recognition ability is one of the key technologies for autonomous robots. The authors have designed and implemented a small size optical range sensor for their experimental mobile robot. The sensor consists of a laser slit generator, a CCD image sensor and a processing unit. Using this sensor, the real-time obstacle avoiding function is realized and added to the autonomous navigation aspect of the robot.<>",https://ieeexplore.ieee.org/document/174485/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONIELECOMP.2014.6808580,Implementation of an embedded system on a TS7800 board for robot control,IEEE,Conferences,"Growing Functional Modules (GFM) learning based controllers need to be experimented on real robots. In 2009, looking to develop a flexible and generic embedded interface for such robots, we decided to use a TS-7800 single board computer (SBC) with a Debian Linux operating system. Despite the many advantages of this board, implementing the embedded system has been a complex task. This paper describes the implementation of protocols through the TS-7800 different ports (RS232, TCP/IP, USB, analog and digital pins) as well as the connection of external boards (TS-ADC24, TS-DIO64, SSC-32 and LCD display). This implementation was required to connect a large range of actuators, sensors and other peripherals. Furthermore, the architecture of the embedded system is exposed in detail, including topics such as the XML configuration file that specifies the peripherals connected to the SBC, the concept of virtual sensors, the implementation of parallelism and the embedded system interface launcher. Technical aspects such as the optimization of video capture and processing are detailed because their execution required specific compilers versions, EABI emulation and extra libraries (openCV libjpg and libpngand libv4l). The final embedded system was implemented in a humanoid robot and connected to the GFM controller in charge of developing its equilibrium subsystem.",https://ieeexplore.ieee.org/document/6808580/,"2014 International Conference on Electronics, Communications and Computers (CONIELECOMP)",26-28 Feb. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2017.8122654,Implementation of human-robot VQA interaction system with dynamic memory networks,IEEE,Conferences,"One of the major functions of intelligent robots such as social or home service robots is to interact with users in natural language. Moving on from simple conversation or retrieval of data stored in computer memory, we present a new Human-Robot Interaction (HRI) system which can understand and reason over environment around the user and provide information about it in a natural language. For its intelligent interaction, we integrated Dynamic Memory Networks (DMN), a deep learning network for Visual Question Answering (VQA). For its hardware, we built a robotic head platform with a tablet PC and a 3 DOF neck. Through an experiment where the user and the robot had question answering interaction in our customized environment and in real time, the feasibility our proposed system was validated, and the effectiveness of deep learning application in real world as well as a new insight on human robot interaction was demonstrated.",https://ieeexplore.ieee.org/document/8122654/,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341029,Improving Unimodal Object Recognition with Multimodal Contrastive Learning,IEEE,Conferences,"Robots perceive their environment using various sensor modalities, e.g., vision, depth, sound or touch. Each modality provides complementary information for perception. However, while it can be assumed that all modalities are available for training, when deploying the robot in real-world scenarios the sensor setup often varies. In order to gain flexibility with respect to the deployed sensor setup we propose a new multimodal approach within the framework of contrastive learning. In particular, we consider the case of learning from RGB-D images while testing with one modality available, i.e., exclusively RGB or depth. We leverage contrastive learning to capture high-level information between different modalities in a compact feature embedding. We extensively evaluate our multimodal contrastive learning method on the Falling Things dataset and learn representations that outperform prior methods for RGB-D object recognition on the NYU-D dataset. Our code and details on the used datasets are available at: https://github.com/meyerjo/MultiModalContrastiveLearning.",https://ieeexplore.ieee.org/document/9341029/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEE50131.2020.9260698,"Indoor and Outdoor Face Recognition for Social Robot, Sanbot Robot as Case Study",IEEE,Conferences,"The interaction between human and robots is of paramount importance in comforting robot and human in the context of social demand. For the purpose of human-robot interaction, the robot should have the ability to perform a variety of actions including face recognition, path planning, etc. In this paper, face recognition has been implemented on the Sanbot robot. Since the Sanbot robot is intended to work in real environment, therefore indoor and outdoor environment is taken into account in proposing the corresponding face recognition algorithm. For each case a robust pre-processing algorithm should be designed and which can circumvent a challenging problem in face recognition, namely, different lighting conditions (light intensity, angle of radiation, etc.). In case of indoor environment, faces in an captured image by the robot HD camera are found using a Haar-cascade algorithm. Afterwards, a histogram equalization is applied to face images in order to standardize them. Then commonly practiced Deep convolutional neural network structures such as Inception and ResNet are used to design a model and trained end-to-end on a customized dataset with strong augmentation. Finally, by using a voting method, proper prediction is carried out on each face. In what concerns the outdoor environment, which has more challenges, upon applying histogram Equalization on the captured image, faces are found using a MultiTask Cascaded Convolutional Neural Network. Then face images are aligned as head orientation are corrected. Finally, cropped face image is fed to Siamese Network in order to extract face features and verifying individuals. From several practical results it has been inferred that the accuracy of the indoor method is nearly 93% without voting and with voting 97%, and the outdoor method is about 95%.",https://ieeexplore.ieee.org/document/9260698/,2020 28th Iranian Conference on Electrical Engineering (ICEE),4-6 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2013.6696336,Inferring categories to accelerate the learning of new classes,IEEE,Conferences,"On-the-fly learning systems are necessary for the deployment of general purpose robots. New training examples for such systems are often supplied by mentor interactions. Due to the cost of acquiring such examples, it is desirable to reduce the number of necessary interactions. Transfer learning has been shown to improve classification results for classes with small numbers of training examples by pooling knowledge from related classes. Standard practice in these works is to assume that the relationship between the transfer target and related classes is already known. In this work, we explore how previously learned categories, or related groupings of classes, can be used to transfer knowledge to novel classes without explicitly known relationships to them. We demonstrate an algorithm for determining the category membership of a novel class, focusing on the difficult case when few training examples are available. We show that classifiers trained via this method outperform classifiers optimized to learn the novel class individually when evaluated on both synthetic and real-world datasets.",https://ieeexplore.ieee.org/document/6696336/,2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,3-7 Nov. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM41043.2020.9155528,Informative Path Planning for Mobile Sensing with Reinforcement Learning,IEEE,Conferences,"Large-scale spatial data such as air quality, thermal conditions and location signatures play a vital role in a variety of applications. Collecting such data manually can be tedious and labour intensive. With the advancement of robotic technologies, it is feasible to automate such tasks using mobile robots with sensing and navigation capabilities. However, due to limited battery lifetime and scarcity of charging stations, it is important to plan paths for the robots that maximize the utility of data collection, also known as the informative path planning (IPP) problem. In this paper, we propose a novel IPP algorithm using reinforcement learning (RL). A constrained exploration and exploitation strategy is designed to address the unique challenges of IPP, and is shown to have fast convergence and better optimality than a classical reinforcement learning approach. Extensive experiments using real-world measurement data demonstrate that the proposed algorithm outperforms state-of-the-art algorithms in most test cases. Interestingly, unlike existing solutions that have to be re-executed when any input parameter changes, our RL-based solution allows a degree of transferability across different problem instances.",https://ieeexplore.ieee.org/document/9155528/,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,6-9 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2001.934451,Initial evolvability experiments on the CAM-brain machines (CBMs),IEEE,Conferences,"Presents the results of some of the first evolvability experiments undertaken on CAM (content-addressable memory) brain machines (CBMs), using the hardware itself (not software simulations). A CBM is a specialised piece of programmable (evolvable) hardware that uses Xilinx XC6264 programmable FPGA chips to grow and evolve, at electronic speeds, 3D cellular automata (CA) based neural network circuit modules of some 1,000 neurons each. A complete run of a genetic algorithm (e.g. with 100 generations and a population size of 100) is executed in a few seconds. 64,000 of these modules can be evolved separately according to the fitness definitions of human evolutionary engineers and downloaded, one by one, into a gigabyte of RAM. Human brain architects then interconnect these modules ""by hand"" according to their artificial brain architectures. The CBM then updates the binary neural signaling of the artificial brain (with 64,000 ""hand"" interconnected modules, i.e. 75 million neurons) at a rate of 130 billion CA cell updates a second, which is fast enough for the real-time control of robots. Before such multi-module artificial brains can be constructed, it is essential that the quality of the evolution (the ""evolvability"") of the individual modules should be adequate. This paper reports on the initial evolvability results obtained on CBM hardware.",https://ieeexplore.ieee.org/document/934451/,Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546),27-30 May 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPRECON52453.2021.9641000,Intelligent Fault Diagnosis Mechanism for Industrial Robot Actuators using Digital Twin Technology,IEEE,Conferences,"Intelligent fault detection is a mechanism’s competency to distinguish between healthy and faulty machine signals for smart and efficient diagnosis. The modelling and analysis of the parameters that contribute to the system’s fundamental operation form the crux of the framework. A heuristic technology to enable real-time intelligent fault detection is digital twin technology. Digital twin technology allows a tandem establishment between real-world machines and the virtual domain, allowing for the inclusion of optimization and maintenance frameworks. Sparsely represented machines in the digital twin domain are linear actuators, which form essential parts of various industrial and commercial machines. Therefore, this study has modelled a data-driven and multi-physics robotic linear actuator digital twin, and integrated it with a custom designed fault detection mechanism using Naïve Bayes classifier. This architecture can autonomously be deployed in tandem to the physical machine to alarm and diagnose electrical faults as soon as they occur in the machine. As compared with conventional diagnostics this will reduce machine down-time and expedite repairs. The resultant model built on MATLAB, Simulink gave an accuracy of 96% and required minimal processing capability to operate. Widespread commercial utilization of the proposed model can pave the path for Industry 4.0 utilization of linear actuators as well as technologies including industrial robots that utilize them.",https://ieeexplore.ieee.org/document/9641000/,2021 IEEE International Power and Renewable Energy Conference (IPRECON),24-26 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2010.5472498,Intelligent control and evolutionary strategies applied to multirobotic systems,IEEE,Conferences,"This paper describes the modeling, implementation, and evaluation of RoBombeiros multirobotic system. The robotic task in this paper is performed over a natural disaster, simulated as a forest fire. The simulator supports several features to allow realistic simulation, like irregular terrains, natural processes (e.g. fire, wind) and physical constraint in the creation and application of mobile robots. The proposed system relies on two steps: (i) group formation planning and (ii) intelligent techniques to perform robots navigation for fire fighting. For planning, we used genetic algorithms to evolve positioning strategies for firefighting robots performance. For robots operation, physically simulated fire-fighting robots were built, and the sensory information of each robot (e.g. GPS, compass, sonar) was used in the input of an artificial neural network (ANN). The ANN controls the vehicle (robot) actuators and allows navigation with obstacle avoidance. Simulation results show that the ANN satisfactorily controls the mobile robots; the genetic algorithm adequately configures the fire fighting strategy and the proposed multi-robotic system can have an essential hole in the planning and execution of fire fighting in real forests.",https://ieeexplore.ieee.org/document/5472498/,2010 IEEE International Conference on Industrial Technology,14-17 March 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCYB.2004.1437660,Intelligent control application on sample identification,IEEE,Conferences,"An intelligent control implementation is proposed for sample differentiation with Raman spectroscopy, which can be used to characterize various samples for decision-making and medical diagnosis. Raman spectra are weak signals whose features are inevitably affected by numerous noises during the calibration process. These noises must be eliminated to an acceptable level. Fuzzy control has been widely used to solve uncertainty, imprecision and vague phenomena, so fuzzy logic can be used for noise filtering. The resulting intrinsic Raman spectrum has been trained using artificial neural networks. Both unsupervised learning and supervised learning are to be conducted in this preliminary research on sample identification. For unsupervised training, principal component analysis (PCA) is exploited, which is based on Hebbian rule and single value decomposition (SVD) approach, respectively. For supervised training, radial basis function (RBF) is presented. A complete procedure for sample identification consists of Raman spectra calibration, noise filtering, unsupervised classification and supervised neural network training. A systematic intelligent control approach is formulated in consequence for sample identification. The long-term objective is to create a real-time approach for sample analysis using a Raman spectrometer directly mounted at the end-effector of the medical robots to enhance robotic remote surgery",https://ieeexplore.ieee.org/document/1437660/,"Second IEEE International Conference on Computational Cybernetics, 2004. ICCC 2004.",30 Aug.-1 Sept. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2012.6651596,Interactive symbol generation of task planning for daily assistive robot,IEEE,Conferences,"For the development of both hardware and software, task plannings become more and more important for robots to perform various tasks. Applying task planning to robotic system for working in real environments has difficulties. Estimating required symbols before planning is difficult because real environments are partially observable. In this paper, we proposed a method for task planning in partially observable environments with unknown objects. To construct conditional plans used in these environments, we extend the description of actions to multi-effect actions, and to deal with unknown objects, robots get new symbols generated by human interaction on demand. Additionally we show experiments of Willow Garage's PR2 executing the task in the real environment with unknown objects.",https://ieeexplore.ieee.org/document/6651596/,2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2012),29 Nov.-1 Dec. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WPNC.2016.7822857,Introducing a novel marker-based geometry model in monocular vision,IEEE,Conferences,"A spherical marker-based distance capture concept using monocular vision is presented in this paper. A novel method is explored, within the concept of a virtual sphere, which shows how to improve the reading measurements of the distance of a moving object from low resolution digital images, and from a single viewpoint. The aim here is to be able to track accurately the object at a furthest possible position. A conclusion with experimental simulations carried out using 3D modeling of markers and representing the real world showing the potency of the marker's geometry on improving the accuracy of the measurements. A potential application field of the proposed method is the implementation of tracking object in mobile robots, marker-based localization, and field of topography.",https://ieeexplore.ieee.org/document/7822857/,"2016 13th Workshop on Positioning, Navigation and Communications (WPNC)",19-20 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETIETR.2018.8529028,IoT Enabled Robots with QR Code Based Localization,IEEE,Conferences,"Robots are sophisticated form of IoT devices as they are smart devices that scrutinize sensor data from multiple sources and observe events to decide the best procedural actions to supervise and manoeuvre objects in the physical world. In this paper, localization of the robot is addressed by QR code Detection and path optimization is accomplished by Dijkstras algorithm. The robot can navigate automatically in its environment with sensors and shortest path is computed whenever heading measurements are updated with QR code landmark recognition. The proposed approach highly reduces computational burden and deployment complexity as it reflects the use of artificial intelligence to self-correct its course when required. An Encrypted communication channel is established over wireless local area network using SSHv2 protocol to transfer or receive sensor data(or commands) making it an IoT enabled Robot.",https://ieeexplore.ieee.org/document/8529028/,2018 International Conference on Emerging Trends and Innovations In Engineering And Technological Research (ICETIETR),11-13 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2018.8665255,Knowledge-Driven Deep Deterministic Policy Gradient for Robotic Multiple Peg-in-Hole Assembly Tasks,IEEE,Conferences,"It remains a formidable challenge for traditional control strategies to perform automatic multiple peg-in-hole assembly tasks due to the complicated and dynamic contact states. Inspired by that human could generalize the learned skills to perform the different assembly tasks well, a general learning-based algorithm based on deep deterministic policy gradient (DDPG) is proposed. To make robots learn the multiple peg-in-hole assembly skills from experience efficiently and stably, the learning process is driven by the basic knowledge like PD force control strategy. To achieve a fast learning process in the real-world assembly tasks, a hybrid exploration strategy is applied to drive a efficient exploration during policy search phase. A dual peg-in-hole assembly simulation and real-world experiments are implemented to verify the effectiveness of the proposed algorithm. The performance measured by the assembly time and the maximum contact forces demonstrates that the multiple peg-in-hole assembly skills could be improved only after 150 training episodes in dual peg-in-hole assembly task.",https://ieeexplore.ieee.org/document/8665255/,2018 IEEE International Conference on Robotics and Biomimetics (ROBIO),12-15 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2015.7281881,Knowledge-driven finite-state machines. Study case in monitoring industrial equipment,IEEE,Conferences,Traditionally state machines are implemented by coding the desired behavior of a given system. This work proposes the use of ontological models to describe and perform computations on state machines by using SPARQL queries. This approach represents a paradigm shift relating to the customary manner in which state machines are stored and computed. The main contribution of the work is an ontological model to represent state machines and a set of generic queries that can be used in any knowledge-driven state machine to compute valuable information. The approach was tested in a study case were the state machines of industrial robots in a manufacturing line were modeled as ontological models and used for monitoring the behavior of these devices on real time.,https://ieeexplore.ieee.org/document/7281881/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636458,Laser-Based Side-by-Side Following for Human-Following Robots,IEEE,Conferences,"A mobile robot that follows behind humans in structured environments has to face the challenge of full occlusion caused by the walls when the target person makes a turn at the corridor intersections. This may result in short-term, even a permanent loss of the target from the field of view of the Human-Following Robots (HFRs). Concerning this issue, a novel side-by-side following method for HFRs is addressed. In this paper, HFRs detect the legs of target person and different types of corridor intersections using the onboard laser scanner at first. Then, we provide a corridor detector method to cluster the geometric structure constraint between the target and corridor intersections. At last, a Side-by-side Following Leg Tracker (SFLT) is designed by integrating the laser information, in order to increase the visible time of the target person, while the target is turning at the corridor intersections. The corridor detector method and SFLT method have been simulated in MATLAB. Moreover, the approach of side-by-side following has been implemented in the Robot Operating System (ROS) of real-life robots in the corridor environment. The results from simulation and practical experiment show that, by using our method, HFRs were able to successfully follow the human92.0% while a mobile robot meeting potential occlusions at corridor intersections.",https://ieeexplore.ieee.org/document/9636458/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCIS.2009.206,Layered Task Allocation in Multi-robot Systems,IEEE,Conferences,"A layered task allocation method is presented for multi-robot systems in a collaboration and adversarial, dynamic, real-time environment with unreliable communication in this paper. The process of task allocation is divided into three layers: task decomposition layer, task evaluation layer and task selection layer. In task decomposition layer, robots categorize their environments into corresponding modes, and fix subtasks in every mode as experts do, in order to reduce candidate tasks and decrease the complexity of task allocation. Q-Learning based on Adaptive Neuro Fuzzy Inference System (ANFIS) is adopted to compute utilities of candidate tasks in task evaluation layer. This can not only avoid the complicated opponent modeling but also make the learning more efficient. In task selection layer, task with the maximum utility is selected in application, but in learning, task is selected according to randomized Boltzmann exploration tactics in order to get more information for optimization. Simulation experiments implemented on simulated robotic soccer show that this approach improves performances of multi-robot systems greatly.",https://ieeexplore.ieee.org/document/5209028/,2009 WRI Global Congress on Intelligent Systems,19-21 May 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2019.8914406,Learning Locomotion Skills via Model-based Proximal Meta-Reinforcement Learning,IEEE,Conferences,"Model-based reinforcement learning methods provide a promising direction for a range of automated applications, such as autonomous vehicles and legged robots, due to their sample-efficiency. However, their asymptotic performance is usually inferior compared to the state-of-the-art model-free reinforcement learning methods in locomotion control domains. One main challenge of model-based reinforcement learning is learning a dynamics model that is accurate enough for planning. This paper mitigates this issue by meta-reinforcement learning from an ensemble of dynamics models. A policy learns from dynamics models that hold different beliefs of a real environment. This procedure improves its adaptability and inaccuracy-tolerance ability. A proximal meta-reinforcement learning algorithm is introduced to improve computational efficiency and reduces variance of higher-order gradient estimation. A heteroscedastic noise is added to the training dataset, thus leading to a robust and efficient model learning. Subsequently, proximal meta-reinforcement learning maximizes the expected returns by sampling “imaginary” trajectories from the learned dynamics, which does not require real environment data and can be deployed on many servers in parallel to speed up the whole learning process. The aim of this work is to reduce the sample-complexity and computational cost of reinforcement learning in robot locomotion tasks. Simulation experiments show that the proposed algorithm achieves an asymptotic performance compared with the state-of-the-art model-free reinforcement learning methods with significantly fewer samples, which confirm our theoretical results.",https://ieeexplore.ieee.org/document/8914406/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341458,Learning Motion Parameterizations of Mobile Pick and Place Actions from Observing Humans in Virtual Environments,IEEE,Conferences,"In this paper, we present an approach and an implemented pipeline for transferring data acquired from observing humans in virtual environments onto robots acting in the real world, and adapting the data accordingly to achieve successful task execution. We demonstrate our pipeline by inferring seven different symbolic and subsymbolic motion parameters of mobile pick and place actions, which allows the robot to set a simple breakfast table. We propose an approach to learn general motion parameter models and discuss, which parameters can be learned at which abstraction level.",https://ieeexplore.ieee.org/document/9341458/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR52253.2021.9494666,Learning Multi-modal Attentional Consensus in Action Recognition for Elderly-Care Robots,IEEE,Conferences,"This paper addresses a practical action recognition method for elderly-care robots. Multi-stream based models are one of the promising approaches for solving the complexity of real-world environments. While multi-modal action recognition have been actively studied, there is a lack of research on models that effectively combine features of different modalities. This paper proposes a new mid-level feature fusion method for two-stream based action recognition network. In multi-modal approaches, extracting complementary information between different modalities is an essential task. Our network model is designed to fuse features at an intermediate level of feature extraction, which leverages a whole feature map from each modality. Consensus feature map and consensus attention mechanism are proposed as effective ways to extract information from two different modalities: RGB data and motion features. We also introduce ETRI-Activity3D-LivingLab, a real-world RGB-D dataset for robots to recognize daily activities of the elderly. It is the first 3D action recognition dataset obtained in a variety of home environments where the elderly actually reside. We expect our new dataset to contribute to the practical study of action recognition with the previously released ETRI-Activity3D dataset. To prove the effectiveness of the method, extensive experiments are performed on NTU RGB+D, ETRI-Activity3D and, ETRI-Activity3D-LivingLab dataset. Our mid-level fusion method achieves competitive performance in various experimental settings, especially for domain-changing situations.",https://ieeexplore.ieee.org/document/9494666/,2021 18th International Conference on Ubiquitous Robots (UR),12-14 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.1993.339280,Learning behavioral control by reinforcement for an autonomous mobile robot,IEEE,Conferences,"We present an implementation of a reinforcement learning algorithm through the use of a special neural network topology, the AHC (adaptive heuristic critic). The AHC constitutes a fusion supervisor of primitive behaviours in order to execute more complex robot behaviours as for example go to goal. This fusion supervisor is part of an architecture for the execution of mobile robot tasks which are composed of several primitive behaviours which act in a simultaneous or concurrent fashion. The architecture allows for learning to take place at the execution level, it incorporates the experience gained in executing primitive behaviours as well as the overall task. The implementation of the autonomous learning approach has been tested within OPMOR, a simulation environment for mobile robots and with our mobile platform UPM Robuter. Both simulated and real results are presented. The performance of the AHC neural network is adequate. Portions of this work have been implemented in the EEC ESPRIT 2483 PANORAMA Project.<>",https://ieeexplore.ieee.org/document/339280/,Proceedings of IECON '93 - 19th Annual Conference of IEEE Industrial Electronics,15-19 Nov. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2010.5598659,Learning grasp stability based on tactile data and HMMs,IEEE,Conferences,"In this paper, the problem of learning grasp stability in robotic object grasping based on tactile measurements is studied. Although grasp stability modeling and estimation has been studied for a long time, there are few robots today able of demonstrating extensive grasping skills. The main contribution of the work presented here is an investigation of probabilistic modeling for inferring grasp stability based on learning from examples. The main objective is classification of a grasp as stable or unstable before applying further actions on it, e.g. lifting. The problem cannot be solved by visual sensing which is typically used to execute an initial robot hand positioning with respect to the object. The output of the classification system can trigger a regrasping step if an unstable grasp is identified. An off-line learning process is implemented and used for reasoning about grasp stability for a three-fingered robotic hand using Hidden Markov models. To evaluate the proposed method, experiments are performed both in simulation and on a real robot system.",https://ieeexplore.ieee.org/document/5598659/,19th International Symposium in Robot and Human Interactive Communication,13-15 Sept. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2001.932897,Learning momentum: integration and experimentation,IEEE,Conferences,"We further study the effects of learning momentum as defined by Arkin, Clark and Ram (1992) on robots, both simulated and real, attempting to traverse obstacle fields in order to reach a goal. Integration of these results into a large-scale software architecture, MissionLab, provides the ability to exercise these algorithms in novel ways. Insight is also sought in reference to when different learning momentum strategies should be used.",https://ieeexplore.ieee.org/document/932897/,Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),21-26 May 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2014.6943031,Learning robot tactile sensing for object manipulation,IEEE,Conferences,"Tactile sensing is a fundamental component of object manipulation and tool handling skills. With robots entering unstructured environments, tactile feedback also becomes an important ability for robot manipulation. In this work, we explore how a robot can learn to use tactile sensing in object manipulation tasks. We first address the problem of in-hand object localization and adapt three pose estimation algorithms from computer vision. Second, we employ dynamic motor primitives to learn robot movements from human demonstrations and record desired tactile signal trajectories. Then, we add tactile feedback to the control loop and apply relative entropy policy search to learn the parameters of the tactile coupling. Additionally, we show how the learning of tactile feedback can be performed more efficiently by reducing the dimensionality of the tactile information through spectral clustering and principal component analysis. Our approach is implemented on a real robot, which learns to perform a scraping task with a spatula in an altered environment.",https://ieeexplore.ieee.org/document/6943031/,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,14-18 Sept. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCVW.2019.00309,Learning to Navigate Robotic Wheelchairs from Demonstration: Is Training in Simulation Viable?,IEEE,Conferences,"Learning from demonstration (LfD) enables robots to learn complex relationships between their state, perception and actions that are hard to express in an optimization framework. While people intuitively know what they would like to do in a given situation, they often have difficulty representing their decision process precisely enough to enable an implementation. Here, we are interested in robots that carry passengers, such as robotic wheelchairs, where user preferences, comfort and the feeling of safety are important for autonomous navigation. Balancing these requirements is not straightforward. While robots can be trained in an LfD framework in which users drive the robot according to their preferences, performing these demonstrations can be time-consuming, expensive, and possibly dangerous. Inspired by recent efforts for generating synthetic data for training autonomous driving systems, we investigate whether it is possible to train a robot based on simulations to reduce the time requirements, cost and potential risk. A key characteristic of our approach is that the input is not images, but the locations of people and obstacles relative to the robot. We argue that this allows us to transfer the classifier from the simulator to the physical world and to previously unseen environments that do not match the appearance of the training set. Experiments with 14 subjects providing physical and simulated demonstrations validate our claim.",https://ieeexplore.ieee.org/document/9022271/,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),27-28 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811662,Learning to Socially Navigate in Pedestrian-rich Environments with Interaction Capacity,IEEE,Conferences,"Existing navigation policies for autonomous robots tend to focus on collision avoidance while ignoring human-robot interactions in social life. For instance, robots can pass along the corridor safer and easier if pedestrians notice them. Sounds have been considered as an efficient way to attract the attention of pedestrians, which can alleviate the freezing robot problem. In this work, we present a new deep reinforcement learning (DRL) based social navigation approach for autonomous robots to move in pedestrian-rich environments with interaction capacity. Most existing DRL based methods intend to train a general policy that outputs both navigation actions, i.e., expected robot&#x0027;s linear and angular velocities, and interaction actions, i.e., the beep action, in the context of reinforcement learning. Different from these methods, we intend to train the policy via both supervised learning and reinforcement learning. In specific, we first train an interaction policy in the context of supervised learning, which provides a better understanding of the social situation, then we use this interaction policy to train the navigation policy via multiple reinforcement learning algorithms. We evaluate our approach in various simulation environments and compare it to other methods. The experimental results show that our approach outperforms others in terms of the success rate. We also deploy the trained policy on a real-world robot, which shows a nice performance in crowded environments.",https://ieeexplore.ieee.org/document/9811662/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEC.1995.487489,Learning to achieve co-operation by temporal-spatial fitness sharing,IEEE,Conferences,"We propose a co-operative GA-based learning system that would make real-world heterogeneous agents feasible with the minimum amount of communication hardware. The problem is identical to a distributed GA implemented on processors connected by local and very slow communication lines. We have developed an extension of the fitness sharing method that incorporates sharing over temporally-spatially distributed populations. Restricting an agent's task to the inter-agent conflict avoidance, this sharing is realised by exchanging estimated fitness values over all agents. The mechanism of finding conflict avoidance actions is similar to that of a self-organisation mechanism of a Kohonen-type network. Our results from simulations of a bump-avoidance task for multiple mobile robots show that it elicits a notable performance improvement compared to normal classifier systems.",https://ieeexplore.ieee.org/document/487489/,Proceedings of 1995 IEEE International Conference on Evolutionary Computation,29 Nov.-1 Dec. 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1993.390770,Learning to coordinate behaviors for real-time path planning of autonomous systems,IEEE,Conferences,"We present a neural network (NN) system which learns the appropriate simultaneous activation of primitive behaviors in order to execute more complex robot behaviors. The NN implementation is part of an architecture for the execution of mobile robot tasks which are composed of several primitive behaviors in a simultaneous or concurrent fashion. We use a supervised learning technique with a human trainer generating appropriate training for the simultaneous activation of behavior in a simulated environment. The NN implementation has been tested within OPMOR, a simulation environment for mobile robots and several results are presented. The performance of the neural network is adequate. Portions of this work has been implemented in the EEC ESPRIT 2483 PANORAMA Project.<>",https://ieeexplore.ieee.org/document/390770/,Proceedings of IEEE Systems Man and Cybernetics Conference - SMC,17-20 Oct. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2019.8914455,Learning waste Recycling by playing with a Social Robot,IEEE,Conferences,"In this paper we investigate the use of a social robot as an interface to a serious game aiming to train kids in how to recycle materials correctly. Serious games are mostly used to induce motivations and engagement in users and support knowledge transfer during playing. They are especially effective when the goal of the game concerns behavior change. In addition, social robots have been used effectively in educational settings to engage children in the learning process. Following this trend, we designed a serious game in which the social robot Pepper plays with a child to teach him to correctly recycle the materials. To endow the robot with the capability of detecting and classifying the waste material we developed an image recognition module based on a Convolutional Neural Network. Preliminary experimental results show that the implementation of a serious game about recycling into the Pepper robot improves its social behavior. The use of real objects as waste items during the game turns out to be a successful approach not only for perceived learning effectiveness but also for engagement of the children.",https://ieeexplore.ieee.org/document/8914455/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC45102.2020.9294425,Learning-to-Fly: Learning-based Collision Avoidance for Scalable Urban Air Mobility,IEEE,Conferences,"With increasing urban population, there is global interest in Urban Air Mobility (UAM), where hundreds of autonomous Unmanned Aircraft Systems (UAS) execute missions in the airspace above cities. Unlike traditional human-inthe-loop air traffic management, UAM requires decentralized autonomous approaches that scale for an order of magnitude higher aircraft densities and are applicable to urban settings. We present Learning-to-Fly (L2F), a decentralized on-demand airborne collision avoidance framework for multiple UAS that allows them to independently plan and safely execute missions with spatial, temporal and reactive objectives expressed using Signal Temporal Logic. We formulate the problem of predictively avoiding collisions between two UAS without violating mission objectives as a Mixed Integer Linear Program (MILP). This however is intractable to solve online. Instead, we develop L2F, a two-stage collision avoidance method that consists of: 1) a learning-based decision-making scheme and 2) a distributed, linear programming-based UAS control algorithm. Through extensive simulations, we show the real-time applicability of our method which is ≈6000× faster than the MILP approach and can resolve 100% of collisions when there is ample room to maneuver, and shows graceful degradation in performance otherwise. We also compare L2F to two other methods and demonstrate an implementation on quad-rotor robots.",https://ieeexplore.ieee.org/document/9294425/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IIAI-AAI50415.2020.00021,Library Intelligent Book Recommendation System Using Facial Expression Recognition,IEEE,Conferences,"To solve the problem of information overload, the recommendation system has developed in various fields. Faced with a variety of forms and rich masses of books, the book recommendation system based on various recommendation methods is applied in the library. In the traditional book recommendation system, there are some problems, such as single recommendation mode, lack of pertinence, too centralized recommended books and so on. In order to solve these problems, this paper proposes a personalized book recommendation system. Through user expression recognition to obtain user preferences, according to user preferences to recommend books to user. Facial expression recognition is realized by using convolution neural network model. This kind of recommendation method has real-time and authenticity. The book recommendation system based on facial expression recognition can improve users' sense of use when applied to library robots.",https://ieeexplore.ieee.org/document/9430442/,2020 9th International Congress on Advanced Applied Informatics (IIAI-AAI),1-15 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HRI.2019.8673212,Lifespan Design of Conversational Agent with Growth and Regression Metaphor for the Natural Supervision on Robot Intelligence,IEEE,Conferences,"Human's direct supervision on robot's erroneous behavior is crucial to enhance a robot intelligence for a `flawless' human-robot interaction. Motivating humans to engage more actively for this purpose is however difficult. To alleviate such strain, this research proposes a novel approach, a growth and regression metaphoric interaction design inspired from human's communicative, intellectual, social competence aspect of developmental stages. We implemented the interaction design principle unto a conversational agent combined with a set of synthetic sensors. Within this context, we aim to show that the agent successfully encourages the online labeling activity in response to the faulty behavior of robots as a supervision process. The field study is going to be conducted to evaluate the efficacy of our proposal by measuring the annotation performance of real-time activity events in the wild. We expect to provide a more effective and practical means to supervise robot by real-time data labeling process for long-term usage in the human-robot interaction.",https://ieeexplore.ieee.org/document/8673212/,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HSI49210.2020.9142636,Lightweight Convolutional Neural Network for Real-Time Face Detector on CPU Supporting Interaction of Service Robot,IEEE,Conferences,"Face detection plays an essential role in the success of the interaction between service robots and consumers. This method is the initial stage for face-related applications. Practical applications require face detection to work in real-time and can be implemented on low-cost devices such as CPU. Traditional methods have problems when the face is not frontal, blocked, and partially covered, but real-time speed is not an obstacle. On the other hand, deep learning has succeeded in accurately distinguishing facial features and backgrounds. Face sizes that tend to be medium and large when robot interaction with consumers so it can employ Convolutional Neural Networks (CNN) with light weights. In this paper, a real-time face detector is built that can work on the CPU. This detector will be implemented explicitly in service robots to support interactions with consumers. It can overcome the occlusion and not-frontal face. Detector architecture consists of the backbone as rapidly features extractor, transition module as a transformer of prediction map, and the dual-detection layer is head of a network prediction based on scale assignment. As a result, the detector can work at speeds of 301 frames per second on CPU without ignoring the accuracy.",https://ieeexplore.ieee.org/document/9142636/,2020 13th International Conference on Human System Interaction (HSI),6-8 June 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCO53203.2021.9663861,MAS agents development for mining industry,IEEE,Conferences,"The essence of multi-agent technology is a fundamentally new method of solving problems. In contrast to the classical method, when a search is carried out a well-defined (deterministic) algorithm, which allows find the best solution to the problem in multi-technology solution is obtained automatically as a result of the interaction of many self-parking enforcement targeted software modules - the so-called software agents ants. Often classical methods for solving problems are not applicable in real life. There are various fields where MAS could be implemented, for the research of this paper the mining industry where taken.This paper explains MAS, its classification, shows the possibility of use of agent modeling in real industry. The article describes the steps of the development of agents, and the testing of them on a build-up layout of quarry and on working prototypes of the dumper and excavator robots.",https://ieeexplore.ieee.org/document/9663861/,2021 16th International Conference on Electronics Computer and Computation (ICECCO),25-26 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM.2018.8485910,MV-Sports: A Motion and Vision Sensor Integration-Based Sports Analysis System,IEEE,Conferences,"Recently, intelligent sports analytics is becoming a hot area in both industry and academia for coaching, practicing tactic and technical analysis. With the growing trend of bringing sports analytics to live broadcasting, sports robots and common playfield, a low cost system that is easy to deploy and performs real-time and accurate sports analytics is very desirable. However, existing systems, such as Hawk-Eye, cannot satisfy these requirements due to various factors. In this paper, we present MV-Sports, a cost-effective system for real-time sports analysis based on motion and vision sensor integration. Taking tennis as a case study, we aim to recognize player shot types and measure ball states. For fine-grained player action recognition, we leverage motion signal for fast action highlighting and propose a long short term memory (LSTM)-based framework to integrate MV data for training and classification. For ball state measurement, we compute the initial ball state via motion sensing and devise an extended kalman filter (EKF)-based approach to combine ball motion physics-based tracking and vision positioning-based tracking to get more accurate ball state. We implement MV-Sports on commercial off-the-shelf (COTS) devices and conduct real-world experiments to evaluate the performance of our system. The results show our approach can achieve accurate player action recognition and ball state measurement with sub-second latency.",https://ieeexplore.ieee.org/document/8485910/,IEEE INFOCOM 2018 - IEEE Conference on Computer Communications,16-19 April 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCC/SmartCity/DSS.2019.00339,Machine Learning Based CloudBot Detection Using Multi-Layer Traffic Statistics,IEEE,Conferences,"With the rapid development of e-commerce services and online transactions, an increasing number of advanced web robots are utilized by speculators and hackers in underground economy to perform click fraud, register fake accounts and commit other kinds of frauds, seriously harming the profit of businesses and the fairness of online activities. There is solid evidence that the vast majority of such malicious bot traffic comes from data centers. The malicious bot deployed on the hosts of data centers is referred to as a CloudBot. How to detect and block CloudBots effectively has become an urgent problem in practice, while the research on it can be seldom seen in public. To this end, we propose a traffic-based quasi-real-time method for CloudBot detection using machine learning, which exploits a new sample partitioning approach, as well as innovative multi-layer features that reveal the essential difference between CloudBots and human traffic. Our method achieves 93.4% precision in the experiment and performs well on the real-world dataset, which proves to be effective to detect unknown CloudBots and combat the concept drift caused by varying time. Besides, the approach is also privacy-preserving without using any specific application layer information. We believe our work can benefit network economy security and fairness in practice.",https://ieeexplore.ieee.org/document/8855536/,2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS),10-12 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00038,Machine Learning Concepts for Dual-Arm Robots within Virtual Reality,IEEE,Conferences,"The collaboration between humans and artificial intelligence (AI) driven robots lay the foundations for new approaches in industrial production. However, intensive research is required to develop machine learning behavior that is not only able to execute shared tasks but also acts following the expectations of the human partner. Rigid setups and restrictive safety measures deny the acquisition of adequate training samples to build general-purpose machine learning solutions for evaluation within experimental studies. Based on established research that trains AI systems within simulated environments, we present a machine learning implementation that enables the training of a dual-arm robot within a virtual reality (VR) application. Building upon preceding research, an activity diagram for a shared task for the machine learning model to learn, was conceptualized. A first approach, using vector distances, led to flawed results, whereas a revised solution based on collision boxes resulted in a stable outcome. While the implementation of the machine learning model is fixed on the activity diagram of the shared task, the presented approach is expandable as a universal platform for evaluating Human-Robot Collaboration (HRC) scenarios in VR. Future iterations of this VR sandbox application can be used to explore optimal workplace arrangements and procedures with autonomous industrial robots in a wide range of possible scenarios.",https://ieeexplore.ieee.org/document/9644376/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData52589.2021.9671792,Machine learning for robot locomotion: Grounded simulation learning and adaptive planner parameter learning,IEEE,Conferences,"Summary form only given. The complete presentation was not made available for publication as part of the conference proceedings. : Robust locomotion is one of the most fundamental requirements for autonomous mobile robots. With the widespread deployment of robots in factories, warehouses, and homes, it is tempting to think that locomotion is a solved problem. However for certain robot morphologies (e.g. humanoids) and environmental conditions (e.g. narrow passages), significant challenges remain. This talk begins by introducing Grounded Simulation Learning as a way to bridge the so-called reality gap between simulators and the real world in order to enable transfer learning from simulation to a real robot (sim-toreal). It then introduces Adaptive Planner Parameter Learning as a way of leveraging human input (learning from demonstration) towards making existing robot motion planners more robust, without losing their safety properties. Grounded Simulation Learning has led to the fastest known stable walk on a widely used humanoid robot, and Adaptive Planner Parameter Learning has led to efficient learning of robust navigation policies in highly constrained spaces.",https://ieeexplore.ieee.org/document/9671792/,2021 IEEE International Conference on Big Data (Big Data),15-18 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793485,Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks,IEEE,Conferences,"Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. We present results in simulation and on a real robot.",https://ieeexplore.ieee.org/document/8793485/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DDCLS52934.2021.9455635,Mask-based Object Pose Estimation with Domain Transfer,IEEE,Conferences,"Object pose estimation is important for robots to understand and interact with the real world. This problem is challenging because the various objects, clutter and occlusions between objects in the scene. Deep learning methods show better performances than traditional problems in this problem but training a convolutional neural network needs lots of annotated data which is expensive to obtain. This paper proposes a general method by using domain transfer technology to efficiently solve object pose estimation problem. Besides, the proposed method obtains mask to achieve high quality performance by combing an instance segmentation framework, Mask R-CNN. We present the results of our experiments with the LineMOD dataset. We also deploy our method to robotic grasp object based on the estimated pose.",https://ieeexplore.ieee.org/document/9455635/,2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS),14-16 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS.2008.23,MecaTeam Framework: An Infrastructure for the Development of Soccer Agents for Simulated Robots,IEEE,Conferences,"This paper presents the MecaTeam framework, a solution to reduce the effort on developing new soccer teams of robots for the 2D simulation category of the RoboCup. MecaTeam is an object-oriented framework based on features of two robot soccer teams: the MecaTeam 2006 and Uva Trilearn. The architecture of the proposed framework is presented and aspects of its use are discussed. Besides facilitating the development of new teams, the use of the MecaTeam framework may decrease the impact of changes in chunks of related code. Finally, the MecaTeam framework can be used by new researchers interested in simulated robots for soccer games.",https://ieeexplore.ieee.org/document/4812639/,2008 IEEE Latin American Robotic Symposium,29-30 Oct. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN47096.2020.9223436,Migratable AI: Effect of identity and information migration on users' perception of conversational AI agents,IEEE,Conferences,"Conversational AI agents are proliferating, embodying a range of devices such as smart speakers, smart displays, robots, cars, and more. We can envision a future where a personal conversational agent could migrate across different form factors and environments to always accompany and assist its user to support a far more continuous, personalized and collaborative experience. This opens the question of what properties of a conversational AI agent migrates across forms, and how it would impact user perception. To explore this, we developed a Migratable AI system where a user's information and/or the agent's identity can be preserved as it migrates across form factors to help its user with a task. We validated the system by designing a 2x2 between-subjects study to explore the effects of information migration and identity migration on user perceptions of trust, competence, likeability and social presence. Our results suggest that identity migration had a positive effect on trust, competence and social presence, while information migration had a positive effect on trust, competence and likeability. Overall, users report highest trust, competence, likeability and social presence towards the conversational agent when both identity and information were migrated across embodiments.",https://ieeexplore.ieee.org/document/9223436/,2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),31 Aug.-4 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1998.681416,Mobile robot exploration and map-building with continuous localization,IEEE,Conferences,"Our research addresses how to integrate exploration and localization for mobile robots. A robot exploring and mapping an unknown environment needs to know its own location, but it may need a map in order to determine that location. In order to solve this problem, we have developed ARIEL, a mobile robot system that combines frontier based exploration with continuous localization. ARIEL explores by navigating to frontiers, regions on the boundary between unexplored space and space that is known to be open. ARIEL finds these regions in the occupancy grid map that it builds as it explores the world. ARIEL localizes by matching its recent perceptions with the information stored in the occupancy grid. We have implemented ARIEL on a real mobile robot and tested ARIEL in a real-world office environment. We present quantitative results that demonstrate that ARIEL can localize accurately while exploring, and thereby build accurate maps of its environment.",https://ieeexplore.ieee.org/document/681416/,Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146),20-20 May 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VLSI-TSA.2018.8403807,Mobile/embedded DNN and AI SoCs,IEEE,Conferences,"Summary form only. Recently, Deep Neural Networks are changing not only the technology paradigm in electronics but also the society itself with Artificial Intelligence technologies. In this presentation, firstly, the status of AI and DNN SoCs will be reviewed from two perspectives; the data-center oriented and the mobile and embedded AIs. This dichotomy shows clearly the possible application areas for the emerging future AIs. Especially, mobile and embedded deep learning hardware will be introduced together with CNN (Convolutional Neural Network) and RNN (Recurrent Neural Network). In addition, real CMOS chip implementation results of mobile/embedded DNNs will be explained with measurement results. Secondly, KAIST's approach integrating both sides of brain, right-brain for ""approximation and adaptation hardware"" and left-brain for""precise and programmable Von Neumann architecture"", will be explained with novel design methodology. The deep neural networks and the specialized intelligent hardware (mimicking right brain) capable of statistical processing or learning and the multi-core processors (mimicking left brain) performing the precise computations including software AI are integrated on the same SoC. Based on this brain-mimicking SoCs, the object recognition and the augmented reality applications are implemented with low-power and high-performance for wearable devices such as smart glasses, autonomous vehicles, and intelligent robots.",https://ieeexplore.ieee.org/document/8403807/,"2018 International Symposium on VLSI Technology, Systems and Application (VLSI-TSA)",16-19 April 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARM49381.2020.9195341,Model-Based Reinforcement Learning For Robot Control,IEEE,Conferences,"Model-free deep reinforcement learning (MFRL) algorithms have achieved many impressive results. But they are generally stricken with high sample complexity, which puts forward a critical challenge for their application to real-world robots. Dynamic models are essential for robot control laws, but it is often hard to obtain accurate analytical dynamic models. Therefore a data-driven approach to learning models becomes significant for reinforcement learning to increase data efficiency. Model-based algorithms are effective methods to reduce sample complexity by learning the system dynamic model. However, in certain environments, it has been proven that learning an accurate system dynamic model is a formidable problem, and their asymptotic performance cannot achieve to the same level as model-free algorithms. In our work, we use an ensemble of deep neural networks to learn system dynamics and incorporate model uncertainty. Then in order to merge the high asymptotic performance of the advanced model-free methods, the deep deterministic policy gradient (DDPG) algorithm is adopted to optimize robot control policy. Furthermore, it has been implemented within ROS for controlling a Baxter robot in the simulation environment.",https://ieeexplore.ieee.org/document/9195341/,2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM),18-21 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2016.7487661,Model-predictive control with stochastic collision avoidance using Bayesian policy optimization,IEEE,Conferences,"Robots are increasingly expected to move out of the controlled environment of research labs and into populated streets and workplaces. Collision avoidance in such cluttered and dynamic environments is of increasing importance as robots gain more autonomy. However, efficient avoidance is fundamentally difficult since computing safe trajectories may require considering both dynamics and uncertainty. While heuristics are often used in practice, we take a holistic stochastic trajectory optimization perspective that merges both collision avoidance and control. We examine dynamic obstacles moving without prior coordination, like pedestrians or vehicles. We find that common stochastic simplifications lead to poor approximations when obstacle behavior is difficult to predict. We instead compute efficient approximations by drawing upon techniques from machine learning. We propose to combine policy search with model-predictive control. This allows us to use recent fast constrained model-predictive control solvers, while gaining the stochastic properties of policy-based methods. We exploit recent advances in Bayesian optimization to efficiently solve the resulting probabilistically-constrained policy optimization problems. Finally, we present a real-time implementation of an obstacle avoiding controller for a quadcopter. We demonstrate the results in simulation as well as with real flight experiments.",https://ieeexplore.ieee.org/document/7487661/,2016 IEEE International Conference on Robotics and Automation (ICRA),16-21 May 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2017.7965938,Modeling direction selective visual neural network with ON and OFF pathways for extracting motion cues from cluttered background,IEEE,Conferences,"The nature endows animals robust vision systems for extracting and recognizing different motion cues, detecting predators, chasing preys/mates in dynamic and cluttered environments. Direction selective neurons (DSNs), with preference to certain orientation visual stimulus, have been found in both vertebrates and invertebrates for decades. In this paper, with respect to recent biological research progress in motion-detecting circuitry, we propose a novel way to model DSNs for recognizing movements on four cardinal directions. It is based on an architecture of ON and OFF visual pathways underlies a theory of splitting motion signals into parallel channels, encoding brightness increments and decrements separately. To enhance the edge selectivity and speed response to moving objects, we put forth a bio-plausible spatial-temporal network structure with multiple connections of same polarity ON/OFF cells. Each pair-wised combination is filtered with dynamic delay depending on sampling distance. The proposed vision system was challenged against image streams from both synthetic and cluttered real physical scenarios. The results demonstrated three major contributions: first, the neural network fulfilled the characteristics of a postulated physiological map of conveying visual information through different neuropile layers; second, the DSNs model can extract useful directional motion cues from cluttered background robustly and timely, which hits at potential of quick implementation in vision-based micro mobile robots; moreover, it also represents better speed response compared to a state-of-the-art elementary motion detector.",https://ieeexplore.ieee.org/document/7965938/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2015.7139899,Modeling of movement control architectures based on motion primitives using domain-specific languages,IEEE,Conferences,"This paper introduces a model-driven approach for engineering complex movement control architectures based on motion primitives, which in recent years have been a central development towards adaptive and flexible control of complex and compliant robots. We consider rich motor skills realized through the composition of motion primitives as our domain. In this domain we analyze the control architectures of representative example systems to identify common abstractions. It turns out that the introduced notion of motion primitives implemented as dynamical systems with machine learning capabilities, provide the computational building block for a large class of such control architectures. Building on the identified concepts, we introduce domain-specific languages that allow the compact specification of movement control architectures based on motion primitives and their coordination respectively. Using a proper tool chain, we show how to employ this model-driven approach in a case study for the real world example of automatic laundry grasping with the KUKA LWR-IV, where executable source-code is automatically generated from the domain-specific language specification.",https://ieeexplore.ieee.org/document/7139899/,2015 IEEE International Conference on Robotics and Automation (ICRA),26-30 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2005.1513775,Modularity and integration in the design of a socially interactive robot,IEEE,Conferences,"Designing robots that are capable of interacting with humans in real life settings is a challenging task. One key issue is the integration of multiple modalities (e.g., mobility, physical structure, navigation, vision, audition, dialogue, reasoning) into a coherent framework. Taking the AAAI mobile robot challenge (making a robot attend the national conference on artificial intelligence) as the experimental context, we are currently addressing hardware, software and computation integration issues involved in designing a robot capable of sophisticated interaction with humans. This paper reports on our design solutions and the current status of the work, along with the potential impacts this design on human-robot interaction research.",https://ieeexplore.ieee.org/document/1513775/,"ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.",13-15 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593871,"Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning",IEEE,Conferences,"Robots that navigate among pedestrians use collision avoidance algorithms to enable safe and efficient operation. Recent works present deep reinforcement learning as a framework to model the complex interactions and cooperation. However, they are implemented using key assumptions about other agents' behavior that deviate from reality as the number of agents in the environment increases. This work extends our previous approach to develop an algorithm that learns collision avoidance among a variety of types of dynamic agents without assuming they follow any particular behavior rules. This work also introduces a strategy using LSTM that enables the algorithm to use observations of an arbitrary number of other agents, instead of previous methods that have a fixed observation size. The proposed algorithm outperforms our previous approach in simulation as the number of agents increases, and the algorithm is demonstrated on a fully autonomous robotic vehicle traveling at human walking speed.",https://ieeexplore.ieee.org/document/8593871/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR.2013.6766513,Move and the robot will learn: Vision-based autonomous learning of object models,IEEE,Conferences,"As robots are increasingly deployed in complex real-world domains, visual object recognition continues to be an open problem. Existing algorithms for learning and recognizing objects are predominantly computationally expensive, and require considerable training or domain knowledge. Our algorithm enables robots to use motion cues to identify and focus on a set of interesting objects, automatically extracting appearance-based and contextual cues from a small number of images to efficiently learn representative models of these objects. Learned models exploit complementary strengths of: (a) relative spatial arrangement of gradient features; (b) graph-based models of neighborhoods of gradient features; (c) parts-based models of image segments; (d) color distributions; and (e) mixture models of local context. The learned models are used in conjunction with an energy minimization algorithm and a generative model of information fusion for reliable and efficient recognition in novel scenes. The algorithm is evaluated on mobile robots in indoor and outdoor domains, and on images from benchmark datasets.",https://ieeexplore.ieee.org/document/6766513/,2013 16th International Conference on Advanced Robotics (ICAR),25-29 Nov. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM42981.2021.9488669,Multi-Robot Path Planning for Mobile Sensing through Deep Reinforcement Learning,IEEE,Conferences,"Mobile sensing is an effective way to collect environmental data such as air quality, humidity and temperature at low costs. However, mobile robots are typically battery powered and have limited travel distances. To accelerate data collection in large geographical areas, it is beneficial to deploy multiple robots to perform tasks in parallel. In this paper, we investigate the Multi-Robot Informative Path Planning (MIPP) problem, namely, to plan the most informative paths in a target area subject to the budget constraints of multiple robots. We develop two deep reinforcement learning (RL) based cooperative strategies: independent learning through credit assignment and sequential rollout based learning for MIPP. Both strategies are highly scalable with the number of robots. Extensive experiments are conducted to evaluate the performance of the proposed and baseline approaches using real-world WiFi Received Signal Strength (RSS) data. In most cases, the RL based solutions achieve superior or similar performance as a baseline genetic algorithm (GA)-based solution but at only a fraction of running time during inference. Furthermore, when the budgets and initial positions of the robots change, the pre-trained policies can be applied directly.",https://ieeexplore.ieee.org/document/9488669/,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,10-13 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2015.7354094,Multi-robot 6D graph SLAM connecting decoupled local reference filters,IEEE,Conferences,"Teams of mobile robots can be deployed in search and rescue missions to explore previously unknown environments. Methods for joint localization and mapping constitute the basis for (semi-)autonomous cooperative action, in particular when navigating in GPS-denied areas. As communication losses may occur, a decentralized solution is required. With these challenges in mind, we designed a submap-based SLAM system that relies on inertial measurements and stereo-vision to create multi-robot dense 3D maps. For online pose and map estimation, we integrate the results of keyframe-based local reference filters through incremental graph SLAM. To the best of our knowledge, we are the first to combine these two methods to benefit from their particular advantages for 6D multi-robot localization and mapping: Local reference filters on each robot provide real-time, long-term stable state estimates that are required for stabilization, control and fast obstacle avoidance, whereas online graph optimization provides global multi-robot pose and map estimates needed for cooperative planning. We propose a novel graph topology for a decoupled integration of local filter estimates from multiple robots into a SLAM graph according to the filters' uncertainty estimates and independence assumptions and evaluated its benefits on two different robots in indoor, outdoor and mixed scenarios. Further, we performed two extended experiments in a multi-robot setup to evaluate the full SLAM system, including visual robot detections and submap matches as inter-robot loop closure constraints.",https://ieeexplore.ieee.org/document/7354094/,2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Sept.-2 Oct. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2001.977153,Multi-robot path planning for dynamic environments: a case study,IEEE,Conferences,"Most multi-robot navigation planning methods make assumptions about the kind of navigation problems they are to solve and the capabilities of the robots they are to control. In this paper, we propose to select problem-adequate navigation planning methods based on empirical investigations, that is, the robots should learn by experimentation to use the best planning methods. To support this development strategy we provide software tools that enable the robots to automatically learn predictive models for the performance of different navigation planning methods in a given application domain. We show, in the context of robot soccer, that the hybrid planning method which selects planning methods based on a learned predictive model outperforms the individual planning methods. The results are validated in extensive experiments using a realistic and accurate robot simulator that has learned the dynamic model of the real robots.",https://ieeexplore.ieee.org/document/977153/,Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180),29 Oct.-3 Nov. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593899,Multisensor Online Transfer Learning for 3D LiDAR-Based Human Detection with a Mobile Robot,IEEE,Conferences,"Human detection and tracking is an essential task for service robots, where the combined use of multiple sensors has potential advantages that are yet to be fully exploited. In this paper, we introduce a framework allowing a robot to learn a new 3D LiDAR-based human classifier from other sensors over time, taking advantage of a multisensor tracking system. The main innovation is the use of different detectors for existing sensors (i.e. RGB-D camera, 2D LiDAR) to train, online, a new 3D LiDAR-based human classifier based on a new “trajectory probability”. Our framework uses this probability to check whether new detection belongs to a human trajectory, estimated by different sensors and/or detectors, and to learn a human classifier in a semi-supervised fashion. The framework has been implemented and tested on a real-world dataset collected by a mobile robot. We present experiments illustrating that our system is able to effectively learn from different sensors and from the environment, and that the performance of the 3D LiDAR-based human classification improves with the number of sensors/detectors used.",https://ieeexplore.ieee.org/document/8593899/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON.2000.845570,Navigational task-planning of distributed cooperative robotic vehicles,IEEE,Conferences,"Real time obstacle avoidance is one of the intelligent tasks for intelligent mobile robots. Mobile robots should possess the ability to act autonomously in the presence of uncertainty and to adjust their action based on sensed information. They also should be capable of accepting high level mission oriented commands, integrate several kinds of data, including task specification, able to handle information about their own state and the state of the environment too, and be capable of reasoning under uncertainty without human intervention. The proposed technique is a behavior-based approach that blends subset navigational behaviors such as reflexive, potential field and wall following techniques. We discuss each approach separately and present a technique for adaptive navigational behavior switching that is conditional based on the availability of sensory information locally and globally. Our navigation control strategies are developed fully in the FMCell environment. We present some simulation results from FMCell software.",https://ieeexplore.ieee.org/document/845570/,Proceedings of the IEEE SoutheastCon 2000. 'Preparing for The New Millennium' (Cat. No.00CH37105),9-9 April 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/eIT53891.2022.9814051,Near-Optimal Synchronization of Multiple Robot Carts Using Online Reinforcement Learning,IEEE,Conferences,"This paper studies the optimal synchronization of multiple robot carts using online reinforcement learning. The objective is to validate the use of online reinforcement learning to cooperatively control multiagent systems, which may render the potential applications in optimization and control of engineered multiple dynamical systems, such as autonomous vehicles platoon, power grids, sensor networks, et.al. To control the robots, we pro-pose an online reinforcement learning based cooperative control algorithm, which enables the individual agent to learn its own optimal control law in real time by minimizing the overall group cost function. The result of the online reinforcement learning leads to a model free near optimal cooperative control algorithm for multiagent systems. To experimentally verify the control algorithm, two robots are constructed with several integrated sensor modules. Via the XBee module, robots can communicate position and velocity information to one another and receive position and velocity information from a virtual leader. This virtual leader is following a desired trajectory, allowing us to see the control algorithm synchronize the two followers with the leader on a desired trajectory. The control algorithm is implemented using an NVIDIA Jetson Nano. The testing results have proved the effectiveness of the proposed design.",https://ieeexplore.ieee.org/document/9814051/,2022 IEEE International Conference on Electro Information Technology (eIT),19-21 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR55393.2022.9826265,Neural Network Model of eFAST Target Prediction for Robotic Ultrasound Diagnostics in Austere Environments,IEEE,Conferences,"Modern robotic technology has the potential to solve complex problems in healthcare, such as providing technology to support medical care in austere environments characterized by the restricted availability of local medical professionals or with high patient-to-caregiver ratios. Medical robots can be deployed as a force multiplier in situations when skilled healthcare providers are limited and can reduce the risk of medical failures during diagnostic and intervention procedures. One example is detecting free-flowing blood in the abdomen and pneumothorax by performing the extended Focused Assessment with Sonography for Trauma (eFAST) ultrasound diagnostics examination. We developed a semi-autonomous robotic ultrasound system that intelligently perceives the patient&#x2019;s pose and determines the corresponding eFAST configuration for the robotic-assisted procedure. The dynamic pose of the patient is identified in real-time using an optimization-based algorithm, while the eFAST configurations are predicted by a neural network model. A robot manipulator holding an ultrasound device is autonomously driven to the body-normal vector of each eFAST target. This approach accomplishes robust prediction for non-linear problems, even with dynamic posture of the detected body in highly unstructured sites with variable patient shape and pose.",https://ieeexplore.ieee.org/document/9826265/,2022 19th International Conference on Ubiquitous Robots (UR),4-6 July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2004.1490796,Neural network based control of a four rotor helicopter,IEEE,Conferences,"In this paper the design and development of an intelligent controller based on neural networks for a hoverable flying robot to be capable of achieving vertical take off and landing and to be able to sustain a specified attitude is presented. The ability to be able to autonomously navigate through a predefined path was designated for a future phase. This work is different from most autonomous flying robots as it focuses on a four-propeller configuration. This is a very rare helicopter design because of its inherent instability and it is believed that an autonomous robot of this configuration has not yet been successfully developed. In addition, this project uses fixed pitch propellers instead of variable pitch rotors resulting in a greatly reduced cost and mechanical complexity. The downside is that this introduces significant additional challenges in the control. Relative stability was achieved in three axis and all the supporting modules were successfully designed and implemented. However, significant challenges were encountered including the complexities of creating a neural networks controller (NNC) to work in real-time in a slow microcontroller as well as to develop the training process.",https://ieeexplore.ieee.org/document/1490796/,"2004 IEEE International Conference on Industrial Technology, 2004. IEEE ICIT '04.",8-10 Dec. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COASE.2008.4626446,Neural network based path planning for a multi-robot system with moving obstacles,IEEE,Conferences,"Recently, a Coordinated Hybrid Agent (CHA) framework was proposed for the control of Multi-Agent Systems (MASs). In the past few years, it has been applied to both homogeneous and heterogeneous multi-agent systems. In previous studies, the coordination among agents were implemented based on the designer’s knowledge of the system. For large complex systems, it would be desirable if we can plan the coordination among agents dynamically. It was demonstrated that an intelligent planner can be designed for the CHA framework to automatically generate desired actions for multiple robots in a multi-agent system. However, in previous studies, only static obstacles in the environment were considered. In this paper, a neural network based approach is proposed for a multi-robot system with moving obstacles. A biologically inspired neural network based intelligent planner is designed for the coordination of multi-agent systems. The dynamics of each neuron in the topologically organized neural network is characterized by a shunting neural equation. A landscape of the neural activities for all neurons of a CHA agent contains information about the agent’s local goal, and moving obstacles. The objective for building the intelligent planner is to plan actions for multiple mobile robots to coordinate with others and to achieve the global goal. The proposed approach is able to plan the paths for multiple robots while avoiding moving obstacles. The proposed approach is simulated using both Matlab and Vortex. The virtual physical world is built using Vortex to test and develop navigation strategies for robot platforms. The Vortex module executes control commands from the control system module, and provides the outputs describing the vehicle state and terrain information, which are in turn used in the control module to produce the control commands. Simulation results show that an intelligent planner can be designed for the CHA framework to control a large complex system so that coordination among agents can be achieved.",https://ieeexplore.ieee.org/document/4626446/,2008 IEEE International Conference on Automation Science and Engineering,23-26 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCMB.2014.7020689,Neuromodulation based control of autonomous robots in ROS environment,IEEE,Conferences,"The paper presents a control approach based on vertebrate neuromodulation and its implementation on autonomous robots in the open-source, open-access environment of robot operating system (ROS) within a cloud computing framework. A spiking neural network (SNN) is used to model the neuromodulatory function for generating context based behavioral responses of the robots to sensory input signals. The neural network incorporates three types of neurons- cholinergic and noradrenergic (ACh/NE) neurons for attention focusing and action selection, dopaminergic (DA) neurons for rewards- and curiosity-seeking, and serotonergic (5-HT) neurons for risk aversion behaviors. The model depicts description of neuron activity that is biologically realistic but computationally efficient to allow for large-scale simulation of thousands of neurons. The model is implemented using graphics processing units (GPUs) for parallel computing in real-time using the ROS environment. The model is implemented to study the risk-taking, risk-aversive, and distracted behaviors of the neuromodulated robots in single- and multi-robot configurations. The entire process is implemented in a distributed computing framework using ROS where the robots communicate wirelessly with the computing nodes through the on-board laptops. Results are presented for both single- and multi-robot configurations demonstrating interesting behaviors.",https://ieeexplore.ieee.org/document/7020689/,"2014 IEEE Symposium on Computational Intelligence, Cognitive Algorithms, Mind, and Brain (CCMB)",9-12 Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/2830772.2830789,Neuromorphic accelerators: A comparison between neuroscience and machine-learning approaches,IEEE,Conferences,"A vast array of devices, ranging from industrial robots to self-driven cars or smartphones, require increasingly sophisticated processing of real-world input data (image, voice, radio, ...). Interestingly, hardware neural network accelerators are emerging again as attractive candidate architectures for such tasks. The neural network algorithms considered come from two, largely separate, domains: machine-learning and neuroscience. These neural networks have very different characteristics, so it is unclear which approach should be favored for hardware implementation. Yet, few studies compare them from a hardware perspective. We implement both types of networks down to the layout, and we compare the relative merit of each approach in terms of energy, speed, area cost, accuracy and functionality. Within the limit of our study (current SNN and machine learning NN algorithms, current best effort at hardware implementation efforts, and workloads used in this study), our analysis helps dispel the notion that hardware neural network accelerators inspired from neuroscience, such as SNN+STDP, are currently a competitive alternative to hardware neural networks accelerators inspired from machine-learning, such as MLP+BP: not only in terms of accuracy, but also in terms of hardware cost for realistic implementations, which is less expected. However, we also outline that SNN+STDP carry potential for reduced hardware cost compared to machine-learning networks at very large scales, if accuracy issues can be controlled (or for applications where they are less important). We also identify the key sources of inaccuracy of SNN+STDP which are less related to the loss of information due to spike coding than to the nature of the STDP learning algorithm. Finally, we outline that for the category of applications which require permanent online learning and moderate accuracy, SNN+STDP hardware accelerators could be a very cost-efficient solution.",https://ieeexplore.ieee.org/document/7856622/,2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),5-9 Dec. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ASCC56756.2022.9828170,Object-Oriented Navigation with a Multi-layer Semantic Map,IEEE,Conferences,"In this paper, a novel architecture is proposed for object-oriented navigation of a mobile robot. The idea is to autonomous navigate and docking with respect to a given object in an indoor environment. For this purpose, the navigation scheme is designed based on a novel multi-layer semantic map. The map includes an occupancy layer for path planning, a localization layer for control and an object pose estimation layer for docking control. The two-dimensional (2D) occupancy map is generated from a 3D RTAB-Map method. The multi-layer semantic map integrates the outputs of a deep-learning model of real-time 3D object pose estimation for robot localization. Experimental results show that the proposed navigation system can plan a collision-free path for the mobile robot to reach and dock to the target object. The LiDAR-based navigation control runs at 15 Hz and the object-based localization is updated at 4 Hz on the Jetson TX2 embedded controller. In the current implementation, the mobile robot can dock to an object with an accuracy of 8 cm in position and 5 degrees in orientation. The navigation design has the potential to be applied for daily life tasks of mobile robots in an indoor environment.",https://ieeexplore.ieee.org/document/9828170/,2022 13th Asian Control Conference (ASCC),4-7 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CYBER.2017.8446065,Obstacle avoidance of aerial vehicle based on monocular vision,IEEE,Conferences,"Collision-free autonomous navigation is extremely important for quadcopter and other flying robots. The implementation of autonomic moving capabilities can contribute significantly to their promotion and usage in fields such as goods delivering, aerial photos shooting, and monitoring. In order to realize the autonomous flight without crash, the obstacle avoidance problem demands a prompt solution. Also, with the concern of cost and endurance, using only single camera to perform this task would be a better choice for low-cost flying robots. Thus, this paper focuses on achieving quadcopter's collision avoidance in unknown stable (rarely changes, such as high sky or inner room) environment only by single camera. The algorithm proposed by this paper is composed of PTAM (Parallel Tracking and Mapping), DTAM (Dense Tracking and Mapping in Real-Time) algorithm and CNN (Convolutional Neural Network). PTAM is used to create the 3D map of the environment. DTAM is used to obtain the depth map of those image frames. And the CNN is used to train and get a model used for automatically avoidance. Finally, this algorithm is proved to be valid by an experiment.",https://ieeexplore.ieee.org/document/8446065/,"2017 IEEE 7th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",31 July-4 Aug. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2017.8280907,Obstacle avoidance of hexapod robots using fuzzy Q-learning,IEEE,Conferences,"Safe and autonomous obstacle avoidance plays an important role in the navigation control of hexapod robots. In this paper, we combine the method of reinforcement learning with fuzzy control to achieve the autonomous obstacle avoidance for a hexapod robot in complex environments. A fuzzy Q-learning algorithm is first presented and an obstacle avoidance approach is proposed using the Fuzzy Q-learning algorithm regarding the specific requirements of the hexapod robot. Then, the proposed approach is implemented for a real hexapod robot system that uses ultrasonic sensors to detect the obstacles in an unknown environment and learns an optimal policy to avoid the obstacles. Several groups of experiments are carried out to verify the performance of the proposed approach.",https://ieeexplore.ieee.org/document/8280907/,2017 IEEE Symposium Series on Computational Intelligence (SSCI),27 Nov.-1 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDARW.2019.30062,OctShuffleMLT: A Compact Octave Based Neural Network for End-to-End Multilingual Text Detection and Recognition,IEEE,Conferences,"In recent years, scene text detection has witnessed rapid progress especially with the recent development of convolutional neural networks. However, there still exist many challenges in applying very deep networks to many real-world applications, that have hardware limitations, such as robots, and smartphones. To address these challenges, in this paper, we propose the OctShuffleMLT, an effective fully convolutional neural network, with fewer layers and parameters, which can precisely detect multilingual scene text. Our proposed model is based on the Octave Convolutions that use compact blocks, which reduces memory inference by 13.16%, FLOPS by 71.86%, and the number of parameters by 34.04% when compared to the baseline system. Extensive experiments were conducted on ICDAR 2015 and ICDAR 2017 datasets. Experimental results show that our model can produce accurate detection recognition results on both datasets. The code for the paper is made available on the GitHub repository https://github.com/victoic/OctShuffle-MLT.",https://ieeexplore.ieee.org/document/8892934/,2019 International Conference on Document Analysis and Recognition Workshops (ICDARW),22-25 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812026,Off Environment Evaluation Using Convex Risk Minimization,IEEE,Conferences,"Applying reinforcement learning (RL) methods on robots typically involves training a policy in simulation and deploying it on a robot in the real world. Because of the model mismatch between the real world and the simulator, RL agents deployed in this manner tend to perform suboptimally. To tackle this problem, researchers have developed robust policy learning algorithms that rely on synthetic noise disturbances. However, such methods do not guarantee performance in the target environment. We propose a convex risk minimization algorithm to estimate the model mismatch between the simulator and the target domain using trajectory data from both environments. We show that this estimator can be used along with the simulator to evaluate performance of an RL agents in the target domain, effectively bridging the gap between these two environments. We also show that the convergence rate of our estimator to be of the order of <tex>$n^{-1/4}$</tex>, where <tex>$n$</tex> is the number of training samples. In simulation, we demonstrate how our method effectively approximates and evaluates performance on Gridworld, Cartpole, and Reacher environments on a range of policies. We also show that the our method is able to estimate performance of a 7 DOF robotic arm using the simulator and remotely collected data from the robot in the real world.",https://ieeexplore.ieee.org/document/9812026/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/URAI.2018.8441890,On Humanoid Co-Robot Locomotion when Mechanically Coupled to a Human Partner,IEEE,Conferences,"This work focuses on the implementation of mechanically coupled tasks between a humanoid robot and a human. The latter focus comes from the push for robots to work with humans in everyday life as an overarching goal for the field. Co-robots, or robots that work alongside humans, may be guided by the humans through physical contact, such as the human grasping the robot's hand to gently guide it along a desired path. In this work the single-handed mechanically coupled task of guiding a robot through a course is implemented with four different methods of human input. These methods include: 1) using only force-torque sensors in the wrist of the robot for the control input from the human while the arm is under high-gain position control, creating a rigid coupling between the human and the robot, 2) using the force-torque sensors in the wrist of the robot for the control input while the arm is under low-gain position control with gravity compensation, creating compliant coupling between the human and the robot, 3) using the position of the end-effector of the robot for the control input while the arm is under low-gain position control with gravity compensation, and 4) using the force-torque sensors in the wrist and the position of the end-effector of the robot for the control input while the arm is under low-gain position control with gravity compensation. Tests are performed on the real-world and simulated adult-size humanoid robot DRC-Hubo++. During these tests the human and robot are walking together “hand in hand” with the human guiding the robot in a “figure eight” path. These tests show that having a compliant arm on the robot, when the human is guiding it via moving its end-effector, is beneficial over a rigid arm.",https://ieeexplore.ieee.org/document/8441890/,2018 15th International Conference on Ubiquitous Robots (UR),26-30 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COGINF.2008.4639192,On Visual Semantic Algebra (VSA) and the cognitive process of pattern recognition,IEEE,Conferences,"A new form of denotational mathematics known as Visual Semantic Algebra (VSA) is presented for abstract visual object and architecture manipulation. The cognitive theories for pattern recognition, such as cognitive principles of visual perception and basic mechanisms of object and pattern recognition, are explored. A number of case studies on VSA in pattern recognition are presented to demonstrate VAS’ expressive power for algebraic manipulation of visual objects. The cognitive process of pattern recognition is rigorously modeled using VSA and Real-Time Process Algebra (RTPA), which reveals the fundamental mechanisms of natural pattern recognition by the brain. The theories and case studies demonstrate that VSA can be applied not only in machine visual and spatial reasoning, but also in computational intelligence system designs as a powerful man-machine language for representing and manipulating visual geometrical systems. On the basis of VSA, computational intelligence systems such as robots and cognitive computers may process and inference visual and image object rigorously and efficiently.",https://ieeexplore.ieee.org/document/4639192/,2008 7th IEEE International Conference on Cognitive Informatics,14-16 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI50451.2021.9660138,On the suitability of incremental learning for regression tasks in exoskeleton control,IEEE,Conferences,"In recent times, a new generation of modern exoskeleton robots has come into existence, that aims to utilize machine learning to learn the specific needs and preferences of its users. A simple way to facilitate a personalization of an exoskeleton to the end user is to make use of incremental algorithms that keep learning throughout their deployment. However, it is not clear, if any standard algorithms are fast enough to keep pace with sudden change points in the data stream, like for example the change in movement pattern from a normal walk to going up the stairs. In this paper, we study how well common incremental regression algorithms are suited to predict such an ongoing data stream. We use both, theoretical benchmarks and real world human movement data, to evaluate how fast an algorithm reacts to change points in the data, and how well it is able to remember reoccurring patterns. The results show that a simple KNN algorithm outperforms all other more sophisticated models.",https://ieeexplore.ieee.org/document/9660138/,2021 IEEE Symposium Series on Computational Intelligence (SSCI),5-7 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/M2VIP49856.2021.9665036,On-the-fly Learning of New Objects and Instances,IEEE,Conferences,"This paper focus on efficient learning of new objects by robots, at both instance level and class level. Despite the great achievements in deep learning, new objects are not well handled due to the big data dependency and the closed set assumption, which are not satisfied in many applications. For example, in service robots, the desired objects vary among customers and environments, and it is hard to collect big data for all objects. In addition, new objects occur from time to time that are not contained in the training data. This paper aims to learn new objects on-the-fly after deployment of the robot, without the dependency on pre-defined big data. A practical system is proposed to learn a new instance in 1.5 minutes and a new class in 15 to 25 minutes, by integrating state-of-the-art works including online learning, incremental learning, salient detection, object detection, tracking, re-identification, etc. A novel incremental object detection framework is further proposed with better performance than the state-of-the-arts. Extensive evaluations are conducted to examine the contribution of each module. The proposed system is also deployed on a real robot for end to end performance test.",https://ieeexplore.ieee.org/document/9665036/,2021 27th International Conference on Mechatronics and Machine Vision in Practice (M2VIP),26-28 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2014.23,Online Tool for Benchmarking of Simulated Intervention Autonomous Underwater Vehicles: Evaluating Position Controllers in Changing Underwater Currents,IEEE,Conferences,"Benchmarking is nowadays an issue on robotic research platforms, due to the fact that it is not easy to reproduce previous experiments and knowing in detail in which real conditions other algorithms have been performed. Having a web-based tool to configure and execute benchmarks opens the door to new opportunities as the design of virtual tele-laboratories that permit the implementation of new algorithms using specific and detailed constraints. This is fundamental for designing benchmarks that allow the experiments to be made in a more scientific manner, taking into account that these experiments should be able to be reproduced again by other people under the same circumstances. In the context of underwater interventions with semi-autonomous robots, the situation gets even more interesting, specially those performed on real sea scenarios, which are expensive, and difficult to perform and reproduce. This paper presents the recent advances in the online configuration tool for benchmarking, a tool that is continuously being improved in our laboratory. Our last contribution focuses on evaluating position controllers for changing underwater currents and the possibility for the user to upload its own controllers to the benchmarking tool to get online performance results.",https://ieeexplore.ieee.org/document/7102468/,"2014 2nd International Conference on Artificial Intelligence, Modelling and Simulation",18-20 Nov. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8206344,Online multi-target learning of inverse dynamics models for computed-torque control of compliant manipulators,IEEE,Conferences,"Inverse dynamics models are applied to a plethora of robot control tasks such as computed-torque control, which are essential for trajectory execution. The analytical derivation of such dynamics models for robotic manipulators can be challenging and depends on their physical characteristics. This paper proposes a machine learning approach for modeling inverse dynamics and provides information about its implementation on a physical robotic system. The proposed algorithm can perform online multi-target learning, thus allowing efficient implementations on real robots. Our approach has been tested both offline, on datasets captured from three different robotic systems and online, on a physical system. The proposed algorithm exhibits state-of-the-art performance in terms of generalization ability and convergence. Furthermore, it has been implemented within ROS for controlling a Baxter robot. Evaluation results show that its performance is comparable to the built-in inverse dynamics model of the robot.",https://ieeexplore.ieee.org/document/8206344/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-Berlin47944.2019.8966156,Optimizing Deep Learning Based Semantic Video Segmentation on Embedded GPUs,IEEE,Conferences,"Decision making in many industries today is being improved drastically thanks to artificial intelligence and deep learning. New algorithms address challenges such as genome mapping, medical diagnostics, self-driving cars, autonomous robots and more. Deep learning in embedded systems requires high optimization due to the high computational demand, given that power, heat dissipation, size and price constraints are numerous. In this paper we analyze several acceleration methods which include utilization of GPUs for most complex variants of deep learning, such as semantic video segmentation operating in real time. Specifically, we propose mapping of acceleration routines commonly present within deep learning SDKs to different network layers in semantic segmentation. Finally, we evaluate one implementation utilizing the enumerated techniques for semantic segmentation of front camera in autonomous driving front view.",https://ieeexplore.ieee.org/document/8966156/,2019 IEEE 9th International Conference on Consumer Electronics (ICCE-Berlin),8-11 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812167,Parametric Path Optimization for Wheeled Robots Navigation,IEEE,Conferences,"Collision risk and smoothness are the most important factors in global path planning. Currently, planning methods that reduce global path collision risk and improve its smoothness through numerical optimization have achieved good results. However, these methods cannot always optimize the path. The reason is all points on the path are considered as decision variables, which leads to the high dimensionality of the defined optimization problem. Therefore, we propose a novel global path optimization method. The method characterizes the path as a parametric curve and then optimizes the curve&#x0027;s parameters with a defined objective function, which successfully reduces the dimension of optimization problem. The proposed method is compared with baseline and state-of-the-art methods. Experimental results show the path optimized by our method is not only optimal in collision risk, but also in efficiency and smoothness. Furthermore, the proposed method is also implemented and tested in both simulation and real robots.",https://ieeexplore.ieee.org/document/9812167/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDC.2006.377499,Path Generation Using Matrix Representations of Previous Robot State Data,IEEE,Conferences,"Humans learn by repetition and using past experiences. It is possible for robots to act in a similar fashion. By representing past path traversal experiences with matrices, a new path can be generated without relying on calculations of complex dynamics or control laws. This paper presents one approach for allowing robots to use past experience to generate new paths and control actions. This approach relies on using several matrices to associate each new input value with previous robot states. An example is provided and analyzed which shows a successful simulated implementation of this approach. In addition a real world test of the approach was conducted which demonstrates that the implementation not only generates new paths, but does so fast enough to be feasible for real time systems",https://ieeexplore.ieee.org/document/4178112/,Proceedings of the 45th IEEE Conference on Decision and Control,13-15 Dec. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWECAI50956.2020.00019,Path Planning Obstacle Avoidance Algorithm Based on Wheeled Robot,IEEE,Conferences,"There are many obstacles and movements in the indoor environment. Indoor robots need to cope with the changing environment. This paper studies the obstacle avoidance problem of wheeled robots moving in an unknown environment. Firstly, the dynamic path planning algorithm for robot autonomous obstacle avoidance is studied, and the algorithm is implemented in C# language. Then use the Unity3D game engine to simulate the algorithm. The innovations of this algorithm are as follows: 1. Vectorize the path of the robot; 2. Summarize the motion state of the obstacle and the robot into six cases. During the movement process, the obstacle movement state is continuously judged, and the speed and direction of the obstacle are analyzed. The judgment result must belong to six situations. The experiment proves that the algorithm can solve the obstacle avoidance problem when encountering obstacles of different speeds and sizes, and has stronger applicability.",https://ieeexplore.ieee.org/document/9221693/,2020 International Workshop on Electronic Communication and Artificial Intelligence (IWECAI),12-14 June 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DISA.2018.8490605,Path Planning on Robot Based on D Lite Algorithm,IEEE,Conferences,"The increasing need of autonomous behavior of robots in fields of science and technology formed the requirement for path planning implemented by the robot without the human assistance. In this paper, D* Lite, which is a path planning graph-based algorithm, was used in order to compute the shortest path from a start to goal point in a real environment and make a Pepper robot move in a computed trajectory. The movement of robot was conducted in a static environment, with the map of the environment already known. This paper is a first step in the research focusing on a creation of a so-called intelligent workspace.",https://ieeexplore.ieee.org/document/8490605/,2018 World Symposium on Digital Intelligence for Systems and Machines (DISA),23-25 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICINS43215.2020.9134006,Path Planning with Improved Artificial Potential Field Method Based on Decision Tree,IEEE,Conferences,"Path planning is one of the key research directions in the field of mobile robots. It ensures that moving objects can reach the target point safely and without collision in a complex obstacle environment. The path planning is to search an optimal path from the starting point to the target point for the mobile robot in an environment with obstacles, according to certain evaluation criteria (such as the time, the best path, the minimum energy consumption, etc.). The path planning based on artificial potential field method has been paid more and more attention because of its advantages such as convenient calculation, simple implementation of hardware and outstanding real-time performance. However, the artificial potential field method has some limitations, such as the local minimum, the oscillation of moving objects among obstacles and so on. To solve these problems, we can introduce the idea of decision tree into the artificial potential field method for improvement. In machine learning, decision tree is usually used for classification. It is a prediction model, which represents a mapping relationship between object attributes and object values. By utilizing the advantages of decision tree in rule expression and extraction, an improved artificial potential field path planning model based on decision tree is constructed, which can realize real-time and accurate identification of current behavior and fast decision-making of next time behavior in path planning. Aiming at the dynamic path planning problem of mobile robots in indoor complex environment, based on the traditional artificial potential field method, this paper introduces the distance term into the potential field function, and proposes an improved artificial potential field method based on the idea of decision tree, to solve the local minimum, the oscillation between obstacles and concave obstacle problems. According to repulsion coefficient, deflection angle of resultant force and velocity, a reasonable classification decision is made to meet the needs of different obstacle distribution scenarios, and the effectiveness of the proposed method is verified by simulation experiments. Simulation results show that, compared with the traditional artificial potential field method, the planning time of improved algorithm is reduced by 50%, and the smoothness of path planning by the improved algorithm is increased by 43.3%.",https://ieeexplore.ieee.org/document/9134006/,2020 27th Saint Petersburg International Conference on Integrated Navigation Systems (ICINS),25-27 May 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCEAI52939.2021.00074,Pedestrian Recognition System for Smart Security Robot using Pedestrian Re-identification Algorithm,IEEE,Conferences,"The security system is an important guarantee for the safety of citizens' lives and property. In recent years, security robots have been more and more widely used in security systems. At present, domestic security robots generally lack of pedestrian recognition ability under complex circumstances. Therefore, this paper designs and implements pedestrian recognition system for smart security robots using improved pedestrian re-identification algorithm. Experiment result shows that the system has success rate of 90 % and response speed compliance rate of 94.4% under real circumstances, which is much better than traditional system.",https://ieeexplore.ieee.org/document/9544430/,2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI),27-29 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1999.816641,"Perception, reasoning and learning of multiple agent systems for robot soccer",IEEE,Conferences,"Presents a supervisory control strategy for coordination of soccer playing mobile robots. Within the framework of a hierarchical control structure, three layered components of supervisor, coordinator, and executor emulate the basic three concepts of human intelligence, perception, reasoning, and learning. A small size discrete event system model is derived and implemented in the supervisor and coordinator for state-action reasoning and coordination of multiple robotic agents for a successful soccer game. Experimental results of real soccer games are given to demonstrate the feasibility and effectiveness of the developed supervisory control strategy in terms of structural simplicity and computational speed for real-time control.",https://ieeexplore.ieee.org/document/816641/,"IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)",12-15 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCT.2015.7226205,Performance analysis of path planning techniques for autonomous mobile robots,IEEE,Conferences,"This paper presents a comparative study on path planning techniques for autonomous mobile robots in a cluttered environment. It investigates four well known path planning algorithms and compares their performance with the proposed free configuration eigen-spaces (FCE) path planning method. In total, five path planning algorithms are considered towards the solution of the path planning problem under certain working parameters. These working parameters are the computation time needed to find a solution, the distance traveled and the amount of turning by the autonomous mobile robot. A comparison of results has been analyzed. This study will enable readers to identify, which of the proposed methods is most suitable for application under the working parameters the user wants to optimize. The findings have been summarized in the conclusion section. The techniques were implemented in the real-time robotic software Player/Stage. Further analysis were done using MATLAB mathematical computation software.",https://ieeexplore.ieee.org/document/7226205/,"2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)",5-7 March 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBSII51839.2021.9445124,Positioning the 5-DOF Robotic Arm using Single Stage Deep CNN Model,IEEE,Conferences,"In teleoperation mechanism, the surgical robots are controlled using hand gestures from remote location. The remote location robotic arm control using hand gesture recognition is a challenging computer vision problem. The hand action recognition under complex environment (cluttered background, lighting variation, scale variation etc.) is a difficult and time consuming process. In this paper, a light weight Convolutional Neural Network (CNN) model Single Shot Detector (SSD) Lite MobileNet-V2 is proposed for real-time hand gesture recognition. SSD Lite versions tend to run hand gesture recognition applications on low-power computing devices like Raspberry Pi due to its light weight and timely recognition. The model is deployed using a Camera and two Raspberry Pi Controllers For the hand gesture recognition and data transfer to the cloud server, the Raspberry Pi controller 1 is used. The Raspberry Pi Controller 2 receives the cloud information and controls the Robotic arm operations. The performance of the proposed model is also compared with a SSD Inception-V2 model for the MITI Hand dataset-II (MITI HD-II). The average precision, average recall and F1-score for SSD Lite MobileNet-V2 and SSD Inception-V2 models are analyzed by training and testing the model with the learning rate of 0.0002 using Adam optimizer. SSD MobileNet-V2 model obtained an Average precision of 98.74% and SSD Inception-V2 model as 99.27%, The prediction time for SSD Lite MobileNet-V2 model using Raspberry Pi controller takes only 0.67s whereas, 1.2s for SSD Inception-V2 Model.",https://ieeexplore.ieee.org/document/9445124/,"2021 Seventh International conference on Bio Signals, Images, and Instrumentation (ICBSII)",25-27 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO49542.2019.8961870,Probabilistic Inferences on Quadruped Robots: An Experimental Comparison,IEEE,Conferences,"Due to the reality gap, computer software cannot fully model the physical robot in its environment, with noise, ground friction, and energy consumption. Consequently, a limited number of researchers work on applying machine learning in real-world robots. In this paper, we use two intelligent black-box optimization algorithms, Bayesian Optimization (BO) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES), to solve a quadruped robot gait's parametric search problem in 10 dimensions, and compare these two methods to find which one is more suitable for legged robots' controller parameters tuning. Our results show that both methods can find an optimal solution in 130 iterations. BO converges faster than CMA-ES within its constrained range, while CMA-ES finds the optimum in the continuous space. Compared with the specific controller parameters of two methods, we also find that for quadruped robot's oscillators, the angular amplitude is the most important parameter. Thus, it is very beneficial for the quick parametric search of legged robots&#x2019; controllers and avoids time-consuming manual tuning.",https://ieeexplore.ieee.org/document/8961870/,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),6-8 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV45572.2020.9093599,Probabilistic Object Detection: Definition and Evaluation,IEEE,Conferences,"We introduce Probabilistic Object Detection, the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detections. Given the lack of methods capable of assessing such probabilistic object detections, we present the new Probability-based Detection Quality measure (PDQ). Unlike AP-based measures, PDQ has no arbitrary thresholds and rewards spatial and label quality, and foreground/background separation quality while explicitly penalising false positive and false negative detections. We contrast PDQ with existing mAP and moLRP measures by evaluating state-of-the-art detectors and a Bayesian object detector based on Monte Carlo Dropout. Our experiments indicate that conventional object detectors tend to be spatially overconfident and thus perform poorly on the task of probabilistic object detection. Our paper aims to encourage the development of new object detection approaches that provide detections with accurately estimated spatial and label uncertainties and are of critical importance for deployment on robots and embodied AI systems in the real world.",https://ieeexplore.ieee.org/document/9093599/,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),1-5 March 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2014.6889947,Qualitative approach for inverse kinematic modeling of a Compact Bionic Handling Assistant trunk,IEEE,Conferences,"Compact Bionic Handling Assistant (CBHA) is a continuum manipulator, with pneumatic-based actuation and compliant gripper. This bionic arm is attached to a mobile robot named Robotino. Inspired by the elephant's trunk, it can reproduce biological behaviors of trunks, tentacles, or snakes. Unlike rigid link robot manipulators, the development of high performance control algorithm of continuum robot manipulators remains a challenge, particularly due to their complex mechanical design, hyper-redundancy and presence of uncertainties. Numerous studies have been investigated for modeling of such complex systems. Such continuum robots, like the CBHA present a set of nonlinearities and uncertainties, making difficult to build an accurate analytical model, which can be used for control strategies development. Hence, learning approach becomes a suitable tool in such scenarios in order to capture un-modeled nonlinear behaviors of the continuous robots. In this paper, we present a qualitative modeling approach, based on neuronal model of the inverse kinematic of CBHA. A penalty term constraint is added to the inverse objective function into Distal Supervised Learning (DSL) scheme to select one particular inverse model from the redundancy manifold. The inverse kinematic neuronal model is validated by conducting a real-time implementation on a CBHA trunk.",https://ieeexplore.ieee.org/document/6889947/,2014 International Joint Conference on Neural Networks (IJCNN),6-11 July 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968072,Quickly Inserting Pegs into Uncertain Holes using Multi-view Images and Deep Network Trained on Synthetic Data,IEEE,Conferences,"This paper explores the use of robots to autonomously assemble parts with variations in colors and textures. Specifically, we focus on peg-in-hole assembly with some initial position uncertainty and holes located on surfaces of different colors and textures. Two in-hand cameras and a force-torque sensor are used to account for the position uncertainty. A program sequence comprising learning-based visual servoing, spiral search, and impedance control is implemented to perform the peg-in-hole task with feedback from the above sensors. Contributions are mainly made in the learning-based visual servoing component of the sequence, where a deep neural network is trained with various sets of synthetic data generated using the concept of domain randomization to predict where a hole is. In the experiments and analysis section, the network is analyzed and compared, and a real-world robotic system to insert pegs to holes using the proposed method is implemented. The results show that the implemented peg-in-hole assembly system can perform successful peg-in-hole insertions on surfaces with various colors and textures. It can generally speed up the entire peg-in-hole process, especially when the initial position uncertainty is large.",https://ieeexplore.ieee.org/document/8968072/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2002.1045636,RG mapping: learning compact and structured 2D line maps of indoor environments,IEEE,Conferences,"In this paper we present region and gateway (RG) mapping, a novel approach to laser-based 2D line mapping of indoor environments. RG mapping is capable of acquiring very compact, structured, and semantically annotated maps. We present and empirically analyze the method based on map acquisition experiments with autonomous mobile robots. The experiments show that RG mapping drastically compresses the data contained in line scan maps without substantial loss of accuracy.",https://ieeexplore.ieee.org/document/1045636/,Proceedings. 11th IEEE International Workshop on Robot and Human Interactive Communication,27-27 Sept. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561277,Rapid Pose Label Generation through Sparse Representation of Unknown Objects,IEEE,Conferences,"Deep Convolutional Neural Networks (CNNs) have been successfully deployed on robots for 6-DoF object pose estimation through visual perception. However, obtaining labeled data on a scale required for the supervised training of CNNs is a difficult task - exacerbated if the object is novel and a 3D model is unavailable. To this end, this work presents an approach for rapidly generating real-world, pose-annotated RGB-D data for unknown objects. Our method not only circumvents the need for a prior 3D object model (textured or otherwise) but also bypasses complicated setups of fiducial markers, turntables, and sensors. With the help of a human user, we first source minimalistic labelings of an ordered set of arbitrarily chosen keypoints over a set of RGB-D videos. Then, by solving an optimization problem, we combine these labels under a world frame to recover a sparse, keypoint-based representation of the object. The sparse representation leads to the development of a dense model and the pose labels for each image frame in the set of scenes. We show that the sparse model can also be efficiently used for scaling to a large number of new scenes. We demonstrate the practicality of the generated labeled dataset by training a CNN based 6-DoF object pose estimator.",https://ieeexplore.ieee.org/document/9561277/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2017.7989184,Rapidly exploring learning trees,IEEE,Conferences,"Inverse Reinforcement Learning (IRL) for path planning enables robots to learn cost functions for difficult tasks from demonstration, instead of hard-coding them. However, IRL methods face practical limitations that stem from the need to repeat costly planning procedures. In this paper, we propose Rapidly Exploring Learning Trees (RLT*), which learns the cost functions of Optimal Rapidly Exploring Random Trees (RRT*) from demonstration, thereby making inverse learning methods applicable to more complex tasks. Our approach extends Maximum Margin Planning to work with RRT* cost functions. Furthermore, we propose a caching scheme that greatly reduces the computational cost of this approach. Experimental results on simulated and real-robot data from a social navigation scenario show that RLT* achieves better performance at lower computational cost than existing methods. We also successfully deploy control policies learned with RLT* on a real telepresence robot.",https://ieeexplore.ieee.org/document/7989184/,2017 IEEE International Conference on Robotics and Automation (ICRA),29 May-3 June 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMRTS.1999.777446,Rate modulation of soft real-time tasks in autonomous robot control systems,IEEE,Conferences,"Due to the high number of sensors managed and need to perform complex reasoning activities, real-time control systems of autonomous robots exhibit a high potential for overload, i.e., real-time tasks missing their deadlines. In these systems overload should be regarded as a likely occurrence and hence managed accordingly. In this paper we illustrate a novel scheduling technique for adaptation of soft real-time load to available computational capacity in the context of autonomous robot control architectures. The technique is based on rate modulation of a set of periodic tasks in a range of admissible rates. The technique is shown to be easily computable and several variations in implementation are reviewed within the paper.",https://ieeexplore.ieee.org/document/777446/,Proceedings of 11th Euromicro Conference on Real-Time Systems. Euromicro RTS'99,9-11 June 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SmartWorld.2018.00106,Real-Time Data Processing Architecture for Multi-Robots Based on Differential Federated Learning,IEEE,Conferences,"The emergency of ubiquitous intelligence in various things has become the ultimate cornerstone in building a smart interconnection of the physical world and the human world, which also caters to the idea of Internet of Things (IoT). Nowadays, robots as a new type of ubiquitous IoT devices have gained much attention. With the increasing number of distributed multi-robots, such smart environment generates unprecedented amounts of data. Robotic applications are faced with challenges of such big data: the serious real-time assurance and data privacy. Therefore, in order to obtain the big data values via knowledge sharing under the premise of ensuring the real-time data processing and data privacy, we propose a real-time data processing architecture for multi-robots based on the differential federated learning, called RT-robots architecture. A global shared model with differential privacy protection is trained on the cloud iteratively and distributed to multiple edge robots in each round, and the robotic tasks are processed locally in real time. Our implementation and experiments demonstrate that our architecture can be applied on multiple robotic recognition tasks, balance the trade-off between the performance and privacy.",https://ieeexplore.ieee.org/document/8560084/,"2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",8-12 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNSI.2011.67,Real-Time Face-Detection Engine for Robustness to Variable Illumination and Rotated Faces,IEEE,Conferences,"In this paper, we proposes a novel hardware architecture and FPGA implementation method of high performance real-time face-detection engine for robustness to variable illumination and rotation. The proposed face detection algorithm improved its performance by using MCT (Modified Census Transform), rotation transformation and AdaBoost learning algorithm. For implementation, we used a QVGA class camera, LCD display, and Virtex5 LX330 FPGA made by Xilinx Corporation. The verification results showed that it is possible to detect at least 32 faces in a wide variety of sizes at a maximum speed of 43 frames per second in real time. This finding can be applied to artificial intelligence robots for human recognition, conventional security systems for identity certification, and cutting-edge digital cameras using image processing techniques.",https://ieeexplore.ieee.org/document/5954277/,"2011 First ACIS/JNU International Conference on Computers, Networks, Systems and Industrial Engineering",23-25 May 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Cybermatics_2018.2018.00131,Real-Time Object Recognition Based on NAO Humanoid Robot,IEEE,Conferences,"This paper focuses on the real-time object recognition based indoor humanoid robots like Nao robots. Improving the perceptive ability of service robot has always been a research hotspot. The breakthrough of computer vision technology represented by object recognition provides a broader idea for this purpose. We deployed a micro-cloud layer that connects the robot with the computer vision, thereby realized the concepts of RaaS (Robot as a service). In this paper, in order to make the Nao robot to detect objects faster. We present an architecture about real-time object recognition on Nao, and offload the task of control and data collection from robot to a PC. Next, the image data is transmitted over Ethernet to the workstation, which runs multiple parallel image processing services. These services are built with the current popular deep neural network by TensorFlow and running on a GPU GTX1080 Ti. In the micro-cloud layer, we designed a universal robotic visual task queue model, and a PC registers the task queue to the LAN. There are multiple workers in the LAN, and each worker is an independent service processer. Service processer obtains the task queue from the network and processes the queue, and then the processer puts the results back to the manager. The experimental results of the Nao robot in the simulation and real word show that our model and method are effective. The robot can recognize about 90 kinds of common objects, and each frame of image processing time is about 100 milliseconds.",https://ieeexplore.ieee.org/document/8726687/,"2018 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)",30 July-3 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2005.1527001,Real-Time Path Planning for Mobile Robots,IEEE,Conferences,"A new on-line real-time approach with obstacle avoidance for mobile robots moving in an uncertain environment has been proposed and implemented. With the integration of global planning and local planning, this path planning approach is based on polar coordinates in which the desirable direction angle is taken into consideration as an optimization index. Detecting unknown obstacles with local feedback information by robot’s sensor system, this approach orients the desirable direction of mobile robot so as to generate local sub-goal in every planning window. As a result, the difference between real direction angle and desirable direction angle of robot motion steers the mobile robot to detour collisions and advance toward the target without stopping to re-plan a path when new sensor data become available. This approach is not only simple and flexible, but also overcomes flaws of global planning and local planning. The effectiveness, feasibility, real-time performance, optimization capability, high precision and perfect stability are demonstrated by means of simulation examples.",https://ieeexplore.ieee.org/document/1527001/,2005 International Conference on Machine Learning and Cybernetics,18-21 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RoboSoft54090.2022.9762066,Real-Time Pressure Estimation and Localisation with Optical Tomography-inspired Soft Skin Sensors,IEEE,Conferences,"Sensing and localising pressure resulting from physical interaction between a robot and its environment is a key requirement in the deployment of soft robots in real-life scenarios. In order to adapt the robot&#x0027;s behaviour in real-time, we argue that sensors must have a high sampling rate. In this paper, we present a novel tactile sensing strategy for soft sensors, based on an imaging technique known as optical tomography. Instead of transmitting light through the soft sensor in a sequential way (as commonly done in tomography systems), we demonstrate that concurrently illuminating the sensor with multiple light sources and reading out the sensor response has several advantages. Firstly, it drastically increases the sampling rate of the sensor when compared to standard tomography approaches, making it more suitable to sense sudden and short-lived contacts. Secondly, by concurrently switching on the light sources, we increase performance in terms of pressure localisation and pressure estimation achieved through Machine Learning techniques. We carry out experiments demonstrating that our approach allows for a robust pressure estimation and contact point localisation with an accuracy up to 91.1 &#x0025; (vs 70.3&#x0025;) at a higher sampling rate.",https://ieeexplore.ieee.org/document/9762066/,2022 IEEE 5th International Conference on Soft Robotics (RoboSoft),4-8 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2018.8581288,Real-Time Robot Vision on Low-Performance Computing Hardware,IEEE,Conferences,"Small robots have numerous interesting applications in domains like industry, education, scientific research, and services. For most applications vision is important, however, the limitations of the computing hardware make this a challenging task. In this paper, we address the problem of real-time object recognition and propose the Fast Regions of Interest Search (FROIS) algorithm to quickly find the ROIs of the objects in small robots with low-performance hardware. Subsequently, we use two methods to analyze the ROIs. First, we develop a Convolutional Neural Network on a desktop and deploy it onto the low-performance hardware for object recognition. Second, we adopt the Histogram of Oriented Gradients descriptor and linear Support Vector Machines classifier and optimize the HOG component for faster speed. The experimental results show that the methods work well on our small robots with Raspberry Pi 3 embedded 1.2 GHz ARM CPUs to recognize the objects. Furthermore, we obtain valuable insights about the trade-offs between speed and accuracy.",https://ieeexplore.ieee.org/document/8581288/,"2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)",18-21 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV50864.2020.00032,Real-time Motion Planning for Robotic Teleoperation Using Dynamic-goal Deep Reinforcement Learning,IEEE,Conferences,"We propose Dynamic-goal Deep Reinforcement Learning (DGDRL) method to address the problem of robot arm motion planning in telemanipulation applications. This method intuitively maps human hand motions to a robot arm in real-time, while avoiding collisions, joint limits and singularities. We further propose a novel hardware setup, based on the HTC VIVE VR system, that enables users to smoothly control the robot tool position and orientation with hand motions, while monitoring its movements in a 3D virtual reality environment. A VIVE controller captures 6D hand movements and gives them as reference trajectories to a deep neural policy network for controlling the robot’s joint movements. Our DGDRL method leverages the state-of-art Proximal Policy Optimization (PPO) algorithm for deep reinforcement learning to train the policy network with the robot joint values and reference trajectory observed at each iteration. Since training the network on a real robot is time-consuming and unsafe, we developed a simulation environment called RobotPath which provides kinematic modeling, collision analysis and a 3D VR graphical simulation of industrial robots. The deep neural network trained using RobotPath is then deployed on a physical robot (ABB IRB 120) to evaluate its performance. We show that the policies trained in the simulation environment can be successfully used for trajectory planning on a real robot. The the codes, data and video presenting our experiments are available at https://github.com/kavehkamali/ppoRobotPath.",https://ieeexplore.ieee.org/document/9108691/,2020 17th Conference on Computer and Robot Vision (CRV),13-15 May 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC53003.2021.9727507,Real-time Object Detection Algorithm For Underwater Robots,IEEE,Conferences,"The marine aquaculture fishing industry has caused many problems such as low work efficiency and hidden safety hazards due to low automation. Research on marine fishing robots that can replace humans for autonomous operations has excellent significance and prospects. Aiming at the problems of uneven lighting, poor visibility, and many magazines in the underwater environment, the underwater fishing robot can quickly and autonomously detect marine products and other targets. This paper implements an underwater image enhancement algorithm that can be deployed on a small AI system and an object detection algorithm based on YOLOv5 (You Only Look Once Version 5). The algorithm includes using the limited contrast adaptive histogram equalization image enhancement technology to solve the image quality problems of blue-green and blurry underwater images and then using the YOLOv5 object detection network to detect and locate underwater creatures. Experimental results show that the algorithm can effectively solve poor underwater image quality and unclear targets and rapidly detect seafood targets. The detection accuracy of this algorithm can reach 85%. It has been applied to the underwater fishing robot independently developed by the team and deployed on the Jetson Xavier NX small AI system. The detection accuracy can reach 80%, and the detection speed can reach 30FPS.",https://ieeexplore.ieee.org/document/9727507/,2021 China Automation Congress (CAC),22-24 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LifeTech52111.2021.9391811,Real-time Object Detection with Deep Learning for Robot Vision on Mixed Reality Device,IEEE,Conferences,"Mixed reality device sensing capabilities are valuable for robots, for example, the inertial measurement unit (IMU) sensor and time-of-flight (TOF) depth sensor can support the robot in navigating its environment. This paper demonstrates a deep learning (YOLO model) background, realtime object detection system implemented on mixed reality device. The goal of the system is to create a real-time communication system between HoloLens and Ubuntu systems to enable real-time object detection using the YOLO model. The experimental results show that the proposed method has a fast speed to achieve real-time object detection using HoloLens. This enables Microsoft HoloLens as a device for robot vision. To enhance human-robot interaction, we will apply it to a wearable robot arm system to automatically grasp objects in the future.",https://ieeexplore.ieee.org/document/9391811/,2021 IEEE 3rd Global Conference on Life Sciences and Technologies (LifeTech),9-11 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCA.2007.4389266,Real-time Obstacle Avoidance Strategy for Mobile Robot Based On Improved Coordinating Potential Field with Genetic Algorithm,IEEE,Conferences,"To overcome the problems during navigation of mobile robots in dynamic environment using the traditional artificial potential field (APF) method, a novel improved method called coordinating potential field (CPF) is proposed. The local potential field is constructed by using local subgoals, which obtained by updating dynamic windows. The questions of local minima, oscillation between multiple obstacles and real-time dynamic obstacle avoidance are solved. At last multi-objective parameter optimization is implemented by using adaptive genetic algorithm. Simulation results indicate that this strategy is practicable and effective.",https://ieeexplore.ieee.org/document/4389266/,2007 IEEE International Conference on Control Applications,1-3 Oct. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561861,Real-time Optimal Navigation Planning Using Learned Motion Costs,IEEE,Conferences,"Navigation on challenging terrain topographies requires the understanding of robots’ locomotion capabilities to produce optimal solutions. We present an integrated framework for real-time autonomous navigation of mobile robots based on elevation maps. The framework performs rapid global path planning and optimization that is aware of the locomotion capabilities of the robot. A GPU-aided, sampling-based path planner combined with a gradient-based path optimizer provides optimal paths by using a neural network-based locomotion cost predictor which is trained in simulation. We show that our approach is capable of planning and optimizing paths three orders of magnitude faster than RRT* on GPU-enabled hardware, enabling real-time deployment on mobile platforms. We successfully evaluate the framework on the ANYmal C quadrupedal robot in both simulations and real-world environments for path planning tasks on multiple complex terrains.",https://ieeexplore.ieee.org/document/9561861/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INTECH.2017.8102423,Real-time emotional state detection from facial expression on embedded devices,IEEE,Conferences,"From the last decade, researches on human facial emotion recognition disclosed that computing models built on regression modelling can produce applicable performance. However, many systems need extensive computing power to be run that prevents its wide applications such as robots and smart devices. In this proposed system, a real-time automatic facial expression system was designed, implemented and tested on an embedded device such as FPGA that can be a first step for a specific facial expression recognition chip for a social robot. The system was built and simulated in MATLAB and then was built on FPGA and it can carry out real time continuously emotional state recognition at 30 fps with 47.44% accuracy. The proposed graphic user interface is able to display the participant video and two dimensional predict labels of the emotion in real time together.",https://ieeexplore.ieee.org/document/8102423/,2017 Seventh International Conference on Innovative Computing Technology (INTECH),16-18 Aug. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRIA53009.2021.9588681,Real-time gesture control UAV with a low resource framework,IEEE,Conferences,"This study showcases a low-resource framework that enables people with no technical know-how to interact with drones, it also explores the capabilities of 2D- computer vision and deep learning techniques for gesture based interface systems on a low-cost micro drone with an onboard RGB camera. This Human-Robot Interaction system processes the real-time human pose to allow a user to command the drone, i.e., by providing direction to move and execute actions. A linear PD controller and image processing techniques are implemented to track humans whilst maintaining a safe distance from the user by perceiving depth information through pose estimation. We incorporated the gesture recognition results into a drone using the Robot Operating System (ROS) and evaluated system performance indoor and outdoor. This low computation framework can be applied further to control robotic arms or mobile robots.",https://ieeexplore.ieee.org/document/9588681/,2021 International Symposium of Asian Control Association on Intelligent Robotics and Industrial Automation (IRIA),20-22 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1993.291973,Real-time implementation of neural network learning control of a flexible Space manipulator,IEEE,Conferences,"A neural network approach to online learning control and real-time implementation for a flexible space robot manipulator is presented. An overview of the motivation and system development of the self-mobile space modulator (SM/sup 2/) is given. The neural network learns control by updating feedforward dynamics based on feedback control input. Implementation issues associated with online training strategies are addressed and a single stochastic training scheme is presented. A recurrent neural network architecture with improved performance is proposed. Using the proposed learning scheme, the manipulator tracking error is reduced by 85% compared to that of conventional proportional-integral-derivative (PID) control. The approach possesses a high degree of generality and adaptability to various applications. It will be a valuable learning control method for robots working in unconstructed environments.<>",https://ieeexplore.ieee.org/document/291973/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETS.2017.7968218,Real-time self-learning for control law adaptation in nonlinear systems using encoded check states,IEEE,Conferences,"With the wide proliferation of autonomous sense-and-control real-time systems (such as robots and self-driven cars), a key research objective is rapid recovery from the effects of anomalies and impairments arising from performance degradation of sensors and actuators and electro-mechanical subsystems due to field wear and tear. This must be achieved with minimal impact on system performance while maintaining low implementation overhead and high coverage of multi-parameter failure mechanisms. In this work, we propose a reinforcement learning framework for on-line control law adaptation in autonomous nonlinear systems assisted by system state encodings. These encodings are exploited to generate time-varying error signals whose (transient) waveforms in relation to the input stimulus, contain root-cause diagnostic information. This establishes a statistical correlation between the transient waveforms and the parameters of the optimal nonlinear controller under arbitrary multi-parameter perturbations of sensor/actuator and subsystem performances. Consequently this correlation is tapped, using pre-deployment supervised learning algorithms, to predict near-optimal controller parameter values whenever sufficiently large parameter deviations are detected (due to non-zero error signals). From these near-optimal starting conditions, an actor-critic reinforcement learning controller for nonlinear systems quickly converges to the optimal control law for the parameter-perturbed system (up to 10× faster than for systems not assisted by the diagnostic information provided by the state encoding driven error signal above). We implement the proposed methodology on two nonlinear systems demonstrating fast performance recovery in real time.",https://ieeexplore.ieee.org/document/7968218/,2017 22nd IEEE European Test Symposium (ETS),22-26 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMCCE51767.2020.00092,Real-time virtual UR5 robot imitation of human motion based on 3D camera,IEEE,Conferences,"Human robot interaction is playing significant roles in many industries, in which humans are allowed to corporate with robots or control robots to complete some tasks. Achieving realtime control of robots by demonstrating human arm motion is one of the effective methods to improve the quality of human robot interaction. In this paper, a new method to accomplish human skeleton key points extraction is proposed and a system for controlling a virtual UR5 robot (which is modeling in Unity 3D) is developed to imitate human motions. A sequential architecture composed of convolutional networks is used to identify the 2D coordinates; the depth information from Kinect v2 sensor is used to obtain the actual 3D coordinates and the result of the proposed method is used for joint angles calculation. The appropriate mapping relationships between human arm and UR5 robots are built by transforming the arm angel to UR5's degrees of freedom. The performance of this method is measured by the degree of real-time simulation of human hand trajectory. Further, the comparison between the method in this paper and other method is implemented. The experimental results indicate that the developed system and proposed method are reliable and efficient to accomplish human motion imitation.",https://ieeexplore.ieee.org/document/9421649/,"2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",25-27 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2017.0-161,Realistic Traffic Generation for Web Robots,IEEE,Conferences,"Critical to evaluating the capacity, scalability, and availability of web systems are realistic web traffic generators. Web traffic generation is a classic research problem, no generator accounts for the characteristics of web robots or crawlers that are now the dominant source of traffic to a web server. Administrators are thus unable to test, stress, and evaluate how their systems perform in the face of ever increasing levels of web robot traffic. To resolve this problem, this paper introduces a novel approach to generate synthetic web robot traffic with high fidelity. It generates traffic that accounts for both the temporal and behavioral qualities of robot traffic by statistical and Bayesian models that are fitted to the properties of robot traffic seen in web logs from North America and Europe. We evaluate our traffic generator by comparing the characteristics of generated traffic to those of the original data. We look at session arrival rates, inter-arrival times and session lengths, comparing and contrasting them between generated and real traffic. Finally, we show that our generated traffic affects cache performance similarly to actual traffic, using the common LRU and LFU eviction policies.",https://ieeexplore.ieee.org/document/8260631/,2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA),18-21 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII.2010.5708353,Realization and analysis of giant-swing motion using Q-Learning,IEEE,Conferences,"Many research papers have reported studies on sports robots that realize giant-swing motion. However, almost all these robots were controlled using trajectory planning methods, and few robots realized giant-swing motion by learning. Consequently, in this study, we attempted to construct a humanoid robot that realizes giant-swing motion by Q-learning, a reinforcement learning technique. The significant aspect of our study is that few robotic models were constructed beforehand; the robot learns giant-swing motion only by interaction with the environment during simulations. Our implementation faced several problems such as imperfect perception of the velocity state and robot posture issues caused by using only the arm angle. However, our real robot realized giant-swing motion by averaging the Q value and by using rewards - the absolute angle of the foot angle and the angular velocity of the arm angle-in the simulated learning data; the sampling time was 250 ms. Furthermore, the feasibility of generalization of learning for realizing selective motion in the forward and backward rotational directions was investigated; it was revealed that the generalization of learning is feasible as long as it does not interfere with the robot's motions.",https://ieeexplore.ieee.org/document/5708353/,2010 IEEE/SICE International Symposium on System Integration,21-22 Dec. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WFCS53837.2022.9779202,Redundancy Concepts for Real-Time Cloud- and Edge-based Control of Autonomous Mobile Robots,IEEE,Conferences,"Deploying navigation algorithms on an edge or cloud server according to the Software-as-a-Service paradigm has many advantages for autonomous mobile robots in indus-trial environments, e.g. cooperative planning and less onboard energy consumption. However, outsourcing corresponding real-time critical control functions requires a high level of reliability, which cannot be guaranteed either by modern wireless networks nor by the outsourced computing infrastructure. This work introduces redundancy concepts, which enable real-time capability within these uncertain infrastructures by providing redundant computation nodes, as well as robot-controlled switching between them. Redundancies can vary regarding their physical location, robot behavior during the switchover process and degree of activeness while quality of service concerning the primary controller is sufficient. In the case that fallback redun-dancies are not continuously active, when a disturbance occurs an initial state estimation of the robot pose has to be provided and an activation time has to be anticipated. To gain some insights on expected behavior, redundant computation nodes are deployed locally on the robot and on an outsourced computation node and consequently evaluated empirically. Quantitative and qualitative results in simulation and a real environment show that redun-dancies help to significantly improve the robot-trajectory within an unreliable network. Moreover, resource-saving redundancies, which are not continuously active, can robustly take over control by using an estimated state.",https://ieeexplore.ieee.org/document/9779202/,2022 IEEE 18th International Conference on Factory Communication Systems (WFCS),27-29 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793627,Reinforcement Learning Meets Hybrid Zero Dynamics: A Case Study for RABBIT,IEEE,Conferences,"The design of feedback controllers for bipedal robots is challenging due to the hybrid nature of its dynamics and the complexity imposed by high-dimensional bipedal models. In this paper, we present a novel approach for the design of feedback controllers using Reinforcement Learning (RL) and Hybrid Zero Dynamics (HZD). Existing RL approaches for bipedal walking are inefficient as they do not consider the underlying physics, often requires substantial training, and the resulting controller may not be applicable to real robots. HZD is a powerful tool for bipedal control with local stability guarantees of the walking limit cycles. In this paper, we propose a non traditional RL structure that embeds the HZD framework into the policy learning. More specifically, we propose to use RL to find a control policy that maps from the robot’s reduced order states to a set of parameters that define the desired trajectories for the robot’s joints through the virtual constraints. Then, these trajectories are tracked using an adaptive PD controller. The method results in a stable and robust control policy that is able to track variable speed within a continuous interval. Robustness of the policy is evaluated by applying external forces to the torso of the robot. The proposed RL framework is implemented and demonstrated in OpenAI Gym with the MuJoCo physics engine based on the well-known RABBIT robot model.",https://ieeexplore.ieee.org/document/8793627/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AI4I46381.2019.00027,Reinforcement Learning of a Robot Cell Control Logic using a Software-in-the-Loop Simulation as Environment,IEEE,Conferences,"This paper introduces a method for automatic robot programming of industrial robots using reinforcement learning on a Software-in-the-loop simulation. The focus of the the paper is on the higher levels of a hierarchical robot programming problem. While the lower levels the skills are stored as domain specific program code, the combination of the skills into a robot control program to solve a specific task is automated. The reinforcement learning learning approach allows the shopfloor workers and technicians just to define the end result of the manufacturing process through a reward function. The programming and process optimization is done within the learning procedure. The Software-in-the-loop simulation with the robot control software makes it possible to to interpret the real program code and generate the exact motion. The exact motion of the robot is needed in order to find not just an optimal but also a collision-free policy.",https://ieeexplore.ieee.org/document/9027783/,2019 Second International Conference on Artificial Intelligence for Industries (AI4I),25-27 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9340948,Reinforcement co-Learning of Deep and Spiking Neural Networks for Energy-Efficient Mapless Navigation with Neuromorphic Hardware,IEEE,Conferences,"Energy-efficient mapless navigation is crucial for mobile robots as they explore unknown environments with limited on-board resources. Although the recent deep rein-forcement learning (DRL) approaches have been successfully applied to navigation, their high energy consumption limits their use in several robotic applications. Here, we propose a neuromorphic approach that combines the energy-efficiency of spiking neural networks with the optimality of DRL and benchmark it in learning control policies for mapless navigation. Our hybrid framework, spiking deep deterministic policy gradient (SDDPG), consists of a spiking actor network (SAN) and a deep critic network, where the two networks were trained jointly using gradient descent. The co-learning enabled synergistic information exchange between the two networks, allowing them to overcome each other's limitations through a shared representation learning. To evaluate our approach, we deployed the trained SAN on Intel's Loihi neuromorphic processor. When validated on simulated and real-world complex environments, our method on Loihi consumed 75 times less energy per inference as compared to DDPG on Jetson TX2, and also exhibited a higher rate of successful navigation to the goal, which ranged from 1% to 4.2% and depended on the forward-propagation timestep size. These results reinforce our ongoing efforts to design brain-inspired algorithms for controlling autonomous robots with neuromorphic hardware.",https://ieeexplore.ieee.org/document/9340948/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZY.1998.687475,Reinforcement function design and bias for efficient learning in mobile robots,IEEE,Conferences,"The main paradigm in sub-symbolic learning robot domain is the reinforcement learning method. Various techniques have been developed to deal with the memorization/generalization problem, demonstrating the superior ability of artificial neural network implementations. In this paper, we address the issue of designing the reinforcement so as to optimize the exploration part of the learning. We also present and summarize works relative to the use of bias intended to achieve the effective synthesis of the desired behavior. Demonstrative experiments involving a self-organizing map implementation of the Q-learning and real mobile robots (Nomad 200 and Khepera) in a task of obstacle avoidance behavior synthesis are described.",https://ieeexplore.ieee.org/document/687475/,1998 IEEE International Conference on Fuzzy Systems Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36228),4-9 May 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2005.1626784,Reinforcement learning based group navigation approach for multiple autonomous robotic system,IEEE,Conferences,"In several complex applications, the use of multiple autonomous robotic systems (ARS) becomes necessary to achieve different tasks such as foraging and transport of heavy and large objects with less cost and more efficiency. They have to achieve a high level of flexibility, adaptability and efficiency in real environments. In this paper, a reinforcement learning (RL) based group navigation approach for multiple ARS is suggested. Indeed, the robots must have the ability to form geometric figures and navigate without collisions while maintaining the formation. Thus, each robot must learn how to take its place in the formation and avoid obstacles and other ARS from its interaction with the environment. This approach must provide ARS with capability to acquire the group navigation approach among several ARS from elementary behaviors by learning with trial and error search. Then, simulation results display the ability of the suggested approach to provide ARS with capability to navigate in a group formation in dynamic environments.",https://ieeexplore.ieee.org/document/1626784/,"IEEE International Conference Mechatronics and Automation, 2005",29 July-1 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2012.1301,Relevance Vector Machine based multi-feature integration for semantic place recogntion,IET,Conferences,"In order to work in realistic scenarios, it is a desirable feature for autonomous robots to extract semantic concepts from environments. In this paper, A Relevance Vector Machine (RVM) based approach is presented for the task of visual semantic place recognition. The high sparsity and Bayesian property makes this approach capable of obtaining probabilistic confidence estimation, and computationally efficient during the online prediction stage. Meanwhile, in order to take advantage of discriminative powers of different feature descriptors, a multiple kernel technique is introduced in our system, resulting in a very flexible model where arbitrary feature descriptors can be integrated smoothly. In this paper we choose three popular descriptors for our implementation. Experiments carried out on real typical office environment datasets show the feasibility and robustness of our approach.",https://ieeexplore.ieee.org/document/6492908/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICT4DA53266.2021.9672240,Rescuing the Fresh Water Lakes of Africa through the Use of Drones and Underwater Robots,IEEE,Conferences,"In this paper, we present a conceptual system architecture for real-time monitoring, predicting and controlling of invasive water hyacinth in freshwater bodies through the use of emerging technologies. The proposed system is planned to be deployed as one of the rescue efforts to preserve the fresh water lakes of Africa. The case study and the system presented in this paper are based on the Lake Tana, situated near the city of Bahir Dar, in Ethiopia. The rescuing efforts of Lake Tana so far focused on removal of the weed by hand and using harvesting machines. With the weed invasion doubling every two weeks, the current approaches will not be able to control the rapid invasion of the weed, which is causing considerable socioeconomic losses. The proposed system architecture employs networked underwater robots, aerial drones and other environmental sensors for better mapping of the weed coverage in real-time, predicting the floating paths of the weed, and learning the favourable environmental conditions of the lake for eradicating the invasive weed. The advantages of the proposed technical intervention lie not only in accurate monitoring and fast removal of the weed, but also in facilitating data collection for better understanding of the underlying environmental and chemical conditions that facilitate the rapid infestation and growth of the invasive weed.",https://ieeexplore.ieee.org/document/9672240/,2021 International Conference on Information and Communication Technology for Development for Africa (ICT4DA),22-24 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2019.8816557,Research on V-SLAM Methods,IEEE,Conferences,"With the development of intelligent mobile robots, SLAM, especially V-SLAM, as the basic technology of robot localization and navigation, has the advantages of strong adaptability, high precision and strong intelligence compared with the traditional localization technology. It is widely used in smart devices such as unmanned aerial vehicle, automatic driving and sweeping robots. According to different implementation methods, the visual SLAM is divided into: filter V-SLAM based on probability model, key frame BA-based V-SLAM using nonlinear optimization theory, direct tracking of V-SLAM under the assumption of luminosity invariance, space occupying V-SLAM that focuses on building three-dimensional dense maps. This paper focuses on representative systems of various V-SLAMs and gives their respective applicable scenarios and characteristics. Finally, this article forecasts the development of V-SLAM combining with multi-information fusion technology, semantic deep and learning technology.",https://ieeexplore.ieee.org/document/8816557/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIME.2010.5477962,Research on the embedded system of facial expression recognition based on HMM,IEEE,Conferences,"With the rapid development of artificial intelligence, how to make robots have feelings has become a research highlight today. A portable device for facial expression recognition has been designed, based on the hardware platform of Intel embedded processor PXA270, and the software platform of embedded operation system Linux. The functions of video capture, image processing and face tracking have been achieved, through the application of the USB, Video4Linux programming technology, image processing technology, and artificial psychology theory. At the same time, an emotion model more consistent with the characteristics of human emotions is proposed under the theoretical framework of HMM according to the characteristics of human emotions. A new real-time facial expression recognition method is proposed based on the face detection and facial feature detection technology. Experimental results show that, the effect of face recognition is satisfactory, and the method is effective.",https://ieeexplore.ieee.org/document/5477962/,2010 2nd IEEE International Conference on Information Management and Engineering,16-18 April 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197386,Residual Reactive Navigation: Combining Classical and Learned Navigation Strategies For Deployment in Unknown Environments,IEEE,Conferences,"In this work we focus on improving the efficiency and generalisation of learned navigation strategies when transferred from its training environment to previously unseen ones. We present an extension of the residual reinforcement learning framework from the robotic manipulation literature and adapt it to the vast and unstructured environments that mobile robots can operate in. The concept is based on learning a residual control effect to add to a typical sub-optimal classical controller in order to close the performance gap, whilst guiding the exploration process during training for improved data efficiency. We exploit this tight coupling and propose a novel deployment strategy, switching Residual Reactive Navigation (sRRN), which yields efficient trajectories whilst probabilistically switching to a classical controller in cases of high policy uncertainty. Our approach achieves improved performance over end-to-end alternatives and can be incorporated as part of a complete navigation stack for cluttered indoor navigation tasks in the real world. The code and training environment for this project is made publicly available at https://sites.google.com/view/srrn/home.",https://ieeexplore.ieee.org/document/9197386/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACII.2013.88,Reversal Learning Based on Somatic Markers,IEEE,Conferences,"One of the main aspects in the field of Artificial Intelligence is the creation of agents with the ability to learn like human beings do. Based on made experiences humans are able to adapt their behaviour in order to solve tasks. Another important aspect of human decision making is the ability to discard learned behaviour when the usual decisions, concerning a stimulus, lead to a bad outcome. For robots intended to be embedded in a social environment, the adaptability of behaviour is an important factor. Research of human decision behaviour shows, that emotions play a decisive role, even for learning and reversal learning. In this paper, improvements and further results of a previously presented framework for decision making based on an emotional memory are presented. The improvements include the reduction of the amount of previous knowledge that has to be implemented and an evaluation concerning reversal learning. For evaluation purposes, a typical reversal learning task, performed by real subjects, has been used. The results show that this framework allows the adaption of behaviour comparable to human subjects and offers decisive improvements, which lead to better results in reversal learning tasks without the need to directly declare a task as such one.",https://ieeexplore.ieee.org/document/6681479/,2013 Humaine Association Conference on Affective Computing and Intelligent Interaction,2-5 Sept. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM.2019.8868670,Rhino: An Open-source Embedded Motherboard Design Enabling Complex Behavior of Intelligent Robots,IEEE,Conferences,"In recent years, Robot Operating System (ROS) has become a de facto standard for many robotic systems. However, there lacks a general-purpose control hardware to perfectly support ROS applications in an embedded fashion. In this paper, we take a hardware/software co-design methodology and a loosely coupled design methodology to develop a ROSoriented motherboard dedicatedly for facilitating high-end intelligent robotic applications. First, orienting around the ROS computational graph level, the hardware/software co-design is proposed to realize a mirrored modular design paradigm. Second, an open-source ROS motherboard, namely the ""Rhino"", is accordingly designed with the highlight of accelerating the embedded neuromorphic computation. Third, real-time performance and feasibility of Rhino are validated at different scales. Experimentation shows that the open-source prototype motherboard is eligible for ROS-based robot development and outperforms the conventional IPC and tailor-made control board. ROS-oriented hardware/software codesign paradigm complements the ROS ecosystem with an open-source AI-enabled motherboard for developing intelligent robots.",https://ieeexplore.ieee.org/document/8868670/,2019 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),8-12 July 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2015.7139395,RoboSherlock: Unstructured information processing for robot perception,IEEE,Conferences,"We present RoboSherlock, an open source software framework for implementing perception systems for robots performing human-scale everyday manipulation tasks. In RoboSherlock, perception and interpretation of realistic scenes is formulated as an unstructured information management (UIM) problem. The application of the UIM principle supports the implementation of perception systems that can answer task-relevant queries about objects in a scene, boost object recognition performance by combining the strengths of multiple perception algorithms, support knowledge-enabled reasoning about objects and enable automatic and knowledge-driven generation of processing pipelines. We demonstrate the potential of the proposed framework by three feasibility studies of systems for real-world scene perception that have been built on top of RoboSherlock.",https://ieeexplore.ieee.org/document/7139395/,2015 IEEE International Conference on Robotics and Automation (ICRA),26-30 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO49542.2019.8961517,Robot Control in Human Environment using Deep Reinforcement Learning and Convolutional Neural Network,IEEE,Conferences,"Deep reinforcement learning (DRL) has been employed in numerous applications where complex decision-making is needed. Robot control in a human environment is an example. Such algorithm offers possibilities to achieve end-to-end training which learns from image directly. However, training on a physical robotic system under human environments using DRL is inefficient and even dangerous. Several recent works have used simulators for training models before implementing to physical robots. Although simulation provides efficiency to obtain DRL trained models, it poses challenges for the transformation from simulation to reality. Since a human environment is often cluttered, dynamic and complex, the policy trained with simulation images is not applicable for reality. Therefore, in this paper, we propose a DRL method to achieve end-to-end training in simulation, as well as to adapt to reality without any further finetune. Firstly, a Deep Deterministic Policy Gradient algorithm (DDPG) is employed to learn policy for robot control. Secondly, a pre-trained Convolutional Neural Network algorithm (CNN) is used to visually track the target in image. This technique provides the efficient and safe DRL training in simulation while offering robust application when a real robot is placed in dynamic human environment. Simulation and experiment are conducted for validation and can be seen in the attached video. The results have shown successful demonstration under various complex environments.",https://ieeexplore.ieee.org/document/8961517/,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),6-8 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967847,Robot Localization in Floor Plans Using a Room Layout Edge Extraction Network,IEEE,Conferences,"Indoor localization is one of the crucial enablers for deployment of service robots. Although several successful techniques for indoor localization have been proposed, the majority of them relies on maps generated from data gathered with the same sensor modality used for localization. Typically, tedious labor by experts is needed to acquire this data, thus limiting the readiness of the system as well as its ease of installation for inexperienced operators. In this paper, we propose a memory and computationally efficient monocular camera-based localization system that allows a robot to estimate its pose given an architectural floor plan. Our method employs a convolutional neural network to predict room layout edges from a single camera image and estimates the robot pose using a particle filter that matches the extracted edges to the given floor plan. We evaluate our localization system using multiple real-world experiments and demonstrate that it has the robustness and accuracy required for reliable indoor navigation.",https://ieeexplore.ieee.org/document/8967847/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9560893,Robot Navigation in Constrained Pedestrian Environments using Reinforcement Learning,IEEE,Conferences,"Navigating fluently around pedestrians is a necessary capability for mobile robots deployed in human environments, such as buildings and homes. While research on social navigation has focused mainly on the scalability with the number of pedestrians in open spaces, typical indoor environments present the additional challenge of constrained spaces such as corridors and doorways that limit maneuverability and influence patterns of pedestrian interaction. We present an approach based on reinforcement learning (RL) to learn policies capable of dynamic adaptation to the presence of moving pedestrians while navigating between desired locations in constrained environments. The policy network receives guidance from a motion planner that provides waypoints to follow a globally planned trajectory, whereas RL handles the local interactions. We explore a compositional principle for multi-layout training and find that policies trained in a small set of geometrically simple layouts successfully generalize to more complex unseen layouts that exhibit composition of the structural elements available during training. Going beyond walls-world like domains, we show transfer of the learned policy to unseen 3D reconstructions of two real environments. These results support the applicability of the compositional principle to navigation in real-world buildings and indicate promising usage of multi-agent simulation within reconstructed environments for tasks that involve interaction. https://ai.stanford.edu/∼cdarpino/socialnavconstrained/",https://ieeexplore.ieee.org/document/9560893/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2000.845355,Robot improv: using drama to create believable agents,IEEE,Conferences,"Believable agents usually depend upon explicit, model-based simulations of human emotions. This work appeals instead to the sensibilities of dramatic acting to create agents that are believable. The chosen task is that of comedy improvisation as it provides a solid demonstration of the agents' believability in the context of a high-level deliberative goal. Furthermore, this work employs physical robots as the actors, employing the real-time sensor values from the robots as inputs into the acting process. This paper describes the dramatic approach to acting that we used and describes the Java-based implementation on two Nomad Scout robots. Actual, improvised scripts created by the robots are included and analyzed.",https://ieeexplore.ieee.org/document/845355/,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),24-28 April 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561545,Robot in a China Shop: Using Reinforcement Learning for Location-Specific Navigation Behaviour,IEEE,Conferences,"Robots need to be able to work in multiple different environments. Even when performing similar tasks, different behaviour should be deployed to best fit the current environment. In this paper, We propose a new approach to navigation, where it is treated as a multi-task learning problem. This enables the robot to learn to behave differently in visual navigation tasks for different environments while also learning shared expertise across environments. We evaluated our approach in both simulated environments as well as real-world data. Our method allows our system to converge with a 26% reduction in training time, while also increasing accuracy.",https://ieeexplore.ieee.org/document/9561545/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IISA.2017.8316452,Robot painting recognition based on deep belief learning,IEEE,Conferences,"In a society where the number of elderly people is increasing rapidly, autonomous wheelchair robots are expected to be widely used for mobility of elderly people. In this paper we focus on how we can utilize wheelchair robots operating in museums. In this paper, we propose a deep learning based painting recognition and its application for the wheelchair robot. We consider the case when the user clicks on the painting he/she wants to see. The robot searches, recognizes and reaches the painting using deep learning. This is in difference from the most traditional methods where the robot explains the exhibited objects in a sequential order. The deep neural network generates a series of high dimensional features for each painting resulting in a high recognition rate. In our implementation, the wheelchair robot recognizes the painting in real time using the video stream.",https://ieeexplore.ieee.org/document/8316452/,"2017 8th International Conference on Information, Intelligence, Systems & Applications (IISA)",27-30 Aug. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2011.6181679,"Robot self-preservation and adaptation to user preferences in game play, a preliminary study",IEEE,Conferences,"It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presented.",https://ieeexplore.ieee.org/document/6181679/,2011 IEEE International Conference on Robotics and Biomimetics,7-11 Dec. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2016.7743924,Robot-to-human handover with obstacle avoidance via continuous time Recurrent Neural Network,IEEE,Conferences,"Parallel with the development of service robots, it is vital for the robots to carry out handovers autonomously. Robot-to-human handover is a coordination in time and space for a robot to deliver an object to human. A good robot-to-human handover should consider human safety and preference, natural motion planning that mimics human and adaptability to the changes of the environment. Conventional handover motion mostly rely on sampling-based algorithms that emphasizes on kinematic and dynamic analysis. This kind of motion planning could become complicated and slow in response if the handover motion is implemented in a dynamic environment where real time motion planning is required. To simplify the implementation of robot-to-human handover, a motion learning and generation framework that based on Continuous Time Recurrent Neural Network(CTRNN) is proposed. The proposed framework is equipped with the capabilities of object recognition, motion generation based on past learning experience and obstacle adaptation. As compared with conventional method, the proposed framework could be easily extended to handover motion with high dimensional configuration spaces as the motion can be generated from the learnt experience. In the proposed framework, the handover behaviour can be learnt via human-guided motion teaching which provides an intuitive and visible solution for motion planning. The proposed framework has been experimentally evaluated on a customized design robot via robotto-human handover testing. Based on the testing, the feasibility of the proposed framework had been justified.",https://ieeexplore.ieee.org/document/7743924/,2016 IEEE Congress on Evolutionary Computation (CEC),24-29 July 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793720,Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots,IEEE,Conferences,"Co-speech gestures enhance interaction experiences between humans as well as between humans and robots. Most existing robots use rule-based speech-gesture association, but this requires human labor and prior knowledge of experts to be implemented. We present a learning-based co-speech gesture generation that is learned from 52 h of TED talks. The proposed end-to-end neural network model consists of an encoder for speech text understanding and a decoder to generate a sequence of gestures. The model successfully produces various gestures including iconic, metaphoric, deictic, and beat gestures. In a subjective evaluation, participants reported that the gestures were human-like and matched the speech content. We also demonstrate a co-speech gesture with a NAO robot working in real time.",https://ieeexplore.ieee.org/document/8793720/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8462969,Robust Human Following by Deep Bayesian Trajectory Prediction for Home Service Robots,IEEE,Conferences,"The capability of following a person is crucial in service-oriented robots for human assistance and cooperation. Though a vast variety of following systems exist, they lack robustness against dynamic changes of the environment and relocating to continue following a lost target. Here we present a robust human following system that has the extendability to commercial service robot platforms having a RGB-D camera. The proposed framework integrates deep learning methods for perception and variational Bayesian techniques for trajectory prediction. Deep learning modules enable robots to accompany a person by detecting the target, learning the target and following while avoiding collision within the dynamic home environment. The variational Bayesian techniques robustly predict the trajectory of the target by empowering the following ability of the robot when target is lost. We experimentally demonstrate the capability of the deep Bayesian trajectory prediction method on real-time usage, following abilities, collision avoidance and trajectory prediction of the system. The proposed system was deployed at the RoboCup@Home 2017 Social Standard Platform League and successfully demonstrated its robust functions and smooth person following capability resulting in winning the 1st place.",https://ieeexplore.ieee.org/document/8462969/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICAI.2019.8701333,Robust LQR Based ANFIS Control of x-z Inverted Pendulum,IEEE,Conferences,"Inverted pendulum is a highly unstable, nonlinear and an under-actuated system. Its dynamics resembles many real-time systems such as segways, self-balancing robots, vertical take-off and landing aircraft (VTOL) and crane lifting containers etc. These real-time applications demand the need of a robust controller. In literature, many different control strategies have been discussed to stabilize an inverted pendulum, out of them, the most robust being fuzzy control and sliding mode control. The former is difficult to tune and has a problem of rule explosion for multivariable system, whereas the latter has a problem of discontinuity and chattering. To address the issues in fuzzy controller, a novel robust linear quadratic regulator (LQR) based adaptive-network fuzzy inference system (ANFIS) controller is proposed and implemented on the stabilization of x-z inverted pendulum. The proposed controller is able to solve the problem of robustness in the LQR controller as well as the difficulty in tuning along with rule explosion in fuzzy controller. Furthermore, the designed controller is tested for different pendulum masses and the results show that as compared with conventional PID controller, the proposed controller gives better performance in achieving lesser overshoot and settling time along with better robustness properties.",https://ieeexplore.ieee.org/document/8701333/,2019 Amity International Conference on Artificial Intelligence (AICAI),4-6 Feb. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594067,Robust Object Recognition Through Symbiotic Deep Learning In Mobile Robots,IEEE,Conferences,"Despite the recent success of state-of-the-art deep learning algorithms in object recognition, when these are deployed as-is on a mobile service robot, we observed that they failed to recognize many objects in real human environments. In this paper, we introduce a learning algorithm in which robots address this flaw by asking humans for help, also known as a symbiotic autonomy approach. In particular, we bootstrap YOLOv2, a state-of-the-art deep neural network and train a new neural network, that we call HHELP, using only data collected from human help. Using an RGB camera and an onboard tablet, the robot proactively seeks human input to assist it in labeling surrounding objects. Pepper, located at CMU, and Monarch Mbot, located at ISR-Lisbon, were the service robots that we used to validate the proposed approach. We conducted a study in a realistic domestic environment over the course of 20 days with 6 research participants. To improve object detection, we used the two neural networks, YOLOv2 + HHELP, in parallel. Following this methodology, the robot was able to detect twice the number of objects compared to the initial YOLOv2 neural network, and achieved a higher mAP (mean Average Precision) score. Using the learning algorithm the robot also collected data about where an object was located and to whom it belonged to by asking humans. This enabled us to explore a future use case where robots can search for a specific person's object. We view the contribution of this work to be relevant for service robots in general, in addition to Pepper, and Mbot.",https://ieeexplore.ieee.org/document/8594067/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII.2011.6147636,Robust localization system using online / offline hybrid learning,IEEE,Conferences,"In this paper, we propose an online motion model parameter estimation method. To achieve accurate localization, accurate estimation of motion model parameters is needed. However, the true values of motion model parameters change sequentially according to alteration of surrounding environments. Therefore the online estimation is absolutely imperative. As a typical method to estimate motion model parameters sequentially, Augmented Kalman Filter (AKF) is there. AKF achieves parameter estimation through Kalman filtering algorithm. However, AKF has serious problems to be implemented in real robot operation. These problems are the accuracy of observation and the limitation to motion control of robots. To solve these problems and achieve accurate motion model parameter estimation, proposed method introduces discriminative training. The introduction of discriminative training increases the convergence performance and stability of parameter estimation through AKF. The proposal method achieves accurate motion model parameter estimation in real robot operation. This paper describes the efficiency of our technique through simulations and an outdoor experiment.",https://ieeexplore.ieee.org/document/6147636/,2011 IEEE/SICE International Symposium on System Integration (SII),20-22 Dec. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8206604,Robust real-time visual tracking using dual-frame deep comparison network integrated with correlation filters,IEEE,Conferences,"In recent years, applications of visual tracking algorithms has seen a substantial growth with deployments in intelligent robots such as drones for human tracking. The algorithms for such tasks has to be efficient in terms of computational cost while been robust, accurate and fast. Object tracking algorithms based on handcrafted heuristics and constraints are widely used in uav applications. The handcrafted heuristics are mostly implemented for task-oriented applications which limits the extensions in uav's capability beyond the predefined functions. This paper considers the challenges of tracking and landing an autonomous uav on a speed high moving target, and presents a visual tracking algorithm that integrates correlation filters with deep comparison network for real-time tracking with state-of-the-art accuracy. The method first tracks the target upto translation using an online learnt model via local search technique. The changes in scale is estimated by a deep comparison network (DCN) instead of the commonly used pyramidal approach. In a single network evaluation, DCN can estimate the changes in scale as well as compensate the drifting of the tracker by refining the object region estimated by the correlation filters. The network is end-to-end trained which attempts to learn a powerful matching function for object localization using a known template. Generally, the integrated framework can be viewed as coarse-to-fine level motion estimation. Moreover, the framework can redetect the lost target without a need for a separate detector.",https://ieeexplore.ieee.org/document/8206604/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EURCON.2007.4400663,Role Selection Mechanism for the Soccer Robot System using Petri Net,IEEE,Conferences,"Robot soccer is a challenging platform for multi-agent research, involving topics such as real-time image processing and control, robot path planning, obstacle avoidance and machine learning. The system consists of a supervisory controller, and controllers for defending and goalkeeping robots. These controllers are designed using Petri net. The robot soccer game presents an uncertain and dynamic environment for cooperating agents. Dynamic role switching and formation control are crucial for a successful game. A soccer robot has to take an appropriate decision based on environment situation. With the role of a robot fixed as goalkeeper, the supervisor, according to the game situation, assigns the role of attacking or defending to the other robots and then respective controllers control the robots. The Petri net model is implemented in Petri net toolbox under MATLAB environment.",https://ieeexplore.ieee.org/document/4400663/,"EUROCON 2007 - The International Conference on ""Computer as a Tool""",9-12 Sept. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2013.86,RvGIST: A Holistic Road Feature for Real-Time Road-Scene Understanding,IEEE,Conferences,"Image-based road scene understanding is a critical issue for intelligent vehicles and autonomous mobile robots. It is challenging to deal with varying road conditions in a dynamic environment in real time. This paper presents an effective while simple approach to classify road types and locate the road-related elements through the analysis of a holistic visual road feature. The feature is abstracted from responses of Gabor-filter-set and grouped into super-pixel grids, consisting of road-scene textural context and dominant orientation distribution. From this feature we successively deduce the information of horizon line, road type and coarse locations of road surface, lanes, on-road obstacles and off-road regions. Experiments show that the proposed analyzing method based on the new holistic feature is beneficial to road estimation and vehicle detection in complex road scenes, achieving improvements in both accuracy and efficiency.",https://ieeexplore.ieee.org/document/6598534/,"2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",1-3 July 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSRR.2019.8848957,Sample Efficient Reinforcement Learning for Navigation in Complex Environments,IEEE,Conferences,"Navigation of mobile robots in unstructured, time-varying environments is challenging. It becomes even more complicated in disaster scenarios where logistical difficulties, as well as technical issues such as reactive and time-varying obstacles, exist. These scenarios are too complex for classical obstacle avoidance methods to navigate through successfully. This paper presents a sample efficient reinforcement learning algorithm for navigation in complex environments. The approach augments training data with randomly generated target location data to accelerate learning. A Q-learning approach is implemented, which is capable of quick training with limited episodes. The procedure is tested in four scenarios in Gazebo and one scenario in a real-world experiment. In the two simulation scenarios with no obstacles, the method can learn to navigate towards the target in fewer than 200 episodes. For environments with moving obstacles, training takes slightly longer, but the process is still able to learn an effective policy quickly.",https://ieeexplore.ieee.org/document/8848957/,"2019 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)",2-4 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RoboSoft48309.2020.9116004,Scalable sim-to-real transfer of soft robot designs,IEEE,Conferences,"The manual design of soft robots and their controllers is notoriously challenging, but it could be augmented-or, in some cases, entirely replaced-by automated design tools. Machine learning algorithms can automatically propose, test, and refine designs in simulation, and the most promising ones can then be manufactured in reality (sim2real). However, it is currently not known how to guarantee that behavior generated in simulation can be preserved when deployed in reality. Although many previous studies have devised training protocols that facilitate sim2real transfer of control polices, little to no work has investigated the simulation-reality gap as a function of morphology. This is due in part to an overall lack of tools capable of systematically designing and rapidly manufacturing robots. Here we introduce a low cost, open source, and modular soft robot design and construction kit, and use it to simulate, fabricate, and measure the simulation-reality gap of minimally complex yet soft, locomoting machines. We prove the scalability of this approach by transferring an order of magnitude more robot designs from simulation to reality than any other method. The kit and its instructions can be found here: github.com/skriegman/sim2real4designs.",https://ieeexplore.ieee.org/document/9116004/,2020 3rd IEEE International Conference on Soft Robotics (RoboSoft),15 May-15 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCIOT45285.2018.9032441,Segmental Deployment of Neural Network in Cloud Robotic System,IEEE,Conferences,"In this paper, we describe a new method for ep neural networks in the field of computer vision, which can effectively solve the difficulty of applying deep learning in the cloud robotic system. By segmenting the trained network, most of the computing tasks can be cut out and offloaded to the cloud. By effective feature extraction and compression methods, the computing power of robot and cloud can be integrated and coordinated. A method of selecting the split points of the network model and a method of data transmission and compression in the communication between robots and cloud after segmenting are given based on the characteristics of machine vision tasks, and the theoretical analysis is carried out. In the experiment, the effectiveness of all the above methods is verified by comparing the compression capability, response time and network performance of the actual network model. The experimental results show that with the use of segmental methods in cloud robotic system, the task of deep network is processed in real time, while the performance is almost guaranteed.",https://ieeexplore.ieee.org/document/9032441/,2018 IEEE 3rd International Conference on Cloud Computing and Internet of Things (CCIOT),20-21 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCAI53970.2022.9752493,Segmentation of Lidar Point Cloud Data using SOM,IEEE,Conferences,"Sensors play an important role in detecting and perceiving the environment. Many applications of designing a system to understand the environment semantically include vision for navigation, reverse engineering, Simultaneous Localization and Mapping (SLAM), modelling, autonomous vehicles, change detection, and autonomous robots. Accuracy concern, the Lidar sensor is now used in stunning systematic investigations. Lidar sensors generate data in various file formats such as LAS and point cloud data from the sensed environment. The ability to form simple various applications with error-free environment perception from the Lidar point cloud is critical. Navigation of an autonomous vehicle without colliding with obstacles and locating the vehicle on its own may be a difficult task, which is commonly referred to as SLAM. Creating an accurate map for local features in SLAM problems can be a time-consuming process. Many studies are concerned with the creation of a local map for a structured environment. This study will look into the implementation of a self-organized map for an unknown environment. The self-organized map (SOM) learns about the environment and acquires semantic knowledge from a local expert. This is taken as an input and further segments the region, especially unstructured environments like agricultural land, nonindustrial environments. Heretofore, the developed works for clustering and segmentation only for the roadside habitat and industrial purpose. In this paper, we implemented the real-time SOM to perceive any world environment regardless of illumination and is experimented. Implementation outputs are analysed for further rectification.",https://ieeexplore.ieee.org/document/9752493/,"2022 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)",28-29 Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8460655,Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation,IEEE,Conferences,"Enabling robots to autonomously navigate complex environments is essential for real-world deployment. Prior methods approach this problem by having the robot maintain an internal map of the world, and then use a localization and planning method to navigate through the internal map. However, these approaches often include a variety of assumptions, are computationally intensive, and do not learn from failures. In contrast, learning-based methods improve as the robot acts in the environment, but are difficult to deploy in the real-world due to their high sample complexity. To address the need to learn complex policies with few samples, we propose a generalized computation graph that subsumes value-based model-free methods and model-based methods, with specific instantiations interpolating between model-free and model-based. We then instantiate this graph to form a navigation model that learns from raw images and is sample efficient. Our simulated car experiments explore the design decisions of our navigation model, and show our approach outperforms single-step and N-step double Q-learning. We also evaluate our approach on a real-world RC car and show it can learn to navigate through a complex indoor environment with a few hours of fully autonomous, self-supervised training. Videos of the experiments and code can be found at github.com/gkahn13/gcg.",https://ieeexplore.ieee.org/document/8460655/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2000.844830,Self-learning vision-guided robots for searching and grasping objects,IEEE,Conferences,"An approach to control vision-guided robots is introduced. It allows searching and grasping differently shaped objects that may be located anywhere in the robot's work space, even not visible in the initial fields of view of cameras. It eliminates the need for a calibration of the robot and of the vision system, it uses no world coordinates and no inverse perspective or kinematic transformations, and it comprises an automatic adaptation to changing parameters. The approach has been implemented on a calibration-free vision-guided manipulator with five degrees of freedom (DOF) and was evaluated in real-word experiments.",https://ieeexplore.ieee.org/document/844830/,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),24-28 April 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2002.1014331,Self-organized flocking with agent failure: Off-line optimization and demonstration with real robots,IEEE,Conferences,"This paper presents an investigation of flocking by teams of autonomous mobile robots using principles of Swarm Intelligence. First, we present a simple flocking task, and we describe a leaderless distributed flocking algorithm (LD) that is more conducive to implementation on embodied agents than the established algorithms used in computer animation. Next, we use an embodied simulator and reinforcement learning techniques to optimize LD performance under different conditions, showing that this method can be used not only to improve performance but also to gain insight into which algorithm components contribute most to system behavior. Finally, we demonstrate that a group of real robots executing LD with emulated sensors can successfully flock (even in the presence of individual agent failure) and that systematic characterization (and therefore optimization) of real robot flocking performance is achievable.",https://ieeexplore.ieee.org/document/1014331/,Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292),11-15 May 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2008.4600739,Semantic category acquisition in dialogue for interactive object learning,IEEE,Conferences,"An important aspect of humanoid robots in a natural environment is the ability to acquire new knowledge through learning mechanisms, which enhances an artificial system with the ability to adapt to a changing or new environment. In contrast to most learning algorithms applied in machine learning today, which mainly work with offline learning on training samples, such learning mechanisms need to be performed autonomously and through interaction with the environment or with other agents/humans. In this paper we describe a learning algorithm as a dialogue approach for learning semantic categories and object description in object learning. New objects are introduced to the robot and learning dialogues are conducted as a means of information acquisition. In dialogue, the robot can acquire semantic categories, type and properties of objects, learn new words for object descriptions and learn and association to visual identification from object recognition. In contrast to existing work, this approach combines recognition of real objects, new words learning and semantic categories in one learning dialogue. The presented approach has been implemented in a dialogue system and evaluated on the humanoid robot Armar III.",https://ieeexplore.ieee.org/document/4600739/,RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication,1-3 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS48674.2020.9214063,Semantic situation awareness of ellipse shapes via deep learning for multirotor aerial robots with a 2D LIDAR,IEEE,Conferences,"In this work, we present a semantic situation awareness system for multirotor aerial robots equipped with a 2D LIDAR sensor, focusing on the understanding of the environment, provided to have a drift-free precise localization of the robot (e.g. given by GNSS/INS or motion capture system). Our algorithm generates in real-time a semantic map of the objects of the environment as a list of ellipses represented by their radii, and their pose and velocity, both in world coordinates. Two different Convolutional Neural Network (CNN) architectures are proposed and trained using an artificially generated dataset and a custom loss function, to detect ellipses in a segmented (i.e. with one single object) LIDAR measurement. In cascade, a specifically designed indirect-EKF estimates the ellipses based semantic map in world coordinates, as well as their velocity. We have quantitative and qualitatively evaluated the performance of our proposed situation awareness system. Two sets of Software-In-The-Loop simulations using CoppeliaSim with one and multiple static and moving cylindrical objects are used to evaluate the accuracy and performance of our algorithm. In addition, we have demonstrated the robustness of our proposed algorithm when handling real environments thanks to real laboratory experiments with non-cylindrical static (i.e. a barrel) objects and moving persons.",https://ieeexplore.ieee.org/document/9214063/,2020 International Conference on Unmanned Aircraft Systems (ICUAS),1-4 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759769,Sensor substitution for video-based action recognition,IEEE,Conferences,"There are many applications where domain-specific sensing, such as accelerometers, kinematics, or force sensing, provide unique and important information for control or for analysis of motion. However, it is not always the case that these sensors can be deployed or accessed beyond laboratory environments. For example, it is possible to instrument humans or robots to measure motion in the laboratory in ways that it is not possible to replicate in the wild. An alternative, which we explore in this paper, is to address situations where accurate sensing is available while training an algorithm, but for which only video is available for deployment. We present two examples of this sensory substitution methodology. The first variation trains a convolutional neural network to regress real-valued signals, including robot end-effector pose, from video. The second example regresses binary signals derived from accelerometer data which signifies when specific objects are in motion. We evaluate these on the JIGSAWS dataset for robotic surgery training assessment and the 50 Salads dataset for modeling complex structured cooking tasks. We evaluate the trained models for video-based action recognition and show that the trained models provide information that is comparable to the sensory signals they replace.",https://ieeexplore.ieee.org/document/7759769/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.01165,Sim-Real Joint Reinforcement Transfer for 3D Indoor Navigation,IEEE,Conferences,"There has been an increasing interest in 3D indoor navigation, where a robot in an environment moves to a target according to an instruction. To deploy a robot for navigation in the physical world, lots of training data is required to learn an effective policy. It is quite labour intensive to obtain sufficient real environment data for training robots while synthetic data is much easier to construct by render-ing. Though it is promising to utilize the synthetic environments to facilitate navigation training in the real world, real environment are heterogeneous from synthetic environment in two aspects. First, the visual representation of the two environments have significant variances. Second, the houseplans of these two environments are quite different. There-fore two types of information,i.e. visual representation and policy behavior, need to be adapted in the reinforce mentmodel. The learning procedure of visual representation and that of policy behavior are presumably reciprocal. We pro-pose to jointly adapt visual representation and policy behavior to leverage the mutual impacts of environment and policy. Specifically, our method employs an adversarial feature adaptation model for visual representation transfer anda policy mimic strategy for policy behavior imitation. Experiment shows that our method outperforms the baseline by 19.47% without any additional human annotations.",https://ieeexplore.ieee.org/document/8953924/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2000.859462,"Simulating the evolution of 2D pattern recognition on the CAM-Brain Machine, an evolvable hardware tool for building a 75 million neuron artificial brain",IEEE,Conferences,"This paper presents some simulation results of the evolution of 2D visual pattern recognizers to be implemented very shortly on real hardware, namely the ""CAM-Brain Machine"" (CBM), an FPGA based piece of evolvable hardware which implements a genetic algorithm (GA) to evolve a 3D cellular automata (CA) based neural network circuit module, of approximately 1,000 neurons, in about a second, i.e. a complete run of a GA, with tens of thousands of circuit growths and performance evaluations. Up to 65,000 of these modules, each of which is evolved with a humanly specified function, can be downloaded into a large RAM space, and interconnected according to humanly specified artificial brain architectures. This RAM, containing an artificial brain with up to 75 million neurons, is then updated by the CBM at a rate of 130 billion CA cells per second. Such speeds will enable real time control of robots and hopefully the birth of a new research field that we call ""brain building"". The first such artificial brain, to be built at STARLAB in 2000 and beyond, will be used to control the behaviors of a life sized kitten robot called ""Robokitty"". This kitten robot will need 2D pattern recognizers in the visual section of its artificial brain. This paper presents simulation results on the evolvability and generalization properties of such recognizers.",https://ieeexplore.ieee.org/document/859462/,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,27-27 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC.2008.4597309,Simulation of robot localization based on virtual sensors,IEEE,Conferences,"A flexible simulation frame based on concept of component is presented. An indoor robots localization simulation environment based on virtual sensors RoboSimer is built with OpenGL. The parameters here are easily adjusted and controlled by customs and the simulation module is easy to integrate. It can integrate any sensors, environment and robot shape into the simulation software. The interfaces of simulation software are coincided with the real hardware platform. It provides a convenient condition for the further study on robot localization.",https://ieeexplore.ieee.org/document/4597309/,2008 Chinese Control and Decision Conference,2-4 July 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593518,Simultaneous End-User Programming of Goals and Actions for Robotic Shelf Organization,IEEE,Conferences,"Arrangement of items on shelves in stores or warehouses is a tedious, repetitive task that can be feasible for robots to perform. The diversity of products that are available in stores and the different setups and preferences of each store makes pre-programming a robot for this task extremely challenging. Instead, our work argues for enabling end-users to customize the robot to their specific objects and setup at deployment time by programming it themselves. To that end, this paper contributes (i) a task representation for shelf arrangements based on a large dataset of grocery store shelf images, (ii) a method for inferring goal configurations from user inputs including demonstrations and direct parameter specifications, and (iii) a system implementation of the proposed approach that allows simultaneously learning task goals and actions. We evaluate our goal inference approach with ten different teaching strategies that combine alternative user inputs in different ways on the large dataset of grocery configurations, as well as with real human teachers through an online user study (N=32). We evaluate our full system implemented on a Fetch mobile manipulator on eight benchmark tasks that demonstrate end-to-end programming and execution of shelf arrangement tasks.",https://ieeexplore.ieee.org/document/8593518/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636012,Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation,IEEE,Conferences,"Grasping in cluttered scenes has always been a great challenge for robots, due to the requirement of the ability to well understand the scene and object information. Previous works usually assume that the geometry information of the objects is available, or utilize a step-wise, multi-stage strategy to predict the feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF grasp pose estimation as a simultaneous multi-task learning problem. In a unified framework, we jointly predict the feasible 6-DoF grasp poses, instance semantic segmentation, and collision information. The whole framework is jointly optimized and end-to-end differentiable. Our model is evaluated on large-scale benchmarks as well as the real robot system. On the public dataset, our method outperforms prior state-of-the-art methods by a large margin (+4.08 AP). We also demonstrate the implementation of our model on a real robotic platform and show that the robot can accurately grasp target objects in cluttered scenarios with a high success rate. Project link: https://openbyterobotics.github.io/sscl.",https://ieeexplore.ieee.org/document/9636012/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICHR.2010.5686285,SkyAI: Highly modularized reinforcement learning library,IEEE,Conferences,"This paper introduces a software library of reinforcement learning (RL) methods, named SkyAI. SkyAI is a highly modularized RL library for real/simulated robots to learn behaviors. Our ultimate goal is to develop an artificial intelligence (AI) program with which the robots can learn to behave as their users' wish. In this paper, we describe the concepts, the requirements, and the current implementation of SkyAI. SkyAI provides two conflicting features: high execution-speed enough for real robot systems and high flexibility to design learning systems. We also demonstrate the applications to crawling tasks of both a humanoid robot in simulation and a real spider robot.",https://ieeexplore.ieee.org/document/5686285/,2010 10th IEEE-RAS International Conference on Humanoid Robots,6-8 Dec. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigComp51126.2021.00051,Smart Energy Management System based on Reconfigurable AI Chip and Electrical Vehicles,IEEE,Conferences,"Almost every larger city in Europe has ambitious smart city projects. This is particularly true for Hamburg, a Hanseatic city in the north of Germany. Hamburg is the smartest city in Germany according to a Federal Association for Information Technology. Although there are no megacities in the European Union (the largest city in the European Union is Berlin with 3.7 million inhabitants), the increasing urbanization is apparent and produces problems to be solved. At the same time rural depopulation creates conjugated problems.One category of these problems is mobility. Mobility can be regarded as the need to move persons and freight. In densely populated cities an increasing amount of transport users have to share a decreasing amount of space with conflicting needs. At the same time in rural areas, a dwindling supply of local public transport makes the mobility of the remaining residents more difficult. The same applies to parcel delivery or the supply of goods. Autonomous systems have great potential to create a sustainable and livable environment. The author has initiated a publicly funded project to investigate technologies of autonomous mobile systems which interact with a smart city. The test area intelligent urban mobility (Testfeld intelligente Quartiersmobilitat) at the campus of Hamburgs University of Applied Sciences is created to do research on connected and autonomous mobile systems like multipurpose robots and other mobility users like pedestrians with a smartphone. A particular focus is on neighborhood mobility. This means that distances of less than 3 kilometers usually have to be covered. The special type of needs in neighborhood mobility has two important aspects that affect development of autonomous mobile systems: It is slow mobility and the transport users are especially vulnerable. The acceptance of the residents of autonomous systems is equally important, as is the protection of privacy when collecting environmental data. They are expected to make decisions on their own in complex environments. The real world usually differs from a simulation or an experimental setup in a laboratory - a problem commonly referred to as Sim-2-Real gap. Active and non-destructive exploration is expected from an autonomous system to solve unexpected problems. Machine learning methods come into play which in turn have their own pitfalls. The author has built a specialized laboratory to investigate machine learning technology applied to autonomous systems. In this laboratory miniature autonomous vehicles are developed. The general idea of this experimental setup allows research on new methodologies for autonomous systems in a very small scale",https://ieeexplore.ieee.org/document/9373129/,2021 IEEE International Conference on Big Data and Smart Computing (BigComp),17-20 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8460968,Socially Compliant Navigation Through Raw Depth Inputs with Generative Adversarial Imitation Learning,IEEE,Conferences,"We present an approach for mobile robots to learn to navigate in dynamic environments with pedestrians via raw depth inputs, in a socially compliant manner. To achieve this, we adopt a generative adversarial imitation learning (GAIL) strategy, which improves upon a pre-trained behavior cloning policy. Our approach overcomes the disadvantages of previous methods, as they heavily depend on the full knowledge of the location and velocity information of nearby pedestrians, which not only requires specific sensors, but also the extraction of such state information from raw sensory input could consume much computation time. In this paper, our proposed GAIL-based model performs directly on raw depth inputs and plans in real-time. Experiments show that our GAIL-based approach greatly improves the safety and efficiency of the behavior of mobile robots from pure behavior cloning. The real-world deployment also shows that our method is capable of guiding autonomous vehicles to navigate in a socially compliant manner directly through raw depth inputs. In addition, we release a simulation plugin for modeling pedestrian behaviors based on the social force model.",https://ieeexplore.ieee.org/document/8460968/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341328,Software Development Framework for Cooperating Robots with High-level Mission Specification,IEEE,Conferences,"In recent years, there has been a growing interest in multiple robots performing a single task through different types of collaboration. There are two software challenges when deploying collaborative robots: how to specify a cooperative mission and how to program each robot to accomplish its mission. In this paper, we propose a novel software development framework to support distributed robot systems, swarm robots, and their hybrid. We extend the service-oriented and model-based (SeMo) framework [1] to improve the robustness, scalability, and flexibility of robot collaboration. To enable a casual user to specify various types of cooperative missions easily, the high-level mission scripting language is extended with new features such as team hierarchy, group service, one-to-many communication. The script program is refined to the robot codes through two intermediate steps, strategy description and task graph generation, in the proposed framework. The viability of the proposed framework is evidenced by two preliminary experiments using real robots and a robot simulator.",https://ieeexplore.ieee.org/document/9341328/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1996.570655,Specification and validation of a control architecture for autonomous mobile robots,IEEE,Conferences,"We describe the specification of a software control architecture for autonomous mobile robots. The architecture, designed to provide the robot (in a task-dependent context) with the capacity to react to events but also to intelligently anticipate the future and plan its actions, is based on the decomposition of the robot system into a functional and a decisional level. The article is mainly focused on some aspects of the organisation and of the operation of the system such as execution control, inter-levels communication, reactivity. An important aspect that is developed is the possibility to prove some temporal and logical properties of parts of the system.",https://ieeexplore.ieee.org/document/570655/,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS '96,8-8 Nov. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR55393.2022.9826285,Stereoscopic low-latency vision system via ethernet network for Humanoid Teleoperation,IEEE,Conferences,"Despite the recent rise of artificial intelligence, the automatic operation of the robot in an unstructured environment is still insufficient to perform complicated and detailed tasks and requires remote operation by a human. The most important thing to improve the efficiency of such Teleoperation is the vision system that enables the human operator to recognize the surrounding situation of the robot. While constructing the system, immersive experience for the operator was considered. Low latency and curved plane rendering are proposed to provide an immersive experience for operators. Moreover, considering the operator&#x2019;s exhaustion during the teleoperation that comes from Virtual Reality sickness, the reduction of Virtual Reality sickness is also suggested. Finally, to check the entire system&#x2019;s visual feedback latency, display-to-display latency is quantitatively measured. This paper presents an implementation of a vision system for the Teleoperation of humanoid robots through Virtual Reality.",https://ieeexplore.ieee.org/document/9826285/,2022 19th International Conference on Ubiquitous Robots (UR),4-6 July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISIM.2008.21,Strategy Description and Modelling for Multi-Agent Systems,IEEE,Conferences,"The field of robot soccer provides numerous opportunities for the application of AI methods for game strategy development. Robot soccer is a part of standard applications of distributed system control in real time. The software part of a distributed control system is realized by decision making and executive agents. The algorithm of agents' cooperation was proposed with the control agent on a higher level. The algorithms for agents realized in robots are the same. Real-time dynamic simple strategy description and strategy learning possibility based on game observation is important for discovering opponent's strategies and searching for tactical group movements, simulation and synthesis of suitable counter-strategies. For the improvement of game strategy, we are developing an abstract description of the game and propose ways to use this description (e.g. for learning rules and adapting team strategies to every single opponent).",https://ieeexplore.ieee.org/document/4557834/,2008 7th Computer Information Systems and Industrial Management Applications,26-28 June 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UKSIM.2008.117,Strategy Description and Modelling for Multi-agent Systems (Invited Paper),IEEE,Conferences,"The field of robot soccer provides numerous opportunities for the application of AI methods for game strategy development. Robot soccer is a part of standard applications of distributed system control in real time. The software part of a distributed control system is realized by decision making and executive agents. The algorithm of agents’ cooperation was proposed with the control agent on a higher level. The algorithms for agents realized in robots are the same. Real-time dynamic simple strategy description and strategy learning possibility based on game observation is important for discovering opponents strategies and searching for tactical group movements, simulation and synthesis of suitable counter-strategies. For the improvement of game strategy, we are developing an abstract description of the game and propose ways to use this description (e.g. for learning rules and adapting team strategies to every single opponent).",https://ieeexplore.ieee.org/document/4489006/,Tenth International Conference on Computer Modeling and Simulation (uksim 2008),1-3 April 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SACI.2016.7507375,Superior vision: Always on human-like vision for intelligent devices,IEEE,Conferences,"Summary form only given. We live in a world of intelligent robots, drones and mobile phones/assistants. The technologies behind these products are heralded by a new wave of deep learning applications ported on mobile devices. The robotic and vision technologies behind these products will shift the applications of electronic devices to a more superior level of intelligence that will change our world. From a hardware point of view, these devices will become a hub of learning sensors that will be able to capture and learn from their surrounding environment and their context. On top of the aforementioned deep learning technologies, the most important sense for these devices will be their vision capabilities. To make this sense as close as possible to human vision, will require advanced technology to capture, process, understand and provide analytics in real time to enable their applications make critical “educated: decisions”. “Always on” vision capabilities are envisaged as the paradigm for the next generation of mobile This Keynote will cover in this context the state of the art in the domain of vision with the latest image processing, computer vision and deep learning algorithms implemented on various low-power processing architectures.",https://ieeexplore.ieee.org/document/7507375/,2016 IEEE 11th International Symposium on Applied Computational Intelligence and Informatics (SACI),12-14 May 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635867,SurRoL: An Open-source Reinforcement Learning Centered and dVRK Compatible Platform for Surgical Robot Learning,IEEE,Conferences,"Autonomous surgical execution relieves tedious routines and surgeon&#x2019;s fatigue. Recent learning-based methods, especially reinforcement learning (RL) based methods, achieve promising performance for dexterous manipulation, which usually requires the simulation to collect data efficiently and reduce the hardware cost. The existing learning-based simulation platforms for medical robots suffer from limited scenarios and simplified physical interactions, which degrades the real-world performance of learned policies. In this work, we designed SurRoL, an RL-centered simulation platform for surgical robot learning compatible with the da Vinci Research Kit (dVRK). The designed SurRoL integrates a user-friendly RL library for algorithm development and a real-time physics engine, which is able to support more PSM/ECM scenarios and more realistic physical interactions. Ten learning-based surgical tasks are built in the platform, which are common in the real autonomous surgical execution. We evaluate SurRoL using RL algorithms in simulation, provide in-depth analysis, deploy the trained policies on the real dVRK, and show that our SurRoL achieves better transferability in the real world.",https://ieeexplore.ieee.org/document/9635867/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF49454.2021.9382607,Teaching System for Multimodal Object Categorization by Human-Robot Interaction in Mixed Reality,IEEE,Conferences,"As service robots are becoming essential to support aging societies, teaching them how to perform general service tasks is still a major challenge preventing their deployment in daily-life environments. In addition, developing an artificial intelligence for general service tasks requires bottom-up, unsupervised approaches to let the robots learn from their own observations and interactions with the users. However, compared to the top-down, supervised approaches such as deep learning where the extent of the learning is directly related to the amount and variety of the pre-existing data provided to the robots, and thus relatively easy to understand from a human perspective, the learning status in bottom-up approaches is by their nature much harder to appreciate and visualize. To address these issues, we propose a teaching system for multimodal object categorization by human-robot interaction through Mixed Reality (MR) visualization. In particular, our proposed system enables a user to monitor and intervene in the robot&#x2019;s object categorization process based on Multimodal Latent Dirichlet Allocation (MLDA) to solve unexpected results and accelerate the learning. Our contribution is twofold by 1) describing the integration of a service robot, MR interactions, and MLDA object categorization in a unified system, and 2) proposing an MR user interface to teach robots through intuitive visualization and interactions.",https://ieeexplore.ieee.org/document/9382607/,2021 IEEE/SICE International Symposium on System Integration (SII),11-14 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2013.6696802,Teaching mobile robots to cooperatively navigate in populated environments,IEEE,Conferences,"Mobile service robots are envisioned to operate in environments that are populated by humans and therefore ought to navigate in a socially compliant way. Since the desired behavior of the robots highly depends on the application, we need flexible means for teaching a robot a certain navigation policy. We present an approach that allows a mobile robot to learn how to navigate in the presence of humans while it is being teleoperated in its designated environment. Our method applies feature-based maximum entropy learning to derive a navigation policy from the interactions with the humans. The resulting policy maintains a probability distribution over the trajectories of all the agents that allows the robot to cooperatively avoid collisions with humans. In particular, our method reasons about multiple homotopy classes of the agents' trajectories, i. e., on which sides the agents pass each other. We implemented our approach on a real mobile robot and demonstrate that it is able to successfully navigate in an office environment in the presence of humans relying only on on-board sensors.",https://ieeexplore.ieee.org/document/6696802/,2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,3-7 Nov. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPDWinter52325.2021.00048,Technology-driven Service Innovation in University Libraries,IEEE,Conferences,"Libraries are building a digital environment and preparing for the post-corona era. They attempt to increase operational efficiency and competitiveness by applying new technologies and strive to build a more intelligent and user-friendly environment using big data, the Internet of things (IoT), artificial intelligence (AI), virtual reality (VR), 3D printing, and automated robots. This study aims to present the direction for future university libraries by analyzing technology-based service innovation cases in line with the current era, centering on university libraries",https://ieeexplore.ieee.org/document/9403519/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISCAE48440.2019.221655,The Construction of Portrait Identification Tracking System Based on Mask R-CNN,IEEE,Conferences,"A portrait identification and tracking system with strong real-time performance, good flexibility and controllable cost is designed and implemented in the paper. Firstly, Mask R-CNN neural network is used to extract the features of the target, and the COCO dataset is used to train and establish the portrait data model. As a result, accuracy of the portrait recognition is improved. Then, a portrait tracking system including monocular camera, data acquisition module, data processing module, steering gear and control system is built. And the ""Raspberry Pi"" control method is used to control the MG955 steering gear group. Finally, the recognition and tracking of characters can be realized through wired, WIFI, Bluetooth and other ways, which improves the universality of the system. The designed system has simple structure, complete functions and can be used for automatic aiming and tracking of other objects. The modular system can also be used for unmanned aerial vehicles, robots and other platforms.",https://ieeexplore.ieee.org/document/9075573/,2019 2nd International Conference on Information Systems and Computer Aided Education (ICISCAE),28-30 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRC52766.2021.9620166,The Development of an Omnidirectional Mobile Robot Based on Hub Motor,IEEE,Conferences,"To enable the ability of moving in a narrow space, the robots are required to move in all directions. However, traditional robots with omnidirectional mobile function are easily wearing, with poor bearing capacity, and with complex structure. We designed and proposed an omnidirectional mobile robot (OMR) with the active split offset caster (ASoC) wheelset as the driving wheelset. Specifically, we first established the mathematical model of the robot based on the differential drive principle. Then the mathematical model is verified with synchronized real and simulated movement of the designed robot. A motion capture system is used to track the actual motion trajectory. Finally, a road test experiment is applied to test the stability of the designed robot in a real environment. The tiny differences between real and simulated trajectory show that the designed robot has the ability to move omnidirectionally in a narrow space. The road test experiment proves the advanced adaptability in real environment. Therefore, the designed robot can assist in a narrow space.",https://ieeexplore.ieee.org/document/9620166/,"2021 6th International Conference on Control, Robotics and Cybernetics (CRC)",9-11 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDFS55398.2022.9800837,The Necessity of Emotion Recognition from Speech Signals for Natural and Effective Human-Robot Interaction in Society 5.0,IEEE,Conferences,"The history of humanity has reached Industry 4.0 that aims to the integration of information technologies and especially artificial intelligence with all life-sustaining mechanisms in the 21st century, and consecutively, the transformation of Society 5.0 has begun. Society 5.0 means a smart society in which humans share life with physical robots and software robots as well as smart devices based on augmented reality. Industry 4.0 contains main structures such as the internet of things, big data analytics, digital transformation, cyber-physical systems, artificial intelligence, and business processes optimization. It is impossible to consider the machines to be without emotions and emotional intelligence within the transformation of smart tools and artificial intelligence, in addition, while it is planned to give most of the commands with voice and speaking, it became more important to develop algorithms that can detect emotions. In the smart society, new and rapid methods are needed for speech recognition, emotion recognition, and speech emotion recognition areas to maximize human-computer (HCI) or human-robot interaction (HRI) and collaboration. In this study, speech recognition and speech emotion recognition studies in robot technology are investigated and developments are revealed.",https://ieeexplore.ieee.org/document/9800837/,2022 10th International Symposium on Digital Forensics and Security (ISDFS),6-7 June 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2002.1174408,The approach of extracting features from the local environment for mobile robot,IEEE,Conferences,"A new data fusion method to extract features from the local environment for a mobile robot's navigation has been developed and implemented. This method, named the obstacle group, compresses data in a series of levels in order to reduce the quantity of data for communication between modules in a distributed single-robot system, or between all the robots and the central station in a multi-robot system. The method based on a grid map and an active window has strong adaptability and is real-time in a crowded environment. Experimental results demonstrate that the robot can successfully avoid collisions and plan its path by using this method.",https://ieeexplore.ieee.org/document/1174408/,Proceedings. International Conference on Machine Learning and Cybernetics,4-5 Nov. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRI-05.2005.1506505,The behavior evolving model and application of virtual robots,IEEE,Conferences,"We suggest a model that evolves the behavioral knowledge of a virtual robot. The knowledge is represented in classification rules and a neural network, and is learned by a genetic algorithm. The model consists of a virtual robot with behavior knowledge, an environment that it moves in, and an evolution performer that includes a genetic algorithm. We have also applied our model to an environment where the robots gather food into a nest. When comparing our model with the conventional method on various test cases, our model showed superior overall learning.",https://ieeexplore.ieee.org/document/1506505/,"IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.",15-17 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2010.5650765,The design of LEO: A 2D bipedal walking robot for online autonomous Reinforcement Learning,IEEE,Conferences,"Real robots demonstrating online Reinforcement Learning (RL) to learn new tasks are hard to find. The specific properties and limitations of real robots have a large impact on their suitability for RL experiments. In this work, we derive the main hardware and software requirements that a RL robot should fulfill, and present our biped robot LEO that was specifically designed to meet these requirements. We verify its aptitude in autonomous walking experiments using a pre-programmed controller. Although there is room for improvement in the design, the robot was able to walk, fall and stand up without human intervention for 8 hours, during which it made over 43; 000 footsteps.",https://ieeexplore.ieee.org/document/5650765/,2010 IEEE/RSJ International Conference on Intelligent Robots and Systems,18-22 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.1994.639471,The development of a fully autonomous ground vehicle (FAGV),IEEE,Conferences,"As a first step toward the creation of a fully autonomous vehicle that operates in a real world environment, we are currently developing a prototype autonomous ground vehicle (AGV) for use in factories and other industrial/business sites based on behavior-based artificial intelligence (AI) control. This flexible and fully autonomous AGV (FAGV) is expected to operate efficiently in a normal industrial environment without any external guidance. The crucial technique employed is a non-Cartesian way of organizing software agents for the creation of a highly responsive control program. The resulting software is considerably reduced in size. Through numerous experiments using mobile robots we confirmed that these new control programs excel in functionality, efficiency, flexibility and robustness. The second key technique in the planning stage is evolutionary computation, of which genetic algorithms are a principal technique. An online, real-time evolution of the control program will be incorporated in later phases of the project to make FAGVs adaptable to any given operational environment after deployment. The first prototype FAGV has an active vision and behaviour-based control system.",https://ieeexplore.ieee.org/document/639471/,Proceedings of the Intelligent Vehicles '94 Symposium,24-26 Oct. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2002.1004464,The force model: reducing the complexity by reformulating the problem,IEEE,Conferences,"Most experiments in research on autonomous agents and mobile robots are performed either in simulation or on robots with static physical properties; evolvable hardware is hardly ever used. One of the very rare exceptions is the eyebot on which Lichtensteiger and Eggenberger have evolved simplified insect eyes. Even though substantially improved, the evolutionary models currently applied still lack both scalability and noise-resistance. To tackle these problems, this paper proposes a biologically-inspired force model for this class of real-world applications. The simulation results clearly indicate that this model provides a significant improvement over existing limitations. Furthermore, this paper argues that the force model is of more general utility.",https://ieeexplore.ieee.org/document/1004464/,Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600),12-17 May 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.1998.711559,The sensor-control Jacobian as a basis for controlling calibration-free robots,IEEE,Conferences,"A method for controlling the motions of robots is presented. It is based on the newly introduced sensor-control Jacobian matrix and avoids all quantitative modeling of the robot and the sensor system. The sensor-control Jacobian contains the coefficients that relate those changes in sensor data which are caused by a motion of the robot to the robot control words that caused the robot to move and, thus, the sensor data to change. A wide variety of tasks of robots can be reduced to minimizing the differences between actual sensor data and a set of hypothetical sensor data corresponding to some desired state. All these tasks can be solved by this method. The method is especially useful for calibration-free robots, since neither quantitative models of the mechanical, kinematic and control characteristics of the robot, nor knowledge of the sensor characteristics are required. The sensor-control Jacobian may be determined automatically in real time while the robot is operating. This yields a high degree of adaptability and flexibility against unforeseen changes in the robot's parameters. Because the concept has an open structure it allows further extensions and improvements, e.g., in terms of the utilization of sensor data redundancy and machine learning. For the purpose of evaluation, the concept has been implemented on a calibration-free camera-manipulator system. Real-world grasping experiments have demonstrated the effectiveness of the method.",https://ieeexplore.ieee.org/document/711559/,IEEE International Symposium on Industrial Electronics. Proceedings. ISIE'98 (Cat. No.98TH8357),7-10 July 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968514,Timepix Radiation Detector for Autonomous Radiation Localization and Mapping by Micro Unmanned Vehicles,IEEE,Conferences,"A system for measuring radiation intensity and for radiation mapping by a micro unmanned robot using the Timepix detector is presented in this paper. Timepix detectors are extremely small, but powerful 14 × 14 mm, 256 × 256 px CMOS hybrid pixel detectors, capable of measuring ionizing alpha, beta, gamma radiation, and heaving ions. The detectors, developed at CERN, produce an image free of any digital noise thanks to per-pixel calibration and signal digitization. Traces of individual ionizing particles passing through the sensors can be resolved in the detector images. Particle type and energy estimates can be extracted automatically using machine learning algorithms. This opens unique possibilities in the task of flexible radiation detection by very small unmanned robotic platforms. The detectors are well suited for the use of mobile robots thanks to their small size, lightweight, and minimal power consumption. This sensor is especially appealing for micro aerial vehicles due to their high maneuverability, which can increase the range and resolution of such novel sensory system. We present a ROS-based readout software and real-time image processing pipeline and review options for 3-D localization of radiation sources using pixel detectors. The provided software supports off-the-shelf FITPix, USB Lite readout electronics with Timepix detectors.",https://ieeexplore.ieee.org/document/8968514/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2011.5980333,To look or not to look: A hierarchical representation for visual planning on mobile robots,IEEE,Conferences,"Mobile robots are increasingly being used in real-world applications due to the ready availability of high-fidelity sensors and the development of sophisticated information processing algorithms. However, one key challenge to the widespread deployment of mobile robots equipped with multiple sensors and processing algorithms is the ability to autonomously tailor sensing and information processing to the task at hand. This paper poses this challenge as the task of planning under uncertainty, and more specifically as an instance of probabilistic sequential decision-making. A novel hierarchy of partially observable Markov decision processes (POMDPs) is incorporated, which uses constrained-convolutional policies and automatic belief propagation to achieve efficient and reliable operation on mobile robots. All algorithms are implemented and evaluated on simulated and physical robot platforms for the task of searching for target objects in dynamic indoor environments.",https://ieeexplore.ieee.org/document/5980333/,2011 IEEE International Conference on Robotics and Automation,9-13 May 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CoASE.2014.6899348,Toward safe close-proximity human-robot interaction with standard industrial robots,IEEE,Conferences,"Allowing humans and robots to interact in close proximity to each other has great potential for increasing the effectiveness of human-robot teams across a large variety of domains. However, as we move toward enabling humans and robots to interact at ever-decreasing distances of separation, effective safety technologies must also be developed. While new, inherently human-safe robot designs have been established, millions of industrial robots are already deployed worldwide, which makes it attractive to develop technologies that can turn these standard industrial robots into human-safe platforms. In this work, we present a real-time safety system capable of allowing safe human-robot interaction at very low distances of separation, without the need for robot hardware modification or replacement. By leveraging known robot joint angle values and accurate measurements of human positioning in the workspace, we can achieve precise robot speed adjustment by utilizing real-time measurements of separation distance. This, in turn, allows for collision prevention in a manner comfortable for the human user.We demonstrate our system achieves latencies below 9.64 ms with 95% probability, 11.10 ms with 99% probability, and 14.08 ms with 99.99% probability, resulting in robust real-time performance.",https://ieeexplore.ieee.org/document/6899348/,2014 IEEE International Conference on Automation Science and Engineering (CASE),18-22 Aug. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968166,Towards a Robot Architecture for Situated Lifelong Object Learning,IEEE,Conferences,"The ability to acquire knowledge incrementally and after deployment is of utmost importance for robots operating in the real world. Moreover, robots that have to operate alongside people need to be able to interact in a way that is intuitive for the users, e.g., by understanding and producing natural language. In this paper we present a first prototype of a robot architecture developed for situated lifelong object learning. The system is able to communicate with its users through natural language and perform object learning and recognition on the spot through situated interactions. In this first stage, we evaluate the system in terms of recognition accuracy which gives an indirect measure of the quality of the collected data with the proposed pipeline. Our results show that the robot can use this data for both learning and recognition with acceptable incremental performance. We also discuss limitations and steps that are necessary in order to improve performance as well as to shed some light on system usability.",https://ieeexplore.ieee.org/document/8968166/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2002.1045641,Towards grounded human-robot communication,IEEE,Conferences,"Future robots are expected to communicate with humans using natural language. The naive human user will expect a robot to easily understand what he/she is meaning by instructions concerning robot's tasks. This implies that the robot will need to have a means of grounding, in its own sensors, the natural language terms and constructions used by the human user. This paper presents an approach to solve this problem that is based on the integration of a ""learning server"" in the software architecture of the robot. Such server should be capable of on-line, incremental learning from examples; it should handle multiple problems concurrently and it should have meta-learning capabilities. A learning server already developed by the authors is presented. Complementarily, the dimensionality reduction problem is also addressed, using a Blocked DCT approach. Experimental results are obtained in a scenario in which three concepts (corresponding to natural language expressions) are concurrently learned.",https://ieeexplore.ieee.org/document/1045641/,Proceedings. 11th IEEE International Workshop on Robot and Human Interactive Communication,27-27 Sept. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2012.6301137,Towards hierarchical self-optimization in autonomous groups of mobile robots,IEEE,Conferences,"We present a real-world scenario for investigating and demonstrating hierarchical self-optimization in autonomous groups of mobile robots. The scenario is highly dynamic and easily expandable. It offers adequate starting points for the integration of hierarchical self-optimization. Reinforcement learning, e. g., can be introduced in order to improve the individual behavior of a single robot. Also swarm intelligence algorithms can improve the overall team behavior with respect to common goals. A reference behavior system incorporating a dynamic role assignment and hierarchical state machines was implemented and has been applied to the miniature robot BeBot. The system was evaluated by conducting several tests.",https://ieeexplore.ieee.org/document/6301137/,IEEE 10th International Conference on Industrial Informatics,25-27 July 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2011.6033258,Towards the grounding of abstract words: A Neural Network model for cognitive robots,IEEE,Conferences,"In this paper, a model based on Artificial Neural Networks (ANNs) extends the symbol grounding mechanism to abstract words for cognitive robots. The aim of this work is to obtain a semantic representation of abstract concepts through the grounding in sensorimotor experiences for a humanoid robotic platform. Simulation experiments have been developed on a software environment for the iCub robot. Words that express general actions with a sensorimotor component are first taught to the simulated robot. During the training stage the robot first learns to perform a set of basic action primitives through the mechanism of direct grounding. Subsequently, the grounding of action primitives, acquired via direct sensorimotor experience, is transferred to higher-order words via linguistic descriptions. The idea is that by combining words grounded in sensorimotor experience the simulated robot can acquire more abstract concepts. The experiments aim to teach the robot the meaning of abstract words by making it experience sensorimotor actions. The iCub humanoid robot will be used for testing experiments on a real robotic architecture.",https://ieeexplore.ieee.org/document/6033258/,The 2011 International Joint Conference on Neural Networks,31 July-5 Aug. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAR55106.2022.9782656,Trade-off on Sim2Real Learning: Real-world Learning Faster than Simulations,IEEE,Conferences,"Deep Reinforcement Learning (DRL) experiments are commonly performed in simulated environments due to the tremendous training sample demands from deep neural networks. In contrast, model-based Bayesian Learning allows a robot to learn good policies within a few trials in the real world. Although it takes fewer iterations, Bayesian methods pay a relatively higher computational cost per trial, and the advantage of such methods is strongly tied to dimensionality and noise. In here, we compare a Deep Bayesian Learning algorithm with a model-free DRL algorithm while analyzing our results collected from both simulations and real-world experiments. While considering Sim and Real learning, our experiments show that the sample-efficient Deep Bayesian RL performance is better than DRL even when computation time (as opposed to number of iterations) is taken in consideration. Additionally, the difference in computation time between Deep Bayesian RL performed in simulation and in experiments point to a viable path to traverse the reality gap. We also show that a mix between Sim and Real does not outperform a purely Real approach, pointing to the possibility that reality can provide the best prior knowledge to a Bayesian Learning. Roboticists design and build robots every day, and our results show that a higher learning efficiency in the real-world will shorten the time between design and deployment by skipping simulations.",https://ieeexplore.ieee.org/document/9782656/,"2022 8th International Conference on Control, Automation and Robotics (ICCAR)",8-10 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2017.8324512,Trajectory tracking control of a unicycle-type mobile robot with a new planning algorithm,IEEE,Conferences,"Trajectory tracking control is one of the core techniques that impacts the auto-driving performance of a mobile robot. Whereas, there lacks enough work on reference trajectory generation and controller design for practical usage. This paper considers mobile robots with unicycle vehicle model on which most of automatic guided vehicles (AGVs) in real world are built. A new trajectory planning algorithm is developed, and is applied along with a control law considering constraints of the unicycle model and limited motor capabilities. The proposed algorithm is easy to be implemented on real world AGVs, and it yields a fast, accurate and robust trajectory tracking performance. The effectiveness of the algorithm is validated by simulation tests.",https://ieeexplore.ieee.org/document/8324512/,2017 IEEE International Conference on Robotics and Biomimetics (ROBIO),5-8 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACR51161.2020.9265509,Transfer of Inter-Robotic Inductive Classifier,IEEE,Conferences,"In multi-robot deployments, the robots need to share and integrate their own experience and perform transfer learning. Under the assumption that the robots have the same morphology and carry equivalent sensory equipment, the problem of transfer learning can be considered incremental learning. Thus, the transfer learning problem inherits the challenges of incremental learning, such as catastrophic forgetting and concept drift. In catastrophic forgetting, the model abruptly forgets the previously learned knowledge during the learning process. The concept drift arises with different experiences between consecutively sampled models. However, state-of-the-art robotic transfer learning approaches do not address both challenges at once. In this paper, we propose to use an incremental classifier on a transfer learning problem. The feasibility of the proposed approach is demonstrated in a real deployment. The robot consistently merges two classifiers learned on two different tasks into a classifier that performs well on both tasks.",https://ieeexplore.ieee.org/document/9265509/,"2020 4th International Conference on Automation, Control and Robots (ICACR)",11-13 Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN46459.2019.8956420,Trust Repair in Human-Swarm Teams+,IEEE,Conferences,"Swarm robots are coordinated via simple control laws to generate emergent behaviors such as flocking, rendezvous, and deployment. Human-swarm teaming has been widely proposed for scenarios, such as human-supervised teams of unmanned aerial vehicles (UAV) for disaster rescue, UAV and ground vehicle cooperation for building security, and soldier-UAV teaming in combat. Effective cooperation requires an appropriate level of trust, between a human and a swarm. When an UAV swarm is deployed in a real-world environment, its performance is subject to real-world factors, such as system reliability and wind disturbances. Degraded performance of a robot can cause undesired swarm behaviors, decreasing human trust. This loss of trust, in turn, can trigger human intervention in UAVs' task executions, decreasing cooperation effectiveness if inappropriate. Therefore, to promote effective cooperation we propose and test a trust-repairing method (Trust-repair) restoring performance and human trust in the swarm to an appropriate level by correcting undesired swarm behaviors. Faulty swarms caused by both external and internal factors were simulated to evaluate the performance of the Trust-repair algorithm in repairing swarm performance and restoring human trust. Results show that Trust-repair is effective in restoring trust to a level intermediate between normal and faulty conditions.",https://ieeexplore.ieee.org/document/8956420/,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),14-18 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASID50160.2020.9271762,Use of LSTM Regression and Rotation Classification to Improve Camera Pose Localization Estimation,IEEE,Conferences,"More accurately estimating camera pose can be used to greatly improve localization in applications such as augmented reality, autonomous driving, and intelligent robots. Deep learning methods have achieved great progress to improve accuracy but still have limitations with respect to rotation, which results in angle regression errors. In this work, we combine a LSTM module with rotation classification loss to regress the camera pose. The algorithm uses a robust processing pipeline to supervise the pose estimation with dynamic, weighted, multi-losses in order to limit separate Euler angle (yaw, pitch, roll) losses, and common translation-quaternion losses. An empirical test on the 7Scenes benchmark dataset shows better results than when using common absolute pose regression methods.",https://ieeexplore.ieee.org/document/9271762/,"2020 IEEE 14th International Conference on Anti-counterfeiting, Security, and Identification (ASID)",30 Oct.-1 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iFUZZY50310.2020.9297367,Using Interval Type-2 Recurrent Fuzzy Cerebellar Model Articulation Controller Based on Improved Differential Evolution for Cooperative Carrying Controller of Mobile Robots,IEEE,Conferences,"Mobile robot is widely utilized in various fields such as navigation control, obstacle avoidance and object carrying. For keeping away from obstacles to avoid collision and preventing object carrying from dropping down, we propose a state manager (SM) designed to assist the mobile robots so that they can switch operation between wall-following carrying (WFC) and toward goal carrying (TGC) by different external condition. In this controlling model, interval type-2 recurrent fuzzy cerebellar model articulation controller (IT2RFCMAC), embedded with a modified evolutionary optimization and dynamic grouping differential evolution (DGDE), is implemented for WFC and TGC. By adopting reinforcement learning strategy, mobile robots equip with adaptively wall-following control to make cooperative carrying control in real.",https://ieeexplore.ieee.org/document/9297367/,2020 International Conference on Fuzzy Theory and Its Applications (iFUZZY),4-7 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEPDS.2018.8571820,Using Robot and Electric Drive in Fall Prediction,IEEE,Conferences,"The global aging phenomenon has motivated active research in human fall injuries. The fall prevention has hence become a popular topic in health informatics. An effective fall prevention paradigm could save millions of people from injury and avoid considerable casualties. Through comparison studies, detail-oriented simulations, and pragmatic field tests, an effective fall prediction method has been developed by authors. The finding is presented in this paper. Three techniques for fall prediction are discussed in this paper. A comparison technique to mimic the traditional stateless fall prediction techniques, along with an algorithm using artificial neural network, was first implemented in authors' previous paper. Then a robotic scheme was developed to simulate human fall by transplanting a proven fall prediction paradigm for humanoid robots with controlled electric drive systems to human subjects. Due to its simulation nature far from the human fall scenarios in reality, the robotic paradigm has obvious limits in real world applications. It was also used more like a reference framework for our last scheme. Eventually we built the third approach that eliminated the above limitation. The third approach is elaborated in this paper. Our test and simulation have proved its pragmatic superiority over other two approaches, along with vast majority of traditional paradigms.",https://ieeexplore.ieee.org/document/8571820/,2018 X International Conference on Electrical Power Drive Systems (ICEPDS),3-6 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.2015.7119180,Utilizing Artificial Intelligence to achieve a robust architecture for future robotic spacecraft,IEEE,Conferences,"This paper presents a novel failure-tolerant architecture for future robotic spacecraft. It is based on the Time and Space Partitioning (TSP) principle as well as a combination of Artificial Intelligence (AI) and traditional concepts for system failure detection, isolation and recovery (FDIR). Contrary to classic payload that is separated from the platform, robotic devices attached onto a satellite become an integral part of the spacecraft itself. Hence, the robot needs to be integrated into the overall satellite FDIR concept in order to prevent fatal damage upon hardware or software failure. In addition, complex dexterous manipulators as required for onorbit servicing (OOS) tasks may reach unexpected failure states, where classic FDIR methods reach the edge of their capabilities with respect to successfully detecting and resolving them. Combining, and partly replacing traditional methods with flexible AI approaches aims to yield a control environment that features increased robustness, safety and reliability for space robots. The developed architecture is based on a modular on-board operational framework that features deterministic partition scheduling, an OS abstraction layer and a middleware for standardized inter-component and external communication. The supervisor (SUV) concept is utilized for exception and health management as well as deterministic system control and error management. In addition, a Kohonen self-organizing map (SOM) approach was implemented yielding a real-time robot sensor confidence analysis and failure detection. The SOM features nonsupervized training given a typical set of defined world states. By compiling a set of reviewable three-dimensional maps, alternative strategies in case of a failure can be found, increasing operational robustness. As demonstrator, a satellite simulator was set up featuring a client satellite that is to be captured by a servicing satellite with a 7-DoF dexterous manipulator. The avionics and robot control were integrated on an embedded, space-qualified Airbus e.Cube on-board computer. The experiments showed that the integration of SOM for robot failure detection positively complemented the capabilities of traditional FDIR methods.",https://ieeexplore.ieee.org/document/7119180/,2015 IEEE Aerospace Conference,7-14 March 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAMI.2016.7422985,Vehicle navigation by fuzzy cognitive maps using sonar and RFID technologies,IEEE,Conferences,"Emerging concept of the so-called intelligent space (IS) offers means for use of mobile autonomous devices like vehicles or robots in a very broad area without necessity for these devices to own all necessary sensors. From this reason also new navigation methods are developing, which utilize IS means, with the aim to offer maybe not so accurate but first of all cheep and reliable solutions for a wide variety of devices. Our paper deals with the examination of possibility to interconnect sparsely deployed RFID tags with sonars. As signals produced by these two technologies are often affected by uncertainty and incompleteness we use fuzzy logic for their processing as well as control of the entire navigation process. For this purpose a special type of a fuzzy cognitive map was proposed. The paper describes real navigation experiments with a simple vehicle and evaluates them by selected criteria. Based on obtained results their explanations and conclusions for potential future research are sketched.",https://ieeexplore.ieee.org/document/7422985/,2016 IEEE 14th International Symposium on Applied Machine Intelligence and Informatics (SAMI),21-23 Jan. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341569,Velocity Regulation of 3D Bipedal Walking Robots with Uncertain Dynamics Through Adaptive Neural Network Controller,IEEE,Conferences,"This paper presents a neural-network based adaptive feedback control structure to regulate the velocity of 3D bipedal robots under dynamics uncertainties. Existing Hybrid Zero Dynamics (HZD)-based controllers regulate velocity through the implementation of heuristic regulators that do not consider model and environmental uncertainties, which may significantly affect the tracking performance of the controllers. In this paper, we address the uncertainties in the robot dynamics from the perspective of the reduced dimensional representation of virtual constraints and propose the integration of an adaptive neural network-based controller to regulate the robot velocity in the presence of model parameter uncertainties. The proposed approach yields improved tracking performance under dynamics uncertainties. The shallow adaptive neural network used in this paper does not require training a priori and has the potential to be implemented on the real-time robotic controller. A comparative simulation study of a 3D Cassie robot is presented to illustrate the performance of the proposed approach under various scenarios.",https://ieeexplore.ieee.org/document/9341569/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITST.2017.7972192,Virtual assistants and self-driving cars,IEEE,Conferences,"Self-driving cars are technologically a reality and in the next decade they are expected to reach the highest level of automation. While there is general agreement that an advanced human-autonomous vehicle (HAV) interaction is key to achieve the benefits of self-driving cars, it is less clear what role artificial intelligence (AI) should play in this context. While the scientific community is debating on the role and intersections of AI, autonomous vehicles and related issues, above all ethics, the automotive industry is already presenting AI-based products and services that may influence, in a direction or in another, our technological and societal futures. This paper focuses on virtual assistants, the personification of the car intelligence incorporating, among others, an algorithmic “brain”, a synthetic human “voice” and powerful sensor-based “senses”. Should virtual assistants just assist humans or replace them whenever necessary? Should their scope of action be limited to safety-related driving tasks or to any activity performed in the car or controlled from the car? Although at a very early stage of commercial development, the paper will review the state-of-the-art of in-car virtual assistants underlining their role and functions in the connected and automated driving ecosystem. By drawing from earlier reflections on automation, robots and intelligent agents, it will then identify a series of issues to be addressed by the scientific community, policy-makers and the automotive industry stakeholders.",https://ieeexplore.ieee.org/document/7972192/,2017 15th International Conference on ITS Telecommunications (ITST),29-31 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMICND.2005.1558827,Virtual environment for robots interfaces design and testing,IEEE,Conferences,"This paper refers to the implementation of a virtual environment for the robot interfaces testing. This software environment is very useful because, comparing to the experiments with real robots, it allow the testing and evaluation of different types of interfaces and different working environments with diverse configurations. A very important facility of this interactive software environment is the fact that the designers of the robots sensors and interfaces are able to work in parallel to design test, optimize and realize different control devices for the robot.",https://ieeexplore.ieee.org/document/1558827/,"CAS 2005 Proceedings. 2005 International Semiconductor Conference, 2005.",3-5 Oct. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANS.1995.528724,Virtual world visualization for an autonomous underwater vehicle,IEEE,Conferences,"A critical bottleneck exists in autonomous underwater vehicle (AUV) design and development. It is tremendously difficult to observe, communicate with and test underwater robots, because they operate in a remote and hazardous environment where physical dynamics and sensing modalities are counterintuitive. An underwater virtual world can comprehensively model all necessary functional characteristics of the real world in real time. This virtual world is designed from the perspective of the robot, enabling realistic AUV evaluation and testing in the laboratory. 3D real-time graphics are our window into the virtual world, enabling multiple observers to visualize complex interactions. A networked architecture enables multiple world components to operate collectively in real time, and also permits world-wide observation and collaboration with other scientists interested in the robot and virtual world.",https://ieeexplore.ieee.org/document/528724/,'Challenges of Our Changing Global Environment'. Conference Proceedings. OCEANS '95 MTS/IEEE,9-12 Oct. 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/i-Society.2016.7854189,Virtualized higher education: Where e-learning trends and new faculty roles converge towards personalization,IEEE,Conferences,"Virtual Higher Education is in sharp focus lately with onslaught of Online Learning Platforms (MOOC, Coursera, Khan Academy, Udacity) offering Free Courses in every discipline. Originally targeted at populations in underdeveloped countries not otherwise able to offer a world-class education, these courses have become popular and mainstream in developed countries being free, online and caters to most professionals to build skills and explore career changes. Current delivery format robs an academic the ability to express their teaching style. The PLErify Application seeks to address the need to support, through a `DIY approach', an Academic's teaching style and retain ownership of materials. PLErify is discussed in the paper relative to the Robots role vis a vis the traditional Professor's role and the prevailing thought of the best ways to energize and modernize current teaching methods.",https://ieeexplore.ieee.org/document/7854189/,2016 International Conference on Information Society (i-Society),10-13 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AQTR.2006.254650,Vision based algorithm for path planning of a mobile robot by using cellular neural networks,IEEE,Conferences,"The paper presents a new vision based algorithm for mobile robots path planning in an environment with obstacles. Cellular neural networks (CNNs) processing techniques are used here for real time motion planning to reach a fixed target. The CNN methods have been considered a solution for image processing in autonomous mobile robots guidance. The choice of CNNs for the visual processing is based on the possibility of their hardware implementation in large networks on a single VLSI chip (cellular neural networks -universal machine, CNN-UM (Roska and Chua, 1993 and Kim et al., 2002))",https://ieeexplore.ieee.org/document/4022973/,"2006 IEEE International Conference on Automation, Quality and Testing, Robotics",25-28 May 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICGEC.2012.151,Vision-Based Coordinate Transformation with Back Propagation Neural Networks on Mobile Robots,IEEE,Conferences,"Target tracking is important for vision-based robots to implement tasks of grasping, assembling and avoiding obstacles. the purpose of a target tracking system is to identify a target and then to estimate the position of the target. the targets' positions are usually described by various coordinate systems for different purposes. This study focuses on the problem of coordinate transformation on mobile robots and employs the techniques of Back-Propagation Neural Networks to discover the prediction models. with such prediction models, coordinate transformation can be done with less processing time. the techniques have been implemented and integrated with a four-wheeled vision-based security robot and has been verified in real environments. the experimental results show that the proposed method is able to produce simple and precise transformation models and improves the robot's performances.",https://ieeexplore.ieee.org/document/6456866/,2012 Sixth International Conference on Genetic and Evolutionary Computing,25-28 Aug. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8794123,Visual Guidance and Automatic Control for Robotic Personalized Stent Graft Manufacturing,IEEE,Conferences,"Personalized stent graft is designed to treat Abdominal Aortic Aneurysms (AAA). Due to the individual difference in arterial structures, stent graft has to be custom made for each AAA patient. Robotic platforms for autonomous personalized stent graft manufacturing have been proposed in recently which rely upon stereo vision systems for coordinating multiple robots for fabricating customized stent grafts. This paper proposes a novel hybrid vision system for real-time visual-sevoing for personalized stent-graft manufacturing. To coordinate the robotic arms, this system is based on projecting a dynamic stereo microscope coordinate system onto a static wide angle view stereo webcam coordinate system. The multiple stereo camera configuration enables accurate localization of the needle in 3D during the sewing process. The scale-invariant feature transform (SIFT) method and color filtering are implemented for stereo matching and feature identifications for object localization. To maintain the clear view of the sewing process, a visual-servoing system is developed for guiding the stereo microscopes for tracking the needle movements. The deep deterministic policy gradient (DDPG) reinforcement learning algorithm is developed for real-time intelligent robotic control. Experimental results have shown that the robotic arm can learn to reach the desired targets autonomously.",https://ieeexplore.ieee.org/document/8794123/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSESS52187.2021.9522158,Visual Loop Closure Detection Based on Lightweight Convolutional Neural Network and Product Quantization,IEEE,Conferences,"Mobile robots rely heavily on the creation of the scene map and the positioning in the map in an unknown environment, but no matter what type of map is created, it is inevitably affected by cumulative errors. This presents a huge challenge for loop closure detection technology. Using traditional loop closure detection methods to perform scene recognition is difficult to extract the appearance changes caused by time, weather, or seasonal conditions in the image and deep semantic information, and the speed of extracting image features is slow, which is difficult to meet the real-time performance of robots. Because of the success of deep convolutional neural networks(CNN), it is possible to enrich the information of image features. First of all, this paper uses the pre-trained CNN model SSE-Net to extract the deep visual appearance and semantic features of the image, and obtain the feature description vector. Then, after product quantization(PQ) and encoding, the final pair of candidate frames is quickly searched and matched to obtain the most similar pair of candidate frames and judged as a loop . After the verification of the New collage dataset and the City Center dataset, this algorithm has achieved a good Precision-Recall rate and a faster speed compared with the recently proposed large-scale convolution network VGG16 method and traditional feature extraction methods.",https://ieeexplore.ieee.org/document/9522158/,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),20-22 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2019.8926916,Visual Subterranean Junction Recognition for MAVs based on Convolutional Neural Networks,IEEE,Conferences,"This article proposes a novel visual framework for detecting tunnel crossings/junctions in underground mine areas towards the autonomous navigation of Micro Aerial Vehicles (MAVs). Usually mine environments have complex geometries, including multiple crossings with different tunnels that challenge the autonomous planning of aerial robots. Towards the envisioned scenario of autonomous or semi-autonomous deployment of MAVs with limited Line-of-Sight in subterranean environments, the proposed module acknowledges the existence of junctions by providing crucial information to the autonomy and planning layers of the aerial vehicle. The capability for a junction detection is necessary in the majority of mission scenarios, including unknown area exploration, known area inspection and robot homing missions. The proposed novel method has the ability to feed the image stream from the vehicles on-board forward facing camera in a Convolutional Neural Network (CNN) classification architecture, expressed in four categories: 1) left junction, 2) right junction, 3) left & right junction, and 4) no junction in the local vicinity of the vehicle. The core contribution stems for the incorporation of AlexNet in a transfer learning scheme for detecting multiple branches in a subterranean environment. The validity of the proposed method has been validated through multiple data-sets collected from real underground environments, demonstrating the performance and merits of the proposed module.",https://ieeexplore.ieee.org/document/8926916/,IECON 2019 - 45th Annual Conference of the IEEE Industrial Electronics Society,14-17 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1997.655065,Visual navigation in an open environment without map,IEEE,Conferences,We describe how a mobile robot controlled only by visual information can retrieve a particular goal location in an open environment. Our model does not need a precise map nor to learn all the possible positions in the environment. The system is a neural architecture inspired from neurobiological studies using the recognition of visual patterns called landmarks. The robot merges this visual information and its azimuth to build a plastic representation of its location. This representation is used to learn the best movement to reach the goal. A simple and fast online learning of a few places located near the goal allows the robot to reach the goal from anywhere in its neighborhood. The system uses only an egocentric representation of the robot environment and presents very high generalization capabilities. We describe an efficient implementation tested on our robot in two real indoor environments. We show the limitations of the model and its possible extensions to create autonomous robots only guided by visual information.,https://ieeexplore.ieee.org/document/655065/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDS49703.2020.00012,Welding Seam Recognition Robots Based on Edge Computing,IEEE,Conferences,"In order to meet the requirements of the accuracy and real-time performance during the working process of underwater welding robots, a scheme of welding seam recognition robots system based on the edge computing is proposed in this paper. A number of pre-processing methods for capturing welding seam image were designed, including Thresholding, Filtering and Edge Detect. A Convolutional Neural Network(CNN) model for welding seam recognition was also created. In the experiments, the image pre-processing and CNN algorithms were integrated in and deployed to the robots, and the learning and training algorithms of the CNN were deployed to the cloud servers. The image pre-processing methods filtered the interference in underwater operations and achieved the image compression and feature extraction. The cloud servers fulfilled the training and parameter optimization of the CNN, which improved the accuracy of welding seam image recognition.",https://ieeexplore.ieee.org/document/9275963/,2020 International Conference on Computing and Data Science (CDS),1-2 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCA.2009.5410442,Wheeled mobile robot control using virtual pheromones and neural networks,IEEE,Conferences,"This paper presents a novel approach on the implementation of the concept of ¿virtual pheromones¿ for use in controlling autonomous mobile robots. Rather than being deployed in the environment, the virtual pheromones are stored in a map of the environment maintained and updated by a ¿pheromone server¿. This map acts like a shared memory for all the agents, by means of a radio communication link between each agent and the pheromone server. No direct communication between agents is required. The pheromone server can be implemented on a regular computer, a handheld device, or an embedded controller carried by a leader robot. The technique described is equally applicable for guiding individual robot and robot swarms. The experiments, performed with mobile robot Pioneer 3-DX show that this method allows significant simplification and cost reduction of the autonomous agents. Several possible applications are discussed.",https://ieeexplore.ieee.org/document/5410442/,2009 IEEE International Conference on Control and Automation,9-11 Dec. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9413066,Yolo+FPN: 2D and 3D Fused Object Detection With an RGB-D Camera,IEEE,Conferences,"In this paper we propose a new deep neural network system, called Yolo+FPN, which fuses both 2D and 3D object detection algorithms to achieve better real-time object detection results and faster inference speed, to be used on real robots. Finding an optimized fusion strategy to efficiently combine 3D object detection with 2D detection information is useful and challenging for both indoor and outdoor robots. In order to satisfy real-time requirements, a trade-off between accuracy and efficiency is needed. We not only have improved training and test accuracies and lower mean losses on the KITTI object detection benchmark comparing with our baseline method, but also achieve competitive average precision on 3D detection of all classes in three levels of difficulty comparing with other state-of-the-art methods. Also, we implemented Yolo+FPN system using an RGB-D camera, and compared the speed of object detection using different GPUs. For the real implementation of both indoor and outdoor scenes, we focus on person detection, which is the most challenging and important among the three classes.",https://ieeexplore.ieee.org/document/9413066/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAS.2009.2,[Title page iii],IEEE,Conferences,The following topics are dealt with: real-time chain-structured synchronous dataflow; memory requirement formal determination; linear singular descriptor differential system; execution-optimized paths; greedy strategy; load trend evaluation; self-managed P2P streaming; context-aware ambient assisted living application; self-adaptive distributed model; autonomic systems; wireless sensor networks; topology control; learning based method; self-recovery method; mobile data sharing; heterogeneous QoS resource manager; component-based self-healing; NGN mobility; interactive user activity; .NET Windows service agent technology; agent based Web browser; resource-definition policies; autonomic computing; autonomic system administration; automatic database performance tuning; knowledge management; adaptive reinforcement learning; VoIP services; autonomic RSS: distributed virtual reality simulations; virtual machines resources allocation; multi-lier distributed systems; network I/O extensibility; virtual keyboards; self-configuring smart homes; legged underwater vehicles; particle filters; reusable semantic components; multi-agent systems; fixed-wing unmanned aerial vehicles; fuzzy inference system; robot swarms; mobile robots; optimization architecture; autonomous unmanned helicopter landing system design; heterogeneous multi-database environments; autonomic software license management system; Web server crashes prediction; laser range finder; video quality; wireless networks; ITU-T G.1030; open IMS core; context-aware data mining methodology; supply chain finance cooperative systems; autonomous pervasive environments; distributed generic stress tool; dynamic adaptive systems; multisensory media effects and user preference.,https://ieeexplore.ieee.org/document/4976566/,2009 Fifth International Conference on Autonomic and Autonomous Systems,20-25 April 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR47638.2019.9043946,libSmart: an Open-Source Tool for Simple Integration of Deep Learning into Intelligent Robotic Systems,IEEE,Conferences,"Intelligent robotic systems can be empowered by advanced deep learning techniques. Robotic operations such as object recognition are well investigated by researchers involved in machine learning. However, these solutions have often led to ad-hoc implementation in experimental settings. Less reported is systematic implementation of deep learning models in industrial robots. The lack of standard implementation platforms has impeded widespread use of deep learning modules in industrial robots. It is of great importance to have development platforms that can coordinate several deep learning modules of a complex system. In this paper, a scalable deep-learning friendly robot task organization system named libSmart is introduced. Similar to ROS, the architecture of the proposed system allows users to plug and play various devices but the proposed architecture is also highly compatible with deep learning modules. Specifically, the deployment of deep learning models is handled using a novel data graph method with distributed computing. In this way, the computationally expensive training and inferencing processes of deep learning models can be handled with isolated accelerating hardware to reduce the overall system latency. Successful implementation of simultaneous object recognition and pose estimation by an industrial robot has been presented as a case study. The proposed system is open source for all users to build their own intelligent systems with customized deep-learning models. (https://github.com/RustIron/libSmart.git).",https://ieeexplore.ieee.org/document/9043946/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2001.969840,STNS-R: a learning method for seamless transplantation from a virtual agent to a physical robot,IEEE,Conferences,"In this paper, we are concerned with the problem of how a physical robot can get an appropriate internal representation to its task and environment. Learning from experience is effective for the problem, but it is very time-consuming to learn a representation from the beginning in a real environment. On the other hand, the representation learned only in a simulated environment has the risk of not serving the purpose in a real environment because of the uncertainty in sensors, actuators, and the environment. In, order to have the best of both worlds, it is effective to transplant the learned state representation of a virtual agent to a physical robot. For this purpose., we improved our developed incremental learning architecture for use in the real environment and developed a new architecture, called STNS-R. In this architecture, inappropriate negative instances caused by uncertainties are found on the basis of the distribution of instances and removed in order to correct the distorted shapes of the states. The effectiveness of STNS-R is shown in the experimental results.",https://ieeexplore.ieee.org/document/969840/,"2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)",7-10 Oct. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIMCA.2005.1631415,Self-Organization of Spiking Neural Network Generating Autonomous Behavior in a Real Mobile Robot,IEEE,Conferences,"In this paper, we study the relation between neural dynamics and robot behavior to develop self-organization algorithm of spiking neural network applicable to autonomous robot. We first formulated a spiking neural network model whose inputs and outputs were analog. We then implemented it into a miniature mobile robot Khepera. In order to see whether or not a solution(s) for the given task exists with the spiking neural network, the robot was evolved with the genetic algorithm (GA) in an environment. The robot acquired the obstacle avoidance and navigation task successfully, exhibiting the presence of the solution. Then, a self-organization algorithm based on the use-dependent synaptic potentiation and depotentiation was formulated and implemented into the robot. In the environment, the robot gradually organized the network and the obstacle avoidance behavior was formed. The time needed for the training was much less than with genetic evolution, approximately one fifth (1/5)",https://ieeexplore.ieee.org/document/1631415/,"International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)",28-30 Nov. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSCAA.2008.4776238,Self-motion planning of redundant robot manipulators based on quadratic program and shown via PA10 example,IEEE,Conferences,"In this paper, a criterion is proposed in the form of a quadratic function for the purpose of self-motion planning of redundant robot arms. The proposed self-motion scheme with joint physical limits considered could be formulated as a quadratic programming (QP) problem subject to equality, (inequality) and bound constraints. A primal-dual neural network based on linear variational inequalities (LVI) is developed as the real-time solver for the resultant quadratic-program. The so-called LVI-based primal-dual neural network has a simple piecewise-linear dynamics and a global exponential convergence to optimal solutions of QP problems. Computer-simulations performed based on PA10 robot arm substantiate the efficacy of the proposed QP-based neural self-motion-planning scheme.",https://ieeexplore.ieee.org/document/4776238/,2008 2nd International Symposium on Systems and Control in Aerospace and Astronautics,10-12 Dec. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1997.625763,Self-organized learning and its implementation of robot movements,IEEE,Conferences,The self-organizing map algorithm using an artificial neural network originally developed by Kohonen and extended and modified later provides a distributed and autonomous learning procedure in engineering modeling of the human sensory-motor mapping mechanism. Its extension and adaptation to a control problem of a robot manipulator has been intensively discussed in past years. In this article the application of the self-organizing map algorithm to the generation of a visuo-motor map is focused on. A task-oriented inverse kinematic solution to a redundant manipulator is formed and real-time implementation of the map on a mechanical manipulator is performed.,https://ieeexplore.ieee.org/document/625763/,"1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation",12-15 Oct. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2006.1642213,Self-organizing approach for robot's behavior imitation,IEEE,Conferences,"In this paper, an approach for behavior imitation using visual information was introduced. The imitation process is done by a self organizing neural network module. From several demonstrations of task operation, a vision system captures movement of the demonstrator mobile robot and associated objects in an operation field. Then, the movement features are extracted to present to an imitation engine. Finally, skill or decision policy from teacher's demonstration is extracted and embedded into a self organizing neural network without explicit external supervisory signals. A simple action selection algorithm for choosing action from learned network is proposed. The algorithm was implemented and tested on a simulated robot and a real mobile robot to imitate two simple robot soccer behaviors: approaching the target and obstacle avoidance. Furthermore, the concept of similarity measure is introduced to evaluate imitation performance from the demonstrator",https://ieeexplore.ieee.org/document/1642213/,"Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.",15-19 May 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1996.506506,Semantic learning by an autonomous mobile robot,IEEE,Conferences,Describes the design and implementation of a learning system for control of an autonomous mobile robot. The robot learns reactive behaviors that allow it to retreat from potential collisions and to explore its environment by seeking out nearby objects. No external teaching input is required. Results from experiments with a real robot are presented. The learned reactive behaviors become the basis for the acquisition of more complex behaviors. Sensory/motor states are classified and then associated with lexical items to form a simple command language which is then used to direct the robot.,https://ieeexplore.ieee.org/document/506506/,Proceedings of IEEE International Conference on Robotics and Automation,22-28 April 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8206048,Sensor fusion for robot control through deep reinforcement learning,IEEE,Conferences,"Deep reinforcement learning is becoming increasingly popular for robot control algorithms, with the aim for a robot to self-learn useful feature representations from unstructured sensory input leading to the optimal actuation policy. In addition to sensors mounted on the robot, sensors might also be deployed in the environment, although these might need to be accessed via an unreliable wireless connection. In this paper, we demonstrate deep neural network architectures that are able to fuse information generated by multiple sensors and are robust to sensor failures at runtime. We evaluate our method on a search and pick task for a robot both in simulation and the real world.",https://ieeexplore.ieee.org/document/8206048/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR.2015.7251437,Simultaneous human-robot adaptation for effective skill transfer,IEEE,Conferences,"In this paper, we propose and implement a human-in-the loop robot skill synthesis framework that involves simultaneous adaptation of the human and the robot. In this framework, the human demonstrator learns to control the robot in real-time to make it perform a given task. At the same time, the robot learns from the human guided control creating a non-trivial coupled dynamical system. The research question we address is how this system can be tuned to facilitate faster skill transfer or improve the performance level of the transferred skill. In the current paper we report our initial work for the latter. At the beginning of the skill transfer session, the human demonstrator controls the robot exclusively as in teleoperation. As the task performance improves the robot takes increasingly more share in control, eventually reaching full autonomy. The proposed framework is implemented and shown to work on a physical cart-pole setup. To assess whether simultaneous learning has advantage over the standard sequential learning (where the robot learns from the human observation but does not interfere with the control) experiments with two groups of subjects were performed. The results indicate that the final autonomous controller obtained via simultaneous learning has a higher performance measured as the average deviation from the upright posture of the pole.",https://ieeexplore.ieee.org/document/7251437/,2015 International Conference on Advanced Robotics (ICAR),27-31 July 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICIP53388.2021.9642219,Sliding Mode Control Algorithm of Upper Limb Exoskeleton Rehabilitation Robot Based on RBF Neural Network,IEEE,Conferences,"Aiming at the nonlinear and uncertain problems of upper limb exoskeleton rehabilitation robot (ULERR) during passive training, a sliding model controller based on radial basis neural network is designed in this paper. Firstly, a four-degree-of-freedom ULERR is designed for stroke patients in soft paralysis and spasticity, and a kinetic model was established. Secondly, RBF neural network is used to approximate the uncertainty caused by spastic disturbance of patients in the system. The weight in the neural network is replaced by a single parameter, and the adaptive algorithm is easy to adjust and has strong real-time performance. The asymptotic stability of the controller is verified by Lyapunov theorem. Finally, the desired training trajectory of the upper limb is obtained by a three-dimensional motion capture system, and the simulation experiments are carried out with Matlab software to prove that the proposed control method solves the chattering problem of traditional sliding mode control, to meet the control requirements of real-time rehabilitation training.",https://ieeexplore.ieee.org/document/9642219/,2021 11th International Conference on Intelligent Control and Information Processing (ICICIP),3-7 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SaCoNeT.2018.8585616,Smart Navigation of Mobile Robot Using Neural Network Controller,IEEE,Conferences,"The field of autonomous navigation of mobile robot is advancing so fast especially with the development of machine learning algorithms. This study aims to introduce a neural network controller that controls the trajectory and the obstacle avoidance of a non-holonomic mobile robot.We train the robot in environment containing multiple obstacles with different places. This paper includes both a kinematic and a dynamic study of a mobile robot. Different training schemes have been studied that tackle the learning objectives differently. The trained controller is producing the Pulse Width Modulation (PWM) signals that could be implemented in a microprocessor and validated by simulations. Unlike some other recent approaches, this work was validated by a 3D simulation which is similar to the real model.",https://ieeexplore.ieee.org/document/8585616/,2018 International Conference on Smart Communications in Network Technologies (SaCoNeT),27-31 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2004.1308781,Software approach for the autonomous inspection robot MAKRO,IEEE,Conferences,"The sewer inspection robot MAKRO is an autonomous multi-segment robot with worm-like shape driven by wheels. It is currently under development in the project MAKRO-PLUS. The robot has to navigate autonomously within sewer systems. Its first tasks is to take water probes, analyze them onboard, and measure positions of manholes and pipes to detect pollution loaded sewage and to improve current maps of sewer systems. One of the challenging problems is the control software, which should enable the robot to navigate in the sewer system and perform the inspection tasks autonomously, while always taking care of its own safety. Tests in our test environment and in a real sewer system show promising results. This paper focuses on the software approach. To manage the complexity a layered architecture has been chosen, each layer defining a different level of abstraction. After determining the abstraction levels, we use different methods for implementation. For the highest abstraction level a standard AI-planning algorithm is used. For the next level, finite state automata has been chosen. For ""simple"" task implementation we use a modular C++ based method (MCA2), which is also used on the lowest software level.",https://ieeexplore.ieee.org/document/1308781/,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",26 April-1 May 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE46568.2020.9042995,Stroke Signs Detection System by SNS Agency Robot,IEEE,Conferences,"This paper proposes a system which implements the Cincinnati Prehospital Stroke Scale (CPSS), the widely used screening method for the initial symptoms of a stroke, in a communication robot. AI on cloud analyses an acquired video through a conversation with the robot in real time and automatically determines the abnormalities. The judgement result is informed to his/her families by SNS. This study implemented two of the three CPSS scales such as “Arms” and “Speech”, we confirmed that the system enables to acquire, analyze and notify the information in real time.",https://ieeexplore.ieee.org/document/9042995/,2020 IEEE International Conference on Consumer Electronics (ICCE),4-6 Jan. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1991.187403,Symbolic feature-based representation and planning for an agent based robot controller,IEEE,Conferences,"A prototype system has been developed to input 3-D computer-aided-design (CAD) data to automatically generate and implement robot trajectories for such application as waterjet cutting, surface finishing and polishing. The system can drive various robot configurations and handle contingencies such as collisions and singularities. The system provides a framework for integration of high-level reasoning, real-time path and trajectory planning, and various levels of feedback based on contingency detection algorithms or sensor data. Although initial CAD data have been in the form of IGES geometric entities, a higher-level CAD description based on manufacturing features which incorporates both geometric and process information is being developed. This feature-based CAD representation provides a direct interface between the CAD design data and the planning system for robot control. An agent actor paradigm is proposed as the representation for software/hardware system specifications and associated software and hardware modules.<>",https://ieeexplore.ieee.org/document/187403/,Proceedings of the 1991 IEEE International Symposium on Intelligent Control,13-15 Aug. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSICT.2018.8565654,System Simulation for Robot Control Based on AI Approach,IEEE,Conferences,"In this paper, we focus on real robot development and control for different surface conditions using an AI based approach. The core components of our existing robot are pressure sensor, servomotor and software-driven microcontroller. We performed robot system simulation for various ground surface conditions to control the robot with respect to pressure-sensing data that incorporates the two-way interactions between robot and ground. We have used an artificial neural network (ANN) approach for pressure-sensor-data analysis. The analysis result shows that above 25 hidden neurons and an increasing number of training cycles will deliver better performance in terms of mean square error (mse), learning time and improved nearest surface-pattern recognition. The analysis results are useful for next generation AI-chip development for real-time robot control and movement.",https://ieeexplore.ieee.org/document/8565654/,2018 14th IEEE International Conference on Solid-State and Integrated Circuit Technology (ICSICT),31 Oct.-3 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA52036.2021.9512666,Target Detection and Tracking of Ground Mobile Robot Based on Improved Single Shot Multibox Detector Network,IEEE,Conferences,"To solve the problems of slow labeling speed of the traditional labellmg data set establishment method, and slow running speed of target classification and detection algorithm based on Single Shot Multibox Detector(SSD) deep learning network, this paper proposes a fast data set labeling algorithm and a fast SSD network for target real-time detection and tracking research. First, a data set is established quickly by using TLD target detection and tracking algorithms, cropping and mirroring methods are used to strengthen the data set. Then, SSD backbone network is improved based on depth-wise separable convolution to establish a fast SSD network. Finally, the ground mobile robot in RoboMasters(RM) competition is used as the detection and tracking target indoors and outdoors, as well as with other different scenarios with shield to test the real-time performance, accuracy and effectiveness of the algorithm. The results show that compared with traditional SSD network research, in terms of the analysis and processing system deployed on low-performance hardware, the improved fast SSD network can better meet the real-time requirements of target detection and tracking.",https://ieeexplore.ieee.org/document/9512666/,2021 IEEE International Conference on Mechatronics and Automation (ICMA),8-11 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1999.812762,Task-model based human robot cooperation using vision,IEEE,Conferences,"In order to assist a human, the robot must recognize human motion in real time by vision, and must plan and execute the needed assistance motion based on the task purpose and the context. In this research, we tried to solve such problems. We defined the abstract task model, analyzed the human demonstration by using events and an event stack, and automatically generated the task models needed in the assistance by the robot. The robot planned and executed the appropriate assistance motions based on the task: models according to the human motions in the cooperation with the human. We implemented a 3D object recognition system and a human grasp recognition system by using trinocular stereo color cameras and a real time range finder. The effectiveness of these methods was tested through an experiment in which the human and the robotic hand assembled toy parts in cooperation.",https://ieeexplore.ieee.org/document/812762/,Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289),17-21 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PADSW.2000.884672,Teleoperation system for real world robots-adaptive robot navigation based on sensor fusion,IEEE,Conferences,"The authors propose a teleoperation system with an autonomous robot which is able to solve tasks even without a large load for the operator and the system. Most teleoperation systems require skilled operators and expensive interfaces to solve tasks because they assume that the operator controls a robot completely. For these problems, we propose a teleoperation system which consists of an operation system and an autonomous robot. The operation system has a man-machine interface and allows a user to specify the working space and the tasks to be done. The autonomous robot follows the instruction from the operation system to solve the specific tasks. The paper focuses on navigation problems of the autonomous robot as an essential part of the proposed system. Namely, the autonomous robot should keep on the instructed paths in the real world to achieve a goal of the tasks. Our approach is based on a sensor fusion method based on two learning schemes: self-organizing map (SOM) and reinforcement learning. These learning schemes allow the system to be able to solve the tasks in an unreliable environment such as outdoors. Computational simulations reveal the effectiveness and robustness of the proposed method in the navigation problem.",https://ieeexplore.ieee.org/document/884672/,Proceedings Seventh International Conference on Parallel and Distributed Systems: Workshops,4-7 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAE47758.2019.9221734,The Real-Time Object Detection System on Mobile Soccer Robot using YOLO v3,IEEE,Conferences,"In a Soccer robot, each robot must be able to detect the object such as ball, goal and circle line in the game field through the vision system using a camera. The object detection in the past decade was used color filtering method, then it's methods was develop into neural network based. Neural network detection has also a lot of progress, start from R-CNN, fast R-CNN, and new method is YOLOv3. In this paper we will explain the implementation of YOLO V3 to detect object at Barelang mobile soccer robot. This system is run on SHUTTLE Xl MINI PC has Octa Core Intel Core i7-7700HQ processor, 16 GB of RAM and with 3GB of NVIDIA GeForce GTX 1060 graphics card has cuda core score 1152. The experiments result show the object detection get 28.3 fps. Datasets performance of the proposed method is IOU 71.76%, recall 0.92, precision 0.92 and mAP 87.07%. YOLO v3 capable to detect and distinguish objects in the different lighting condition, with max distance 3 m for ball object and 8 m for goal object.",https://ieeexplore.ieee.org/document/9221734/,2019 2nd International Conference on Applied Engineering (ICAE),2-3 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BEC.2010.5631008,Timed Automata based provably correct robot control,IEEE,Conferences,"This paper presents a feasibility study on the usage of Uppaal Timed Automata (UPTA) for deliberative level robotic control. The study is based on the Scrub Nurse Robot case-study. Our experience confirms that UPTA model based control enables the control loop to be defined and maintained during the robot operation autonomously with minimum human intervention. Specifically, in our robot architecture the control model is constructed automatically using unsupervised learning. Correctness of the model is verified on-the-fly against safety, reachability, and performance requirements. Finally, it is demonstrated that UPTA model based robot control, action planning and model updates have natural implementation based on existing model execution and conformance testing tool Uppaal Tron.",https://ieeexplore.ieee.org/document/5631008/,2010 12th Biennial Baltic Electronics Conference,4-6 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN50785.2021.9515348,Towards Out-of-Sight Predictive Tracking for Long-Term Indoor Navigation of Non-Holonomic Person Following Robot,IEEE,Conferences,"The ability to predict the movements of the target person allows a person following robot (PFR) to coexist with the person while still complying with the social norms. In human-robot collaboration, this is an essential requisite for long-term time-dependent navigation and not losing sight of the person during momentary occlusions that may arise from a crowd due to static or dynamic obstacles, other human beings, or intersections in the local surrounding. The PFR must not only traverse to the previously unknown goal position but also relocate the target person after the miss, and resume following. In this paper, we try to solve this as a coupled motion-planning and control problem by formulating a model predictive control (MPC) controller with non-linear constraints for a wheeled differential-drive robot. And, using a human motion prediction strategy based on the recorded pose and trajectory information of both the moving target person and the PFR, add additional constraints to the same MPC, to recompute the optimal controls to the wheels. We make comparisons with RNNs like LSTM and Early Relocation for learning the best-predicted reference path.MPC is best suited for complex constrained problems because it allows the PFR to periodically update the tracking information, as well as to adapt to the moving person’s stride. We show the results using a simulated indoor environment and lay the foundation for its implementation on a real robot. Our proposed method offers a robust person following behaviour without the explicit need for policy learning or offline computation, allowing us to design a generalized framework.",https://ieeexplore.ieee.org/document/9515348/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00060,Towards a Music Visualization on Robot (MVR) Prototype,IEEE,Conferences,"This paper presents a Music Visualization on Robot (MVR) prototype system which automatically links the flashlight, color and emotion of a robot through music. The MVR system is divided into three portions. Firstly, the system calculates the waiting time for a flashlight by beat tracking. Secondly, the system calculates the emotion correlated with music mood. Thirdly, the system links the color with emotion. To illustrate the prototype on a robot, the prototype implementation is based on a programmable robot called Zenbo because Zenbo has 8 LED light colors on 2 wheels and 24 face emotions to support various compositions.",https://ieeexplore.ieee.org/document/8613679/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIPROCESS.2016.7888345,Towards robust ego-centric hand gesture analysis for robot control,IEEE,Conferences,"Wearable device with an ego-centric camera would be the next generation device for human-computer interaction such as robot control. Hand gesture is a natural way of ego-centric human-computer interaction. In this paper, we present an ego-centric multi-stage hand gesture analysis pipeline for robot control which works robustly in the unconstrained environment with varying luminance. In particular, we first propose an adaptive color and contour based hand segmentation method to segment hand region from the ego-centric viewpoint. We then propose a convex U-shaped curve detection algorithm to precisely detect positions of fingertips. And parallelly, we utilize the convolutional neural networks to recognize hand gestures. Based on these techniques, we combine most information of hand to control the robot and develop a hand gesture analysis system on an iPhone and a robot arm platform to validate its effectiveness. Experimental result demonstrates that our method works perfectly on controlling the robot arm by hand gesture in real time.",https://ieeexplore.ieee.org/document/7888345/,2016 IEEE International Conference on Signal and Image Processing (ICSIP),13-15 Aug. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCICT.2012.6398104,Tracking of a target person using face recognition by surveillance robot,IEEE,Conferences,"In this paper we designed an experimental setup in order to have human-robot interaction i.e. first we are going to detect the face and after that we recognise the detected face. Afterwards we get the persons upper body torso color as a key feature. As we extracted the color feature we can compute the moments and also evaluate the motion parameters so that the surveillance robot can track the person accordingly. We also had introduced Speech module in order to have a interaction between the remote and base station. Surveillance robot must track the targeted person in a robust manner in indoor and outdoor environment in different light and dynamic varying conditions. In our proposed setup we use PCA which is going to recognise the person in a real time environment and should communicate to the person via speech module deployed in the surveillance robot, as face recognition works on real time environment we are getting average recognition rate of 98%. Experiment demonstration validates the efficient performance of the approach.",https://ieeexplore.ieee.org/document/6398104/,"2012 International Conference on Communication, Information & Computing Technology (ICCICT)",19-20 Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2010.5509160,Transfer of skills between human operators through haptic training with robot coordination,IEEE,Conferences,"In this paper, we discuss a coordinated haptic training architecture useful for transferring expertise in teleoperation-based manipulation between two human users. The objective is to construct a reality-based haptic interaction system for knowledge transfer by linking an expert's skill with robotic movement in real time. The benefits from this approach include 1) a representation of an expert's knowledge into a more compact and general form by learning from a minimized set of training samples, and 2) an increase in the capability of a novice user by coupling learned skills absorbed by a robotic system with haptic feedback. In order to evaluate our ideas and present the effectiveness of our paradigm, human handwriting is selected as our experiment of interest. For the learning algorithms, artificial neural network (ANN) and support vector machine (SVM) are utilized and their performances are compared. For the evaluation of the performance of the output of the learning modules, a modified Longest Common Subsequence (LCSS) algorithm is implemented. Results show that one or two experts' samples are sufficient for the generation of haptic training knowledge, which can successfully recreate manipulation motion with a robotic system and transfer haptic forces to an untrained user with a haptic device. Also in the case of handwriting comparison, the similarity measures result in up to an 88% match even with a minimized set of training samples.",https://ieeexplore.ieee.org/document/5509160/,2010 IEEE International Conference on Robotics and Automation,3-7 May 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1998.680515,Unsupervised learning to recognize environments from behavior sequences in a mobile robot,IEEE,Conferences,"We describe the development of a mobile robot which does unsupervised learning for recognizing environments from behavior sequences. Most studies on recognizing an environment have tried to build precise geometric maps with high sensitive and global sensors. However such precise and global information may not be obtained in real environments. Furthermore unsupervised-learning is necessary for recognition in unknown environments without help of a teacher. Thus we attempt to build a mobile robot which does unsupervised-learning to recognize environments with low sensitivity and local sensors. The mobile robot is behavior-based and does wall-following in enclosures. Then the sequences of behaviors executed in each enclosure are transformed into input vectors for a self-organizing network. Learning without a teacher is done, and the robot becomes able to identify enclosures. Moreover we developed a method to identify environments independent of a start point using a partial sequence. We have fully implemented the system with a real mobile robot, and made experiments for evaluating the ability. As a result, we found out that the environment recognition was done well and our method was adaptive to noisy environments.",https://ieeexplore.ieee.org/document/680515/,Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146),20-20 May 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SPW50608.2020.00045,Using Taint Analysis and Reinforcement Learning (TARL) to Repair Autonomous Robot Software,IEEE,Conferences,"It is important to be able to establish formal performance bounds for autonomous systems. However, formal verification techniques require a model of the environment in which the system operates; a challenge for autonomous systems, especially those expected to operate over longer timescales. This paper describes work in progress to automate the monitor and repair of ROS-based autonomous robot software written for an apriori partially known and possibly incorrect environment model. A taint analysis method is used to automatically extract the dataflow sequence from input topic to publish topic, and instrument that code. A unique reinforcement learning approximation of MDP utility is calculated, an empirical and non-invasive characterization of the inherent objectives of the software designers. By comparing design (a-priori) utility with deploy (deployed system) utility, we show, using a small but real ROS example, that it's possible to monitor a performance criterion and relate violations of the criterion to parts of the software. The software is then patched using automated software repair techniques and evaluated against the original off-line utility.",https://ieeexplore.ieee.org/document/9283859/,2020 IEEE Security and Privacy Workshops (SPW),21-21 May 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HNICEM.2018.8666242,Utilization of Fuzzy Logic Control in a Waste Robot,IEEE,Conferences,"This research aimed to design and develop an autonomous robot to feasibly address waste disposal issues in common indoor places. The researchers explored opportunities to improve path planning using Fuzzy Logic Control (FLC). The researchers utilized a Microcontroller Unit (MCU) to control input proximity, sound, and infrared sensors, and output geared Direct Current (DC) motors through machine learning and electromechanical interface. The researchers simulated an adaptive algorithm using Mamdani-type FLC model, implemented using C programming language, then downloaded as machine code to a real prototype. Based on significant test results, the waste robot accurately detected human interference, a feature that would be pivotal in overcoming individual indifferences on waste management.",https://ieeexplore.ieee.org/document/8666242/,"2018 IEEE 10th International Conference on Humanoid, Nanotechnology, Information Technology,Communication and Control, Environment and Management (HNICEM)",29 Nov.-2 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2018.8526952,Vision-Based Line-Following Control of a Two-Wheel Self-Balancing Robot,IEEE,Conferences,"This paper presents a vision-based two-wheel self-balancing (TWSB) robot to follow a black line using visual feedback. We use MATLAB software to connect to the URL of IP camera and use image processing toolbox to process the image from the IP camera. After image processing, this paper sets 10 coordinates to detect if the black line is straight or the black line is in different kind of situation. This paper considers the black line including straight line, curve line, intersection and inconsecutive line. Thus, a cascade intelligent motion control system is proposed to control the balancing and moment of the vision-based TWSB robot with tracking the position and direction commands from MATLAB software. Finally, it shows that the vision-based TWSB robot can trace the black line on the map very well from the real-time experimental results.",https://ieeexplore.ieee.org/document/8526952/,2018 International Conference on Machine Learning and Cybernetics (ICMLC),15-18 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOVET50258.2020.9230275,Vision-Based Robot Hand Using Open Source Software,IEEE,Conferences,"The robot hand can plan grasp movements based on the position of the target object and the motion of the robot hand. The position of the target object is recognized from the image captured by a camera mounted on the robot arm, and the motion of the robot hand is estimated from the inertial measurement unit (IMU). We also adopt a variety of mechanisms to determine the target object among the objects detected in the camera scene. Experiments are conducted to verify the validity of control system. The experiments proved that the developed system can support the user to grasp the target object.",https://ieeexplore.ieee.org/document/9230275/,2020 4th International Conference on Vocational Education and Training (ICOVET),19-19 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FSKD.2017.8393254,Visual control system design of wheeled inverted pendulum robot based on Beaglebone Black,IEEE,Conferences,"The wheeled inverted pendulum robot has broad prospects of applications in real life. It can use two coaxial wheels to achieve the body self-balancing, forward moving and turning. But the general wheeled inverted pendulum robot seldom has vision function to perceive enviromental change. In order to realize the robust visual control, a wheeled inverted-pendulum vision robot with attitude sensors, photoelectric encoders, ultrasonic sensors and so on is designed based on Beaglebone Black board. The moving object is separated in the space domain by obtaining the image sequence which is sent by a robot-mounted camera, and the modeling, identification and tracking of target sequence are implemented in the time domain. The balance PD, speed PI and steering PD controllers are designed to realize the dynamic balance, forward and steering function of the robot. To satisfy the functional requirements of the visual tracking system, an improved tracking-learning-detection algorithm based on kernelized correlation filtering is used, and a tracking anomaly based on spatial context is detected to determine the tracking state and reduce the error rate. Experimental results show that the robot reaches the requirement of design and achieves better visual control effectiveness.",https://ieeexplore.ieee.org/document/8393254/,"2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)",29-31 July 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2006.53,Web Robot Learning Powered by Bluetooth Communication System,IEEE,Conferences,"This paper presents a web robot web-robot learning powered by Bluetooth communication system. The web-robot system is used as the virtual robot laboratory integrating a number of disciplines in engineering. This virtual laboratory is a valuable teaching tool for engineering education used at any time and from any location through Internet. The mobile robot was controlled with robot server named as control center. The server can be connected to mobile robot via Bluetooth adapter. The mobile robot system focuses on vision sensing. Real time image processing techniques are realized by the web robot system. This system can also realize monitoring, tele-controlling, parameter adjusting and reprogramming through Internet exclusively with a standard Web browser without the need of any additional software",https://ieeexplore.ieee.org/document/4041484/,2006 5th International Conference on Machine Learning and Applications (ICMLA'06),14-16 Dec. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2007.19,Web-based maze robot learning using fuzzy motion control system,IEEE,Conferences,"In this study, a Web based maze robot system has been designed and implemented for solving different maze algorithms with the help of machine learning approaches. The robot system has a map-based heuristic maze solving algorithm. The algorithm used for solving the maze is based on map creation and produces a control signal for robot direction. Robot motions were controlled by a fuzzy motion control system running on a chip. The control algorithm can be easily changed with the help of an algorithm via web interface controlled by the control center. The control center program powered by MATLAB functions and special libraries (image and control) in DELPHI manage all robotic activities. These activities are: command interpreter, image capturing, processing and serving, machine learning techniques, Web serving, database management, communication with robot, and compiling microcontroller programs. The results have shown that the proposed, designed and implemented system provides amazing new features to the applicants doing their real-time programming exercises on Web.",https://ieeexplore.ieee.org/document/4457243/,Sixth International Conference on Machine Learning and Applications (ICMLA 2007),13-15 Dec. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1996.506991,Fast connectionist learning for trailer backing using a real robot,IEEE,Conferences,This paper presents the application of a connectionist control-learning system to an autonomous mini-robot. The system's design is severely constrained by the computing power and memory available on board the mini-robot and the on-board training time is greatly limited by the short life of the battery. The system is capable of rapid unsupervised learning of output responses in temporal domains through the use of eligibility traces and data sharing within topologically defined neighborhoods.,https://ieeexplore.ieee.org/document/506991/,Proceedings of IEEE International Conference on Robotics and Automation,22-28 April 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR.2018.8621723,Fault-Tolerant and Self-Adaptive Market-Based Coordination Using Hoplites Framework for Multi-Robot Patrolling Tasks,IEEE,Conferences,"An autonomous robot team can be employed for continuous coverage of a dynamic environment. In this paper, we propose a novel approach for creating multi-robot patrolling policies, which is fault-tolerant and self-adaptive. A dynamic priority queue and time-out replanning mechanism are maintained by each robot to schedule the tasks fault-tolerantly in the context of the market-based method. Hoplites framework is adapted by introducing a self-adaptive threshold adjustment and sharing mechanism to provide a high-level coordination. This work is demonstrated by a multi-robot patrolling task implemented on Robot Operating System (ROS). A flexible tool, Stage, is leveraged to provide the simulated environment. The experimental results validate the effectiveness and availability.",https://ieeexplore.ieee.org/document/8621723/,2018 IEEE International Conference on Real-time Computing and Robotics (RCAR),1-5 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII.2011.6147520,Forming an artificial pheromone potential field using mobile robot and RFID tags,IEEE,Conferences,"In the biological world, social insects such as ants and bees use a volatile substance called pheromone for their foraging or homing tasks. This study deals with how to utilize the concept of the chemical pheromone as an artificial potential field for robotic purposes. This paper first models a pheromone-based potential field, which is constructed through the interaction between mobile robot and RFID tags. The emphasis in the modeling of the system is on the possibility of the practical implementable ideas. The stability analysis of the pheromone potential field is carried out with the aim of implementing the model on a real robotic system. The comprehensive analysis on stability provides the criteria for how the parameters are to be set for the proper potential field, and has also led to a new filter design scheme called pheromone filter. The designed filter satisfies both the stability and accuracy of the field, and facilitates a more straightforward and practical implementation for building and shaping the potential field. The effectiveness of the proposed algorithm is validated through both computer simulation and real experiment.",https://ieeexplore.ieee.org/document/6147520/,2011 IEEE/SICE International Symposium on System Integration (SII),20-22 Dec. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSCT53883.2021.9642700,Fruits Detection and Distance Estimation using RGB-D camera for Harvesting Robot,IEEE,Conferences,"In this paper, fruit detection and distance estimation from detected fruits are processed in parallel based on a low-cost and compact RGB-D camera. Fruits are detected in RGB images and surrounded by bounding boxes using modified deep-learning models, which are optimized for high accuracy based on limited computational resources. Distances from the detected fruits to the camera as well as the size of those fruits are estimated based on their respective boxes in depth images, which provided third-dimensional knowledge or 3D localization of the fruits. The proposed method is implemented on an embedded computer connected to an RGB-D camera and validated in the real environment. Experimental results show that high accuracy in detection, distance, and fruit’s size estimation has been achieved. The promising results enable the further grasping action for a harvesting robot.",https://ieeexplore.ieee.org/document/9642700/,2021 International Conference on Science & Contemporary Technologies (ICSCT),5-7 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2017.8172498,Functional imitation task in the context of robot-assisted Autism Spectrum Disorder diagnostics: Preliminary investigations,IEEE,Conferences,"This paper presents a functional imitation task aimed at facilitating Autism Spectrum Disorder (ASD) diagnostics in children. Imitation plays a key role in the development of social skills at a young age, and studies have shown that the ability to imitate is impaired in children with ASD. Therefore, we expect imitation-based tasks to have diagnostic value. In this paper, we introduce two novel elements of human-robot interaction in the context of autism diagnostics. Instead of pure motoric imitation, we propose imitation tasks involving real objects in the environment. The introduction of physical objects strongly emphasizes joint attention skills, another area that is typically impaired in children with ASD. Furthermore, we present simple object detection, manipulation, tracking and gesture recognition algorithms, suitable for real-time, onboard execution on the small-scale humanoid robot NAO. The proposed system paves the way for fully autonomous execution of diagnostic tasks, which would simplify the deployment of robotic assistants in clinical settings. The source code for all described functionalities has been made publicly available as open-source software. We present a preliminary evaluation of the proposed system with a control group of typically developing preschool children and a group of seven children diagnosed with ASD.",https://ieeexplore.ieee.org/document/8172498/,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),28 Aug.-1 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MMAR.2019.8864671,Fusion of Gesture and Speech for Increased Accuracy in Human Robot Interaction,IEEE,Conferences,"An approach for decision-level fusion for gesture and speech based human-robot interaction (HRI) is proposed. A rule-based method is compared with several machine learning approaches. Gestures and speech signals are initially classified using hidden Markov models, reaching accuracies of 89.6% and 84% respectively. The rule-based approach reached 91.6% while SVM, which was the best of all evaluated machine learning algorithms, reached an accuracy of 98.2% on the test data. A complete framework is deployed in real time humanoid robot (NAO) which proves the efficacy of the system.",https://ieeexplore.ieee.org/document/8864671/,2019 24th International Conference on Methods and Models in Automation and Robotics (MMAR),26-29 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM.2009.5229761,Fuzzy and Neural controllers for acute obstacle avoidance in mobile robot navigation,IEEE,Conferences,"Robot navigation is the technique to guide the mobile robot move towards the desired goal where dynamic and unknown environment is involved. The environment is distinguished by variable terrain and also certain objects which are known as obstacles that may block the movement of the robot in reaching the desired destination. Fuzzy Logic (FL) and Artificial Neural Network (ANN) are used to assist autonomous mobile robot move, learn the environment and reach the desired goal. This research study is focused on exploring the four combinations of training algorithms composed of FL and ANN that avoid acute obstacles in the environment. Path Remembering algorithm proposed in this paper will assist the mobile robot to come out from acute obstacles. Virtual wall building method also is proposed in order to prevent the mobile robot reentering the same acute obstacle once it has been turned away from the wall. MATLAB simulation is developed to verify and validate the algorithms before they are implemented in real time on Team AmigoBot™ robot. The results obtained from both simulation and actual application confirmed the flexibility and robustness of the controllers designed in avoiding acute obstacles and a comparison of all the four combinations of algorithms is done to find the best combination of algorithms to perform the required navigation to avoid acute obstacles.",https://ieeexplore.ieee.org/document/5229761/,2009 IEEE/ASME International Conference on Advanced Intelligent Mechatronics,14-17 July 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196608,Gershgorin Loss Stabilizes the Recurrent Neural Network Compartment of an End-to-end Robot Learning Scheme,IEEE,Conferences,"Traditional robotic control suits require profound task-specific knowledge for designing, building and testing control software. The rise of Deep Learning has enabled end-to-end solutions to be learned entirely from data, requiring minimal knowledge about the application area. We design a learning scheme to train end-to-end linear dynamical systems (LDS)s by gradient descent in imitation learning robotic domains. We introduce a new regularization loss component together with a learning algorithm that improves the stability of the learned autonomous system, by forcing the eigenvalues of the internal state updates of an LDS to be negative reals. We evaluate our approach on a series of real-life and simulated robotic experiments, in comparison to linear and nonlinear Recurrent Neural Network (RNN) architectures. Our results show that our stabilizing method significantly improves test performance of LDS, enabling such linear models to match the performance of contemporary nonlinear RNN architectures. A video of the obstacle avoidance performance of our method on a mobile robot, in unseen environments, compared to other methods can be viewed at https://youtu.be/mhEsCoNao5E.",https://ieeexplore.ieee.org/document/9196608/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2009.5326235,Gestural teleoperation of a mobile robot based on visual recognition of sign language static handshapes,IEEE,Conferences,"This paper presents results achieved in the frames of a national research project (titled ldquoDIANOEMArdquo), where visual analysis and sign recognition techniques have been explored on Greek Sign Language (GSL) data. Besides GSL modelling, the aim was to develop a pilot application for teleoperating a mobile robot using natural hand signs. A small vocabulary of hand signs has been designed to enable desktopbased teleoperation at a high-level of supervisory telerobotic control. Real-time visual recognition of the hand images is performed by training a multi-layer perceptron (MLP) neural network. Various shape descriptors of the segmented hand posture images have been explored as inputs to the MLP network. These include Fourier shape descriptors on the contour of the segmented hand sign images, moments, compactness, eccentricity, and histogram of the curvature. We have examined which of these shape descriptors are best suited for real-time recognition of hand signs, in relation to the number and choice of hand postures, in order to achieve maximum recognition performance. The hand-sign recognizer has been integrated in a graphical user interface, and has been implemented with success on a pilot application for real-time desktop-based gestural teleoperation of a mobile robot vehicle.",https://ieeexplore.ieee.org/document/5326235/,RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication,27 Sept.-2 Oct. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635831,Guiding Robot Model Construction with Prior Features,IEEE,Conferences,"Virtually all robot control methods benefit from the availability of an accurate mathematical model of the robot. However, obtaining a sufficient amount of informative data for constructing dynamic models can be difficult, especially when the models are to be learned during robot deployment. Under such circumstances, standard data-driven model learning techniques often yield models that do not comply with the physics of the robot. We extend a symbolic regression algorithm based on Single Node Genetic Programming by including the prior model information into the model construction process. In this way, symbolic regression automatically builds models that compensate for theoretical or empirical model deficiencies. We experimentally demonstrate the approach on two real-world systems: the TurtleBot 2 mobile robot and the Parrot Bebop 2 drone. The results show that the proposed model-learning algorithm produces realistic models that fit well the training data even when using small training sets. Passing the prior model information to the algorithm significantly improves the model accuracy while speeding up the search.",https://ieeexplore.ieee.org/document/9635831/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561034,High-Speed Robot Navigation using Predicted Occupancy Maps,IEEE,Conferences,"Safe and high-speed navigation is a key enabling capability for real world deployment of robotic systems. A significant limitation of existing approaches is the computational bottleneck associated with explicit mapping and the limited field of view (FOV) of existing sensor technologies. In this paper, we study algorithmic approaches that allow the robot to predict spaces extending beyond the sensor horizon for robust planning at high speeds. We accomplish this using a generative neural network trained from real-world data without requiring human annotated labels. Further, we extend our existing control algorithms to support leveraging the predicted spaces to improve collision-free planning and navigation at high speeds. Our experiments are conducted on a physical robot based on the MIT race car using an RGBD sensor where were able to demonstrate improved performance at 4 m/s compared to a controller not operating on predicted regions of the map.",https://ieeexplore.ieee.org/document/9561034/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV.2010.55,Human Upper Body Pose Recognition Using Adaboost Template for Natural Human Robot Interaction,IEEE,Conferences,"In this paper, we propose a novel Adaboost template to recognize human upper body poses from disparity images for natural human robot interaction (HRI). First, the upper body poses of standing persons are classified into seven categories of views. For each category, a mean template, variance template, and percentage template are generated. Then, the template region is divided into positive and negative regions, corresponding to the region of bodies and surrounding open space. A weak classifier is designed for each pixel in the template. A new EM-like Adaboost learning algorithm is designed to learn the Adaboost template. Different from existing Adaboost classifiers, we show that the Adaboost template can be used not only for recognition but also for adaptive top-down segmentation. By using Adaboost template, only a few positive samples for each category are required for learning. Comparison with conventional template matching techniques has been made. Experimental results show that significant improvements can be achieved in both cases. The method has been deployed in a social robot to estimate human attentions to the robot in real-time human robot interaction.",https://ieeexplore.ieee.org/document/5479162/,2010 Canadian Conference on Computer and Robot Vision,31 May-2 June 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/YAC53711.2021.9486647,Human-Robot Interaction System Design for Manipulator Control Using Reinforcement Learning,IEEE,Conferences,"In this article, a novel human-robot interaction (HRI) system is presented and applied in the robotic arm coordinated operation control task. The presented HRI system includes two parts, the impedance model controller and the robotic arm controller, which allows the operator to manipulate the robotic arm to accomplish the given task with minimal human effort. First, the model-based reinforcement learning (RL) method is applied in the impedance model for operator adaptation. The impedance model controller can transform human input into the specific signal for the manipulator. Second, a novel adaptive manipulator controller is designed. In contrast to existing controllers, a velocity-free filter is implemented in our controller, which is developed to replace the manipulator actuator's speed signal. The effectiveness of the presented HRI system is verified by the simulation based on real manipulator parameters.",https://ieeexplore.ieee.org/document/9486647/,2021 36th Youth Academic Annual Conference of Chinese Association of Automation (YAC),28-30 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EPIA.2005.341221,Hybrid State Machines with Timed Synchronization for Multi-Robot System Specification,IEEE,Conferences,"In multi-robot systems, the need for precise modeling or specification of agent behaviors arises due to the high complexity of the robot agent interactions and the dynamics of the environment. Since the behavior of agents usually can be understood as driven by external events and internal states, it is obvious to model multiagent systems by state transition diagrams. The corresponding formalisms come equipped with a formal semantics which is advantageous. In this paper, a combination of UML statecharts and hybrid automata is proposed, allowing formal system specification on different levels on abstraction on the one hand, and expressing real-time system behavior with continuous variables on the other hand. One important aspect of multi-robot systems is the need of coordination and hence synchronization of behavior. For both, statecharts and hybrid automata, it is assumed that synchronization takes zero time. This is sometimes unrealistic. Therefore, a new notation and implementation of synchronization is proposed here, which overcomes this problem. The proposed method is illustrated with a case study from the robotic soccer domain",https://ieeexplore.ieee.org/document/4145962/,2005 portuguese conference on artificial intelligence,5-8 Dec. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSESS.2011.5982262,Implement the TABLE TENNIS game by robot arm with forward-backward neural network,IEEE,Conferences,"In this paper, implementing the table tennis game by robot arm with forward-backward neural network has been investigated. The main contributions have been made as the followings, (1) Based on 2-dimensions (2-D), both ideal model and improved model have been developed, creating a Ping-pong game by employing Visual C++. The example provide by this paper has shown that the two new model operate very well. (2) The 2-D approach for the game has been proposed and the calculation formulas of this approach have been strictly proofed by employing the small amplitude disturbed theory. (3) A novel 3-D approach has also been put up forwarded. By using 3-D approach, one can implement the game in 3 dimensions; it acts as a real game. Finally, the proposed 2-D and 3-D approaches are not only used in the Ping-pong game, can also be exploited to a motion target tracking such as a Ballistics Missile Defense System.",https://ieeexplore.ieee.org/document/5982262/,2011 IEEE 2nd International Conference on Software Engineering and Service Science,15-17 July 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iFuzzy.2013.6825409,Implementation of human following mission by using fuzzy head motion control and Q-learning wheel motion control for home service robot,IEEE,Conferences,"This paper mainly implements human following function for home service robot, May, developed in our laboratory. In order to follow the operator accurately, visual tracking is composed by Tracing-Learning-Detection (TLD) and Kinect skeleton, where TLD plays the role as re-detecting the situation that operator is occluded or disappeared, and Kinect skeleton is adopted to track all other situations while TLD is learning how to enlarge operator image patterns in order to enhance recognition rates. For the sake of improving tracking capability, fuzzy head motion control is added in the visual tracking system to compensate the constraints that the mobile platform of May cannot react rapidly. Every instant movement of the operator can be captured by fuzzy head motion control in real time. Q-learning is applied to discover the pose switching of the mobile platform such that May possesses more robust following ability. By Q-learning, states setting are based on three dimensional position, actions are created by the pose of four wheel independent steering and four wheel independent driven (4WIS4WID) platform, and rewards are established on state transitions. Finally, both the experimental results in the laboratory and competition consequents of Follow Me Mission in robot@home league at RoboCup Japan Open 2013 Tokyo demonstrate that our robot May can fluently switch its poses to follow operator by utilizing the proposed schemes.",https://ieeexplore.ieee.org/document/6825409/,2013 International Conference on Fuzzy Theory and Its Applications (iFUZZY),6-8 Dec. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INES.1997.632397,Implementation of neural network sliding-mode controller for DD robot,IEEE,Conferences,"The experimental development of a trajectory tracking neural network controller based on the theory of continuous sliding-mode controllers is shown in the paper. The neural network control law was verified on a real direct drive 3 DOF PUMA mechanism. The new neural network sliding-mode controller was successfully tested for trajectory tracking sudden changes in the manipulator dynamics (load). The comparision between the neural network sliding mode controller, a computer torque method controller and a continuous sliding mode controller with PI-estimator for sudden load changes on the real robot mechanism is shown.",https://ieeexplore.ieee.org/document/632397/,Proceedings of IEEE International Conference on Intelligent Engineering Systems,17-17 Sept. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CT.2014.7092212,Implementation of synthetic brain concept in humanoid robot,IEEE,Conferences,"This paper is elaborate the model of humanoid robot interacts with human being and perform various operation as per the command given by the human being. A humanoid robot having Synthetic brain can able to do Interaction, communication, Object detection, information acquisition about any object, response to voice command, chatting logically with human beings. Object detection will be done by this robot for that purpose there is use image processing concept (HAAR Technique), And to make the system intelligent that is whenever system interact, communicate, chat with human it gives proper response, question / answers there is integrates artificial intelligence and DFA / NFA automata and Prolog language concept for answering logically over the complex and relevant strings or data.",https://ieeexplore.ieee.org/document/7092212/,International Conference for Convergence for Technology-2014,6-8 April 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COGINF.2011.6016164,Improved mobile robot's Corridor-Scene Classifier based on probabilistic Spiking Neuron Model,IEEE,Conferences,"The ability of cognition and recognition for complex environment is very important for a real autonomous robot. A improved Corridor-Scene-Classifier based on probabilistic Spiking Neuron Model(pSNM) for mobile robot is designed. In the SNN classifier, the model pSNM is used. As network's training, Thorpe's learning rule is used. The experimental results show that the improved Classifier is more effective and it also has stronger robustness than the previous classifier based on Integrated-and-Fire (IAF) spiking neuron model for the structural corridor-scene. It also has better robustness than the traditional kernel-pca and the BP Corridor-Scene-classifier.",https://ieeexplore.ieee.org/document/6016164/,IEEE 10th International Conference on Cognitive Informatics and Cognitive Computing (ICCI-CC'11),18-20 Aug. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ELECSYM.2018.8615506,Improving Field and Ball Detector for Humanoid Robot Soccer EROS Platform,IEEE,Conferences,"Humanoid robot soccer perceives environment mostly through cameras. The performance decrement in our humanoid soccer platform (EROS) is primarily due to the visual perception that is less robust to the RoboCup new rule which specifically reducing color coding in the field. Notable works favorably employ simple color segmentation, image morphology, and blob detector due to simplicity in the implementation and run in real-time for most embedded hardware, while some employ a more advanced supervised learning running in sophisticated hardware to boost detection accuracy. In this paper, a visual perception system consisting of field and ball detection is developed in our platform EROS to address the RoboCup new rule. Color segmentation and image morphology are stacked with a more advanced supervised learning cascade classifier. In this way, the favorable color segmentation and image morphology help to reduce the number of object candidates while the cascade classifier helps to boost the accuracy of detection. Experiments show encouraging result for detecting field and ball position. Our approach has successfully been implemented in practice and achieves remarkably result in Indonesian humanoid robot soccer competition.",https://ieeexplore.ieee.org/document/8615506/,2018 International Electronics Symposium on Engineering Technology and Applications (IES-ETA),29-30 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRW55335.2022.00107,Indirect Robot Manipulation using Eye Gazing and Head Movement For Future of Work in Mixed Reality,IEEE,Conferences,"Most robot manipulations are guided by the worker&#x0027;s both hands using a teach-pendant or other gadgets. However, manipulating the robot is more difficult when both hands are occupied, which requires a new manipulation method. This study proposes a novel indirect robot interaction and manipulation method using eye gaze and head movement in mixed reality. The proposed approach has been implemented to provide hands-free robot interaction for pick-and-place tasks. The results show a promising direction for the future of work in human-robot collaboration.",https://ieeexplore.ieee.org/document/9757386/,2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),12-16 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1992.219999,Integrated planning and execution control of autonomous robot actions,IEEE,Conferences,"The authors describe an implemented integrated system allowing a mobile robot to plan its actions, taking into account temporal constraints, and to control their execution in real time. The general architecture has three levels, and the approach is related to hierarchical planning: the plan produced by the temporal planner is further refined at the control level, which in turn supervises its execution by a functional level. The framework of the French Mars Rover project VAP is used as an illustration of the various aspects discussed.<>",https://ieeexplore.ieee.org/document/219999/,Proceedings 1992 IEEE International Conference on Robotics and Automation,12-14 May 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.1997.613877,Intelligence computing for direct human-robot communication using natural language and cognitive graphics,IEEE,Conferences,"A direct human-robot communication system based on natural language (NL) and cognitive graphics (CG) as a part of a new hierarchical structure of an AI control system of a mobile robot for service use (MRSU) is developed. The software of the simulation system for direct human-robot communication and MRSU behavior based on NL and CG is described. The NL and CG are used for description and representation of possible external worlds in the robot artificial life. This system allows us to evaluate the control algorithms of real time robot behavior and to reduce difficulties connected with such troubles as robot collisions with real objects and robot hardware damage. The mathematical background of direct NL human-robot communication and robot behavior simulation system is knowledge engineering based on spatio-temporal and action logics, default reasoning, cognitive graphics and soft computing. The main concepts, structure, conceptual model of the simulation system for description of the artificial life of the MRSU are discussed. A simulation example and real experimental results are described.",https://ieeexplore.ieee.org/document/613877/,Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation',10-11 July 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SKIMA.2014.7083542,Intelligent Appearance and shape based facial emotion recognition for a humanoid robot,IEEE,Conferences,"In this paper, we present an intelligent facial emotion recognition system with real-time face tracking for a humanoid robot. The system is able to detect facial actions and emotions from images with up to 60 degrees of pose variations. We employ the Active Appearance Model to perform real-time face tracking and extract both texture and geometric representations of images. A POSIT algorithm is also used to identify head rotations. The extracted texture and shape features are employed to detect 18 facial actions and seven basic emotions. The overall system is integrated with a humanoid robot platform to further extend its vision APIs. The system is proved to be able to deal with challenging facial emotion recognition tasks with various pose variations.",https://ieeexplore.ieee.org/document/7083542/,"The 8th International Conference on Software, Knowledge, Information Management and Applications (SKIMA 2014)",18-20 Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ARIS50834.2020.9205772,Intelligent Robot for Worker Safety Surveillance: Deep Learning Perception and Visual Navigation,IEEE,Conferences,"The fatal injury rate for the construction industry is higher than the average for all industries. Recently, researchers have shown an increased interest in occupational safety in the construction industry. However, all the current methods using conventional machine learning with stationary cameras suffer from some severe limitations, perceptual aliasing (e.g., different places/objects can appear identical), occlusion (e.g., place/object appearance changes between visits), seasonal / illumination changes, significant viewpoint changes, etc. This paper proposes a perception module using end-to-end deep-learning and visual SLAM (Simultaneous Localization and Mapping) for an effective and efficient object recognition and navigation using a differential-drive mobile robot. Various deep-learning frameworks and visual navigation strategies with evaluation metrics are implemented and validated for the selection of the best model. The deep-learning model's predictions are evaluated via the metrics (model speed, accuracy, complexity, precision, recall, P-R curve, F1 score). The YOLOv3 shows the best trade-off among all algorithms, 57.9% mean average precision (mAP), in real-world settings, and can process 45 frames per second (FPS) on NVIDIA Jetson TX2 which makes it suitable for real-time detection, as well as a right candidate for deploying the neural network on a mobile robot. The evaluation metrics used for the comparison of laser SLAM are Root Mean Square Error (RMSE). The Google Cartographer SLAM shows the lowest RMSE and acceptable processing time. The experimental results demonstrate that the perception module can meet the requirements of head protection criteria in Occupational Safety and Health Administration (OSHA) standards for construction. To be more precise, this module can effectively detect construction worker's non-hardhat-use in different construction site conditions and can facilitate improved safety inspection and supervision.",https://ieeexplore.ieee.org/document/9205772/,2020 International Conference on Advanced Robotics and Intelligent Systems (ARIS),19-21 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1994.407388,Intelligent robot hand control system using a tailorable parallel computer concept,IEEE,Conferences,"Presents a control system for the intelligent force control of multifingered robot grips. The multilevel system architecture combines both a fuzzy-based adaptation level and a neural-based one with a conventional PID-controller that allows autonomous performance of fine manipulations of an object. Two kinds of fuzzy controllers are presented. They use a decision making logic which expresses the a priori knowledge about the grasp force behaviour inside the friction cones and the necessary reactions regarding the criterion for grip stability. A neural controller, based on a Hooke-Jeeves optimisation approach, has been developed as well. The neural control algorithm is implemented by a three-layered backpropagation neural network. The neural and fuzzy controllers can be used separately or in parallel. In the last case the neural controller can be on-line trained using the input-output information from the fuzzy one. A computer based simulation system for the peg-in-hole insertion task is developed to analyse and to compare the capabilities of both control algorithms. The demands of flexibility and real-time control of the implementation of the control system are suited by a tailorable parallel computer concept. The two basic ideas of the concept are to set up each processing element individually for its application and connect these elements with different communicational methods according to the applicational demands. As this happens before runtime the concept is called static flexibility and is implemented using a new mechanical computer structure.<>",https://ieeexplore.ieee.org/document/407388/,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS'94),12-16 Sept. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2005.1545188,Interactive evolution of human-robot communication in real world,IEEE,Conferences,"This paper describes how to implement interactive evolutionary computation (IEC) into a human-robot communication system. IEC is an evolutionary computation (EC) in which the fitness function is performed by human assessors. We used IEC to configure the human-robot communication system. We have already simulated IEC's application. In this paper, we implemented IEC into a real robot. Since this experiment leads considerable burdens on both the robot and experimental subjects, we propose the human-machine hybrid evaluation (HMHE) to increase the diversity within the genetic pool without increasing the number of interactions. We used a communication robot, WAMOEBA-3 (Waseda artificial mind on emotion base), which is appropriate for this experiment. In the experiment, human assessors interacted with WAMOEBA-3 in various ways. The fitness values increased gradually, and assessors felt the robot learnt the motions they desired. Therefore, it was confirmed that the IEC is most suitable as the communication learning system.",https://ieeexplore.ieee.org/document/1545188/,2005 IEEE/RSJ International Conference on Intelligent Robots and Systems,2-6 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2003.1223995,Investigating models of social development using a humanoid robot,IEEE,Conferences,"Human social dynamics rely upon the ability to correctly attribute beliefs, goals, and percepts to other people. The set of abilities that allow an individual to infer these hidden mental states based on observed actions and behavior has been called a ""theory of mind"". Drawing from the models of Baron-Cohen (1995) and Leslie (1994), a novel architecture called embodied theory of mind was developed to link high-level cognitive skills to the low-level perceptual abilities of a humanoid robot. The implemented system determines visual saliency based on inherent object attributes, high-level task constraints, and the attentional states of others. Objects of interest are tracked in real-time to produce motion trajectories which are analyzed by a set of naive physical laws designed to discriminate animate from inanimate movement. Animate objects can be the source of attentional states (detected by finding faces and head orientation) as well as intentional states (determined by motion trajectories between objects). Individual components are evaluated by comparisons to human performance on similar tasks, and the complete system is evaluated in the context of a basic social learning mechanism that allows the robot to mimic observed movements.",https://ieeexplore.ieee.org/document/1223995/,"Proceedings of the International Joint Conference on Neural Networks, 2003.",20-24 July 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMSEC.2011.6009874,Kinematics simulation of upper limb rehabilitant robot based on virtual reality techniques,IEEE,Conferences,"The wearable exoskeletal robot for upper extremity rehabilitation is taken as the research object. According to D-H method, an accurate three-dimensional mechanism model for the robot system is established by SolidWorks software. The virtual set was generated in Simulink/VRML to carry out dynamic simulation. The variable parameters were set based on robotic practical joint range movement. The simulation of all joints and terminal trajectory and space motion area provided theoretical basis for position control, remote control and trajectory planning, realizing the rehabilitation robot visualizations and system interaction.",https://ieeexplore.ieee.org/document/6009874/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2007.4522258,Layered omnidirectional walking controller for the humanoid soccer robot,IEEE,Conferences,"This paper proposes the layered omnidirectional walking controller for the humanoid soccer robot. The gait of the robot can be parameterized using the destination posititon and the desired direction while reaching the destination. Its implementation in our RoboCup simulation team — SEU-3D is detailed in this paper. Our approach generates smooth robot trajectories without stop before changing direction or turning, and is fast enough to meet the real-time requirements. The proposed approach has been tested in the RoboCup soccer 3D server platform. The results showed that omnidirectional walking has advantages in dynamic environments.",https://ieeexplore.ieee.org/document/4522258/,2007 IEEE International Conference on Robotics and Biomimetics (ROBIO),15-18 Dec. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTC49870.2020.9289214,Learning Control Policy with Previous Experiences from Robot Simulator,IEEE,Conferences,"Advances in deep reinforcement learning enabled cost-efficient training of control policy of physical robot actions from robot simulators. Learning control policy in a simulated environment is cost-efficient over learning in a real environment. Reward engineering is one of the key components to train efficient control policy. For tasks with long horizons such as navigation and manipulation, a sparse reward is providing limited information. The robot simulator for a physical engine of physical robot manipulation has made it easy for researchers in the field of deep reinforcement learning to simulate complicated robot manipulation environments. In this paper, A robot manipulation simulator and a deep RL framework are utilized for implement a training control policy by utilizing previous experiences. For implementation, Recent innovation Hindsight Experience Replay (HER) algorithms with previous experiences to calculate dense rewards from a sparse reward is leveraged . Proposed implementation showed an approach to investigate the reward engineering method to formulate dense reward in robot manipulator tasks.",https://ieeexplore.ieee.org/document/9289214/,2020 International Conference on Information and Communication Technology Convergence (ICTC),21-23 Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812011,Learning Crowd-Aware Robot Navigation from Challenging Environments via Distributed Deep Reinforcement Learning,IEEE,Conferences,"This paper presents a deep reinforcement learning (DRL) sframework for safe and efficient navigation in crowded environments. Here, the robot learns cooperative behavior using a new reward function that penalizes robot actions interfering with the pedestrian&#x0027;s movement. Also, we propose a simulated pedestrian policy reflecting data from actual pedestrian movements. Furthermore, we introduce a collision detection that considers the pedestrian&#x0027;s personal space to generate affinity robot behavior. To efficiently explore this simulation environment, we propose distributed learning using Ape-X [1]. We deployed the robot in a real environment and verified its crowd-aware navigation performance compared with an actual human in terms of path length, travel time, and the number of abrupt avoidances.",https://ieeexplore.ieee.org/document/9812011/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2017.8324818,Learning complex assembly skills from kinect based human robot interaction,IEEE,Conferences,"Acquiring complex assembly skills is still a challenging task for robot programming. Because of the sensory and body structure differences, the human knowledge has to be demonstrated, recorded, converted and finally learned by the robot, in an inexplicit and indirect way. During this process, “how to demonstrate”, “how to convert” and “how to learn” are the key problems. In this paper, Kinect sensor is utilized to provide the behavior information of the human demonstrator. Through natural human robot interaction, body skeleton and joint 3D coordinates are provided in real-time, which can fully describe the human intension and task related skills. To overcome the structural and individual differences, a Cartesian level unified mapping method is proposed to convert the human motion and match the specified robot. The recorded data set are modeled using Gaussian mixture model(GMM) and Gaussian mixture regression(GMR), which can extract redundancies across multiple demonstrations and build robust models to regenerate the dynamics of the recorded movements. The proposed methodologies are implemented in the imNEU humanoid robot platform. Experimental results verify the effectiveness.",https://ieeexplore.ieee.org/document/8324818/,2017 IEEE International Conference on Robotics and Biomimetics (ROBIO),5-8 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1994.374669,Learning dynamic balance of a biped walking robot,IEEE,Conferences,"This paper discusses the application of CMAC (cerebellar model arithmetic computer) neural networks to the problem of biped walking with dynamic balance. The project goal is to develop biped control strategies based on a hierarchy of simple gait oscillators, PID controllers and neural network learning, but requiring no detailed dynamic models. The focus of this report is on real-time control studies using a ten axis biped robot with joint position, foot force and body acceleration sensors. While efficient walking has not yet been achieved, the experimental biped has learned the closed chain kinematics necessary to shift body weight from side-to-side while maintaining good foot contact and has learned the dynamic balance required in order to lift a foot off the floor for a desired length of time, during which the foot can be moved to a new location relative to the body. Using these skills, the biped is able to link short steps without falling.<>",https://ieeexplore.ieee.org/document/374669/,Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94),28 June-2 July 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1993.716991,Learning goal-directed navigation as attractor dynamics for a sensory motor system. (An experiment by the mobile robot YAMABICO),IEEE,Conferences,"This paper describes experimental results based on the authors' prior-proposed scheme: learning of sensory-based, goal-directed behavior. The scheme was implemented on the mobile robot ""YAMABICO"" and learning of a set of goal-directed navigations were conducted. The experiment assumed that the robot receives no global information such as position nor prior environment model. Instead, the robot was trained to learn adequate maneuvering in the adopted workspace by building a correct mapping between a spatio-temporal sequence of sensory inputs and maneuvering outputs on a neural structure. The experimental results showed that sufficient training generated rigid dynamical structure of a fixed point and limit cycling in the sensory-based state space, which realized robust navigations of homing and cyclic routing even against certain changes of environment as well as miscellaneous noises in the real world.",https://ieeexplore.ieee.org/document/716991/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2009.5420526,Learning-based action planning for real-time robot telecontrol with binocular vision in enhanced reality environment,IEEE,Conferences,"Action planning is one of the pivot issues in robot telecontrol, in which the action instructions are often given by the controller from remote site with the help of vision systems. In this paper, we present a learning-based strategy for action planning in robot telecontrol, in which the parameters of sophisticated actions of the remote robot equipped with a binocular vision system could be pre-scheduled with a virtual robot at the control terminal. The remote robot will then be 'taught' with the scheduled action plan with a series of parameter sets obtained form try-outs with the virtual robot and object in the enhanced environment, thus implementing dedicated actions assigned correctly. The action planning process is implemented within a enhanced reality environment, in which both the virtual and the real robot will be displayed simultaneously for the purpose of being deeply immersed. Experiment results demonstrate that the proposed method is capable of promoting the action precision of the remote robot, and effective and valid to designated applications, where action precision plays a critical role.",https://ieeexplore.ieee.org/document/5420526/,2009 IEEE International Conference on Robotics and Biomimetics (ROBIO),19-23 Dec. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ChiCC.2018.8483251,Local Gaussian Processes for Identifying Complex Mobile robot System,IEEE,Conferences,"Nonparametric Gaussian processes regression (GPR) is an important tool in machine learning, can be applied in identifying nonlinear models from experimental data, especially, the prediction of mean and variance present the useful advantage. However, when dealing with the large number of training data for the prediction of a complex dynamics system, GPR is not suitable to implement in real-time learning systems. To reduce the computation effort, local learning algorithm is introduced to improve the global Gaussian processes (GP) model in this paper. In this paper, a convenient and effective method for building local model network is proposed and then local GP for weighted regression is performed. The proposed local GPR method is implemented on a simulated example of online identification and prediction fast for a complex dynamic system of wheeled mobile robot.",https://ieeexplore.ieee.org/document/8483251/,2018 37th Chinese Control Conference (CCC),25-27 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSE.2014.6926425,MKL-SVM-based human detection for autonomous navigation of a robot,IEEE,Conferences,"This paper presents a classifier trained by a multiple kernel-learning support vector machine (MKL-SVM) to detect a human in sequential images from a video stream. The developed method consists of two aspects: multiple features consisting of HOG features and HOF features suitable for moving objects, and combined nonlinear kernels for SVM. For the purpose of real time application in autonomous navigation, the SimpleMKL algorithm is implemented into the proposed MKL-SVM classifier. It is able to converge rapidly with comparable efficiency through a weighted 2-norm regularization formulation with an additional constraint on the weights. The classifier is compared with the state-of-the-art linear SVM using a dataset called TUD-Brussels, which is available on line. The results show that the proposed classifier outperforms the Linear SVM with respect to accuracy.",https://ieeexplore.ieee.org/document/6926425/,2014 9th International Conference on Computer Science & Education,22-24 Aug. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV.2009.5403083,ML-fusion based multi-model human detection and tracking for robust human-robot interfaces,IEEE,Conferences,"A novel stereo vision system for real-time human detection and tracking on a mobile service robot is presented in this paper. The system integrates the individually enhanced stereo-based human detection, HOG-based human detection, color-based tracking, and motion estimation for the robust detection and tracking of humans with large appearance and scale variations in real-world environments. A new framework of maximum likelihood based multi-model fusion is proposed to fuse these four human detection and tracking models according to the detection-track associations in 3D space, which is robust to the possible missed detections, false detections, and duplicated responses from the individual models. Multi-person tracking is implemented in a sequential near-to-far way, which well alleviates the difficulties caused by human-over-human occlusions. Extensive experimental results demonstrate the robustness of the proposed system under real-world scenarios with large variations in lighting conditions, cluttered backgrounds, human clothes and postures, and complex occlusion situations. Significant improvements in human detection and tracking have been achieved. The system has been deployed on six robot butlers to serve drinks, and showed encouraging performance in open ceremony events.",https://ieeexplore.ieee.org/document/5403083/,2009 Workshop on Applications of Computer Vision (WACV),7-8 Dec. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MFI-2003.2003.1232635,Map generation based on the interaction between robot body and its surrounding environment,IEEE,Conferences,"This paper presents a method for map generation based on the interaction between a robot body and its surrounding environment. While a robot moves in the environment, the robot interacts with its surrounding environment. If the effect of the environment on the robot changes, such interactions also change. By observing the robot's body, our method detects such change of the interaction and generates a description representing the type of change and the location where such change is observed. In the current implementation, we assume that there are two types of the change in the interaction. The real robot experiments are conducted in order to show the validity of our method.",https://ieeexplore.ieee.org/document/1232635/,"Proceedings of IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, MFI2003.",1-1 Aug. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2016.7850169,Memetic robot control evolution and adaption to reality,IEEE,Conferences,"Inspired by animals' ability to learn and adapt to changes in their environment during life, hybrid evolutionary algorithms have been developed and successfully applied in a number of research areas. This paper explores the effects of learning combined with a genetic algorithm to evolve control system parameters for a four-legged robot. Here, learning corresponds to the application of a local search algorithm on individuals during evolution. Two types of learning were implemented and tested, i.e. Baldwinian and Lamarckian learning. On the direct results from evolution in simulation, Lamarckian learning showed promising results, with a significant increase in final fitness compared with the results from evolution without learning. Further experiments with learning on the real robot demonstrated an efficient adaptation of the robot gait to the real world environment, and increased the performance to the level measured in simulation. This paper demonstrates that Lamarckian evolution is effective in improving the performance of robot controller evolution, and that the same learning process on the physical robot efficiently reduces the negative impact of the simulation-reality gap.",https://ieeexplore.ieee.org/document/7850169/,2016 IEEE Symposium Series on Computational Intelligence (SSCI),6-9 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CINTI.2011.6108528,Mobile robot control using combined neural-fuzzy and neural network,IEEE,Conferences,"This paper describes the concept of the navigation system for a mobile robot. The system is using a combination of two navigation algorithms: self-learning neural network, necessary to form a movement plan for a robot, and a collision-free control algorithm based on heuristic neuro-fuzzy approach. The basic task of neural network is to generate initial path. Heuristic base of rules for collision free algorithm are limited and does not cover all situations. Main contribution of proposed navigation is related to neural network property to supplements special cases that are not covered by present heuristic rules from the data base. Both algorithms are adapted and implemented to navigate real platform of a mobile robot equipped by two independent wheel drives, encoders and a set of short-range sonars. The combined reactive algorithm (using two control methods) is used in real time for obstacle navigation in robotized system. Navigation algorithms are placed into a PC, which is connected to mobile robot by wireless and wired links. Experiments have shown ability of collision-free navigation of mobile robot in real time.",https://ieeexplore.ieee.org/document/6108528/,2011 IEEE 12th International Symposium on Computational Intelligence and Informatics (CINTI),21-22 Nov. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SISY.2009.5291131,Mobile robot control using self-learning neural network,IEEE,Conferences,"The paper describes the concept of the navigation system for a mobile robot. The system is using a navigation algorithm based on self-learning neural network, necessary to form a movement plan for a robot. The algorithm is adapted and implemented to navigate real platform of a mobile robot equipped by two independent wheel drives, encoders and a set of short-range sonars. Navigation algorithm is placed into a PC, which is connected to mobile robot by wireless and wired links. Experiments have shown ability of collision-free navigation of mobile robot in real time.",https://ieeexplore.ieee.org/document/5291131/,2009 7th International Symposium on Intelligent Systems and Informatics,25-26 Sept. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1998.713694,Mobile robot localization using the Hough transform and neural networks,IEEE,Conferences,"For an autonomous mobile robot to navigate in an unknown environment it is essential to know the location of the robot on a real-time basis. Finding position and orientation of a mobile robot in a world coordinate system is a problem in localization. Dead-reckoning is commonly used for localization, but position and orientation errors from dead-reckoning tend to accumulate over time. The objective of the paper is to develop a feature-based localization method that uses the Hough transform to detect wall-like features in the environment based on sonar range data. Although the Hough transform is an effective method for detecting lines and curves from noisy data it has the drawback of being sensitive to discretization resolution. Results of line detection using various discretization resolutions and using a winner-take-all neural network are presented and discussed. The results indicate that the Hough transform method is able to reliably recognize wall-like features from noisy sonar data, but the accuracy of detected features is dependent heavily on the choice of resolution of parameter discretization.",https://ieeexplore.ieee.org/document/713694/,Proceedings of the 1998 IEEE International Symposium on Intelligent Control (ISIC) held jointly with IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA) Intell,17-17 Sept. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AMC.2006.1631674,Mobile robot navigation in an unknown environment,IEEE,Conferences,"This paper discusses application of an intelligent system in order to navigate in real-time a small size, four wheeled, indoor mobile robot accurately using ultra-light (160 gr), inexpensive laser range finder without prior information of the environment. A recurrent neural network is used to find the best path to the target of the robot. An accurate grid-based map is generated using a laser range finder scene and location found by a modified dead reckoning system. Finally a motion control method is presented. These approaches are implemented and tested in Resquake mobile robot",https://ieeexplore.ieee.org/document/1631674/,"9th IEEE International Workshop on Advanced Motion Control, 2006.",27-29 March 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2009.5326151,"Motion capture and classification for real-time interaction with a bipedal robot using on-body, fully wireless, motion capture specknets",IEEE,Conferences,"This paper presents, to the best of our knowledge, the first instance of real-time human-robot interaction using motion capture (mocap) data obtained from fully wireless, on-body sensor networks. During the learning phase, data for motion such as waving of the hands, standing on a leg, performing sit-ups and squats is captured from a human strapped with the orient motion capture specks. Key features are extracted from the captured motion data using unsupervised learning algorithms. During subsequent interactions with the robot, the motion of the operator, speckled with orients, is classified and the robot selects to play the closest motion. This approach is particularly useful in situations where the robot operates a well defined vocabulary of motion, and the advantages are the real-time interaction and the rapidity (in a matter of minutes) in programming new behaviour compared to a heuristics-based approach. This paper compares the performances of three unsupervised learning algorithms: c-means, k-means and expectation maximisation (EM) for the four motion scenarios. Nine best candidates for the three learning algorithms for each of the four motion scenarios were selected in the Webots robot simulator and then transferred to the real robot. Metrics were defined for each motion scenario and their performances compared for the three learning algorithms. In all the cases the motions were able to be imitated; c-means was the best, followed closely by the k-means algorithms, and the reasons have been analysed.",https://ieeexplore.ieee.org/document/5326151/,RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication,27 Sept.-2 Oct. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1990.125999,Motion planning for a whole-sensitive robot arm manipulator,IEEE,Conferences,"A sensor-based motion planning system for a robot arm manipulator must include these four basic components: sensor hardware; real-time signal/sensory data processing hardware/software a local step planning subsystem that works at the basic sample rate of the arm; and a subsystem for global planning. The objective of this work is to develop the fourth component, a real-time implementable algorithm that realizes the upper, global level of planning. Based on the current collection of local normals, the algorithm generates preferable directions of motion around obstacles, in order to guarantee reaching the target position if it is reachable. Experimental results from testing the developed system are also discussed.<>",https://ieeexplore.ieee.org/document/125999/,"Proceedings., IEEE International Conference on Robotics and Automation",13-18 May 1990,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIAM54119.2021.00121,Multi-Robot Cooperative Hunting Based on Virtual Reality Technology,IEEE,Conferences,"Multi-robot cooperative rounding up is one of the important methods to test the intelligence level of multi-robot system. In order to test the intelligence of multi-robot system, a multi-robot cooperative rounding up system is established in this paper. In order to realize the rounding up system, this paper uses finite state machine to simplify the rounding up behavior of multi-robot. At the same time, in order to observe the multi-robot system more intuitively, this paper uses Vega software to realize virtual reality technology and applies it to the rounding up system established in this paper. Experiments show that the virtual reality technology based on VEGA can effectively simulate the multi-robot roundup, so that observers can observe the system more intuitively and effectively, so as to find out whether there are problems in the system.",https://ieeexplore.ieee.org/document/9724756/,2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM),23-25 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN45523.2021.9557363,Multi-Robot Multiple Camera People Detection and Tracking in Automated Warehouses,IEEE,Conferences,"In this work a multi-robot system is presented for people detection and tracking in automated warehouses. Each Automated Guided Vehicle (AGV) is equipped with multiple RGB cameras that can track the workers’ current locations on the floor thanks to a neural network that provides human pose estimation. Based on the local perception of the environment each AGV can exploit information about the tracked people for self-motion planning or collision avoidance.Additionally, data collected from each robot contributes to a global people detection and tracking system. A warehouse central management software fuses information received from all AGVs into a map of the current locations of workers. The estimated locations of workers are sent back to the AGVs to prevent potential collision. The proposed method is based on two-level hierarchy of Kalman filters. Experiments performed in a real warehouse show the viability of the proposed approach.",https://ieeexplore.ieee.org/document/9557363/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISKE.2010.5680764,Multilevel fuzzy navigation control scheme applied to a monitoring mobile robot,IEEE,Conferences,"The design, construction and real time performance of a mobile monitoring system based on a Khepera mobile robot are presented. The functions performed by the system are: (a) line following, (b) obstacle avoidance, (c) identification of test points along the path, (d) recognition of the mark (bar code) located at each test point and, (e) measuring of a physical parameter. For the navigation, an innovative multilevel fuzzy control scheme is implemented in which the fuzzy sensor fusion, related to the perception of the environment, reduces the complexity of the navigation function. Other distinctive characteristics are the identification of test points by means of a Kohonen's neural network and the processing of a one-dimensional video signal for recognition of landmarks located at each test point.",https://ieeexplore.ieee.org/document/5680764/,2010 IEEE International Conference on Intelligent Systems and Knowledge Engineering,15-16 Nov. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.1995.531939,Multimedia sensing system for robot,IEEE,Conferences,"The purpose of this study is to realize a multimedia sensing system for robot. Using both image and sound processing, the system makes a robot track the person who is speaking. The sound direction is calculated from the phase difference between the sounds arriving at the right and left microphones (ears) of the robot. Then by detecting the synchronization between the sound and image changes, the system identifies the speaker. Furthermore, by introducing a multi-level synchronization checking and context analysis, the action pattern of the robot can be regulated to make the robot perform in a complicated environment with plural speakers. All the processes are performed in real-time. The proposed system is implemented in the information assistant robot ""Hadaly"".",https://ieeexplore.ieee.org/document/531939/,Proceedings 4th IEEE International Workshop on Robot and Human Communication,5-7 July 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2007.4424774,Multiple sensors data integration using MFAM for mobile robot navigation,IEEE,Conferences,"The mobile robot navigation with complex environment needs more input space to match the environmental data into robot outputs in order to perform realistic task. At the same time, the number of rules at the rule base needs to be optimized to reduce the computing time and to provide the possibilities for real time operation. In this paper, the optimization of fuzzy rules using a modified fuzzy associative memory (MFAM) is designed and implemented. MFAM provides good flexibility to use multiple input space and reduction of rule base for robot navigation. This paper presents the MFAM model to generate the rule base for robot navigation. The behavior rules obtained from MFAM model are tested using simulation and real world experiments, and the results are discussed in the paper and compared with the existing methods.",https://ieeexplore.ieee.org/document/4424774/,2007 IEEE Congress on Evolutionary Computation,25-28 Sept. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2016.7872960,Muscle-gesture robot hand control based on sEMG signals with wavelet transform features and neural network classifier,IEEE,Conferences,"In this paper, we propose a muscle gesture-computer interface (MGCI) system for a five-fingered robotic hand control employing a commercial wearable MYO gesture armband. Eight channels of surface EMG (sEMG) signals were acquired and segmented. Then four levels of Daubechies 5 Wavelet family were performed to analyze the EMG signal. Totally 72 features were extracted from the EMG raw data for 16 hand motions recognition utilizing artificial Neural Networks. The average of best overall classification rate during off-line training is 87.8%. Consequently, real-time hand gesture recognition was implemented to evaluate the performance of the proposed system and the average recognition accuracy was 89.38%. Finally, it was applied to control a five-fingered robot hand.",https://ieeexplore.ieee.org/document/7872960/,2016 International Conference on Machine Learning and Cybernetics (ICMLC),10-13 July 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2000.972604,NN controller of the constrained robot under unknown constraint,IEEE,Conferences,"In this paper, the problems faced in the constrained force control is studied (uncertainties in dynamic model and the unknown constraints). A neural network (NN) controller is proposed based on the derived dynamic model of robot in the task space. The feed-forward neural network is used to adaptively compensate for the uncertainties in the robot dynamics. Training signals are proposed for the feed-forward neural network controller. The NN weights are tuned online, with no off-line learning phase required. An online estimation algorithm is developed to estimate the local shape of the constraint surface by using measured data on the force and position of the end-effector. The suggested controller is simple in structure and can be implemented easily. Real-time experiments are conducted using the five-bar robot to demonstrate the effectiveness of the proposed controller.",https://ieeexplore.ieee.org/document/972604/,"2000 26th Annual Conference of the IEEE Industrial Electronics Society. IECON 2000. 2000 IEEE International Conference on Industrial Electronics, Control and Instrumentation. 21st Century Technologies",22-28 Oct. 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA52036.2021.9512587,Natural Residual Reinforcement Learning for Bicycle Robot Control,IEEE,Conferences,"This work focuses on motion control of the bicycle robot by using the proposed NRRL algorithm. Unlike the traditional RL algorithm, decomposing the main tasks into subtasks manually and introducing qualitative prior knowledge to the agent have been applied in the NRRL algorithm. Simulation results show that better performance and better sample efficiency of the proposed NRRL algorithm have been achieved in terms of balance control and path tracking of bicycle robot. It's believed that the NRRL algorithm is available on the real physical bicycle robot, and the deployment of the algorithm will be realized soon, as the real physical bicycle robot has been constructed currently.",https://ieeexplore.ieee.org/document/9512587/,2021 IEEE International Conference on Mechatronics and Automation (ICMA),8-11 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1991.174585,Neural network approach to path planning for two dimensional robot motion,IEEE,Conferences,"A method for robot obstacle avoidance and path planning is proposed. The algorithm is based on a camera image feedback loop utilizing a neural network for image processing. The method can successfully generate collision-free paths in a 2D robot workspace containing randomly-placed polygonal obstacles. The control algorithm is simple and robust and has low computational requirements. Controller simulation implemented on a personal computer can generate collision-free robot paths in real time, requiring approximately 1 sec. per robot move.<>",https://ieeexplore.ieee.org/document/174585/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1995.525344,Neural network based iterative learning controller for robot manipulators,IEEE,Conferences,"An efficient neural network based learning control scheme is proposed to solve the trajectory tracking controI problem of robot manipulators. The proposed approach has four distinctive characteristics: 1) good tracking performance can be achieved during the first learning trial; 2) learning algorithm for adjusting neural network weights is independent of the manipulator dynamic model, thus displays strong robustness to torque disturbances and model parameter uncertainty; 3) no acceleration measurement or estimation is needed; and 4) real-time implementation with a higher sampling rate is readily possible. Simulation results on a 3 degree-of-freedom manipulator are presented to show its validity.",https://ieeexplore.ieee.org/document/525344/,Proceedings of 1995 IEEE International Conference on Robotics and Automation,21-27 May 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2003.1250614,Non-learning artificial neural network approach to motion planning for the Pioneer robot,IEEE,Conferences,This paper describes the implementation of a biologically-inspired non-learning artificial neural network for robot motion planning on the Pioneer 2DX robot. This motion planner fits within the Saphira operating system. The deliberative ANN motion planner is able to respond to changing situations in real time and complements the reactive behaviours.,https://ieeexplore.ieee.org/document/1250614/,Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),27-31 Oct. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2006.347441,Obstacle avoidance algorithm based on biological patterns for anthropomorphic robot manipulator,IEEE,Conferences,"This study addresses the problem of collision-free controlling of 3-DOF (degree of freedom) anthropomorphic manipulators with given a priori unrestricted trajectory. The robot constraints resulting from the physical robot's actuators are also taken into account during the robot movement. Obstacle avoidance algorithm is based on penalty function, which is minimized when collision is predicted. Mathematical construction of penalty function and minimization process allows modeling of variety behaviors of robot elusion moves. Implementation of artificial neural network (ANN) inside the control process gives the additional flexibility needed to remember most important robot behaviors based on biological pattern of human arm moves. Thanks to the fast collisions' detection, the presented algorithm appears to be applicable to the industrial real-time implementations. Numerical simulations of the anthropomorphic manipulator operating in three dimensional space with obstacles is also presented",https://ieeexplore.ieee.org/document/4152937/,IECON 2006 - 32nd Annual Conference on IEEE Industrial Electronics,6-10 Nov. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561790,Occupancy Map Inpainting for Online Robot Navigation,IEEE,Conferences,"In this work, we focus on mobile robot navigation in indoor environments where occlusions and field-of-view limitations hinder onboard sensing capabilities. We show that the footprint of a camera mounted on a robot can be drastically improved using learning-based approaches. Specifically, we consider the task of building an occupancy map for autonomous navigation of a robot equipped with a depth camera. In our approach, a local occupancy map is first computed using measurements from the camera directly. Afterwards, an inpainting network adds further information, the occupancy probabilities of unseen grid cells, to the map. A novel aspect of our approach is that rather than direct supervision from ground truth, we combine the information from a second camera with a better field-of-view for supervision. The training focuses on predicting extensions of the sensed data. To test the effectiveness of our approach, we use a robot setup with a single camera placed at 0.5m above the ground. We compare the navigation performance using raw maps from only this camera’s input (baseline) versus using inpainted maps augmented with our network. Our method outperforms the baseline approach even in completely new environments not included in the training set and can yield 21% shorter paths than the baseline approach. A real-time implementation of our method on a mobile robot is also tested in home and office environments.",https://ieeexplore.ieee.org/document/9561790/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6630740,Off-line path integral reinforcement learning using stochastic robot dynamics approximated by sparse pseudo-input Gaussian processes: Application to humanoid robot motor learning in the real environment,IEEE,Conferences,"We develop fast reinforcement learning (RL) framework using the approximated dynamics of a humanoid robot. Although RL is a useful non-linear optimizer, applying it to real robotic systems is usually difficult due to the large number of iterations required to acquire suitable policies. In this study, we approximate the dynamics using data from a real robot with sparse pseudo-input Gaussian processes (SPGPs). By using SPGPs, we estimated the probability distribution considering both the input vector and output signal variances. In real environments, since the observations from robotic sensors include large noise, SPGPs can suitably approximate the stochastic dynamics of a real humanoid robot. We use the approximated dynamics to improve the performance of a movement task in a path integral RL framework, which updates a policy from the sampled trajectories of the state and action vectors and the cost. We implemented our proposed method on a real humanoid robot and tested on a via-point reaching task. The robot achieved successful performance with fewer number of interactions with the real environment by using the proposed method than a conventional approach which dose not use the simulated dynamics.",https://ieeexplore.ieee.org/document/6630740/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAS.2006.40,On the conception of an autonomous and modular robot based on an Event Driven OSEK System with deterministic real-time behavior,IEEE,Conferences,"In this paper, we are interested in the design of an autonomous and modular self-reconfigurable robot having self-assembly capability and deterministic behavior. The ability of a modular robot to meet its mission strongly depends on the artificial intelligence software and on the underlying hardware and software architecture. The artificial intelligence software of a robot is mapped into several elementary tasks with different real-time constraints. We propose in this paper a real-time analysis taking into account kernel overheads for the validation of the real-time behavior of an artificial intelligence software. We study the OSEK operating system that requires few hardware resources and is cost effective. The overheads are due to the context switching mechanism which activates, terminates, and reschedules tasks, and to the periodic timer used to create the time base which is necessary for the periodic tasks model. We show how to take into account those overheads in the feasibility conditions. We compare the theoretical worst case response time obtained with kernel overheads to the response time obtained on a task set, on a real robot, based on the event driven OSEK implementation. We show that the kernel overheads cannot be neglected and that the theoretical results are valid and can be used to ensure a deterministic behavior of the robot",https://ieeexplore.ieee.org/document/1690225/,International Conference on Autonomic and Autonomous Systems (ICAS'06),19-21 July 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MWSCAS.1991.251981,On the fast robot dynamic parameters learning,IEEE,Conferences,"A computationally efficient solution to the problem of identifying the dynamic parameters of a robot manipulator is presented. The identification procedure is based on a simplified form of the dynamics. The approach has three important characteristics. First, being based on the Lagrangian representation, the equations are linear in the dynamic parameters, which makes possible the application of linear identification techniques. Second, the dynamic parameters are easily recognized, extracted, and grouped. Third, the equations are amenable to the implementation of parallel processing schemes. For the identification a recursive least squares algorithm is used. The algorithm is distributed over a network of transputers. Real-time results have been produced to demonstrate the speedup and efficiency of the proposed technique. A case study is given for the first three links of the Stanford Arm positioning system.<>",https://ieeexplore.ieee.org/document/251981/,[1991] Proceedings of the 34th Midwest Symposium on Circuits and Systems,14-17 May 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2007.4415153,Online Affect Detection and Adaptation in Robot Assisted Rehabilitation for Children with Autism,IEEE,Conferences,"This paper presents a novel affect-sensitive human-robot interaction framework for rehabilitation of children with autism spectrum disorder (ASD) where the robot can detect the affective cues of the children implicitly and response to them appropriately. Psychophysiological analysis is performed that uses subjective reports of the affective states from a clinical observer. Comprehensive physiological indices are investigated that may correlate with the affective states of children with ASD. A robot uses a support vector machines based affect model to detect the affective cues. A reinforcement learning based adaptation mechanism is employed to allow the robot to adjust its behaviors autonomously as a function of the predicted children's affective state. Four adolescents diagnosed with high-functioning autism participated in the experiments. This is the first time, to our knowledge, that the affective states of children with ASD have been detected via physiology-based affective modeling technique in real-time. This is also the first time that impact of affect-sensitive interaction between a robot and children with ASD in closed loop has been demonstrated experimentally.",https://ieeexplore.ieee.org/document/4415153/,RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication,26-29 Aug. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2019.8816298,Online Learning of the Inverse Dynamics with Parallel Drifting Gaussian Processes: Implementation of an Approach for Feedforward Control of a Parallel Kinematic Industrial Robot,IEEE,Conferences,"The present paper deals with an online approach to learn the inverse dynamics of any robot. This is realized by the use of Gaussian Processes drifting parallel along the system data. An extension by a database enables the efficient use of data points from the past. The central component of this work is the implementation of such a method in a controller in order to achieve the actual goal: the feedforward control of an industrial robot by means of machine learning. This is done by splitting the procedure into two threads running parallel so that the prediction is decoupled from the computing-intensive training of the models. Experiments show that the method reduces the tracking errors more clearly than an elaborately identified rigid body model including friction. For a defined trajectory, the squared areas of the tracking errors of all axes are reduced by more than 54% compared to motion without pre-control. In addition, a highly dynamic pick-and-place experiment is used to investigate the possible changes in system dynamics. Compared to an offline trained model, the approximation error of the proposed online approach is smaller for the remaining time of the experiment after an initial phase. Furthermore, this error is smaller throughout the experiment for online learning with parallel drifting Gaussian Processes than when using a single one.",https://ieeexplore.ieee.org/document/8816298/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2012.6224998,Online audio beat tracking for a dancing robot in the presence of ego-motion noise in a real environment,IEEE,Conferences,"This paper presents the design and implementation of a real-time real-world beat tracking system which runs on a dancing robot. The main problem of such a robot is that, while it is moving, ego noise is generated due to its motors, and this directly degrades the quality of the audio signal features used for beat tracking. Therefore, we propose to incorporate ego noise reduction as a pre-processing stage prior to our tempo induction and beat tracking system. The beat tracking algorithm is based on an online strategy of competing agents sequentially processing a continuous musical input, while considering parallel hypotheses regarding tempo and beats. This system is applied to a humanoid robot processing the audio from its embedded microphones on-the-fly, while performing simplistic dancing motions. A detailed and multi-criteria based evaluation of the system across different music genres and varying stationary/non-stationary noise conditions is presented. It shows improved performance and noise robustness, outperforming our conventional beat tracker (i.e., without ego noise suppression) by 15.2 points in tempo estimation and 15.0 points in beat-times prediction.",https://ieeexplore.ieee.org/document/6224998/,2012 IEEE International Conference on Robotics and Automation,14-18 May 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1998.726546,Online learning neural network controller for pneumatic robot position control,IEEE,Conferences,"This paper presents the implementation of online learning neural network controller in the pneumatic robot position servo control. The advantages of this design include: the ability to compensate for nonlinearities, and it is insensitive to system parameter time-varying. The traditional PID controller is replaced by neural network controller trained online to learn the inverse model of the pneumatic manipulator by backpropagation of the performance error. The simulation studies and experimental results on the PID controller, online learning neural network controller and off-line training neural network controller, are presented and discussed.",https://ieeexplore.ieee.org/document/726546/,"SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)",14-14 Oct. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2012.6285128,Online learning of COM trajectory for humanoid robot locomotion,IEEE,Conferences,"Center Of Mass (COM) trajectory is an essential factor for stable and natural robot locomotion. Unlike previous research in which COM trajectory either restricted by ZMP trajectory or directly predefined by simple function such as sinusoid, this research aims to establish the COM trajectory by online autonomous learning under the objective of locomotion stability and naturalness, which is expressed as a self-consistent measure in this paper. It provides an alternative that may avoid or weaken the mismatch between theoretical planning and practical implementation. The experimental results on a real humanoid robot PKU-HR4 show its effectiveness and promising future.",https://ieeexplore.ieee.org/document/6285128/,2012 IEEE International Conference on Mechatronics and Automation,5-8 Aug. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2006.275808,"Ontology for Cognitics, Closed-Loop Agility Constraint, and Case Study - a Mobile Robot with Industrial-Grade Components",IEEE,Conferences,"The paper refers to intelligent industrial automation. The objective is to present key elements and methods for best practice, as well as some results obtained. The first part presents an ontology for automated cognition (cognitics), where, based on information and time, the main cognitive concepts, including those of complexity, knowledge, expertise, learning, intelligence abstraction, and concretization are rigorously defined, along with corresponding metrics and specific units. Among important conclusions at this point are the fact that reality is much too complex to be approached better than through much simplified models, in very restricted contexts. Another conclusion is the necessity to be focused on goal. Extensions are made here for group behavior. The second part briefly presents a basic law governing the choice of overall control architecture: achievable performance level of control system in terms of agility, relative to process dynamics, dictates the type of approaches which is suitable, in a spectrum which ranges from simple threshold-based switching, to classical closed-loop calculus (PID, state space multivariable systems, etc.), up to ""impossible"" cases where additional controllers must be considered, leading to cascaded, hierarchical control structures. For complex cases such as latter ones, new tools and methodologies must be designed, as is typical in O3NEIDA initiative, at least for software components. Finally, a large part of the paper presents a case study, a mobile robot, i.e. an embedded autonomous system with distributed, networked control, featuring industry-grade components, designed with the main goal of robust functionality. The case illustrates several of the concepts introduced earlier in the paper.",https://ieeexplore.ieee.org/document/4053562/,2006 4th IEEE International Conference on Industrial Informatics,16-18 Aug. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR49135.2020.9144838,Outdoor Robot Navigation System using Game-Based DQN and Augmented Reality,IEEE,Conferences,"This paper presents a deep reinforcement learning based robot outdoor navigation method using visual information. The deep q network (DQN) maps the visual data to robot action in a goal location reaching task. An advantage of the proposed method is that the implemented DQN is trained in the first-person shooter (FPS) game-based simulated environment provided by ViZDoom. The FPS simulated environment reduces the differences between the training and the real environments resulting in a good performance of trained DQNs. In our implementation a marker-based augmented reality algorithm with a simple object detection method is used to train the DQN. The proposed outdoor navigation system is tested in the simulation and real robot implementation, with no additional training. Experimental results showed that the navigation system trained inside the game-based simulation can guide the real robot in outdoor goal directed navigation tasks.",https://ieeexplore.ieee.org/document/9144838/,2020 17th International Conference on Ubiquitous Robots (UR),22-26 June 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1991.131722,Parallel robot motion planning,IEEE,Conferences,"A fast, parallel method for computing configuration space maps is presented. The method is made possible by recognizing that one can compute a family of primitive maps which can be combined by superposition based on the distribution of real obstacles. This motion planner has been implemented for the first three degrees-of-freedom of a Puma robot in *Lisp on a Connection Machine with 8 K processors. A six degree-of-freedom version of the algorithm which performs a sequential search of the six-dimensional configuration space, building three-dimensional cross sections in parallel, has also been implemented.<>",https://ieeexplore.ieee.org/document/131722/,Proceedings. 1991 IEEE International Conference on Robotics and Automation,9-11 April 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/URAI.2012.6463006,Path planning through maze routing for a mobile robot with nonholonomic constraints,IEEE,Conferences,"A comprehensive technique to plan path for a mobile robot with nonholonomic constraints through maze routing technique has been presented. Our robot uses a stereo vision based approach to detect the obstacles by creating dense 3D point clouds from the stereo images. ROS packages have been implemented on the robot for specific tasks of providing: i) Linear and angular velocity commands, ii) Calibration and rectification of the stereo images for generating point clouds, iii) Simulating the URDF (Unified Robot Description Format) module in real time, with respect to the real robot and iv) For visualizing the sensor data. For efficient path planning a hybrid technique using Lee's algorithm, modified by Hadlock and Soukup's algorithm has been implemented. Different path planning results have been shown using the maze routing algorithms. Preliminary results shows that Lee's algorithm is more time consuming in comparison with other algorithms. A hybrid of Lee's with Soukup's algorithm is more efficient but unpredictable for minimal path. A hybrid of Lee's with Hadlock's algorithm is the most efficient and least time consuming.",https://ieeexplore.ieee.org/document/6463006/,2012 9th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),26-28 Nov. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1996.571370,Perception-action method for mobile robot plan and control based on driving experience,IEEE,Conferences,"A method of navigation and control for mobile robot is introduced. It combines task planning and path tracking together with the principle of ""perception-action"" under the guidance of ""goal planning"". First, we discuss the behavior of the mobile robot in an outdoor real world for the purpose of setting up a mixed layered architecture with ""perception-action"" and ""goal planning"". Then a simple but effective approach of sensor based navigation and control is described for the implementation of the architecture. Finally, we give some improvements based on the human-driving experience concerning path tracking and control for the mobile robot moving on outdoor semistructured roads. The experiments carried out on our THMR-III (Tsinghua Mobile Robot III) mobile robot navigating in the real world showed the method described was effective and robust.",https://ieeexplore.ieee.org/document/571370/,"1996 IEEE International Conference on Systems, Man and Cybernetics. Information Intelligence and Systems (Cat. No.96CH35929)",14-17 Oct. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS52745.2021.9649792,Performance Evaluation of YOLOv3 and YOLOv4 Detectors on Elevator Button Dataset for Mobile Robot,IEEE,Conferences,"The performance evaluation of an AI network model is the important part for building an effective solution before its deployment in real-world on the robot. In our study, we have implemented YOLOv3-tiny and YOLOv4-tiny darknet based frameworks for performance evaluation of the elevator button recognition task and tested both variants on image and video datasets. The objective of our study is two-fold: First, to overcome the limitation of elevator buttons dataset by creating new dataset and increasing its quantity without compromising the quality; Second, to provide a comparative analysis through experimental results and the performance evaluation of both detectors using four machine learning metrics. The purpose of our work is to assist the researchers and developers in decision making of suitable detector selection for deployment in the elevator robot towards button recognition application. The results show that YOLOv4-tiny outperforms YOLOv3-tiny with an overall accuracy of 98.60% compared to 97.91% at 0.5 IoU.",https://ieeexplore.ieee.org/document/9649792/,"2021 21st International Conference on Control, Automation and Systems (ICCAS)",12-15 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE51582.2022.9831515,Person Re-Identification on a Mobile Robot Using a Depth Camera,IEEE,Conferences,"In this paper, we designed and implemented a real-time person re-identification API on a mobile robot, for a closed-and open-world setting, using only the IR gray value image of a depth camera. Since common datasets are not usable we created our own dataset using the IR gray value images, the pose and image processing techniques. Then we trained the state-of-the-art neural network for person re-identification with common parameters and methods. For running it in real-time, we sped up the model as well as the application. It is possible to re-identify three persons at once, at around 10 FPS. Our model reaches as closed-world setting a rank-1-accuracy of 95.5 &#x0025;. With an additional threshold, coming from rank-1-accuracy of closed-world setting, our real-time application reaches as open-world setting a f&#x0027;l-score of 79.44 &#x0025; and a recall of 68.44 &#x0025;.",https://ieeexplore.ieee.org/document/9831515/,2022 IEEE 31st International Symposium on Industrial Electronics (ISIE),1-3 June 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2011.5979792,Physical human robot interaction in imitation learning,IEEE,Conferences,"This video presents our recent research on the integration of physical human-robot interaction (pHRI) into imitation learning. First, a marker control approach for real time human motion imitation is shown. Secondly, physical coaching in addition to observational learning is applied for the incremental learning of motion primitives. Last, we extend imitation learning to learning pHRI which includes the establishment of intended physical contacts. The proposed methods were implemented and tested using the IRT humanoid robot and DLR's humanoid upper-body robot Justin.",https://ieeexplore.ieee.org/document/5979792/,2011 IEEE International Conference on Robotics and Automation,9-13 May 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2014.6907470,Posture control of a three-segmented tracked robot with torque minimization during step climbing,IEEE,Conferences,"In this paper, we present a posture control scheme for step climbing by an in-house developed three-segmented tracked robot, miniUGV. The posture control scheme results in minimum torque at the actuated joints of the segments. Non-linear optimization is carried out offline for progressively decreasing distance of the robot from the step with torque minimization as objective function and force balance, motor torque limits, slippage avoidance and interference avoidance constraints. The resulting angles of the joints are fitted to a third degree polynomial as a function of the robot distance from the step and the step height. It is shown that a single set of polynomial functions is sufficient for climbing steps of all permissible heights and angles of attack of the front segment. The methodology has been verified through simulation followed by implementation on the real robot. As a consequence of this optimization we find that the average current reduced by more than thirty percent, reducing power consumption and confirming the efficacy of the optimization framework.",https://ieeexplore.ieee.org/document/6907470/,2014 IEEE International Conference on Robotics and Automation (ICRA),31 May-7 June 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2014.6889830,Predictive Hebbian association of time-delayed inputs with actions in a developmental robot platform,IEEE,Conferences,"The work described here explores a neural network architecture that can be embedded directly in the realtime sensorimotor coordination loop of a developmental robot platform. We take inspiration from the way children are able to learn while interacting with a teacher, in particular the use of prediction of the teacher actions to improve own learning. The architecture is based on two neural networks that operate online, and in parallel, one for learning and one for prediction. A Hebbian learning rule is used to associate the high-dimensional afferent sensor input at different time-delays with the current efferent motor commands corresponding to the teacher demonstration. The predictions of future motor commands are used to limit the growth of the neural network weights, and to enable the robot to smoothly continue movements the teacher has begun. Results on a simulated iCub robot learning object interaction tasks are presented, including an analysis of the sensitivity to changes in the task setup. We also outline the first implementation on the real iCub platform.",https://ieeexplore.ieee.org/document/6889830/,2014 International Joint Conference on Neural Networks (IJCNN),6-11 July 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAAI54685.2021.00062,Preliminary Implementation of Grasping Operation by a Collaborative Robot Arm: Using a Ball as Example,IEEE,Conferences,"Grasping objects is one of the basic functions of a robot arm. This study completed the implementation of the process in which a collaborative robot (cobot) arm grasps an object. Hardware components included a depth camera, cobot arm, and artificial intelligence equipment for edge computing. Software components included computer visualization techniques, deep learning, and robot operating system. To complete the preliminary implementation of the system, the grasping operation of the robot arm was set to target a ball. This system implementation sheds light on how robot arms and deep learning techniques are applied to real-life problems. Experiments verified that the preliminary system implemented was able to correctly complete the ball-grasping operation and achieve the pragmatic goal.",https://ieeexplore.ieee.org/document/9778057/,2021 International Conference on Technologies and Applications of Artificial Intelligence (TAAI),18-20 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMTC.1999.776982,Problems and solutions in acquisition and interpretation of sensorial data on a mobile robot,IEEE,Conferences,"We discuss some guidelines to cope with problems that arise when using cheap and simple sensors on mobile, autonomous, robotic agents. In particular we focus on the perceptual aliasing problem and on the possibility to perform active sensor data acquisition. We present a robotic architecture that we have implemented on a real robot following the proposed guidelines. The obtained mobile robot satisfies the design specifications, navigating autonomously in an unstructured environment.",https://ieeexplore.ieee.org/document/776982/,IMTC/99. Proceedings of the 16th IEEE Instrumentation and Measurement Technology Conference (Cat. No.99CH36309),24-26 May 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEC.1996.542714,Propagating learned behaviors from a virtual agent to a physical robot in reinforcement learning,IEEE,Conferences,"For a physical robot to acquire behaviors, it is important for it to learn in the physical environment. Since reinforcement learning requires large computation costs as well as a lot of time in the physical environment, most research has performed learning by simulation. However, this does not work well in the real world. Realizing reinforcement learning of a physical robot in a physical environment requires both an adaptation for the diversity of possible situations and a high-speed learning method that can learn from fewer trials. This paper describes cooperative reinforcement learning based on propagating the learned behaviors of a virtual agent to a physical robot in order to accelerate learning in a physical environment. The method consists of two parts: (1) preparation learning in a virtual environment to accelerate initial learning, which accounts for most of the learning cost; and, (2) refinement learning in a physical environment by using the virtual learning results as an initial behavior set of a physical robot. Experimental results are given for a ball-pushing task with the physical robot and a virtual agent.",https://ieeexplore.ieee.org/document/542714/,Proceedings of IEEE International Conference on Evolutionary Computation,20-22 May 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICIP47338.2019.9012202,RBF Neural Network-Sliding Model Control Approach for Lower Limb Rehabilitation Robot Based on Gait Trajectories of SEMG Estimation,IEEE,Conferences,"This paper designed and developed a new RBF neural network-sliding model controller for patients with stroke and lower extremity motor dysfunction, and applied it to a 3 degrees of freedom (3-DOF) lower limb rehabilitation robot (LLRR) for passive rehabilitation of patients. At first, a simple LLRR structure is designed that can be adjusted to fit the patient at the hip, knee, and ankle joints. Then, the patient's sEMG signal is obtained to predict the expected trajectory of the LLRR system, where the EMG signal is detected by BIOPAC software. Moreover, a RBF neural network-sliding model approach is designed for the dynamics model of the LLRR, and the asymptotic stability of the controller is verified via a Lyapunov theorem. Finally, LLRR system is experimentally verified by the MATLAB software, which exploit that the proposed control approach is feasible and effective for the lower extremity patients. Thereby, the developed control approach has illustrated high efficiency and robustness for the patient's passive rehabilitation training in real-time.",https://ieeexplore.ieee.org/document/9012202/,2019 Tenth International Conference on Intelligent Control and Information Processing (ICICIP),14-19 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.2005.1529114,Real time implementation of a selective attention model for the intelligent robot with autonomous mental development,IEEE,Conferences,"We propose a biologically motivated selective attention model to find an object based on context free search for an intelligent robot with an autonomous mental development (AMD) mechanism. For real-time operation of the selective attention model in the robot system, we have considered a way to reduce the computational load of the selective attention model, which uses a simplified symmetry operation with retina-topic sampling and look-up table in the localized candidate attention region. As a result, our model can perform within 270 ms at Pentium-4 2.8Ghz, and obtain a plausible human-like visual scan path in order to pay attention to an object preferentially. Then, we implemented an intelligent mobile robot with selective attention for an AMD mechanism.",https://ieeexplore.ieee.org/document/1529114/,"Proceedings of the IEEE International Symposium on Industrial Electronics, 2005. ISIE 2005.",20-23 June 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863455,Real time tracking in robot teleoperation system,IEEE,Conferences,"The robot teleoperation system based on stereo vision was developed by the State Key Lab of Intelligent Technology and System of Tsinghua University. The paper presents the design frame of the whole system, and describes in detail some of the key design and implementation problems. Finally, the paper analyses the difficulty of applying this technology to virtual reality and augmented reality systems, and some suggestions are provided. The success of this system can contribute to further research on augmented reality.",https://ieeexplore.ieee.org/document/863455/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2006.259112,Real-Time Implementation of a PD+DFNNS Controller for Compliance Robot,IEEE,Conferences,"This paper presents the compliance control of a robot manipulator under a constrained environment. The controller design proposed herein is based on the intelligence adaptive control scheme. In this design, the DFNNs (dynamic fuzzy neural networks) and PD feedback controllers control the position and the contact force of robot end-effector. The DFNNs controller is employed to compensate for environmental variations such as payload mass and disturbance torque during the operation process; PD feedback controllers control the position and the contact force of end-effector. Applying these controllers allows us to adapt the manipulator to the unknown surface of the surrounding environment and to have close contact with the curved surface",https://ieeexplore.ieee.org/document/4028106/,2006 International Conference on Machine Learning and Cybernetics,13-16 Aug. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2005.1490961,Real-Time Multi-View Face Tracking for Human-Robot Interaction,IEEE,Conferences,"For face tracking in a video sequence, various face tracking algorithms have been proposed. However, most of them have difficulty in finding the initial position and size of a face automatically. In this paper, we present a fast and robust method for fully automatic multi-view face detection and tracking. Using a small number of critical rectangle features selected and trained by the Adaboost learning algorithm, we can detect the initial position, size and view of a face correctly. Once a face is reliably detected, we can extract face and upper body color distribution from the detected facial regions and upper body regions for building robust color modeling respectively. Simultaneously, each color modeling is performed by using k-means clustering and multiple Gaussian models. Then, fast and efficient multi-view face tracking is executed by using several critical features. Our proposed algorithm is robust to rotation, partial occlusions, and scale changes in front of dynamic, unstructured background. In addition, our proposed method is computationally efficient. Therefore, it can be executed in real-time",https://ieeexplore.ieee.org/document/1490961/,"Proceedings. The 4th International Conference on Development and Learning, 2005",19-21 July 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYSOSE.2015.7151922,Real-time FPGA decentralized inverse optimal neural control for a Shrimp robot,IEEE,Conferences,"This paper presents a field programmable gate array (FPGA) implementation for a decentralized inverse optimal neural controller for unknown nonlinear systems, in presence of external disturbances and parameter uncertainties. This controller is based on two techniques: first, an identifier using a discrete-time recurrent high order neural network (RHONN) trained with an extended Kalman filter (EKF) algorithm; second, on the basis of the neural identifier a controller which uses inverse optimal control, is designed to avoid solving the Hamilton Jacobi Bellman (HJB) equation. The proposed scheme is implemented in real-time to control a Shrimp robot.",https://ieeexplore.ieee.org/document/7151922/,2015 10th System of Systems Engineering Conference (SoSE),17-20 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCA.1994.381367,Real-time control of a robot using neural networks,IEEE,Conferences,"The real-time computation of the robot kinematics is very important. The basic transformations are the direct kinematic transformation (DKT) and the inverse kinematic transformation (IKT). The DKT can be computed in a straightforward way using the Denavit-Hartenberg notation. No such general method yet exists for the IKT, although this transformation is of major interest for control purposes. In this paper a neural network is presented that maps the IKT independent of the type of robot. After training, the network achieves very good accuracy and may easily be implemented in real-time. The performance of the algorithm is tested an the RTX robot, a SCARA-type robot with six degrees of freedom. This robot is controlled by a distributed control system. A host computer realizes the continuous path control and a network of 5 slave-transputers is used to compute the local controls and to drive the DC servomotors.<>",https://ieeexplore.ieee.org/document/381367/,1994 Proceedings of IEEE International Conference on Control and Applications,24-26 Aug. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2013.6706785,Real-time decentralized inverse optimal neural control for a Shrimp robot,IEEE,Conferences,"This paper deals with a decentralized inverse optimal neural controller for MIMO discrete-time unknown nonlinear systems, in a presence of external disturbances and parameter uncertainties. It uses two techniques: first, an identifier based on a discrete-time recurrent high order neural network (RHONN) trained with an extended Kalman filter (EKF) algorithm; second, on the basis of the real identifier a controller which uses inverse optimal control, is designed to avoid solving the Hamilton Jacobi Bellman (HJB) equation. The proposed scheme is implemented in real-time to control a Shrimp robot.",https://ieeexplore.ieee.org/document/6706785/,The 2013 International Joint Conference on Neural Networks (IJCNN),4-9 Aug. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCA.2009.5280998,Real-time decentralized neural backstepping controller for a robot manipulator,IEEE,Conferences,This paper deals with adaptive trajectory tracking for discrete-time MIMO nonlinear systems. A high order neural network (HONN) is used to approximate a decentralized control law designed by the backstepping technique as applied to a block strict feedback form (BSFF). The HONN learning is performed online by an Extended Kalman Filter (EKF) algorithm. The proposed scheme is implemented in real-time to control a two DOF robot manipulator.,https://ieeexplore.ieee.org/document/5280998/,"2009 IEEE Control Applications, (CCA) & Intelligent Control, (ISIC)",8-10 July 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2009.5354338,Real-time decentralized neural block controller for a robot manipulator,IEEE,Conferences,"This paper presents a discrete-time decentralized control scheme for identification and trajectory tracking of a two degrees of freedom (DOF) robot manipulator. A recurrent high order neural network (RHONN) structure is used to identify the plant model and based on this model, a discrete-time control law is derived, which combines discrete-time block control and sliding modes techniques. The neural network learning is performed online by Kalman filtering. A controller is designed for each joint, using only local angular position and velocity measurements. These simple local joint controllers allow trajectory tracking with reduced computations. The proposed scheme is implemented in real-time to control a two DOF robot manipulator.",https://ieeexplore.ieee.org/document/5354338/,2009 IEEE/RSJ International Conference on Intelligent Robots and Systems,10-15 Oct. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2010.5612924,Real-time five DOF robot control using a decentralized neural backstepping scheme,IEEE,Conferences,This paper presents a discrete-time decentralized control scheme for trajectory tracking of a five degrees of freedom (DOF) redundant robot. A high order neural network (HONN) is used to approximate a decentralized control law designed by the backstepping technique as applied to a block strict feedback form (BSFF). The neural network learning is performed on-line by Kalman filtering. The controllers are designed for each joint using only local angular position and velocity measurements. These simple local joint controllers allow trajectory tracking with reduced computations. The applicability of the proposed scheme is illustrated via real-time implementation.,https://ieeexplore.ieee.org/document/5612924/,2010 IEEE International Symposium on Intelligent Control,8-10 Sept. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR.1997.620222,Real-time navigation of a mobile robot using Kohonen's topology conserving neural network,IEEE,Conferences,"This paper proposes a real-time sensor based navigation method using Kohonen's topology conserving network for navigation of a mobile robot in any uncertain environment. The sensory information including target location with respect to current location of the mobile robot, have been discretely conserved using a two dimensional Kohonen lattice. Reinforcement learning based on a stochastic real valued technique have been implemented to compute the action space for this Kohonen lattice. The proposed scheme learns the input and output weight space of the Kohonen lattice which is generalized to any workspace. The effectiveness of the proposed scheme has been established by simulation where the complete domain of the input-space is quantized based on experience on sensory data encountered in real-time. The input-output mapping conserved by the Kohonen lattice during simulation was used to guide a mobile robot in a real-time environment. Successful navigation of the mobile robot without further training confirms the robustness of the proposed scheme.",https://ieeexplore.ieee.org/document/620222/,1997 8th International Conference on Advanced Robotics. Proceedings. ICAR'97,7-9 July 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2001.932598,Real-time robot learning,IEEE,Conferences,"This paper presents the design, implementation and testing of a real-time system using computer vision and machine learning techniques to demonstrate learning behavior in a miniature mobile robot. The miniature robot, through environmental sensing, learns to navigate a maze choosing the optimum route. Several reinforcement learning based algorithms, such as the Q-learning, Q(/spl lambda/)-learning, fast online Q(/spl lambda/)-learning and DYNA structure, are considered. Experimental results based on simulation and an integrated real-time system are presented for varying density of obstacles in a 15/spl times/15 maze.",https://ieeexplore.ieee.org/document/932598/,Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),21-26 May 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACEH51803.2020.9366217,Realization of Human and Fish Robot Interaction with Artificial Intelligence Using Hand Gesture,IEEE,Conferences,This study mimicked a real fish movement in the aquarium which was controlled by hand signals. The main idea to develop an aquarium robotic fish with hand gestures. Control actions include directions and stop and go of the fish. The inputs are given by human hands known as bio-mimetic ornamental. We implemented control algorithms to recognize hand gestures. The experimental results showed the effective control of robot fish with hand gestures.,https://ieeexplore.ieee.org/document/9366217/,"2020 IEEE 2nd International Conference on Architecture, Construction, Environment and Hydraulics (ICACEH)",25-27 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GTSD50082.2020.9303087,Receptionist and Security Robot Using Face Recognition with Standardized Data Collecting Method,IEEE,Conferences,"Face recognition has become the front runner for deep learning applications in the real world and this paper focuses on its implementation in a human-robot interaction and security system. For this specific project, it is inherent that restraints are created to allow the system to produce greater performance within the requirements of a receptionist and security robot. A k-nearest neighbors classifier is applied to further enhance the accuracy of face recognition. By sequencing images from videos, we create large datasets to train our own classifier in various conditions to increase its accuracy and lower false-positive rates in poor lighting environments. With the goal of creating a service robot, we have standardized our method of data collection for new inputs that will assist the recognition process in variable conditions of operation. The resulting product is a system that can accurately predict known and unknown faces with Asian features.",https://ieeexplore.ieee.org/document/9303087/,2020 5th International Conference on Green Technology and Sustainable Development (GTSD),27-28 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRDS.2002.1043897,Recognizing and remembering individuals: online and unsupervised face recognition for humanoid robot,IEEE,Conferences,"Individual recognition is a widely reported phenomenon in the animal world, where it contributes to successful maternal interaction, parental care, group breeding, cooperation, mate choice, etc. This work addresses the question of how one may implement such social competence in a humanoid robot. We argue that the robot must be able to recognize people and learn about their various characteristics through embodied social interaction. Thus, we proposed an initial implementation of an online and unsupervised face recognition system for Kismet, our sociable robotic platform. We show how specific features of this particular application drove our decision and implementation process, challenged by the difficulty of the face recognition problem, which has so far been explored in the supervised manner. Experimental results are reported to illustrate what was solved and the lessons learned from the current implementation.",https://ieeexplore.ieee.org/document/1043897/,IEEE/RSJ International Conference on Intelligent Robots and Systems,30 Sept.-4 Oct. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BioRob49111.2020.9224392,Reinforcement Learning Assist-as-needed Control for Robot Assisted Gait Training,IEEE,Conferences,"The primary goal of an assist-as-needed (AAN) controller is to maximize subjects’ active participation during motor training tasks while allowing moderate tracking errors to encourage human learning of a target movement. Impedance control is typically employed by AAN controllers to create a compliant force-field around the desired motion trajectory. To accommodate different individuals with varying motor abilities, most of the existing AAN controllers require extensive manual tuning of the control parameters, resulting in a tedious and time-consuming process. In this paper, we propose a reinforcement learning AAN controller that can autonomously reshape the force-field in real-time based on subjects’ training performances. The use of action-dependent heuristic dynamic programming enables a model-free implementation of the proposed controller. To experimentally validate the controller, a group of healthy individuals participated in a gait training session wherein they were asked to learn a modified gait pattern with the help of a powered ankle-foot orthosis. Results indicated the potential of the proposed control strategy for robot-assisted gait training.",https://ieeexplore.ieee.org/document/9224392/,2020 8th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob),29 Nov.-1 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII52469.2022.9708882,Reinforcement Learning based Hierarchical Control for Path Tracking of a Wheeled Bipedal Robot with Sim-to-Real Framework,IEEE,Conferences,"We propose a reinforcement learning (RL) based hierarchical control framework for path tracking of a wheeled bipedal robot. The framework consists of three control levels. 1) The high-level RL is used to obtain an optimal policy through trial and error in a simulated environment. 2) The middle-level Lyapunov-based non-linear controller is utilized to track a desired line with strong robustness and high stability. 3) The low-level PID-based controller is implemented to simultaneously achieve both balancing and velocity tracking for a physical wheeled bipedal robot in real world. Thanks to the middle-level controller, the offline trained policy in simulation can be directly employed on the physical robot in real time without tuning any parameters. Moreover, the high-level policy network is able to improve optimality and generality for the task of path tracking, as well to avoid the cumbersome process of manually tuning control gains. The experiment results in both simulation and real world demonstrate that the proposed hierarchical control framework can achieve quick, robust, and stable path tracking for a wheeled bipedal robot.",https://ieeexplore.ieee.org/document/9708882/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.2007.382878,Reinforcement Learning with a Supervisor for a Mobile Robot in a Real-world Environment,IEEE,Conferences,"This paper describes two experiments with supervised reinforcement learning (RL) on a real, mobile robot. Two types of experiments were preformed. One tests the robot's reliability in implementing a navigation task it has been taught by a supervisor. The other, in which new obstacles are placed along the previously learned path to the goal, measures the robot's robustness to changes in environment. Supervision consisted of human-guided, remote-controlled runs through a navigation task during the initial stages of reinforcement learning. The RL algorithms deployed enabled the robot to learn a path to a goal yet retain the ability to explore different solutions when confronted with a new obstacle. Experimental analysis was based on measurements of average time to reach the goal, the number of failed states encountered during an episode, and how closely the RL learner matched the supervisor's actions.",https://ieeexplore.ieee.org/document/4269878/,2007 International Symposium on Computational Intelligence in Robotics and Automation,20-23 June 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2012.6377767,Reinforcement learning-based tracking control for wheeled mobile robot,IEEE,Conferences,This paper proposes a new method to design a reinforcement learning-based integrated kinematic and dynamic tracking control scheme for a nonholonomic wheeled mobile robot. The scheme uses just only one neural network to design an online adaptive synchronous policy iteration algorithm implemented as an actor critic structure. Our tuning law for the single neural network not only learns online a tracking-HJB equation to approximate both the optimal cost and the optimal adaptive control law but also guarantees closed-loop stability in real-time. The convergence and stability of the overall system are proven by Lyapunov theory. The simulation results for wheeled mobile robot verify the effectiveness of the proposed controller.,https://ieeexplore.ieee.org/document/6377767/,"2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",14-17 Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CYBER.2012.6392582,Reinforecement learning-based optimal tracking control for wheeled mobile robot,IEEE,Conferences,This paper proposes a new method to design a reinforcement learning-based integrated kinematic and dynamic tracking control scheme for a nonholonomic wheeled mobile robot. The scheme uses just only one neural network to design an online adaptive synchronous policy iteration algorithm implemented as an actor critic structure. Our tuning law for the single neural network not only learns online a tracking-HJB equation to approximate both the optimal cost and the optimal control law but also guarantees closed-loop stability in real-time. The convergence and stability of the overall system are proven by Lyapunov theory. The simulation results for wheeled mobile robot verify the effectiveness of the proposed controller.,https://ieeexplore.ieee.org/document/6392582/,"2012 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 May 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863464,Related value set algorithm for robot to distinguish image,IEEE,Conferences,"The image recognition from image database is different from robot vision in real world because the robot concerned is movable. The size of the image that captured by the robot varies with the distance between robot's camera and the real picture, or object. We analyzed the process of how human remembers and identifies an image. We derived eight properties of the key feature in an image. We then developed a related value set algorithm. With the algorithm, we defined a set to represent the key features of an image. The elements of the set are represented with the related value. The key features retrieved from an image with the algorithm satisfied the eight properties, thus the algorithm can be used for robot to distinguish images or object. The key features can also be used in image knowledge representation.",https://ieeexplore.ieee.org/document/863464/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR47638.2019.9044114,Research on omnidirectional mobile robot motion control based on integration of traction and steering wheel,IEEE,Conferences,"In order to solve the automatic transportation of heavy materials under the limited working space of production workshops and warehouses, two sets of heavy-duty omnidirectional mobile robot motion control systems with steering wheel drive units were designed. The steering wheel combination drive unit of the “walking + steering” set is used to build the mobile robot chassis, and the mechatronics servo system and mathematical model of multi-motor coordinated motion are constructed. The communication between the controller and the steering wheel combination drive unit is established through the CAN bus. The specific implementation is to capture and analyze the control signal through the controller to obtain the desired motion mode, to obtain the motion of each set of steering wheel unit through the mathematical model, and to realize the desired motion through the synthesis of each set of steering wheel unit motion. It has been verified by experiments that the two sets of steering wheel unit-driven mobile robot control system realizes the zero turning radius, 360-degree omnidirectional movement of the robot and rotation during the movement. It can be used for flexible work in tight spaces.",https://ieeexplore.ieee.org/document/9044114/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2004.1343675,Research on remote controlled robot motion control system based on agent theory,IEEE,Conferences,"This paper introduces remote controlled robot motion control system based on agent theory. Task planning and reactive behavior control are discussed and implemented. This paper describes design of manager agent and motion control agent in detail. Agent theory are implemented in the robot control system to realize distributed intelligence based on M/A/R(Man/Agent/Robot) architecture. Thereby autonomy, reliability and real-time operation are improved.",https://ieeexplore.ieee.org/document/1343675/,Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788),15-19 June 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8794127,Residual Reinforcement Learning for Robot Control,IEEE,Conferences,"Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.",https://ieeexplore.ieee.org/document/8794127/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAIS53221.2021.9483964,Robot First Aid: Autonomous Vehicles Could Help in Emergencies,IEEE,Conferences,"Safety is of critical importance in designing autonomous vehicles (AVs) that will be able to perform effectively in complex, mixed-traffic, real-world urban environments. Some prior research has looked at how to proactively avoid accidents with safe distancing and driver monitoring, but currently little research has explored strategies to recover afterwards from emergencies, from crime to natural disasters. The current short paper reports on our ongoing work using a speculative prototyping approach to explore this expansive design space, in the context of how a robot inside an AV could be deployed to support first aid. As a result, we present some proposals for how to detect emergencies, and examine and help victims, as well as lessons learned in prototyping. Thereby, our aim is to stimulate discussion and ideation that-by considering the prevalence of Murphy's law in our complex world, and the various technical, ethical, and practical concerns raised-could potentially lead to useful safety innovations.",https://ieeexplore.ieee.org/document/9483964/,2021 Swedish Artificial Intelligence Society Workshop (SAIS),14-15 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNSC48988.2020.9238090,Robot Navigation with Map-Based Deep Reinforcement Learning,IEEE,Conferences,"This paper proposes an end-to-end deep reinforcement learning approach for mobile robot navigation with dynamic obstacles avoidance. Using experience collected in a simulation environment, a convolutional neural network (CNN) is trained to predict proper steering actions of a robot from its egocentric local occupancy maps, which accommodate various sensors and fusion algorithms. The trained neural network is then transferred and executed on a real-world mobile robot to guide its local path planning. The new approach is evaluated both qualitatively and quantitatively in simulation and realworld robot experiments. The results show that the map-based end-to-end navigation model is easy to be deployed to a robotic platform, robust to sensor noise and outperforms other existing DRL-based models in many indicators.",https://ieeexplore.ieee.org/document/9238090/,"2020 IEEE International Conference on Networking, Sensing and Control (ICNSC)",30 Oct.-2 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00131,"Robot Swarm Navigation: Methods, Analysis, and Applications",IEEE,Conferences,"Swarm navigation is one of the possible collective behaviours a swarm robotic system can possess. Research in multiple robot navigation has gained more attention given their potential real-world applications, such as search and rescue, transportation, precision farming, and environmental monitoring. In this paper, we analyze the recent advances in the field of swarm navigation, focusing mainly on design and analysis methods that contribute to collective exploration, coordinated motion, and collective transport. Moreover, various experimental and real-world applications of swarm navigation, instead of works that involve simulations, are described. Several challenges that restrict the implementation of successful laboratory works of swarm systems to industrial applications are also identified and described. To tackle these challenges, some interesting future research directions are proposed and discussed.",https://ieeexplore.ieee.org/document/9696121/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MECHATRONIKA.2014.7018286,Robot imitation of human arm via Artificial Neural Network,IEEE,Conferences,"In this study, a robot arm that can imitate human arm is designed and presented. The potentiometers are located to the joints of the human arm in order to detect movements of human gestures, and data were collected by this way. The collected data named as “movement of human arm” are classified by the help of Artificial Neural Network (ANN). The robot performs its movements according to the classified movements of the human. Real robot and real data are used in this study. Obtained results show that the learning application of imitating human action via the robot was successfully implemented. With this application, the platforms of robot arm in an industrial environment can be controlled more easily; on the other hand, robotic automation systems which have the capability of making a standard movements of a human can become more resistant to the errors.",https://ieeexplore.ieee.org/document/7018286/,Proceedings of the 16th International Conference on Mechatronics - Mechatronika 2014,3-5 Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASAMA.1999.805407,Robot media communication: an interactive real-world guide agent,IEEE,Conferences,"Describes a guide system and the software architecture for an autonomous, interactive robot based on a multi-agent system. A robot navigation system has been developed allowing the robot to guide people through halls in various types of exhibitions. Our approach uses an infrared location system in the hallway ceilings, making the environment part of a sensor-distributed robot system. The real-world guide agent is composed of a guide agent on a hand-held mobile computer and a robot agent on an autonomous mobile robot. The guide agent plays the role of ""robot media"" in order to integrate information in the information space of the mobile computer and the physical space of the exhibits in order to guide visitors through the physical space. This research aims to develop a cooperative adaptive system using two-way communication among spaces, media and human beings to construct transparent knowledge boundaries between the real space and the virtual space. The virtual space is generated from computer data using shared space technology and it creates a distributed intelligence in order to manage the communication and control the guide in a laboratory. We have experimented with and verified this software architecture using a prototype autonomous mobile robot equipped with a compass.",https://ieeexplore.ieee.org/document/805407/,"Proceedings. First and Third International Symposium on Agent Systems Applications, and Mobile Agents",6-6 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2008.4811760,Robot navigation using KFLANN place field,IEEE,Conferences,"This paper presents an implementation of place cells for a robot navigation using the K-iterations fast learning artificial neural networks (KFLANN) clustering algorithm. The KFLANN possesses several desirable properties suitable for place cell robot navigation tasks. The technique proposed is able to autonomously adjust the resolution of cells according to the complexity of the environment. This is achieved through two parameters known as the tolerance and the vigilance of the network. In addition, a navigation system consisting of a topological map building and a place cell path planning strategy is presented. A physical implementation of the system was developed on an autonomous platform and actual results were obtained. The experimental results obtained indicate that the system was able to navigate successfully through the experimental space and also tolerate unexpected discrepancies arising from motor and sensor errors present in a real environment. Furthermore, despite abrupt changes in an environment due to the deliberate introduction of obstacles, the system was still able to cope without changes to the program. The experiment was also extended to include a kidnapped robot scenario and the results were favorable, indicating a positive use of allothetic cue recognition capabilities.",https://ieeexplore.ieee.org/document/4811760/,"2008 IEEE International Conference on Systems, Man and Cybernetics",12-15 Oct. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2016.7844958,Robot position control in pipes using Q Learning,IEEE,Conferences,"In the most critical hydro crisis in Brazil, 37 percent of the whole amount of treated water is wasted before reaching consumers. A robot with a position control to travel inside a pipe is an important step in the pursuit of an autonomous solution to detect and correct pipes failures. This paper shows a Q Learning controller algorithm implemented using a microcontroller in a mechanical body of a commercial pipe inspection robot. Using only the measurements of a gyroscope, and controlling the wheels' motors on the left and right sides, the controller learned the best set of movements to ride inside a 300mm sewer pipe, in the tested conditions. Real tests in a 300mm pipe were performed using the developed algorithm and it was compared to a random movement and to a straight forward movement.",https://ieeexplore.ieee.org/document/7844958/,"2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",9-12 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/1957656.1957814,Robot self-initiative and personalization by learning through repeated interactions,IEEE,Conferences,"We have developed a robotic system that interacts with the user, and through repeated interactions, adapts to the user so that the system becomes semi-autonomous and acts proactively. In this work we show how to design a system to meet a user's preferences, show how robot pro-activity can be learned and provide an integrated system using verbal instructions. All these behaviors are implemented in a real platform that achieves all these behaviors and is evaluated in terms of user acceptability and efficiency of interaction.",https://ieeexplore.ieee.org/document/6281377/,2011 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI),8-11 March 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN47096.2020.9223566,Robust Real-Time Hand Gestural Recognition for Non-Verbal Communication with Tabletop Robot Haru,IEEE,Conferences,"In this paper, we present our work in close-distance non-verbal communication with tabletop robot Haru through hand gestural interaction. We implemented a novel hand gestural understanding system by training a machine-learning architecture for real-time hand gesture recognition with the Leap Motion. The proposed system is activated based on the velocity of a user's palm and index finger movement, and subsequently labels the detected movement segments under an early classification scheme. Our system is able to combine multiple gesture labels for recognition of consecutive gestures without clear movement boundaries. System evaluation is conducted on data simulating real human-robot interaction conditions, taking into account relevant performance variables such as movement style, timing and posture. Our results show robustness in hand gesture classification performance under variant conditions. We furthermore examine system behavior under sequential data input, paving the way towards seamless and natural real-time close-distance hand-gestural communication in the future.",https://ieeexplore.ieee.org/document/9223566/,2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),31 Aug.-4 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC.2019.8832613,Robust Zhang Neural Network for Tracking Control of Parallel Robot Manipulators With Unknown Parameters,IEEE,Conferences,"Under the situation of parameter uncertainty, the tracking control of parallel robot manipulators is a challenging problem in robotic research. Unlike conventional Zhang neural network (ZNN) relying on the assumption that the robot parameter information is fully and accurately known, this paper proposes a robust Zhang neural network (RZNN) for tracking control problems solving of parallel robot manipulators in the absence of parameter information. The proposed RZNN features the full utilization of effector feedback information, and shows a robust tracking performance even with unknown robot parameter information. Then, the continuous-time model of the RZNN is discretized via Euler forward formula (EFF) for numerical implementation. Finally, comprehensive simulative experiments including robustness test verify the effectiveness of the RZNN model for the real-time tracking control of parallel robot manipulators with unknown parameters.",https://ieeexplore.ieee.org/document/8832613/,2019 Chinese Control And Decision Conference (CCDC),3-5 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2014.7041487,Robust fall detection with an assistive humanoid robot,IEEE,Conferences,"Summary form only given. In this video we introduce a robot assistant that monitors a person in a household environment to promptly detect fall events. In contrast to the use of a fixed sensor, the humanoid robot will track and keep the moving person in the scene while performing daily activities. For this purpose, we extended the humanoid Nao1 with a depth sensor2 attached to its head. The tracking framework implemented with OpenNI3 segments and tracks the person's position and body posture. We use a learning neural framework for processing the extracted body features and detecting abnormal behaviors, e.g. a fall event [1]. The neural architecture consists of a hierarchy of self-organizing neural networks for attenuating noise caused by tracking errors and detecting fall events from video stream in real time. The tracking application, the neural framework, and the humanoid actuators communicate over Robot Operating System (ROS)4. We use communication over the ROS network implemented with publisher-subscriber nodes. When a fall event is detected, Nao will approach the person and ask whether assistance is needed. In any case, Nao will take a picture of the scene that can be sent to the caregiver or a relative for further human evaluation and agile intervention. The combination of this sensor technology with our neural network approach allows to tailor the robust detection of falls independently from the background surroundings and in the presence of noise (tracking errors and occlusions) introduced by a real-world scenario. The video shows experiments run in a home-like environment.",https://ieeexplore.ieee.org/document/7041487/,2014 IEEE-RAS International Conference on Humanoid Robots,18-20 Nov. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIAS.2012.6306173,SCARA robot control using neural networks,IEEE,Conferences,"A SCARA industrial robot model is identified based on a 4-axis structure using Lagrangian mechanics, also the dynamic model for the electromechanical actuator and motion transmission systems is identified. A conventional PD controller is implemented and compared to neural networks control system to achieve precise position control of SCARA manipulator. The performance of the modeled system is simulated using several desired tracking motion for each joint. Neural networks control method has shown a remarkable improvement of tracking capabilities for the SCARA robot over conventional PD controller. The proposed neural network controller has the potential to accurately control real-time manipulator applications.",https://ieeexplore.ieee.org/document/6306173/,2012 4th International Conference on Intelligent and Advanced Systems (ICIAS2012),12-14 June 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561020,SQRP: Sensing Quality-aware Robot Programming System for Non-expert Programmers,IEEE,Conferences,"Robot programming typically makes use of a set of mechanical skills that is acquired by machine learning. Because there is in general no guarantee that machine learning produces robot programs that are free of surprising behavior, the safe execution of a robot program must utilize monitoring modules that take sensor data as inputs in real time to ensure the correctness of the skill execution. Owing to the fact that sensors and monitoring algorithms are usually subject to physical restrictions and that effective robot programming is sensitive to the selection of skill parameters, these considerations may lead to different sensor input qualities such as the view coverage of a vision system that determines whether a skill can be successfully deployed in performing a task. Choosing improper skill parameters may cause the monitoring modules to delay or miss the detection of important events such as a mechanical failure. These failures may reduce the throughput in robotic manufacturing and could even cause a destructive system crash. To address above issues, we propose a sensing quality-aware robot programming system that automatically computes the sensing qualities as a function of the robot’s environment and uses the information to guide non-expert users to select proper skill parameters in the programming phase. We demonstrate our system framework on a 6DOF robot arm for an object pick-up task.",https://ieeexplore.ieee.org/document/9561020/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EEI48997.2019.00073,FPGA Implementation of Family Service Robot Based on Neural Network PID Motion Control System,IEEE,Conferences,"For the fixed invariability of control parameters in the PID closed-loop control algorithm of existing mobile robot, and the poor real-time response and stability of robot chassis to the upper machine motion control command, a robot motion control system is designed based on BP neural network PID motion control algorithm. Firstly, according to the three-wheel omni-directional mobile robot motion characteristics and the principle of neural network PID control algorithm, the control system is modeled and simulated on simulink, it theoretically demonstrates that the BP neural network PID closed-loop control algorithm is superior to the traditional PID control algorithm. The simulation results show that the overshoot is small and the real-time performance is good, which can greatly improve the flexibility and stability of the system. Then, through the top-down design method by Verilog language, the FPGA design of BP neural network PID closed-loop control system is carried out. The three-wheel omni-directional mobile robot chassis is used as the experimental platform, which is controlled by the robot upper machine to follow and avoid obstacles. The test results show that the control system improves the robot's running speed by 11.6% and accuracy by 13.4%. Compared with the open-loop control system, which effectively verifies the feasibility and practicability of the closed-loop control system.",https://ieeexplore.ieee.org/document/8990996/,2019 International Conference on Electronic Engineering and Informatics (EEI),8-10 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPT.2009.5377635,FPGA implementation of mixed integer quadratic programming solver for mobile robot control,IEEE,Conferences,"We propose a high-speed mixed integer quadratic programming (MIQP) solver on an FPGA. The MIQP solver can be applied to various optimizing applications including real-time robot control. In order to rapidly solve the MIQP problem, we implement reusing a first solution (first point), pipeline architecture, and multi-core architecture on the single FPGA. By making use of them, we confirmed that 79.5% of the cycle times are reduced, compared with straightforward sequential processing. The operating frequency is 67 MHz, although a core 2 duo PC requires 3.16 GHz in processing the same size problem. The power consumption of the MIQP solver is 4.2 W.",https://ieeexplore.ieee.org/document/5377635/,2009 International Conference on Field-Programmable Technology,9-11 Dec. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR52367.2021.9517666,FPGA-based Deep Learning Acceleration for Visual Grasping Control of Manipulator,IEEE,Conferences,"The vision-based robotic arm control system is an important solution for intelligent production, and the robotic arm visual grasping system based on deep learning is an important branch. Aiming at the requirements of fast visual recognition speed, low power consumption and high precision of mobile visual grasping robot, a deep learning target detection scheme based on FPGA hardware acceleration is proposed. Use Vivado and Petalinux development kit to build the software and hardware system, then deploy YOLOv3 model in the system. Experiments show that the solution meets the demand of robotic arm visual grasping, and the real-time performance is better. The recognition speed is 18 times that of the CPU, the power consumption is 1/13 of the GPU, and the cost is lower.",https://ieeexplore.ieee.org/document/9517666/,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),15-19 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2006.258689,Facial Tracking for an Emotion-Diagnosis Robot to Support E-Learning,IEEE,Conferences,"There have been a lot of researches on the detection/estimation of human emotions from facial expressions. However, most of them have extracted facial features for some specific emotions from the still pictures of artificial actions or performances. This paper describes facial tracking for e-learning support robot which can estimate a emotion of e-learning user from his/her facial expression in real-time; (1) the criteria of the facial expression to classify the eight emotions was obtained by the time sequential subjective evaluation on the emotions as well as the time sequential analysis of a facial expression by image processing. (2) The coincidence ratio between the discriminated emotions based upon the criteria of emotion diagnosis and the time sequential subjective evaluation on emotions for ten e-learning subjects was 69%. (3) Then, the possibility of the real time emotion diagnosis robot to support e-learning was confirmed by the facial image processing at the 15 frame/sec. rate as well as the simple emotion diagnosis algorithm based upon the Mahalonobis distance",https://ieeexplore.ieee.org/document/4028735/,2006 International Conference on Machine Learning and Cybernetics,13-16 Aug. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1997.633250,Facial interaction between animated 3D face robot and human beings,IEEE,Conferences,"We study the realization of a realistic human-like response of an animated 3D face robot in communicative interaction with human beings. The face robot can produce human-like facial expressions and recognize human facial expressions using facial image data obtained by a CCD camera mounted inside the left eyeball. We developed the real time machine recognition of facial expressions by using a layered neural network and achieved a high correct recognition ratio of 85% with respect to 6 typical facial expressions of 15 subjects in 55 ms. We also developed a new small-size actuator for display of facial expressions on the face robot, giving the same speed in dynamic facial expressions as in human even in the case of a high-speed expression of ""surprise"". For facial interactive communication between the face robot and human beings, we integrated these two technologies to produce the facial expression in respond to the recognition result of the human facial expression in real time. This implies a high technological potential for the animated face robot to undertake interactive communication with human when an artificial emotion being implemented.",https://ieeexplore.ieee.org/document/633250/,"1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation",12-15 Oct. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197159,Fast Adaptation of Deep Reinforcement Learning-Based Navigation Skills to Human Preference,IEEE,Conferences,"Deep reinforcement learning (RL) is being actively studied for robot navigation due to its promise of superior performance and robustness. However, most existing deep RL navigation agents are trained using fixed parameters, such as maximum velocities and weightings of reward components. Since the optimal choice of parameters depends on the use-case, it can be difficult to deploy such existing methods in a variety of real-world service scenarios. In this paper, we propose a novel deep RL navigation method that can adapt its policy to a wide range of parameters and reward functions without expensive retraining. Additionally, we explore a Bayesian deep learning method to optimize these parameters that requires only a small amount of preference data. We empirically show that our method can learn diverse navigation skills and quickly adapt its policy to a given performance metric or to human preference. We also demonstrate our method in real-world scenarios.",https://ieeexplore.ieee.org/document/9197159/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPS51978.2022.9816965,Fast Person Detector with Efficient Multi-level Contextual Block for Supporting Assistive Robot,IEEE,Conferences,"The robotic demand a vision method to work in real-time on embedded devices. Besides, an assistive robot requires person detection, which is widely used to help automatically interact with the user. This work presents a fast real-time person detection (Fast-PdNet) to localize human areas implemented on a Jetson Nano. This device has been commonly used as an embedded system and is suitable for synchronizing sensors and actuators. The proposed architecture contains layers of Convolutional Neural Network consisting of two main modules: backbone and detection. An efficient extractor module with a multi-level contextual block is employed to extract the spatial features quickly. It avoids high-cost computing to distinguish interest features of the human body and background features. The lightweight learning attention selects suspected specific features area without generating excessive parameters. The end-to-end training was conducted on MS COCO 2017 to generate efficiently weighted models. The Fast-PdNet achieves competitive performance with other light detectors evaluated on the MS COCO 2017, PASCAL VOC 2007, and 2012 datasets. Moreover, this detector can run 35 frames per second when working in real-time on Jetson Nano.",https://ieeexplore.ieee.org/document/9816965/,2022 IEEE 5th International Conference on Industrial Cyber-Physical Systems (ICPS),24-26 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISPACS.2018.8923369,Fast Recognition and Control of Walking Mode for Humanoid Robot Based on Pressure Sensors and Nearest Neighbor Search,IEEE,Conferences,"In this paper, we propose a nearest-neighbor multi-reference learning system for control of humanoid-robot movements, using real-time data from pressure sensors embedded in the robot feet, which is processed with parallelized pipeline architecture for high-speed recognition of actual surface conditions. A first nearest-neighbor (1-NN) classifier is used to recognize the most similar reference pattern in terms of the smallest Euclidean distance. Our proposed architecture achieves a classification time of about 2.4μ s with a total power consumption of 8.53mW at 100 MHz operating frequency when implemented on a low-cost FPGA (Cyclone-V GX-Series). The analysis results are further useful for a next-generation-ASIC-based AI-chip design for a robust real-time robot-learning system.",https://ieeexplore.ieee.org/document/8923369/,2018 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS),27-30 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967966,Fast and Safe Policy Adaptation via Alignment-based Transfer,IEEE,Conferences,"Applying deep reinforcement learning to physical systems, as opposed to learning in simulation, presents additional challenges in terms of sample efficiency and safety. Collecting large amounts of hardware demonstration data is time-consuming and the exploratory behavior of reinforcement learning algorithms may lead the system into dangerous states, especially during the early stages of training. To address these challenges, we apply transfer learning to reuse a previously learned policy instead of learning from scratch. In this paper, we propose a method where given a source policy, policy adaptation is performed via transfer learning to produce a target policy suitable for real-world deployment. For policy adaptation, alignment-based transfer learning is applied to trajectories generated by the source policy and their corresponding safe target trajectories. We apply this method to manipulators and show that the proposed method is applicable to both inter-task and inter-robot transfer whilst considering safety. We also show that the resulting target policy is robust and can be further improved with reinforcement learning.",https://ieeexplore.ieee.org/document/8967966/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.2005.1554338,Faster learning in embodied systems through characteristic attitudes,IEEE,Conferences,"Classical reinforcement learning is a general learning paradigm with wide applicability in many problem domains. Where embodied agents are concerned, however, it is unable to take advantage of the structured, regular nature of the physical world to maximise learning efficiency. Here, using a model of a three joint robot arm, we show initial learning accelerated by an order of magnitude using simple constraints to produce characteristic attitudes, implemented as part of the learning algorithm. We point out possible parallels with constraints on the movement of natural organisms owing to their detailed mechanical structure. The work forms part of our EMBER framework for reinforcement learning in embodied agents introduced and developed in 2004.",https://ieeexplore.ieee.org/document/1554338/,2005 International Symposium on Computational Intelligence in Robotics and Automation,27-30 June 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1993.292135,Fixed computation real-time sonar fusion for local navigation,IEEE,Conferences,"A system is described for the spatial and temporal fusion of multiple sonars into a dynamic model which serves as a basis for local navigation. In order to guarantee that the robot's actions remain in synchrony with the changing state of the world, there is a continuous mapping from sonar readings to local model to navigation plan and, finally, to actuator commands. Because of this tight coupling of sensing and action, the responsiveness of the system is limited by the computational power of the processor and the required fidelity of the fused model. To address this tradeoff, the system is implemented using the GAPPS/REX circuit-based language. Analysis of the resulting fixed sized circuits allows computation tradeoffs to be made between the system fidelity and the responsiveness required by the operating environment. A description is presented of the sensor fusion and navigation algorithms, as well as the results of the system controlling MITRE's mobile robot Uncle Bob.<>",https://ieeexplore.ieee.org/document/292135/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2014.7041373,Footstep planning on uneven terrain with mixed-integer convex optimization,IEEE,Conferences,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].",https://ieeexplore.ieee.org/document/7041373/,2014 IEEE-RAS International Conference on Humanoid Robots,18-20 Nov. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2000.973216,Force control in robotic assembly under extreme uncertainty using ANN,IEEE,Conferences,"Robotic assembly operations can be performed by specifying an exact model of the operation. However, the uncertainties involved during assembly make it difficult to conceive such a model In these cases, the use of a connectionist model may be advantageous. In this paper, the design of a robotic cell based on the adaptive resonance theory artificial neural network and a PC host-slave architecture that overcame these uncertainties is presented. Different sources of uncertainty under real conditions are identified and their contribution in a typical assembly operation evaluated. The robotic system is implemented using a PUMA 761 industrial robot with six degrees of freedom (DOF) and a force/torque (F/T) sensor attached to its wrist which conveys force information to the neural network controller (NNC). Results during assembly operations are presented which validate the approach. Furthermore, the method is generic and can be implemented onto other manipulators.",https://ieeexplore.ieee.org/document/973216/,"2000 26th Annual Conference of the IEEE Industrial Electronics Society. IECON 2000. 2000 IEEE International Conference on Industrial Electronics, Control and Instrumentation. 21st Century Technologies",22-28 Oct. 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AiDAS47888.2019.8970881,"Framework Of Malay Intelligent Autonomous Helper (Min@H): Text, Speech And Knowledge Dimension Towards Artificial Wisdom For Future Military Training System",IEEE,Conferences,"Industrial Revolution 4.0 is expected to improve the way of military training system. Most of the assistant systems use English for their Human Machine Interaction (HMI) such `SARA' a virtual socially aware robot assistant which exclude Malay socio-emotional aspects. This scenario opens a suggestion, to internalize socio-emotional aspects based on Malay culture, custom and beliefs to military autonomous training systems (i.e. MIN@H) that can improve the `collaborative' skills between Malaysian military personnel and the systems. Therefore, to increase the wisdom of the systems, they must have feature to capture information for their human users or helping human users to learn new knowledge and ensure the interaction is comfortable and engaging. For that reason, the systems must understand Malay language and be able to interpret emotion and expression behavior according to the Malay culture and custom, furthermore, the systems able to differentiate the level of user's understanding and build a good rapport or feeling of harmony that makes communication possible or easy between the systems and users. This concept of the systems is referred as Malay Artificial Wisdom System (AWS). There are three fundamental aspects to achieve the AWS. First, to computationally model the conversational strategies and rapport between the system and human users based-on user's understanding and system's articulation. Second, to computationally model, recognize and synthesize the emotion and expression behavior according to the Malay culture, custom and beliefs. Third, the AWS can do analytical reasoning and responding in relation to falsehood analysis and users' understanding level. Knowledge discovery and inference technique as well as HMI that cater the inputs and output of the MIN@H will be developed to accomplish the AWS concept. This program could embrace military training system in Malaysia to enhance military personnel skills and experts in various areas.",https://ieeexplore.ieee.org/document/8970881/,2019 1st International Conference on Artificial Intelligence and Data Sciences (AiDAS),19-19 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967568,From Pixels to Buildings: End-to-end Probabilistic Deep Networks for Large-scale Semantic Mapping,IEEE,Conferences,"We introduce TopoNets, end-to-end probabilistic deep networks for modeling semantic maps with structure reflecting the topology of large-scale environments. TopoNets build a unified deep network spanning multiple levels of abstraction and spatial scales, from pixels representing geometry of local places to high-level descriptions of semantics of buildings. To this end, TopoNets leverage complex spatial relations expressed in terms of arbitrary, dynamic graphs. We demonstrate how TopoNets can be used to perform end-to-end semantic mapping from partial sensory observations and noisy topological relations discovered by a robot exploring large-scale office spaces. Thanks to their probabilistic nature and generative properties, TopoNets extend the problem of semantic mapping beyond classification. We show that TopoNets successfully perform uncertain reasoning about yet unexplored space and detect novel and incongruent environment configurations unknown to the robot. Our implementation of TopoNets achieves real-time, tractable and exact inference, which makes these new deep models a promising, practical solution to mobile robot spatial understanding at scale.",https://ieeexplore.ieee.org/document/8967568/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2005.1490934,From Unknown Sensors and Actuators to Visually Guided Movement,IEEE,Conferences,"This paper describes a developmental system implemented on a real robot that learns a model of its own sensory and actuator apparatuses. There is no innate knowledge regarding the modality or representation of the sensoric input and the actuators, and the system relies on generic properties of the robot's world such as piecewise smooth effects of movement on sensory changes. The robot develops the model of its sensorimotor system by first performing random movements to create an informational map of the sensors. Using this map the robot then learns what effects the different possible actions have on the sensors. After this developmental process the robot can perform simple motion tracking",https://ieeexplore.ieee.org/document/1490934/,"Proceedings. The 4th International Conference on Development and Learning, 2005",19-21 July 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSE.2015.7248059,Fully integrated artificial intelligence solution for real time route tracking,IEEE,Conferences,"In this paper the authors propose a solution in which an intelligent algorithm - genetic algorithm in our case - is used to generate commands for a robot in real time, so that the robot can determine the optimal moves considering several aspects: route tracking and low power consumption. Genetic algorithms are intelligent solutions for multi-criteria optimization and using them to find solutions to the optimization problems containing constrictions. However, they were designed as algorithms running on computer and therefore cannot ensure rapid generation of the solutions. On the other hand, the problem of determining the optimal response to command a robot requires real time response. The paper presents a method for hardware implementation and integration in a FPGA circuit of a genetic algorithm, in order to accelerate the convergence and to generate solutions in real time.",https://ieeexplore.ieee.org/document/7248059/,2015 38th International Spring Seminar on Electronics Technology (ISSE),6-10 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1994.367848,Fuzzy neural network implementation of self tuning PID control systems,IEEE,Conferences,"The fuzzy cognitive map (FCM) is a powerful universal method for representation of knowledge in various domains. The fuzzy inference engine can be implemented in the form of a network of FCMs. FCM implementation of the inference engine provides a suitable mechanism for expert control systems and information engineers to embed acquired human expertise, which is often imprecise, vague, or incomplete. The exploitation of an online learning algorithm empowers the fuzzy inference engine with the ability to modify its incomplete or possibly inconsistent knowledge base resulting in continuous improvement of the embedded knowledge. The fact that learning is an inherent feature of neural networks has inspired several researchers with the idea of using neural networks to implement fuzzy inference engines capable of learning. This paper presents a method for neural network FCM implementation of the fuzzy inference engine using the fuzzy columnar neural network architecture (FCNA). In this method the available human expertise is mapped first into an initial set of weights for the neurons. A new learning algorithm is then used to enhance the embedded knowledge in the neural network as a result of real time experience. The fuzzy inference engine (the neural network FCM) is used in computer simulations to control the speed of an underwater autonomous mobile robot. Results and computer simulation experiments are presented along with an evaluation of the new approach.<>",https://ieeexplore.ieee.org/document/367848/,Proceedings of 1994 9th IEEE International Symposium on Intelligent Control,16-18 Aug. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2017.8324476,Generalized framework for the parallel semantic segmentation of multiple objects and posterior manipulation,IEEE,Conferences,"The end-to-end approach presented in this paper deals with the recognition, detection, segmentation and grasping of objects, assuming no prior knowledge of the environment nor objects. The proposed pipeline is as follows: 1) Usage of a trained Convolutional Neural Net (CNN) that recognizes up to 80 different classes of objects in real time and generates bounding boxes around them. 2) An algorithm to derive in parallel the pointclouds of said regions of interest (ROI). 3) Eight different segmentation methods to remove background data and noise from the pointclouds and obtain a precise result of the semantically segmented objects. 4) Registration of the object's pointclouds over time to generate the best possible model. 5) Utilization of an algorithm to detect an array of grasping positions and orientations based mainly on the geometry of the object's model. 6) Implementation of the system on the humanoid robot MyBot, developed in the RIT Lab at KAIST. 7) An algorithm to find the bounding box of the object's model in 3D to then create a collision object and add it to the octomap. The collision checking between robot's hand and the object is removed to allow grasping using the MoveIt libraries. 8) Selection of the best grasping pose for a certain object, plus execution of the grasping movement. 9) Retrieval of the object and moving it to a desired final position.",https://ieeexplore.ieee.org/document/8324476/,2017 IEEE International Conference on Robotics and Biomimetics (ROBIO),5-8 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1994.407570,Generation of optimal configuration for a redundant manipulator with a trained neural network,IEEE,Conferences,"Redundant manipulators have more degrees of freedom than what is absolutely necessary for performing a task. The extra degrees of freedom can be used for avoiding obstacles or to optimize certain performance indices like manipulability or task compatibility. Maximizing manipulability keeps the manipulator away from singularities and provides more velocity transmission ratios in all directions. Optimizing task compatibility improves the force/velocity transmission ratios in the specified directions. However, the real time implementation of various optimizing algorithms is difficult because of the need of large computing time. In the present work, robot configurations for an optimum performance index are computed throughout the workspace. These configurations are then used to train a layered feed forward neural network (FFNN). During operation of the robot, the trained neural net outputs optimal configurations in real-time. The neural net captures the gross behaviour of the training data rather than memorizing the individual data, as in a lookup table. Thus its output is smooth and ideally suited for control purposes. We have simulated this approach on a 3-DOF redundant planar manipulator and the results are discussed in this paper.<>",https://ieeexplore.ieee.org/document/407570/,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS'94),12-16 Sept. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2001.933128,Generation of optimal trajectory for real system of an underactuated manipulator,IEEE,Conferences,"Trajectory of an underactuated manipulator is usually generated according to both kinematics and dynamics of the manipulator, different to that of conventional manipulator. The real trajectory by a real system may differ greatly from the trajectory generated from the dynamics model because there always exist errors in the dynamic model, and the feedback control is less effective in an underactuated manipulator. A method for generation of optimal trajectory for the real system of an underactuated manipulator with nonholonomic constraints is proposed. By using this method, the dynamics model of a real system can be improved by learning, and an optimal trajectory is generated according to the model improved sequentially. The effectiveness of the method is confirmed by experiment with a golf swinging robot. The implementation and experimental results obtained of the control method are described.",https://ieeexplore.ieee.org/document/933128/,Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),21-26 May 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRIS.2016.8066077,Gesture based robotic arm control for meal time care using a wearable sensory jacket,IEEE,Conferences,"This work presents the development of a wireless, low cost, wearable sensor jacket for the purpose of controlling a robot arm by mimicking the motion and behaviour of a humans arm. The intended use of our system is to provide remote daily nursing care by using our system from a distant place such as a nursing home or hospital, to control a stationed robotic arm placed in an elderly or patient's home. The final system is comprised of a wearable jacket which is embedded with IMU and flex sensors to detect and track the wearers arm movements and behaviour. The system is capable of detecting up to 5 degrees of freedom of the human arm and replicate these motions using a 6 DOF robot arm. The usability, accuracy and precision of our jacket system is evaluated through a user study and the results demonstrated that our system was more accurate and easier to use for operators than a conventional robotic arm joystick controller. In a water bottle transfer task our developed wearable jacket system demonstrated an average error distance of 29.36mm from the target point, while the results using the conventional joystick demonstrated an average error distance of 37.48mm. Furthermore subjects using our system were able to complete the transfer task in an average time of 44.1s per trial which was more efficient than the joystick method in which subjects averaged 55.55s per trial. Finally, we report a feasibility study with the jacket and a subject to demonstrate the capability of this system of giving a patient water to drink. The feasibility experiment showed an 86.66% success rate in giving a patient water via video stream teleoperation control.",https://ieeexplore.ieee.org/document/8066077/,2016 IEEE International Symposium on Robotics and Intelligent Sensors (IRIS),17-20 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594070,HARK-Bird-Box: A Portable Real-time Bird Song Scene Analysis System,IEEE,Conferences,"This paper addresses real-time bird song scene analysis. Observation of animal behavior such as communication of wild birds would be aided by a portable device implementing a real-time system that can localize sound sources, measure their timing, classify their sources, and visualize these factors of sources. The difficulty of such a system is an integration of these functions considering the real-time requirement. To realize such a system, we propose a cascaded approach, cascading sound source detection, localization, separation, feature extraction, classification, and visualization for bird song analysis. Our system is constructed by combining an open source software for robot audition called HARK and a deep learning library to implement a bird song classifier based on a convolutional neural network (CNN). Considering portability, we implemented this system on a single-board computer, Jetson TX2, with a microphone array and developed a prototype device for bird song scene analysis. A preliminary experiment confirms a computational time for the whole system to realize a real-time system. Also, an additional experiment with a bird song dataset revealed a trade-off relationship between classification accuracy and time consuming and the effectiveness of our classifier.",https://ieeexplore.ieee.org/document/8594070/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2014.6958165,HOG based multi-object detection for urban navigation,IEEE,Conferences,"A necessary condition to perform a fully autonomous driving system in urban environment is to detect object types in real scenes. Visual object recognition is a key solution, but multi-object detection still remain unsolved. In this paper, we present a fast and efficient multi-object detection system built to recognize, at the same time, pedestrians cars and bicycles. For each target type, we construct a holistic detector in a cascade manner, using a dense overlapping grid based on histograms of oriented gradients (HOG). The selection of HOG features is obtained through a learning process using AdaBoost algorithm. Experiments have been conducted on the car-like robot Robucar, where the single detectors are combined and implemented on its embedded computer, which is endowed with a modular software platform. Results are promising as the system can process up to 20 fps with VGA images.",https://ieeexplore.ieee.org/document/6958165/,17th International IEEE Conference on Intelligent Transportation Systems (ITSC),8-11 Oct. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593709,"Hands and Faces, Fast: Mono-Camera User Detection Robust Enough to Directly Control a UAV in Flight",IEEE,Conferences,"We present a robust real-time system for simultaneous detection of hands and faces in RGB and gray-scale images, and a novel dataset used for training. Our goal is to provide a robust sensor front-end suitable for real-time human-robot interaction using face-engagement and gestures. Using hand-labelled videos obtained from real human-UAV interaction experiments, we re-trained the YOLOv2 Deep Convolutional Neural Network to detect only hands and faces. This model was then used to automatically label several much larger third-party datasets. After manual correction of these results, we modified and re-trained the model on all this labelled data. We obtain qualitatively good detection results at 60Hz on a commodity GPU: our simultaneous hand-and-face detector gives state of the art accuracy and speed in a hand detection benchmark and competitive results in a face detection benchmark. To demonstrate its effectiveness for human-robot interaction we describe its use as the input to a simple but practical gestural human-UAV interface for entertainment or industrial applications. All software, training and test data are freely available.",https://ieeexplore.ieee.org/document/8593709/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISVLSI.2016.101,Hardware Design Automation of Convolutional Neural Networks,IEEE,Conferences,"Convolutional Neural Networks (CNNs) are a variation of feed-forward Neural Networks inspired by the biological process in the visual cortex of animals. The interest in this supervised learning algorithm has rapidly grown in many fields like image and video recognition and natural language processing. Nowadays they have become the state of the art in various applications like mobile robot vision, video surveillance and Big Data analytics. The specific computation pattern of CNNs results to be highly suitable for hardware acceleration, in fact different types of accelerators have been proposed based on GPU, Field Programmable Gate Array (FPGA) and ASIC. In particular, in the embedded systems context, due to real time and power consumption challenges, it is crucial to find the right tradeoff between performance, energy efficiency, fast development round and cost. This work proposes a framework meant as a tool for the user to accelerate and simplify the design and the implementation of CNNs on FPGAs by leveraging High Level Synthesis, still providing a certain level of customization of the hardware design.",https://ieeexplore.ieee.org/document/7560201/,2016 IEEE Computer Society Annual Symposium on VLSI (ISVLSI),11-13 July 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAEE.2017.8255341,Hardware and software implementation of real time electrooculogram (EOG) acquisition system to control computer cursor with eyeball movement,IEEE,Conferences,"Human computer interface (HCI) is an emerging technology of neuroscience and artificial intelligence. Development of HCI system using bio signal e.g. Electrooculogram (EOG), Electromyogram (EMG), Electroencephalogram (EEG), Functional near-infrared spectroscopy (fNIRS) etc. are attracted more and more attention of researchers all over the world in recent years because through this it is possible to get acquainted with advanced technologies of artificial intelligence. This paper presents the design and implementation of a fully functional Electrooculogram (EOG) based human computer interface. In this work we have designed and implemented necessary hardware and software for EOG signal acquisition along with controlling hardware such as wheelchair, robotic arm, mobile robot etc., and move computer mouse cursor simultaneously using EOG signal. This interface has three portion: EOG signal acquisition and amplification, analog to digital conversion, and real time hardware and mouse cursor movement. Eye movement is detected by measuring potential difference between cornea and retina using five Ag-Agcl disposable electrodes. Frequency range of EOG signal is considered as 0.3 to 15Hz, so this frequency range is taken using an active high and low pass filter so that accurate EOG signal can be achieved. The analog output of the EOG signal from filter is converted into digital signal by using an Arduino. Arduino serialize the EOG data for calibration and provides a threshold reference point which is used for controlling Hardware. The Classification module e.g. Support Vector machine (SVM) and Linear Discriminant Analysis (LDA) classify live data with respect to the horizontal and vertical data. This works as a binary classifier and choose optimal hyper-plane between two variables. According to each update on the eye position, cursor automatically accelerated in particular direction. PyMouse module in python is used for this task. Eye gesture based Hardware like robot, wheelchair etc. control and mouse cursor movement are the principle outcome of this research work.",https://ieeexplore.ieee.org/document/8255341/,2017 4th International Conference on Advances in Electrical Engineering (ICAEE),28-30 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1993.292013,Hidden Markov model approach to skill learning and its application in telerobotics,IEEE,Conferences,"The problem of how human skill can be represented as a parametric model using a hidden Markov (HMM), and how an HMM-based skill model can be used to learn human skill, is discussed. The HMM is feasible for characterizing two stochastic processes, measurable action and immeasurable mental states that are involved in the skill learning. Based on the most likely performance criterion, the best action sequence can be selected from previously measured action data by modeling the skill as an HMM. This selection process can be updated in real-time by feeding new action data and modifying HMM parameters. The implementation of the proposed method in a teleoperation-controlled space robot is discussed. The results demonstrate the feasibility of the method.<>",https://ieeexplore.ieee.org/document/292013/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2018.00176,Hierarchical Control Architecture Regulating Competition between Model-Based and Context-Dependent Model-Free Reinforcement Learning Strategies,IEEE,Conferences,"Recent evidence in neuroscience and psychology suggests that a single reinforcement learning (RL) algorithm only accounts for less than 60% of the variance of human choice behavior in an uncertain and dynamic environment, where the amount of uncertainty in state-action-state transitions drift over time. The prediction performance further decreases when the size of the state space increases. We proposed a hierarchical context-dependent RL control framework that dynamically exerted control weights on model-based (MB) and multiple model-free (MF) RL strategies associated with different task goals. To properly assess the validity of the proposed method, we considered a two-stage Markov decision task (MDT) in which the three different types of context changed over time. We trained 57 different RL control models on a Caltech MDT data set; then, we assessed their prediction performance using a Bayesian model comparison. This large-scale computer simulation analysis revealed that the model providing the most accurate prediction was the version that implemented the competition between the MB and multiple goal-dependent MF RL strategies. The present study demonstrates the applicability of the goal-driven RL control to a variety of real-world human-robot interaction scenarios.",https://ieeexplore.ieee.org/document/8616172/,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",7-10 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2007.4371289,Hierarchical MMC Networks as a manipulable body model,IEEE,Conferences,"A cognitive control system for a walking robot should be able to solve from simple reactive tasks up to complex tasks, including tasks which need cognitive capabilities and setting up plans. Planning ahead involves some kind of internal representation: most important a model of the own body. Considering planning as mental simulation, this model must be fully functional: it is constrained in the same way as the body itself and it can move and be used in the same way as the body. This model can then be used to try out movements mentally without doing the action in reality. For this purpose it must be possible to decouple the body itself from the action controlling modules to use the original controllers for control of the internal representations. In this publication we introduce a hierarchical model, implemented as an recurrent neural network based on the MMC principle.",https://ieeexplore.ieee.org/document/4371289/,2007 International Joint Conference on Neural Networks,12-17 Aug. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2018.8502527,Holo Pick'n'Place,IEEE,Conferences,"In this paper we contribute to the research on facilitating industrial robot programming by presenting a concept for intuitive drag and drop like programming of pick and place tasks with Augmented Reality (AR). We propose a service-oriented architecture to achieve easy exchangeability of components and scalability with respect to AR devices and robot workplaces. Our implementation uses a HoloLens and a UR5 robot, which are integrated into a framework of RESTful web services. The user can drag recognized objects and drop them at a desired position to initiate a pick and place task. Although the positioning accuracy is unsatisfactory yet, our implemented prototype achieves most of the desired advantages to proof the concept.",https://ieeexplore.ieee.org/document/8502527/,2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA),4-7 Sept. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HAPTIC.2010.5444617,IN-HAPTICS: Interactive navigation using haptics,IEEE,Conferences,"We present a computational framework and experimental platform for robot navigation that allows for a user-friendly, graphical and haptic interaction with the human operator during the deployment process. The operator can see, feel, and manipulate the artificial potential field that drives the robot through an environment cluttered with obstacles. We present a case study in which the operator rescues a robot trapped in a local minimum of a navigation potential field.",https://ieeexplore.ieee.org/document/5444617/,2010 IEEE Haptics Symposium,25-26 March 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636439,Image-Based Joint State Estimation Pipeline for Sensorless Manipulators,IEEE,Conferences,"Motion planning is a largely solved problem for robot arms with joint state feedback, but remains an area of research for sensorless manipulators such as toy robot arms and heavy equipment such as excavators and cranes. A promising approach to this problem is deep learning, which employs a pre-trained convolutional neural network to identify manipulator links and estimate joint states from a monocular camera video feed. Whereas manual labeling of training image sets is tedious and non-transferable, a simulation environment can automatically generate labeled training image sets of any size. The issue is the gap between simulated and real-world images. This paper solves this problem by implementing a Generative Adversarial Network. The complete joint state estimation pipeline is implemented and tested in hardware experiments to validate our proposed approach.",https://ieeexplore.ieee.org/document/9636439/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISP-BMEI48845.2019.8965907,Implementation and Verification of a Virtual Testing System Based on ROS and Unity for Computer Vision Algorithms,IEEE,Conferences,"With the development of artificial intelligence technology, Computer vision algorithm is playing an increasingly important role. Computer vision algorithm testing is a vital link to ensure the safe and reliable operation of agents. However, the traditional testing methods based on real scenes provide single samples and are challenging to obtain ground truth, which makes it inefficient to test computer vision algorithms. To solve this problem, the testing method of computer vision algorithms using virtual scenes instead of real scenes has been applied. In this paper, Unity and robot operating system (ROS) are selected to build a virtual testing system named URCV for computer vision algorithms. The feasibility of the system and the influence of virtual scene elements on the testing of the monocular ORB_SLAM2 algorithm are verified, including the rendering path of Unity's RGB camera, texture accuracy, and illumination model.",https://ieeexplore.ieee.org/document/8965907/,"2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",19-21 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ELECSYM.2018.8615503,Implementation of Victims Detection Framework on Post Disaster Scenario,IEEE,Conferences,"Disasters are prone to occur in Indonesia due to geographical factors, such as tectonic plate movements, which can cause an earthquake. Earthquakes are one of the most frequent disasters, they have broad impacts in a short time and are unpredictable. Thus, an extensive search process in a short time is highly critical to determine the victims location. In this paper, a victims detection framework is developed starting from acquiring images using an unmanned aerial vehicle and further processing using convolutional neural network (CNN) to locate victims robustly on post-disaster. Input images are then sent to victim detector dedicated ground station server for further high processing robustly locating the possibility of victims. A simulation system mimicking a real environment is developed to test our framework in real time. A transmission protocol is also developed for effectively transmitting data between the robot and the server. The treatment on the detection process of the victim is different from the normal human detection, some pre-processing stages are applied to increase the variation of the given dataset. An embedded system is used for taking images and additional sensors data, such as location and time using Global Navigation Satellite System.",https://ieeexplore.ieee.org/document/8615503/,2018 International Electronics Symposium on Engineering Technology and Applications (IES-ETA),29-30 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSST.1998.660084,Implementation of a navigational neural network on a parallel DSP board,IEEE,Conferences,"This work presents a neural network architecture that is motivated by the learning and memory characteristics of a part of the brain known as hippocampus, which is important in navigational behavior in humans and animals. Neural networks perform nonlinear transformations on data to yield suitable classification or control actions. In our case, the navigation network takes the distance information as data and maps it to control actions by the mobile robot. Navigation is a very important engineering problem for unknown or hazardous environments to ensure the safety of equipment and human life. Hardware implementation can benefit applications in real time where speed is the major concern. Our objective is to implement such a navigational neural network in parallel so that real time performance can be achieved by using a parallel DSP board system. Supplementary studies are also being carried out on the IBM SP2 supercomputer to understand the design and scaling properties of the parallel algorithm.",https://ieeexplore.ieee.org/document/660084/,Proceedings of Thirtieth Southeastern Symposium on System Theory,10-10 March 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2008.4633972,Implementation of a neural network based visual motor control algorithm for A 7 DOF redundant manipulator,IEEE,Conferences,"This paper deals with visual-motor coordination of a 7 dof robot manipulator for pick and place applications. Three issues are dealt with in this paper - finding a feasible inverse kinematic solution without using any orientation information, resolving redundancy at position level and finally maintaining the fidelity of information during clustering process thereby increasing accuracy of inverse kinematic solution. A 3-dimensional KSOM lattice is used to locally linearize the inverse kinematic relationship. The joint angle vector is divided into two groups and their effect on end-effector position is decoupled using a concept called function decomposition. It is shown that function decomposition leads to significant improvement in accuracy of inverse kinematic solution. However, this method yields a unique inverse kinematic solution for a given target point. A concept called sub-clustering in configuration space is suggested to preserve redundancy during learning process and redundancy is resolved at position level using several criteria. Even though the training is carried out off-line, the trained network is used online to compute the required joint angle vector in only one step. The accuracy attained is better than the current state of art. The experiment is implemented in real-time and the results are found to corroborate theoretical findings.",https://ieeexplore.ieee.org/document/4633972/,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),1-8 June 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSCCN.2011.6024574,Implementation of neural network based controller using Verilog,IEEE,Conferences,"A simple and robust real time controller that works very well for linear systems with optimal gain tunings is the PID controller. But, PID controllers do not work properly if plant dynamics are changing fast or when the plant is highly nonlinear. However, many of the industries still rely on it. Hence, in most of the plants an auxiliary controller coexists to help the primary PID controller to work better by compensating for uncertainties present during control operation. Neural network has proven to be a good candidate as this auxiliary nonlinear controller. Neural network can effectively compensate for unknown uncertainties and also act as an intelligent control. The success of the neural network as an auxiliary controller has been reported in practical applications such as motion control system, signal processing and controlling robot manipulators. Nowadays, parallel programmable logic devices, such as the field programmable gate array (FPGA), have become powerful hardware options, offering low cost, high execution speed, reconfigurability and parallelism. This work intends to exploit the current available resources in commercial FPGAs to implement servo control for hard disk drive system. Simulation and experimental results included in this paper show the viability of exploiting the parallelism and modularity of a Virtex 6 FPGA to implement a high sampling rate neural network-RBF based controller. This control system platform will allow fast prototyping of new control concepts and evaluation of non-linear control.",https://ieeexplore.ieee.org/document/6024574/,"2011 International Conference on Signal Processing, Communication, Computing and Networking Technologies",21-22 July 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1995.525808,Implementation of real time spatial mapping in robotic systems through self-organizing neural networks,IEEE,Conferences,"Presents a methodology which allows an autonomous agent i.e., a mobile robot, to learn and build maps of its operating environment by relying only on its range sensors. The maps, described with respect to the robot's inertial frame, are developed in real time by correlating robot position and sensory data. This latter feature characterizes part of the uniqueness of the authors' approach. These maps are topologically isomorphic to the maps created for the same room(s) by humans. The methodology exploits the principle of self-organization, implemented as an artificial neural network module which processes incoming sensor range data. The generation of environmental maps can be visualized as an elastic string of neurons whereby every neuron represents a finite portion of the physical world. This elastic string stretches dynamically so as to take on the shape of the environment, a unique characteristic of the authors' methodology. In this respect, the neural net provides a discretized representation of the ""continuous"" physical environment as the latter is seen through the robot's own sensors. Experiments, focused on indoor applications, have successfully demonstrated the ability of a robot to build maps of geometrically complex environments. The results presented in this paper, compared with the authors' earlier efforts, show significant improvement in that every single sensor data point contributes equally to the location of the neurons of the spatial map at the end of the learning process. This is important because the authors wish to minimize the effect of the order in which data points are processed.",https://ieeexplore.ieee.org/document/525808/,Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots,5-9 Aug. 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMTMA.2018.00075,Implementing Multi-DOF Trajectory Tracking Control System for Robotic Arm Experimental Platform,IEEE,Conferences,"To implement the control system of a multi-DOF robotic manipulator (Dobot), the robot dynamics, trajectory planning algorithm and motion control strategy are studied for designing the trajectory tracking control system. In this paper, the hardware and software of Dobot magician control system are designed. The hardware mainly includes STM32 controller. The software part mainly builds the host computer display interface, completes the protocol communication between the robot manipulator and the PC, so as to realize the trajectory tracking control of the robot manipulator and implement the track-following in real time. The experimental results show that the control system can accurately track the trajectory of robotic manipulator with a certain degree of real-time and stability.",https://ieeexplore.ieee.org/document/8337386/,2018 10th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA),10-11 Feb. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.1996.547231,Incremental learning for vision-based navigation,IEEE,Conferences,"In this paper, we explore the issue of incremental learning for autonomous navigation of a mobile robot. The autonomous navigation problem is regarded as a content-based retrieval problem where the robot learns the navigation experience using a hierarchical recursive partition tree (RPT). During real navigation, each time a new image is grabbed to retrieve the learned tree. The associated control signals of the retrieved are used to control the new action of the robot. Use of RPT can achieve efficient retrieval. In the proposed incremental learning scheme, a new image with the associated control signals is learned or rejected according to whether its retrieved output control signals are within tolerance of the desired control signals of the input query image. We use the eigen-subspace method for feature extraction in our incremental learning. The proposed algorithm has a real-time implementation for both learning and performance phases. Experimental results are shown to confirm the effectiveness of proposed method.",https://ieeexplore.ieee.org/document/547231/,Proceedings of 13th International Conference on Pattern Recognition,25-29 Aug. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2010.5650519,Incremental motion primitive learning by physical coaching using impedance control,IEEE,Conferences,"We present an approach for kinesthetic teaching of motion primitives for a humanoid robot. The proposed teaching method allows for iterative execution and motion refinement using a forgetting factor. During the iterative motion refinement, a confidence value specifies an area of allowed refinement around the nominal trajectory. A novel method for continuous generation of motions from a hidden Markov model (HMM) representation of motion primitives is proposed, which incorporates relative time information for each state. On the real-time control level, the kinesthetic teaching is handled by a customized impedance controller, which combines tracking performance with soft physical interaction and allows to implement soft boundaries for the motion refinement. The proposed methods were implemented and tested using DLR's humanoid upper-body robot Justin.",https://ieeexplore.ieee.org/document/5650519/,2010 IEEE/RSJ International Conference on Intelligent Robots and Systems,18-22 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2010.5509682,Indoor scene recognition through object detection,IEEE,Conferences,"Scene recognition is a highly valuable perceptual ability for an indoor mobile robot, however, current approaches for scene recognition present a significant drop in performance for the case of indoor scenes. We believe that this can be explained by the high appearance variability of indoor environments. This stresses the need to include high-level semantic information in the recognition process. In this work we propose a new approach for indoor scene recognition based on a generative probabilistic hierarchical model that uses common objects as an intermediate semantic representation. Under this model, we use object classifiers to associate low-level visual features to objects, and at the same time, we use contextual relations to associate objects to scenes. As a further contribution, we improve the performance of current state-of-the-art category-level object classifiers by including geometrical information obtained from a 3D range sensor that facilitates the implementation of a focus of attention mechanism within a Monte Carlo sampling scheme. We test our approach using real data, showing significant advantages with respect to previous state-of-the-art methods.",https://ieeexplore.ieee.org/document/5509682/,2010 IEEE International Conference on Robotics and Automation,3-7 May 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1991.131908,Instinctive behaviors and personalities in societies of cellular robots,IEEE,Conferences,"A description is presented of the social organization of societies of cellular mobile units featuring instinctive behavior. Each robotic unit has its own personality and lives independently from the others. Useful tasks are carried out through collaboration rather than by individual effort. The behavior of each unit derives from a subsumption-like control structure, which emphasizes the roles of innate personality, external stimuli, and communication. A number of different robotic personalities are described and techniques of implementing them in real robot units are outlined. The implementation of instinctive behavior is described for the case of a robotic vehicle system (ROBBIE).<>",https://ieeexplore.ieee.org/document/131908/,Proceedings. 1991 IEEE International Conference on Robotics and Automation,9-11 April 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICALT.2006.1652652,Instruction Through The Ages: Building Pervasive Virtual Instructors for Life Long Learning,IEEE,Conferences,"A pervasive virtual instructor is an artificially intelligent instructor that may appear transparent to the learner or appear in the form of a three-dimensional graphical character, digital toy, or robot with capabilities to inhabit mixed reality environments, and provide personalized instruction anytime, anywhere, and at an-pace. Similarly to pedagogical agents, or traditional virtual instructors, pervasive virtual instructors are expected to behave autonomously, respond to human verbal/non-verbal input, and deliver information to human users. Unique to pervasive virtual instructors in this paper are capabilities to provide instruction across distributed networks, interact with human learners using context-aware intelligence, and apply empirically researched pedagogical/andragogical techniques. Technology challenges remain for building pervasive virtual instructors to achieve aforementioned capabilities. This paper summarizes technical challenges and an approach for building pervasive instructors that provide life long instructional services",https://ieeexplore.ieee.org/document/1652652/,Sixth IEEE International Conference on Advanced Learning Technologies (ICALT'06),5-7 July 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2007.378811,Integrating high-level sensor features via STDP for bio-inspired navigation,IEEE,Conferences,"Correlation based algorithms have been found to explain many basic behaviors in simple animals. In this paper the authors investigate the problem of navigation control of a robot from the viewpoint of bio-inspired perception. In this paper the authors study how to go up, through learning, from the implementation of a reactive system, towards behaviors of increasing complexity. The whole control system is based on networks of spiking neurons. A correlation based rule, namely the spike timing dependent plasticity (STDP), is implemented for an efficient learning. The main interesting consequence is that the system is able to learn high-level sensor features, based on a set of basic reflexes, depending on some low-level sensor inputs. The whole methodology is presented through simulation results and also through its implementation on an FPGA based system for real time working on a roving robot.",https://ieeexplore.ieee.org/document/4252708/,2007 IEEE International Symposium on Circuits and Systems,27-30 May 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS-SBR.2016.49,Integration of People Detection and Simultaneous Localization and Mapping Systems for an Autonomous Robotic Platform,IEEE,Conferences,"This paper presents the implementation of a people detection system for a robotic platform able to perform Simultaneous Localization and Mapping (SLAM), allowing the exploration and navigation of the robot considering people detection interaction. The robotic platform consists of a Pioneer 3DX robot equipped with an RGB-D camera, a Sick Lms200 sensor laser and a computer using the robot operating system ROS. The idea is to integrate the people detection system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. Furthermore, this paper presents an evaluation of two different approaches for the people detection system. The first one uses a manual feature extraction technique, and the other one is based on deep learning methods. The manual feature extraction method in the first approach is based on HOG (Histogram of Oriented Gradients) detectors. The accuracy of the techniques was evaluated using two different libraries. The PCL library (Point Cloud Library) implemented in C ++ and the VLFeat MatLab library with two HOG variants, the original one, and the DPM (Deformable Part Model) variant. The second approaches are based on a Deep Convolutional Neural Network (CNN), and it was implemented using the MatLab MatConvNet library. Tests were made objecting the evaluation of losses and false positives in the people's detection process in both approaches. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.",https://ieeexplore.ieee.org/document/7783535/,2016 XIII Latin American Robotics Symposium and IV Brazilian Robotics Symposium (LARS/SBR),8-12 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII52469.2022.9708896,Integration of a reconfigurable robotic workcell for assembly operations in automotive industry,IEEE,Conferences,"This paper deals with the integration of a flexible, reconfigurable work cell performing assembly of parts in the automotive industry. The unique feature of the developed cell is that it can function in two modes: a) entirely autonomously or b) in cooperation with a human, where the operation of the robot dynamically adapts to human actions. We have implemented technologies for online recognition of human intention and for real-time learning of robust assembly policies to achieve the desired outcome. This challenging goals dictate the integration of modern deep learning algorithms, statistical learning, and compliant robot control into a unique ROS-based robot control system.",https://ieeexplore.ieee.org/document/9708896/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2011.6119682,Integration of grey model and neural network for robotic application,IEEE,Conferences,"This paper proposes an intelligent forecasting system based on a feedforward neural network aided grey model (FNAGM), integrating a first-order single variable grey model (GM(1,1)) and a feedforward neural network. The system includes three phases: initialization phase, GM(1,1) prediction phase, and FNAGM prediction phase. A number of parameters required for the FNAGM are selected in the initialization phase. A one-step ahead predictive value is generated in the GM(1,1) prediction phase, followed by the implementation of a feedforward neural network used to determine the prediction error of the GM(1,1) and compensate for it in the FNAGM prediction phase. We also adopted on-line batch training to adjust the network according to the Levenberg-Marquardt algorithm in real-time. According to the experimental results of a robot, the proposed intelligent forecasting system can provide high accuracy for both trajectory prediction and target tracking.",https://ieeexplore.ieee.org/document/6119682/,IECON 2011 - 37th Annual Conference of the IEEE Industrial Electronics Society,7-10 Nov. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WOCC.2018.8372718,Integration of open source platform duckietown and gesture recognition as an interactive interface for the museum robotic guide,IEEE,Conferences,"In recent years, population aging becomes a serious problem. To decrease the demand for labor when navigating visitors in museums, exhibitions, or libraries, this research designs an automatic museum robotic guide which integrates image and gesture recognition technologies to enhance the guided tour quality of visitors. The robot is a self-propelled vehicle developed by ROS (Robot Operating System), in which we achieve the automatic driving based on the function of lane-following via image recognition. This enables the robot to lead guests to visit artworks following the preplanned route. In conjunction with the vocal service about each artwork, the robot can convey the detailed description of the artwork to the guest. We also design a simple wearable device to perform gesture recognition. As a human machine interface, the guest is allowed to interact with the robot by his or her hand gestures. To improve the accuracy of gesture recognition, we design a two phase hybrid machine learning-based framework. In the first phase (or training phase), k-means algorithm is used to train historical data and filter outlier samples to prevent future interference in the recognition phase. Then, in the second phase (or recognition phase), we apply KNN (k-nearest neighboring) algorithm to recognize the hand gesture of users in real time. Experiments show that our method can work in real time and get better accuracy than other methods.",https://ieeexplore.ieee.org/document/8372718/,2018 27th Wireless and Optical Communication Conference (WOCC),30 April-1 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2014.6889647,Intelligent Facial Action and emotion recognition for humanoid robots,IEEE,Conferences,"This research focuses on the development of a realtime intelligent facial emotion recognition system for a humanoid robot. In our system, Facial Action Coding System is used to guide the automatic analysis of emotional facial behaviours. The work includes both an upper and a lower facial Action Units (AU) analyser. The upper facial analyser is able to recognise six AUs including Inner and Outer Brow Raiser, Upper Lid Raiser etc, while the lower facial analyser is able to detect eleven AUs including Upper Lip Raiser, Lip Corner Puller, Chin Raiser, etc. Both of the upper and lower analysers are implemented using feedforward Neural Networks (NN). The work also further decodes six basic emotions from the recognised AUs. Two types of facial emotion recognisers are implemented, NN-based and multi-class Support Vector Machine (SVM) based. The NN-based facial emotion recogniser with the above recognised AUs as inputs performs robustly and efficiently. The Multi-class SVM with the radial basis function kernel enables the robot to outperform the NN-based emotion recogniser in real-time posed facial emotion detection tasks for diverse testing subjects.",https://ieeexplore.ieee.org/document/6889647/,2014 International Joint Conference on Neural Networks (IJCNN),6-11 July 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAMechS.2016.7813486,Intelligent adaptive precrash control for autonmous vehicle agents (CBR Engine & hybrid A path planner),IEEE,Conferences,"PreCrash problem of Intelligent Control of autonomous vehicles robot is a very complex problem, especially vehicle pre-crash scenariws and at points of intersections in real-time environmenta. This Paper presents a novel architecture of Intelligent adaptive control for autonomous vehicle agent that depends on Artificial Intelligence Techniques that applies case-based reasoning techniques, where Parallel CBR Engines are implemented for different scenarios' of PreCrash problem and sub-problems of intersection safety and collision avoidance, in the higher level of the controller and A∗ path planner for path planning and at lower-levels it also uses some features of autonomous vehicle dynamics. Moreover, the planner is enhanced by combination of Case-Based Planner. All modules are presented and discussed. Experimental results are conducted in the framework of Webots autonomous vehicle tool and overall results are good for the CBR Engine for Adaptive control and also for the hybrid Case-Based Planner, A∗ and D∗ motion planner along with conclusion and future work.",https://ieeexplore.ieee.org/document/7813486/,2016 International Conference on Advanced Mechatronic Systems (ICAMechS),30 Nov.-3 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2003.1253948,Internet-based remote control by using Adaline neural networks,IEEE,Conferences,"In this paper, we present a remote control scheme for Internet-based teleoperation. This control scheme relies on the real-time estimation of concurrent roundtrip delays in order to optimally assign tasks between the user and the robot. For this purpose, we employ an adaptive linear (Adaline) neural network for which most conventional learning algorithms are infeasible since the computation is usually too intensive to be practical. To get around this problem, we introduce a novel learning algorithm that is based on the maximum entropy principle. Compared to traditional learning algorithms, the computing cost of this algorithm is very low, which makes it possible for the proposed neural network to be implemented on-line in real-time.",https://ieeexplore.ieee.org/document/1253948/,Proceedings of the 2003 IEEE International Symposium on Intelligent Control,8-8 Oct. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSCS52333.2021.9497411,Inverted Pendulum Control with a Robotic Arm using Deep Reinforcement Learning,IEEE,Conferences,"Inverted pendulum control is a benchmark control problem that researchers have used to test the new control strategies over the past 50 years. Deep Reinforcement Learning Algorithm is used recently on the inverted pendulum on a straightforward form. The inverted pendulum had only one degree of freedom and was moving on a plane. This paper demonstrates a successful implementation of a deep reinforcement learning algorithm on an inverted pendulum that rotates freely on a spherical joint with an industrial 6 degrees freedom robot arm. This research used the Deep Reinforcement Learning algorithm in Robot Operating System (ROS) and Gazebo Simulation. Experimental results show that the proposed method achieved promising outputs and reaches the control objectives. We were able to control the inverted pendulum upward for 30 and 20 seconds in two case studies. Two other significant novelties in this research are using an inertial measurement unit (IMU) on the tip of the pendulum, that will facilitate implementation on the real robot for future work and different reward functions in comparing to past publications that enable continuous learning and mastering control in a vertical position",https://ieeexplore.ieee.org/document/9497411/,"2021 International Symposium on Signals, Circuits and Systems (ISSCS)",15-16 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACC.1994.735001,Investigation of kinematics and inverse dynamics algorithm with a DSP implementation of a neural network,IEEE,Conferences,"An investigation is described to demonstrate the benefits which can be gained by using a digital signal processor (DSP) to implement robot related control schemes, kinematics, and inverse dynamics with a neural network. A neural network adaptive controller is given and applied to a robot manipulator having a closed kinematic chain, a configuration which is not well suited to the popular serial link algorithms. The Lyapunov's stability approach is used to develop a learning rule for the neural network controller that would guarantee the stability of the training process under mild conditions. The controller hardware consists of a PC-386, a fixed point DSP, and a floating point DSP. The software installed on each of these processors has the requirements of satisfying the specific responsibility assigned to that processor and of communicating with other processors so that necessary data is passed on in a timely manner. A computational software package has been built to further enhance the speed of the general control scheme and the neural network algorithm. The techniques used in the DSP implementation of the adaptive control algorithm in real-time are also discussed.",https://ieeexplore.ieee.org/document/735001/,Proceedings of 1994 American Control Conference - ACC '94,29 June-1 July 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.2009.5221750,Joint control of ROBOKER arm using a neural chip embedded on FPGA,IEEE,Conferences,"This paper presents implementation of a neural chip to proceed neural processing of the radial basis function (RBF) network. RBF network along with a primary PD controller is trained in on-line fashion. Radial basis function network processing is embedded on a field programmable gate array(FPGA) chip to achieve real-time control. To enable nonlinear function calculation, a floating point processor is designed to allow assembly programming for learning algorithm. Other necessary hardware modules for control purposes are also designed and implemented. A humanoid robot called the ROBOKER with two arms of 6 degrees-of-freedom each is controlled. Joint angles of the ROBOKER arms are controlled and tracking performances by the neural chip are compared with those by PD controllers.",https://ieeexplore.ieee.org/document/5221750/,2009 IEEE International Symposium on Industrial Electronics,5-8 July 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICALT.2003.1215101,Kana-input navigation system for kids based on the cyber assistant,IEEE,Conferences,"In Japan, it has increased the opportunity for young children to experience the personal computer in elementary schools. However, in order to use computer, many domestic barriers have confronted young children (kids) because they cannot read Kanji characters and had not learnt Roman alphabet yet. As a result, they cannot input text strings by JIS keyboard. We developed Kana-input navigation system for kids (KINVS) based on the cyber assistant system (CAS). CAS is a human-style software robot based on the 3D-CG real-time animation and voice synthesis technology. KINVS enables to input Hiragana/Katakana characters by mouse operation only (without keyboard) and CAS supports them by using speaking, facial expression, body action and sound effects. KINVS displays the 3D-stage like a classroom. In this room, blackboard, interactive parts to input Kana-characters, and CAS are placed. Mouse input method of KINVS are designed to use only single click and wheeler rotation. To input characters, kids clicks or rotates the interactive parts. KINVS reports all information by voice speaking and Kana subtitles instead of Kanji text. Furthermore, to verify the functional feature of KINVS, we measured how long kids had taken to input long text by using KINVS.",https://ieeexplore.ieee.org/document/1215101/,Proceedings 3rd IEEE International Conference on Advanced Technologies,9-11 July 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636547,Learning Contact-Rich Assembly Skills Using Residual Admittance Policy,IEEE,Conferences,"Contact-rich assembly tasks may result in large and unpredictable forces and torques when the locations of the contacting parts are uncertain. The ability to correct the trajectory in response to haptic feedback and accomplish the task despite location uncertainties is an important skill. We hypothesize that this skill would facilitate generalization and support direct transfer from simulations to real world. To reduce sample complexity, we propose to learn a residual admittance policy (RAP). RAP is learned to correct the movements generated by a baseline policy in the framework of dynamic movement primitives. Given the reference trajectories generated by the baseline policy, the action space of RAP is limited to the admittance parameters. Using deep reinforcement learning, a deep neural network is trained to map task specifications to proper admittance parameters. We demonstrate that RAP handles uncertainties in board location, generalizes well over space, size and shape, and facilitates quick transfer learning. Most impressively, we demonstrate that the policy learned in simulations achieves similar robustness to uncertainties, generalization and performance when deployed on an industrial robot (UR5e) without further training. See accompanying video for demonstrations.",https://ieeexplore.ieee.org/document/9636547/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811624,Learning Design and Construction with Varying-Sized Materials via Prioritized Memory Resets,IEEE,Conferences,"Can a robot autonomously learn to design and construct a bridge from varying-sized blocks without a blueprint? It is a challenging task with long horizon and sparse reward - the robot has to figure out physically stable design schemes and feasible actions to manipulate and transport blocks. Due to diverse block sizes, the state space and action trajectories are vast to explore. In this paper, we propose a hierarchical approach for this problem. It consists of a reinforcement-learning designer to propose high-level building instructions and a motion-planning-based action generator to manipulate blocks at the low level. For high-level learning, we develop a novel technique, prioritized memory resetting (PMR) to improve exploration. PMR adaptively resets the state to those most critical configurations from a replay buffer so that the robot can resume training on partial architectures instead of from scratch. Furthermore, we augment PMR with auxiliary training objectives and fine-tune the designer with the locomotion generator. Our experiments in simulation and on a real deployed robotic system demonstrate that it is able to effectively construct bridges with blocks of varying sizes at a high success rate. Demos can be found at https://sites.google.com/view/bridge-pmr.",https://ieeexplore.ieee.org/document/9811624/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196785,Learning Resilient Behaviors for Navigation Under Uncertainty,IEEE,Conferences,"Deep reinforcement learning has great potential to acquire complex, adaptive behaviors for autonomous agents automatically. However, the underlying neural network polices have not been widely deployed in real-world applications, especially in these safety-critical tasks (e.g., autonomous driving). One of the reasons is that the learned policy cannot perform flexible and resilient behaviors as traditional methods to adapt to diverse environments. In this paper, we consider the problem that a mobile robot learns adaptive and resilient behaviors for navigating in unseen uncertain environments while avoiding collisions. We present a novel approach for uncertainty-aware navigation by introducing an uncertainty-aware predictor to model the environmental uncertainty, and we propose a novel uncertainty-aware navigation network to learn resilient behaviors in the prior unknown environments. To train the proposed uncertainty-aware network more stably and efficiently, we present the temperature decay training paradigm, which balances exploration and exploitation during the training process. Our experimental evaluation demonstrates that our approach can learn resilient behaviors in diverse environments and generate adaptive trajectories according to environmental uncertainties.",https://ieeexplore.ieee.org/document/9196785/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IIAI-AAI.2013.74,Learning Which Features to Imitate in a Painting Task,IEEE,Conferences,"Learning is essential for an autonomous agent to adapt to an environment. One method of learning is through trial and error, however, this method is impractical in a complex environment because of the long learning time required by the agent. Therefore, guidelines are necessary in order to expedite the learning process in such environments, and imitation is one such guideline. Sakato, Ozeki, and Oka (2012) recently proposed a computational model of imitation and autonomous behavior by which an agent can reduce its learning time through imitation. In this paper, we apply the model to a real robot, Nao, and evaluate the model using simple features in a simple environment. We also report on the progress of implementation of the model, and evaluations of the performance of imitation using the implemented model. Our experimental results indicate that the model adapted to the experimental environment by imitation.",https://ieeexplore.ieee.org/document/6630378/,2013 Second IIAI International Conference on Advanced Applied Informatics,31 Aug.-4 Sept. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMR48346.2021.9661514,Learning from Demonstrations for Autonomous Soft-tissue Retraction,IEEE,Conferences,"The current research focus in Robot-Assisted Minimally Invasive Surgery (RAMIS) is directed towards increasing the level of robot autonomy, to place surgeons in a supervisory position. Although Learning from Demonstrations (LfD) approaches are among the preferred ways for an autonomous surgical system to learn expert gestures, they require a high number of demonstrations and show poor generalization to the variable conditions of the surgical environment. In this work, we propose an LfD methodology based on Generative Adversarial Imitation Learning (GAIL) that is built on a Deep Reinforcement Learning (DRL) setting. GAIL combines generative adversarial networks to learn the distribution of expert trajectories with a DRL setting to ensure generalisation of trajectories providing human-like behaviour. We consider automation of tissue retraction, a common RAMIS task that involves soft tissues manipulation to expose a region of interest. In our proposed methodology, a small set of expert trajectories can be acquired through the da Vinci Research Kit (dVRK) and used to train the proposed LfD method inside a simulated environment. Results indicate that our methodology can accomplish the tissue retraction task with human-like behaviour while being more sample-efficient than the baseline DRL method. Towards the end, we show that the learnt policies can be successfully transferred to the real robotic platform and deployed for soft tissue retraction on a synthetic phantom.",https://ieeexplore.ieee.org/document/9661514/,2021 International Symposium on Medical Robotics (ISMR),17-19 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IS.2018.8710525,Learning from Virtual Experience: Mapless Navigation with Neuro-Fuzzy Intelligence,IEEE,Conferences,"Traditional robot navigation approaches normally rely on creating a precise map of the environment which is a computationally expensive procedure and highly depends on an accurate sensory system. Even for motion planning in similar terrains, the planner needs to prepare or obtain a map beforehand. In this paper, this issue is addressed, and a neurofuzzy motion planner is presented for mobile robot navigation without a map. We show that, by means of a virtual experience model and a neuro-fuzzy system, a mapless motion planning approach can learn basic navigation primitives in simple obstacle arrangements without any prior demonstration. The virtual experience model creates a large number of test environments with a random set of arbitrarily shaped obstacles and places the robot in a random pose with different start and goal positions in different instances. Then, based on the readings of the robot's sensors and a collection of predefined general linguistic rules, a set of control commands including the robot's linear and angular velocity is calculated as the outputs of the virtual experience. The resulting dataset is then loaded into an adaptive neuro-fuzzy inference system to create and optimize a fuzzy motion planner using the subtractive clustering method and a hybrid technique combining the back-propagation algorithm and the least square adaptation method respectively, which guides the robot in simple unknown environments without requiring a global obstacle map. To validate the effectiveness of the proposed model, the motion planner was implemented on a nonholonomic differential drive robot to test its performance in two real navigation tasks. Experimental studies show that the proposed mapless motion planner can efficiently guide the robot in similar arrangements of convex obstacles.",https://ieeexplore.ieee.org/document/8710525/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2010.5641727,Learning from conflicts in real world environments for the realization of Cognitive Technical Systems,IEEE,Conferences,"In this contribution, a novel learning method realizing the refinement of a Cognitive Technical System's pattern recognition and attention capabilities is presented. The method is implemented within a cognitive architecture with a representational level based on Situation-Operator-Modeling and high-level Petri Nets. Through the representational level, it is possible to realize a mental model mapping the complex structure of the real world internally in a compact format reduced to the relevant aspects. The mental model can be created and modified automatically by learning from interaction. If the perceived real world does not correspond to the system's mental model, the system detects ambiguities (or conflicts) inevitably. Then, the system tries to solve the conflicts by a more detailed view to the measured sensor inputs. Thus, new significant features (on a high abstraction level) can be derived from the measurements and taken into account to distinguish different (before apparently equal) situations. The contribution describes the proposed method and its fundamentals in detail. Furthermore, the realization of a cognitive mobile robot is presented as an application example illustrating the proposed method.",https://ieeexplore.ieee.org/document/5641727/,"2010 IEEE International Conference on Systems, Man and Cybernetics",10-13 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDC.1998.761747,"Learning helicopter control through ""teaching by showing""",IEEE,Conferences,"A model-free ""teaching by showing"" methodology is developed to train a fuzzy-neural controller for an autonomous robot helicopter. The controller is generated and tuned using training data gathered while a teacher operates the helicopter. A hierarchical behavior-based control architecture is used, with each behavior implemented as a hybrid fuzzy logic controller (FLC) and general regression neural network controller (GRNNC). The FLCs and GRNNCs are generated through ""teaching by showing"". The FLCs are built during initial controller generation, remain static once created, and provide coarse control of the helicopter. The GRNNCs are incrementally built and modified whenever the controller does not meet performance criteria, are dynamic, and provide fine control, enhancing the control of the FLCs. The methodology has been successfully applied in simulation and, in the future, will be applied on a radio control model helicopter for real world validation.",https://ieeexplore.ieee.org/document/761747/,Proceedings of the 37th IEEE Conference on Decision and Control (Cat. No.98CH36171),18-18 Dec. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2012.6224553,Learning organizational principles in human environments,IEEE,Conferences,"In the context of robotic assistants in human everyday environments, pick and place tasks are beginning to be competently solved at the technical level. The question of where to place objects or where to pick them up from, among other higher-level reasoning tasks, is therefore gaining practical relevance. In this work, we consider the problem of identifying the organizational structure within an environment, i.e. the problem of determining organizational principles that would allow a robot to infer where to best place a particular, previously unseen object or where to reasonably search for a particular type of object given past observations about the allocation of objects to locations in the environment. This problem can be reasonably formulated as a classification task. We claim that organizational principles are governed by the notion of similarity and provide an empirical analysis of the importance of various features in datasets describing the organizational structure of kitchens. For the aforementioned classification tasks, we compare standard classification methods, reaching average accuracies of at least 79% in all scenarios. We thereby show that, in particular, ontology-based similarity measures are well-suited as highly discriminative features. We demonstrate the use of learned models of organizational principles in a kitchen environment on a real robot system, where the robot identifies a newly acquired item, determines a suitable location and then stores the item accordingly.",https://ieeexplore.ieee.org/document/6224553/,2012 IEEE International Conference on Robotics and Automation,14-18 May 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IIAI-AAI.2014.174,Learning through Imitation and Reinforcement Learning: Toward the Acquisition of Painting Motions,IEEE,Conferences,"Learning is essential for an autonomous agent to adapt to an environment. One method of learning is through trial and error, however, this method is impractical in a complex environment because of the long learning time required by the agent. Therefore, guidelines are necessary in order to expedite the learning process in such environments, and imitation is one such guideline. Sakato, Ozeki, and Oka (2012-2013) recently proposed a computational model of imitation and autonomous behavior by which an agent can reduce its learning time through imitation. They evaluate the model in discrete and continuous spaces, and apply the model to a real robot in order to acquire painting skills. Their experimental results indicate that the model adapted to the experimental environment by imitation. In this paper, we introduce the model and discuss what are needed to improve the model.",https://ieeexplore.ieee.org/document/6913418/,2014 IIAI 3rd International Conference on Advanced Applied Informatics,31 Aug.-4 Sept. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2016.7487506,Learning time series models for pedestrian motion prediction,IEEE,Conferences,"Robot systems deployed in real-world environments often need to interact with other dynamic objects, such as pedestrians, cars, bicycles or other vehicles. In such cases, it is useful to have a good predictive model of the object's motion to factor in when optimizing the robot's own behaviour. In this paper we consider motion models cast in the Predictive Linear Gaussian (PLG) model, and propose two learning approaches for this framework: one based on the method of moments and the other on a least-squares criteria. We evaluate the approaches on several synthetic datasets, and deploy the system on a wheelchair robot, to improve its ability to follow a walking companion.",https://ieeexplore.ieee.org/document/7487506/,2016 IEEE International Conference on Robotics and Automation (ICRA),16-21 May 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594204,Learning to Fly by MySelf: A Self-Supervised CNN-Based Approach for Autonomous Navigation,IEEE,Conferences,"Nowadays, Unmanned Aerial Vehicles (UAVs)are becoming increasingly popular facilitated by their extensive availability. Autonomous navigation methods can act as an enabler for the safe deployment of drones on a wide range of real-world civilian applications. In this work, we introduce a self-supervised CNN-based approach for indoor robot navigation. Our method addresses the problem of real-time obstacle avoidance, by employing a regression CNN that predicts the agent's distance-to-collision in view of the raw visual input of its on-board monocular camera. The proposed CNN is trained on our custom indoor-flight dataset which is collected and annotated with real-distance labels, in a self-supervised manner using external sensors mounted on an UAV. By simultaneously processing the current and previous input frame, the proposed CNN extracts spatio-temporal features that encapsulate both static appearance and motion information to estimate the robot's distance to its closest obstacle towards multiple directions. These predictions are used to modulate the yaw and linear velocity of the UAV, in order to navigate autonomously and avoid collisions. Experimental evaluation demonstrates that the proposed approach learns a navigation policy that achieves high accuracy on real-world indoor flights, outperforming previously proposed methods from the literature.",https://ieeexplore.ieee.org/document/8594204/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811554,"Learning to Rock-and-Walk: Dynamic, Non-Prehensile, and Underactuated Object Locomotion Through Reinforcement Learning",IEEE,Conferences,"When moving objects that are too bulky or heavy to be grasped or lifted, robotic manipulation can benefit from the object&#x0027;s interaction with the support surface and its natural dynamics under gravity. In this work, we show that such dynamic, underactuated manipulation capability can be acquired through reinforcement learning and deployed on real robot systems. First, we present a framework to learn a control policy for object transport in a dynamic simulation environment, featuring the object and the support surface. We then demonstrate successful object locomotion with the learned policy through a set of simulated and real-world experiments, performed with a robot arm and an aerial robot interacting with the object in a non-prehensile manner. While the object, which is in contact with the support surface, oscillates sideways passively under gravity, the robot uses the learned policy to move the object forward with a steady gait by regulating the mechanical energy and the posture of the object. Our experiment results show that the learned policy can transport the object through unmodeled effects of terrain and perturbation.",https://ieeexplore.ieee.org/document/9811554/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812363,Learning-based Ellipse Detection for Robotic Grasps of Cylinders and Ellipsoids,IEEE,Conferences,"In our daily life, there are many objects represented by cylindrical shapes and ellipsoids. The tops of these objects are formed by elliptic shape primitives. Thus, it is available for a robot to manipulate these objects by ellipse detection. In this work, we propose a novel approach to generating ground truth for training the model based on domain randomization. Using synthetic data generated in this manner, we build an end-to-end deep neural network with a detection backbone and then, combine multiple branches archived from the backbone for sharing the multiple-scale features; further, after employing active rotation filters, the features pass through the region proposal net to form the prediction branches of the box, orientation regression, and object classification; finally, these branches are fused to do ellipse detection, allowing robotic manipulations of cylinders and ellipsoids. To demonstrate the capabilities of the proposed detector, we show the comparison results with the state-of-the-art detector on synthetic and public datasets. The proposed model for ellipse detection and data generation pipeline based on domain randomization in a simulation are evaluated by a series of robotic manipulations implemented in real application scenarios. The results illustrate a high success rate on real-world grasp attempts despite having only been trained on a synthetic dataset. (A video of some robotic experiments is available on YouTube: https://youtu.be/Ueg1XSI2S98).",https://ieeexplore.ieee.org/document/9812363/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2015.7169038,Live demonstration: Spiking neural circuit based navigation inspired by C. elegans thermotaxis,IEEE,Conferences,"We demonstrate a Spiking Neural Network (SNN) driven autonomous navigation system implemented on a robot. The neural architecture is inspired by those in nematode Caenorhabditis elegans used for thermotaxis, the behavior of tracking thermal isotherms. Our network uses light intensity as the sensor input, instead of temperature in the worm. The network is able to detect the gradations in sensor-input based on local information, and to make decisions in real time. This enables the robot to do a random search and to track specific intensity regions.",https://ieeexplore.ieee.org/document/7169038/,2015 IEEE International Symposium on Circuits and Systems (ISCAS),24-27 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968004,Long Range Neural Navigation Policies for the Real World,IEEE,Conferences,"Learned Neural Network based policies have shown promising results for robot navigation. However, most of these approaches fall short of being used on a real robot due to the extensive simulated training they require. These simulations lack the visuals and dynamics of the real world, which makes it infeasible to deploy on a real robot. We present a novel Neural Net based policy, NavNet, which allows for easy deployment on a real robot. It consists of two sub policies - a high level policy which can understand real images and perform long range planning expressed in high level commands; a low level policy that can translate the long range plan into low level commands on a specific platform in a safe and robust manner. For every new deployment, the high level policy is trained on an easily obtainable scan of the environment modeling its visuals and layout. We detail the design of such an environment and how one can use it for training a final navigation policy. Further, we demonstrate a learned low-level policy. We deploy the model in a large office building and test it extensively, achieving 0.80 success rate over long navigation runs and outperforming SLAM-based models in the same settings.",https://ieeexplore.ieee.org/document/8968004/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SOCC.2016.7905478,Low-power real-time intelligent SoCs for smart machines,IEEE,Conferences,"In this paper, we introduce low-power and real-time intelligent SoCs aimed at smart machines. To implement intelligent functions under low-power consumption, machine learning methods are tightly integrated with the traditional algorithms. At first, an object recognition processor (ORP) accelerating scale-invariant feature transform (SIFT) is presented with a visual attention based on convolutional neural network (CNN). For user interface (UI), a speech and gesture recognition processor (SGRP) based on convolutional deep belief network (CDBN) is presented with a voice activity detection (VAD) and a hand segmentation. At last, an artificial intelligence processor (AIP) for autonomous navigation is presented using A* tree search for path planning and reinforcement learning (RL) for dynamic obstacle avoidance. As a result, a prototype robot system integrating the presented SoCs is implemented and successfully demonstrated in the indoor environment.",https://ieeexplore.ieee.org/document/7905478/,2016 29th IEEE International System-on-Chip Conference (SOCC),6-9 Sept. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPRW.2019.00020,M2U-Net: Effective and Efficient Retinal Vessel Segmentation for Real-World Applications,IEEE,Conferences,"In this paper, we present a novel neural network architecture for retinal vessel segmentation that improves over the state of the art on two benchmark datasets, is the first to run in real time on high resolution images, and its small memory and processing requirements make it deployable in mobile and embedded systems. The M2U-Net has a new encoder-decoder architecture that is inspired by the U-Net. It adds pretrained components of MobileNetV2 in the encoder part and novel contractive bottleneck blocks in the decoder part that, combined with bilinear upsampling, drastically reduce the parameter count to 0.55M compared to 31.03M in the original U-Net. We have evaluated its performance against a wide body of previously published results on three public datasets. On two of them, the M2U-Net achieves new state-of-the-art performance by a considerable margin. When implemented on a GPU, our method is the first to achieve real-time inference speeds on high-resolution fundus images. We also implemented our proposed network on an ARM-based embedded system where it segments images in between 0.6 and 15 sec, depending on the resolution. Thus, the M2U-Net enables a number of applications of retinal vessel structure extraction, such as early diagnosis of eye diseases, retinal biometric authentication systems, and robot assisted microsurgery.",https://ieeexplore.ieee.org/document/9025339/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),16-17 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561878,MFPN-6D : Real-time One-stage Pose Estimation of Objects on RGB Images,IEEE,Conferences,"6D pose estimation of objects is an important part of robot grasping. The latest research trend on 6D pose estimation is to train a deep neural network to directly predict the 2D projection position of the 3D key points from the image, establish the corresponding relationship, and finally use Pespective-n-Point (PnP) algorithm performs pose estimation. The current challenge of pose estimation is that when the object texture-less, occluded and scene clutter, the detection accuracy will be reduced, and most of the existing algorithm models are large and cannot take the real-time requirements. In this paper, we introduce a Multi-directional Feature Pyramid Network, MFPN, which can efficiently integrate and utilize features. We combined the Cross Stage Partial Network (CSPNet) with MFPN to design a new network for 6D pose estimation, MFPN-6D. At the same time, we propose a new confidence calculation method for object pose estimation, which can fully consider spatial information and plane information. At last, we tested our method on the LINEMOD and Occluded-LINEMOD datasets. The experimental results demonstrate that our algorithm is robust to textureless materials and occlusion, while running more efficiently compared to other methods.",https://ieeexplore.ieee.org/document/9561878/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ComPE49325.2020.9200129,Machine Learning Algorithm based Disease Detection in Tomato with Automated Image Telemetry for Vertical Farming,IEEE,Conferences,"This paper is highlighting an outline of disease detection in tomato using computer vision and machine learning algorithms. Readily available hardware is used to build a system where a camera mounted system can detect and identify spot disease in tomatoes in real-time. As an initial prototype only spot disease can be detected. The complete development can be divided into two parts. The first part is the software and algorithm which aimed to detect and identify disease in crops and generate a report for the user. It is successful in building the algorithm and GUI (graphical user interface) for the user which can detect spot disease in tomatoes. Using the Viola-Jones algorithm and Haar like feature extraction method for the machine learning process in MATLAB, an XML (an image trained file) file for spot disease in tomatoes is designed using 377 images of infected tomatoes. The second part is the hardware implementation which consists of a simple robot rig that carries the camera and the system scans the tomatoes for the disease. For the vast majority of the time, spot detection is accurate. Many other diseases which exist for the animal, human and crops can easily be added to the system. In terms of reliability, the system is a success with acceptable false positives.",https://ieeexplore.ieee.org/document/9200129/,2020 International Conference on Computational Performance Evaluation (ComPE),2-4 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIKE.2018.00051,Management of Subdivided Dynamic Indoor Environments by Autonomous Scanning System,IEEE,Conferences,"With the development of sensing technologies, various spatial applications have been expanding into indoor spaces. For smooth spatial services, grasping indoor space information is most essential task. However, the indoor spaces is not only becoming increasingly complex, but also frequently changed than outdoor spaces. This makes it hard to provide an accurate location based service in an indoor space. This paper propose a way of managing a dynamic indoor environment by defining a multi-layered indoor model in terms of an object mobility. It allows an indoor space to be managed more elaborate and realistic than up-to-date indoor models which only consider an indoor floor plan. We firstly define a classification of indoor objects based on their characteristic to frequently change location, and propose three-layers indoor model followed by the classified objects with its mobility. Secondly, we design and implement an autonomous scanning system to understand changes of indoor situation quickly and automatically. The system is made up of a combination of IoT devices, including a programmable robot, lidar scanner and single-board computer. Finally, we demonstrate an implementation of the system with constructing the proposed model from a real indoor environment.",https://ieeexplore.ieee.org/document/8527483/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA51294.2020.00021,MaskedFusion: Mask-based 6D Object Pose Estimation,IEEE,Conferences,"MaskedFusion is a framework to estimate the 6D pose of objects using RGB-D data, with an architecture that leverages multiple sub-tasks in a pipeline to achieve accurate 6D poses. 6D pose estimation is an open challenge due to complex world objects and many possible problems when capturing data from the real world, e.g., occlusions, truncations, and noise in the data. Achieving accurate 6D poses will improve results in other open problems like robot grasping or positioning objects in augmented reality. MaskedFusion improves the state-of-the-art by using object masks to eliminate non-relevant data. With the inclusion of the masks on the neural network that estimates the 6D pose of an object we also have features that represent the object shape. MaskedFusion is a modular pipeline where each sub-task can have different methods that achieve the objective. MaskedFusion achieved 97.3% on average using the ADD metric on the LineMOD dataset and 93.3% using the ADD-S AUC metric on YCB-Video Dataset, which is an improvement, compared to the state-of-the-art methods.",https://ieeexplore.ieee.org/document/9356139/,2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA),14-17 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196540,Meta Reinforcement Learning for Sim-to-real Domain Adaptation,IEEE,Conferences,"Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.",https://ieeexplore.ieee.org/document/9196540/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2010.71,Microcontroller Based Neural Network Controlled Low Cost Autonomous Vehicle,IEEE,Conferences,"In this paper, design of a low cost autonomous vehicle based on neural network for navigation in unknown environments is presented. The vehicle is equipped with four ultrasonic sensors for hurdle distance measurement, a wheel encoder for measuring distance traveled, a compass for heading information, a GPS receiver for goal position information, a GSM modem for changing destination place on run time and a nonvolatile RAM for storing waypoint data; all interfaced to a low cost AT89C52 microcontroller. The microcontroller processes the information acquired from the sensors and generates robot motion commands accordingly through neural network. The neural network running inside the microcontroller is a multilayer feed-forward network with back-propagation training algorithm. The network is trained offline with tangent-sigmoid as activation function for neurons and is implemented in real time with piecewise linear approximation of tangent-sigmoid function. Results have shown that upto twenty neurons can be implemented in hidden layer with this technique. The vehicle is tested with varying destination places in outdoor environments containing stationary as well as moving obstacles and is found to reach the set targets successfully.",https://ieeexplore.ieee.org/document/5460762/,2010 Second International Conference on Machine Learning and Computing,9-11 Feb. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00017,Mirrorlabs - creating accessible Digital Twins of robotic production environment with Mixed Reality,IEEE,Conferences,How to visualize recorded production data in Virtual Reality? How to use state of the art Augmented Reality displays that can show robot data? This paper introduces an opensource ICT framework approach for combining Unity-based Mixed Reality applications with robotic production equipment using ROS Industrial. This publication gives details on the implementation and demonstrates the use as a data analysis tool in the context of scientific exchange within the area of Mixed Reality enabled human-robot co-production.,https://ieeexplore.ieee.org/document/9319071/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWRSP.1999.779052,Model based multi-level prototyping,IEEE,Conferences,"In this paper, we present our approach to rapid prototyping of robot software. We propose model based multi-level prototyping using UML in combination with a refinement design flow to synchronize development of an early virtual prototype, detailed simulation models and the final real prototype. This is achieved by a core model which is the common reference for model based multi-level prototyping. We demonstrate our methodology at hand of the design of a motor control software for the RoboCup robot platform of GMD. We show that parameters obtained with the virtual prototype and tested in the simulation model are well suited estimations for the final real prototype and therefore allow to reduce time-consuming experiments with the real prototype to a minimum.",https://ieeexplore.ieee.org/document/779052/,Proceedings Tenth IEEE International Workshop on Rapid System Prototyping. Shortening the Path from Specification to Prototype (Cat. No.PR00246),16-18 June 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1991.131806,"Model based, sensor directed remediation of underground storage tanks",IEEE,Conferences,"Experimental investigations into the application of intelligent robot control technology to the problem of removing waste stored in tanks are described. The authors discuss the experimental environment employed, with particular attention to the computing and software control environment. Intelligent system control is achieved through the integration of extensive geometric and kinematic world models with real-time sensor-based control. All operator interactions with the system are through fully animated, graphical representations which validate all operator commands before execution to provide for safe operation. Sensing is used to add information to the robot system's world model and to allow sensor-based servo control during selected operations. Initial test results are reported, and the potential for applying advanced intelligent control concepts to the removal of waste in storage tanks is discussed.<>",https://ieeexplore.ieee.org/document/131806/,Proceedings. 1991 IEEE International Conference on Robotics and Automation,9-11 April 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1992.220046,Model-driven pose correction,IEEE,Conferences,"Pose determination for robot navigation is discussed. The problem is to maintain the system's instantaneous precept of its position and orientation in space for performing various tasks. The authors describe a system in which models were used to guide the sensory interpretation and to correct expectations. In this system, simulated images were used to analyze the real images and to correct the pose parameters. The reported techniques have been implemented and experiments with real images in a real environment have been performed.<>",https://ieeexplore.ieee.org/document/220046/,Proceedings 1992 IEEE International Conference on Robotics and Automation,12-14 May 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLSP.2015.7324313,Modelling LGMD2 visual neuron system,IEEE,Conferences,"Two Lobula Giant Movement Detectors (LGMDs) have been identified in the lobula region of the locust visual system: LGMD1 and LGMD2. LGMD1 had been successfully used in robot navigation to avoid impending collision. LGMD2 also responds to looming stimuli in depth, and shares most the same properties with LGMD1; however, LGMD2 has its specific collision selective responds when dealing with different visual stimulus. Therefore, in this paper, we propose a novel way to model LGMD2, in order to emulate its predicted bio-functions, moreover, to solve some defects of previous LGMD1 computational models. The mechanism of ON and OFF cells, as well as bio-inspired nonlinear functions, are introduced in our model, to achieve LGMD2's collision selectivity. Our model has been tested by a miniature mobile robot in real time. The results suggested this model has an ideal performance in both software and hardware for collision recognition.",https://ieeexplore.ieee.org/document/7324313/,2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP),17-20 Sept. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iCMLDE.2018.00023,Monocular SLAM and Obstacle Removal for Indoor Navigation,IEEE,Conferences,"Visual Simultaneous Localization and Mapping (SLAM) is one of the hot topics in computer vision. For the past few years, the AI and deep learning technology research have been widespread used in self-driving technology and surveillance system etc., gaining more and more attention from researchers and public media. The combination of AI technology and robot perception is inevitably going to be a trend. This paper aims at removing the obstacle to enhance the SLAM system performance that based on popular open source framework ORB-SLAM2 in dynamic environment. Moving objects will bring noise in camera pose estimation, besides, when in re-localization, the robot returns to the previous place finding the previous landmark mismatches because of its movement. The system will be confused and misdirected. A novel approach is proposed to remove the obstacle in real environment by using convolutional neural network (CNN) to generate a segmentation mask of obstacle object so as to eliminate the interference by moving object. Our experiment result shows an impressive outcome of practical use and benchmark dataset test.",https://ieeexplore.ieee.org/document/8614006/,2018 International Conference on Machine Learning and Data Engineering (iCMLDE),3-7 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635912,Monocular Teach-and-Repeat Navigation using a Deep Steering Network with Scale Estimation,IEEE,Conferences,"This paper proposes a novel monocular teach-and-repeat navigation system with the capability of scale awareness, i.e. the absolute distance between observation and goal images. It decomposes the navigation task into a sequence of visual servoing sub-tasks to approach consecutive goal/node images in a topological map. To be specific, a novel hybrid model, named deep steering network is proposed to infer the navigation primitives according to the learned local feature and scale for each visual servoing sub-task. A novel architecture, Scale-Transformer, is developed to estimate the absolute scale between the observation and goal image pair from a set of matched deep representations to assist repeating navigation. The experiments demonstrate that our scale-aware teach-and-repeat method achieves satisfying navigation accuracy, and converges faster than the monocular methods without scale correction given an inaccurate initial pose. The proposed network is integrated into an onboard system deployed on a real robot to achieve real-time navigation in a real environment. A demonstration video can be found online: https://youtu.be/ctlwDaMKnHw",https://ieeexplore.ieee.org/document/9635912/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8462967,Auctioning over Probabilistic Options for Temporal Logic-Based Multi-Robot Cooperation Under Uncertainty,IEEE,Conferences,"Coordinating a team of robots to fulfill a common task is still a demanding problem. This is even more the case when considering uncertainty in the environment, as well as temporal dependencies within the task specification. A multi-robot cooperation from a single goal specification requires mechanisms for decomposing the goal as well as an efficient planning for the team. However, planning action sequences offline is insufficient in real world applications. Rather, due to uncertainties, the robots also need to closely coordinate during execution and adjust their policies when additional observations are made. The framework presented in this paper enables the robot team to cooperatively fulfill tasks given as temporal logic specifications while explicitly considering uncertainty and incorporating observations during execution. We present the effectiveness of our ROS implementation of this approach in a case study scenario.",https://ieeexplore.ieee.org/document/8462967/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIU.2015.7130068,Audio-visual human tracking for active robot perception,IEEE,Conferences,"In this paper, a multimodal system is designed in the form of an active audio-vision in order to improve the perceptual capability of a robot in a noisy environment. The system running in real-time consists of 1) audition modality, 2) a complementary vision modality and 3) motion modality incorporating intelligent behaviors based on the data obtained from both modalities. The tasks of audition and vision are to detect, localize and track a speaker independently. The aim of motion modality is to enable a robot to have intelligent and human-like behaviors by using localization results from the sensor fusion. The system is implemented on a mobile robot platform in a real-time environment and the speaker tracking performance of the fusion is confirmed to be improved compared to each of sensory modalities.",https://ieeexplore.ieee.org/document/7130068/,2015 23nd Signal Processing and Communications Applications Conference (SIU),16-19 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO52101.2021.9597142,Automated Robot Control for a Game of Chess in Unity Game Engine through Artificial Intelligence,IEEE,Conferences,"The topic of this paper is to study the possibility of using Unity game development engine for robot control. The aim of the work is to create a virtual environment in which the game of chess is simulated, through a duel of two robots controlled by artificial intelligence. As part of the work, real robot models were implemented in the Unity game engine. The simulated robots were ABB's IRB-120 arms with two joints. The movement of the robot is fully simulated within the physics simulation in the Unity system. The Forward and Backward Reaching Inverse Kinematics (FABRIK) algorithm was used for the inverse kinematics algorithm. For calculating the next move, external artificial intelligence library Stockfish was used and integrated with the Unity game engine. The final application has automated moves between the robots, has the option of a simple change of the viewpoint through camera movement, and is intended to be used in future work for the control of a real robot.",https://ieeexplore.ieee.org/document/9597142/,"2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO)",27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2003.1223997,Automatic language acquisition by an autonomous robot,IEEE,Conferences,"There is no such thing as a disembodied mind. We posit that cognitive development can only occur through interaction with the physical world. To this end, we are developing a robotic platform for the purpose of studying cognition. We suggest that the central component of cognition is a memory which is primarily associative, one where learning occurs as the correlation of events from diverse inputs. We also posit that human-like cognition requires a well-integrated sensory-motor system, to provide these diverse inputs. As implemented in our robot, this system includes binaural hearing, stereo vision, tactile sense, and basic proprioceptive control. On top of these abilities, we are implementing and studying various models of processing, learning and decision making. Our goal is to produce a robot that will learn to carry out simple tasks in response to natural language requests. The robot's understanding of language will be learned concurrently with its other cognitive abilities. We have already developed a robust system and conducted a number or experiments on the way to this goal, some details of which appear in this paper. This is a first progress report of what we believe will be a long term project with significant implications.",https://ieeexplore.ieee.org/document/1223997/,"Proceedings of the International Joint Conference on Neural Networks, 2003.",20-24 July 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2018.8624922,Autonomous Dual-Arm Manipulation of Familiar Objects,IEEE,Conferences,"Autonomous dual-arm manipulation is an essential skill to deploy robots in unstructured scenarios. However, this is a challenging undertaking, particularly in terms of perception and planning. Unstructured scenarios are full of objects with different shapes and appearances that have to be grasped in a very specific manner so they can be functionally used. In this paper we present an integrated approach to perform dual-arm pick tasks autonomously. Our method consists of semantic segmentation, object pose estimation, deformable model registration, grasp planning and arm trajectory optimization. The entire pipeline can be executed onboard and is suitable for on-line grasping scenarios. For this, our approach makes use of accumulated knowledge expressed as convolutional neural network models and low-dimensional latent shape spaces. For manipulating objects, we propose a stochastic trajectory optimization that includes a kinematic chain closure constraint. Evaluation in simulation and on the real robot corroborates the feasibility and applicability of the proposed methods on a task of picking up unknown watering cans and drills using both arms.",https://ieeexplore.ieee.org/document/8624922/,2018 IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids),6-9 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341657,Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs,IEEE,Conferences,"We consider an autonomous exploration problem in which a range-sensing mobile robot is tasked with accurately mapping the landmarks in an a priori unknown environment efficiently in real-time; it must choose sensing actions that both curb localization uncertainty and achieve information gain. For this problem, belief space planning methods that forward- simulate robot sensing and estimation may often fail in real-time implementation, scaling poorly with increasing size of the state, belief and action spaces. We propose a novel approach that uses graph neural networks (GNNs) in conjunction with deep reinforcement learning (DRL), enabling decision-making over graphs containing exploration information to predict a robot's optimal sensing action in belief space. The policy, which is trained in different random environments without human intervention, offers a real-time, scalable decision-making process whose high-performance exploratory sensing actions yield accurate maps and high rates of information gain.",https://ieeexplore.ieee.org/document/9341657/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.2006.322654,Autonomous Robots as a Generic Teaching Tool,IEEE,Conferences,"An undergraduate bioengineering laboratory course using small autonomous robots has been developed to demonstrate control theory, learning, and behavior. The lab consists of several modules that demonstrate concepts in classical control theory, fuzzy logic, neural network control, and genetic algorithms. The autonomous agents are easy-to-build, inexpensive kit robots. Each robot functions independently in a real-world environment. Students program and retrieve data wirelessly using handheld computers. The hands-on nature of the lab modules engages students in ways that lectures, readings and software simulations cannot. By interacting with these robots, students directly experience the effects of unexpected environmental factors on designs and deviations from software simulations. The robots are easily adapted for use in many different aspects of two-year college and K-12 STEM education. Students are motivated to understand engineering, math and science principles in order to control the robots. Examples of use of the robots and modules by a local community college are presented",https://ieeexplore.ieee.org/document/4117154/,Proceedings. Frontiers in Education. 36th Annual Conference,27-31 Oct. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO47225.2020.9172808,Autonomous UAV Navigation for Active Perception of Targets in Uncertain and Cluttered Environments,IEEE,Conferences,"The use of Small Unmanned Aerial Vehicles (sUAVs) has grown exponentially owing to an increasing number of autonomous capabilities. Automated functions include the return to home at critical energy levels, collision avoidance, take-off and landing, and target tracking. However, sUAVs applications in real-world and time-critical scenarios, such as Search and Rescue (SAR) is still limited. In SAR applications, the overarching aim of autonomous sUAV navigation is the quick localisation, identification and quantification of victims to prioritise emergency response in affected zones. Traditionally, sUAV pilots are exposed to prolonged use of visual systems to interact with the environment, which causes fatigue and sensory overloads. Nevertheless, the search for victims onboard a sUAV is challenging because of noise in the data, low image resolution, illumination conditions, and partial (or full) occlusion between the victims and surrounding structures. This paper presents an autonomous Sequential Decision Process (SDP) for sUAV navigation that incorporates target detection uncertainty from vision-based cameras. The SDP is modelled as a Partially Observable Markov Decision Process (POMDP) and solved online using the Adaptive Belief Tree (ABT) algorithm. In particular, a detailed model of target detection uncertainty from deep learning-based models is shown. The presented formulation is tested under Software in the Loop (SITL) through Gazebo, Robot Operating System (ROS), and PX4 firmware. A Hardware in the Loop (HITL) implementation is also presented using an Intel Myriad Vision Processing Unit (VPU) device and ROS. Tests are conducted in a simulated SAR GPS-denied scenario, aimed to find a person at different levels of location and pose uncertainty.",https://ieeexplore.ieee.org/document/9172808/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIMPAR.2016.7862403,Autonomous exploration by expected information gain from probabilistic occupancy grid mapping,IEEE,Conferences,"Occupancy grid maps are spatial representations of environments, where the space of interest is decomposed into a number of cells that are considered either occupied or free. This paper focuses on exploring occupancy grid maps by predicting the uncertainty of the map. Based on recent improvements in computing occupancy probability, this paper presents a novel approach for selecting robot poses designed to maximize expected map information gain represented by the change in entropy. This result is simplified with several approximations to develop an algorithm suitable for real-time implementation. The predicted information gain proposed in this paper governs an effective autonomous exploration strategy when applied in conjunction with an existing motion planner to avoid obstacles, which is illustrated by numerical examples.",https://ieeexplore.ieee.org/document/7862403/,"2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)",13-16 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2011.5980435,Autonomous learning of vision-based layered object models on mobile robots,IEEE,Conferences,"Although mobile robots are increasingly being used in real-world applications, the ability to robustly sense and interact with the environment is still missing. A key requirement for the widespread deployment of mobile robots is the ability to operate autonomously by learning desired environmental models and revising the learned models in response to environmental changes. This paper presents an approach that enables a mobile robot to autonomously learn layered models for environmental objects using temporal, local and global visual cues. A temporal assessment of image gradient features is used to detect candidate objects, which are then modeled using color distribution statistics and a spatial representation of gradient features. The robot incrementally revises the learned models and uses them for object recognition and tracking based on a matching scheme comprising a spatial similarity measure and second order distribution statistics. All algorithms are implemented and tested on a wheeled robot platform in dynamic indoor environments.",https://ieeexplore.ieee.org/document/5980435/,2011 IEEE International Conference on Robotics and Automation,9-13 May 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYSOSE.2008.4724191,Autonomous navigation based on a Q-learning algorithm for a robot in a real environment,IEEE,Conferences,"This paper explores autonomous navigation and obstacle avoidance techniques based on Q-learning for a mobile robot in a real environment. The implemented algorithm focuses on simplicity and efficiency. The learning process takes place in both simulation and real world allowing the combination of a longer learning time in the simulator with a more accurate knowledge from the real world. After learning is completed in simulation and in the real world, the robot was able to navigate without hitting obstacles and able to generate control law for complex situations such as corners and small objects.",https://ieeexplore.ieee.org/document/4724191/,2008 IEEE International Conference on System of Systems Engineering,2-4 June 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CARPI.2010.5624410,Autonomous navigation for underground energy line inspection robot,IEEE,Conferences,"This work proposes architecture of an inspection robot's navigation system, aiming at monitoring underground energy lines. This architecture is composed of two modules: i. feature extraction from environment; ii navigation approach. The feature extraction module is based on the use of the edge detector by Canny algorithm and Hough transform for identification of lines from images of environment to monitoring. The lines identified correspond to cable conformation inside the duct. This information will serve to help the navigation system. For the implementation of the navigation system two approaches were proposed: navigation based on artificial neural network and navigation based on PID control. The navigation architecture can be used in real or simulated scenarios, and it was tested in a simulated environment.",https://ieeexplore.ieee.org/document/5624410/,2010 1st International Conference on Applied Robotics for the Power Industry,5-7 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2013.6706877,Autonomous reinforcement of behavioral sequences in neural dynamics,IEEE,Conferences,"We introduce a dynamic neural algorithm called Dynamic Neural (DN) SARSA(λ) for learning a behavioral sequence from delayed reward. DN-SARSA(λ) combines Dynamic Field Theory models of behavioral sequence representation, classical reinforcement learning, and a computational neuroscience model of working memory, called Item and Order working memory, which serves as an eligibility trace. DN-SARSA(λ) is implemented on both a simulated and real robot that must learn a specific rewarding sequence of elementary behaviors from exploration. Results show DN-SARSA(λ) performs on the level of the discrete SARSA(λ), validating the feasibility of general reinforcement learning without compromising neural dynamics.",https://ieeexplore.ieee.org/document/6706877/,The 2013 International Joint Conference on Neural Networks (IJCNN),4-9 Aug. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2002.1157759,Autonomous robot navigation based on fuzzy sensor fusion and reinforcement learning,IEEE,Conferences,"This paper presents the design and implementation of an autonomous robot navigation system for intelligent target collection in dynamic environments. A feature-based multi-stage fuzzy logic (MSFL) sensor fusion system is developed for target recognition, which is capable of mapping noisy sensor inputs into reliable decisions. The robot exploration and path planning are based on a grid map oriented reinforcement path learning system (GMRPL), which allows for long-term predictions and path adaptation via dynamic interactions with physical environments. In our implementation, the MSFL and GMRPL are integrated into a subsumption architecture for intelligent target-collecting applications. The subsumption architecture is a layered reactive agent structure that enables the robot to implement higher-layer functions including path learning and target recognition regardless of lower-layer functions such as obstacle detection and avoidance. Real-world application using a Khepera robot shows the robustness and flexibility of the developed system in dealing with robotic behavior such as target collecting in an ever-changing physical environment.",https://ieeexplore.ieee.org/document/1157759/,Proceedings of the IEEE Internatinal Symposium on Intelligent Control,30-30 Oct. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IST.2012.6295593,Autonomous robotic ground penetrating radar surveys of ice sheets; Using machine learning to identify hidden crevasses,IEEE,Conferences,"This paper presents methods to continue development of a completely autonomous robotic system employing ground penetrating radar imaging of the glacier sub-surface. We use well established machine learning algorithms and appropriate un-biased processing, particularly those which are also suitable for real-time image analysis and detection. We tested and evaluated three processing schemes in conjunction with a Support Vector Machine (SVM) trained on 15 examples of Antarctic GPR imagery, collected by our robot and a Pisten Bully tractor in 2010 in the shear zone near McMurdo Station. Using a modified cross validation technique, we correctly classified all examples with a radial basis kernel SVM trained and evaluated on down-sampled and texture-mapped GPR images of crevasses, compared to 60% classification rate using raw data. We also test the most successful processing scheme on a larger dataset, comprised of 94 GPR images of crevasse crossings recorded in the same deployment. Our experiments demonstrate the promise and reliability of real-time object detection and classification with robotic GPR imaging surveys.",https://ieeexplore.ieee.org/document/6295593/,2012 IEEE International Conference on Imaging Systems and Techniques Proceedings,16-17 July 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6631235,Autonomous robotic valve turning: A hierarchical learning approach,IEEE,Conferences,"Autonomous valve turning is an extremely challenging task for an Autonomous Underwater Vehicle (AUV). To resolve this challenge, this paper proposes a set of different computational techniques integrated in a three-layer hierarchical scheme. Each layer realizes specific subtasks to improve the persistent autonomy of the system. In the first layer, the robot acquires the motor skills of approaching and grasping the valve by kinesthetic teaching. A Reactive Fuzzy Decision Maker (RFDM) is devised in the second layer which reacts to the relative movement between the valve and the AUV, and alters the robot's movement accordingly. Apprenticeship learning method, implemented in the third layer, performs tuning of the RFDM based on expert knowledge. Although the long-term goal is to perform the valve turning task on a real AUV, as a first step the proposed approach is tested in a laboratory environment.",https://ieeexplore.ieee.org/document/6631235/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.2008.4649235,BMI cyberworkstation: Enabling dynamic data-driven brain-machine interface research through cyberinfrastructure,IEEE,Conferences,"Dynamic data-driven brain-machine interfaces (DDDBMI) have great potential to advance the understanding of neural systems and improve the design of brain-inspired rehabilitative systems. This paper presents a novel cyberinfrastructure that couples in vivo neurophysiology experimentation with massive computational resources to provide seamless and efficient support of DDDBMI research. Closed-loop experiments can be conducted with in vivo data acquisition, reliable network transfer, parallel model computation, and real-time robot control. Behavioral experiments with live animals are supported with real-time guarantees. Offline studies can be performed with various configurations for extensive analysis and training. A Web-based portal is also provided to allow users to conveniently interact with the cyberinfrastructure, conducting both experimentation and analysis. New motor control models are developed based on this approach, which include recursive least square based (RLS) and reinforcement learning based (RLBMI) algorithms. The results from an online RLBMI experiment shows that the cyberinfrastructure can successfully support DDDBMI experiments and meet the desired real-time requirements.",https://ieeexplore.ieee.org/document/4649235/,2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20-25 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2012.6359467,BMP: A self-balancing mobile platform,IEEE,Conferences,"The development of two wheels self-balancing robot has gained more and more attention over the past years. It is suitable for working in outdoor environment especially where the ground is not flat. This paper describes our works on building a self-Balancing Mobile Platform named BMP. We build the dynamic model to find the relation between the robot posture and output voltages applying on the motors. An accelerometer and gyroscope are used to estimate the posture through Kalman filter algorithm. Then a multi-segment PID controller changes the motor voltage according to the platform motion mode and posture. The experiments are carried out in simulation, then on the real platform. The results of the experiments show that BMP works effectively and robustly.",https://ieeexplore.ieee.org/document/6359467/,2012 International Conference on Machine Learning and Cybernetics,15-17 July 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IES53407.2021.9594013,Ball Position Transformation with Artificial Intelligence Based on Tensorflow Libraries,IEEE,Conferences,Research on wheeled soccer robots has been carried out by several researchers. This is due to the existence of national and international competitions. Previous research was to create a ball position transformation system with a modified method of neural network architecture. This research was developed by building an intelligent transformation system with the Tensorflow library. This transformation system aims to be able to directly measure the distance of objects in real terms without first changing the environmental image from an omni field to a flat plane with conventional camera calibration techniques. This process can replace manual calibration with a variety of field size changes The system can transform with mean error 0.0000026 on epoch 10000 using “conda-tensorflowneural network” libraries. It can transform the position of the ball from the omni space to the cartesian space. This system was implemented on wheeled soccer robot as keeper.,https://ieeexplore.ieee.org/document/9594013/,2021 International Electronics Symposium (IES),29-30 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMOCO.2001.973435,"Behavior learning to predict using neural networks (NN): Ttowards a fast, cooperative and adversarial robot team (RoboCup)",IEEE,Conferences,"To build a fast, cooperative and adversarial robot team (RoboCup), prediction behaviors became necessary. In the paper, a behavior learning method using neural networks (NN) is developed to enhance the behavior of GMD mobile robots. In fact, the suggested NN called NN-Prediction learns to predict successfulness of the elementary behavior ""Kick"" the ball towards the goal in order to act as consequence. The training is carried out by the supervised gradient back-propagation learning paradigm. This NN-Prediction has been specified on the Dual Dynamics Designer, to be thereafter implemented and tested on both the Dual Dynamics Simulator and GMD mobile robots, and analyzed on the Real-Time Trace Tool. NN-prediction demonstrated, during the 4/sup th/ World Championships RoboCup 2000, cooperative and adversarial behaviors especially face to situations where the successfulness of ""Kick"" is not guaranteed. Then, a discussion is given dealing with the suggested prediction behavior and how it relates to some other works.",https://ieeexplore.ieee.org/document/973435/,Proceedings of the Second International Workshop on Robot Motion and Control. RoMoCo'01 (IEEE Cat. No.01EX535),20-20 Oct. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMWRTS.1996.557846,Behaviour-oriented commands: from distributed knowledge representation to real-time implementation,IEEE,Conferences,"This paper presents a general methodology to model and implement real time control of complex systems with high reactivity. It is based on an original concept called ""behaviour oriented commands"" (BOCs). This methodology has been applied successfully in our mobile robot. BOCs incorporate mechanisms to model the set of rules (knowledge) which describes the restrictions and actions to achieve a goal. Basic rules are well encapsulated by entities called ""behaviours"", while global co-operating rules are explicited by the association link managed by the BOC's control unit. The model is easily translated into a real time implementation. This fusion between knowledge and real time is the main contribution of our work to the RT area.",https://ieeexplore.ieee.org/document/557846/,Proceedings of the Eighth Euromicro Workshop on Real-Time Systems,12-14 June 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197470,"Benchmark for Skill Learning from Demonstration: Impact of User Experience, Task Complexity, and Start Configuration on Performance",IEEE,Conferences,"We contribute a study benchmarking the performance of multiple motion-based learning from demonstration approaches. Given the number and diversity of existing methods, it is critical that comprehensive empirical studies be performed comparing the relative strengths of these techniques. In particular, we evaluate four approaches based on properties an end user may desire for real-world tasks. To perform this evaluation, we collected data from nine participants, across four manipulation tasks. The resulting demonstrations were used to train 180 task models and evaluated on 720 task reproductions on a physical robot. Our results detail how i) complexity of the task, ii) the expertise of the human demonstrator, and iii) the starting configuration of the robot affect task performance. The collected dataset of demonstrations, robot executions, and evaluations are publicly available. Research insights and guidelines are also provided to guide future research and deployment choices about these approaches.",https://ieeexplore.ieee.org/document/9197470/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2018.8581174,Bi-Manual Articulated Robot Teleoperation using an External RGB-D Range Sensor,IEEE,Conferences,"In this paper, we present an implementation of a bi-manual teleoperation system, controlled by a human through three-dimensional (3D) skeleton extraction. The input data is given from a cheap RGB-D range sensor, such as the ASUS Xtion PRO. To achieve this, we have implemented a 3D version of the impressive OpenPose package, which was recently developed. The first stage of our method contains the execution of the OpenPose Convolutional Neural Network (CNN), using a sequence of RGB images as input. The extracted human skeleton pose localisation in two-dimensions (2D) is followed by the mapping of the extracted joint location estimations into their 3D pose in the camera frame. The output of this process is then used as input to drive the end-pose of the robotic hands relative to the human hand movements, through a whole-body inverse kinematics process in the Cartesian space. Finally, we implement the method as a ROS wrapper package and we test it on the centaur-like CENTAURO robot. Our demonstrated task is of a box and lever manipulation in real-time, as a result of a human task demonstration.",https://ieeexplore.ieee.org/document/8581174/,"2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)",18-21 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2008.4620720,Bi-criteria joint torque minimization of redundant robot arms using LVI-based primal-dual neural network,IEEE,Conferences,"To diminish the discontinuity and divergence of infinity norm torque minimization scheme, a bi-criteria weighting scheme is proposed for online redundancy resolution of redundant robot arms. Such a scheme can easily be reformulated into a quadratic program (QP) subject to equality, inequality and bound constraints. To solve this QP problem online, a primal-dual dynamical-system solver is further presented based on linear variational inequalities (LVI). Compared to previous research work, the adopted QP-solver has simple piecewise-linear dynamics, and does not entail real-time matrix inversion. Computer simulations are performed based on a PUMA560 manipulator to verify the performance and effectiveness of the proposed torque-optimization method.",https://ieeexplore.ieee.org/document/4620720/,2008 International Conference on Machine Learning and Cybernetics,12-15 July 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2008.4633875,Bio-inspired stochastic chance-constrained multi-robot task allocation using WSN,IEEE,Conferences,"The multi-robot task allocation (MRTA) especially in unknown complex environment is one of the fundamental problems, a mostly important object in research of multi-robot. The MRTA problem is initially formulated as a chance-constrained optimization problem. Monte Carlo simulation is used to verify the accuracy of the solution provided by the algorithm. Ant colony optimization (ACO) algorithm based on bionic swarm intelligence was used. A hybrid intelligent algorithm combined Monte Carlo simulation and neural network is used for solving stochastic chance constrained models of MRTA. A practical implementation with real WSN and real mobile robots were carried out. In environment the successful implementation of tasks without collision validates the efficiency, stability and accuracy of the proposed algorithm. The convergence curve shows that as iterative generation grows, the utility increases and finally reaches a stable and optimal value. Results show that using sensor information fusion can greatly improve the efficiency. The algorithm is proved better than tradition algorithms without WSN for MRTA in real time.",https://ieeexplore.ieee.org/document/4633875/,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),1-8 June 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BIOROB.2018.8487202,Bioinspired Adaptive Spiking Neural Network to Control NAO Robot in a Pavlovian Conditioning Task,IEEE,Conferences,"The cerebellum has a central role in fine motor control and in various neural processes, as in associative paradigms. In this work, a bioinspired adaptive model, developed by means of a spiking neural network made of thousands of artificial neurons, has been leveraged to control a humanoid NAO robot in real-time. The learning properties of the system have been challenged in a classic cerebellum-driven paradigm, the Pavlovian timing association between two provided stimuli, here implemented as a laser-avoidance task. The neurophysiological principles used to develop the model, succeeded in driving an adaptive motor control protocol with acquisition and extinction phases. The spiking neural network model showed learning behaviors similar to the ones experimentally measured with human subjects in the same conditioning task. The model processed in real-time external inputs, encoded as spikes, and the generated spiking activity of its output neurons was decoded, in order to trigger the proper response with a correct timing. Three long-term plasticity rules have been embedded for different connections and with different time-scales. The plasticities shaped the firing activity of the output layer neurons of the network. In the Pavlovian protocol, the neurorobot successfully learned the correct timing association, generating appropriate responses. Therefore, the spiking cerebellar model was able to reproduce in the robotic platform how biological systems acquire and extinguish associative responses, dealing with noise and uncertainties of a real-world environment.",https://ieeexplore.ieee.org/document/8487202/,2018 7th IEEE International Conference on Biomedical Robotics and Biomechatronics (Biorob),26-29 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2000.859467,Biologically inspired neural controllers for motor control in a quadruped robot,IEEE,Conferences,"This paper presents biologically inspired neural controllers for generating motor patterns in a quadruped robot. Sets of artificial neural networks are presented which provide 1) pattern generation and gait control, allowing continuous passage from walking to trotting to galloping, 2) control of sitting and lying down behaviors, and 3) control of scratching. The neural controllers consist of sets of oscillators composed of leaky-integrator neurons, which control pairs of flexor-extensor muscles attached to each joint. The networks receive sensory feedback proportional to the contraction of simulated muscles and to joint flexion. Similarly to what is observed in cats, locomotion can be initiated by either applying tonic (i.e. non-oscillating) input to the locomotion network or by sensory feedback from extending the legs. The networks are implemented in a quadruped robot. It is shown that computation can be carried out in real time and that the networks can generate the above mentioned motor behaviors.",https://ieeexplore.ieee.org/document/859467/,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,27-27 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INES.2018.8523877,Body State Recognition for a Quadruped Mobile Robot,IEEE,Conferences,"The body states must be tracked by the onboard software on the robot to make good decisions. A human can pick up this machine or if the robot encounters anomalies (e.g. fall over) during locomotion, the state changes must be identified to execute the necessary responses. The authors of this paper developed a machine learning model which can recognize four states (normal, pick-up, fall over, poked) of a Sony AIBO robot. A deep neural network classifier with these predictors achieved 98% accuracy on unseen data and actual test runs on the robot proved the practical use with real-time execution speed. These properties made the proposed method a good candidate for adaption to other legged robots.",https://ieeexplore.ieee.org/document/8523877/,2018 IEEE 22nd International Conference on Intelligent Engineering Systems (INES),21-23 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2006.281655,Brush Footprint Acquisition and Preliminary Analysis for Chinese Calligraphy using a Robot Drawing Platform,IEEE,Conferences,"A robot drawing platform supporting four degrees of freedom (x, y, z and z-rotation) of a brush-pen motion for studying Chinese painting and calligraphy has been operational in our laboratory. This paper describes the real-time capturing and data analysis of the brush footprint using the new hardware and software capabilities in the platform. They include a transparent drawing plate and an underneath camera system, together with projective rectification and video segmentation algorithms. Preliminary result of the footprint analysis and nonparametric modeling, and their applications to well-known Chinese calligraphy are demonstrated",https://ieeexplore.ieee.org/document/4059247/,2006 IEEE/RSJ International Conference on Intelligent Robots and Systems,9-15 Oct. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV52889.2021.00009,Building Facades to Normal Maps: Adversarial Learning from Single View Images,IEEE,Conferences,"Surface normal estimation is an essential component of several computer and robot vision pipelines. While this problem has been extensively studied, most approaches are geared towards indoor scenes and often rely on multiple modalities (depth, multiple views) for accurate estimation of normal maps. Outdoor scenes pose a greater challenge as they exhibit significant lighting variation, often contain occluders, and structures like building facades are often ridden with numerous windows and protrusions. Conventional supervised learning schemes excel in indoor scenes, but do not exhibit competitive performance when trained and deployed in outdoor environments. Furthermore, they involve complex network architectures and require many more trainable parameters. To tackle these challenges, we present an adversarial learning scheme that regularizes the output normal maps from a neural network to appear more realistic, by using a small number of precisely annotated examples. Our method presents a lightweight and simpler architecture, while improving performance by at least 1.5x across most metrics. We evaluate our approaches against the state-of-the-art on normal map estimation, on a synthetic and a real outdoor dataset, and observe significant performance enhancements.",https://ieeexplore.ieee.org/document/9469450/,2021 18th Conference on Robots and Vision (CRV),26-28 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2005.1555934,Building a cheaper artificial brain,IEEE,Conferences,"This paper presents a methodology for building artificial brains that is much cheaper than the first author's earlier attempt. What initially cost $500,000, now costs about $3000. The much cheaper approach uses a Celoxica(.com) programmable board containing a Xilinx Virtex II FPGA chip with 3 million programmable logic gates, to evolve neural networks at electronic speeds. The genetic algorithm (GA) and the neural network model are programmed using a high level language called Handel-C, whose code is (silicon) compiled into the chip. The elite circuit is downloaded from the board into the memory of a PC. This process occurs up to several 10,000 s of times, once for each neural net circuit module having a unique function. Special software in the PC is used to specify the connections between the modules, according to the designs of human BAs (brain architects). The PC is then used to execute the neural signaling of the artificial brain (A-brain) in real time, defined to be 25 Hz per neuron. At this speed, the PC can handle several 10,000 s of modules. We would use our A-brain to control the behaviors of a small, four wheeled radio controlled robot with a CCD camera and gripper. The robot's task is to detect and collect unexploded cluster bomblets and deposit them in some central place. The total price of the PC, Celoxica board, and robot is less than $3000, making it affordable to virtually any research group interested in building artificial brains.",https://ieeexplore.ieee.org/document/1555934/,"Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.",31 July-4 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635991,BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models,IEEE,Conferences,"Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method’s reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack",https://ieeexplore.ieee.org/document/9635991/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1990.126097,CSL: a cost-sensitive learning system for sensing and grasping objects,IEEE,Conferences,"The goal of the research reported is to build a learning robot which can survive in an unknown environment for a long time. Such a robot must learn which sensors to use, where to use them, and how to generate an inexpensive and reliable robot control procedure to accomplish its task. This is beyond machine learning methods because they usually ignore robot execution costs and are ill-prepared to handle failures. A cost-sensitive, noise-tolerant and inductive robot learning system, CSL, that represents the first steps toward achieving this goal is described, emphasizing the cost and noise issues in learning. CSL has been implemented in a real-world robot for sensing objects and selecting their grasping procedures.<>",https://ieeexplore.ieee.org/document/126097/,"Proceedings., IEEE International Conference on Robotics and Automation",13-18 May 1990,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-Berlin50680.2020.9352201,Camera-LIDAR Object Detection and Distance Estimation with Application in Collision Avoidance System,IEEE,Conferences,"Nowadays we are aware of accelerated development of automotive software. Numerous of ADAS (Advanced Driver Assistance Systems) systems are being developed these days. One such system is the forward CAS (Collision Avoidance System). In order to implement such a system, this paper presents one solution for detecting an object located directly in front of the vehicle and estimating its distance. The solution is based on the use of camera and LIDAR (Light Detection and Ranging) sensor fusion. The camera was used for object detection and classification, while 3D data obtained from LIDAR sensor were used for distance estimation. In order to map the 3D data from the LIDAR to the 2D image space, a spatial calibration was used. The solution was developed as a prototype using the ROS (Robot Operating System) based Autoware open source platform. This platform is essentially a framework intended for the development and testing of automotive software. ROS as the framework on which the Autoware platform is based, provides a library for the Python and C++ programming languages, intended for creating new applications. For the reason that this is a prototype project, and it is popular for application in machine learning, we decided to use the Python programming language. The solution was tested inside the CARLA simulator, where the estimation of the obstacle distance obtained at the output of our algorithm was compared with the ground truth values obtained from the simulator itself. Measurements were performed under different weather conditions, where this algorithm showed satisfactory results, with real-time processing.",https://ieeexplore.ieee.org/document/9352201/,2020 IEEE 10th International Conference on Consumer Electronics (ICCE-Berlin),9-11 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2014.7041490,Can active impedance protect robots from landing impact?,IEEE,Conferences,"This paper studies the effect of passive and active impedance for protecting jumping robots from landing impacts. The theory of force transmissibility is used for selecting the passive impedance of the system to minimize the shock propagation. The active impedance is regulated online by a joint-level controller. On top of this controller, a reflex-based leg retraction scheme is implemented which is optimized using direct policy search reinforcement learning based on particle filtering. Experiments are conducted both in simulation and on a real-world hopping leg. We show that although the impact dynamics is fast, the addition of passive impedance provides enough time for the active impedance controller to react to the impact and protect the robot from damage.",https://ieeexplore.ieee.org/document/7041490/,2014 IEEE-RAS International Conference on Humanoid Robots,18-20 Nov. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN52387.2021.9533738,CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous Cars on the Loihi Neuromorphic Research Processor,IEEE,Conferences,"Autonomous Driving (AD) related features provide new forms of mobility that are also beneficial for other kind of intelligent and autonomous systems like robots, smart transportation, and smart industries. For these applications, the decisions need to be made fast and in real-time. Moreover, in the quest for electric mobility, this task must follow low power policy, without affecting much the autonomy of the mean of transport or the robot. These two challenges can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed on a specialized neuromorphic hardware, SNNs can achieve high performance with low latency and low power consumption. In this paper, we use an SNN connected to an event-based camera for facing one of the key problems for AD, i.e., the classification between cars and other objects. To consume less power than traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS) [1]. The experiments are made following an offline supervised learning rule, followed by mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip [2]. Our best experiment achieves an accuracy on offline implementation of 86%, that drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware implementation has maximum 0.72 ms of latency for every sample, and consumes only 310 mW. To the best of our knowledge, this work is the first implementation of an event-based car classifier on a Neuromorphic Chip.",https://ieeexplore.ieee.org/document/9533738/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341134,Catch the Ball: Accurate High-Speed Motions for Mobile Manipulators via Inverse Dynamics Learning,IEEE,Conferences,"Mobile manipulators consist of a mobile platform equipped with one or more robot arms and are of interest for a wide array of challenging tasks because of their extended workspace and dexterity. Typically, mobile manipulators are deployed in slow-motion collaborative robot scenarios. In this paper, we consider scenarios where accurate high-speed motions are required. We introduce a framework for this regime of tasks including two main components: (i) a bi-level motion optimization algorithm for real-time trajectory generation, which relies on Sequential Quadratic Programming (SQP) and Quadratic Programming (QP), respectively; and (ii) a learning-based controller optimized for precise tracking of high-speed motions via a learned inverse dynamics model. We evaluate our framework with a mobile manipulator platform through numerous high-speed ball catching experiments, where we show a success rate of 85.33%. To the best of our knowledge, this success rate exceeds the reported performance of existing related systems [1], [2] and sets a new state of the art.",https://ieeexplore.ieee.org/document/9341134/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRC.2019.00021,Category Classification of Deformable Object using Hybrid Dynamic Model for Robotic Grasping,IEEE,Conferences,"This work studies the problem of classification of a hung garment in the unfolding procedure by a home service robot. The sheer number of unpredictable configurations that the deformable object can end up in makes the visual identification of the object shape and size difficult. In this paper, we propose a hybrid dynamic model to recognize the pose of hung garment using a single manipulator. A dataset of hung garment is generated by capturing the depth images of real garments at the robotic platform (real images) and also the images of garment mesh model from offline simulation (synthetic images) respectively. Deep convolutional neural network is implemented to classify the category and estimate the pose of garment. Experiment results show that the proposed method performs well and is applicable to different garments in robotic manipulation.",https://ieeexplore.ieee.org/document/9058831/,"2019 4th International Conference on Control, Robotics and Cybernetics (CRC)",27-30 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIFEr.2019.8759121,Chatbot Application on Cryptocurrency,IEEE,Conferences,"Many chatbots have been developed that provide a multitude of services through a wide range of methods. A chatbot is a brand-new conversational agent in the highspeed changing technology world. With the advance of Artificial Intelligence and machine learning, chatbots are becoming more and more popular. A chatbot is the extension of human interface mediums such as the phone and social platforms. Similarly, Cryptocurrency is a new extension of digital or virtual currency designed to work as a medium of exchange. In the current digital exchanging world, investors and interested parties are eager to know more information about, and the capabilites of, this new type of currency. One of the potential paths to retrieve the info automatically and quickly is through a chatbot. We explored the open source python library, Chatterbot, to apply Itchat API (a WeChat interface) with the aim of building a robot chatting application, I&#x0026;C Chat, on the topic of cryptocurrency. First, we collected question and answer pairs datasets from Quora websites. Furthermore, we also created API calls to query the real time quote for the top 25 cryptocurrencies. Then we used the collected data to train our chatbot and implemented a logic adapter to receive the price quote of cryptocurrencies based on the incoming question. The Itchat API method will return the best matched answer to the asking party automatically. The response time of different questions has been investigated. The results imply that this application is quite useful, feasible and beneficial to the digital currency world.",https://ieeexplore.ieee.org/document/8759121/,2019 IEEE Conference on Computational Intelligence for Financial Engineering & Economics (CIFEr),4-5 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561926,Circus ANYmal: A Quadruped Learning Dexterous Manipulation with Its Limbs,IEEE,Conferences,"Quadrupedal robots are skillful at locomotion tasks while lacking manipulation skills, not to mention dexterous manipulation abilities. Inspired by the animal behavior and the duality between multi-legged locomotion and multi-fingered manipulation, we showcase a circus ball challenge on a quadrupedal robot, ANYmal. We employ a model-free reinforcement learning approach to train a deep policy that enables the robot to balance and manipulate a light-weight ball robustly using its limbs without any contact measurement sensor. The policy is trained in the simulation, in which we randomize many physical properties with additive noise and inject random disturbance force during manipulation, and achieves zero-shot deployment on the real robot without any adjustment. In the hardware experiments, dynamic performance is achieved with a maximum rotation speed of 15 °/s, and robust recovery is showcased under external poking. To our best knowledge, it is the first work that demonstrates the dexterous dynamic manipulation on a real quadrupedal robot.",https://ieeexplore.ieee.org/document/9561926/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE46568.2020.9042997,Cliff-sensor-based Low-level Obstacle Detection for a Wheeled Robot in an Indoor Environment,IEEE,Conferences,"A ramp and uneven ground formed by a low-level obstacle – whose height is too low from the ground – often stalls a robot’s navigation in an indoor environment. Few-centimeter differences between a low-level obstacle and a low-level non-obstacle are very difficult to be precisely measured in a constant distance at a mobile robot. In this paper, a wheeled mobile robot thus makes physical contact onto a low-level object, in order to measure such subtle differences. We use one or more cliff sensors typically in place for a mobile robot in order to avoid a drop-off. A wheeled robot climbs over a low-level object, classifies an obstacle vs. a non-obstacle using a cliff sensor’s timeseries data, and rapidly backs up before getting stuck onto an obstacle. While adopting a simplified deep-learning architecture, we suggest a rapid and accurate obstacle detection technique in real-time. We implemented our technique on an embedded robot platform of LG Hom-Bot. The supplementary video on the physical robot experiment can be accessed at https://youtu.be/yK57S857_II.",https://ieeexplore.ieee.org/document/9042997/,2020 IEEE International Conference on Consumer Electronics (ICCE),4-6 Jan. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594311,Cognition-enabled Framework for Mixed Human-Robot Rescue Teams,IEEE,Conferences,"With the advancements in robotic technology and the progress in human-robot interaction research, the interest in deploying mixed human-robot teams in rescue missions is increasing. Due to their complementary capabilities in terms of locomotion, visibility and reachability of areas, human-robot teams are considerably deployed in real-world settings, albeit the robotic agents in such scenarios are normally fully teleoperated. A major barrier to successful and efficient mission execution in those teams is the lack of cognitive skills in robotic systems. In this paper, we present a cognition-enabled framework and an implemented system where robotic agents are equipped with cognitive capabilities to naturally communicate with humans and autonomously perform tasks. The framework allows for natural tasking of robots, reasoning about robot behavior, capabilities and actions, and a common belief state representation for shared mission awareness of robots and human operators.",https://ieeexplore.ieee.org/document/8594311/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEE.ICCC.2017.20,Cognitive Acoustic Analytics Service for Internet of Things,IEEE,Conferences,"The rapid development of the Internet of Things (IoT) has brought great changes for non-contact and non-destructive sensing and diagnosis. For every inanimate object can tell us something by the sound it makes, acoustic sensor demonstrates great advantages comparing to conventional electronic and mechanic sensors in such cases: overcoming environmental obstacles, mapping to existing use cases of detecting problems with human ears, low cost for deployment, etc. It could be widely applied to various domains, such as predictive maintenance of machinery, robot sensory, elderly and baby care in smart home, etc. Whether we can use the acoustic sensor data to understand what is happening and to predict what will happen relies heavily on the analytics capabilities we apply to the acoustic data, which has to overcome the obstacles of noise, disturbance and errors, and has to meet the requirement of real-time processing of high volume signals with large number of sensors. In this paper, we propose a scalable cognitive acoustics analytics service for IoT that provides the user an incremental learning approach to evolve their analytics capability on non-intuitive and unstructured acoustic data through the combination of acoustic signal processing and machine learning technology. It first performs acoustic signal processing and denoising, enables acoustic signal based abnormal detection based on sound intensity, spectral centroid, etc. Then based on the accumulated abnormal data, a supervised learning method is performed as baseline and a neural network based classifier is used to recognize acoustic events in different scenarios with various volume of sample data and requirement of accuracy. In addition, acoustic sensor arrays processing is supported for localization of moving acoustic source in more complex scenario. In this paper, we designed a hybrid computing structure. Finally, we conduct experiments on acoustic event recognition for machinery diagnosis, and show that the proposed system can achieve high accuracy.",https://ieeexplore.ieee.org/document/8029228/,2017 IEEE International Conference on Cognitive Computing (ICCC),25-30 June 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2011.6034840,Cognitive decision unit applied to autonomous biped robot NAO,IEEE,Conferences,"The novel approach to use meta-psychology - the theoretic foundation of psychoanalysis - as archetype for a decision making framework for autonomous agents was realized in simulations recently. In addition, multiple studies showed the capability of a robot to sense and interact in its environment. This work fills the gap between sensing, environmental interaction and decision making by grounding these topics with an agents internal needs using the concepts of meta-psychology. The bodies of typical agents are equipped with internal systems which can generate bodily needs - for example the urgent need for food. As proof-of-concept we implemented this concept on a simulated agent as well as on a physical real humanoid biped robot to additionally proof the concept within a fully controlled simulated environment. The use of the common humanoid robot platform NAO, which has 25 degrees of freedom and biped locomotion, enforced us to deal with complex situations and disturbed sensor readings. NAO provides various internal sensors like engine temperature or battery level as well as external sensors like sonar or cameras. An implemented visual marker detecting system allowed us to detect objects in the surrounding environmental, representing food or energy sources. We show, how it is possible to use the psychoanalytically inspired framework ARS to control a real world application, the robot NAO.",https://ieeexplore.ieee.org/document/6034840/,2011 9th IEEE International Conference on Industrial Informatics,26-29 July 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CARE.2013.6733739,Cognitive learning enabled real time object search robot,IEEE,Conferences,"Object Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Most works are focused on a specific application, such as tracking human, car, or pre-learned objects. All these require database and considerable amount of training time to detect the current object and to track it. In this paper we propose a method to track objects where a pre-stored database is not a requirement. The proposed method uses a combination of Scale Invariant Feature Transform (SIFT) based feature extraction, Kalman filter and Cognitive learning. The algorithm has the ability to make its own database of the objects in the due course of time by interacting with the user through text based communication. This algorithm is deployed on a search robot which does the operation of searching an object in real time upon a command from the user. The search operation of robot is made more flexible using Bluetooth wireless communication protocol.",https://ieeexplore.ieee.org/document/6733739/,"2013 International Conference on Control, Automation, Robotics and Embedded Systems (CARE)",16-18 Dec. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2019.00023,Collision-Free Path Finding for Dynamic Gaming and Real Time Robot Navigation,IEEE,Conferences,"Collision-free path finding is crucial for multi-agent traversing environments like gaming systems. An efficient and accurate technique is proposed for avoiding collisions with potential obstacles in virtual and real time environments. Potential field is a coherent technique but it eventuates with various problems like static map usage and pre-calculated potential field map of the environment. It is unsuitable for dynamically changing or unknown environments. Agents can get stuck inside a local minima incompetent in escaping without a workaround implementation. This paper presents efficient and accurate solutions to find collision free path using potential field for dynamic gaming and real time robot navigation. A surfing game in two testing environments with a Gamecar and a physical robot called Robocar is created with dynamic and solid obstacles. Sensor like proximity, line and ultrasonic are used along with the camera as different agents for path finding. The proposed intelligent agent (IA) technique is compared with other path planing algorithms and games in terms of time complexity, cost metrics, decision making complexity, action repertoire, interagent communication, reactivity and temporally continuous. It traverses for 135 meters(m) in 55.8 seconds(s) covering 20 goals and 419.3 m in 8.7 minutes while avoiding 10 local minimas successfully. Proposed technique shows comparable results to path finding with techniques using neural networks and A* algorithm. Experimental results prove the efficiency with run time overload, time complexity and resource consumption of the proposed technique.",https://ieeexplore.ieee.org/document/8995276/,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),4-6 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAP.2017.8090332,Color based moving object tracking with an active camera using motion information,IEEE,Conferences,"In this study, a real-time system design, which can track (RGB) targets in dynamic environments with an active camera, was implemented. Object tracking applications are quite important for military, surveillance system and operational robot applications and getting more important day by day. This design allows us tracking an object using fewer cameras. The design consists of three main parts that are object detection, mapping, tracking the object. When the camera catches the object, first it detects the shape of the object and creates bounding box, according to bounding box information, the algorithm calculates the centroid of the object. Object coordinates are determined using centroid of the object then tracking process works by activating motors via Arduino-MATLAB communication. The motors placed on a platform called pan-tilt platform. The platform can turn 270 and 180 degrees on×and y-axis respectively.",https://ieeexplore.ieee.org/document/8090332/,2017 International Artificial Intelligence and Data Processing Symposium (IDAP),16-17 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2017.8246935,Combining deep learning for visuomotor coordination with object identification to realize a high-level interface for robot object-picking,IEEE,Conferences,"We present a proof of concept to show how a deep network for end-to-end visuomotor learning to grasp is coupled with an attention focus mechanism for state-of-the-art object detection with convolutional neural networks. The cognitively motivated integration of both methods in a single robotic system allows us to realize a high-level interface to use the visuomotor network in environments with several objects, which otherwise would only be usable in environments with a single object. The resulting system is deployed on a humanoid robot, and we perform several real-world grasping experiments that demonstrate the feasibility of our approach.",https://ieeexplore.ieee.org/document/8246935/,2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids),15-17 Nov. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1996.571056,Combining probabilistic map and dialog for robust life-long office navigation,IEEE,Conferences,A design of mobile robot for robust life-long navigation in office environment is proposed and evaluated. The key idea is combining probabilistic map and dialog with humans for reducing the location uncertainty. Bayesian inference with the map represented by probabilistic automata is used in order to reduce the number of queries and to evaluate the success rate of planned paths. We experimentally implemented the design using a simple Bayesian network with continuous nodes and demonstrated its effectiveness in a real environment.,https://ieeexplore.ieee.org/document/571056/,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS '96,8-8 Nov. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS47443.2019.8971680,Comparison of Object Recognition Approaches using Traditional Machine Vision and Modern Deep Learning Techniques for Mobile Robot,IEEE,Conferences,"In this paper, we consider the problem of object recognition for a mobile robot in an indoor environment using two different vision approaches. Our first approach uses HOG descriptor with SVM classifier as traditional machine vision model while the second approach uses Tiny-YOLOv3 as modern deep learning model. The purpose of this study is to gain intuitive insight of both approaches for understanding the principles behind these techniques through their practical implementation in real world. We train both approaches with our own dataset for doors. The proposed work is assessed through the real-world implementation of both approaches using mobile robot with Zed camera in real world indoor environment and the robustness has been evaluated by comparing and analyzing the experimental results of both models on same dataset.",https://ieeexplore.ieee.org/document/8971680/,"2019 19th International Conference on Control, Automation and Systems (ICCAS)",15-18 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITNT52450.2021.9649145,Comparison of Reinforcement Learning Algorithms for Motion Control of an Autonomous Robot in Gazebo Simulator,IEEE,Conferences,"This article compares various implementations of deep Q learning as it is one of the most efficient reinforcement learning algorithms for discrete action space systems. The efficiency of the implementations for the classical Cartpole problem ported to the Gazebo environment is investigated. Then, these algorithms are compared for a self-created bipedal robot problem. Since the creation and configuration of a real robotic system is a laborious process, the initial debugging of the robot can be performed using the appropriate software that simulates the real environment. In our case, the Gazebo simulator was used. Using the simulator allows you to conduct research without having a real robotic system. In this case, it is possible to transfer the results from the simulator to the real system. The result of the study is the conclusion about the greatest efficiency of deep Q-learning with the experience reproduction mechanism. Also, the conclusion is that even for a robot with two degrees of freedom, Q-learning algorithms are not effective enough, and a comparative study with other families of reinforcement learning algorithms is needed.",https://ieeexplore.ieee.org/document/9649145/,2021 International Conference on Information Technology and Nanotechnology (ITNT),20-24 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2006.282163,Conceptual Design and Implementation of Arm Wrestling Robot,IEEE,Conferences,"In this paper, we develop a novel robotic arm wrestling system integrated with mechanical arm, elbow/wrist force sensors, servo motor, encoder, 3-D MEMS accelerometer, and USB camera. The arm wrestling robot (AWR) is intended to play arm wrestling game with real human on a table for entertainment. The designing scenario of the prototype model's hardware is performed. Elbow/wrist force sensors, as a crucial device in the force sensing system, are described in details. Software is developed for device driven and interface. The surface electromyographic (EMG) signals from the upper limb are sampled when a real player competes with the force testing system. By using the method of wavelet packet transformation (WPT), the high-frequency noises can be eliminated effectively and the characteristics of EMG signals can be extracted. Artificial neural network is adopted to estimate the elbow joint torque. The effectiveness of the humanoid algorithm using torque control estimated via WRT and neural network is confirmed by experiments",https://ieeexplore.ieee.org/document/4059139/,2006 IEEE/RSJ International Conference on Intelligent Robots and Systems,9-15 Oct. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2009.5354270,Consideration on robotic giant-swing motion generated by reinforcement learning,IEEE,Conferences,"This study attempts to make a compact humanoid robot acquire a giant-swing motion without any robotic models by using reinforcement learning; only the interaction with environment is available. Generally, it is widely said that this type of learning method is not appropriated to obtain dynamic motions because Markov property is not necessarily guaranteed during the dynamic task. However, in this study, we try to avoid this problem by embedding the dynamic information in the robotic state space; the applicability of the proposed method is considered using both the real robot and dynamic simulator. This paper, in particular, discusses how the robot with 5-DOF, in which the Q-Learning algorithm is implemented, acquires a giant-swing motion. Further, we describe the reward effects on the Q-Learning. Finally, this paper demonstrates that the application of the Q-Learning enable the robot to perform a very attractive giant-swing motion.",https://ieeexplore.ieee.org/document/5354270/,2009 IEEE/RSJ International Conference on Intelligent Robots and Systems,10-15 Oct. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AITest.2019.00015,Constraint-Based Testing of An Industrial Multi-Robot Navigation System,IEEE,Conferences,"Intelligent multi-robot systems get more and more deployed in industrial settings to solve complex and repetitive tasks. Due to safety and economic reasons they need to operate dependably. To ensure a high degree of dependability, testing the deployed system has to be done in a rigorous way. Advanced multi-robot systems show a rich set of complex behaviors. Thus, these systems are difficult to test manually. Moreover, the space of potential environments and tasks for such systems is enormous. Therefore, methods that are able to explore this space in a structured way are needed. One way to address these issues is through model-based testing. In this paper we present an approach for testing the navigation system of a fleet of industrial transport robots. We show how all potential environments and navigation behaviors as well as requirements and restrictions can be represented in a formal constraint-based model. Moreover, we present the concept of coverage criteria in order to handle the potentially infinite space of test cases. Finally, we show how test cases can be derived from this model in an efficient way. In order to show the feasibility of the proposed approach we present an empirical evaluation of a prototype implementation using a real industrial use case.",https://ieeexplore.ieee.org/document/8718216/,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),4-9 April 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN47096.2020.9223341,Context Dependent Trajectory Generation using Sequence-to-Sequence Models for Robotic Toilet Cleaning,IEEE,Conferences,"A robust, easy-to-deploy robot for service tasks in a real environment is difficult to construct. Record-and-playback (R&P) is a method used to teach motor-skills to robots for performing service tasks. However, R&P methods do not scale to challenging tasks where even slight changes in the environment, such as localization errors, would either require trajectory modification or a new demonstration. In this paper, we propose a Sequence-to-Sequence (Seq2Seq) based neural network model to generate robot trajectories in configuration space given a context variable based on real-world measurements in Cartesian space. We use the offset between a target pose and the actual pose after localization as the context variable. The model is trained using a few expert demonstrations collected using teleoperation. We apply our proposed method to the task of toilet cleaning where the robot has to clean the surface of a toilet bowl using a compliant end-effector in a constrained toilet setting. In the experiments, the model is given a novel offset context and it generates a modified robot trajectory for that context. We demonstrate that our proposed model is able to generate trajectories for unseen setups and the executed trajectory results in cleaning of the toilet bowl.",https://ieeexplore.ieee.org/document/9223341/,2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),31 Aug.-4 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAUPEC/RobMech/PRASA48453.2020.9041114,Context-Aware Action with a Small Mobile Robot,IEEE,Conferences,"Simultaneous advances in mobile GPU computing and real-time object recognition now enable machines to make decisions and take actions based on the detection of objects of interest in the environment. An implementation of a mobile robot system that combines autonomous exploration and mapping capabilities with a real-time object recognition method based on a deep neural network running on a mobile GPU, is described. The system is able to detect objects of interest and then take real-time actions to interact with the objects, in this case, by moving to acquire inspection-style images of the object, from multiple angles. The robot system is small, self-contained and runs on battery power. The system shows the potential for the development of robotic systems with context awareness, permitting advanced autonomy.",https://ieeexplore.ieee.org/document/9041114/,2020 International SAUPEC/RobMech/PRASA Conference,29-31 Jan. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEEE.2011.6106626,Continuous-time neural control for a 2 DOF vertical robot manipulator,IEEE,Conferences,"This paper presents a continuous-time neural control scheme for identification and control of a two degrees of freedom (DOF) direct drive vertical robot manipulator model, on which effects due to friction and gravitational forces are both considered. A recurrent high-order neural network (RHONN) structure is proposed in order to identify the plant model to then, based on this neural structure, derive a neural controller using the backstepping design methodology. The trajectory tracking performance of the neural controller is illustrated via simulations results, which suggest the validity of the proposed approach for its implementation in real-time.",https://ieeexplore.ieee.org/document/6106626/,"2011 8th International Conference on Electrical Engineering, Computing Science and Automatic Control",26-28 Oct. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WHC.2011.5945522,Control of a desktop mobile haptic interface,IEEE,Conferences,"Most haptic devices share two main limits: they are grounded and they have limited workspace. A possible solution is to create haptic interfaces by combining mobile robots and standard grounded force-feedback devices, the so called Mobile Haptic Interfaces (MHIs). However, MHIs are characterized by dynamical limitations due to performance of the employed devices. This paper focuses on basic design issues and presents a novel (prototype) Mobile Haptics Platform that employs the coordination of numerically controlled wheel torques to render forces to a user handle placed on the top of the device. The interface, consisting in a small omni-directional robot, is link-less, fully portable and it has been designed to support home-rehabilitation exercises. In the present paper we shall review relevant choices concerning the functional aspects and the control design. In particular a specific embedded sensor fusion was implemented to allow the device to move on a desk without drifting. The sensor fusion algorithm has been optimized to provide users with a quality force feedback while ensuring accurate position tracking. The two requirements are in contrast each other and a specific variant of the Extended Kalman Filter (EKF) was required to allow the device working.",https://ieeexplore.ieee.org/document/5945522/,2011 IEEE World Haptics Conference,21-24 June 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197209,Cooperative Multi-Robot Navigation in Dynamic Environment with Deep Reinforcement Learning,IEEE,Conferences,"The challenges of multi-robot navigation in dynamic environments lie in uncertainties in obstacle complexities, partially observation of robots, and policy implementation from simulations to the real world. This paper presents a cooperative approach to address the multi-robot navigation problem (MRNP) under dynamic environments using a deep reinforcement learning (DRL) framework, which can help multiple robots jointly achieve optimal paths despite a certain degree of obstacle complexities. The novelty of this work includes threefold: (1) developing a cooperative architecture that robots can exchange information with each other to select the optimal target locations; (2) developing a DRL based framework which can learn a navigation policy to generate the optimal paths for multiple robots; (3) developing a training mechanism based on dynamics randomization which can make the policy generalized and achieve the maximum performance in the real world. The method is tested with Gazebo simulations and 4 differential drive robots. Both simulation and experiment results validate the superior performance of the proposed method in terms of success rate and travel time when compared with the other state-of-art technologies.",https://ieeexplore.ieee.org/document/9197209/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1998.677351,Cooperative behavior acquisition in multi-mobile robots environment by reinforcement learning based on state vector estimation,IEEE,Conferences,"This paper proposes a method that acquires robots' behaviors based on the estimation of the state vectors. In order to acquire the cooperative behaviors in multi-robot environments, each learning robot estimates the local predictive model between the learner and the other objects separately. Based on the local predictive models, the robots learn the desired behaviors using reinforcement learning. The proposed method is applied to a soccer playing situation, where a rolling ball and other moving robots are well modeled and the learner's behaviors are successfully acquired by the method. Computer simulations and real experiments are shown and a discussion is given.",https://ieeexplore.ieee.org/document/677351/,Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146),20-20 May 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2012.6385982,Cooperative sensing and recognition by a swarm of mobile robots,IEEE,Conferences,"We present an approach for distributed real-time recognition tasks using a swarm of mobile robots. We focus on the visual recognition of hand gestures, but the solutions that we provide have general applicability and address a number of challenges common to many distributed sensing and classification problems. In our approach, robots acquire and process hand images from multiple points of view, most of which do not allow for a satisfactory classification. Each robot is equipped with a statistical classifier, which is used to generate an opinion for the sensed gesture. Using a low-bandwidth wireless channel, the robots locally exchange their opinions. They also exploit mobility to adapt their positions to maximize the mutual information collectively gathered by the swarm. A distributed consensus protocol is implemented, to allow to rapidly settle on a decision once enough evidence is available. The system is implemented and demonstrated on real robots. In addition, extensive quantitative results of emulation experiments, based on a real image dataset, are reported. We consider different scenarios and study the scalability and the robustness of the swarm performance for distributed recognition.",https://ieeexplore.ieee.org/document/6385982/,2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,7-12 Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2014.6942970,Coordination in human-robot teams using mental modeling and plan recognition,IEEE,Conferences,"Beliefs play an important role in human-robot teaming scenarios, where the robots must reason about other agents' intentions and beliefs in order to inform their own plan generation process, and to successfully coordinate plans with the other agents. In this paper, we cast the evolving and complex structure of beliefs, and inference over them, as a planning and plan recognition problem. We use agent beliefs and intentions modeled in terms of predicates in order to create an automated planning problem instance, which is then used along with a known and complete domain model in order to predict the plan of the agent whose beliefs are being modeled. Information extracted from this predicted plan is used to inform the planning process of the modeling agent, to enable coordination. We also look at an extension of this problem to a plan recognition problem. We conclude by presenting an evaluation of our technique through a case study implemented on a real robot.",https://ieeexplore.ieee.org/document/6942970/,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,14-18 Sept. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2000.898390,Coordination strategies for the goal-keeper of a RoboCup mid-size team,IEEE,Conferences,"In robot soccer, as well as in real soccer and in every team effort, coordination between members of a team is a key issue. We describe the coordination strategies that were designed to achieve effective cooperation between a robot goal-keeper and the rest of the ART (Azzurra Robot Team) team that participates in the RoboCup F-2000 (midsize) competitions.",https://ieeexplore.ieee.org/document/898390/,Proceedings of the IEEE Intelligent Vehicles Symposium 2000 (Cat. No.00TH8511),5-5 Oct. 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593374,Cost of Transport Estimation for Legged Robot Based on Terrain Features Inference from Aerial Scan,IEEE,Conferences,"The effectiveness of the robot locomotion can be measured using the cost of transport (CoT) which represents the amount of energy that is needed for traversing from one place to another. Terrains excerpt different mechanical properties when crawled by a multi-legged robot, and thus different values of the CoT. It is therefore desirable to estimate the CoT in advance and plan the robot motion accordingly. However, the CoT might not be known prior the robot deployment, e.g., in extraterrestrial missions; hence, a robot has to learn different terrains as it crawls through the environment incrementally. In this work, we focus on estimating the CoT from visual and geometrical data of the crawled terrain. A thorough analysis of different terrain descriptors within the context of incremental learning is presented to select the best performing approach. We report on the achieved results and experimental verification of the selected approaches with a real hexapod robot crawling over six different terrains.",https://ieeexplore.ieee.org/document/8593374/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIBD55127.2022.9820142,D-Leader Control Algorithm for Multi-Robot Formation Transformation,IEEE,Conferences,"The Leader-Follower algorithm stands out among many multi-robot formation control algorithms due to its simple operation and easy implementation. However, the Leader-Follower algorithm has a strong dependence on the pilot robot and poor information feedback. To solve this problem, the multivariable D-leader algorithm based on multi-robot formation transformation is proposed in this paper. Based on the Leader-Follower algorithm, the idea of dynamic pilot is introduced to make the multi-robot formation flexibly, and realize the dynamic adjustment of the pilot robot while changing the multi-robot formation. At the same time, the Runge-Kutta algorithm and the transformation method of particle model and difference model are combined to realize the flexible adjustment of path and position during formation transformation. The cross-mode experimental simulation shows that compared with the Leader-Follower algorithm, the formation adjustment rate of multi-robot is increased by an average of 66.7&#x0025; and the dynamic navigation rate of pilot robot is increased by an average of 53.35&#x0025; under the D-Leader algorithm. The D-Leader algorithm not only realizes the formation adjustment of multi-robot but also effectively improves the performance of multi-robot formation.",https://ieeexplore.ieee.org/document/9820142/,2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD),27-30 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635949,DRQN-based 3D Obstacle Avoidance with a Limited Field of View,IEEE,Conferences,"In this paper, we propose a map-based end-to-end DRL approach for three-dimensional (3D) obstacle avoidance in a partially observed environment, which is applied to achieve autonomous navigation for an indoor mobile robot using a depth camera with a narrow field of view. We first train a neural network with LSTM units in a 3D simulator of mobile robots to approximate the Q-value function in double DRQN. We also use a curriculum learning strategy to accelerate and stabilize the training process. Then we deploy the trained model to a real robot to perform 3D obstacle avoidance in its navigation. We evaluate the proposed approach both in the simulated environment and on a robot in the real world. The experimental results show that the approach is efficient and easy to be deployed, and it performs well for 3D obstacle avoidance with a narrow observation angle, which outperforms other existing DRL-based models by 15.5% on success rate.",https://ieeexplore.ieee.org/document/9635949/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2018.8525717,Data-driven development of Virtual Sign Language Communication Agents,IEEE,Conferences,"Engaging deaf and hearing people in common discussions requires interfaces to help them understand each other, such as robot agents that translate spoken language into Sign Language (SL) expressions and vice-versa. However, the recognition and generation of signed sentences is a complex task of high dimensionality that cannot be solved in sufficient quality yet. Thus, it is necessary to develop new technologies of improved performances. The sequence to sequence neural network model, traditionally used for machine translation, is adapted to the above two tasks by treating a SL sequence as a multi-dimensional sentence. We defined an encoding of the SL annotations and conducted experiments on the network structure to define a most accurate translation model. This study proves the network trainable and possibly applicable in real-life with an extended dataset, which shall be tested for deployment in virtual translation assistants in the following.",https://ieeexplore.ieee.org/document/8525717/,2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),27-31 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561066,Decentralized Connectivity Maintenance with Time Delays using Control Barrier Functions,IEEE,Conferences,"Connectivity maintenance is crucial for the real world deployment of multi-robot systems, as it ultimately allows the robots to communicate, coordinate and perform tasks in a collaborative way. A connectivity maintenance controller must keep the multi-robot system connected independently from the system’s mission and in the presence of undesired real world effects such as communication delays, model errors, and computational time delays, among others. In this paper we present the implementation, on a real robotic setup, of a connectivity maintenance control strategy based on Control Barrier Functions. During experimentation, we found that the presence of communication delays has a significant impact on the performance of the controlled system, with respect to the ideal case. We propose a heuristic to counteract the effects of communication delays, and we verify its efficacy both in simulation and with physical robot experiments.",https://ieeexplore.ieee.org/document/9561066/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2016.7487617,Decentralized multi-agent exploration with online-learning of Gaussian processes,IEEE,Conferences,"Exploration is a crucial problem in safety of life applications, such as search and rescue missions. Gaussian processes constitute an interesting underlying data model that leverages the spatial correlations of the process to be explored to reduce the required sampling of data. Furthermore, multi-agent approaches offer well known advantages for exploration. Previous decentralized multi-agent exploration algorithms that use Gaussian processes as underlying data model, have only been validated through simulations. However, the implementation of an exploration algorithm brings difficulties that were not tackle yet. In this work, we propose an exploration algorithm that deals with the following challenges: (i) which information to transmit to achieve multi-agent coordination; (ii) how to implement a light-weight collision avoidance; (iii) how to learn the data's model without prior information. We validate our algorithm with two experiments employing real robots. First, we explore the magnetic field intensity with a ground-based robot. Second, two quadcopters equipped with an ultrasound sensor explore a terrain profile. We show that our algorithm outperforms a meander and a random trajectory, as well as we are able to learn the data's model online while exploring.",https://ieeexplore.ieee.org/document/7487617/,2016 IEEE International Conference on Robotics and Automation (ICRA),16-21 May 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967874,Deep Dive into Faces: Pose & Illumination Invariant Multi-Face Emotion Recognition System,IEEE,Conferences,"One of the advancements in humanization of robots is its ability to recognize human emotions. Facial expression plays a key role in identifying human emotions relative to other cues. In this research, an intelligent network capable of real-time emotion recognition from multiple faces using deep learning technique is presented. The proposed network is based on Convolution Neural Network (CNN) in which three blocks of Convolution layers for feature extraction and two blocks of Dense layers for classification are used. The novelty of this method lies in recognizing emotions from multiple faces simultaneously in real time and its invariance to head pose, illumination and age factor. Most of reported work in literature for multiple faces is for frontal face without illumination variation. The proposed emotion recognition system is deployed on Raspberry Pi3 B+ for human robot interaction applications and achieved an average accuracy of 95.8% in real time.",https://ieeexplore.ieee.org/document/8967874/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRoM48714.2019.9071857,Deep Learning Approach For Object Tracking Of RoboEye,IEEE,Conferences,"RoboEye is a spherical 3RRR parallel robot which has been developed for its high precision. It can provide high speeds, so can be used for fast tracking tasks. To this end, in this paper proper deep learning approaches are combined with classical control methods. Deep learning algorithms are employed to detect an object of interest among various ones in a monocular image, and then obtain an estimatation of the distance to the camera. So, simultaneous depth estimation, and object detection with a monocular camera for real time implementation is proposed here. For fast calculations, also to overcome manufacturing uncertainties, inverse kinematic equations are computed by a multi-layer perceptron (MLP) network based on real data. Finally, a classical PID controller can perform a fast tracking of the object.",https://ieeexplore.ieee.org/document/9071857/,2019 7th International Conference on Robotics and Mechatronics (ICRoM),20-21 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR52253.2021.9494704,Deep Learning based Food Instance Segmentation using Synthetic Data,IEEE,Conferences,"In the process of intelligently segmenting foods in images using deep neural networks for diet management, data collection and labeling for network training are very important but labor-intensive tasks. In order to solve the difficulties of data collection and annotations, this paper proposes a food segmentation method applicable to real-world through synthetic data. To perform food segmentation on healthcare robot systems, such as meal assistance robot arm, we generate synthetic data using the open-source 3D graphics software Blender placing multiple objects on meal plate and train Mask R-CNN for instance segmentation. Also, we build a data collection system and verify our segmentation model on real-world food data. As a result, on our real-world dataset, the model trained only synthetic data is available to segment food instances that are not trained with 52.2% mask AP@all, and improve performance by +6.4%p after fine-tuning comparing to the model trained from scratch. In addition, we also confirm the possibility and performance improvement on the public dataset for fair analysis.",https://ieeexplore.ieee.org/document/9494704/,2021 18th International Conference on Ubiquitous Robots (UR),12-14 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8794187,Deep Learning based Motion Prediction for Exoskeleton Robot Control in Upper Limb Rehabilitation,IEEE,Conferences,"The synchronization of the movement between exoskeleton robot and human arm is crucial for Robot-assisted training (RAT) in upper limb rehabilitation. In this paper, we propose a deep learning based motion prediction model which is applied to our recently developed 8 degrees-of-freedom (DoFs) upper limb rehabilitation exoskeleton, named NTUH-II. The human arm dynamics and surface electromyography (sEMG) can be first measured by two wireless sensors and used as input of deep learning model to predict user's motion. Then, the prediction can be used as desired motion trajectory of the exoskeleton. As a result, the robot arm can follow the movement on either side of the user's arm in real-time. Various experiments have been conducted to verify the performance of the proposed motion prediction model, and the results show that the proposed motion prediction implementation can reduce the mean absolute error and the average delay time of movement between human arm and robot arm.",https://ieeexplore.ieee.org/document/8794187/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS52745.2021.9650051,Deep Learning based-State Estimation for Holonomic Mobile Robots Using Intrinsic Sensors,IEEE,Conferences,"State estimation is a fundamental component of the navigation system of autonomous mobile robots. Generally, the robot setup is equipped with intrinsic and extrinsic sensors. The state estimators have relied almost on intrinsic sensors such as wheel encoders and inertial measurement units in textureless and structureless environments. This paper will analyze and propose the learning state estimation frameworks for the dead-reckoning of autonomous holonomic vehicles based only on intrinsic sensors. First, we review and categories the intrinsic-only estimation problem. Second, we describe the problem formulation using learning-based techniques. Next, the learning inertial-only estimation is presented with several strategies using the deep learning technique. The initial experiment results are analyzed and deployed using a holonomic mobile robot in real-world environments.",https://ieeexplore.ieee.org/document/9650051/,"2021 21st International Conference on Control, Automation and Systems (ICCAS)",12-15 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE48305.2020.9216881,Deep Learning for Early Damage Detection of Tailing Pipes Joints with a Robotic Device,IEEE,Conferences,"In the mining industry, it is usual to employ several kilometers of pipes to carry tailing from the plant to a dam. Only in the Salobo Mine, a copper operation in the Amazon forest from Vale S.A., there are more than three and a half kilometers of tailing pipes. Since the material passing through the tailing pipe causes an abrasion effect that could lead to failures, regular inspections are needed. However, given the risky environment to perform manual inspections, a teleoperated or autonomous robot is a crucial tool to keep track of the pipe health. In this work, we propose a deep-learning methodology to process the data stream of images from the robot, aiming to detect early failures directly on the onboard computer of the device in real-time. Multiple architectures of deep-learning image classification were evaluated to detect the anomalies. We validated the early damage detection accuracy and pinpointed the approximate location of the anomalies using the Class Activation Mapping of the networks. Then, we tested the runtime for the network architectures that obtained the best results on different hardware to analyze the need for a GPU onboard in the robot. Moreover, we also trained a Single Shot object Detector to find the boundaries of the pipe joints, which means that the anomaly classification is performed only when a joint is detected. Our results show that it is possible to build an automatic anomaly detection system in the software of the robot.",https://ieeexplore.ieee.org/document/9216881/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/R10-HTC.2018.8629836,Deep Learning-Based Eye Gaze Controlled Robotic Car,IEEE,Conferences,"In recent years Eye gaze tracking (EGT) has emerged as an attractive alternative to conventional communication modes. Gaze estimation can be effectively used in human-computer interaction, assistive devices for motor-disabled persons, autonomous robot control systems, safe car driving, diagnosis of diseases and even in human sentiment assessment. Implementation in any of these areas however mostly depends on the efficiency of detection algorithm along with usability and robustness of detection process. In this context we have proposed a Convolutional Neural Network (CNN) architecture to estimate the eye gaze direction from detected eyes which outperforms all other state of the art results for Eye-Chimera dataset. The overall accuracies are 90.21&#x0025; and 99.19&#x0025; for Eye-Chimera and HPEG datasets respectively. This paper also introduces a new dataset EGDC for which proposed algorithm finds 86.93&#x0025; accuracy. We have developed a real-time eye gaze controlled robotic car as a prototype for possible implementations of our algorithm.",https://ieeexplore.ieee.org/document/8629836/,2018 IEEE Region 10 Humanitarian Technology Conference (R10-HTC),6-8 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2018.8489239,Deep Learning-based Cooperative Trail Following for Multi-Robot System,IEEE,Conferences,"Following trails in the wild is an essential capability of out-door autonomous mobile robots. Recently, deep learningbased approaches have made great advancements in this field. However, the existing research only focuses on the trail following with a single robot. In contrast, many robotic tasks in the reality, such as search and patrolling, are conducted by a group of robots. While these robots are grouped to move in the wild, they can cooperate to significantly promote the trail following accuracy, for example, by sharing images of different view angles or real-time decision fusion. This paper proposes such an approach named DL-Cooper that enables multi-robot visionbased trail following based on deep learning algorithms. It allows each robot to make a decision respectively with deep neural network and then fusion the decisions on the collective level with the support of back-end cloud computing infrastructure. It also takes Quality of Service (QoS) assurance, a very essential property of robotic software, into consideration. By limiting the condition to fusion decisions, the time latency can be minimally sacrificed. Experiments on the real-world dataset show that our approach has significantly improved the accuracy of the singlerobot system.",https://ieeexplore.ieee.org/document/8489239/,2018 International Joint Conference on Neural Networks (IJCNN),8-13 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ASCC56756.2022.9828060,Deep Learning-based Real-time Object Detection for Empty-Dish Recycling Robot,IEEE,Conferences,"The world is facing a shrinking workforce by the sagging birth rate and an aging population. Robot techniques are one of the best solutions for taking place of humans and overcoming this emergency issue. This paper introduces a deep learning-based empty-dish recycling robot for realizing the automatic empty-dish recycling after breakfast, dinner, or lunch in a restaurant, canteen, or cafeteria. A deep learning model&#x2013;You Only Look Once (YOLO)&#x2013;is equipped for dish detection such as cups, bowls, chopsticks, towels et al., and catch points are calculated for controlling the robot arm to recycle the target dishes. Finally, the YOLOv4 model is quantized by TensorRT and deployed on Jetson Nano. The real-time dish detection YOLO is focused on this paper, the experimental results show that after the YOLO model quantization, the detection time of a single image is increased from 3.93s to 0.44s, with more than 96.00% high accuracy on Precision, Recall, and F1 values. The functions of empty-dish recycling are realized, which will lead to further development toward practical use.",https://ieeexplore.ieee.org/document/9828060/,2022 13th Asian Control Conference (ASCC),4-7 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561729,Deep Neuromorphic Controller with Dynamic Topology for Aerial Robots,IEEE,Conferences,"Current aerial robots are increasingly adaptive; they can morph to enable operation in changing conditions to complete diverse missions. Each mission may require the robot to conduct a different task. A conventional learning approach can handle these variations when the system is trained for similar tasks in a representative environment. However, it may result in overfitting to the new data stream or the failure to adapt, leading to degradation or a potential crash. These problems can be mitigated with an excessive amount of data and embedded model, but the computational power and the memory of the aerial robots are limited. In order to address the variations in the model, environment as well as the tasks within onboard computation limitations, we propose a deep neuromorphic controller approach with variable topologies to handle each different condition and the data stream with a feasible computation and memory allocation. The proposed approach is based on a deep neuromorphic (multi and variable layered neural network) controller with dynamic depth and progressive layer adaptation for each new data stream. This adaptive structure is combined with a switching function to form a sliding mode controller. The network parameter update rule guarantees the stability of the closed loop system by the convergence of the error dynamics to the sliding surface. Being the first implementation on an aerial robot in this context, the results illustrate the adaptation capability, stability, computational efficiency as well as the real-time validation.",https://ieeexplore.ieee.org/document/9561729/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2018.8665274,Deep Reinforcement Learning Based Brachiation Control for Two-Link Bio-Primate Robot,IEEE,Conferences,"Manually designing an effective and efficient controller for complex mechanics, such as bio-inspired robots or underactuated mechanical system, typically are very difficult. It requires precise motion planning and dynamic control. Reinforcement learning or genetic algorithm based learning methods suffers from representing the high dimensional models. The combination of deep learning and reinforcement learning provide a feasible way to handle such difficulties. However, priori-less searching sometimes tends to be low efficient and usually finds the “mechanic” solution instead of the “natural” one. In this paper, the traditional nonlinear control concept is integrated into the deep reinforcement learning (DRL) framework. The whole process is implemented on the brachiation control problem of a two link bio-primate robot. Deep Deterministic Policy Gradient (DDPG) is used to search for the optimal control policy. The searching process is realized by interacting with the dynamic model instead of real robot. The energy based planning and control concept is adopted, which utilize the fact that when the shoulder joint angle is fixed, energy of the whole system keeps constant. By regulating the angle and energy, the robot can be restricted on a particular trajectory. The energy concept is encoded within the reward function and trained in the Gym environment. For varying targets point-to-point control, the network structure is also modified to accept the target coordinates. Effectiveness of the proposed methods are verified by simulation and experimental results.",https://ieeexplore.ieee.org/document/8665274/,2018 IEEE International Conference on Robotics and Biomimetics (ROBIO),12-15 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ASCC56756.2022.9828057,Deep Reinforcement Learning Based Tracking Control of Unmanned Vehicle with Safety Guarantee,IEEE,Conferences,"It is well known that the development of efficient real-time path following strategy and collision avoidance mechanism is critical to the practical implementation of autonomous driving technique. Within this context, this paper presents a new kind of hybrid control strategy consisting of the robot Stanley's trajectory tracking algorithm [1] and deep reinforcement learning (DRL) technique to achieve the goal of tracking control of unmanned vehicle with safety guarantee. By introducing the DRL technique, the tracking accuracy of the robot Stanley's trajectory tracking algorithm is improved and a safe control algorithm with collision avoidance is obtained. Furthermore, the complexity of the learning algorithm involved in the tracking controller is significantly reduced by using the Stanley's trajectory tracking algorithm, which makes the learning converge fast. Finally, numerical simulations are performed to verify that the proposed tracking algorithm has obviously advantages on tracking accuracy and training efficiency over some existing ones.",https://ieeexplore.ieee.org/document/9828057/,2022 13th Asian Control Conference (ASCC),4-7 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN48605.2020.9207332,Deep Reinforcement Learning Control of Hand-Eye Coordination with a Software Retina,IEEE,Conferences,"Deep Reinforcement Learning (DRL) has gained much attention for solving robotic hand-eye coordination tasks from raw pixel values. Despite promising results, training agents using images is hardware intensive often requiring millions of training steps to converge incurring long training times and increased risk of wear and tear on the robot. To speed up training, images are often cropped and downscaled resulting in a smaller field of view and loss of valuable high-frequency data. In this paper, we propose training the vision system using supervised learning prior to training robotic actuation using Deep Deterministic Policy Gradient (DDPG). The vision system uses a software retina, based on the mammalian retino-cortical transform, to preprocess full-size images to compress image data while preserving the full field of view and high-frequency visual information around the fixation point prior to processing by a Deep Convolutional Neural Network (DCNN) to extract visual state information. Using the vision system to preprocess the environment improves the agent's sample complexity and network update speed leading to significantly faster training with reduced image data loss. Our method is used to train a DRL system to control a real Baxter robot's arm, processing full-size images captured by an in-wrist camera to locate an object on a table and centre the camera over it by actuating the robot arm.",https://ieeexplore.ieee.org/document/9207332/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SoutheastCon44009.2020.9249654,Deep Reinforcement Learning For Visual Navigation of Wheeled Mobile Robots,IEEE,Conferences,"A study is presented on applying deep reinforcement learning (DRL) for visual navigation of wheeled mobile robots (WMR) in dynamic and unknown environments. Two DRL algorithms, namely, value-learning deep Q-network (DQN) and policy gradient based asynchronous advantage actor critic ( A 3C), have been considered. RGB (red, green and blue) and depth images have been used as inputs in implementation of both DRL algorithms to generate control commands for autonomous navigation of WMR in simulation environments. The initial DRL networks were generated and trained progressively in OpenAI Gym Gazebo based simulation environments within robot operating system (ROS) framework for a popular target WMR, Kobuki TurtleBot2. A pre-trained deep neural network ResNet50 was used after further training with regrouped objects commonly found in laboratory setting for target-driven mapless visual navigation of Turlebot2 through DRL. The performance of A 3C with multiple computation threads (4, 6, and 8) was simulated on a desktop. The navigation performance of DQN and A 3C networks, in terms of reward statistics and completion time, was compared in three simulation environments. As expected, A 3C with multiple threads (4, 6, and 8) performed better than DQN and the performance of A 3C improved with number of threads. Details of the methodology, simulation results are presented and recommendations for future work towards real-time implementation through transfer learning of the DRL models are outlined.",https://ieeexplore.ieee.org/document/9249654/,2020 SoutheastCon,28-29 March 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCNC45663.2020.9120611,Deep Reinforcement Learning based Indoor Air Quality Sensing by Cooperative Mobile Robots,IEEE,Conferences,"Confronted with the severe indoor air pollution nowadays, we propose the usage of multiple robots to detect the indoor air quality (IAQ) cooperatively for fewer sensors and larger sensing area. To acquire the complete real-time IAQ distribution map, we exploit the real statistical data to construct the IAQ data model and adopt Kalman Filter to obtain the estimation of the unmeasured area. Since the movement of the robots affects the estimation accuracy, a proper movement strategy should be planned to minimize the total estimation error. To solve this optimization problem, we design a deep Q-learning approach, which provides sub-optimal movement strategies for real-time robot sensing. By simulations, we verify the adopted IAQ data model and testify the effectiveness of the proposed solution. For application considerations, we have deployed this system in Peking University since Dec. 2018 and developed a website to visualize the IAQ distribution.",https://ieeexplore.ieee.org/document/9120611/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS50221.2020.9268370,Deep Reinforcement Learning-based ROS-Controlled RC Car for Autonomous Path Exploration in the Unknown Environment,IEEE,Conferences,"Nowadays, Deep reinforcement learning has become the front runner to solve problems in the field of robot navigation and avoidance. This paper presents a LiDAR-equipped RC car trained in the GAZEBO environment using the deep reinforcement learning method. This paper uses reshaped LiDAR data as the data input of the neural architecture of the training network. This paper also presents a unique way to convert the LiDAR data into a 2D grid map for the input of training neural architecture. It also presents the test result from the training network in different GAZEBO environment. It also shows the development of hardware and software systems of embedded RC car. The hardware system includes-Jetson AGX Xavier, teensyduino and Hokuyo LiDAR; the software system includes-ROS and Arduino C. Finally, this paper presents the test result in the real world using the model generated from training simulation.",https://ieeexplore.ieee.org/document/9268370/,"2020 20th International Conference on Control, Automation and Systems (ICCAS)",13-16 Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARSC52212.2021.9429811,Deep Reinforcement Multi-Directional Kick-Learning of a Simulated Robot with Toes,IEEE,Conferences,"This paper describes a thorough analysis of using PPO to learn kick behaviors with simulated NAO robots in the simspark environment. The analysis includes an investigation of the influence of PPO hyperparameters, network size, training setups and performance in real games. We believe to improve the state of the art mainly in four points: first, the kicks are learned with a toed version of the NAO robot, second, we improve the reliability with respect to kickable area and avoidance of falls, third, the kick can be parameterized with desired distance and direction as input to the deep network and fourth, the approach allows to integrate the learned behavior seamlessly into soccer games. The result is a significant improvement of the general level of play.",https://ieeexplore.ieee.org/document/9429811/,2021 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC),28-29 April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS.2019.8797770,Deep learning based semantic situation awareness system for multirotor aerial robots using LIDAR,IEEE,Conferences,"In this work, we present a semantic situation awareness system for multirotor aerial robots, based on 2D LIDAR measurements, targeting the understanding of the environment and assuming to have a precise robot localization as an input of our algorithm. Our proposed situation awareness system calculates a semantic map of the objects of the environment as a list of circles represented by their radius, and the position and the velocity of their center in world coordinates. Our proposed algorithm includes three main parts. First, the LIDAR measurements are preprocessed and an object segmentation clusters the candidate objects present in the environment. Secondly, a Convolutional Neural Network (CNN) that has been designed and trained using an artificially generated dataset, computes the radius and the position of the center of individual circles in sensor coordinates. Finally, an indirect-EKF provides the estimate of the semantic map in world coordinates, including the velocity of the center of the circles in world coordinates.We have quantitative and qualitative evaluated the performance of our proposed situation awareness system by means of Software-In-The-Loop simulations using VRep with one and multiple static and moving cylindrical objects in the scene, obtaining results that support our proposed algorithm. In addition, we have demonstrated that our proposed algorithm is capable of handling real environments thanks to real laboratory experiments with non-cylindrical static (i.e. a barrel) and moving (i.e. a person) objects.",https://ieeexplore.ieee.org/document/8797770/,2019 International Conference on Unmanned Aircraft Systems (ICUAS),11-14 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASCC.2017.8287420,Deep learning for picking point detection in dense cluster,IEEE,Conferences,"This paper considers the problem of picking objects in cluster. This requires the robot to reliably detect the picking point for the known or unseen objects under the environment with occlusion, disorder and a variety of objects. We present a novel pipeline to detect picking point based on deep convolutional neural network (CNN). A two-dimensional picking configuration is proposed, thus an extensive data augmentation strategy is enabled and a labeled dataset is established quickly and easily. At last, we demonstrate the implementation of our method on a real robot and show that our method can accurately detect picking point of unseen objects and achieve a pick success of 91% in cluster bin-picking scenario.",https://ieeexplore.ieee.org/document/8287420/,2017 11th Asian Control Conference (ASCC),17-20 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8206046,Deep predictive policy training using reinforcement learning,IEEE,Conferences,"Skilled robot task learning is best implemented by predictive action policies due to the inherent latency of sensorimotor processes. However, training such predictive policies is challenging as it involves finding a trajectory of motor activations for the full duration of the action. We propose a data-efficient deep predictive policy training (DPPT) framework with a deep neural network policy architecture which maps an image observation to a sequence of motor activations. The architecture consists of three sub-networks referred to as the perception, policy and behavior super-layers. The perception and behavior super-layers force an abstraction of visual and motor data trained with synthetic and simulated training samples, respectively. The policy super-layer is a small subnetwork with fewer parameters that maps data in-between the abstracted manifolds. It is trained for each task using methods for policy search reinforcement learning. We demonstrate the suitability of the proposed architecture and learning framework by training predictive policies for skilled object grasping and ball throwing on a PR2 robot. The effectiveness of the method is illustrated by the fact that these tasks are trained using only about 180 real robot attempts with qualitative terminal rewards.",https://ieeexplore.ieee.org/document/8206046/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2017.8172429,Deep recurrent Q-learning of behavioral intervention delivery by a robot from demonstration data,IEEE,Conferences,"We present a learning from demonstration (LfD) framework that uses a deep recurrent Q-network (DRQN) to learn how to deliver a behavioral intervention (BI) from demonstrations performed by a human. The trained DRQN enables a robot to deliver a similar BI in an autonomous manner. BIs are highly structured procedures wherein children with developmental delays/disorders (e.g. autism, ADHD, etc.) are trained to perform new behaviors and life-skills. Mounting anecdotal evidence from human-robot interaction (HRI) research has shown that BI benefits from the use of robots as a delivery tool. Most of the HRI research on robot-based intervention relies on tele-operated robots. However, the need for autonomy has become increasingly evident, especially when it comes to the real-world deployment of these systems. The few studies that have used autonomy in robot-based BI relied on hand-picked features of the environment in order to trigger correct robot actions. Additionally, none of these automated architectures attempted to learn the BI from human demonstrations, though this appears to be the most natural way of learning. This paper represents the first attempt to design a robot that uses LfD to learn BI. We generate a model then correctly predict appropriate actions with greater than 80% accuracy. To the best of our knowledge, this is the first attempt to employ DRQN within an LfD framework to learn high level reasoning embedded in human actions and behaviors simply from observations.",https://ieeexplore.ieee.org/document/8172429/,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),28 Aug.-1 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR49640.2020.9303294,Deep-Learning Based Robotic Manipulation of Flexible PCBs,IEEE,Conferences,"In the past 10 years, due to the fast development of 3C industries such as mobile phones and computers, people have higher requirements for the automatic soldering technology of flexible PCBs. However, the deformation and the small size of flexible PCBs open up significant challenges to robotic soldering. This paper proposes a deep-learning based manipulation scheme for automatic soldering of flexible PCBs. The proposed controller can enable the robot to automatically contact the flexible PCB first, then actively control the flexible PCB to the desired position with the visual feedback, and finally, the soldering machine will solder the flexible PCBs smoothly. First, the approach of deep learning is used to detect the position of the solder pad (feature). Then, the vision-based controller drives the robot to manipulate the solder pad to the desired position, such that the soldering machine can work to solder two pieces of flexible PCBs together. The use of a deep learning approach can explore the human’s experience to improve the accuracy of detection and hence deals with the issues of clustered environment, change of illumination, and different initial position, etc. The proposed detection approach and control scheme is implemented in a soldering robot for flexible PCBs and the results validate the performance of the proposed methods.",https://ieeexplore.ieee.org/document/9303294/,2020 IEEE International Conference on Real-time Computing and Robotics (RCAR),28-29 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635917,Delay Aware Universal Notice Network: Real world multi-robot transfer learning,IEEE,Conferences,"General purpose simulators provide cheap training data to learn complex robotic skills. However, the transition from simulation to reality is often very challenging for the agent. One major issue is the delay on the physical robot that may deteriorate the performance of the deployed agent. Furthermore, once a successfully trained learning-based control policy is available, re-purposing the knowledge acquired by the agent to enable a structurally distinct agent to perform the same task is hazardous if done naively. In this work, we address the above issues with a single method, the DA-UNN (Delay Aware Universal Notice Network), which decomposes the knowledge into robot-specific and task-specific modules for fast transfer. Our framework deals with delays immanent to physical systems in order to improve sim2real transfer. We evaluate the efficiency of our approach using simulated and actual robots on a dynamic manipulation task where delay management is crucial.",https://ieeexplore.ieee.org/document/9635917/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.00346,DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion,IEEE,Conferences,"A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose.",https://ieeexplore.ieee.org/document/8953386/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6631325,Deploying artificial landmarks to foster data association in simultaneous localization and mapping,IEEE,Conferences,"Data association is an essential problem in simultaneous localization and mapping. It is hard to solve correctly, especially in ambiguous environments. We consider a scenario where the robot can ease the data association problem by deploying a limited number of uniquely identifiable artificial landmarks along its path and use them afterwards as fixed anchors. Obviously, the choice of the positions where the robot should drop these markers is crucial as poor choices might prevent the robot from establishing accurate data associations. In this paper, we present a novel approach for learning when to drop the landmarks so as to optimize the data association performance. We use Monte Carlo reinforcement learning for computing an optimal policy and apply a statistical convergence test to decide if the policy is converged and the learning process can be stopped. Extensive experiments also carried out with a real robot demonstrate that the data association performance using landmarks deployed according to our learned policies is significantly higher compared to other strategies.",https://ieeexplore.ieee.org/document/6631325/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSSIT48917.2020.9214125,Design & implementation of real time autonomous car by using image processing & IoT,IEEE,Conferences,"Because of the inaccessibility of Vehicle-to-Infrastructure correspondence in the present delivering frameworks, (TLD), Traffic Sign Detection and path identification are as yet thought to be a significant task in self-governing vehicles and Driver Assistance Systems (DAS) or Self Driving Car. For progressively exact outcome, businesses are moving to profound Neural Network Models Like Convolutional Neural Network (CNN) as opposed to Traditional models like HOG and so forth. Profound neural Network can remove and take in increasingly unadulterated highlights from the Raw RGB picture got from nature. In any case, profound neural systems like CNN have a highly complex calculation. This paper proposes an Autonomous vehicle or robot that can identify the diverse article in condition and group them utilizing CNN model and through this information can take some continuous choice which can be utilized in the Self Driving vehicle or Autonomous Car or Driving Assistant System (DAS).",https://ieeexplore.ieee.org/document/9214125/,2020 Third International Conference on Smart Systems and Inventive Technology (ICSSIT),20-22 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPL.2005.1515790,Design and FPGA implementation of an embedded real-time biologically plausible spiking neural network processor,IEEE,Conferences,"The implementation of a large scale, leaky-integrate-and-fire neural network processor using the Xilinx Virtex-II family of field programmable gate array (FPGA) is presented. The processor has been designed to model biologically plausible networks of spiking neurons in real-time to assist with the control of a mobile robot. The real-time constraint has led to a re-evaluation of some of the established architectural and algorithmic features of previous spiking neural network based hardware. The design was coded and simulated using Handel-C hardware description language (HDL) and the DK3 design suite from Celoxica. The processor has been physically implemented and tested on a RC200 development board, also from Celoxica.",https://ieeexplore.ieee.org/document/1515790/,"International Conference on Field Programmable Logic and Applications, 2005.",24-26 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCI54379.2022.9740974,Design and Implementation of Arduino Insect Robot for Real Time Applications,IEEE,Conferences,"A robotic project that mimic the insect motions is the Arduino-based Insect Robot. This Arduino Insect robot&#x0027;s characteristics are part of the current advancement of Artificial Intelligence. The characteristics include obstacle avoidance, light attraction, and random resting abilities, which are quite like insect behavior. The project includes an Arduino product, the Arduino-Pro Mini, that serves as the project&#x0027;s principal controller. It has five servo motors that move its front right leg, front left leg, center, or shaft, which is coupled to the two back servos, back right leg and back left leg. The shaft servo used in this project aids robot operation by allowing the rear servos to move at a higher angle. The project also includes three ultrasonic sensors that measure obstacle distance and have configurable parameters that control how many steps the insect robot will take to escape the barrier. Photodiodes with pull-up resistors are also included in the project, which are used to detect light. Two USB connections are also included, with channel 1 rated at 1A, channel 2 rated at 2.1A, and a toggle switch for power supply. A solar panel is also utilized in this project, which absorbs the sun&#x0027;s rays and converts them to electricity. Simply said, they are utilized to produce power through the photovoltaic effect.",https://ieeexplore.ieee.org/document/9740974/,2022 International Conference on Computer Communication and Informatics (ICCCI),25-27 Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BDAI52447.2021.9515251,Design and Simulation of Tracked Mobile Robot Path Planning,IEEE,Conferences,"Aiming at the problem of autonomous navigation of robots in unknown environments, the robot operating system ROS is used as a development platform, and an autonomous navigation system is designed based on open source function packages such as Gmapping and Navigation, so that robots equipped with this system can learn map information in unknown environments. And based on the map to achieve positioning, path planning, obstacle avoidance and other functions. The 3D physical simulation software Gazebo simulates the environment and loads the designed URDF robot model to simulate and verify the autonomous navigation system. The results show that the autonomous navigation system can enable the robot to achieve accurate mapping, positioning, real-time obstacle avoidance and path planning in an unknown environment.",https://ieeexplore.ieee.org/document/9515251/,2021 IEEE 4th International Conference on Big Data and Artificial Intelligence (BDAI),2-4 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AINL-ISMW-FRUCT.2015.7382967,Design and implementation Raspberry Pi-based omni-wheel mobile robot,IEEE,Conferences,Nowadays simultaneous localization and mapping (SLAM) algorithms are being tested at least in two phases: software simulation and real hardware platform testing. This paper describes hardware design and control software for small size omni-directional wheels robot implemented for indoor testing SLAM algorithms.,https://ieeexplore.ieee.org/document/7382967/,"2015 Artificial Intelligence and Natural Language and Information Extraction, Social Media and Web Search FRUCT Conference (AINL-ISMW FRUCT)",9-14 Nov. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2015.7301549,Design and implementation for multiple-robot deployment in intelligent space,IEEE,Conferences,"This paper presents the problem of robot deployment for a number of scattered tasks. We aim to minimize the duration it takes for all robots to reach their assigned task locations. In previous work, we have proposed a team composed of one carrier robot (CR) and several servant robots to accomplish the mission. Then we have suggested an algorithm that determines a path of the CR for an efficient deployment under a few constraints, which is verified by simulations. Assuming that the servant robots are unmanned aerial vehicles (UAVs), the present paper extends the discussion to a real robot experiment. We design and implement a deployment system in intelligent space. The feasibility of the study is demonstrated through an experiment.",https://ieeexplore.ieee.org/document/7301549/,2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA),8-11 Sept. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC.2019.8785457,Design and implementation of a cooperative dual body vehicle,IEEE,Conferences,"In order to reduce the over-dependence of low-cost underwater vehicles on cables, and considering that it is difficult to transmit radio signals underwater, proposeding a design scheme of vehicle combining unmanned ship with underwater vehicles on the basis of the ROS platform (robot operating system).The work shows that under the condition of reasonable utilization of ROS compatibility, advantages of USV (unmanned surface vessel) and ROV (remote operating vehicle) can be complemented greatly. The USV shares part of the functions of the underwater working module, while providing auxiliary means for inertial navigation devices, it maintains better endurance and maneuverability. ROV which focuses on underwater operation has practical functions and low volume, thus saving production costs. This system has navigation performance, intelligent level, remote control ability and expansion ability, which can provide reference for the design of some low cost underwater vehicles in a way.",https://ieeexplore.ieee.org/document/8785457/,2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),24-26 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2009.5212155,Design and implementation of a visual servo system,IEEE,Conferences,"Robot visual servo system has an important status in robotic research and application, also has a decisive influence on robot intelligence. According to robotic kinematics and dynamics, the method of visual servo controller based on position is adopted in this paper. Binocular camera stereo vision technology and a kind of real-time control card Q8 based on MATLAB are used in motion control. The paper describes the control structure, hardware, software, designs a visual servo system platform and implements the real-time mission that track, capture or access moving targets on the platform.",https://ieeexplore.ieee.org/document/5212155/,2009 International Conference on Machine Learning and Cybernetics,12-15 July 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AEMCSE51986.2021.00262,Design of Motion Control System of Handling Robot,IEEE,Conferences,"Handling robots are very important for improving productivity, reducing work intensity, improving the surrounding environment, and ensuring work safety. Based on the kinematics analysis of a two-degree-of-freedom manipulator based on a slider mechanism, this paper uses an open motion controller as a development platform to carry out hardware design and software development to realize the flexible control of the handling robot, with simple structure and real-time control, open structure, standard programming and rich man-machine interface, etc. which can be widely used in material destacking and palletizing handling.",https://ieeexplore.ieee.org/document/9513197/,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",26-28 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIEA53260.2021.00013,Design of a Real-time Robot Control System oriented for Human-Robot Cooperation,IEEE,Conferences,"An open real-time control system based on the EtherCAT fieldbus communication technology is proposed to fulfill the high real-time requirement of the human-robot cooperation controller in this paper. An open source real-time kernel of Xenomai is employed as the real-time software platform of the robot control system. Based on this, four-layer interfaces architecture are accomplished, which are human-machine cooperation control layer, motion control layer, robot axis control layer and hardware abstraction layer, through the corresponding four real-time tasks to meet the demand of human-robot cooperation operations. In addition, the scheduling task is developed to manage the 4 real-time tasks. The dual buffer communication mechanisms and priority-based scheduling strategy between layers was exploited to synchronize these real-time tasks. The underlying hardware abstract interface and the human-robot collaborative control algorithm interface are opened in the control system as the quadric exploitation interfaces to meet the need of developing application tasks in real-time space. Experiment results which are conducted on a self-developed 6-DOF collaborative robot show that the proposed control system is effective in real-time control applications of human-robot cooperative control at the control cycle of 5 milliseconds.",https://ieeexplore.ieee.org/document/9525600/,2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),14-16 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDSCA53499.2021.9650125,Design of an Indoor Surveying and Mapping Robot Based on SLAM Technology,IEEE,Conferences,"Simultaneous localization and mapping (SLAM) are a key technology for indoor mapping and navigation of mobile robots. As traditional surveying and mapping work depends on the general simple tools or Artificial completed exist inaccuracies, insecurity and high repeatability errors. To solve the problems mentioned above, an experimental system of mobile robot mapping based on SLAM technology is designed and implemented. The mobile robot is equipped with lidar and odometer sensor. Through the cartographer algorithm[1], lidar data and odometer data are integrated to realize indoor map construction and navigation. The open-source robot operating system (ROS) based on Linux was used for realizing information communication, data processing, real-time display between the upper machine and lower machine. The system simulation and real scene test are carried out. The mobile robot can build reliable and efficient map. Compared with the traditional technology of surveying and mapping, simulation and real experiment results show that the system has high practicability and reliability.",https://ieeexplore.ieee.org/document/9650125/,2021 IEEE International Conference on Data Science and Computer Application (ICDSCA),29-31 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2016.7848000,Design of hardware circuit based on a neural network model for rapid detection of center of gravity position,IEEE,Conferences,"This paper proposes a rapid detection method for the center of gravity based on a neural network model. It is suitable for the rapid response requirement such as attitude control of a gait robot or real time torque control of a running car. The proposed method detects the center of gravity position on a straight line by using only the hardware circuit composing of common electronic devices instead of software, microprocessor and AD converter. The circuit employs some neural based comparators without the learning function to simplify the circuit structure. The detection circuit using some parallel processing neural comparators rapidly detects the center of gravity position on a straight line. In this paper, the circuit is designed and fabricated with electronic devices, and the circuit experiment shows the performance of the position detection.",https://ieeexplore.ieee.org/document/7848000/,2016 IEEE Region 10 Conference (TENCON),22-25 Nov. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CMCE.2010.5609659,Design of mobile robot system with remote control based on CAN-bus,IEEE,Conferences,"In order to realizing remote control and information collection quickly and reliably, the mobile robot with remote control is designed. In the paper, according to analysis of the overall structure, hardware circuit of the robot system is designed. Because the CAN2.0 standard only makes physical layer protocol and data link layer protocol, application layer protocol is ruled according to robot control system. In the last part of this paper, the software of master/slave computer is introduced in detail. The experiment shows that running performance of robot control system is balanced, efficient and has satisfied the practical demand.",https://ieeexplore.ieee.org/document/5609659/,"2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering",24-26 Aug. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1999.813052,Designing rhythmic motions using neural oscillators,IEEE,Conferences,"Neural oscillators offer simple and robust solutions to problems such as locomotion and dynamic manipulation. However, the parameters of these systems are notoriously difficult to tune. This paper presents an analysis technique which alleviates the difficulty of tuning. The method is based on describing function analysis, and can predict the steady state motion of the system, analyze stability, and be used to determine robustness to system changes. The method is illustrated using a number of design examples including an implementation of juggling on a real robot.",https://ieeexplore.ieee.org/document/813052/,Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289),17-21 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1998.726514,"Designing, making, and using a mobile robot",IEEE,Conferences,"Describes the building and control of a mobile robot which is capable of navigating in a well defined workspace by means of generating an optimal trajectory. The basic control architecture of the mobile robot is implemented with a combination of an MC68HC11 microcontroller and a personal computer. The kinematics of the proposed differential impulse is analyzed which allow us to select the appropriate steering DC motors and speed measurement requirements of the system. The motor control is performed by a PWM scheme and PI controllers. A path planning stage finds the optimal trajectory, taking a graphical description of the workspace and using potential fields and dynamic programming to solve the optimization problem and avoid the obstacles. Clearly there are two specific problems: building a complete device that will allow having an electric powered robot and the use of these resources to obtain controlled and collision-free movement in a real workspace.",https://ieeexplore.ieee.org/document/726514/,"SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)",14-14 Oct. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICoSTA48221.2020.1570615971,Detecting Features of Middle Size Soccer Field using Omnidirectional Camera for Robot Soccer ERSOW,IEEE,Conferences,"ERSOW (EEPIS Robot Soccer on Wheeled) is robot soccer developed by Politeknik Elektronika Negeri Surabaya that is designed and implemented on a Middle Size League division by following the rules of RoboCup, an international robot competition. One of the most famous division is a soccer robot, that is divided into two divisions: (1) SSL (Small Size League) and (2) MSL (Middle Size League). There are many research fields related to soccer robot which must be developed in robot ERSOW such as Artificial Intelligence (AI), Computer Vision, Embedded System, Mechanic Systems, and Hardware. This paper focuses on computer vision research for robot ERSOW, especially detecting features of the middle size soccer field, so that specific features of the field like X-junction, T-junction and L-junction can be detected to help robot positioning task where the result is represented into x and y in real-world coordinate. By knowing the position of the features, the robot position can be calculated. The localization system at robot ERSOW uses odometry, which has a large percentage of data errors. Therefore, we attempt to extract the feature of X-junction that is done to find its x and y coordinates and then the obtained coordinate can be used as a reference for correcting odometry data by AI.",https://ieeexplore.ieee.org/document/9079260/,2020 International Conference on Smart Technology and Applications (ICoSTA),20-20 Feb. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594335,Detection- Tracking for Efficient Person Analysis: The DetTA Pipeline,IEEE,Conferences,"In the past decade many robots were deployed in the wild, and people detection and tracking is an important component of such deployments. On top of that, one often needs to run modules which analyze persons and extract higher level attributes such as age and gender, or dynamic information like gaze and pose. The latter ones are especially necessary for building a reactive, social robot-person interaction. In this paper, we combine those components in a fully modular detection-tracking-analysis pipeline, called DetTA. We investigate the benefits of such an integration on the example of head and skeleton pose, by using the consistent track ID for a temporal filtering of the analysis modules' observations, showing a slight improvement in a challenging real-world scenario. We also study the potential of a so-called “free-flight” mode, where the analysis of a person attribute only relies on the filter's predictions for certain frames. Here, our study shows that this boosts the runtime dramatically, while the prediction quality remains stable. This insight is especially important for reducing power consumption and sharing precious (GPU-)memory when running many analysis components on a mobile platform, especially so in the era of expensive deep learning methods.",https://ieeexplore.ieee.org/document/8594335/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF49454.2021.9382646,Development and Testing of Garbage Detection for Autonomous Robots in Outdoor Environments,IEEE,Conferences,"In Japan, there is a growing concern about labor shortages due to the declining birthrate and aging population, and there are high expectations for robots to help solve such social problems and create industries. However, due to the prohibition of public road tests in Japan, there are few examples of actual applications of robots. Therefore, considerations and problems in the practical application of robots are still unclear. In this paper, by focusing on the implementation of garbage collection technology, we have developed an autonomous garbage collection robot using deep learning. In addition, we have verified the usefulness of our garbage detection technology in outdoor environments by conducting actual demonstrations at HANEDA INNOVATION CITY, which is a large-scale commercial and business complex belonged private property, Utsunomiya University, and Nakanoshima Challenge 2019, which is a field of demonstration experiment in the outdoor environment. Our garbage detector was designed to detect cans, plastic bottles, and lunch boxes automatically. Through experiments on test data and outdoor experiments in the real-world, we have confirmed that our detector has a 95.6% Precision and 96.8% Recall. Conparisons to other state-of-the-art detectors are also presented.",https://ieeexplore.ieee.org/document/9382646/,2021 IEEE/SICE International Symposium on System Integration (SII),11-14 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2004.1307521,Development and deployment of a line of sight virtual sensor for heterogeneous teams,IEEE,Conferences,"For a team of cooperating robots, geometry plays a vital role in operation. Knowledge of line of sight to local obstacles and adjacent teammates is critical in both the movement and planning stages to avoid collisions, maintain formation and localize the team. However, determining if other robots are within the line of sight of one another is difficult with existing sensor platforms - especially as the scale of the robot is reduced. We describe a method of exploiting collective team information to generate a virtual sensor that provides line of sight determination, greater range and resolution and the ability to generalize local sensing. We develop this sensor and apply it to the control of a tightly coupled, resource-limited robot team called Millibots.",https://ieeexplore.ieee.org/document/1307521/,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",26 April-1 May 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON48115.2021.9589075,Development of Agricultural Robot Platform with Virtual Laboratory Capabilities,IEEE,Conferences,"Agricultural robots are called to help in many tasks in emerging clean and sustainable agriculture. These complex electro-mechanical systems can actually integrate artificial intelligence (AI), the Internet of Things (IoT), sensors, actuators, and advanced control methods to accomplish functions in autonomous or in collaborative ways. Before the deployment of such techniques in the field, it is convenient to carry out laboratory validations. These last could be at the sub-system, e.g., sensors or servos operation, or the whole system level. This paper proposes the development of the hardware and software parts of a platform of agricultural robot. The proposed system, highly motivated by the restrictions imposed by COVID-19 context, enables laboratory tests virtualization while keeping real-time functionalities",https://ieeexplore.ieee.org/document/9589075/,IECON 2021 – 47th Annual Conference of the IEEE Industrial Electronics Society,13-16 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMS.2017.12,Development of Components of Multi-agent CASE-System for Describing the Logic of Behavior of Mobile Robots,IEEE,Conferences,"In the article there are substantiation of architectural and technical solutions, with the basis of the universal CASE-tool for describing (""programming"") the behavior of mobile robots. The development tool intended for carrying out experiments in the field of artificial intelligence and it is based on multi-agent technology. In addition, the toolkit will be the maximum possible reuse of elements (tasks, processes, etc.). The basis for the development is the idea of combining, within the framework of one tool, both the real execution of the algorithm by the robot, and its simulation. It allows talking about testing partially implemented hardware (sensors and actuators). Development is carried out based on open source technology; all texts of programs are available at web source: https://github.com/unclesal/tenguai.",https://ieeexplore.ieee.org/document/8356782/,2017 European Modelling Symposium (EMS),20-21 Nov. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIS.2018.8466473,Development of a GPU-Based Human Emotion Recognition Robot Eye for Service Robot by Using Convolutional Neural Network,IEEE,Conferences,"Service robots can be used widely to assist elderly and disable population due to the lack of caregivers in future. Real-time human tracking, detection, focusing and implementing various algorithms are a wide range of application in emotion recognition service robots. Therefore service robots must have a properly designed robot eye model to be human-friendly with accurate human-robot interaction. Developed robot eye can be recognized the human emotional states by using well trained deep convolutional neural networks (ConvNet). This paper describes graphics processing units (GPUs) based human emotion recognition robot eye by using ConvNet. Mainly, the robot eye performs two processes in the intelligent systems. They are the robot eye focus to the human face and head by using pre-trained haar cascade classifier and recognizes the human emotional states probability with percentages as happy, sad or relaxes by using pre-trained ConvNet. The developed robot eye was implemented and tested by using different people successfully and the results of them are presented. According to the results, the emotions are detected more than 85% of overall accuracy for each person.",https://ieeexplore.ieee.org/document/8466473/,2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS),6-8 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FarEastCon.2018.8602651,Development of a Transport Robot for Automated Warehouses,IEEE,Conferences,"Industrial robots and manipulators are widely used as transport-loading devices in automated production. It is possible to combine equipment into coordinated production complexes of various sizes with the help of robots and they will not be bound by rigid planning and the number of installed units. Transport robots have proven themselves as flexible automated means of realizing intra-shop and interoperation material connections. More and more companies are developing technologies for vehicles through which they can communicate with each other and use real-time data from production infrastructure facilities. Electric vehicles and unmanned vehicles have become a new technological trend. In this regard, the paper deals with a prototype of an innovative transport robot, created for automated warehouses. It is proposed to use a computer vision system with image recognition based on the embedded software for the transport robot positioning inside the production facilities. The algorithm of deep machine learning was adapted to solve this problem. Using this algorithm, the prototype tests were performed successfully.",https://ieeexplore.ieee.org/document/8602651/,2018 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon),3-4 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSSE.2011.5961891,Development of simulator for kid-sized humanoid soccer in RoboCup,IEEE,Conferences,"The robot soccer game system is a challenge for real-time control, which can be moderately abstracted from the standpoint of AI (Artificial Intelligence) and multi-agent systems. This simulator is developed for RoboCup Soccer Humanoid League. A humanoid robot belongs to highly intelligent system. The intelligent technologies of the humanoid robots include mechanism design, vision system, and algorithms in software programming. Therefore, its competitions can encourage creativity and technical development. To facilitate the strategy testing of these humanoid robot soccer competitions, a strategy simulator for kid-sized humanoid soccer in RoboCup is proposed. In this simulator, strategies compiled to DLL files may be explicitly loaded at run-time.",https://ieeexplore.ieee.org/document/5961891/,Proceedings 2011 International Conference on System Science and Engineering,8-10 June 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2018.8761037,Developmental Bayesian Optimization of Black-Box with Visual Similarity-Based Transfer Learning,IEEE,Conferences,"We present a developmental framework based on a long-term memory and reasoning mechanisms (Vision Similarity and Bayesian Optimisation). This architecture allows a robot to optimize autonomously hyper-parameters that need to be tuned from any action and/or vision module, treated as a black-box. The learning can take advantage of past experiences (stored in the episodic and procedural memories) in order to warm-start the exploration using a set of hyper-parameters previously optimized from objects similar to the new unknown one (stored in a semantic memory). As example, the system has been used to optimized 9 continuous hyper-parameters of a professional software (Kamido) both in simulation and with a real robot (industrial robotic arm Fanuc) with a total of 13 different objects. The robot is able to find a good object-specific optimization in 68 (simulation) or 40 (real) trials. In simulation, we demonstrate the benefit of the transfer learning based on visual similarity, as opposed to an amnesic learning (i.e. learning from scratch all the time). Moreover, with the real robot, we show that the method consistently outperforms the manual optimization from an expert with less than 2 hours of training time to achieve more than 88% of success.",https://ieeexplore.ieee.org/document/8761037/,2018 Joint IEEE 8th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob),17-20 Sept. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9562061,Dexterous Manoeuvre through Touch in a Cluttered Scene,IEEE,Conferences,"Manipulation in a densely cluttered environment creates complex challenges in perception to close the control loop, many of which are due to the sophisticated physical interaction between the environment and the manipulator. Drawing from biological sensory-motor control, to handle the task in such a scenario, tactile sensing can be used to provide an additional dimension of the rich contact information from the interaction for decision making and action selection to manoeuvre towards a target. In this paper, a new tactile-based motion planning and control framework based on bioinspiration is proposed and developed for a robot manipulator to manoeuvre in a cluttered environment. An iterative two-stage machine learning approach is used in this framework: an autoencoder is used to extract important cues from tactile sensory readings while a reinforcement learning technique is used to generate optimal motion sequence to efficiently reach the given target. The framework is implemented on a KUKA LBR iiwa robot mounted with a SynTouch BioTac tactile sensor and tested with real-life experiments. The results show that the system is able to move the end-effector through the cluttered environment to reach the target effectively.",https://ieeexplore.ieee.org/document/9562061/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CT54291.2022.9825379,Digitization of Chess Board and Prediction of Next Move,IEEE,Conferences,"In recent years digital technologies and gadgets have grown so much. Along with other things, the field of Artificial Intelligence has also grown dramatically. Computer vision is one of the most compelling types of AI, which everyone must have experienced in one or the other way like in OCR, Image Recognition, object detection, google lens, etc. One might even come to the assumption that given any image or video to a computer, the computer can understand what&#x2019;s going on in it and can comment a few things on it, just like WE HUMANS. But that assumption is not yet very true. It may be possible in the future but at the present these need some work to become reality. Leveraging this same motivation, we intend to give some contribution for our dream future to become reality. Recognizing and understanding any arbitrary chess board from the image, thereby digitizing it, is the thing that we will be achieving from the proposed system. Along with all that, giving a comment on the current state of chess board, thereby predicting the next optimal move in the game is another thing which is included in proposed system. Think of a human-like robot trying to play chess. Robots must have something like this in order to play the game. In the proposed research work digitization of Chess Board is done by server application from chess board image acquired by user. Current state of the board is generated and sent back as a response to client application. With the help of GUI, the chessboard is generated from the received Forsyth-Edwards Notation (FEN) on the client side. Depending upon blacks turn or whites turn next optimal move to be played is shown to the user. With our proposed system digital instance is generated with accuracy of over 90%. All blank cells of the board are generated with almost 100% accuracy.",https://ieeexplore.ieee.org/document/9825379/,2022 IEEE 7th International conference for Convergence in Technology (I2CT),7-9 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCTA41146.2020.9206279,Direct Force Feedback using Gaussian Process based Model Predictive Control,IEEE,Conferences,"Many robotic applications require control of the applied forces or moments. Model predictive control allows for the direct or indirect control of forces, while taking constraints into account. However, challenges arise when the robot environment that affects the force is highly variable, uncertain and difficult to model. Learning supported model predictive control makes it possible to combine the advantages of optimal control, such as the explicit consideration of constraints, with the advantages of machine learning, such as adaptive data-based modeling. In this paper Gaussian processes are used to model the contact forces that are applied in model predictive force control. The Gaussian process learns the static output mapping describing the interaction of the robot with the environment. It is shown that stability guarantees can be derived in a similar way as in classical predictive control. A proof-of-concept experimental implementation of a direct hybrid position force controller for a lightweight robot shows real-time feasibility.",https://ieeexplore.ieee.org/document/9206279/,2020 IEEE Conference on Control Technology and Applications (CCTA),24-26 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Agro-Geoinformatics.2017.8047016,Disease detection on the leaves of the tomato plants by using deep learning,IEEE,Conferences,"The aim of this work is to detect diseases that occur on plants in tomato fields or in their greenhouses. For this purpose, deep learning was used to detect the various diseases on the leaves of tomato plants. In the study, it was aimed that the deep learning algorithm should be run in real time on the robot. So the robot will be able to detect the diseases of the plants while wandering manually or autonomously on the field or in the greenhouse. Likewise, diseases can also be detected from close-up photographs taken from plants by sensors built in fabricated greenhouses. The examined diseases in this study cause physical changes in the leaves of the tomato plant. These changes on the leaves can be seen with RGB cameras. In the previous studies, standard feature extraction methods on plant leaf images to detect diseases have been used. In this study, deep learning methods were used to detect diseases. Deep learning architecture selection was the key issue for the implementation. So that, two different deep learning network architectures were tested first AlexNet and then SqueezeNet. For both of these deep learning networks training and validation were done on the Nvidia Jetson TX1. Tomato leaf images from the PlantVillage dataset has been used for the training. Ten different classes including healthy images are used. Trained networks are also tested on the images from the internet.",https://ieeexplore.ieee.org/document/8047016/,2017 6th International Conference on Agro-Geoinformatics,7-10 Aug. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR53236.2021.9659389,Distributed Active Learning for Semantic Segmentation on Walking Robots,IEEE,Conferences,"Quickly adapting to new and unknown environments is a vital functionality for autonomous robots. To increase their capabilities, they need a sophisticated self- and environmental awareness. Understanding the surroundings can be aided by a pixelwise semantic segmentation of the images taken by the robot. However, to achieve viable results a large database of annotated images is needed in advance. To aid in a quick understanding of the surroundings a distributed active learning approach for semantic segmentation is presented. The robot evaluates and selects images where it thinks annotation would lead to a better overall segmentation results. Since the robot is potentially on an autonomous remote mission, the communication might be limited. Therefore, the robot selects images which are stored in a buffer before they are transmitted in batches to the base station. The annotation of the images and training of a new model for the segmentation can be performed asynchronously to the robot's mission. The model of the robot is then updated once ready. Four different stream-based query metrics are deployed and tested on a dataset and on ANYmal in a real scenario. The proposed approach allows a robot to be deployed in an unknown surrounding and through quick and continuous learning it gains an understanding of the area.",https://ieeexplore.ieee.org/document/9659389/,2021 20th International Conference on Advanced Robotics (ICAR),6-10 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2015.7353785,Distributed Particle Swarm Optimization - particle allocation and neighborhood topologies for the learning of cooperative robotic behaviors,IEEE,Conferences,"In this article we address the automatic synthesis of controllers for the coordinated movement of multiple mobile robots, as a canonical example of cooperative robotic behavior. We use five distributed noise-resistant variations of Particle Swarm Optimization (PSO) to learn in simulation a set of 50 weights of an artificial neural network. They differ on the way the particles are allocated and evaluated on the robots, and on how the PSO neighborhood is implemented. In addition, we use a centralized approach that allows for benchmarking with the distributed versions. Regardless of the learning approach, each robot measures locally and individually the performance of the group using exclusively on-board resources. Results show that four of the distributed variations obtain similar fitnesses as the centralized version, and are always able to learn. The other distributed variation fails to properly learn on some of the runs, and results in lower fitness when it succeeds. We test systematically the controllers learned in simulation in real robot experiments.",https://ieeexplore.ieee.org/document/7353785/,2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Sept.-2 Oct. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2012.1127,Distributed parallel processing of mobile robot PF-SLAM,IET,Conferences,"Real-time property is a fundamental requirement for a practical robot system. For this purpose, this article proposes an implementation architecture of robot SLAM by adopting two parallel threads processing. Since the dominant factor which determines the computational complexity is the employed particle number, two distributed threads with different particle set size are executed simultaneously. Conventional PF-SLAM algorithm occupies one of threads, and the other thread which hires more particles is activated whenever robot has significant motion changes. Advantages of this presented idea are validated by experiment carried on Pioneer robot.",https://ieeexplore.ieee.org/document/6492734/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAR.2017.7942676,Door and cabinet recognition using Convolutional Neural Nets and real-time method fusion for handle detection and grasping,IEEE,Conferences,"In this paper we present a new method that robustly identifies doors, cabinets and their respective handles, with special emphasis on extracting useful features from handles to be then manipulated. The novelty of this system relies on the combination of a Convolutional Neural Net (CNN), as a form of reducing the search space, several methods to extract point cloud data and a mobile robot to interact with the objects. The framework consists of the following components: The implementation of a CNN to extract a Region of Interest (ROI) from an image corresponding to a door or cabinet. Several vision based techniques to detect handles inside the ROI and its 3D positioning. A complementary plane segmentation method to differentiate door/cabinet from the handle. An algorithm to fuse both approaches robustly and extract essential information from the handle for robotic grasping (i.e. handle point cloud, door plane model, grasping locations, turning orientation, orthogonal vector to door). A mobile robot for grasping the handle. The system assumes no prior knowledge of the environment.",https://ieeexplore.ieee.org/document/7942676/,"2017 3rd International Conference on Control, Automation and Robotics (ICCAR)",24-26 April 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RoboSoft51838.2021.9479353,DroneTrap: Drone Catching in Midair by Soft Robotic Hand with Color-Based Force Detection and Hand Gesture Recognition,IEEE,Conferences,"The paper proposes a novel concept of docking drones to make this process as safe and fast as possible. The idea behind the project is that a robot with a soft gripper grasps the drone in midair. The human operator navigates the robotic arm with the ML-based gesture recognition interface. The 3-finger robot hand with soft fingers is equipped with touch sensors, making it possible to achieve safe drone catching and avoid inadvertent damage to the drone's propellers and motors. Additionally, the soft hand is featured with a unique color-based force estimation technology based on a computer vision (CV) system. Moreover, the visual color-changing system makes it easier for the human operator to interpret the applied forces.Without any additional programming, the operator has full real-time control of robot's motion and task execution by wearing a mocap glove with gesture recognition, which was developed and applied for the high-level control of DroneTrap.The experimental results revealed that the developed color-based force estimation can be applied for rigid object capturing with high precision (95.3%). The proposed technology can potentially revolutionize the landing and deployment of drones for parcel delivery on uneven ground, structure maintenance and inspection, risque operations, and etc.",https://ieeexplore.ieee.org/document/9479353/,2021 IEEE 4th International Conference on Soft Robotics (RoboSoft),12-16 April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2018.8630497,Dynamic Gesture Recognition Algorithm based on ROI and CNN for Social Robots,IEEE,Conferences,"To eliminate barriers to communication between social robots and disabled people in listening and/or speaking, this work proposes a gesture recognition method that is based on the region of interest (ROI) and a convolutional neural network (CNN) for social robots. This method can track and recognize gestures in real time in a complex background. ROI and OpenCV are first used to obtain the dynamic gesture, which is then treated as the input of a CNN model to output a gesture feature model. Furthermore, a gesture-controlled social robot is implemented by the obtained feature model. Finally, the performance of the system is verified, and experimental results show that the proposed technology can track and recognize user gestures in real time.",https://ieeexplore.ieee.org/document/8630497/,2018 13th World Congress on Intelligent Control and Automation (WCICA),4-8 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR46387.2019.8981552,Dynamic Movement Primitives: Volumetric Obstacle Avoidance,IEEE,Conferences,"Dynamic Movement Primitives (DMPs) are a framework for learning a trajectory from a demonstration. The trajectory can be learned efficiently after only one demonstration, and it is immediate to adapt it to new goal positions and time duration. Moreover, the trajectory is also robust against perturbations. However, obstacle avoidance for DMPs is still an open problem. In this work, we propose an extension of DMPs to support volumetric obstacle avoidance based on the use of superquadric potentials. We show the advantages of this approach when obstacles have known shape, and we extend it to unknown objects using minimal enclosing ellipsoids. A simulation and experiments with a real robot validate the framework, and we make freely available our implementation.",https://ieeexplore.ieee.org/document/8981552/,2019 19th International Conference on Advanced Robotics (ICAR),2-6 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2008.4640818,Dynamic field theory of sequential action: A model and its implementation on an embodied agent,IEEE,Conferences,"How sequences of actions are learned, remembered, and generated is a core problem of cognition. Despite considerable theoretical work on serial order, it typically remains unexamined how physical agents may direct sequential actions at the environment within which they are embedded. Situated physical agents face a key problem - the need to accommodate variable amounts of time it takes to terminate each individual action within the sequence. Here we examine how Dynamic Field Theory (DFT), a neuronally grounded dynamical systems approach to embodied cognition, may address sequence learning and sequence generation. To demonstrate that the proposed DFT solution works with real and potentially noisy sensory systems as well as with real physical action systems, we implement the approach on a simple autonomous robot. We demonstrate how the robot acquires sequences from experiencing the associated sensory information and how the robot generates sequences based on visual information from its environment using low-level visual features.",https://ieeexplore.ieee.org/document/4640818/,2008 7th IEEE International Conference on Development and Learning,9-12 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1995.537949,Dynamic path planning,IEEE,Conferences,"Path planning is dynamic when the path is continually recomputed as more information becomes available. A computational framework for dynamic path planning is proposed which has the ability to provide navigational directions during the computation of the plan. Path planning is performed using a potential field approach. We use a specific type of potential function-a harmonic function-which has no local minima. The implementation is parallel and consists of a collection of communicating processes, across a network of SPARC & SGI workstations using a message passing software package called PVM. The computation of the plan is performed independently of the execution of the plan. A hierarchical coarse-to-fine procedure is used to guarantee a correct control strategy at the expense of accuracy. We have successfully navigated a Nomad robot around our lab space with no a priori map in real-time. The result of the described approach is a parallel implementation which permits dynamic path planning using available processor resources.",https://ieeexplore.ieee.org/document/537949/,"1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent Systems for the 21st Century",22-25 Oct. 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341406,EU Long-term Dataset with Multiple Sensors for Autonomous Driving,IEEE,Conferences,"The field of autonomous driving has grown tremendously over the past few years, along with the rapid progress in sensor technology. One of the major purposes of using sensors is to provide environment perception for vehicle understanding, learning and reasoning, and ultimately interacting with the environment. In this paper, we first introduce a multisensor platform allowing vehicle to perceive its surroundings and locate itself in a more efficient and accurate way. The platform integrates eleven heterogeneous sensors including various cameras and lidars, a radar, an IMU (Inertial Measurement Unit), and a GPS-RTK (Global Positioning System / Real-Time Kinematic), while exploits a ROS (Robot Operating System) based software to process the sensory data. Then, we present a new dataset (https://epan-utbm.github.io/utbm_robocar_dataset/) for autonomous driving captured many new research challenges (e.g. highly dynamic environment), and especially for long-term autonomy (e.g. creating and maintaining maps), collected with our instrumented vehicle, publicly available to the community.",https://ieeexplore.ieee.org/document/9341406/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2008.97,Early-Life Cycle Reuse Approach for Component-Based Software of Autonomous Mobile Robot System,IEEE,Conferences,"Applying software reuse to many embedded realtime systems, such as autonomous mobile robot system poses significant challenges to industrial software processes due to the resource-constrained and realtime requirements of the systems. An approach for early life-cycle systematic reuse for component-based software engineering (ELCRA) of autonomous mobile robot software is developed. The approach allows reuse at the early stage of software development process by integrating analysis patterns, component model, and component-oriented programming framework. The results of applying the approach in developing software for real robots show that the strategies and processes proposed in the approach can fulfill requirements for self-contained, platform-independent and real-time predictable mobile robot.",https://ieeexplore.ieee.org/document/4617381/,"2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing",6-8 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCloud-EdgeCom49738.2020.00050,Edge Computing-based 3D Pose Estimation and Calibration for Robot Arms,IEEE,Conferences,"Industrial robots are widely used in current production lines, and complex pipeline processes, especially those with different assembly requirements, are designed for intelligent manufacturing in the era of industry 4.0. During the new crown epidemic, a large number of car companies used the production line to transform production of medical materials such as masks and protective clothing, which provided a strong guarantee for fighting the epidemic. In this scenario, a pipeline is often assembled from robotic arms from multiple suppliers. The traditional methods is complex and takes a lot of time. In this paper, we propose a novel deep learning based robot arm 3D pose estimation and calibration model with simple Kinect stereo cameras which can be deployed on light-weight edge computing systems. The light-weight deep CNN model can detection 5 predefined key points based on RGB-D data. In this way, when the assembly line composed of different robot arms needs to be reassembled, our model can quickly provide the robot’s pose information without additional tuning processes. Testing in Webots with Rokae xb4 robot arm model shows that our model can quickly estimate the key point of the robot arm.",https://ieeexplore.ieee.org/document/9170983/,2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom),1-3 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793748,Effect of Mechanical Resistance on Cognitive Conflict in Physical Human-Robot Collaboration,IEEE,Conferences,"Physical Human-Robot Collaboration (pHRC) is about the interaction between one or more human operator(s) and one or more robot(s) in direct contact and voluntarily exchanging forces to accomplish a common task. In any pHRC, the intuitiveness of the interaction has always been a priority, so that the operator can comfortably and safely interact with the robot. So far, the intuitiveness has always been described in a qualitative way. In this paper, we suggest an objective way to evaluate intuitiveness, known as prediction error negativity (PEN) using electroencephalogram (EEG). PEN is defined as a negative deflection in event related potential (ERP) due to cognitive conflict, as a consequence of a mismatch between perception and reality. Experimental results showed that the forces exchanged between robot and human during pHRC modulate the amplitude of PEN, representing different levels of cognitive conflict. We also found that PEN amplitude significantly decreases (p <; 0.05) when a mechanical resistance is being applied smoothly and more time in advance before an invisible obstacle, when compared to a scenario in which the resistance is applied abruptly before the obstacle. These results indicate that an earlier and smoother resistance reduces the conflict level. Consequently, this suggests that smoother changes in resistance make the interaction more intuitive.",https://ieeexplore.ieee.org/document/8793748/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2011.6005223,Effect of human guidance and state space size on Interactive Reinforcement Learning,IEEE,Conferences,"The Interactive Reinforcement Learning algorithm enables a human user to train a robot by providing rewards in response to past actions and anticipatory guidance to guide the selection of future actions. Past work with software agents has shown that incorporating user guidance into the policy learning process through Interactive Reinforcement Learning significantly improves the policy learning time by reducing the number of states the agent explores. We present the first study of Interactive Reinforcement Learning in real-world robotic systems. We report on four experiments that study the effects that teacher guidance and state space size have on policy learning performance. We discuss modifications made to apply Interactive Reinforcement Learning to a real-world system and show that guidance significantly reduces the learning rate, and that its positive effects increase with state space size.",https://ieeexplore.ieee.org/document/6005223/,2011 RO-MAN,31 July-3 Aug. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197211,Efficient Pig Counting in Crowds with Keypoints Tracking and Spatial-aware Temporal Response Filtering,IEEE,Conferences,"Pig counting is a crucial task for large-scale pig farming. Pigs are usually visually counted by human. But this process is very time-consuming and error-prone. Few studies in literature developed automated pig counting method. The existing works only focused on pig counting using single image, and its level of accuracy faced challenges due to pig movements, occlusion and overlapping. Especially, the field of view of a single image is very limited, and could not meet the needs of pig counting for large pig grouping houses. Towards addressing these challenges, we presented a real-time automated pig counting system in crowds using only one monocular fisheye camera with an inspection robot. Our system showed that it achieved performance superior to human. Our pipeline began with a novel bottom-up pig detection algorithm to avoid false negatives due to overlapping, occlusion and deformable pig shapes. This detection included a deep convolution neural network (CNN) for pig body part keypoints detection and the keypoints association method to identify individual pigs. It then employed an efficient on-line tracking method to associate pigs across image frames. Finally, pig counts were estimated by a novel spatial-aware temporal response filtering (STRF) method to suppress false positives caused by pig or camera movements or tracking failures. The whole pipeline has been deployed in an edge computing device, and demonstrated the effectiveness.",https://ieeexplore.ieee.org/document/9197211/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR.2018.8621810,Efficient and Low-Cost Deep-Learning Based Gaze Estimator for Surgical Robot Control,IEEE,Conferences,"Surgical robots are playing more and more important role in modern operating room. However, operations by using surgical robot are not easy to handle by doctors. Vision based human-computer interaction (HCI) is a way to ease the difficulty to control surgical robots. While the problem of this method is that eyes tracking devices are expensive. In this paper, a low cost and robust deep-learning based on gaze estimator is proposed to control surgical robots. By this method, doctors can easily control the robot by specifying the starting point and ending point of the surgical robot using eye gazing. Surgical robots can also be controlled to move in 9 directions using controllers' eyes gazing information. A Densely Connected convolutional Neural Networks (Dense CNN) model for 9-direction/36-direction gaze estimation is built. The Dense CNN architecture has much more less trainable parameters compared to traditional CNN network architecture (AlexNet like/VGG like) which is more feasible to deploy on the Field-Programmable Gate Array (FPGA) and other hardware with limited memories.",https://ieeexplore.ieee.org/document/8621810/,2018 IEEE International Conference on Real-time Computing and Robotics (RCAR),1-5 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANS.2014.7003085,Efficient multi-AUV cooperation using semantic knowledge representation for underwater archaeology missions,IEEE,Conferences,"Advances in the fields of communication technology and software, electrical and mechanical engineering enable the replacement of a single robot by cooperative robotic team in highly demanding applications, such as search and rescue. A robotic team could perform better than a single robot, if certain challenges, such as action planning, coordination, and decision making, are successfully tackled. One key factor for the successful performance of a robotic team is the multi-robot task allocation. Specifically, the challenge is to define which robot executes which task, considering an efficient solution for the successful completion of the complex mission. The task allocation could be even more challenging when real-world communication constraints and uncertainties are presented, such as limited bandwidth, high latency and high packet loss. In the current study, we attempt to resolve the issue of a cooperative robotic team under communication constraints. To reach this goal, the use of a distributed world model for multi-robot task allocation is proposed. This ontology based distributed world model is capable of successfully handling to a great extent the aforementioned communications limitations, thus allowing successful mission execution even under harsh communication conditions. An efficient centralised task allocation mechanism, using k-means clustering, is described, and its performance is compared to a greedy centralised task allocation method. Experimental simulation results indicate that the efficient method performs better on average than the greedy one, without extra time requirements.",https://ieeexplore.ieee.org/document/7003085/,2014 Oceans - St. John's,14-19 Sept. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2009.5326159,Efficient parsing of spoken inputs for human-robot interaction,IEEE,Conferences,"The use of deep parsers in spoken dialogue systems is usually subject to strong performance requirements. This is particularly the case in human-robot interaction, where the computing resources are limited and must be shared by many components in parallel. A real-time dialogue system must be capable of responding quickly to any given utterance, even in the presence of noisy, ambiguous or distorted input. The parser must therefore ensure that the number of analyses remains bounded at every processing step. The paper presents a practical approach to addressing this issue in the context of deep parsers designed for spoken dialogue. The approach is based on a word lattice parser combined with a statistical model for parse selection. Each word lattice is parsed incrementally, word by word, and a discriminative model is applied at each incremental step to prune the set of resulting partial analyses. The model incorporates a wide range of linguistic and contextual features and can be trained with a simple perceptron. The approach is fully implemented as part of a spoken dialogue system for human-robot interaction. Evaluation results on a Wizard-of-Oz test suite demonstrate significant improvements in parsing time.",https://ieeexplore.ieee.org/document/5326159/,RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication,27 Sept.-2 Oct. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICE2T.2017.8215992,Elevator button and floor number recognition through hybrid image classification approach for navigation of service robot in buildings,IEEE,Conferences,"To successfully move a robot into the building, the elevator button and elevator floor number detection and recognition can play an important role. It can help a robot move in the building, just as it also can help a visually impaired person who wants to move another floor in the building. Due to vision-based approach, the difference in lighting condition and the complex background are the main obstacles in this research. A hybrid image classification model is presented in this research to overcome all these difficulties. This hybrid model is the combination of histogram of oriented gradients and bag of words models, which later reduces the dimension of image features by using the feature selection algorithm. An artificial neural network has been implemented to get the experimental result by training and testing. In order to get training performance, 1000 training image samples have been used and additional 1000 image samples also been used to get the testing performance. The experimental results of this research indicate that this proposed framework is important for real-time implementation to implement the elevator button and elevator floor number recognition framework.",https://ieeexplore.ieee.org/document/8215992/,2017 International Conference on Engineering Technology and Technopreneurship (ICE2T),18-20 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IHMSC.2013.225,Embedded Motion Controller Design Based on RTEX Network,IEEE,Conferences,"Adopting embedded framework and network communication mode, a kind of multi-axis embedded motion controller hardware platform design for the Panasonic A5N drive is proposed, which is Based on the modular control core (ARM + FPGA) and can adapt to the new real-time network RTEX. The processes of controller's functional design, hardware design and software design are explained in detail. Up to now, the motion controller's hardware platform have been completed and verified by communication experiments, position and velocity control experiments, and the results of which show that the controller have good scalability, reliability, flexibility and openness and could well meet the needs of the multi-axis robot's motion control.",https://ieeexplore.ieee.org/document/6642753/,2013 5th International Conference on Intelligent Human-Machine Systems and Cybernetics,26-27 Aug. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEV.2018.8641023,Embedded System based Bangla Intelligent Social Virtual Robot with Sentiment Analysis,IEEE,Conferences,"Bangla is the mother tongue of millions of people all over the world. Despite being a very popular language, any social virtual robot that can intelligently communicate in Bangla is a fairytale till now. One of the main reason of this is lack of rich text corpus and previous research on Bangla language. The proposed Bangla Intelligent Social Virtual Robot can communicate in Bangla intelligently and can express its reflective emotion virtually with the help of machine learning algorithms and sentiment analysis. In this paper, we discuss the approached system, design methodology and implementation details of first ever Bangla virtual embedded robot followed by the methodology of building a rich Bangla text corpus. The proposed embedded virtual robot turns out better performer when compared with only known Bangla intelligent chatbot named ‘Golpo’ and the embedded system performance efficiency has been upgraded with the help CPU over-clocking technique.",https://ieeexplore.ieee.org/document/8641023/,"2018 Joint 7th International Conference on Informatics, Electronics & Vision (ICIEV) and 2018 2nd International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",25-29 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR47638.2019.9043931,Embedded UAV Real-Time Visual Object Detection and Tracking,IEEE,Conferences,"The use of camera-equipped Unmanned Aerial Vehicles (UAVs, or “drones”) for a wide range of aerial video capturing applications, including media production, surveillance, search and rescue operations, etc., has exploded in recent years. Technological progress has led to commercially available UAVs with a degree of cognitive autonomy and perceptual capabilities, such as automated, on-line detection and tracking of target objects upon the captured footage. However, the limited computational hardware, the possibly high camera-to-target distance and the fact that both the UAV/camera and the target(s) are moving, makes it challenging to achieve both high accuracy and stable real-time performance. In this paper, the current state-of-the-art on real-time object detection/tracking is overviewed. Additionally, a relevant, modular implementation suitable for on-drone execution (running on top of the popular Robot Operating System) is presented and empirically evaluated on a number of relevant datasets. The results indicate that a sophisticated, neural network-based detection and tracking system can be deployed at real-time even on embedded devices.",https://ieeexplore.ieee.org/document/9043931/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEAMS51251.2021.00015,Enhancing Human-in-the-Loop Adaptive Systems through Digital Twins and VR Interfaces,IEEE,Conferences,"Self-adaptation approaches usually rely on closed-loop controllers that avoid human intervention from adaptation. While such fully automated approaches have proven successful in many application domains, there are situations where human involvement in the adaptation process is beneficial or even necessary. For such “human-in-the-loop” adaptive systems, two major challenges, namely transparency and controllability, have to be addressed to include the human in the self-adaptation loop. Transparency means that relevant context information about the adaptive systems and its context is represented based on a digital twin enabling the human an immersive and realistic view. Concerning controllability, the decision-making and adaptation operations should be managed in a natural and interactive way. As existing human-in-the-loop adaptation approaches do not fully cover these aspects, we investigate alternative human-in-the-loop strategies by using a combination of digital twins and virtual reality (VR) interfaces. Based on the concept of the digital twin, we represent a self-adaptive system and its respective context in a virtual environment. With the help of a VR interface, we support an immersive and realistic human involvement in the self-adaptation loop by mirroring the physical entities of the real world to the VR interface. For integrating the human in the decision-making and adaptation process, we have implemented and analyzed two different human-in-the-loop strategies in VR: a procedural control where the human can control the decision making-process and adaptations through VR interactions (human-controlled) and a declarative control where the human specifies the goal state and the configuration is delegated to an AI planner (mixed-initiative). We illustrate and evaluate our approach based on an autonomic robot system that is accessible and controlled through a VR interface.",https://ieeexplore.ieee.org/document/9462035/,2021 International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),18-24 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9562114,Enhancing Robot Perception in Grasping and Dexterous Manipulation through Crowdsourcing and Gamification,IEEE,Conferences,"Robot grasping and manipulation planning in unstructured and dynamic environments is heavily dependent on the attributes of manipulated objects. Although deep learning approaches have delivered exceptional performance in robot perception, human perception and reasoning are still superior in processing novel object classes. Moreover, training such models requires large datasets that are generally expensive to obtain. This work combines crowdsourcing and gamification to leverage human intelligence, enhancing the object recognition and attribute estimation aspects of robot perception. The framework employs an attribute matching system that encodes visual information into an online puzzle game, utilizing the collective intelligence of players to expand an initial attribute database and react to real-time perception conflicts. The framework is deployed and evaluated in a proof-of-concept application for enhancing object recognition in autonomous robot grasping and a model for estimating the response time is proposed. The obtained results demonstrate that given enough players, the framework can offer near real-time labeling of novel objects, based purely on visual information and human experience.",https://ieeexplore.ieee.org/document/9562114/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2001.971506,Enhancing control architectures using CORBA,IEEE,Conferences,"The nature of applied research in intelligent robot controllers makes having a versatile software architecture a real need for exploring alternative designs in robotic minds construction. The paper presents an experiment in the adaptation of a multi-robot cooperation architecture to a CORBA-based schema. The paper demonstrates not only the feasibility but the convenience of using, state-of-the-art, modular software technologies for the construction of advanced intelligent controllers.",https://ieeexplore.ieee.org/document/971506/,Proceeding of the 2001 IEEE International Symposium on Intelligent Control (ISIC '01) (Cat. No.01CH37206),5-7 Sept. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197510,Episodic Koopman Learning of Nonlinear Robot Dynamics with Application to Fast Multirotor Landing,IEEE,Conferences,"This paper presents a novel episodic method to learn a robot's nonlinear dynamics model and an increasingly optimal control sequence for a set of tasks. The method is based on the Koopman operator approach to nonlinear dynamical systems analysis, which models the flow of observables in a function space, rather than a flow in a state space. Practically, this method estimates a nonlinear diffeomorphism that lifts the dynamics to a higher dimensional space where they are linear. Efficient Model Predictive Control methods can then be applied to the lifted model. This approach allows for real time implementation in on-board hardware, with rigorous incorporation of both input and state constraints during learning. We demonstrate the method in a real-time implementation of fast multirotor landing, where the nonlinear ground effect is learned and used to improve landing speed and quality.",https://ieeexplore.ieee.org/document/9197510/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863468,Estimated force emulation for space robot using neural networks,IEEE,Conferences,"This paper introduces the telerobotic system estimated force emulation using neural networks. A delay-compensating 3D stereo-graphic simulator is implemented in SGI ONYX/4 RE/sup 2/. The estimated force emulation can protect the real robot in time from being damaged in collision. The neural network is used to learn the mapping between the contact force error and the accommodated position command to the controller of the space robot. Finally, the controller can feel the emulated force with a two-hand 6-DOF master arm using the force feedback interface.",https://ieeexplore.ieee.org/document/863468/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2007.4415203,Evolving Personality of a Genetic Robot in Ubiquitous Environment,IEEE,Conferences,"This paper discusses the personality of genetic robot and its evolving algorithm within the purview of the broader ubiquitous robot framework. Ubiquitous robot systems blends mobile robot technology (Mobot) with distributed sensor systems (Embot) and overseeing software intelligence (Sobot), for various integrated services. The Sobot is a critical question since it performs the dual purpose of overseeing intelligence as well as user interface. The Sobot is hence modelled as an artificial creature with autonomously driven behavior. The artificial creature has its own genome and in which each chromosome consists of many genes that contribute to defining its personality. This paper proposes evolving the personality of an artificial creature. A genome population is evolved such that it customized the genome satisfying a set of personality traits desired by the user. Evaluation procedure for each genome of the population is carried out in a virtual environment. Effectiveness of this scheme is demonstrated by using an artificial creature, Rity in the virtual 3D world created in a PC.",https://ieeexplore.ieee.org/document/4415203/,RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication,26-29 Aug. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITA.2005.135,Experiences with simulated robot soccer as a teaching tool,IEEE,Conferences,"The development of assignments for undergraduate teaching typically requires a compromise between what is achievable by an average student and what engages the interest of a more advanced member of the class. Selecting a suitable compromise is particularly problematic for undergraduate artificial intelligence (AI) courses which typically attempt to cover a very broad range of topics, without delving too deeply into the details. Ideally, a single problem would be selected whose solution could be approached with more than one technique covered in the course, enabling students to carry out a comparative analysis of performance. Robot soccer simulation has provided an interesting platform for artificial intelligence research and is increasingly being used as a teaching apparatus. There are a number of limitations with existing simulation methodologies for this purpose. Current robot soccer simulators are aimed at research groups where accuracy is paramount and all facets of the real system must be emulated. However, many of the intricacies of a real robot soccer player are inappropriate for a teaching environment, as they detract from desired learning outcomes. Consequently, there is a need for a simulation that employs a simplified set of game rules and dynamics. This paper describes the design and implementation of such a framework and presents experiences gained from its use as a third year practical.",https://ieeexplore.ieee.org/document/1488833/,Third International Conference on Information Technology and Applications (ICITA'05),4-7 July 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EHB52898.2021.9657712,Experimental Testing and Implementation of a Force &#x2013; Torque Sensor in Automated Percutaneous Needle Insertion Instruments,IEEE,Conferences,"The paper aims to describe the forces and torques that appear during the percutaneous needle insertion (through in vivo laboratory experiments) and investigate the benefits of a force/torque sensor integrated on an automated needle insertion instrument used as a robot end-effector. Based on the experimental results, several events occurring during the needle insertion were identified and described. Furthermore, a solution to implement a force/torque sensor into a needle insertion instrument was proposed.",https://ieeexplore.ieee.org/document/9657712/,2021 International Conference on e-Health and Bioengineering (EHB),18-19 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RIOS.2016.7529488,Experimental stability study of an Octorotor using an intelligent controller,IEEE,Conferences,"In this paper, the attitude stabilization problem of an Octorotor with coaxial motors is studied. To this end, the new method of intelligent adaptive control is presented. The designed controller which includes fuzzy and PID controllers, is completed by resistant adaptive function of approximate external disturbance and changing in the dynamic model. In fact, the regulation factor of PID controller is done by the fuzzy logic system. At first, the Fuzzy-PID and PID controllers are simulated in MATLAB/Simulink. Then, the Fuzzy-PID controller is implemented on the Octorotor with coaxial motors as online auto-tuning. Also, LabVIEW software has been used for tests and the performance analysis of the controllers. All of this experimental operation is done in indoor environment in the presence of wind as disturbance in the hovering operation. All of these operations are real-time and telemetry wireless is done by network connection between the robot and ground station in the LABVIEW software. Finally, the controller efficiency and results are studied.",https://ieeexplore.ieee.org/document/7529488/,2016 Artificial Intelligence and Robotics (IRANOPEN),9-9 April 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVRIS.2018.00078,Exploration of Computer Emotion Decision Based on Artificial Intelligence,IEEE,Conferences,"To carry out the discussion of computer emotion decision based on artificial intelligence, first of all, based on the psychological experiment paradigm of children's game task, and the test process of the artificial emotion generating engine was completed on the emotional spontaneous transfer and the stimulus transfer model. Secondly, the reasoning method and analytic hierarchy process (AHP) were introduced into the multi system, and a kind of multi emotion decision model based on the emotion reasoning was constructed. The hierarchical structure method was used to solve the complex decision problem of the humanoid robot in the intelligent home environment. Then, based on the emotion energy theory, a mood state regulation algorithm based on the combination of HMM-based spontaneous transfer and stimulus transfer was established. In addition, on this basis, the design and implementation of humanoid robot associative memory model was realized. Finally, the theory and algorithm were integrated into the interactive platform of human-computer expression, and the validity of the model was analysed and verified. The results showed that, on this robot platform, the process of human-computer interaction and cooperation which integrated emotion evaluation, emotional decision, associative memory and emotion regulation was realized. As a result, the computer emotion decision based on artificial intelligence can be well applied in many fields.",https://ieeexplore.ieee.org/document/8531405/,2018 International Conference on Virtual Reality and Intelligent Systems (ICVRIS),10-11 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICORR.2019.8779424,Exploring the Impact of Machine-Learned Predictions on Feedback from an Artificial Limb,IEEE,Conferences,"Learning to get by without an arm or hand can be very challenging, and existing prostheses do not yet fill the needs of individuals with amputations. One promising solution is to improve the feedback from the device to the user. Towards this end, we present a simple machine learning interface to supplement the control of a robotic limb with feedback to the user about what the limb will be experiencing in the near future. A real-time prediction learner was implemented to predict impact-related electrical load experienced by a robot limb; the learning system's predictions were then communicated to the device's user to aid in their interactions with a workspace. We tested this system with five able-bodied subjects. Each subject manipulated the robot arm while receiving different forms of vibrotactile feedback regarding the arm's contact with its workspace. Our trials showed that using machine-learned predictions as a basis for feedback led to a statistically significant improvement in task performance when compared to purely reactive feedback from the device. Our study therefore contributes initial evidence that prediction learning and machine intelligence can benefit not just control, but also feedback from an artificial limb. We expect that a greater level of acceptance and ownership can be achieved if the prosthesis itself takes an active role in transmitting learned knowledge about its state and its situation of use.",https://ieeexplore.ieee.org/document/8779424/,2019 IEEE 16th International Conference on Rehabilitation Robotics (ICORR),24-28 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561040,Extendable Navigation Network based Reinforcement Learning for Indoor Robot Exploration,IEEE,Conferences,This paper presents a navigation network based deep reinforcement learning framework for autonomous indoor robot exploration. The presented method features a pattern cognitive non-myopic exploration strategy that can better reflect universal preferences for structure. We propose the Extendable Navigation Network (ENN) to encode the partially observed high-dimensional indoor Euclidean space to a sparse graph representation. The robot’s motion is generated by a learned Q-network whose input is the ENN. The proposed framework is applied to a robot equipped with a 2D LIDAR sensor in the GAZEBO simulation where floor plans of real buildings are implemented. The experiments demonstrate the efficiency of the framework in terms of exploration time.,https://ieeexplore.ieee.org/document/9561040/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CBS46900.2019.9114416,"Motion Prediction of Virtual Patterns, Human Hand Motions, and a simplified Hand Manipulation Task with Hierarchical Temporal Memory",IEEE,Conferences,"In this paper we utilize Numenta's Hierarchical Temporal Memory implementation NuPIC for online visual motion pattern prediction and test its performance on virtual animations as well as real world human motion data. For evaluation we run a series of progressively more complex experiments testing specific capabilities: Prediction of fixed-time noise-free motion animations, prediction of protocol-directed tasks with real-world camera captured human motion data, and lastly prediction of repetitive tasks performed without a strict protocol. Results show that the presented setup is able to predict time sequenced images as well as highly variable human motions increasingly well over several iterations. Limits are faced for non sequential variable hand motion execution: Here, predictions are made but do not improve in quality over time. The network runs online in real time and can be transferred to different tasks without expert knowledge. These characteristics qualify the setup for human robot interaction scenarios without the need for verified prediction accuracy.",https://ieeexplore.ieee.org/document/9114416/,2019 IEEE International Conference on Cyborg and Bionic Systems (CBS),18-20 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIMSA.2009.5069917,Motion planning in unknown environment using an interval fuzzy type-2 and neural network classifier,IEEE,Conferences,"This paper describes environmental recognition and motion control using weightless neural network classifier and interval type-2 fuzzy logic controller. The weightless neural network classifies geometric feature such as U-shape, corridor and left or right corner using ultrasonic sensors. The neural network utilizes previous sensor data and analyzes the situation of the current environment. The behavior of mobile robot is implemented by means of fuzzy control rules. Based on the performance criteria the quality of controller is evaluated to make navigation decisions. This functionality is demonstrated on a mobile robot using modular platform and containing several microcontrollers implies the implementation of a robust architecture. The proposed architecture implemented using low cost range sensor and low cost microprocessor. The experiment results show that the mobile robot can recognize the current environment and was able to successfully avoid obstacle in real time.",https://ieeexplore.ieee.org/document/5069917/,2009 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications,11-13 May 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636777,Multi-Object Grasping – Estimating the Number of Objects in a Robotic Grasp,IEEE,Conferences,"A human hand can grasp a desired number of objects at once from a pile based solely on tactile sensing. To do so, a robot needs to make a grasp in a pile, sense the number of objects in the grasp before lifting, and predict how many will remain in the grasp after lifting. It is a very challenging problem because when making the prediction, the robotic hand is still in the pile and the objects in the grasp are not observable to vision systems. Moreover, some objects in the hand before lifting may fall out the grasp when the lifting starts because they were supported by other objects in the pile instead of the fingers. A robotic hand should sense how many objects are in a grasp using its tactile sensors before lifting. This paper presents novel multi-object grasping analyzing methods to solve this problem. They include a grasp volume calculation, tactile force analysis, and a data-driven deep learning approach. The methods have been implemented on a Barrett hand and then evaluated in simulations and a real setup with a robotic system. The evaluation results conclude that once the Barrett hand grasps multiple objects in the pile, the data-driven models can make a good prediction before lifting on how many objects will remain in the hand after lifting. The root-mean-square errors are 0.74 for balls and 0.58 for cubes in simulations, and 1.06 for balls and 1.45 for cubes in the real system.",https://ieeexplore.ieee.org/document/9636777/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICE54393.2021.00059,Multi-Rotor UAV Autonomous Tracking and Obstacle Avoidance Based on Improved DDPG,IEEE,Conferences,"To solve the problem of multi-rotor UAV autonomous tracking dynamic ground targets in obstacles environment, we used Markov decision process (MDP) to establish an autonomous maneuvering model of multi-rotor. Considering the obstacle avoidance requirements of UAV during the tracking process, we integrated the Long Short-Term Memory (LSTM) neural network with memory unit and time series data processing characteristics into the Deep Deterministic Policy Gradient (DDPG) algorithm framework, so that the Actor network can fully refer to the prior state information when making decisions. Finally, the performance test was implemented on the UAV 3D simulation platform based on Robot Operating System (ROS). The results show that the method proposed in this paper can enable the UAV to complete the whole process of autonomous tracking of the ground dynamic target. Compared with the traditional DDPG algorithm, the DDPG algorithm combined with LSTM has stronger accuracy and real-time performance, and can better meet the tracking and obstacle avoidance mission requirements of the multi-rotor UAV.",https://ieeexplore.ieee.org/document/9797591/,2021 2nd International Conference on Artificial Intelligence and Computer Engineering (ICAICE),5-7 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00816,Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks,IEEE,Conferences,"We present a novel method for multi-view depth estimation from a single video, which is a critical task in various applications, such as perception, reconstruction and robot navigation. Although previous learning-based methods have demonstrated compelling results, most works estimate depth maps of individual video frames independently, without taking into consideration the strong geometric and temporal coherence among the frames. Moreover, current state-of-the-art (SOTA) models mostly adopt a fully 3D convolution network for cost regularization and therefore require high computational cost, thus limiting their deployment in real-world applications. Our method achieves temporally coherent depth estimation results by using a novel Epipolar Spatio-Temporal (EST) transformer to explicitly associate geometric and temporal correlation with multiple estimated depth maps. Furthermore, to reduce the computational cost, inspired by recent Mixture-of-Experts models, we design a compact hybrid network consisting of a 2D context-aware network and a 3D matching network which learn 2D context information and 3D disparity cues separately. Extensive experiments demonstrate that our method achieves higher accuracy in depth estimation and significant speedup than the SOTA methods.",https://ieeexplore.ieee.org/document/9577311/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MILTECHS.2017.7988861,Multiple people detection and identification system integrated with a dynamic simultaneous localization and mapping system for an autonomous mobile robotic platform,IEEE,Conferences,"This paper presents the integration of a multiple people detection and identification system with a dynamic simultaneous localization and mapping system for an autonomous robotic platform. This integration allows the exploration and navigation of the robot considering people identification. The robotic platform consists of a Pioneer 3DX robot equipped with an RGBD camera, a Sick Lms200 sensor laser and a computer using the robot operating system (ROS). The idea is to integrate the people detection and identification system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. The people detection and identification system is performed in two steps. The first one is for detecting multiple people on scene and the other one is for an individual person identification. Both steps are implemented as ROS nodes that works integrated with the SLAM ROS node. The multiple people detection's node uses a manual feature extraction technique based on HOG (Histogram of Oriented Gradients) detectors, implemented using the PCL library (Point Cloud Library) in C ++. The person's identification node is based on a Deep Convolutional Neural Network (CNN) that are implemented using the MatLab MatConvNet library. This step receives the detected people centroid from the previous step and performs the classification of a specific person. After that, the desired person centroid is send to the SLAM node, that consider it during the mapping process. Tests were made objecting the evaluation of accurateness in the people's detection and identification process. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.",https://ieeexplore.ieee.org/document/7988861/,2017 International Conference on Military Technologies (ICMT),31 May-2 June 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS.2010.16,Navigation System for an Underground Distribution Inspection Platform Using Simulation,IEEE,Conferences,"This work proposes an architecture of an inspection robot's navigation system, aiming at monitoring of power cables in underground distribution lines. This architecture is composed of two modules: i. feature extraction from environment, ii navigation approach. The feature extraction module is based on the use of the edge detector by Canny algorithm and Hough transform for identification of lines from images of environment to monitoring. The lines identified correspond to cable conformation inside the duct. This information will serve to help the navigation system. For the implementation of the navigation system two approaches were proposed: navigation based on artificial neural network and navigation based on PID control. The navigation architecture can be used in real or simulated scenarios, and it was tested in a simulated environment.",https://ieeexplore.ieee.org/document/5702190/,2010 Latin American Robotics Symposium and Intelligent Robotics Meeting,23-28 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCCSP.2008.4537417,Neuro-adaptive dynamic control for mobile robots: Experimental validation,IEEE,Conferences,"This paper reports on the design and implementation of a neuro-adaptive controlled nonholonomic mobile robot. It presents experimental results to validate the employed control scheme on a physical setup for the first time, after it was originally proposed by the same authors and tested by simulations only. The control system is composed of a trajectory tracking kinematic controller which generates the reference wheel velocities, and a cascaded dynamic controller which employs a neural network for the real-time estimation of the robot's nonlinear dynamics so as to attain precise velocity tracking, even in the presence of unknown and/or time-varying dynamics. Details about the hardware and software setup, as well as salient implementation issues are also reported in this work.",https://ieeexplore.ieee.org/document/4537417/,"2008 3rd International Symposium on Communications, Control and Signal Processing",12-14 March 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2001.938487,Neuro-controller for high performance induction motor drives in robots,IEEE,Conferences,"Presents an approach to the speed control of an induction motor (IM) as a robust high performance drive (HPD) using an online self-tuning adapted artificial neural network (ANN). Based on motor dynamics and nonlinear unknown load characteristics such as robot systems, a neuro speed controller is developed. The proposed controller is very simple and serves as an identifier and a controller at the same time. The combination of the adaptive learning rate with the epochs used through the online training offers a unique feature of system identification and adaptive control. The performance of the controller was evaluated under various operating conditions to track different speed trajectories. The results validate the efficacy of the ANN for the precise tracking control of IM. Furthermore the use of the ANN makes the drive system robust, accurate, and insensitive to parameter variations. Also the drive system is implemented in real-time using a digital signal processor (DSP) TMS320C31.",https://ieeexplore.ieee.org/document/938487/,IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),15-19 July 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1999.832713,Nonlinear system adaptive trajectory tracking by dynamic neural control,IEEE,Conferences,"In this article, new nonlinear control techniques based on dynamic neural networks are presented. The authors discuss the implementation of a modified identification algorithm using dynamic neural networks as well as a control law, based on the neural identifier, which eliminates modeling error effects via sliding mode techniques. Simulation and real time results are presented for systems like an inverted pendulum and a full actuated robot manipulator.",https://ieeexplore.ieee.org/document/832713/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMVL.2015.39,Novel VLSI Architectures for Real-World Intelligent Systems,IEEE,Conferences,"A real-world intelligent systems platform based on novel architectures is presented in this article. As real-world applications, we consider advanced intelligent systems such as a highly-safe system and an intelligent robot system which make our daily life safe and comfortable.",https://ieeexplore.ieee.org/document/7238146/,2015 IEEE International Symposium on Multiple-Valued Logic,18-20 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN45523.2021.9557354,Novelty Detection for Iterative Learning of MIMO Fuzzy Systems,IEEE,Conferences,"This paper proposes a methodology for iterative learning of multi-input multi-output (MIMO) fuzzy models focusing on dynamic system identification. The first step of the proposed method is the learning of the antecedent part of the fuzzy system, which is learned iteratively, where fuzzy rules can be added or merged based on the presented novelty detection and similarity criteria defined by a recursive extension of the Gath-Geva clustering algorithm. Then, the consequent part consists in the direct implementation of a non-recursive fuzzy approach that uses global least squares, Observer Kalman Filter Identification (OKID) and the Eigensystem Realization Algorithm (ERA). The proposed method is validated using experimental data from a real quadrotor aerial robot, a nonlinear dynamic system. Using quantitative performance metrics, the proposed method is compared with Hammerstein-Wiener models (H.-W.), nonlinear autoregressive models with exogenous input (NARX), and state-space models using subspace method with time-domain data (N4SID), other MIMO system identification techniques. The proposed method achieved better results compared to other techniques, showing the importance and versatility of learning based on novelty detection for MIMO problems.",https://ieeexplore.ieee.org/document/9557354/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISCT.2019.8777441,Object Recognition for Dental Instruments Using SSD-MobileNet,IEEE,Conferences,"In recent technological developments, robot-assisted surgery has become popular due to its tremendous prospects in enhancing the capabilities of surgeons performing open surgery, yet very little effort has been made to make these tools available to dental surgeons. This paper addresses the problem of identifying the problem of real-time object recognition of dental instruments by utilizing deep learning techniques. For this reason, the Single Shot MultiBox Detector (SSD) network was considered as the meta structure and joined with the base Convolutional Neural Network (CNN) MobileNet to shape SSD-MobileNet. The task of object recognition for dental instruments like spatula, elevator, mouth mirror etc is performed, in order to constitute a robotic arm; that works with voice commands using speech recognition, and assists the dentist in surgery. Our method can recognize instruments more precisely and quickly as contrast with other lightweight system strategies and conventional machine learning techniques. We have achieved the precision and accuracy of 87.3% and 98.8% respectively.",https://ieeexplore.ieee.org/document/8777441/,2019 International Conference on Information Science and Communication Technology (ICISCT),9-10 March 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2004.1307138,Obstacle avoidance through incremental learning with attention selection,IEEE,Conferences,"This work presents a learning-based approach to the task of generating local reactive obstacle avoidance. The learning is performed online in real-time by a mobile robot. The robot operated in an unknown bounded 2-D environment populated by static or moving obstacles (with slow speeds) of arbitrary shape. The sensory perception was based on a laser range finder. To greatly reduce the number of training samples needed, an attentional mechanism was used. An efficient, real-time implementation of the approach had been tested, demonstrating smooth obstacle-avoidance behaviors in a corridor with a crowd of moving students as well as static obstacles.",https://ieeexplore.ieee.org/document/1307138/,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",26 April-1 May 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196769,Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure,IEEE,Conferences,"In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.",https://ieeexplore.ieee.org/document/9196769/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2008.4543481,Online constraint network optimization for efficient maximum likelihood map learning,IEEE,Conferences,"In this paper, we address the problem of incrementally optimizing constraint networks for maximum likelihood map learning. Our approach allows a robot to efficiently compute configurations of the network with small errors while the robot moves through the environment. We apply a variant of stochastic gradient descent and use a tree-based parameterization of the nodes in the network. By integrating adaptive learning rates in the parameterization of the network, our algorithm can use previously computed solutions to determine the result of the next optimization run. Additionally, our approach updates only the parts of the network which are affected by the newly incorporated measurements and starts the optimization approach only if the new data reveals inconsistencies with the network constructed so far. These improvements yield an efficient solution for this class of online optimization problems. Our approach has been implemented and tested on simulated and on real data. We present comparisons to recently proposed online and offline methods that address the problem of optimizing constraint network. Experiments illustrate that our approach converges faster to a network configuration with small errors than the previous approaches.",https://ieeexplore.ieee.org/document/4543481/,2008 IEEE International Conference on Robotics and Automation,19-23 May 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2009.5178844,Online temporal pattern learning,IEEE,Conferences,"This paper describes a biologically motivated approach, using hierarchical temporal memory (HTM), to build a high-level self-organizing visual system for a soccer bot. Meanwhile it presents two unsupervised online learning algorithms for temporal patterns in HTMs. The algorithms were implemented in a simulated soccer bot for a real-world evaluation. After a training phase, the robot was able to recognize different static objects in the soccer field. It also learned and recognized high-level objects that are composed of simpler objects, with position invariance and was also able to learn and recognize motions in the objects, all in a completely unsupervised manner.",https://ieeexplore.ieee.org/document/5178844/,2009 International Joint Conference on Neural Networks,14-19 June 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC45564.2020.9147898,Optimal Control of Wheeled Mobile Robots: From Simulation to Real World,IEEE,Conferences,"We study the problem of taking simulations to the real world (RW) for autonomous robotic systems with dynamic uncertainties and unknown disturbances while maintaining the optimal performance and stability of the designed controller designed in simulation. In general, an optimal and robust controller that is designed through simulation often does not perform similarly when deployed in the RW. We focus on using simulations to generate an optimal control policy utilizing the Memetic algorithm (MA) iteratively. The simulation-to-RW performance and stability are realized by using an adaptive fuzzy system to learn the uncertain part of the dynamic model, disturbance and noises. We demonstrate experimentally that this method permits the development of optimal control design in simulations and integrates adaptive learning rules to enable precise and repetitive trajectory tracking for the wheeled mobile robot (WMR) with disturbances and uncertainties.",https://ieeexplore.ieee.org/document/9147898/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636118,Optimal scheduling and non-cooperative distributed model predictive control for multiple robotic manipulators,IEEE,Conferences,"Application of multiple robotic manipulators in a shared workspace is still restricted to repetitive tasks limiting their flexible deployment for production systems. Still, existing motion control algorithms cannot be performed online for arbitrary environments in case of multiple manipulators cooperating with each other. In this work we propose a scalable and real-time capable motion control algorithm based on non-cooperative distributed model predictive control. Furthermore, we propose an optimal scheduling algorithm, which provides optimal setpoints to each robot’s motion controller that prevents possible deadlocks beforehand. We validate our approach on a simulative setup of four robotic manipulators for multiple pick and place scenarios.",https://ieeexplore.ieee.org/document/9636118/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII.2012.6426933,Optimization of obstacle avoidance using reinforcement learning,IEEE,Conferences,Walking through narrow space for multi-legged robot is optimized using reinforcement learning in this paper. The walking is generated by the virtual repulsive force from the estimated obstacle position and the virtual impedance field. The resulted action depends on the parameter of the virtual impedance coefficients. The reinforcement learning is employed to find an optimal motion. The temporal walking through motion consists of each parameter optimized for a situation. Optimization of integrated walking through motion is finally achieved evaluating walking in compound encountering obstacle on simulator. The resulted motion is implemented to a real multi-legged robot and results show the effectiveness of the proposed method.,https://ieeexplore.ieee.org/document/6426933/,2012 IEEE/SICE International Symposium on System Integration (SII),16-18 Dec. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1997.655095,Output methods for an associative operation of programmable artificial retinas,IEEE,Conferences,"The introduction of intelligence near each photosensitive element in focal plane arrays (FPA) leads to sensory devices-called artificial retinas-which may no longer output raw images but, rather, much more concentrated forms of information. In particular, when the on-sensor image processing facilities are powerful enough to allow some structural pattern recognition, lists of pixels of interest become an output format of choice from retina to microprocessor. This implies the development of specific output techniques and operators to be integrated in the focal plane. After an in-depth presentation of the motivations in the context of programmable artificial retinas (PAR) for robot vision, two original solutions to the problem are presented, corresponding to two different trade-offs between efficiency and VLSI implementation cost. The first one is a compact hardware solution, which allows to sense pixels of interest from the sides of the 2D pixel array. The second one, a mainly software technique, exploits the mathematical concept of de Bruijn arrays for a distributed encoding of pixel addresses on the PAR.",https://ieeexplore.ieee.org/document/655095/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1988.12120,Overview of the multiple autonomous underwater vehicles (MAUV) project,IEEE,Conferences,"The US National Bureau of Standard's multiple autonomous underwater vehicles (MAUV) project involves the development of a real-time intelligent control system that performs sensing, world modeling, planning, and execution for underwater robot vehicles. The goal of the project is to have multiple vehicles exhibiting intelligent, autonomous, cooperative behavior. Initial tests have involved two identical vehicles engaged in various scenarios in Lake Winnipesaukee in New Hampshire. All software for controlling the vehicles reside on computer boards mounted onboard the vehicles.<>",https://ieeexplore.ieee.org/document/12120/,Proceedings. 1988 IEEE International Conference on Robotics and Automation,24-29 April 1988,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IOTSMS48152.2019.8939254,Person Identification using Autonomous Drone through Resource Constraint Devices,IEEE,Conferences,"Detecting a specific person from the crowd using drone along with some resource constraint device is a major concern which we are discussing in the paper. Combining the advanced algorithms and some smart hardware material, we will be finding a way to search for a missing individual in a crowd or at some location. We can also search for a person at a specific location by setting our aerial vehicle to fly autonomously and search for the required person. This will help us to cover areas which cannot be reached by humans easily. The flying robot helps to solve real-time problems and come up with some new and more advanced ways to search for the missing ones with more ease, as advanced technological methods are applied, the probability of getting accurate results increases axiomatically. The drone can fly fully autonomously and search or capture videos/photos of the required location. Location commands could be given using PC, mobile and with the help of IoT, using Raspberry Pi.",https://ieeexplore.ieee.org/document/8939254/,"2019 Sixth International Conference on Internet of Things: Systems, Management and Security (IOTSMS)",22-25 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNSC.2014.6906671,Pixelwise object class segmentation based on synthetic data using an optimized training strategy,IEEE,Conferences,"In this paper we present an approach for low-level body part segmentation based on RGB-D data. The RGB-D sensor is thereby placed at the ceiling and observes a shared workspace for human-robot collaboration in the industrial domain. The pixelwise information about certain body parts of the human worker is used by a cognitive system for the optimization of interaction and collaboration processes. In this context, for rational decision making and planning, the pixelwise predictions must be reliable despite the high variability of the appearance of the human worker. In our approach we treat the problem as a pixelwise classification task, where we train a random decision forest classifier on the information contained in depth frames produced by a synthetic representation of the human body and the ceiling sensor, in a virtual environment. As shown in similar approaches, the samples used for training need to cover a broad spectrum of the geometrical characteristics of the human, and possible transformations of the body in the scene. In order to reduce the number of training samples and the complexity of the classifier training, we therefore apply an elaborated and coupled strategy for randomized training data sampling and feature extraction. This allows us to reduce the training set size and training time, by decreasing the dimensionality of the sampling parameter space. In order to keep the creation of synthetic training samples and real-world ground truth data simple, we use a highly reduced virtual representation of the human body, in combination with KINECT skeleton tracking data from a calibrated multi-sensor setup. The optimized training and simplified sample creation allows us to deploy standard hardware for the realization of the presented approach, while yielding a reliable segmentation in real-time, and high performance scores in the evaluation.",https://ieeexplore.ieee.org/document/6906671/,2014 First International Conference on Networks & Soft Computing (ICNSC2014),19-20 Aug. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1991.174701,Planning based sensing and task executing in an autonomous machine,IEEE,Conferences,"Implementing a control system for an autonomous machine is a challenging task. Several techniques have to be applied, such as task planning, hierarchical and/or distributed control, and advanced sensing techniques. In addition, to be useful these various techniques have to be integrated into a system that has to operate more or less in real-time. The authors present a control scheme based on hierarchically organized planning-executing-monitoring-cycles which is used to solve some of the problems related to real-time control of an autonomous machine. The implementation is also presented in which the control system is applied in a pilot system based on an industrial robot.<>",https://ieeexplore.ieee.org/document/174701/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561387,Pointing at Moving Robots: Detecting Events from Wrist IMU Data,IEEE,Conferences,"We propose a practical approach for detecting the event that a human wearing an IMU-equipped bracelet points at a moving robot; the approach uses a learned classifier to verify if the robot motion (as measured by its odometry) matches the wrist motion, and does not require that the relative pose of the operator and robot is known in advance. To train the model and validate the system, we collect datasets containing hundreds of real-world pointing events. Extensive experiments quantify the performance of the classifiers and relevant metrics of the resulting detectors; the approach is implemented in a real-world demonstrator that allows users to land quadrotors by pointing at them.",https://ieeexplore.ieee.org/document/9561387/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2006.96,Polynomial Regression with Automated Degree: A Function Approximator for Autonomous Agents,IEEE,Conferences,"In order for an autonomous agent to behave robustly in a variety of environments, it must have the ability to learn approximations to many different functions. The function approximator used by such an agent is subject to a number of constraints that may not apply in a traditional supervised learning setting. Many different function approximators exist and are appropriate for different problems. This paper proposes a set of criteria for function approximators for autonomous agents. Additionally, for those problems on which polynomial regression is a candidate technique, the paper presents an enhancement that meets these criteria. In particular, using polynomial regression typically requires a manual choice of the polynomial's degree, trading off between function accuracy and computational and memory efficiency. Polynomial regression with automated degree (PRAD) is a novel function approximation method that uses training data to automatically identify an appropriate degree for the polynomial. PRAD is fully implemented. Empirical tests demonstrate its ability to efficiently and accurately approximate both a wide variety of synthetic functions and real-world data gathered by a mobile robot",https://ieeexplore.ieee.org/document/4031933/,2006 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI'06),13-15 Nov. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ARITH.2019.00047,Privacy-Preserving Deep Learning via Additively Homomorphic Encryption,IEEE,Conferences,"We aim at creating a society where we can resolve various social challenges by incorporating the innovations of the fourth industrial revolution (e.g. IoT, big data, AI, robot, and the sharing economy) into every industry and social life. By doing so the society of the future will be one in which new values and services are created continuously, making people's lives more conformable and sustainable. This is Society 5.0, a super-smart society. Security and privacy are key issues to be addressed to realize Society 5.0. Privacy-preserving data analytics will play an important role. In this talk we show our recent works on privacy-preserving data analytics such as privacy-preserving logistic regression and privacy-preserving deep learning. Finally, we show our ongoing research project under JST CREST “AI”. In this project we are developing privacy-preserving financial data analytics systems that can detect fraud with high security and accuracy. To validate the systems, we will perform demonstration tests with several financial institutions and solve the problems necessary for their implementation in the real world.",https://ieeexplore.ieee.org/document/8877418/,2019 IEEE 26th Symposium on Computer Arithmetic (ARITH),10-12 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1999.832705,Programming robots with associative memories,IEEE,Conferences,"Today, there are several drawbacks that impede the necessary and much needed use of robot learning techniques in real applications. First, the time needed to achieve the synthesis of any behavior is prohibitive. Second, the robot behavior during the learning phase is by definition bad, it may even be dangerous. Third, except within the lazy learning approach, a new behavior implies a new learning phase. We propose in this paper to use self-organizing maps to encode the nonexplicit model of the robot-world interaction sampled by the lazy memory, and then generate a robot behavior by means of situations to be achieved, i.e., points on the self-organizing maps. Any behavior can instantaneously be synthesized by the definition of a goal situation. Its performance will be minimal (not evidently bad) and will improve by the mere repetition of the behavior.",https://ieeexplore.ieee.org/document/832705/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EURBOT.1997.633565,Q-learning of complex behaviours on a six-legged walking machine,IEEE,Conferences,"We present work on a six-legged walking machine that uses a hierarchical version of Q-learning (HQL) to learn both the elementary swing and stance movements of individual legs as well as the overall coordination scheme to perform forward movements. The architecture consists of a hierarchy of local controllers implemented in layers. The lowest layer consists of control modules performing elementary actions, like moving a leg up, down, left or right to achieve the elementary swing and stance motions for individual legs. The next level consists of controllers that learn to perform more complex tasks like forward movement by using the previously learned, lower level modules. On the third the highest layer in the architecture presented here the previously learned complex movements are themselves reused to achieve goals in the environment using external sensory input. The work is related to similar, although simulation-based, work by Lin (1993) on hierarchical reinforcement learning and Singh (1994) on compositional Q-learning. We report on the HQL architecture as well as on its implementation on the walking machine SIR ARTHUR. Results from experiments carried out on the real robot are reported to show the applicability of the HQL approach to real world robot problems.",https://ieeexplore.ieee.org/document/633565/,Proceedings Second EUROMICRO Workshop on Advanced Mobile Robots,22-24 Oct. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968551,RONet: Real-time Range-only Indoor Localization via Stacked Bidirectional LSTM with Residual Attention,IEEE,Conferences,"In this study, a three-layered bidirectional Long Short-term Memory (Bi-LSTM) with residual attention, named as RONet, is proposed to achieve localization using range measurements. Accordingly, we acquired our own datasets and tested RONet using realistic conditions. It is shown that the RONet can estimate the position of the mobile robot in real time using the Nvidia Jetson AGX Xavier based only on range measurements. We also analyzed the sequence length of LSTM as a type of hyperparameters. We found that optimal sequence length is eight for more than eight anchors and twelve for fewer anchors compared to sequences with different lengths, given that construction of the network with the optimal sequence length estimates the position precisely and accounts for uncertainties. As verified experimentally, RONet yields more precise performance and results in increased robustness against outliers compared to a conventional range-only approach based on a particle filtering and the other conventional deep-learning-based approaches. We set three cases, reduced the number of anchors, and verified that the RONet was a robust solution. We also confirmed that it is the best solution that yields the smallest Root-Mean-Square-Error (RMSE) values, equal to 4.466 cm, 3.210 cm, and 3.090 cm, in the cases where three, five, and eight anchors were deployed, respectively.",https://ieeexplore.ieee.org/document/8968551/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECICE52819.2021.9645719,RPA and L-System Based Synthetic Data Generator for Cost-efficient Deep Learning Model Training,IEEE,Conferences,"Deep learning (DL) models applied to computer vision have made great progress for image-based plant phenotyping in recent years, mostly for quality control process automation in the agroindustry. On the one hand, these models are able to detect objects in complex and noisy images as fast as human observations, but on the other hand, they are trained with a large amount of labeled data for parameter tuning. This turns the training process into an expensive, repetitive, and time-consuming labor. In this work, a synthetic data generator based on robotic process automation (RPA) and Lindenmayer systems (L-Systems) named RPASD is designed and implemented to train a DL model that detects artichoke seedlings in images captured by a robot. First, the growth artichoke seedling is modeled in L+C language using the LStudio software. Second, the RPASD is developed in Python to produce labeled images of grouped synthetic artichoke seedlings that alongside manually labeled images of real artichoke seedlings, taken by a robot, form the PlantiNet database. Third, a YOLOv3 model is trained with the previously built databases forming three datasets: 1) real and synthetics seedlings, 2) only synthetic seedlings, and 3) only real seedlings. The results show a 55% of Mean Intersection over the Union (mIoU) when training only with the second dataset and testing with the third one, which allows us to conclude that our proposed method could adequately boost DL model training reducing costs and time.",https://ieeexplore.ieee.org/document/9645719/,"2021 IEEE 3rd Eurasia Conference on IOT, Communication and Engineering (ECICE)",29-31 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2014.7064347,RSAW: A situation awareness system for autonomous robots,IEEE,Conferences,"Services and technologies are in evolution in order to develop a new generation of robotic systems that might operate in dynamic real-world environments. In this paper, we focus on the ability of robot to understand and to surpass the blocked situations autonomously without operator intervention. Such situations may occur when the robot cannot succeed the current action and cannot move to the next one. We remark that in the literature, the operator has a crucial role consisting in providing all information about the environment and in making interpretations. In this paper, we propose an RSAW (Robot Situation AWareness) system, developed in order to help a robot to surpass a blocked situation and accomplish its goal whilst minimizing the operator intervention. RSAW is a new general system aiming to increase the autonomy of the robot; It is inspired by the notion of Situation Awareness (SA). In fact, RSAW defines a knowledge representation using ontologies and a process in order to surpass a blocked situation. RSAW is designed according to the Model Driven Engineering (MDE) methodology. This choice is done to preserve the generality of our system. This paper focalizes on the process of the RSAW system and the interaction between the process and the knowledge representation. The experimentations conducted in real environment with the Smart Autonomous Majordomo (SAM) robot, have shown the robustness and the efficiency of the proposed system.",https://ieeexplore.ieee.org/document/7064347/,2014 13th International Conference on Control Automation Robotics & Vision (ICARCV),10-12 Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1992.225088,Reactive behavior design tools,IEEE,Conferences,"The reactive behavior of an autonomous agent can be described as collections of logical behaviors, each member of the collection controlling some aspect of the agent and working in conjunction with all the other behaviors. Such collections of reactive behaviors can be defined as combined, synchronous finite-state automata, using real-time programming languages which have strong formal components. These language tools, such as COSPAN and ESTEREL, require sophisticated users who have deep knowledge of both the syntax and semantics of the language. The authors use the simplicity of graphical finite-state automata editing to specify concurrent synchronous finite-state automata, and from those they produce COSPAN descriptions of these behaviors for analysis, and C language programs to implement the designed behaviors. The usefulness and validity of this approach was confirmed by the design, verification and implementation of several examples, including a controller demon for a robot arm.<>",https://ieeexplore.ieee.org/document/225088/,Proceedings of the 1992 IEEE International Symposium on Intelligent Control,11-13 Aug. 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1993.714313,Real time learning algorithm for redundant manipulator movement control,IEEE,Conferences,"We propose a new learning control strategy to solve the ill-posed inverse kinematics of a redundant robot manipulator. Four distinct characteristics are observed: 1) the inverse solution is context-sensitive, which is a requisite when the manipulator starts from an arbitrary joint configuration or moves in a complex environment; 2) learning and execution are both memory-based and can be implemented in real time; 3) the property of conventional pseudoinverse control, i.e. keeping the incremental changes of joint angles minimum, is intrinsic in our scheme; and 4) control is goal-directed in that only the current end-effector position relative to the goal position is needed.",https://ieeexplore.ieee.org/document/714313/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863435,Real time path smoothing schemes in teleoperation system,IEEE,Conferences,"The Robot Teleoperation System (RTS) based on telepresence, which is aided financially by the National 863 High-Tech Development Plan, was set up by the State Key Laboratory of Intelligence Technology and Systems. This system consists of three main parts: robot control, stereo vision and hand gesture tracking. The controlled robot and the operator form a closed loop, and the operator views the robot's status and the environment through the stereo vision subsystem. RTS is developed from SAROT (an intelligent assembly robot system), which is logically divided into 5 layers: real time control, monitoring and coordination, motion planning, task scheduling and task planning. The paper proposes several active ""path smoothing"" schemes implemented in the system, which carry out the operator's hand gesture tracking in 7 DOF (position: 3, orientation: 3, and pitch: 1).",https://ieeexplore.ieee.org/document/863435/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECCTD.2005.1522965,Real time vision by FPGA implemented CNNs,IEEE,Conferences,"In order to get real time image processing for mobile robot vision, we propose to use a discrete time cellular neural network implementation by a convolutional structure on Altora FPGA using VHDL language. We obtain at least 9 times faster processing than other emulations for the same problem.",https://ieeexplore.ieee.org/document/1522965/,"Proceedings of the 2005 European Conference on Circuit Theory and Design, 2005.",2-2 Sept. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANSKOBE.2018.8559422,Real-Time Automated Evaluation of COLREGS-Constrained Interactions Between Autonomous Surface Vessels and Human Operated Vessels in Collaborative Human-Machine Partnering Missions,IEEE,Conferences,"This paper explores an extension of the real-time evaluation of COLREGS-based collision avoidance interactions between autonomous surface vessels and human-operated surface vessels. Our previous work developed the algorithms that evaluate and quantify a ship's compliance with the collision regulations, safety, and mission efficiencies with respect to the overall goal(s). Our previous work is extended in this paper by establishing a light-weight program to assign penalties to offenders of safety or protocol violations during human-machine collaborative on-water interactions. Vessels interacting in this DARPA-sponsored Aquaticus mission are grouped into teams consisting of both human and robot counterparts. These teams play a “capture the flag” like game while being required to obey the maritime collision avoidance regulations. This paper is a first step in the field toward evaluating collision avoidance rules in the context of a human or robotic vehicle cheating the COLREGS against its opponent to gain a mission advantage. The problem is representative of interactions likely seen on the open ocean using a combination of autonomous and human-operated multi-vehicle collision avoidance interactions to larger scale maritime vessel traffic interactions operating under COLREGS protocol constraints. The vessels deploy in a distributed collaborative pattern to compete against the opposing team. Upon detection of a violation, the offending vessel(s) are required to complete their penalty actions prior to being allowed to proceed in their mission goal. Human offenders are able to be linked to haptic devices that give real-time feedback using vibrations or similar queuing.",https://ieeexplore.ieee.org/document/8559422/,2018 OCEANS - MTS/IEEE Kobe Techno-Oceans (OTO),28-31 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIA.2006.305830,Real-Time Fusion of Multimodal Tracking Data and Generalization of Motion Patterns for Trajectory Prediction,IEEE,Conferences,"A sensor-based model of a service robot's environment is a prerequisite for interaction. Such a model should contain the positions of the robot's interaction partners. Many reasonable applications require this knowledge in realtime. It could for example be used to realize efficient path planning for delivery tasks. Additionally to the actual positions of the partners it is important for the service robot to predict their possible future positions. In this paper we propose an extensible framework that combines different sensor modalities in a general real-time tracking system. Exemplarily, a tracking system is implemented that fuses tracking algorithms in laser range scans as well as in camera images by a particle filter. Furthermore, human trajectories are predicted by deducing them from learned motion patterns. The observed trajectories are generalized to trajectory patterns by a novel method which uses self organizing maps. Those patterns are used to predict trajectories of the currently observed persons. Practical experiments show that multimodality increases the system's robustness to incorrect measurements of single sensors. It is also demonstrated that a self organizing map is suitable for learning and generalizing trajectories. Convenient predictions of future trajectories are presented which are deduced from these generalizations.",https://ieeexplore.ieee.org/document/4097763/,2006 IEEE International Conference on Information Acquisition,20-23 Aug. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CACIS.2019.8825093,Real-Time Robotic Grasping and Localization Using Deep Learning-Based Object Detection Technique,IEEE,Conferences,"This work aims to increase the impact of computer vision on robotic positioning and grasping in industrial assembly lines. Real-time object detection and localization problem is addressed for robotic grasp-and-place operation using Selective Compliant Assembly Robot Arm (SCARA). The movement of SCARA robot is guided by deep learning-based object detection for grasp task and edge detection-based position measurement for place task. Deep Convolutional Neural Network (CNN) model, called KSSnet, is developed for object detection based on CNN Alexnet using transfer learning approach. SCARA training dataset with 4000 images of two object categories associated with 20 different positions is created and labeled to train KSSnet model. The position of the detected object is included in prediction result at the output classification layer. This method achieved the state-of-the-art results at 100% precision of object detection, 100% accuracy for robotic positioning and 100% successful real-time robotic grasping within 0.38 seconds as detection time. A combination of Zerocross and Canny edge detectors is implemented on a circular object to simplify the place task. For accurate position measurement, the distortion of camera lens is removed using camera calibration technique where the measured position represents the desired location to place the grasped object. The result showed that the robot successfully moved to the measured position with positioning Root Mean Square Error (0.361, 0.184) mm and 100% for successful place detection.",https://ieeexplore.ieee.org/document/8825093/,2019 IEEE International Conference on Automatic Control and Intelligent Systems (I2CACIS),29-29 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARM49381.2020.9195281,Real-time Colonoscopy Image Segmentation Based on Ensemble Knowledge Distillation,IEEE,Conferences,"Colonoscopy is an important means of detecting various intestinal diseases such as bleeding, polyps, Merck diverticula, and ulcers. The sooner these diseases are detected, the better the patient's recovery. But colonoscopy is a demanding process, often leads to the high rate of misdiagnosis by experts, professional physicians and nurses and costs a lot of time. Therefore, robot-assisted colonoscopy is considered as an important method to solve this problem. In recent years, many automated deep learning models for colonoscopy have been proposed. However, these models are usually large and time-consuming, and cannot meet actual needs. Besides, due to the disconnection of data between hospitals, the strength of medical resources between different departments in different hospitals is different, so the general multi-classification model cannot fit the characteristics of such data distribution. Therefore, in this article, we ensemble multiple binary classification models (each model detects one disease) and extracted a compression model using knowledge distillation technology, which can simultaneously detect polyps, Merkel diverticula, ulcers and bleeding from colonoscopy. We tested the performance of our model on public and real data sets and found that the model can achieve acceptable results and help doctors make decisions in practice.",https://ieeexplore.ieee.org/document/9195281/,2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM),18-21 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR46387.2019.8981549,Real-time RGB-D semantic keyframe SLAM based on image segmentation learning from industrial CAD models,IEEE,Conferences,"This paper presents methods for performing realtime semantic SLAM aimed at autonomous navigation and control of a humanoid robot in a manufacturing scenario. A novel multi-keyframe approach is proposed that simultaneously minimizes a semantic cost based on class-level features in addition to common photometric and geometric costs. The approach is shown to robustly construct a 3D map with associated class labels relevant to robotic tasks. Alternatively to existing approaches, the segmentation of these semantic classes have been learnt using RGB-D sensor data aligned with an industrial CAD manufacturing model to obtain noisy pixel-wise labels. This dataset confronts the proposed approach in a complicated real-world setting and provides insight into the practical use case scenarios. The semantic segmentation network was fine tuned for the given use case and was trained in a semi-supervised manner using noisy labels. The developed software is real-time and integrated with ROS to obtain a complete semantic reconstruction for the control and navigation of the HRP4 robot. Experiments in-situ at the Airbus manufacturing site in Saint-Nazaire validate the proposed approach.",https://ieeexplore.ieee.org/document/8981549/,2019 19th International Conference on Advanced Robotics (ICAR),2-6 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863254,Real-time bilateral control of Internet-based teleoperation,IEEE,Conferences,"The growth of the Internet has been accompanied by an increase in its applications. One of the most interesting of these is teleoperation, where the Internet is used as a bridge between operators and machines. However, teleoperation over the Internet comes with several problems: delay, lost packets and disconnection. All of these limitations may cause instability in teleoperation systems, especially for those systems include haptic feedback. Most of the previous work in Internet based teleoperation rests on many limiting assumptions, for example, time delay is constant or has an upper bound, control is not in real-time. This paper presents a new real time haptic feedback system that deals with these limitations and difficulties without making any assumptions regarding the time delay. The approach is based on the event based control, which has been implemented or a mobile robot over the Internet. The haptic information include real-time feedback of force and video.",https://ieeexplore.ieee.org/document/863254/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BIOROB.2008.4762823,Real-time isometric pinch force prediction from sEMG,IEEE,Conferences,"This paper describes a real-time isometric pinch force prediction algorithm using surface electromyogram (sEMG). The activities of seven muscles related to the movements of the thumb and index finger joints, which are observable using surface electrodes, were recorded during pinch force experiments. For the successful implementation of the real-time prediction algorithm, an off-line analysis was performed using the recorded activities. From the seven muscles, four muscles were selected for monitoring using the Fisher linear discriminant paradigm in an off-line analysis, and the recordings from these four muscles provided the most effective information for mapping sEMG to the pinch force. An ANN structure was designed to perform efficient training and to avoid both under-fitting and over-fitting problems. Finally, the pinch force prediction algorithm was tested with five volunteers and the results were evaluated using two criteria: normalized root mean squared error (NRMSE) and correlation (CORR). The training time for the subjects was only 2 min 29 sec, but the prediction results were successful with NRMSE = 0.093 plusmn0.047 and CORR = 0.957 plusmn0.031. These results imply that the proposed algorithm is useful to measure the generated pinch force without force sensors. The possible applications of the proposed method include controlling bionic finger robot systems to overcome finger paralysis or amputation.",https://ieeexplore.ieee.org/document/4762823/,2008 2nd IEEE RAS & EMBS International Conference on Biomedical Robotics and Biomechatronics,19-22 Oct. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS.2016.7502588,Real-time unmanned aerial vehicle 3D environment exploration in a mixed reality environment,IEEE,Conferences,"This paper presents a novel human robot interaction system that can be used for real-time 3D environment exploration with an unmanned aerial vehicle (UAV). The method creates a mixed reality environment, in which a user can interactively control a UAV and visualize the exploration data in real-time. The method uses a combination of affordable sensors, and transforms the control and viewing space from the UAV to the controller's perspective. Different hardware and software configurations are studied so that the system can be adjusted to meet different needs and environments. A prototype system is presented and test results are discussed.",https://ieeexplore.ieee.org/document/7502588/,2016 International Conference on Unmanned Aircraft Systems (ICUAS),7-10 June 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAMechS49982.2020.9310146,Realization of Robust Yo-yo Operation,IEEE,Conferences,"In this research, we aim to realize a robust yo-yo operation with a robot arm. For this purpose, we design a model predictive controller that provides input to the robot arm from predicted yo-yo states. Furthermore, we implement a controller that can be performed in real-time by using a neural network. Finally, the implemented controller realizes highly reproducible yo-yo operation by even with a usual disturbance.",https://ieeexplore.ieee.org/document/9310146/,2020 International Conference on Advanced Mechatronic Systems (ICAMechS),10-13 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2014.7090222,Reconfigurable robotic system based on mono-camera guidance,IEEE,Conferences,"The paper proposes an intelligent robotic system which is able to be (re)configured, at demand, for two deployment scenarios. a) The first task is to move the platform after a trajectory determined by the direction to a fixed point and avoid any obstacles occurring in the route. a) The second task is to identify and track a spherical object. The robot is equipped with a navigation system designed to maintain direction in case of interruption of video contact with the target (landmark), meaning when an obstacle interposes. It has two main subsystems: the mobile platform, which is equipped with a video camera and sensors for path correction, and the central processing system for the analysis of received information. The task control is based on extracted features from images. The communication between them is done via a wireless protocol. Algorithms for controlling the mobile platform are implemented on the embedded microcontroller and algorithms for image processing are implemented on the central system.",https://ieeexplore.ieee.org/document/7090222/,"Proceedings of the 2014 6th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",23-25 Oct. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAMechS49982.2020.9310129,Reinforcement Learning based Method for Autonomous Navigation of Mobile Robots in Unknown Environments,IEEE,Conferences,"The Reinforcement Learning is a subset of machine learning that deals with learning decisions from rewards given by the environment. The model classic reinforcement learning (RL) algorithms are usually applied to small sets of states and an action. However, in real applications, the state spaces are of a large scale and this will bring the problems in the generalization and the curse of dimensionality. In this research, authors integrate neural networks into reinforcement learning methods to generalize the value of all the states. The simulation results on the Gazebo software framework show the feasibility of the model proposed method algorithm. The robot can safely navigate an unprotected work environment and becomes a truly intelligent system with the ability to learn and adapt itself to the model.",https://ieeexplore.ieee.org/document/9310129/,2020 International Conference on Advanced Mechatronic Systems (ICAMechS),10-13 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCIA54998.2022.9737166,Reinforcement Learning based Sequential Controller for Mobile Robots with Obstacle Avoidance,IEEE,Conferences,"Obstacle avoidance and path planning play substantial roles in mobile robot applications. This paper has two main parts: first, an object detection algorithm based on YOLO-v4 with custom classes and depth camera in real-time has been implemented in Robot Operating System (ROS) to resolve robot obstacle avoidance issues. Then, controlling of the robot was discussed by dividing the path into three parts with their specific PID or PD controllers. The optimum parameters of each controller are then calculated by Reinforcement Learning (RL). For ensuring the precision of the obstacle avoidance method, the accuracy of distance is evaluated by the significant result of real thorough objects. Then for evaluating the controllers, the desired-and the traveled path were compared and the positional error was calculated.",https://ieeexplore.ieee.org/document/9737166/,"2022 8th International Conference on Control, Instrumentation and Automation (ICCIA)",2-3 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2006.314459,Reinforcement Learning with Human Teachers: Understanding How People Want to Teach Robots,IEEE,Conferences,"While reinforcement learning (RL) is not traditionally designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a robot a task through reinforcement learning: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback -possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. In conclusion, we discuss future extensions to RL to accommodate these lessons",https://ieeexplore.ieee.org/document/4107833/,ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication,6-8 Sept. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSRE.1993.624285,Reliability of uniprocessor and multiprocessor real-time artificial intelligence planning systems,IEEE,Conferences,"By real-time artificial intelligence (AI) planning systems, we mean those systems embedded in process-control systems that must plan and execute control strategies in response to external events within a real-time constraint. We propose a methodology for estimating the reliability of uniprocessor and multiprocessor real-time AI planning systems. We first discuss why there are intrinsic faults in AI planning programs that must be considered in the reliability modeling of real-time AI planning systems. Then, we show that for uniprocessor systems, no single planning algorithm can avoid all types of intrinsic faults. Finally, we investigate a multiprocessor architecture with parallel planning with the objective of reducing intrinsic faults of real-time AI planning systems and improving the reliability of embedded systems. A robot path-planning system in static domains is used as an example to illustrate our methodology.",https://ieeexplore.ieee.org/document/624285/,Proceedings of 1993 IEEE International Symposium on Software Reliability Engineering,3-6 Nov. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIID51893.2021.9456477,Research on Privacy Protection Technology in Face Identity Authentication System Based on Edge Computing,IEEE,Conferences,"In today's society, the rapid development of the Internet makes People's Daily life become more intelligent and diversified. Today's society has entered a multifaceted era where everything is interconnected. Artificial intelligence technology is gradually replacing some traditional human services, such as intelligent robot customer service instead of traditional human customer service, intelligent face scanning security check in railway stations instead of traditional manual ticket checking, unmanned supermarket automatic checkout has liberated some social labor costs. All these changes are the result of the development of artificial intelligence technology in today's society. In recent years, unicorn startups focused on biometrics have sprung up all around us, such as BTU and its MEG VII (Face ++). Thanks to the development of Internet and artificial intelligence technology, in many application fields, the traditional access control and identity authentication technology based on password verification is gradually transforming to the scheme based on biometric identification verification. Secure identity authentication is very important to the application of Internet. Face recognition is the most popular technology among all biometric identification technologies. In the field of biometric identification technology, it has become the most widely used technology in the field of identity authentication because of its unique non-invasive, support for infrared and visible light, no need for user cooperation and many other advantages. In the field of education, examinee identification, pedestrian identification detection at the entrance of railway stations, face electronic payment, intelligent video surveillance system, intelligent attendance and access control system, intelligent unmanned supermarkets and customs clearance ports become the pioneer fields of face recognition applications. It can be seen that the era of “national face brushing” has arrived, and the application of face recognition technology will only be more and more widespread in the current era and in the future. However, due to the sensitivity of biometric data and the heterogeneity and openness of network environment, the privacy leakage of biometric data is difficult to avoid. At present, fog computing and edge computing have been paid more and more attention in many fields. In the case that cloud service providers are unable to provide sufficient security, edge computing shows its advantages. In this paper, mobile edge computing is introduced for the first time into the face privacy protection identity authentication system based on cloud server outsourcing computing. It can not only greatly reduce the interaction frequency between users and cloud server, improve the availability and fault tolerance of the system, but also contribute to the implementation of privacy protection scheme. A deep constitutional neural network for face feature extraction is trained using deep learning framework Cafe. Cosine similarity is used to complete face verification. A privacy protection scheme based on the secure nearest neighbor algorithm is proposed, which can not only protect the security of the face feature data at the edge computing node, but also allow the edge computing node to complete the face recognition operation against the encrypted face feature data. In addition, the encryption scheme does not require large computing resources, and the accuracy of face recognition in cipher text is exactly the same as that in explain. At present, most of the solutions either have high computational complexity or poor security performance. How to reduce the computational complexity and improve the real-time performance of the system while ensuring the high security of the private data has important research significance and value. Therefore, in the cloud server outsourcing computing environment, how to complete biometric identification on the premise of protecting the privacy of biological data has become a research hot spot.",https://ieeexplore.ieee.org/document/9456477/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSCE.2015.7482163,Review on simultaneous localization and mapping (SLAM),IEEE,Conferences,"Simultaneous localization and mapping (SLAM) is a technique applied in artificial intelligence mobile robot for a self-exploration in numerous geographical environment. SLAM becomes fundamental research area in recent days as it promising solution in solving most of problems which related to the self-exploratory oriented artificial intelligence mobile robot field. For example, the capability to explore without any prior knowledge on environment it explores and without any human interference. The unique feature in SLAM is that the process of mapping and localization is done concurrently and recursively. Since SLAM introduction, many SLAM algorithms have been proposed to apply SLAM technique in real practice. The aim of this paper is to provide an insightful review on information background, recent development, feature, implementation and recent issue in SLAM.",https://ieeexplore.ieee.org/document/7482163/,"2015 IEEE International Conference on Control System, Computing and Engineering (ICCSCE)",27-29 Nov. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC.2019.8833453,Road Garbage Cleaning Device Based on ZigBee Gateway and Image Recognition,IEEE,Conferences,"The existing road garbage automatic cleaning device has problems such as low cleaning efficiency, difficulty in management, low accuracy of information transmission and garbage identification, and cannot meet actual needs well. A road garbage cleaning device based on ZigBee gateway and image recognition is designed, which consist of power module, camera pan/tilt module, robot arm module, ZigBee gateway module, GPS module, MCU control module and vehicle model module. The road debris is identified and analyzed by the camera pan/tilt module together with the RBF neural network algorithm, and then the device can automatically go to the target location and clean up garbage by robotic arm; The networking characteristics of the ZigBee gateway module are used to collect information such as positioning and moving speed of each device node in real time. In addition, the APP software or server can view road image information and device information, analyze relevant data, and provide convenience for real-time monitoring and management of road garbage cleaning work.",https://ieeexplore.ieee.org/document/8833453/,2019 Chinese Control And Decision Conference (CCDC),3-5 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII46433.2020.9026297,Robotic Grasping Using Semantic Segmentation and Primitive Geometric Model Based 3D Pose Estimation,IEEE,Conferences,"Due to the rapid development of hardware and software technologies, the machine automation has greatly improved in the past few decades. Robotic manipulators have been used in many factories for fully automated production. Compared with traditional sensor-based robot control, visual servo control has several advantages: high flexibility and precision, and robustness to calibration errors. In this paper, we present an eye-in-hand robotic arm for object recognition and grasping applications. The 2D images and depth information are acquired with an RGB-D camera for instance segmentation using convolutional neural network. The object pose estimation is achieved by ICP using primitive geometric models. Experiments carried out with different categories of objects in the real scene environment have demonstrated the feasibility of the proposed technique.",https://ieeexplore.ieee.org/document/9026297/,2020 IEEE/SICE International Symposium on System Integration (SII),12-15 Jan. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2016.7844571,Robotic attention manager using fuzzy controller with fractal analysis,IEEE,Conferences,"This paper is focused on the application of fractal analysis in the attention management of humanoid robot. We designed a fuzzy controller to combine the face detection, movement detection and the fractal dimension signals to control the head movement of robot Nao. Also, the gaze problem is addressed by the controller. Implementation details are included in the paper, including configuration parameters, which we found optimal according to subjective analysis and possibilities of current hardware. We found the fuzzy controller to be advantageous for implementation of attention manager because of smoothing of the movement of robot when compared to the simple rule based implementation, and also because the fuzzy controller implementation of manager is more clear than a naive if-then heuristics code. We also found the fractal dimension to be useful additional signal for attention management of robot, which can be computed in near real-time on current hardware and static input images.",https://ieeexplore.ieee.org/document/7844571/,"2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",9-12 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1993.298695,Robotic modeling and control using a fuzzy neural network,IEEE,Conferences,"A fuzzy neural network (FNN) is applied to modeling and control of a robot. Comparisons are made between the FNN and standard back propagation neural networks, as well as commercially available neural network software packages for modeling the robot. Observations on the robustness of these networks are presented. A number of experiments demonstrate that the FNN can learn faster and more accurately than the back propagation and commercial neural networks for modeling and control of a real robot.<>",https://ieeexplore.ieee.org/document/298695/,IEEE International Conference on Neural Networks,28 March-1 April 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/C-CODE.2017.7918964,Robotic navigation based on logic-based planning,IEEE,Conferences,"Logic and Planning are interesting artificial intelligence problems in the context of robotic systems, i.e., robotic navigation. For such an autonomous system one of the requisites is that the goal has to be achieved without intervention of human being. We present a practical implementation of autonomous robotic navigation based on logic-based planning. We achieve this by using strength of PROLOG in order to generate plan to reach goal position from an initial. We utilize First Order Logic (FOL) that automatically asserts and retracts facts at runtime dynamically. All possible plans are computed using local search strategies (e.g., Depth and Breadth First) on state space representing a real, dynamic, and unpredictable environment. In order to navigate in the environment following optimized plan - one with fewest states, a balanced size 4-wheel differential drive robot has been carefully constructed. It can turn 90° and actuate forward by controlling linear (νt = 0.25m/s) and angular (ωt = Π/8 rad/s) velocities of two rear motorized wheels. It is also equipped with an Ultrasonic sensor to avoid collision with obstacles. The system is evaluated in an environment comprising of corridors with adjacent rooms. Graphical User Interface (GUI) is developed in .Net (C#) to map situation in Prolog and transmit plan to hardware for execution. Average time calculated for a plan to generate is 0.065 seconds. The robot moves block by block where each block in the state space represents 2m2 area. In addition to minors, our major contribution is that we offer a unified scheme for robotic navigation without calculating odometry data with the assumption the robot cannot be kidnapped nor slipped.",https://ieeexplore.ieee.org/document/7918964/,"2017 International Conference on Communication, Computing and Digital Systems (C-CODE)",8-9 March 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00844,Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments,IEEE,Conferences,"Localizing the camera in a known indoor environment is a key building block for scene mapping, robot navigation, AR, etc. Recent advances estimate the camera pose via optimization over the 2D/3D-3D correspondences established between the coordinates in 2D/3D camera space and 3D world space. Such a mapping is estimated with either a convolution neural network or a decision tree using only the static input image sequence, which makes these approaches vulnerable to dynamic indoor environments that are quite common yet challenging in the real world. To address the aforementioned issues, in this paper, we propose a novel outlier-aware neural tree which bridges the two worlds, deep learning and decision tree approaches. It builds on three important blocks: (a) a hierarchical space partition over the indoor scene to construct the decision tree; (b) a neural routing function, implemented as a deep classification network, employed for better 3D scene understanding; and (c) an outlier rejection module used to filter out dynamic points during the hierarchical routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark developed for camera relocalization in dynamic indoor environments. It achieves robust neural routing through space partitions and outperforms the state-of-the-art approaches by around 30% on camera pose accuracy, while running comparably fast for evaluation.",https://ieeexplore.ieee.org/document/9577932/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC.2018.8430770,Safe Reinforcement Learning: Learning with Supervision Using a Constraint-Admissible Set,IEEE,Conferences,"Despite recent advances in Reinforcement Learning (RL), its applications in real-world engineering systems are still rare. The primary reason is that RL algorithms involve exploratory actions that can lead to system constraint violations. These violations can damage physical systems and even cause safety issues, e.g., battery overheat, robot breakdown, and car crashes, hindering RL deployment in many engineering applications. In this paper, we develop a novel safe RL framework that guarantees safety during learning by exploiting a constraint-admissible set for supervision. System knowledge and recursive feasibility techniques are exploited to construct a state-dependent constraint-admissible set. We develop a new learning scheme where the constraint-admissible set regulates the exploratory actions from the RL agent and simultaneously guides the agent to learn the system constraints with a penalty for control regulation. The proposed safe RL algorithm is demonstrated in an adaptive cruise control example where a nonlinear fuel economy cost function is optimized without violating system constraints. We demonstrate that the safe RL agent is able to learn the system constraints to gradually fade out the control supervisor.",https://ieeexplore.ieee.org/document/8430770/,2018 Annual American Control Conference (ACC),27-29 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636440,Seeing All the Angles: Learning Multiview Manipulation Policies for Contact-Rich Tasks from Demonstrations,IEEE,Conferences,"Learned visuomotor policies have shown considerable success as an alternative to traditional, hand-crafted frameworks for robotic manipulation. Surprisingly, an extension of these methods to the multiview domain is relatively unexplored. A successful multiview policy could be deployed on a mobile manipulation platform, allowing the robot to complete a task regardless of its view of the scene. In this work, we demonstrate that a multiview policy can be found through imitation learning by collecting data from a variety of viewpoints. We illustrate the general applicability of the method by learning to complete several challenging multi-stage and contact-rich tasks, from numerous viewpoints, both in a simulated environment and on a real mobile manipulation platform. Furthermore, we analyze our policies to determine the benefits of learning from multiview data compared to learning with data collected from a fixed perspective. We show that learning from multiview data results in little, if any, penalty to performance for a fixed-view task compared to learning with an equivalent amount of fixed-view data. Finally, we examine the visual features learned by the multiview and fixed-view policies. Our results indicate that multiview policies implicitly learn to identify spatially correlated features.",https://ieeexplore.ieee.org/document/9636440/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSSE.2008.1256,Segmentation Methods of Fruit Image and Comparative Experiments,IEEE,Conferences,"Fruit image segmentation issue on color difference between mature fruits and backgrounds under natural illumination condition is an important and difficult content of fruit-harvesting robot vision. Some studies concerning fruit image segmentation have been presented in the last few years. However, these studies are focused on particular fruit and different from segmentation results. In this paper, four kinds of segmentation methods are presented and applied into fruit image segmentation. The tests show that these methods can segment successful several kinds of fruits image, such as apple, tomato, strawberry, persimmon and orange. Dynamic threshold segmentation method has better performance and least cost time than extended Otsu method, improved Otsu combined with genetic arithmetic and adaptive segmentation method based on LVQ network. Meanwhile, it has satisfactory effect upon fruit image under natural illumination condition. Adaptive segmentation method based on LVQ network can only be applied into balanced color instance of particular fruit, and it isnpsilat adapt to be applied into real-time occasion because of high cost time.",https://ieeexplore.ieee.org/document/4721944/,2008 International Conference on Computer Science and Software Engineering,12-14 Dec. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.1993.633961,Self-adjusting real-time search: a summary of results,IEEE,Conferences,"Real-time search algorithms need to address the deadlines imposed by applications like process control and robot navigation. Possible deadline violations should be predicted ahead of time to allow remedial actions to prevent the undesirable consequences of missing deadlines. The algorithms should also demonstrate progressively optimizing behavior. That is, they should improve the quality of the solutions as time constraints are relaxed. To successfully address these issues, a real-time search algorithm must address the central problem of choosing the proper values for its parameters, which control the time allocated to planning. The authors propose a new approach to determine the parameter values of a real-time search algorithm, in order to enable the algorithm to meet deadlines, exhibit progressively optimizing behavior, and to predict deadline violation prior to the deadline. They provide a theoretical and experimental characterization of the proposed algorithm.",https://ieeexplore.ieee.org/document/633961/,Proceedings of 1993 IEEE Conference on Tools with Al (TAI-93),8-11 Nov. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPA.1994.636137,Self-organizing map for reinforcement learning: obstacle-avoidance with Khepera,IEEE,Conferences,We present a self-organizing map implementation of the Q-learning algorithm. Our goal is to overcome the problems of reinforcement learning: memory requirement and generalization. We consider the map as an associative memory and we use it for obstacle avoidance with the mobile robot Khepera. Results allow real world applications to be envisaged using neural reinforcement learning.,https://ieeexplore.ieee.org/document/636137/,Proceedings of PerAc '94. From Perception to Action,7-9 Sept. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9340814,SelfieDroneStick: A Natural Interface for Quadcopter Photography,IEEE,Conferences,"A physical selfie stick extends the user's reach, enabling the acquisition of personal photos that include more of the background scene. Similarly, a quadcopter can capture photos from vantage points unattainable by the user; but teleoperating a quadcopter to good viewpoints is a difficult task. This paper presents a natural interface for quadcopter photography, the SelfieDroneStick that allows the user to guide the quadcopter to the optimal vantage point based on the phone's sensors. Users specify the composition of their desired long-range selfies using their smartphone, and the quadcopter autonomously flies to a sequence of vantage points from where the desired shots can be taken. The robot controller is trained from a combination of real-world images and simulated flight data. This paper describes two key innovations required to deploy deep reinforcement learning models on a real robot: 1) an abstract state representation for transferring learning from simulation to the hardware platform, and 2) reward shaping and staging paradigms for training the controller. Both of these improvements were found to be essential in learning a robot controller from simulation that transfers successfully to the real robot.",https://ieeexplore.ieee.org/document/9340814/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KIMAS.2003.1245110,Sharing learning policies between multiple mobile robots,IEEE,Conferences,"Learning of a complex task usually requires a long learning period. In order to reduce the time of learning, the task is divided into several subtasks. Multiple agents can be used to serve a complex task by learning these subtasks concurrently. With a good knowledge sharing mechanism, the learning policy can be shared or exchanged among these agents and can enhance their learning efficiency. The learning policy is a mapping from system states to actions. The mechanism of sharing or exchanging learning knowledge among multiagent system is proposed. An index of expertise, which indicates the skill level of each learning agent, is presented. This index is used to select the best preferable advice among multiple advices, which can increase the probability of finding solution in the search space. The experiment in which the learning knowledge is exchanged between a mobile robot and a computer simulated agent is implemented in order to verify the validity of the proposed algorithm. The experimental results show that the learning efficiency of the advisor agent is increased and the advisee robot can use the given advice for avoiding collision with obstacle successfully in the real world implementation.",https://ieeexplore.ieee.org/document/1245110/,IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change (IEEE Cat. No.03CH37502),30 Sept.-4 Oct. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197512,Sim-to-Real Transfer for Optical Tactile Sensing,IEEE,Conferences,"Deep learning and reinforcement learning methods have been shown to enable learning of flexible and complex robot controllers. However, the reliance on large amounts of training data often requires data collection to be carried out in simulation, with a number of sim-to-real transfer methods being developed in recent years. In this paper, we study these techniques for tactile sensing using the TacTip optical tactile sensor, which consists of a deformable tip with a camera observing the positions of pins inside this tip. We designed a model for soft body simulation which was implemented using the Unity physics engine, and trained a neural network to predict the locations and angles of edges when in contact with the sensor. Using domain randomisation techniques for sim-to-real transfer, we show how this framework can be used to accurately predict edges with less than 1 mm prediction error in real-world testing, without any real-world data at all.",https://ieeexplore.ieee.org/document/9197512/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS-SBR-WRE48964.2019.00060,Sim-to-Real in Reinforcement Learning for Everyone,IEEE,Conferences,"In reinforcement learning (RL), it remains a challenge to have a robotic agent perform a task in the real world for which it was trained in simulation. In this paper, we present our work training a low-cost robotic arm in simulation to move towards a predefined target in space, represented by a red ball in an RGB image, and transferring the capability to the real arm. We exercised the entire end-to-end flow including the 3D modeling of the arm, training of a state-of-the-art RL policy in simulation with multiple actors in a distributed fashion, domain randomization in order to close the sim-to-real gap, and finally the execution of the trained model in the real robot. We also implemented a mechanism to edit the image captured from the camera before sending it to the model for inference, which allowed us to automate reward computation in the physical world. Our work highlights important challenges of training RL agents and moving them to the real world, validating important aspects shown by other works as well as detailing steps not explained by some of them (e.g. how to compute the reward in the real world). The conducted experiments show the improvements observed as the techniques were added to the final solution.",https://ieeexplore.ieee.org/document/9018558/,"2019 Latin American Robotics Symposium (LARS), 2019 Brazilian Symposium on Robotics (SBR) and 2019 Workshop on Robotics in Education (WRE)",23-25 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC44109.2020.9176182,Simple Kinematic Feedback Enhances Autonomous Learning in Bio-Inspired Tendon-Driven Systems,IEEE,Conferences,"Error feedback is known to improve performance by correcting control signals in response to perturbations. Here we show how adding simple error feedback can also accelerate and robustify autonomous learning in a tendon-driven robot. We have implemented two versions of the General-to-Particular (G2P) autonomous learning algorithm using a tendon-driven leg with two joints and three tendons: one with and one without real-time kinematic feedback. We have performed a rigorous study on the performance of each system, for both simulation and physical implementation cases, over a wide range of tasks. As expected, feedback improved performance in simulation and hardware. However, we see these improvements even in the presence of sensory delays of up to 100 ms and when experiencing substantial contact collisions. Importantly, feedback accelerates learning and enhances G2P's continual refinement of the initial inverse map by providing the system with more relevant data to train on. This allows the system to perform well even after only 60 seconds of initial motor babbling.",https://ieeexplore.ieee.org/document/9176182/,2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),20-24 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBAIE52039.2021.9390015,Simulation of underwater vehicle control based on code generation technology,IEEE,Conferences,"Model-based design is an effective means for rapid development of embedded software, and automatic code generation is an important technology for model-based development. Combining the automatic code generation method of Matlab and STM32 with the operation and control of the autonomous underwater robot makes the design of the system more convenient. Use tools such as the Simulink library STM32 MAT/Target and STM32 CubeMX of the STM32 microcontroller to realize the automatic generation of readable and portable C code project files. At the same time, based on the design of the model, the control code of the autonomous underwater robot(AUV) is automatically generated, and the control code is added to the automatically generated C code project file. With Matlab/Simulink as the basic software platform, the motion controller of AUV is mounted on the STM32F407, and a real-time simulation system for the closed-loop control of AUV manipulation motion is constructed. The results of the semiphysical real-time simulation test show that the AUV motion controller has good heading depth control performance, realizes the manipulation and control of AUV, and verifies the practicability of the automatically generated code.",https://ieeexplore.ieee.org/document/9390015/,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",26-28 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1990.262374,"Single leg walking with integrated perception, planning and control",IEEE,Conferences,"Describes an integrated system capable of walking over rugged terrain using a single leg suspended below a carriage that rolls along rails. To walk, the system uses a laser scanner to find a foothold, positions the leg above the foothold, contacts the terrain with the foot, and applies force enough to advance the carriage along the rails. Walking both forward and backward, the system has traversed hundreds of meters of rugged terrain including obstacles too tall to step over, trenches too deep to step in, closely spaced rocks, and sand hills. The implemented system consists of a number of task-specific processes (two for planning, two for perception, one for real-time control) and a central control process that directs the flow of communication between processes. Implementing this integrated system is a significant step toward the goal of the CMU Planetary Rover project: to prototype a autonomous six-legged robot for planetary exploration.<>",https://ieeexplore.ieee.org/document/262374/,"EEE International Workshop on Intelligent Robots and Systems, Towards a New Frontier of Applications",3-6 July 1990,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636018,Smart Pointers and Shared Memory Synchronisation for Efficient Inter-process Communication in ROS on an Autonomous Vehicle,IEEE,Conferences,"Despite the stringent requirements of a real-time system, the reliance of the Robot Operating System (ROS) on the loopback network interface imposes a considerable overhead on the transport of high bandwidth data, while the nodelet package, which is an efficient mechanism for intra-process communication, does not address the problem of efficient local inter-process communication (IPC). To remedy this, we propose a novel integration into ROS of smart pointers and synchronisation primitives stored in shared memory. These obey the same semantics and, more importantly, exhibit the same performance as their C++ standard library counterparts, making them preferable to other local IPC mechanisms. We present a series of benchmarks for our mechanism - which we call LOT (Low Overhead Transport) - and use them to assess its performance on realistic data loads based on Five’s Autonomous Vehicle (AV) system, and extend our analysis to the case where multiple ROS nodes are running in Docker containers. We find that our mechanism performs up to two orders of magnitude better than the standard IPC via local loopback. Finally, we apply industry-standard profiling techniques to explore the hotspots of code running in both user and kernel space, comparing our implementation against alternatives.",https://ieeexplore.ieee.org/document/9636018/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2003.1259766,Spatio-temporal representation for multi-dimensional occlusion relation,IEEE,Conferences,"Occlusion relation is the topological relation between the images of two bodies from a viewpoint. Qualitative representation of occlusion relation has been investigated in qualitative spatial reasoning. This research is important for computer vision and robot navigation. The previous models such as LOS and ROC-20 are all based on RCC (the famous topological theory). But those models couldn't support abstract objects such as point and line which are very common in real applications. To deal with this, multi-dimensional spatial occlusion relation (MSO) is put forward. The foundation of MSO is MRCC which is the multi-dimensional extension of RCC. So MSO is suitable for both real and abstract objects. The conception neighborhood and composition of MSO is given. Finally MSO is extended to spatio-temporal relation by adding time feature. MSO is an appropriate frame to express spatio-temporal knowledge.",https://ieeexplore.ieee.org/document/1259766/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR52253.2021.9494646,Splash on ROS 2: A Runtime Software Framework for Autonomous Machines,IEEE,Conferences,"ROS has been completely refactored and evolved into ROS 2 to address the ever-increasing software complexity of autonomous machines. While it has become a de facto standard software platform for autonomous machines, ROS 2 still has room for improvement. It lacks support for essential features such as real-time stream processing, mode change, sensor fusion and rate control for output shaping. Moreover, its programming model is at a lower level than what most programmers would expect.In this paper, we carefully analyze the shortcomings of ROS 2 and propose to augment it with the Splash programming framework. In doing so, we host Splash on top of the ROS 2 software stack by conducting model conversion between Splash and ROS 2. We refer to the end result as Splash on ROS 2. To show its viability, we conducted a case study with a robot arm controller performing DNN-based object detection and motion planning. The case study qualitatively confirms that Splash on ROS 2 relieves the programming burden on developers, increases the software development productivity and improves the quality of the software.",https://ieeexplore.ieee.org/document/9494646/,2021 18th International Conference on Ubiquitous Robots (UR),12-14 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCS48598.2019.9188104,Staged deployment of interactive multi-application HPC workflows,IEEE,Conferences,"Running scientific workflows on a supercomputer can be a daunting task for a scientific domain specialist. Workflow management solutions (WMS) are a standard method for reducing the complexity of application deployment on high performance computing (HPC) infrastructure. We introduce the design for a middleware system that extends and combines the functionality from existing solutions in order to create a high-level, staged usercentric operation/deployment model. This design addresses the requirements of several use cases in the life sciences, with a focus on neuroscience. In this manuscript we focus on two use cases: 1) three coupled neuronal simulators (for three different space/time scales) with in-transit visualization and 2) a closed-loop workflow optimized by machine learning, coupling a robot with a neural network simulation. We provide a detailed overview of the application-integrated monitoring in relationship with the HPC job. We present here a novel usage model for large scale interactive multi-application workflows running on HPC systems which aims at reducing the complexity of deployment and execution, thus enabling new science.",https://ieeexplore.ieee.org/document/9188104/,2019 International Conference on High Performance Computing & Simulation (HPCS),15-19 July 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1996.506888,Stereo sketch: stereo vision-based target reaching behavior acquisition with occlusion detection and avoidance,IEEE,Conferences,"In this paper, we proposed a method by which a stereo vision-based mobile robot learns to reach a target by detecting and avoiding occlusions. We call the internal representation that describes the learning behavior ""stereo sketch"". First, an input scene is segmented into homogeneous regions by the enhanced ISODATA algorithm with minimum description length principle in terms of image coordinates and disparity information obtained from the fast stereo matching unit based on the coarse-to-fine control method. Then, in terms of the segmented regions including the target area and their occlusion status identified during the stereo and motion disparity estimation process, we construct a state space for the reinforcement learning method to obtain a target reaching behavior. As a result the robot can avoid obstacles without explicitly describing them. We give the computer simulation results and real robot implementation to show the validity of our method.",https://ieeexplore.ieee.org/document/506888/,Proceedings of IEEE International Conference on Robotics and Automation,22-28 April 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTAS48715.2020.00-20,SubFlow: A Dynamic Induced-Subgraph Strategy Toward Real-Time DNN Inference and Training,IEEE,Conferences,"We introduce SubFlow-a dynamic adaptation and execution strategy for a deep neural network (DNN), which enables real-time DNN inference and training. The goal of SubFlow is to complete the execution of a DNN task within a timing constraint that may dynamically change while ensuring comparable performance to executing the full network by executing a subset of the DNN at run-time. To this end, we propose two online algorithms that enable SubFlow: 1) dynamic construction of a sub-network which constructs the best subnetwork of the DNN in terms of size and configuration, and 2) time-bound execution which executes the sub-network within a given time budget either for inference or training. We implement and open-source SubFlow by extending TensorFlow with full compatibility by adding SubFlow operations for convolutional and fully-connected layers of a DNN. We evaluate SubFlow with three popular DNN models (LeNet-5, AlexNet, and KWS), which shows that it provides flexible run-time execution and increases the utility of a DNN under dynamic timing constraints, e.g., lx-6.7x range of dynamic execution speed with average -3% of performance (inference accuracy) difference. We also implement an autonomous robot as an example system that uses SubFlow and demonstrate that its obstacle detection DNN is flexibly executed to meet a range of deadlines that varies depending on its running sped.",https://ieeexplore.ieee.org/document/9113121/,2020 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),21-24 April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2003.1223978,Systemic intelligence: methods for growing up artefacts that live,IEEE,Conferences,"The ideas of systemic intelligence provide a set of methodologies and paradigms that are, beside other advantages, suitable for constructing control systems that are capable of growing up. In particular the promising methods of systemic architecture, schedule of structural development, memory organization and rules for learning and adaptation are presented and discussed with respect to grow up an artifact. Of special interest is the concept of growth in the sense of growing up from a kind of infantile stage to a fully matured entity. To grow up an artifact from an infantile stage via a sequence of learned abilities to,a fully matured entity is still a feature of life not yet sufficiently transposed onto technical systems. To enable the capability to grow up artifacts, a set of methodologies and principles is presented in this paper. The developed methodologies are already implemented into physically existing test beds that operate, adapt (and grow up) in real time and in the real world to prove that the proposed approach is feasible under real conditions. Two realizations (robot control, audio signal processing) of a systemic architecture for an up-growing system are presented in this paper.",https://ieeexplore.ieee.org/document/1223978/,"Proceedings of the International Joint Conference on Neural Networks, 2003.",20-24 July 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC48633.2019.8996630,Target Recognition and 3D Pose Estimation Based on Prior Knowledge and Convolutional Neural Network for Robots,IEEE,Conferences,"In the competition of RoboMaster, the robot needs to trigger the target, the “Energy Mechanism” which consists of nine different dynamic flame numbers, in a nine square area by shooting projectiles. Therefore, 3D target detection should be implemented including target recognition and 3D pose estimation in real-time. As the targets are dynamic flame numbers and quite small in the whole image, it increases the difficulty to detect. The robot should achieve to shoot the target in multi-angle and multi-scale to adjust the competition. To address these issues, we propose a fast and accurate method to detect all nine numbers and estimate each 3D pose based on prior knowledge and convolutional neural network only by a monocular camera. The geometric constraints around the target are employed as prior knowledge when estimating the target pose. Then, we utilize the relative position information to detect the region of each dynamic number in the image, which is recognized by a convolutional neural network trained by flame numbers. Experiments in the actual environment show that our method can achieve the detection of each dynamic number in real-time and high accuracy. The runtime is 29ms on average (about 11ms in detection and 18ms in recognition) and the recognition accuracy is about 94.69%. And our method wins the first place in the technical challenge of 2018 RoboMaster competition.",https://ieeexplore.ieee.org/document/8996630/,2019 Chinese Automation Congress (CAC),22-24 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.1997.613854,Task decomposition and dynamic policy merging in the distributed Q-learning classifier system,IEEE,Conferences,A distributed reinforcement learning system is designed and implemented on a mobile robot for the study of complex task decomposition and dynamic policy merging in real robot learning environments. The distributed Q-learning classifier system (DBLCS) is evolved from the standard LCS proposed by Holland (1996). We address two of the limitations of the LCS through the use of Q-learning as the apportionment of credit component and a distributed learning architecture to facilitate complex task decomposition. The Q-learning update equation is derived and its advantages over the complex bucket brigade algorithm (BBA) are discussed. Holistic and monolithic shaping approaches are used to distribute reward among the learning modules of the DBLCS and allow dynamic policy merging in a variety of real robot learning experiments.,https://ieeexplore.ieee.org/document/613854/,Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation',10-11 July 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EIT51626.2021.9491894,Teaching Vehicles to Steer Themselves with Deep Learning,IEEE,Conferences,Traditional approaches for steering a vehicle using machine vision require large amounts of robust hand-crafted software which is both time consuming and expensive. The presented method uses a deep neural network to teach cars to steer themselves without any additional software. We created a labeled dataset for the ACTor (Autonomous Campus TranspORt) electric vehicle by pairing real world images taken during a drive with the associated steering wheel angle. We trained a model end to end using modern deep learning techniques including convolutional neural networks and transfer learning to automatically detect relevant features in the input and provide a predicted output. This means that no traditional hand engineered algorithm features were required for this implementation. We currently use an pretrained inception network on the ImageNet dataset to leverage the high level features learned from ImageNet to the steering problem through transfer learning. We removed the top portion of the network and replaced it with a linear regression node to provide the output. The model is trained end to end using backpropagation. The trained model is integrated with vehicle software on ROS (Robot Operating System) to read image data and provide a corresponding steering angle in real time. The current model achieves 15.2 degree error on average. As development continues the model may replace the current lane centering software and will be used for IGVC Self-Drive competition and campus transportation.,https://ieeexplore.ieee.org/document/9491894/,2021 IEEE International Conference on Electro Information Technology (EIT),14-15 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEKIM52309.2021.00100,The Creation of Multi Intelligence Music Classroom in Children's Enlightenment Stage Based on Virtual Reality Technology,IEEE,Conferences,"With the development of virtual reality technology, the real realization of virtual reality will cause great changes in human life and development. VR is the abbreviation of virtual reality, which means virtual reality in Chinese. Virtual reality is the ultimate application form of multimedia technology. It is the crystallization of the rapid development of computer hardware and software technology, sensor technology, robot technology, artificial intelligence and behavioral psychology. Because of this, the combination of VR and education classroom will become the inevitable trend of future education development. In the context of the development of traditional education, the enlightenment and influence of music education classroom on children is point like, and through the intervention and influence of VR technology, the influence of VR on music education begins to appear face like, and can maximize the mental and thinking ability of preschool children. Through the above analysis, this paper concludes the feasibility and importance of introducing VR technology into the current music education classroom, and will explore for the development of multiple intelligence education.",https://ieeexplore.ieee.org/document/9479537/,"2021 2nd International Conference on Education, Knowledge and Information Management (ICEKIM)",29-31 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.1995.496802,The REAKT project: environment and methodology for the development of real-time knowledge-based systems,IEEE,Conferences,"This paper outlines the main results of the REAKT and REAKT II ESPRIT projects (EP 5146 and 7805). The primary objective of those projects was to develop a set of tools and the associated methodology for applying knowledge-based systems in real-time domains. Two REAKT applications which we think are typical of the kind of real-time knowledge-based systems that can be developed with REAKT are presented. The first application is an online alarm management system which has been running for about one year in a large oil refinery in Italy. This application was developed within the REAKT project to act as a demonstrator for the project results. The second application is a prototype mobile robot system, currently being developed at CRIN-INRIA.",https://ieeexplore.ieee.org/document/496802/,Proceedings 1995 INRIA/IEEE Symposium on Emerging Technologies and Factory Automation. ETFA'95,10-13 Oct. 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VECIMS.2009.5068923,The hand shape recognition of Human Computer Interaction with Artificial Neural Network,IEEE,Conferences,"The hand gestures used in Human Computer Interaction (HCI) are generally posed by complicated and large amplitude actions of arm /hand. Thus usable HCI instructions are few and HCI efficiency is low. This paper presents new hand shapes and the corresponding recognition system for the HCI with robot or Coordinate Measuring Machine. Using a touch pad to precept the touching of fingers, hand shapes posed to express HCI instructions are defined by the combinations of 2 binary status, i.e. status of touching /detaching on touch pad and status of stretching /retracting over touch pad, of Index, Middle, Ring and Little fingers. Method of extracting the features in hand shape image is presented. Based on Neural Network, a decision binary tree is used in the real-time recognition of the hand shapes. A correctness ratio of about 95% is obtained when implemented by DSP processor in the recognition of 12 hand shapes.",https://ieeexplore.ieee.org/document/5068923/,"2009 IEEE International Conference on Virtual Environments, Human-Computer Interfaces and Measurements Systems",11-13 May 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSESS.2015.7339119,The research and implementation of artificial intelligence in mobile applications,IEEE,Conferences,"This paper designs and implements the bionic robot which is as an implementation of soldiers in the strategy game, and the robot has three parts: perception system, target and path system and decision-making system. The perception system is responsible for perceiving information inside the scene; the target and path system is to find the best position for attacking and the optimal path for the robot; the decision-making system determines the behavior of the robot in the next frame. This paper also introduces map information updated in real time in the game. The bionic robot system designed has a good expansibility, and soldiers of different arms using this system, the game is running well.",https://ieeexplore.ieee.org/document/7339119/,2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS),23-25 Sept. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2003.1251856,The role that the internal model of the others plays in cooperative behavior,IEEE,Conferences,"Internal model of the others is claimed to be essential for the prediction of the others' behavior to realize a cooperative behavior. ""Theory of mind"" is known as a leading framework of the internal modeling. The present research consists of two parts. In the former part, experimental results are shown on the cooperation between a human subject and a software model of a mobile robot. The cooperative task given to the human subject is to carry a stick from a start point to a goal point by holding the one edge of the stick, in a simulation world displayed on a computer terminal. A virtual robot holds the other edge, whose behavior is controlled by a simple deterministic rule. The human subject is asked to perform the task under the following two different conditions A and B. In condition A, the subject is not told about the existence of the robot which holds the other edge, and only the stick is displayed. In condition B, the existence of the robot is told, but still not shown on the display. The performance of the carrying task is much better in the condition A, under which the subject can separate the movements of the stick and of the robot for their prediction. This result suggests that a construction of a behavior model of the other robot is helpful to carry out the cooperative task. In the latter part, a neural network replaces the human subject to perform the same cooperative task with the robot. The network outputs a motion to move the stick, and has a simple layered structure with an additional part to learn to predict the next motion of the robot. The experiments show the improvement in the task performance through learning the prediction. This also suggests the explicit modeling of the others is effective for the cooperation.",https://ieeexplore.ieee.org/document/1251856/,"The 12th IEEE International Workshop on Robot and Human Interactive Communication, 2003. Proceedings. ROMAN 2003.",2-2 Nov. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2016.7793038,Tool compensation in walk-through programming for admittance-controlled robots,IEEE,Conferences,"This paper describes a walk-through programming technique, based on admittance control and tool dynamics compensation, to ease and simplify the process of trajectory learning in common industrial setups. In the walk-through programming, the human operator grabs the tool attached at the robot end-effector and “walks” the robot through the desired positions. During the teaching phase, the robot records the positions and then it will be able to interpolate them to reproduce the trajectory back. In the proposed control architecture, the admittance control allows to provide a compliant behavior during the interaction between the human operator and the robot end-effector, while the algorithm of compensation of the tool dynamics allows to directly use the real tool in the teaching phase. In this way, the setup used for the teaching can directly be the one used for performing the reproduction task. Experiments have been performed to validate the proposed control architecture and a pick and place example has been implemented to show a possible application in the industrial field.",https://ieeexplore.ieee.org/document/7793038/,IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,23-26 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASAP.2018.8445099,Towards Hardware Accelerated Reinforcement Learning for Application-Specific Robotic Control,IEEE,Conferences,"Reinforcement Learning (RL) is an area of machine learning in which an agent interacts with the environment by making sequential decisions. The agent receives reward from the environment based on how good the decisions are and tries to find an optimal decision-making policy that maximises its longterm cumulative reward. This paper presents a novel approach which has showon promise in applying accelerated simulation of RL policy training to automating the control of a real robot arm for specific applications. The approach has two steps. First, design space exploration techniques are developed to enhance performance of an FPGA accelerator for RL policy training based on Trust Region Policy Optimisation (TRPO), which results in a 43% speed improvement over a previous FPGA implementation, while achieving 4.65 times speed up against deep learning libraries running on GPU and 19.29 times speed up against CPU. Second, the trained RL policy is transferred to a real robot arm. Our experiments show that the trained arm can successfully reach to and pick up predefined objects, demonstrating the feasibility of our approach.",https://ieeexplore.ieee.org/document/8445099/,"2018 IEEE 29th International Conference on Application-specific Systems, Architectures and Processors (ASAP)",10-12 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197446,Towards Plan Transformations for Real-World Mobile Fetch and Place,IEEE,Conferences,"In this paper, we present an approach and an implemented framework for applying plan transformations to real-world mobile manipulation plans, in order to specialize them to the specific situation at hand. The framework can improve execution cost and achieve better performance by autonomously transforming robot's behavior at runtime. To demonstrate the feasibility of our approach, we apply three example transformations to the plan of a PR2 robot performing simple table setting and cleaning tasks in the real world. Based on a large amount of experiments in a fast plan projection simulator, we make conclusions on improved execution performance.",https://ieeexplore.ieee.org/document/9197446/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV51971.2022.9827355,Towards Real-time Traffic Sign and Traffic Light Detection on Embedded Systems,IEEE,Conferences,"Recent work done on traffic sign and traffic light detection focus on improving detection accuracy in complex scenarios, yet many fail to deliver real-time performance, specifically with limited computational resources. In this work, we propose a simple deep learning based end-to-end detection framework, which effectively tackles challenges inherent to traffic sign and traffic light detection such as small size, large number of classes and complex road scenarios. We optimize the detection models using TensorRT and integrate with Robot Operating System to deploy on an Nvidia Jetson AGX Xavier as our embedded device. The overall system achieves a high inference speed of 63 frames per second, demonstrating the capability of our system to perform in real-time. Furthermore, we introduce CeyRo, which is the first ever large-scale traffic sign and traffic light detection dataset for the Sri Lankan context. Our dataset consists of 7984 total images with 10176 traffic sign and traffic light instances covering 70 traffic sign and 5 traffic light classes. The images have a high resolution of 1920 x 1080 and capture a wide range of challenging road scenarios with different weather and lighting conditions. Our work is publicly available at https://github.com/oshadajay/CeyRo.",https://ieeexplore.ieee.org/document/9827355/,2022 IEEE Intelligent Vehicles Symposium (IV),4-9 June 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MFI.2001.1013553,Towards a learning model for feature integration in attention control,IEEE,Conferences,"We present current efforts towards an approach for the integration of features extracted from multi-modal sensors, with which to guide the attentional behavior of robotic agents. The model can be applied in many situations and different tasks including top-down or bottom-up aspects of attention control. Basically, a pre-attention mechanism enhances attentional features that are relevant to the current task according to a weight function that can be learned. Then, an attention shift mechanism can select one between the various activated stimuli, in order for a robot to foveate on it. Also, in this approach, we consider the robot moving resources or to improve the (visual) sensory information.",https://ieeexplore.ieee.org/document/1013553/,Conference Documentation International Conference on Multisensor Fusion and Integration for Intelligent Systems. MFI 2001 (Cat. No.01TH8590),20-22 Aug. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1990.126044,Towards a real-time architecture for obstacle avoidance and path planning in mobile robots,IEEE,Conferences,"The design and partial implementation of a real-time architecture for a mobile robot, aimed particularly towards a vehicle developed for factory automation, is described. The authors develop a layered design to equip the robot with a number of behavioral competences. They examine sensing and a potential field algorithm especially to achieve modification of behavior at a speed close to the robot's operational speed. It is shown how the layered architecture interfaces to the original onboard architecture, which provided sophisticated localization but no ability to deal with environmental exceptions.<>",https://ieeexplore.ieee.org/document/126044/,"Proceedings., IEEE International Conference on Robotics and Automation",13-18 May 1990,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM43001.2020.9158908,Towards accelerated robotic deployment by supervised learning of latent space observer and policy from simulated experiments with expert policies,IEEE,Conferences,"Up until today robotic tasks in highly variable environments remain very difficult to solve. We propose accelerated robotic deployment through task solving on low-level sensor data in simulation. A simulation allows for a lot of data, which is usually not available in a real world robotic setup due to cost and feasibility. Solving tasks in simulation is safe and a lot easier due to the huge amount of feedback from virtual sensory data. We present a novel sim2real architecture for converting simulated low level sensor data policies to high level real world policies. After solving a task we let the robot complete it a number of times in simulation using domain randomization, while doing so we save the simulated sensor data corresponding to the real robotic setup and actions taken. Given these sensor data and actions a task specific policy can be trained using our architecture. In this paper we work towards a proof of concept by simulating a simple low cost manipulator in pybullet to pick and place an object based on image observations.",https://ieeexplore.ieee.org/document/9158908/,2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),6-9 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC42975.2020.9283277,Towards an Extended POMDP Planning Approach with Adjoint Action Model for Robotic Task,IEEE,Conferences,"In real-world environments, robotic task planning is expected to handle both partial observability and unexpected dynamics of the environment. A robust plan for the task requires the robot's observation actions to concurrently run with the task actions, to observe and adapt to environmental changes. The Partially Observable Markov Decision Process (POMDP) has been widely applied for planning under partially observable domains. For realistic robotic tasks, however, the POMDP model and planning algorithm are quite restrictive and unrealistic. One limitation is that task actions are modelled as atomic entities that only have endpoint effects, with no conditions specified at arbitrary points during task action execution. Also, the observation is obtained only after each task action execution, with no intermediate observations and decision-making during task action execution. To mitigate the limitations of POMDP planning, this paper first proposes an Adjoint Action Model (AAM) that explicitly defines the continuous interaction between robot's observation and task actions. Then we extend the POMDP task action model with intermediate invariant conditions which specifies the runtime properties of action execution. Finally, we propose the AAM-extended POMDP planning approach which handles observation action planning and task replanning for task action execution. We experimentally demonstrate that the plan from our proposed approach is more effective and robust to cope with the environment dynamics, comparing with the standard POMDP planning approach.",https://ieeexplore.ieee.org/document/9283277/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.1999.810023,Towards focused plan monitoring: a technique and an application to mobile robots,IEEE,Conferences,"Until recently, techniques for AI plan generation relied on highly restrictive assumptions that were almost always violated in real-world environments; consequently, robot designers adopted reactive architectures and avoided AI planning techniques. Some recent research efforts have focused on obviating such assumptions by developing techniques that enable the generation and execution of plans in dynamic, uncertain environments. In this paper, we discuss one such technique, rationale-based monitoring, originally introduced by Veloso, Pollack, and Cox (1998), and describe our use of it in a simple mobile robot environment. We review the original approach, describe how it can be adapted for a causal-link planner, and provide experimental results demonstrating that it can lead to improved plans without consuming excessive overhead. We also describe our use of rationale-based monitoring in a mobile robot office-assistant project currently in progress.",https://ieeexplore.ieee.org/document/810023/,Proceedings 1999 IEEE International Symposium on Computational Intelligence in Robotics and Automation. CIRA'99 (Cat. No.99EX375),8-9 Nov. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2011.6037332,Towards incremental learning of task-dependent action sequences using probabilistic parsing,IEEE,Conferences,"We study an incremental process of learning where a set of generic basic actions are used to learn higher-level task-dependent action sequences. A task-dependent action sequence is learned by associating the goal given by a human demonstrator with the task-independent, general-purpose actions in the action repertoire. This process of contextualization is done using probabilistic parsing. We propose stochastic context-free grammars as the representational framework due to its robustness to noise, structural flexibility, and easiness on defining task-independent actions. We demonstrate our implementation on a real-world scenario using a humanoid robot and report implementation issues we had.",https://ieeexplore.ieee.org/document/6037332/,2011 IEEE International Conference on Development and Learning (ICDL),24-27 Aug. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS/SBR/WRE.2018.00068,Traffic Signs Recognition System with Convolution Neural Networks,IEEE,Conferences,"The purpose of this paper is to develop an automatic traffic sign recognition system, making use of computation vision techniques and convolution neural networks. The work is divided in two phases, namely detection and classification, and here is presented a different approach on the detection phase. The tests were performed in a simulator and in a real controlled environment using the framework ROS (Robot Operating System) and implemented with the AmigoBot robot.",https://ieeexplore.ieee.org/document/8588574/,"2018 Latin American Robotic Symposium, 2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on Robotics in Education (WRE)",6-10 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2004.1389956,Trajectory tracking control of a rotational joint using feature-based categorization learning,IEEE,Conferences,"Real world robot applications have to cope with large variations in the operating conditions due to the variability and unpredictability of the environment and its interaction with the robot. Performing an adequate control using conventional control techniques, that require the model of the plant and some knowledge about the influence of the environment, could be almost impossible. An alternative to traditional control techniques is to use an automatic learning system that uses previous experience to learn an adequate control policy. Learning by experience has been formalized in the field of reinforcement learning. But the application of reinforcement learning techniques in complex environments is only feasible when some generalization can be made in order to reduce the required amount of experience. This work presents an algorithm that performs a kind of generalization called categorization. This algorithm is able to perform efficient generalization of the observed situations, and learn accurate control policies in a short time without any previous knowledge of the plant and without the need of any kind of traditional control technique. Its performance is evaluated on the trajectory tracking control with simulated DC motors and compared with PID systems specifically tuned for the same problem.",https://ieeexplore.ieee.org/document/1389956/,2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566),28 Sept.-2 Oct. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793644,Underwater Communication Using Full-Body Gestures and Optimal Variable-Length Prefix Codes,IEEE,Conferences,"In this paper we consider inter-robot communication in the context of joint activities. In particular, we focus on convoying and passive communication for radio-denied environments by using whole-body gestures to provide cues regarding future actions. We develop a communication protocol whereby information described by codewords is transmitted by a series of actions executed by a swimming robot. These action sequences are chosen to optimize robustness and transmission duration given the observability, natural activity of the robot and the frequency of different messages. Our approach uses a convolutional network to make core observations of the pose of the robot being tracked, which is sending messages. The observer robot then uses an adaptation of classical decoding methods to infer a message that is being transmitted. The system is trained and validated using simulated data, tested in the pool and is targeted for deployment in the open ocean. Our decoder achieves.94 precision and.66 recall on real footage of robot gesture execution recorded in a swimming pool.",https://ieeexplore.ieee.org/document/8793644/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO52101.2021.9596823,Underwater ROV Software for Fish Cage Inspection,IEEE,Conferences,"This paper deals with the development of control software for a remotely operated vehicle (ROV) as part of an automated fish cage inspection system in aquaculture. The ROV navigates autonomously around the fish cages streaming video to the topside computer that runs the algorithms for vertical rope detection. The topside computer sends back the velocity references to the ROV in real-time in order to successfully complete the inspection task. These images are also used to determine the state of net biofouling using a pre-trained convolutional neural network to help determine which nets are in need of cleaning. Robot Operating System (ROS) framework is developed to enable the topside computer to access the video stream from the ROV, process the images, and send back velocity references that would result in the complete inspection of fish cages. The inspection task is planned by following the recognizable rope segments of the outer structure of the fish cage downwards by controlling the vehicle's yaw, heave, and depth.",https://ieeexplore.ieee.org/document/9596823/,"2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO)",27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MED48518.2020.9183337,Unsupervised Learning for Subterranean Junction Recognition Based on 2D Point Cloud,IEEE,Conferences,"This article proposes a novel unsupervised learning framework for detecting the number of tunnel junctions in subterranean environments based on acquired 2D point clouds. The implementation of the framework provides valuable information for high level mission planners to navigate an aerial platform in unknown areas or robot homing missions. The framework utilizes spectral clustering, which is capable of uncovering hidden structures from connected data points lying on non-linear manifolds. The spectral clustering algorithm computes a spectral embedding of the original 2D point cloud by utilizing the eigen decomposition of a matrix that is derived from the pairwise similarities of these points. We validate the developed framework using multiple data-sets, collected from multiple realistic simulations, as well as from real flights in underground environments, demonstrating the performance and merits of the proposed methodology.",https://ieeexplore.ieee.org/document/9183337/,2020 28th Mediterranean Conference on Control and Automation (MED),15-18 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2013.6696581,Unsupervised learning of predictive parts for cross-object grasp transfer,IEEE,Conferences,"We present a principled solution to the problem of transferring grasps across objects. Our approach identifies, through autonomous exploration, the size and shape of object parts that consistently predict the applicability of a grasp across multiple objects. The robot can then use these parts to plan grasps onto novel objects. By contrast to most recent methods, we aim to solve the part-learning problem without the help of a human teacher. The robot collects training data autonomously by exploring different grasps on its own. The core principle of our approach is an intensive encoding of low-level sensorimotor uncertainty with probabilistic models, which allows the robot to generalize the noisy autonomously-generated grasps. Object shape, which is our main cue for predicting grasps, is encoded with surface densities, that model the spatial distribution of points that belong to an object's surface. Grasp parameters are modeled with grasp densities, that correspond to the spatial distribution of object-relative gripper poses that lead to a grasp. The size and shape of grasp-predicting parts are identified by sampling the cross-object correlation of local shape and grasp parameters. We approximate sampling and integrals via Monte Carlo methods to make our computer implementation tractable. We demonstrate the applicability of our method in simulation. A proof of concept on a real robot is also provided.",https://ieeexplore.ieee.org/document/6696581/,2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,3-7 Nov. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1993.385064,Use of case-based reasoning techniques for intelligent computer-aided-design systems,IEEE,Conferences,"Reuse of designs is an important research direction for the future intelligent CAD systems. The main applications of such a research are various, from mechanical systems design (spacecraft, robot, ...) to software design. This paper will present a survey of the use of case-based reasoning (CBR) techniques for intelligent CAD systems in order to reuse designs or parts of designs. First, we will briefly resume some work issued from cognitive psychology, showing the importance of analogical-reasoning for design activities and then the origins of the CBR technology in AI. Second, we will then present the main systems using case-based reasoning for design activities followed by a comparative analysis between these systems. To conclude, we will indicate the main directions in CBR for design and will propose to adopt a cognitive approach from knowledge acquisition until the development of real design support systems.<>",https://ieeexplore.ieee.org/document/385064/,Proceedings of IEEE Systems Man and Cybernetics Conference - SMC,17-20 Oct. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561936,ViNG: Learning Open-World Navigation with Visual Goals,IEEE,Conferences,"We propose a learning-based navigation system for reaching visually indicated goals and demonstrate this system on a real mobile robot platform. Learning provides an appealing alternative to conventional methods for robotic navigation: instead of reasoning about environments in terms of geometry and maps, learning can enable a robot to learn about navigational affordances, understand what types of obstacles are traversable (e.g., tall grass) or not (e.g., walls), and generalize over patterns in the environment. However, unlike conventional planning algorithms, it is harder to change the goal for a learned policy during deployment. We propose a method for learning to navigate towards a goal image of the desired destination. By combining a learned policy with a topological graph constructed out of previously observed data, our system can determine how to reach this visually indicated goal even in the presence of variable appearance and lighting. Three key insights, waypoint proposal, graph pruning and negative mining, enable our method to learn to navigate in real-world environments using only offline data, a setting where prior methods struggle. We instantiate our method on a real outdoor ground robot and show that our system, which we call ViNG, outperforms previously-proposed methods for goal-conditioned reinforcement learning, including other methods that incorporate reinforcement learning and search. We also study how ViNG generalizes to unseen environments and evaluate its ability to adapt to such an environment with growing experience. Finally, we demonstrate ViNG on a number of real-world applications, such as last-mile delivery and warehouse inspection. We encourage the reader to visit the project website for videos of our experiments and demonstrations 1.",https://ieeexplore.ieee.org/document/9561936/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/WAC50355.2021.9559586,Virtual Testing and Policy Deployment Framework for Autonomous Navigation of an Unmanned Ground Vehicle Using Reinforcement Learning,IEEE,Conferences,"The use of deep reinforcement learning (DRL) as a framework for training a mobile robot to perform optimal navigation in an unfamiliar environment is a suitable choice for implementing AI with real-time robotic systems. In this study, the environment and surrounding obstacles of an Ackermann-steered UGV are reconstructed into a virtual setting for training the UGV to centrally learn the optimal route (guidance actions to be taken at any given state) towards a desired goal position using Multi-Agent Virtual Exploration in Deep Q-Learning (MVEDQL) for various model configurations. The trained model policies are to be transferred to a physical vehicle and compared based on their individual effectiveness for performing autonomous waypoint navigation. Prior to incorporating the learned model with the physical UGV for testing, this paper outlines the development of a GUI application to provide an interface for remotely deploying the vehicle and a virtual reality framework reconstruction of the training environment to assist safely testing the system using the reinforcement learning model.",https://ieeexplore.ieee.org/document/9559586/,2021 World Automation Congress (WAC),1-5 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2MTC.2019.8826921,Vision-Based Deep Learning Approach for Real-Time Detection of Weeds in Organic Farming,IEEE,Conferences,"Vision-based detection and classification systems for identifying crops and weeds in captured color images have recently being extensively researched due to the advantages that they offer. The use of chemical or synthetic pesticides could drastically be reduced. One of the critical aspects of these systems is the requirement for high data volumes and the resulting lack of real time capability. This paper presents a method for detecting weeds in carrot fields in real time without segmentation and the need of a large dataset. In most vision-based measurement systems the task is divided into multiple processes like separating the objects from the background followed by the detection of the object and lastly the object classification. Our approach uses a convolution neural network to localize and classify the plants simultaneously. A precision of 89 % was achieved with a calculation rate of 18,56 FPS. A lower precision was accepted in favor of a higher calculation rate of about 56 FPS. We implemented and evaluated our system using a multi-platform robot on an organic carrot field located in Germany.",https://ieeexplore.ieee.org/document/8826921/,2019 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),20-23 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1995.525277,Vision-based reinforcement learning for purposive behavior acquisition,IEEE,Conferences,"This paper presents a method of vision-based reinforcement learning by which a robot learns to shoot a ball into a goal, and discusses several issues in applying the reinforcement learning method to a real robot with vision sensor. First, a ""state-action deviation"" problem is found as a form of perceptual aliasing in constructing the state and action spaces that reflect the outputs from physical sensors and actuators, respectively. To cope with this, an action set is constructed in such a way that one action consists of a series of the same action primitive which is successively executed until the current state changes. Next, to speed up the learning time, a mechanism of learning form easy missions (or LEM) which is a similar technique to ""shaping"" in animal learning is implemented. LEM reduces the learning time from the exponential order in the size of the state space to about the linear order in the size of the state space. The results of computer simulations and real robot experiments are given.",https://ieeexplore.ieee.org/document/525277/,Proceedings of 1995 IEEE International Conference on Robotics and Automation,21-27 May 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSTCC.2019.8885611,Visual Analytics Framework for Condition Monitoring in Cyber-Physical Systems,IEEE,Conferences,"One of the biggest challenges facing the factory of the future today is to reduce the time-to-market access and increase through the improvement of competitiveness and efficiency. In order to achieve this target, data analytics in Industrial Cyber-Physical System becomes a feasible option. In this paper, a visual analytics framework for condition monitoring of the machine tool is presented with the aim to manage events and alarms at factory level. The framework is assessed in a particular use case that consists in a multi-threaded cloud-based solution for the global analysis of the behaviour of variables acquired from PLC, CNC and robot manipulator. A human-machine interface is also designed for the real-time visualization of the key performance indicators according to the user's criteria. This tool implemented is a great solution for condition monitoring and decision-making process based on data analytics from simple statistics to complex machine learning methods. The results achieved are part of the vision and implementation of the industrial test bed of “Industry and Society 5.0” platform.",https://ieeexplore.ieee.org/document/8885611/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.2002.1035063,Visual feedback by using a CNN chip prototype system,IEEE,Conferences,"Robot locomotion control passes through a series of sensors that, according to information from the environment, allow the robot to adapt, in real time, its locomotion scheme or trajectory. When the goal of the robot is to reach a target in a non-structured environment the best approach is visual control realized by a fast image processing system. Fast parallel image processing of the CNN-UM cP4000 chip prototype permits one to obtain good performance, even in a real time control problem. The robot controlled by the implemented CNN visual feedback has a hexapod configuration and its locomotion system is also implemented by a multi-layer CNN structure. In this paper a CNN approach for both locomotion generation and visual control of the bio-inspired robot is presented.",https://ieeexplore.ieee.org/document/1035063/,Proceedings of the 2002 7th IEEE International Workshop on Cellular Neural Networks and Their Applications,24-24 July 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1991.131999,Visual navigation around curved obstacles,IEEE,Conferences,"An approach to path-planning around smooth obstacles that exploits visually derived geometry is proposed. A moving robot can scan the silhouette or apparent contour of an obstacle and estimate a minimum length path. This is done by seeking geodesics which can be extrapolated smoothly, around the obstacle and towards the goal. Preliminary implementation of this idea uses a real-time visual contour tracker running at 16 Hz, with a camera mounted on an Adept robot arm. The camera first dithers to generate visual motion, a safe path is estimated, and the robot steers the camera around the obstacle with a clearance of a few millimeters.<>",https://ieeexplore.ieee.org/document/131999/,Proceedings. 1991 IEEE International Conference on Robotics and Automation,9-11 April 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1997.649086,Visually-guided obstacle avoidance in unstructured environments,IEEE,Conferences,"This paper presents an autonomous vision-based obstacle avoidance system. The system consists of three independent vision modules for obstacle detection, each of which is computationally simple and uses a different criterion for detection purposes. These criteria are based on brightness gradients, RGB (red, green, blue) color, and HSV (hue, saturation, value) color, respectively. Selection of which modules are used to command the robot proceeds exclusively from the outputs of the modules themselves. The system is implemented on a small monocular mobile robot and uses very lour resolution images. It has been tested for over 200 hours in diverse environments.",https://ieeexplore.ieee.org/document/649086/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RITAPP.2019.8932854,Which LSTM Type is Better for Interaction Force Estimation?,IEEE,Conferences,"Tactile, one of the five senses classified into the main senses of human, is the first sensation developed when human beings are formed. The tactile includes various information such as pressure, temperature, and texture of objects, it also helps the person to interact with the surrounding environment. One of the tactile information, the pressure is used in various fields such as medical, beauty, mobile devices and so on. However, humans can perceive the real world with multi-modal senses such as sound, vision. In this paper, we study interaction force estimation using haptic sensor and video. Interact ion force estimation through video analysis is one of a cross-modal approach that is applicable such as a software haptic feedback method that can give haptic feedback to remote control of robot arm by predicting interaction force even in absence of haptic sensor. we compare and analyze three types of a deep neural network to predict the interaction force. In particular, the best model for the stacking structure of CNN and LSTM is selected through a detailed analysis of how the structure change of LSTM affects the video regression problem. The average error of the best suit model is MSE 0.1306, RMSE 0.2740, MAE 0.1878.",https://ieeexplore.ieee.org/document/8932854/,2019 7th International Conference on Robot Intelligence Technology and Applications (RiTA),1-3 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWACI.2011.6159663,[Front and back cover],IEEE,Conferences,The following topics are dealt with: advanced computational intelligence; rough set; genetic algorithm; distributed computing; flexible job-shop scheduling; system-of-systems vulnerability analysis; V-BLAST sphere decoding; spanning tree problem; object representation model; real time path planning; ant colony optimization; multi-objective evolutionary algorithm; RBF neural network; particle swarm optimization; multi-color extraction method; adaptive NN tracking control; gravitational chaotic search algorithm; approximate optimal control tracking; multi-target tracking approach; generalize disjunctive paraconsistent data model; discriminative item mining; weighted passive nearest neighbor algorithm; structure-encoding differential evolution algorithm; real-time data stream clustering; event-driven control program automatic verification; threshold signature scheme; data model driven architecture; improved post-nonlinear independent component analysis; pruning algorithm; remote sensing image classification; fuzzy matrices; grey-box neural network; training ANFIS system; delay BAM neural network stability; tuning method; ensemble learning balancing; Kalman filtering; hybrid learning model; multi-focus image fusion; object-based image retrieval; language grounding model; decision fusion; functional network analysis; image interpolation; image deblurring method; discrete-time dynamic system stability; feature selection methods; Lasso logistic regression; vehicle scheduling; cooperative air-defense system; bifurcation analysis; close-loop time-delayed filter system; Newton iteration formula; 3D object recognition; parameter identification; intelligent displacement back-analysis method; single machine total weighted tardiness scheduling problem; dimensionality reduction method; temporal Bayesian network; online leasing problem; network supported intelligent cooperative diagnosis; Hopf bifurcation analysis; multi-mode human-machine interface; GA-fuzzy automatic generation controller; tele-operation robot system; hybrid clonal selection algorithm; modified LEACH protocol; fuzzy Lyapunov synthesis; and underwater vehicle.,https://ieeexplore.ieee.org/document/6159663/,The Fourth International Workshop on Advanced Computational Intelligence,19-21 Oct. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICAS51828.2021.9458401,iELAS: An ELAS-Based Energy-Efficient Accelerator for Real-Time Stereo Matching on FPGA Platform,IEEE,Conferences,"Stereo matching is a critical task for robot navigation and autonomous vehicles, providing the depth estimation of surroundings. Among all stereo matching algorithms, Efficient Large-scale Stereo (ELAS) offers one of the best tradeoffs between efficiency and accuracy. However, due to the inherent iterative process and unpredictable memory access pattern, ELAS can only run at 1.5-3 fps on high-end CPUs and difficult to achieve real-time performance on low-power platforms. In this paper, we propose an energy-efficient architecture for real-time ELAS-based stereo matching on FPGA platform. Moreover, the original computational-intensive and irregular triangulation module is reformed in a regular manner with points interpolation, which is much more hardware-friendly. optimizations, including memory management, parallelism, and pipelining, are further utilized to reduce memory footprint and improve throughput. Compared with Intel i7 CPU and the state-of-the-art $\mathrm{C}\mathrm{P}\mathrm{U}+$FPGA implementation, our FPGA realization achieves up to $ 38.4\times$ and $ 3.32\times$ frame rate improvement, and up to $ 27.1\times$ and $ 1.13\times$ energy efficiency improvement, respectively.",https://ieeexplore.ieee.org/document/9458401/,2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),6-9 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636460,micROS.BT: An Event-Driven Behavior Tree Framework for Swarm Robots,IEEE,Conferences,"In this paper, we propose micROS.BT, an event-driven behavior tree (BT) framework aiming at supporting swarm-robot coordination. Compared with other BT frame-works, micROS.BT implements the event-driven way under the multi-thread mode, which can effectively save computing resources. Moreover, in order to ensure swarm-robot coordination, we optimize the implementation of the traditional blackboard and propose the multi-mode blackboard, which supports inner-tree, inter-tree, and inter-robot data sharing. Furthermore, considering the limited modularity of a single tree, micROS.BT realizes a mechanism called hierarchical tree management which involves inter-tree notifying and waiting functionalities, while ensuring that each tree is independent and self-scheduled. The effectiveness of micROS.BT is verified by simulation and real-robot experiments for different system settings, showing that a substantial improvement is achieved in comparison with the traditional BT implementations.",https://ieeexplore.ieee.org/document/9636460/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
