contentType,identifier,title,publicationName,doi,publisher,publicationDate,abstract,url,database,query_name,query_value
Chapter ConferencePaper,doi:10.1007/978-981-16-9967-2_32,Deep Learning for Part of Speech (PoS) Tagging: Konkani,Smart Trends in Computing and Communications,10.1007/978-981-16-9967-2_32,Springer,2023-01-01,"This is the first time that an experiment using deep learning has been attempted with a Konkani language data set. For this study, over 100,000 PoS tagged Konkani sentences were used. The f-scores for deep learning are 90.73% for training data and 71.43% for test data. These results are better than the ones previously reported for Konkani. We have provided a list of references of PoS tagging for Indian languages specifically using deep learning to place our research in perspective.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9967-2_32,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-2394-4_67,Pain Detection Using Deep Learning Method from 3D Facial Expression and Movement of Motion,Proceedings of Seventh International Congress on Information and Communication Technology,10.1007/978-981-19-2394-4_67,Springer,2023-01-01,"Nowadays, face expression technology is widespread. For instance, 2D pain detection is utilized in hospitals; nevertheless, it has some disadvantages that should be considered. Our goal was to design a 3D pain detection system that anybody may use before coming to the hospital, supporting all orientations. We utilized a dataset from the University of Northern British Columbia (UNBC) as a training set in this study. Pain is classified as not hurting, becoming painful, and painful in our system. The system’s effectiveness was established by comparing its results to those of a highly trained medical and two-dimensional pain identification. To conclude, our study has developed an uncomplicated, cost-effective, and easy to comprehend alternative tool for screening for pain before admission for the public in general and health provider.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-2394-4_67,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9967-2_64,Real-Time Object Detection System with Voice Feedback for the Blind People,Smart Trends in Computing and Communications,10.1007/978-981-16-9967-2_64,Springer,2023-01-01,"Lots of people suffer from temporary or permanent disabilities, and one of them is blindness or the visually impaired. According to the World Health Organization (WHO), more than 1 billion people suffer from blindness. Technology may assist blind and visually impaired persons in a number of ways; objects detecting is still a difficult process. There are so many techniques available for object detection. But, the accuracy and efficiency of detection aren’t good. Therefore, in this paper, authenticate object detection in real time using the YOLO-v3 and deep learning techniques. The main focus of this paper is to create a smartphone application that is cost-efficient with high accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9967-2_64,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9967-2_65,Bird Video Summarization Using Pre-trained CNN Model and Deep Learning,Smart Trends in Computing and Communications,10.1007/978-981-16-9967-2_65,Springer,2023-01-01,"In the past few years, the forms of data have changed drastically from the text formats to images and today, most of the data are available in the video format. With this, there is a huge demand in the techniques that can provide the overall summary of the video. In this paper, we present the summary of birds that are identified from the large datasets using convolutional neural network (CNN). CNN is one of the best image processing and video processing model. The CNN model that we have used for the task is pre-trained AlexNet. The paper clearly proves that the work proposed recognizes the various kinds of birds from the inputted video with accuracy level ranging between 85 and 99%. At last, we provide the overall summary in terms of start time and end time of existence of each bird in the video.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9967-2_65,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-3632-6_37,Multi-dimensional Convolutional Neural Network for Speech Emotion Recognition,The 2021 International Conference on Smart Technologies and Systems for Internet of Things,10.1007/978-981-19-3632-6_37,Springer,2023-01-01,"Speech Emotion Recognition (SER) is a difficulty of deep learning algorithms. The difficulty is that people’s own understanding of emotions is not absolute. Different people may also have different judgments on the same speech. And speech emotion recognition plays a huge role in many real-time applications. With the continuous development of deep learning in recent years, many people use convolutional neural networks (CNN) to extract high-dimensional features in speech from speech spectrograms, thereby improving the accuracy of speech emotion recognition. In contrast, we propose a new model of speech emotion recognition. The model uses the eGeMAPS feature set extracted through the openSMILE toolkit to input into our model. The model learns the correlation and timing between features. In addition, we perform intra-class normalization on the input features to ensure more accurate recognition and faster data fitting. In our model, the key speech segments can be selected through the characteristics of convolutional neural network (CNN), so that the recognition accuracy of the model can achieve a better effect. Our model was evaluated experimentally in the IEMOCAP dataset. Experimental results show that our unweighted accuracy (UA) and weighted accuracy (WA) on the test set reached 60.9% and 63.0%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-3632-6_37,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0095-2_18,Traffic Sign Detection and Recognition,Information and Communication Technology for Competitive Strategies (ICTCS 2021),10.1007/978-981-19-0095-2_18,Springer,2023-01-01,"Surge in the amount of automobiles on street imposes the consumption of automatic systems for driver aid. These structures form significant instruments of self-driving automobiles as well. Traffic Sign Recognition remains such an automatic structure which affords the relative responsiveness aimed at self-driving automobile. In this work we are able to perceive and identify traffic signs in video classifications detailed by an onboard automobile camera. Traffic Sign Recognition (TSR) stands used to control traffic signs, inform a driver and facility or proscribe definite actions. A debauched real-time and vigorous instinctive traffic sign finding and recognition can upkeep and disburden the driver and ominously upsurge heavy protection and ease. Instinctive recognition of traffic signs is also important for automated intellectual driving automobile or driver backing structures. This paper presents a study to identify traffic sign via OpenCV procedure and also convert the detected sign into text and audio signal. The pictures are mined, perceived and recognized by preprocessing through numerous image processing methods. At that time, the phases are accomplished to identify and identify the traffic sign arrangements. The structure is trained and endorsed to find the finest network architecture. Aimed at the network exercise and assessment we have generated a dataset containing of 1012 images of 8 diverse classes. The tentative results demonstrate the exceedingly accurate groupings of traffic sign patterns with composite contextual images and the computational price of the planned system. Though, numerous features make the road sign recognition tricky and problematic such as lighting state changes, occlusion of signs due to hitches, distortion of signs, gesture blur in video images.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0095-2_18,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-07258-1_50,Digital Twins as Testbeds for Vision-Based Post-earthquake Inspections of Buildings,European Workshop on Structural Health Monitoring,10.1007/978-3-031-07258-1_50,Springer,2023-01-01,"Manual visual inspections typically conducted after an earthquake are high-risk, subjective, and time-consuming. Delays from inspections often exacerbate the social and economic impact of the disaster on affected communities. Rapid and autonomous inspection using images acquired from unmanned aerial vehicles offer the potential to reduce such delays. Indeed, a vast amount of research has been conducted toward developing automated vision-based methods to assess the health of infrastructure at the component and structure level. Most proposed methods typically rely on images of the damaged structure, but seldom consider how the images were acquired. To achieve autonomous inspections, methods must be evaluated in a comprehensive end-to-end manner, incorporating both data acquisition and data processing. In this paper, we leverage recent advances in computer generated imagery (CGI) to construct a 3D synthetic environment with a digital twin for simulation of post-earthquake inspections that allows for comprehensive evaluation and validation of autonomous inspection strategies. A critical issue is how to simulate and subsequently render the damage in the structure after an earthquake. To this end, a high-fidelity nonlinear finite element model is incorporated in the synthetic environment to provide a representation of earthquake-induced damage; this finite element model, combined with photo-realistic rendering of the damage, is termed herein a physics-based graphics models (PBGM). The 3D synthetic environment with PBGM as a digital twin provides a comprehensive end-to-end approach for development and validation of autonomous post-earthquake strategies using UAVs.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-07258-1_50,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-04090-0_5,Wavelet Energy Features for Damage Identification: Sensitivity to Measurement Uncertainties,"Model Validation and Uncertainty Quantification, Volume 3",10.1007/978-3-031-04090-0_5,Springer,2023-01-01,"In vibration-based structural parameter identification, wavelet transformation has been widely used for extraction of damage pertinent data for onward identification of structural parameters or the occurrence of anomalies. Among wavelet-based techniques, the use of wavelet packet node energy (WPNE) as damage-sensitive features has attracted much research interest in more recent years. WPNE features contain detailed information which can be highly sensitive to local damages. However, most of the existing studies in the literature on using wavelet energy-based features have been numerical and involved idealised assumptions such as perfect and identical excitations among different tests. This paper presents an investigation into the tolerance of a wavelet packet energy with neural network approach to uncertainties in the input excitations and measurement noises. WPNEs are extracted from vibration signals from impact tests as feature proxies and a back-propagation neural network is used for classification. The method is firstly applied on a beam model using finite element simulations, in which variation in the excitations and measurement noises are incorporated to investigate the susceptibility of the approach to such uncertainties. Subsequently, the method is applied to the experimental data from the laboratory test of a steel beam. The results from both the numerical simulations and the experimental verification demonstrate that the wavelet energy with neural network approach to detecting structural changes is workable, and given a reasonably controlled impact test, it is possible to identify the initiation of damage with good accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-04090-0_5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0095-2_56,Robust Feature Extraction and Recognition Model for Automatic Speech Recognition System on News Report Dataset,Information and Communication Technology for Competitive Strategies (ICTCS 2021),10.1007/978-981-19-0095-2_56,Springer,2023-01-01,"Information processing has become ubiquitous. The process of deriving speech from transcription is known as automatic speech recognition systems. In recent days, most of the real-time applications such as home computer systems, mobile telephones, and various public and private telephony services have been deployed with automatic speech recognition (ASR) systems. Inspired by commercial speech recognition technologies, the study on automatic speech recognition (ASR) systems has developed an immense interest among the researchers. This paper is an enhancement of convolution neural networks (CNNs) via a robust feature extraction model and intelligent recognition systems. First, the news report dataset is collected from a public repository. The collected dataset is subjective to different noises that are preprocessed by min–max normalization. The normalization technique linearly transforms the data into an understandable form. Then, the best sequence of words, corresponding to the audio based on the acoustic and language model, undergoes feature extraction using Mel-frequency Cepstral Coefficients (MFCCs). The transformed features are then fed into convolutional neural networks. Hidden layers perform limited iterations to get robust recognition systems. Experimental results have proved better accuracy of 96.17% than existing ANN.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0095-2_56,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0095-2_35,Designing a Digital Shadow for Pasture Management to Mitigate the Impact of Climate Change,Information and Communication Technology for Competitive Strategies (ICTCS 2021),10.1007/978-981-19-0095-2_35,Springer,2023-01-01,"Pasture is a said to be a natural feed to the animal. Pastures are maintained to ensure a good quality and quantity of the feed to get the best produce from the animals. Traditional ways of managing the pastures are costly and labor intensive. The use of technology in farming introduced the concept of smart farming which made pasture management easier and cost effective. Smart farming integrates advanced technological methods that include Internet of Things (IoT), big data and cloud computing. These enable tracking, monitoring, automating and complex operational analysis operations. However even with the use of these new technological techniques there are still challenges facing pasture management. Pastures are dependent on climatic conditions therefore climate change has a great impact on pasture management. With the onset of climate change, it has become a challenge for livestock farmers to predict weather conditions therefore it is impossible for decision makers to deal more effectively with the effects of climate variability. However, the digital twin concept proved to be the best technique to tackle these challenges due to its complex prediction techniques that include artificial intelligence (AI) and machine learning. This paper analyses the models used in building a digital shadow as the first step in developing a digital twin.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0095-2_35,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-2394-4_68,"The Next-Generation 6G: Trends, Applications, Technologies, Challenges, and Use Cases",Proceedings of Seventh International Congress on Information and Communication Technology,10.1007/978-981-19-2394-4_68,Springer,2023-01-01,"While the 5G generation of mobile communications system has delimited its focus on Internet of Things (IoT) connection and industrial automation systems. The 6G generation will offer an extrasensory experiences through the fusion of the digital, physical, and human world. It will redefine the way we live, work, and manage the world, and it will make us more efficient thanks to the combination of intelligence and vigorous computation capabilities. The sixth generation of mobile communications is still under investigation. Several projects and research have been launched. 6G promises a specific type of communication with very high data rate and capacity, very low latency, maximum coverage, very high reliability, extremely massive connectivity, and very low cost and power. The current article provides an overview of what has been discussed about the future of 6G so far. The major goal of this paper is to present a comprehensive picture of 6G based on the research and projects that have been launched. We describe the potential architectural characteristics of 6G that will give users the experience they expect. We present an important list of technologies that will be the critical element in the rollout of 6G such as artificial intelligence, VLC communications, 3D beamforming, massive MIMO aircraft, and drones. We also exhibit scenarios and use cases that might be lived in this next-generation networks. Finally, we identify the challenges that could be faced by the 6G in different sides ...",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-2394-4_68,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-07258-1_78,Structural Damage Identification from Video Footage Using Artificial Intelligence,European Workshop on Structural Health Monitoring,10.1007/978-3-031-07258-1_78,Springer,2023-01-01,"One of the major advancements in the field of civil engineering from the past few years is intelligent infrastructures. These structures will have the ability of sensing and will be accurately able to respond to the changes in the environment and external damages. To be able to keep up with the advancements we need to detect the damages to monitor the health of the structure. This research study aims to develop an efficient damage detection method using video of the damaged structure taken by mobile or camera mounted on a drone to replace the human visual inspection using Artificial Intelligence. For this system, an AI-based application is developed that will be able to recognize the damages accurately in the bounding box or targeted limiting area via a portable camera. An artificial neural network will be used to train the model and to classify the images obtained from video footage. As a case study and validation, a real damaged bridge video footage is considered.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-07258-1_78,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0105-8_54,Sentiment Analysis on COVID-19 News Videos Using Machine Learning Techniques,Proceedings of International Conference on Frontiers in Computing and Systems,10.1007/978-981-19-0105-8_54,Springer,2023-01-01,"Coronavirus disease (COVID-19) has affected all walks of human life most adversely, from entertainment to education. The whole world is confronting this deadly virus, and no country in this world remains untouched during this pandemic. From the early days of reporting this virus from many parts of the world, many news videos on the same got uploaded in various online platforms such as YouTube, Dailymotion, and Vimeo. Even though the content of many of those videos was unauthentic, people watched them and expressed their views and opinions as comments. Analysing these comments can unearth the patterns hidden in them to study people’s responses to videos on COVID-19. This paper proposes a sentiment analysis approach on people’s response towards such videos, using text mining and machine learning. This work implements different machine learning algorithms to classify people’s sentiments and also uses text mining principles for finding out several latent themes, from the comments collected from YouTube.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0105-8_54,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-3387-5_119,Fast HEVC Intra CTU Partition Algorithm Based on Lightweight CNN,"Signal and Information Processing, Networking and Computers",10.1007/978-981-19-3387-5_119,Springer,2023-01-01,"Recently, several methods have been proposed to reduce the complexity of high efficiency video coding (HEVC) by using neural networks to replace the original rate-distortion optimization (RDO) search for obtaining the optimal coding tree unit (CTU) partition. However, these approaches are difficult to deploy in scenarios with limited storage space due to the complex network structure. In this paper, we propose a lightweight convolutional neural network (L-CNN) for fast CTU partitioning with only a KB-sized model, which makes it applicable in storage space-constrained scenarios. First, we establish a dataset that contains spatial information and image texture for each coding unit (CU). Then, we combine CU spatial information with image data to obtain the CTU partition to further improve the performance of the network. Finally, experimental results show that our method reduces the model size by 13.7× and 5.2× compared with two CNN-based approaches with 60.46% intra-mode encoding time reduction and 2.78% Bjøntegaard delta bit-rate degradation, outperforming the existing state-of-the-art approaches.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-3387-5_119,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9967-2_24,Human Activity Recognition Using LSTM with Feature Extraction Through CNN,Smart Trends in Computing and Communications,10.1007/978-981-16-9967-2_24,Springer,2023-01-01,"Human activity recognition is important for detecting anomalies from videos. The analysis of auspicious activities using videos is increasingly important for security, surveillance, and personal archiving. This research paper has given a model which can recognize activities in random videos. The architecture has been designed by using BiLSTM layer which helps to learn a system based on time dependencies. To convert every frame into a featured vector, the pre-trained GoogLeNet network has been used. The evaluation has been done by using a public HMDB51 data set. The accuracy achieved by using the model is 93.04% for ten classes and 63.96% for 51 classes from same data set only. Then, this network is compared with other state-of-the-art method, and it proves to be a better approach for the recognition of activities. Abstract should summarize the contents of the paper in short terms, i.e. 150–250 words.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9967-2_24,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-3387-5_161,Panoramic Video Quality Assessment Based on Spatial-Temporal Convolutional Neural Networks,"Signal and Information Processing, Networking and Computers",10.1007/978-981-19-3387-5_161,Springer,2023-01-01,"The development of 5G technology and Ultra HD video provide the basis for panoramic video, namely virtual reality (VR). At present, the traditional VQA method is not effective on panoramic video. Therefore, it is crucial to design objective VQA models for the standardization of panoramic video industry. With the development of deep learning, excellent algorithms of VQA methods based on convolutional neural network have emerged. In this paper, we propose a full reference VQA model based on spatial-temporal 3D convolutional neural network, the feature extraction combined the time and spatial information. we verify and optimize the proposed VQA model based on VQA-ODV panoramic video database, its objective score has a higher correlation with subjective scores than that of traditional VQA methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-3387-5_161,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8154-7_39,SatMVS: A Novel 3D Reconstruction Pipeline for Remote Sensing Satellite Imagery,Proceedings of the International Conference on Aerospace System Science and Engineering 2021,10.1007/978-981-16-8154-7_39,Springer,2023-01-01,"Recently, 3D reconstruction based on satellite imagery has been a hot topic in the remote sensing community. Its output called the digital surface model (DSM) can be widely used in urban planning, military navigation, and so on. Nowadays, almost all satellite image 3D reconstruction pipelines are based on traditional stereo matching algorithms which have low accuracy and long runtime. In contrast, the neural networks based on multi-view stereo (MVS) have shown great reconstruction performance in the computer vision community. To transfer the advanced MVS neural networks to the remote sensing community, we propose a novel 3D reconstruction pipeline called SatMVS. First, the input satellite images and their rational polynomial camera parameters (RPC) are cropped into small tiles according to the designated output DSM region. Second, the RPC parameters are converted to the projection matrix for the homography transform which is the core step in MVS neural networks. Third, the advanced MVS neural network is applied to estimate height maps from satellite images. At last, all inferred height maps from small tiles are converted to 3D points in Universal Transverse Mercator (UTM) coordinate system and fused to get the final complete DSM. In order to train and test SatMVS, we build a novel satellite imagery 3D reconstruction dataset called SatMVS3D dataset, which contains satellite images, RPC parameters, and height map ground truth that covers about 3km^2. The experimental results on the SatMVS3D dataset demonstrate that our proposed pipeline can provide robust reconstruction performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8154-7_39,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11053-022-10075-2,Developments in Quantitative Assessment and Modeling of Mineral Resource Potential: An Overview,Natural Resources Research,10.1007/s11053-022-10075-2,Springer,2022-08-01,"The special issue entitled “Developments in Quantitative Assessment and Modeling of Mineral Resource Potential” is composed of 17 papers that cover a diverse range of approaches to mineral resource assessment, including mainly multivariate statistical analysis, fractal and multifractal modeling, geostatistical modeling, machine learning, mathematical morphology and 3D mineral exploration. This introductory article first provides an overview of the developments and existing methods in quantitative assessment of mineral resources. Then, brief introductions of each of the 17 papers are provided. These papers can be grouped into three themes: (1) multifractal theory for mineral resource assessment; (2) machine learning for mineral prospectivity mapping; and (3) GIS-based 3D modeling for mineral exploration. The 17 papers either proposed novel methods or demonstrated innovative applications of existing methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11053-022-10075-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-022-03848-3,Skeleton-based human action recognition with sequential convolutional-LSTM networks and fusion strategies,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-022-03848-3,Springer,2022-08-01,"Human action recognition from skeleton data has drawn a lot of attention from researchers due to the availability of thousands of real videos with many challenges. Existing works attempted to model the spatial characteristics and temporal dependencies of 3D joints using dynamic time warping, hand-crafted, and spatial co-occurrence features. However, the representation derived from the spatial stream overemphasizes the temporal information; thus, it yields limited expressive power. Some studies use skeleton sequences as frames to enhance the expressive power of representations but lose the generalization capability because the derived temporal smoothness is specific to a particular dataset. The proposed work uses joint distance maps as a base representation that encodes the spatial and temporal information to color texture images. We increase the expressive power by extracting the feature maps from pre-trained networks on ImageNet to diversify the texture representation and propose a network architecture to model the temporal dependency explicitly. We also explore various fusion strategies to generate diverse representations from the feature maps of the pre-trained networks. The experimental results show that the proposed method achieves the best recognition accuracy when using decision-level fusion with meta-learners (Random Forest). The analysis also reveals that the use of feature-level fusion yields relatively good results in terms of the trade-off, i.e., on par recognition performance with some decision-level fusion strategies while having less tunable parameters. Extensive experimental results and comparative analysis on three benchmark datasets prove that the proposed representation and network not only yield better recognition accuracy but also exhibit stronger generalization capability on multiple datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-022-03848-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11579-4,Fully automated 2D and 3D convolutional neural networks pipeline for video segmentation and myocardial infarction detection in echocardiography,Multimedia Tools and Applications,10.1007/s11042-021-11579-4,Springer,2022-07-12,"Myocardial infarction (MI) is a life-threatening disorder that occurs due to a prolonged limitation of blood supply to the heart muscles, and which requires an immediate diagnosis to prevent death. To detect MI, cardiologists utilize in particular echocardiography, which is a non-invasive cardiac imaging that generates real-time visualization of the heart chambers and the motion of the heart walls. These videos enable cardiologists to identify almost immediately regional wall motion abnormalities (RWMA) of the left ventricle (LV) chamber, which are highly correlated with MI. However, data acquisition is usually performed during emergency which results in poor-quality and noisy data that can affect the accuracy of the diagnosis. To address the identified problems, we propose in this paper an innovative, real-time and fully automated model based on convolutional neural networks (CNN) to early detect MI in a patient’s echocardiography. Our model is a pipeline consisting of a 2D CNN that performs data preprocessing by segmenting the LV chamber from the apical four-chamber (A4C) view, followed by a 3D CNN that performs a binary classification to detect MI. The pipeline was trained and tested on the HMC-QU dataset consisting of 162 echocardiography. The 2D CNN achieved 97.18% accuracy on data segmentation, and the 3D CNN achieved 90.9% accuracy, 100% precision, 95% recall, and 97.2% F1 score. Our detection results outperformed existing state-of-the-art models that were tested on the HMC-QU dataset for MI detection. This work demonstrates that developing a fully automated system for LV segmentation and MI detection is efficient and propitious.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11579-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11539-y,A dual-branch neural network for DeepFake video detection by detecting spatial and temporal inconsistencies,Multimedia Tools and Applications,10.1007/s11042-021-11539-y,Springer,2022-07-12,"It has become a research hotspot to detect whether a video is natural or DeepFake. However, almost all the existing works focus on detecting the inconsistency in either spatial or temporal. In this paper, a dual-branch (spatial branch and temporal branch) neural network is proposed to detect the inconsistency in both spatial and temporal for DeepFake video detection. The spatial branch aims at detecting spatial inconsistency by the effective EfficientNet model. The temporal branch focuses on temporal inconsistency detection by a new network model. The new temporal model considers optical flow as input, uses the EfficientNet to extract optical flow features, utilize the Bidirectional Long-Short Term Memory (Bi-LSTM) network to capture the temporal inconsistency of optical flow. Moreover, the optical flow frames are stacked before inputting into the EfficientNet. Finally, the softmax scores of two branches are combined with a binary-class linear SVM classifier. Experimental results on the compressed FaceForensics++ dataset and Celeb-DF dataset show that: (a) the proposed dual-branch network model performs better than some recent spatial and temporal models for the Celeb-DF dataset and all the four manipulation methods in FaceForensics++ dataset since these two branches can complement each other; (b) the use of optical flow inputs, Bi-LSTM and dual-branches can greatly improve the detection performance by the ablation experiments.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11539-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11277-022-09937-y,Correction to: Deep Neural Network Routing with Dynamic Space Division for 3D UAV FANETs,Wireless Personal Communications,10.1007/s11277-022-09937-y,Springer,2022-07-12,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11277-022-09937-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11545-0,Data-driven intelligent decision for multimedia medical management,Multimedia Tools and Applications,10.1007/s11042-021-11545-0,Springer,2022-07-12,"Since medical diagnosis runs through the whole journey of human health, the traditional medical diagnosis methods cannot ensure the diagnosis accurate due to the interference of multiple external factors. In response, this paper proposes a multimedia medical management method supported by the data-driven intelligent decision (MMD-DI). This method can be used to predict the survival time of cancer patients under the help of gradient boosting decision tree (GBDT) and hybrid neural network model. First, the feature factors were scanned to match the conditions by GBDT, according to the set value domain; Then the factors were inputted into the neural network. The hybrid neural network was employed to predict the survival time of cancer patients, and it was constructed by combining the convolutional neural network (CNN) and the long short-term memory (LSTM) model. Finally, the stability of the proposed MMD-DI was analyzed, and performance was compared with a series of commonly exploited baseline methods: the mean of cross-validated RMSE (Root Mean Squared Error) evaluation results is 0.183, the mean of cross-validated MAE (Mean Absolute Error) evaluation results is 0.147, the two indicators are both much lower than the commonly exploited baseline methods. A series of experiments proved that MMD-DI has excellent performance and can be used in the multimedia medical management systems.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11545-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00530-022-00969-9,Special issue deep learning for multimedia healthcare,Multimedia Systems,10.1007/s00530-022-00969-9,Springer,2022-07-11,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-022-00969-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-022-03930-5,Semantic segmentation of 3D LiDAR data using deep learning: a review of projection-based methods,Applied Intelligence,10.1007/s10489-022-03930-5,Springer,2022-07-11,"LiDAR sensor is an active remote sensing sensor that is increasingly used to capture 3D information of real-world objects. Real-time decision-making applications such as autonomous driving heavily rely on 3D information to navigate an urban environment. LiDAR data processing is, however, very complex and resource-intensive. Deep learning on point cloud is a recent advancement that is aimed to extract 3D information. Deep learning implementations include procedures where raw points are fed to neural networks and converted to 3D voxels. Individual voxels are fed to 3D convolutional layers and techniques that transform the 3D points into 2D images and utilize the well-established 2D CNNs. Of these, the two former methods are majorly reviewed, while the projection-based methods are less reviewed although the technique is widely used in numerous applications. To fill the gap, this paper examines the existing literature on projection-based methods by detailing the recent progress made. Identifying the state-of-the-art methodology and summarizing the important interventions are among the significant tasks covered in this paper.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-022-03930-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13094-6,Learning-based bypass zone search algorithm for fast motion estimation,Multimedia Tools and Applications,10.1007/s11042-022-13094-6,Springer,2022-07-11,"Video coding has been widely explored by academia and industry in recent years, mainly due to the great popularization of video applications and multimedia-capable devices. The Motion Estimation (ME) process receives special attention since it is one of the most complex steps in video coding. The Test Zone Search (TZS) is the main algorithm employed for integer ME in recent video codecs, such as those based on the High Efficiency Video Coding (HEVC), and has been used in the standardization process of the future Versatile Video Coding (VVC) standard. However, even though it is designed as a fast ME algorithm, the computational effort required by TZS is still very high, compromising the encoding process in multimedia-capable devices that operate on limited energy or computational resources. This work presents the Bypass Zone Search (BZS) algorithm, a learning-based solution for fast ME that improves TZS, aiming at a better tradeoff between compression efficiency and computational cost. First, a set of analyses on TZS is presented, which allowed the design of two strategies to reduce the ME computational cost. The first one, named as Learning-based Bypass Motion Estimation (LBME), consists of a machine learning-based approach that predicts whether the best motion vector has already been found and bypasses the remaining ME steps. The second strategy, named as Astroid Raster Pattern (ARP), is a novel search pattern developed for the most complex TZS step, the Raster Search. By combining the two proposed strategies in BZS, the ME processing time is reduced by 60.98% (Random Access) and 63.05% (Low Delay) in comparison to TZS. The overall HEVC encoding time is reduced by 14.32% (Random Access) and 17.64% (Low Delay), with a negligible loss of 0.0837% (Random Access) and 0.04% (Low Delay) in BD-rate.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13094-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13278-022-00906-8,Predicting the type and target of offensive social media posts in Marathi,Social Network Analysis and Mining,10.1007/s13278-022-00906-8,Springer,2022-07-09,"The presence of offensive language on social media is very common motivating platforms to invest in strategies to make communities safer. This includes developing robust machine learning systems capable of recognizing offensive content online. Apart from a few notable exceptions, most research on automatic offensive language identification has dealt with English and a few other high-resource languages such as French, German, and Spanish. In this paper, we address this gap by tackling offensive language identification in Marathi, a low-resource Indo-Aryan language spoken in India. We introduce the Marathi Offensive Language Dataset v.2.0 or MOLD 2.0 and present multiple experiments on this dataset. MOLD 2.0 is a much larger version of MOLD with expanded annotation to the levels B (type) and C (target) of the popular OLID taxonomy. MOLD 2.0 is the first hierarchical offensive language dataset compiled for Marathi, thus opening new avenues for research in low-resource Indo-Aryan languages. Finally, we also introduce SeMOLD , a larger dataset annotated following the semi-supervised methods presented in SOLID (Rosenthal et al. in SOLID: a large-scale semi-supervised dataset for offensive language identification. In: Findings of ACL, 2021).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13278-022-00906-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s43246-022-00270-2,Deep learning for the rare-event rational design of 3D printed multi-material mechanical metamaterials,Communications Materials,10.1038/s43246-022-00270-2,Nature,2022-07-09,"Multi-material 3D printing techniques are now enabling the rational design of metamaterials with both complex geometries and multiple materials compositions. Here, deep-learning methods are used to identify, among planar network structures, the rare designs that yield very unusual and desirable combinations of materials properties. Emerging multi-material 3D printing techniques enables the rational design of metamaterials with not only complex geometries but also arbitrary distributions of multiple materials within those geometries, yielding unique combinations of elastic properties. However, discovering the rare designs that lead to highly unusual combinations of material properties, such as double-auxeticity and high elastic moduli, remains a non-trivial crucial task. Here, we use computational models and deep learning algorithms to identify rare-event designs. In particular, we study the relationship between random distributions of hard and soft phases in three types of planar lattices and the resulting mechanical properties of the two-dimensional networks. By creating a mapping from the space of design parameters to the space of mechanical properties, we are able to reduce the computational time required for evaluating each design to ≈2.4 × 10^−6 s, and to make the process of evaluating different designs highly parallelizable. We then select ten designs to be 3D printed, mechanically test them, and characterize their behavior using digital image correlation to validate the accuracy of our computational models. Our simulation results show that our deep learning-based algorithms can accurately predict the mechanical behavior of the different designs and that our modeling results match experimental observations.",https://www.nature.com/articles/s43246-022-00270-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-022-09985-6,Machine learning techniques for speech emotion recognition using paralinguistic acoustic features,International Journal of Speech Technology,10.1007/s10772-022-09985-6,Springer,2022-07-08,"Speech emotion recognition is one of the fastest growing areas of interest in the field of affective computing. Emotion detection aids human–computer interaction and finds application in a wide gamut of sectors, ranging from healthcare to retail to education. The present work strives to provide a speech emotion recognition framework that is both reliable and efficient enough to work in real-time environments. Speech emotion recognition can be performed using linguistic as well as paralinguistic aspects of speech; this work focusses on the latter, using non-lexical or paralinguistic attributes of speech like pitch, intensity and mel-frequency cepstral coefficients to train supervised machine learning models for emotion recognition. A combination of prosodic and spectral features is used for experimental analysis and classification is performed using algorithms like Gaussian Naïve Bayes, Random Forest, k -Nearest Neighbours, Support Vector Machine and Multilayer Perceptron. The choice of these ML models was based on the swiftness with which they could be trained, making them more suitable for real-time applications. Comparative analysis of the models reveals SVM and MLP to be the best performers with 77.86% and 79.62% accuracies respectively. The performance of these classifiers is compared with benchmark results in literature, and a significant improvement over state-of-the-art models is presented. The observations and findings of this work can be applied to design real-time emotion recognition frameworks that can be used to design and develop applications and technologies for various domains.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-022-09985-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12559-022-10041-3,An Investigation to Identify Optimal Setup for Automated Assessment of Dysarthric Intelligibility using Deep Learning Technologies,Cognitive Computation,10.1007/s12559-022-10041-3,Springer,2022-07-07,"Recent advances in deep learning have provided an opportunity to improve and automate dysarthria intelligibility assessment, offering a cost-effective, accessible, and less subjective way to assess dysarthric speakers. However, reviewing previous literature in the area determines that the generalization of results on new dysarthric patients was not measured properly or incomplete among the previous studies that yielded very high accuracies due to the gaps in the adopted evaluation methodologies. This is of particular importance as any practical and clinical application of intelligibility assessment approaches must reliably generalize on new patients; otherwise, the clinicians cannot accept the assessment results provided by the system deploying the approach. In this paper, after these gaps are explained, we report on our extensive investigation to propose a deep learning–based dysarthric intelligibility assessment optimal setup. Then, we explain different evaluation strategies that were applied to thoroughly verify how the optimal setup performs with new speakers and across different classes of speech intelligibility. Finally, a comparative study was conducted, benchmarking the performance of our proposed optimal setup against the state of the art by adopting similar strategies previous studies employed. Results indicate an average of 78.2% classification accuracy for unforeseen low intelligibility speakers, 40.6% for moderate intelligibility speakers, and 40.4% for high intelligibility speakers. Furthermore, we noticed a high variance of classification accuracies among individual speakers. Finally, our proposed optimal setup delivered an average of 97.19% classification accuracy when adopting a similar evaluation strategy used by the previous studies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12559-022-10041-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13216-0,Automated image and video object detection based on hybrid heuristic-based U-net segmentation and faster region-convolutional neural network-enabled learning,Multimedia Tools and Applications,10.1007/s11042-022-13216-0,Springer,2022-07-07,"Object detection is one of the major areas of computer vision, which adopts machine learning approaches in diverse contributions. Nowadays, the machine learning field has been directed through Deep Neural Networks (DNNs) that takes eminent features of progressions in data availability and computing power. In all the cases, the quality of images and videos are biased and noisy, and thus, the distributions of data are also considered as imbalanced and disturbed. Different techniques are developed for solving the abovementioned challenges, which are mostly considered based on deep learning and computer vision. Though, traditional algorithms constantly offer poor detection for dense and small objects and yet fail the detection of objects through random geometric transformations. One of the categories of deep learning called Convolutional Neural Network (CNN) is famous and well-matched method for image-related tasks, in which the network is trained for discovering the numerous features like colour differences, corners, and edges in the images and videos that are combined into more complex shapes. This proposal intends to develop improved object detection in images and videos with the advancements of deep learning models. The three main phases of the proposed object detection model are (a) pre-processing, (b) segmentation, and (c) detection. Once the pre-processing of the image is performed by median filtering approach, the adaptive U-Net segmentation is performed for the object segmentation using the newly proposed Sun Flower-Deer Hunting Optimization Algorithm (SF-DHOA). The maximization of segmentation accuracy and dice coefficient is considered as the main objective of the proposed segmentation. The hybrid meta-heuristic algorithm termed SF-DHOA is proposed with Sun Flower Optimization (SFO) and Deer Hunting Optimization Algorithm (DHOA), which is used for optimally tuning the U-Net by optimizing the encoder depth and the number of epoch. Further, the detection is performed by the modified Faster Region-Convolutional Neural Network (Faster-RCNN), in which the optimization of number of epoch is performed by hybrid SF-DHOA algorithm with the intention of minimizing the error and training loss function. The performance of the proposed algorithm is evaluated, and the proposed algorithm shows high improvement when compared to existing deep learning-based algorithms.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13216-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-022-04672-4,Classification and interaction of new media instant music video based on deep learning under the background of artificial intelligence,The Journal of Supercomputing,10.1007/s11227-022-04672-4,Springer,2022-07-06,"With the continuous upgrading and improvement in the Internet and terminal equipment, many instant music videos share information with users through social platforms. This study explores the impact of new media technology on the content of instant music videos on the Internet under Artificial Intelligence (AI) technology to effectively distinguish the elegant and vulgar short videos and improve the quality of short videos on the Internet. Obscene and harmful instant music videos in the massive data are the bottleneck for its development. An improved deep learning model is proposed based on OPEN_NSFW using the AI image detection system technology of the Internet of Things with a powerful processing ability to image information. Experiments demonstrate that this model significantly reduces the false positive rate and improves the recall compared with the traditional machine learning computing model. Besides, it improves the accuracy when discriminating whether the publisher’s head image involves eroticism. In addition, this model can identify and classify the main content of instant music videos to optimize the content. This work provides the characteristic basis for the algorithm to judge and protect the original content. Combining algorithm recommendations and strengthening manual intervention promotes online instant music videos' sustainable and healthy development. These findings can provide an excellent technical guarantee and experimental references for the standardized development of the instant music video industry in the future.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-022-04672-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-022-03507-2,Evolutionary digital twin model with an agent-based discrete-event simulation method,Applied Intelligence,10.1007/s10489-022-03507-2,Springer,2022-07-06,"A digital twin model provides the ability to adjust candidate behavior based on feedback from its physical part. However, small interactions between different subsystems and using real-time data about physical workshops are the primary problems in digital twin models. The essence of the digital twin model is the combination of the physical simulation method and the data-driven simulation method, and agent-based discrete-event modeling theory is an advanced way to build a digital twin model. Thus, the theoretical framework of the Digital Twin Workshop model is improved from the underlying modeling logic based on this new theory. By combining reinforcement learning with the digital twin workshop model, an evolutionary digital twin workshop model is developed in this study. This model is then applied to a real-world case. A comparison is made between the Digital Twin Workshop model with reinforcement learning policy and a heuristic policy and a random policy. The simulation results verify the validity and performance of the proposed model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-022-03507-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40561-022-00205-x,Is Metaverse in education a blessing or a curse: a combined content and bibliometric analysis,Smart Learning Environments,10.1186/s40561-022-00205-x,Springer,2022-07-06,"The Metaverse has been the centre of attraction for educationists for quite some time. This field got renewed interest with the announcement of social media giant Facebook as it rebranding and positioning it as Meta. While several studies conducted literature reviews to summarize the findings related to the Metaverse in general, no study to the best of our knowledge focused on systematically summarizing the finding related to the Metaverse in education. To cover this gap, this study conducts a systematic literature review of the Metaverse in education. It then applies both content and bibliometric analysis to reveal the research trends, focus, and limitations of this research topic. The obtained findings reveal the research gap in lifelogging applications in educational Metaverse. The findings also show that the design of Metaverse in education has evolved over generations, where generation Z is more targeted with artificial intelligence technologies compared to generation X or Y. In terms of learning scenarios, there have been very few studies focusing on mobile learning, hybrid learning, and micro learning. Additionally, no study focused on using the Metaverse in education for students with disabilities. The findings of this study provide a roadmap of future research directions to be taken into consideration and investigated to enhance the adoption of the Metaverse in education worldwide, as well as to enhance the learning and teaching experiences in the Metaverse.",https://www.biomedcentral.com/openurl?doi=10.1186/s40561-022-00205-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12206-022-0628-2,Bi-directional evolutionary 3D topology optimization with a deep neural network,Journal of Mechanical Science and Technology,10.1007/s12206-022-0628-2,Springer,2022-07-06,"The FEM-based topology optimization repeats usually finite element analyses many times to converge to the stopping criteria. If the near-optimal topology data are available in advance at the beginning of an optimization process, the iterative computation could be greatly reduced. In an effort to obtain swiftly optimum topology solutions, the deep learning and neural networks with a special segmentation scheme of digital images are combined with the BESO (bi-directional evolutionary structural optimization) topology method in this study. The pre-trained digital images of 3200 optimum topologies construct the design domain for the main topology optimization. Additionally, a new post-processor is developed in order to reconstruct the relative locations among finite elements in the raw outputs generated by the neural network. The proposed method has been demonstrated to be efficient in lowering the iterations with several 2D and 3D optimization examples. The iteration counts can be reduced 63% for a 2D example and by 72.5% for a 3D one, compared to BESO results alone.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12206-022-0628-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13278-022-00905-9,Speech processing for early Parkinson’s disease diagnosis: machine learning and deep learning-based approach,Social Network Analysis and Mining,10.1007/s13278-022-00905-9,Springer,2022-07-04,"Speech production disorders during Parkinson’s Disease (PD) stand for one of the clinical markers which are representative of the evolution of motor and cognitive disability. Neurologists and scientists are currently searching for non-medical methods relying on speech signal analysis to control the assessment of speech disorders in Parkinsonian patients. In this research work, we propose a speech processing approach for early Parkinson disease diagnosis. In order to elaborate this work, we suggest using Support Vector Machines (SVM) as a machine learning method to classify data. Our database contains voice recordings of healthy people and PD patients. As far as this study is concerned, we set forward three types of features. Firstly, we invest the Mel Frequency Cepstral Coefficients (MFCC). Secondly, we use the deep features selected by AutoEncoder (AE). Finally, we introduce novel characteristics based on Gaussian Mixture Models-Universal Background Model (GMM-UBM) to extract the MFCC-GMM features. Our proposed characteristics: deep features-based AutoEncoder and MFCC-GMM, always present the highest detection accuracy 99% and 100%. This proves that our approach based on speech can detect the PD without having a medical test.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13278-022-00905-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13366-1,Wuthering heights: gauging fear at altitude in virtual reality,Multimedia Tools and Applications,10.1007/s11042-022-13366-1,Springer,2022-07-04,"In this study we propose an approach to assess the fear of heights through a 3D virtual reality environment. We show that an immersive scenario provides a suitable infrastructure to such purpose, when supported by related behavioural and physiological measurements. Our approach is grounded in the principled framework of constructed emotions. This allows to shape fear detection as a case of categorical perception, which is amenable to be formalised as an unsupervised learning problem. Meanwhile, it paves the way for addressing meaningful physiological parameters for the assessment. Gauging fear of heights in individuals, beyond its theoretical relevance, is cogent for the early discernment of workers who are unsuited for operating at altitude and who may require to undergo specific training or, eventually, to be recruited for different positions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13366-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13365-2,User attention and behaviour in virtual reality art encounter,Multimedia Tools and Applications,10.1007/s11042-022-13365-2,Springer,2022-07-04,"With the proliferation of consumer virtual reality (VR) headsets and creative tools, content creators are experimenting with new forms of interactive audience experience using immersive media. Understanding user attention and behaviours in virtual environment can greatly inform the creative processes in VR. We developed an abstract VR painting and an experimentation system to study audience art encounters through eye gaze and movement tracking. The data from a user experiment with 35 participants reveal a range of user activity patterns in art exploration. Deep learning models are used to study the connections between the behavioural data and the audience’s background. The work also introduced new integrated methods to visualise user attention for content creators.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13365-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00500-022-07144-2,Teaching English as a foreign language using the Internet and multimedia environment,Soft Computing,10.1007/s00500-022-07144-2,Springer,2022-07-04,"Under the continuous development of China’s social economy, the Internet and multimedia technology have been developed and innovated and are widely used in all walks of life and all fields. Because of their powerful interactivity, effectiveness, and dynamism, combining them with colleges and universities’ English teaching plays an essential role in enriching teaching resources and improving teaching efficiency and quality. This paper proposes an improved LSTM model based on the web and multimedia environment, which improves the prediction effect of neural networks by adding an attention mechanism to the LSTM layer. The attention mechanism is based on a decision support system that automates the availability of various teaching resources to ensure an interactive teaching and learning environment in the colleges and universities. The decision support system of attention mechanism enhances the model's generalization ability by tuning the model parameters with the genetic algorithm (GA). Finally, this paper proposes some suggestions on improving the effectiveness of teaching English as a foreign language in the online and multimedia environment, taking into account the popular MU teaching format and English APP. The results show that the improved LSTM model outperforms the existing models in terms of various performance metrics.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-022-07144-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10462-022-10224-2,A survey on deep reinforcement learning for audio-based applications,Artificial Intelligence Review,10.1007/s10462-022-10224-2,Springer,2022-07-02,"Deep reinforcement learning (DRL) is poised to revolutionise the field of artificial intelligence (AI) by endowing autonomous systems with high levels of understanding of the real world. Currently, deep learning (DL) is enabling DRL to effectively solve various intractable problems in various fields including computer vision, natural language processing, healthcare, robotics, to name a few. Most importantly, DRL algorithms are also being employed in audio signal processing to learn directly from speech, music and other sound signals in order to create audio-based autonomous systems that have many promising applications in the real world. In this article, we conduct a comprehensive survey on the progress of DRL in the audio domain by bringing together research studies across different but related areas in speech and music. We begin with an introduction to the general field of DL and reinforcement learning (RL), then progress to the main DRL methods and their applications in the audio domain. We conclude by presenting important challenges faced by audio-based DRL agents and by highlighting open areas for future research and investigation. The findings of this paper will guide researchers interested in DRL for the audio domain.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10462-022-10224-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12065-022-00740-z,Optimization for reinforcement learning based 3D animation exercise,Evolutionary Intelligence,10.1007/s12065-022-00740-z,Springer,2022-07-02,"3D animation makes art “live” and gives works of art with vitality, thus enabling artistic charm to be better presented and artistic practice to create more value. Due to the superior 3D model creation of 3D animation, it has important practical prospects in the field of art. At the same time, 3D animation is an important part of the animation industry, and its artistic style affects the quality of animation works and derivatives. Against the backdrop of computer technology, computer has a great influence on 3D animation art, which greatly enhances the artistic expression of 3D animation and promotes new animation forms. The artistic expression of 3D animation is mainly reflected in the performance of light and film, space and motion, lens, details and texture, and traditional artistic expression of 3D technology. However, when viewing 3D animation, we are often affected by the network bandwidth which leads to lag in 3D animation video and affects the user Quality of experience (QoE). According to the duration of 3D animation videos, this paper divides them into three types and obtains better user QoE through reinforcement learning (RL). The datasets of 3D animation videos of different durations are used to train the RL model, and the corresponding reward function parameters of different 3D animation video durations are obtained. Therefore, duration-sensitive Asynchronous Advantage Actor Critic (A3C)—RL algorithm is presented. The experimental results show that the A3C-RL algorithm has a much better user QoE for 3D animation video than that of state-of-the-art algorithms.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12065-022-00740-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11063-022-10929-z,Hybrid 3D/2D Complete Inception Module and Convolutional Neural Network for Hyperspectral Remote Sensing Image Classification,Neural Processing Letters,10.1007/s11063-022-10929-z,Springer,2022-07-02,"Classification in hyperspectral remote sensing images (HRSIs) is a challenging process in image analysis and one of the most popular topics. In recent years, many methods have been proposed to solve the HRSIs classification problem. Compared to traditional machine learning methods, deep learning, especially convolutional neural networks (CNNs), is commonly used in the classification of HRSIs. Deep learning-based methods based on CNNs show remarkable performance in HRSIs classification and greatly support the development of classification technology. In this study, a method in which the Hybrid 3D/2D Complete Inception module and the Hybrid 3D/2D CNN method are used together has been proposed to solve the HRSIs classification problem. In the proposed method, multi-level feature extraction is performed by using multiple convolution layers with the Inception module. This improves the performance of the network. Conventional CNN-based methods use 2D CNN for feature extraction. However, only spatial features are extracted with 2D CNN. 3D CNN is used to extract spatial-spectral features. However, 3D CNN is computationally complex. Therefore, in the proposed method, a hybrid approach is used by first using 3D CNN and then 2D CNN. This reduces computational complexity and extracts more spatial features. In addition, PCA is used as a preprocessing step for optimum spectral band extraction in the proposed method. The proposed method has been tested using Indian pines, Salinas, University of Pavia, HyRANK-Loukia and Houston datasets, which are frequently used in studies for HRSIs classification. The overall accuracy of the proposed method in these five datasets are 99.83%, 100%, 100%, 90.47% and 98.93%, respectively. These results reveal that the proposed method provides higher classification performance compared to state-of-the-art methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11063-022-10929-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-022-04206-z,Advanced multi-GANs towards near to real image and video colorization,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-022-04206-z,Springer,2022-07-02,"Multi-GANs, inspired by traditional GAN, divide each problem space into several smaller and more homogeneous subspaces. It is an architecture of multiple generative adversarial networks that work together to achieve the highest output quality. This paper presents Advanced Multi-GANs architecture for colorization based on two novelties, including the cluster numbers and the color harmonies. Advanced Multi-GANs can intelligently decide the number of clusters using the input test image and its scene complexity, leading to much more realistic colorization. Also, color harmony, which defines a rational relation between pixels of frames and their generated colors, is proposed to keep the harmony of the colors among a sequence of frames in video colorizing. Color harmony helps avoid changing the colors of the same objects between video frames. In experimental results, the evaluation of this study with several protocols, including image and video colorization, is provided. In addition to visual qualitative evaluation, the performance of the proposed method is quantitatively measured in the Advanced Multi-GAN framework. The experimental results show much more realistic outputs in comparison to the traditional approaches and state-of-the-art.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-022-04206-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11277-022-09640-y,Survey of Deep Learning Paradigms for Speech Processing,Wireless Personal Communications,10.1007/s11277-022-09640-y,Springer,2022-07-01,"Over the past decades, a particular focus is given to research on machine learning techniques for speech processing applications. However, in the past few years, research has focused on using deep learning for speech processing applications. This new machine learning field has become a very attractive area of study and has remarkably better performance than the others in the various speech processing applications. This paper presents a brief survey of application deep learning for various speech processing applications such as speech separation, speech enhancement, speech recognition, speaker recognition, emotion recognition, language recognition, music recognition, speech data retrieval, etc. The survey goes on to cover the use of Auto-Encoder, Generative Adversarial Network, Restricted Boltzmann Machine, Deep Belief Network, Deep Neural Network, Convolutional Neural Network, Recurrent Neural Network and Deep Reinforcement Learning for speech processing. Additionally, it focuses on the various speech database and evaluation metrics used by deep learning algorithms for performance evaluation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11277-022-09640-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12399-w,Augmenting machine learning for Amharic speech recognition: a paradigm of patient’s lips motion detection,Multimedia Tools and Applications,10.1007/s11042-022-12399-w,Springer,2022-07-01,"The method of automatic lip motion recognition is an essential input for visual speech detection. It is a technological approach to demystify people who are hard to hear, deaf, and a challenge of silent communication in day-to-day life. However, the recognition process is a challenge in terms of pronunciation variation, speech speeds, gesture variation, color, makeup, the video quality of the camera, and the way of feature extraction. This paper proposed a solution for automatic lip motion recognition by identifying lip movements and characterizing their association with the spoken words for the Amharic language spoken using the information available in lip movements. The input video is converting into consecutive image frames. We use a Viola-Jones object detection algorithm to gain YIQ color space and apply the saturation components to detect lip images from the face area. Sobel’s edge detection and morphological image operations implement to identify and extract the exact contour of the lip. We applied ANN and SVM classifiers on averaging shape information features, and we gained 65.71% and 66.43% classification accuracies of ANN and SVM, respectively. The findings presented in the Amharic Speech Recognition is the newly introduced technology to enhance the academic and linguistic skills of hearing-problem people, health domain experts, physicians, researchers, etc. The future research work presents in the light of the findings.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12399-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-022-02586-3,Deep learning-based 2D/3D registration of an atlas to biplanar X-ray images,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-022-02586-3,Springer,2022-07-01,"Purpose The registration of a 3D atlas image to 2D radiographs enables 3D pre-operative planning without the need to acquire costly and high-dose CT-scans. Recently, many deep-learning-based 2D/3D registration methods have been proposed which tackle the problem as a reconstruction by regressing the 3D image immediately from the radiographs, rather than registering an atlas image. Consequently, they are less constrained against unfeasible reconstructions and have no possibility to warp auxiliary data. Finally, they are, by construction, limited to orthogonal projections. Methods We propose a novel end-to-end trainable 2D/3D registration network that regresses a dense deformation field that warps an atlas image such that the forward projection of the warped atlas matches the input 2D radiographs. We effectively take the projection matrix into account in the regression problem by integrating a projective and inverse projective spatial transform layer into the network. Results Comprehensive experiments conducted on simulated DRRs from patient CT images demonstrate the efficacy of the network. Our network yields an average Dice score of 0.94 and an average symmetric surface distance of 0.84 mm on our test dataset. It has experimentally been determined that projection geometries with 80 $$^{\circ }$$ ∘ to 100 $$^{\circ }$$ ∘ projection angle difference result in the highest accuracy. Conclusion Our network is able to accurately reconstruct patient-specific CT-images from a pair of near-orthogonal calibrated radiographs by regressing a deformation field that warps an atlas image or any other auxiliary data. Our method is not constrained to orthogonal projections, increasing its applicability in medical practices. It remains a future task to extend the network for uncalibrated radiographs.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-022-02586-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12817-z,Intelligent stuttering speech recognition: A succinct review,Multimedia Tools and Applications,10.1007/s11042-022-12817-z,Springer,2022-07-01,"Stuttering speech recognition is a well-studied concept in speech signal processing. Classification of speech disorder is the main focus of this study. Classification of stuttered speech is becoming more important with the enhancement of machine learning and deep learning. In this study, some of the recent and most influencing stuttering speech recognition methods are reviewed with a discussion on different categories of stuttering. The stuttering speech recognition process is divided mainly into four segments-input speech pre-emphasis, segmentation, feature extraction, and stutter classification. All these segments are briefly elaborated and related researches are discussed. It is observed that different traditional machine learning and deep learning classification approaches are employed to recognize stuttered speech in last few decades. A comprehensive analysis is presented on different feature extraction and classification method with their efficiency.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12817-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00607-021-01046-1,Deep reinforcement learning based QoE-aware actor-learner architectures for video streaming in IoT environments,Computing,10.1007/s00607-021-01046-1,Springer,2022-07-01,"The number of connected smart devices enabling multimedia applications has expanded tremendously in Internet-of-Things (IoT) environments. Specifically, the requirement for a high quality of experience (QoE) for video streaming services is a crucial prerequisite for a range of use cases, including smart surveillance, smart healthcare, smart agriculture and many more. However, providing a high QoE for video streaming is challenging due to underlying dynamic network conditions. To address this issue, several adaptive bit rate (ABR) algorithms based on predetermined rules have been developed. However, they do not generalize well to a wide variety of network conditions. ABR algorithms, based on reinforcement learning (RL), have been proven to be more effective at generalizing to varying network conditions but they still have limitations, specifically, constrained exploration and high variance in value estimates. In this paper, we propose asynchronous advantage actor-critic (A3C) based actor-learner architectures for generating the adaptive bit rates for video streaming in IoT environments. To address the existing issues, we propose integrating two advanced A3C algorithms: Follow then Forage Exploration (FFE) and Averaged A3C. We demonstrate their efficacy in improving the QoE over vanilla A3C. Additionally, we also demonstrate the benefits of the proposed architecture for video streaming under different network conditions and for different variants of the QoE metric. We show that advanced A3C methods provide up to 30.70% improvement in QoE over vanilla A3C and a considerably higher QoE over other fixed-rule-based ABR algorithms.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00607-021-01046-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-10973-2,Automatic source scanner identification using 1D convolutional neural network,Multimedia Tools and Applications,10.1007/s11042-021-10973-2,Springer,2022-07-01,"In this digital world, digitized documents can be considered original or a piece of evidence; checking the authenticity of any suspicious image has become an unavoidable concern to preserve the trust in its legitimacy. However, identifying the source of a digital image without any prior embedded information is a very challenging task. This paper proposes a novel one-dimensional convolutional neural network (1D-CNN) model to solve the source scanner identification (SSI) problem blindly. Unlike traditional methods based on handcrafted features, the proposed framework can dynamically learn and extract scanner device-specific features. This work, comprised of the 1D-CNN and a support vector machine (SVM) as a classifier, was trained on nine scanners of different brands and models. The experimental result shows that our model achieves 98.15% accuracy on full images and overall accuracy of 93.13% on segments from test images, outperforming other state-of-art approaches. Our model also proves to be able to distinguish between scanners of the same model. Furthermore, the SVM classifier improved the 1D-CNN accuracy by approximately 3% compared to its original configuration.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-10973-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00330-022-08573-1,Deep learning–based tumour segmentation and total metabolic tumour volume prediction in the prognosis of diffuse large B-cell lymphoma patients in 3D FDG-PET images,European Radiology,10.1007/s00330-022-08573-1,Springer,2022-07-01,"Objectives To demonstrate the effectiveness of automatic segmentation of diffuse large B-cell lymphoma (DLBCL) in 3D FDG-PET scans using a deep learning approach and validate its value in prognosis in an external validation cohort. Methods Two PET datasets were retrospectively analysed: 297 patients from a local centre for training and 117 patients from an external centre for validation. A 3D U-Net architecture was trained on patches randomly sampled within the PET images. Segmentation performance was evaluated by six metrics, including the Dice similarity coefficient (DSC), Jaccard similarity coefficient (JSC), sensitivity (Se), positive predictive value (PPV), Hausdorff distance 95 (HD 95), and average symmetric surface distance (ASSD). Finally, the prognostic value of predictive total metabolic tumour volume (pTMTV) was validated in real clinical applications. Results The mean DSC, JSC, Se, PPV, HD 95, and ASSD (with standard deviation) for the validation cohort were 0.78 ± 0.25, 0.69 ± 0.26, 0.81 ± 0.27, 0.82 ± 0.25, 24.58 ± 35.18, and 4.46 ± 8.92, respectively. The mean ground truth TMTV (gtTMTV) and pTMTV were 276.6 ± 393.5 cm^3 and 301.9 ± 510.5 cm^3 in the validation cohort, respectively. Perfect homogeneity in the Bland–Altman analysis and a strong positive correlation in the linear regression analysis ( R ^2 linear = 0.874, p < 0.001) were demonstrated between gtTMTV and pTMTV. pTMTV (≥ 201.2 cm^3) (PFS: HR = 3.097, p = 0.001; OS: HR = 6.601, p < 0.001) was shown to be an independent factor of PFS and OS. Conclusions The FCN model with a U-Net architecture can accurately segment lymphoma lesions and allow fully automatic assessment of TMTV on PET scans for DLBCL patients. Furthermore, pTMTV is an independent prognostic factor of survival in DLBCL patients. Key Points • The segmentation model based on a U-Net architecture shows high performance in the segmentation of DLBCL patients on FDG-PET images . • The proposed method can provide quantitative information as a predictive TMTV for predicting the prognosis of DLBCL patients .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00330-022-08573-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-022-04371-0,A hybrid deep learning approach for skin cancer diagnosis using subband fusion of 3D wavelets,The Journal of Supercomputing,10.1007/s11227-022-04371-0,Springer,2022-07-01,"An automated system for skin cancer diagnosis is needed for early diagnosis to reduce the mortality rate of skin cancer. Noninvasive clinical routines are essential for diagnosis, but they are largely subjective. In this study, a hybrid deep learning (HDL) approach that uses subband fusion of 3D wavelets is proposed. It is a noninvasive and objective method for inspecting skin images. In the first stage of the HDL approach, simple median filtering is used to remove unwanted information such as hair and noise. In the second stage, the 3D wavelet transform is applied to obtain textural information from the dermoscopic image via a subband fusion approach. In the final stage, multiclass classification is performed by the HDL approach using the fused subband. The performance results of the HDL approach on PH^2 database images indicate that it can discriminate normal, benign, and malignant skin images effectively with 99.33% average accuracy and more than 90% sensitivity and specificity. This study confirms the observation that the HDL approach can realize improved classification results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-022-04371-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12251-1,Learning a perceptual manifold with deep features for animation video resequencing,Multimedia Tools and Applications,10.1007/s11042-022-12251-1,Springer,2022-07-01,"We propose a novel deep learning framework for animation video resequencing. Our system produces new video sequences by minimizing a perceptual distance of images from an existing animation video clip. To measure perceptual distance, we utilize the activations of convolutional neural networks and learn a perceptual distance by training these features on a small network with data comprised of human perceptual judgments. We show that with this perceptual metric and graph-based manifold learning techniques, our framework can produce new smooth and visually appealing animation video results for a variety of animation video styles. In contrast to previous work on animation video resequencing, the proposed framework applies to wide range of image styles and does not require hand-crafted feature extraction, background subtraction, or feature correspondence. In addition, we also show that our framework has applications to appealingly arrange unordered collections of images.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12251-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02135-0,"
              
                
              
              $$\hbox {PISEP}{^2}$$
              
                
                  PISEP
                  
                    
                    2
                  
                
              
            : pseudo-image sequence evolution-based 3D pose prediction",The Visual Computer,10.1007/s00371-021-02135-0,Springer,2022-07-01,"Pose prediction is to predict future poses given a window of previous poses. In this paper, we propose a new problem that predicts poses using 3D positions of skeletal sequences.Different from the traditional pose prediction based on mocap frames, this problem is convenient to use in real applications due to its simple sensors to capture data. We also present a new framework, pseudo-image sequence evolution-based 3D pose prediction, to address this new problem. Specifically, a skeletal representation is proposed by transforming a 3D skeletal sequence into an image sequence, which can model different correlations among different joints. With this image-based skeletal representation, we model the pose prediction as the evolution of an image sequence. Moreover, a novel inference network is proposed to predict multiple future poses in a non-recursive manner using decoders with independent parameters. In contrast to the recursive sequence-to-sequence model, we can improve the computational efficiency and avoid error accumulations significantly. Extensive experiments are carried out on two benchmark datasets (e.g., G3D and FNTU). The proposed method achieves state-of-the-art performance on both datasets, which demonstrates the effectiveness of our proposed method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02135-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-022-04416-4,Emotion recognition models for companion robots,The Journal of Supercomputing,10.1007/s11227-022-04416-4,Springer,2022-07-01,"There has been a steep increase in the use of machine learning for various healthcare applications like the diagnosis of diseases, drug discovery, medical image analysis, etc. Machine learning solutions are proven to be more efficient and less time-consuming than conventional approaches. In this paper, we leverage the advantages of machine learning models to enable a humanoid robot to assist mental health patients. Facial expression and human voice are some of the most demonstrative ways to analyze human emotions, especially for the mentally challenged. We carry out this assistance by constantly monitoring the patient’s voice (or audio) and facial expressions to predict human emotions. To implement the model of audio monitoring, we train three different machine learning and deep learning models to compare and choose the better model. Similarly, for facial recognition, we train a deep learning model using a specific dataset to predict facial expressions from the video captured in real time. We then integrate the better-performing machine learning models into a web interface for demonstration.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-022-04416-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00170-022-09376-w,Mechanical response assessment of antibacterial PA12/TiO_2 3D printed parts: parameters optimization through artificial neural networks modeling,The International Journal of Advanced Manufacturing Technology,10.1007/s00170-022-09376-w,Springer,2022-07-01,"This study investigates the mechanical response of antibacterial PA12/TiO_2 nanocomposite 3D printed specimens by varying the TiO_2 loading in the filament, raster deposition angle, and nozzle temperature. The prediction of the antibacterial and mechanical performance of such nanocomposites is a challenging field, especially nowadays with the covid-19 pandemic dilemma. The experimental work in this study utilizes a fully factorial design approach to analyze the effect of three parameters on the mechanical response of 3D printed components. Therefore, all combinations of these three parameters were tested, resulting in twenty-seven independent experiments, in which each combination was repeated three times (a total of eighty-one experiments). The antibacterial performance of the fabricated PA12/TiO_2 nanocomposite materials was confirmed, and regression and arithmetic artificial neural network (ANN) models were developed and validated for mechanical response prediction. The analysis of the results showed that an increase in the TiO_2% loading decreased the mechanical responses but increased the antibacterial performance of the nanocomposites. In addition, higher nozzle temperatures and zero deposition angles optimize the mechanical performance of all TiO_2% nanocomposites. Independent experiments evaluated the proposed models with mean absolute percentage errors (MAPE) similar to the ANN models. These findings and the interaction charts show a strong interaction between the studied parameters. Therefore, the authors propose the improvement of predictions by utilizing artificial neural network models and genetic algorithms as future work and the spreading of the experimental area with extra variable parameters and levels.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00170-022-09376-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11438-2,An expert video surveillance system to identify and mitigate shoplifting in megastores,Multimedia Tools and Applications,10.1007/s11042-021-11438-2,Springer,2022-07-01,"Shoplifting has got serious concern because of a steep surge in these types of cases all around. People are found stealing the items from the store without being noticed, either by putting them in bags or hiding objects inside clothes. CCTV cameras are generally installed at any such site, but evidences suggest that these cameras are not very effective unless the video feeds are constantly monitored. Therefore, we intend to build an automated and intelligent surveillance system to catch these shoplifters by identifying their stealing actions. This article proposes a deep neural network-based solution to identify these shoplifting activities. The model proposed uses a dual-stream fusion-based network that effectively binds appearance and motion dynamics in the temporal domain to efficiently identify the shoplifting actions. The deep Inception V3 model is used to extract activity-specific body posture features from video streams through two deep neural network pipelines, one each corresponding to appearance and motion information. Next, a recurrent neural network, namely Long Short Term Memory (LSTM) network, is used to build a temporal relation between features extracted from consecutive frames in order to distinguish human stealing actions accurately. Added to it, this article introduces a shoplifting dataset synthesized in our lab, which contains normal human actions and object stealing actions. The proposed methodology supported with experimental results demonstrates encouraging outcomes with the accuracy achieved up to 91.48%, which outperforms other existing methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11438-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02111-8,SGRNN-AM and HRF-DBN: a hybrid machine learning model for cricket video summarization,The Visual Computer,10.1007/s00371-021-02111-8,Springer,2022-07-01,"Summarization is important in sports video analysis; it gives a more compact and interesting representation of content. The automatic cricket video summarization is more challenging as it contains several rules and longer match duration. In this research, a hybrid machine learning approach is proposed to summarize cricket video. It analyzes the excitement, object, and event-based features for the detection of key events from the cricket video. First, the audio is analyzed for the extraction of the exciting clips by using an adaptive threshold, speech-to-text framework, and Stacked Gated Recurrent Neural Network with Attention Module (SGRNN-AM). Then, the scenes of each exciting clip are classified with a new Hybrid Rotation Forest Deep Belief Network (HRF-DBN). Next, the characters and action features are extracted from the scorecard region of each key frame and umpire frames of exciting clips. Finally, SGRNN-AM model is used to detect key events including fours, sixes, and wickets. The accuracy of the proposed SGRNN-AM video summarization model is increased with an attention module in the hidden outputs of Gated Recurrent Unit (GRU) for selecting the significant features. The performance of the suggested technique has been improved on various collections of cricket videos. It achieved a precision of $$96.82\ \%$$ 96.82 % and an accuracy of $$96.32\%$$ 96.32 % that proves its effectiveness.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02111-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12763-w,An optimized machine translation technique for multi-lingual speech to sign language notation,Multimedia Tools and Applications,10.1007/s11042-022-12763-w,Springer,2022-07-01,"Due to the lack of assistive resources, hard-of-hearing people cannot live independently. Sign language or gesture language is the natural language and it is the primary mode of communication for hard-of-hearing people. Researchers and IT companies are continuously trying to find the best solutions to minimize the communication barriers for hearing-impaired people. Existing translation techniques for speech to sign language on the web platform are consuming higher resources. This study presents an optimized technique for direct machine translation of multi-lingual speech to Indian sign language using the HamNoSys notation system, whereas existing techniques were translating speech-text-HamNoSys. Performance comparison of both existing and the proposed techniques is analyzed in this study. The proposed technique optimizes the resources for the following parameters: CPU, heap memory, primary memory, and classes load. The result shows that the existing technique takes 220 MB heap memory, 10 threads, 2236 classes, and CPU for 12 s. The proposed technique consumes only 210.4 MB, 9 threads, 2113 classes, and CPU for 9 s.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12763-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12945-6,Data embedding in scrambled video by rotating motion vectors,Multimedia Tools and Applications,10.1007/s11042-022-12945-6,Springer,2022-07-01,"Data embedding in videos has several important applications including Digital Rights Management, preserving confidentiality of content, authentication and tampering detection. This paper proposes a novel data embedding solution in scrambled videos by rotating motion vectors of predicted macroblocks. The rotation of motion vectors and the propagation of motion compensation error serve another purpose, which is video scrambling. A compliant decoder uses machine learning to counter-rotate the motion vectors and extract embedded message bits. To achieve this, the decoder uses a sequence-dependent approach to train a classifier to distinguish between macroblocks reconstructed using rotated and un-rotated motion vectors. In the testing phase, motion vectors belonging to a classified macroblock are compared against the reviewed rotated motion vectors and the message bits are extracted. Furthermore, to guarantee accurate classification at the decoder, a constrained encoding approach is proposed in which data embedding is restricted to motion vectors that can be correctly counter-rotated at the decoder. The proposed solution is referred to as Classifying Rotated Vectors or CRVs for short. Experimental results revealed that scrambled videos can be reconstructed correctly without quality loss with a bitrate increase at the encoder of around 6% and an average data embedding rate of 1.68 bits per MB.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12945-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12398-x,IPM-Model: AI and metaheuristic-enabled face recognition using image partial matching for multimedia forensics investigation with genetic algorithm,Multimedia Tools and Applications,10.1007/s11042-022-12398-x,Springer,2022-07-01,"The rapid enhancement in the development of information technology has driven the development of human facial image recognition. Recently, facial recognition has been successfully applied in several distinct domains with the help of computing and information technology. This kind of application plays a significant role in the process of digital forensics investigation, recognizing the patterns of a human face based on the partial matching of images that would be in 24-bit color image format, including the spacing of the eyes, the bridging of the nose, the contour of the lips, ears, and chin. In this paper, we have proposed and implemented an image recognition model based on principal component analysis, genetic algorithms, and neural networks, in which PCA reduces the dimension of the benchmark dataset, while genetic algorithms and neural nets optimize the searching patterns of image matching and provide highly efficient output with a minimal amount of time. Through the experiment results on the human facial images dataset of the Georgia Institute of Technology, the overall match showed that the proposed model can achieve the recognition of human face images with an accuracy rate of 93.7%. Moreover, this model helps to examine, analyze, and detect individuals by partial matching with reidentification in the procedure of forensics investigation. The experimental result shows the robustness of the proposed model in terms of efficiency compared to other state-of-the-art methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12398-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00256-021-03984-5,Comparison of AI-powered 3D automated ultrasound tomography with standard handheld ultrasound for the visualization of the hands—clinical proof of concept,Skeletal Radiology,10.1007/s00256-021-03984-5,Springer,2022-07-01,"Objective To assess the ability of a newly developed AI-powered ultrasound 3D hand scanner to visualize joint structures in healthy hands and detect degenerative changes in cadaveric hands. Materials and Methods Twelve individuals (6 males, 6 females, age 43.5 ± 17.8 years) underwent four scans with the 3D ultrasound tomograph (right and left hand, dorsal and palmar, respectively) as well as four sets of handheld ultrasound of predefined anatomic regions. The 3D ultrasound tomographic images and the standard handheld ultrasound images were assessed by two radiologists with regard to visibility of bone contour, joint capsule and space, and tendons. In addition, three cadaveric hands were scanned with the 3D ultrasound tomograph and CT. Results Mean scan time for both hands was significantly faster with handheld ultrasound (10 min 30 s ± 95 s) compared to 3D ultrasound tomography (32 min 9 s ± 6 s; p  < 0.001). Interreader and intermodality agreement was moderate (0.4 <  κ  ≤ 0.6) to substantial (0.6 <  κ  ≤ 0.8). Overall visibility of joint structures was comparable between the modalities at the level of the wrist ( p  = 0.408), and significantly better with handheld ultrasound at the level of the finger joints and the thumb (both p  < 0.001). The 3D ultrasound tomograph was able to detect osteophytes in cadaveric hands which were confirmed by CT. Conclusion The AI-powered 3D ultrasound tomograph was able to visualize joint structures in healthy hands and singular osteophytes in cadaveric hands. Further technical improvements are necessary to shorten scan times and improve automated scanning of the finger joints and the thumb.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00256-021-03984-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-15364-7,Multimodal NASH prognosis using 3D imaging flow cytometry and artificial intelligence to characterize liver cells,Scientific Reports,10.1038/s41598-022-15364-7,Nature,2022-07-01,"To improve the understanding of the complex biological process underlying the development of non-alcoholic steatohepatitis (NASH), 3D imaging flow cytometry (3D-IFC) with transmission and side-scattered images were used to characterize hepatic stellate cell (HSC) and liver endothelial cell (LEC) morphology at single-cell resolution. In this study, HSC and LEC were obtained from biopsy-proven NASH subjects with early-stage NASH (F2-F3) and healthy controls. Here, we applied single-cell imaging and 3D digital reconstructions of healthy and diseased cells to analyze a spatially resolved set of morphometric cellular and texture parameters that showed regression with disease progression. By developing a customized autoencoder convolutional neural network (CNN) based on label-free cell transmission and side scattering images obtained from a 3D imaging flow cytometer, we demonstrated key regulated cell types involved in the development of NASH and cell classification performance superior to conventional machine learning methods.",https://www.nature.com/articles/s41598-022-15364-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00330-021-08533-1,Dynamic 3D radiomics analysis using artificial intelligence to assess the stage of COVID-19 on CT images,European Radiology,10.1007/s00330-021-08533-1,Springer,2022-07-01,"Objective To develop a dynamic 3D radiomics analysis method using artificial intelligence technique for automatically assessing four disease stages (i.e., early, progressive, peak, and absorption stages) of COVID-19 patients on CT images. Methods The dynamic 3D radiomics analysis method was composed of three AI algorithms (the lung segmentation, lesion segmentation, and stage-assessing AI algorithms) that were trained and tested on 313,767 CT images from 520 COVID-19 patients. This proposed method used 3D lung lesion that was segmented by the lung and lesion segmentation algorithms to extract radiomics features, and then combined with clinical metadata to assess the possible stage of COVID-19 patients using stage-assessing algorithm. Area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity were used to evaluate diagnostic performance. Results Of 520 patients, 66 patients (mean age, 57 years ± 15 [standard deviation]; 35 women), including 203 CT scans, were tested. The dynamic 3D radiomics analysis method used 30 features, including 27 radiomics features and 3 clinical features to assess the possible disease stage of COVID-19 with an accuracy of 90%. For the prediction of each stage, the AUC of stage 1 was 0.965 (95% CI: 0.934, 0.997), AUC of stage 2 was 0.958 (95% CI: 0.931, 0.984), AUC of stage 3 was 0.998 (95% CI: 0.994, 1.000), and AUC of stage 4 was 0.975 (95% CI: 0.956, 0.994). Conclusion With high diagnostic performance, the dynamic 3D radiomics analysis using artificial intelligence could represent a potential tool for helping hospitals make appropriate resource allocations and follow-up of treatment response. Key Points • The AI segmentation algorithms were able to accurately segment the lung and lesion of COVID-19 patients of different stages. • The dynamic 3D radiomics analysis method successfully extracted the radiomics features from the 3D lung lesion. • The stage-assessing AI algorithm combining with clinical metadata was able to assess the four stages with an accuracy of 90%, a macro-average AUC of 0.975.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00330-021-08533-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00414-021-02761-2,Human identification performed with skull’s sphenoid sinus based on deep learning,International Journal of Legal Medicine,10.1007/s00414-021-02761-2,Springer,2022-07-01,"Human identification plays a significant role in the investigations of disasters and criminal cases. Human identification could be achieved quickly and efficiently via 3D sphenoid sinus models by customized convolutional neural networks. In this retrospective study, a deep learning neural network was proposed to achieve human identification of 1475 noncontrast thin-slice CT scans. A total of 732 patients were retrieved and studied (82% for model training and 18% for testing). By establishing an individual recognition framework, the anonymous sphenoid sinus model was matched and cross-tested, and the performance of the framework also was evaluated on the test set using the recognition rate, ROC curve and identification speed. Finally, manual matching was performed based on the framework results in the test set. Out of a total of 732 subjects (mean age 46.45 years ± 14.92 (SD); 349 women), 600 subjects were trained, and 132 subjects were tested. The present automatic human identification has achieved Rank 1 and Rank 5 accuracy values of 93.94% and 99.24%, respectively, in the test set. In addition, all the identifications were completed within 55 s, which manifested the inference speed of the test set. We used the comparison results of the MVSS-Net to exclude sphenoid sinus models with low similarity and carried out traditional visual comparisons of the CT anatomical aspects of the sphenoid sinus of 132 individuals with an accuracy of 100%. The customized deep learning framework achieves reliable and fast human identification based on a 3D sphenoid sinus and can assist forensic radiologists in human identification accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00414-021-02761-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-021-02076-0,Multi-classification speech emotion recognition based on two-stage bottleneck features selection and MCJD algorithm,"Signal, Image and Video Processing",10.1007/s11760-021-02076-0,Springer,2022-07-01,"Feature extraction and classification decision play an important role in speech emotion recognition. To improve the performance of the multi-classification speech emotion recognition (SER) system, a two-stage bottleneck features selection model and a novel multi-classifier joint decision (MCJD) algorithm are proposed. In two-stage bottleneck features selection model, firstly, bottleneck features at different hidden layers are extracted from deep neural network (DNN) and fused using genetic algorithm (GA). Secondly, principal component analysis (PCA) is used to eliminate the dimension disaster caused by high-dimensional feature vectors. In addition, to make up for the shortcomings of single SVM classifier in SER, we use different feature sets to train multiple SVM classifiers based on classification targets. The final recognition result is obtained by joint decision of SVMs according to MCJD algorithm. Five-fold cross-validation is used, and an average accuracy of 84.89% is achieved using the two-stage bottleneck features selection model and traditional support vector machines (SVM) classifier. Then, using the MCJD algorithm, the average SER rate of the multi-classification SER system for seven kinds of emotions is 87.08% on Berlin Database, which further improves the performance of SER system and shows the effectiveness of our method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-021-02076-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00034-022-01981-0,Mask Estimation Using Phase Information and Inter-channel Correlation for Speech Enhancement,"Circuits, Systems, and Signal Processing",10.1007/s00034-022-01981-0,Springer,2022-07-01,"The most commonly used training target is masking-based approach which maps noisy speech to the time–frequency (T–F) unit and has a remarkable impact on the performance in the supervised learning algorithms. Traditional T–F masks like ideal ratio mask (IRM) demonstrate a strong performance but are limited to only the magnitude domain in enhancement. Though bounded IRM with phase constraint (BIRMP) includes phase difference but doesn’t exploit channel correlation, the proposed ratio mask (pRM) considers channel correlation but is computed only in the magnitude domain. This work proposes a new mask, i.e., phase correlation ideal ratio mask (PCIRM), which includes both inter-channel correlation and phase difference between the noisy speech ( $$N_\mathrm{S}$$ N S ), noise ( N ) and clean speech ( $$C_\mathrm{S}$$ C S ). Considering these factors increases the percentage of $$C_\mathrm{S}$$ C S and readily decreases the percentage of unwanted noise in the speech components and conversely for the noise components making the mask more precise. The experimental results are conducted under different SNR levels using TIMIT dataset and NOISEX-92 dataset and also compared with the existing state-of-the-art approaches. The results prove that the proposed mask has higher performance than BIRMP and pRM in terms of speech quality and intelligibility.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00034-022-01981-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12698-2,Deep neural network and 3D model for face recognition with multiple disturbing environments,Multimedia Tools and Applications,10.1007/s11042-022-12698-2,Springer,2022-07-01,"This paper presents the proposed bird search-based shuffled shepherd optimization algorithm (BSSSO) for face recognition. Initially, the input image undergoes a noise removal phase to eliminate noise in order to make them suitable for subsequent processing. The noise removal is performed using the type II fuzzy system and cuckoo search optimization algorithm (T2FCS), which detects noisy pixels from the image for improved processing. After the noise removal phase, the feature extraction is carried out using the convolution neural network (CNN) model and landmark enabled 3D morphable model (L3DMM). The obtained features are subjected to deep CNN for face recognition. The training of deep CNN is performed using the bird search-based shuffled shepherd optimization algorithm (BSSSO). Here, the proposed BSSSO is designed by combining the shuffled shepherd optimization algorithm (SSOA) and bird swarm algorithm (BSA) for inheriting the merits of both optimizations towards effective training of deep CNN. The proposed method obtained higher accuracy of 0.8935 and minimum FAR and FRR of 0.2190 and 0.2021 using LFW database with respect to training data.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12698-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-022-02620-4,Predicting conversion from MCI to AD by integration of rs-fMRI and clinical information using 3D-convolutional neural network,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-022-02620-4,Springer,2022-07-01,"Purpose Alzheimer's is the most common irreversible neurodegenerative disease. Its symptoms range from memory impairments to degradation of multiple cognitive abilities and ultimately death. Mild cognitive impairment (MCI) is the earliest detectable stage that happens between normal aging and early dementia, and even though MCI subjects have a chance of changing back to cognitively normal or even staying the same, there is a risk that their condition progresses to Alzheimer's disease (AD) annually. Therefore predicting AD among MCI subjects is pivotal for starting treatments at an opportune time in case of progression, and if staying stable is the case, the need for consistent medical observations would eliminate. Thus, we aim to diagnose possible conversion from MCI to AD by exploiting a class of deep learning (DL) methods called convolutional neural network (CNN). Methods We proposed a three-dimensional CNN (3D-CNN) to combine and analyze resting-state functional magnetic resonance imaging (rs-fMRI), clinical assessment results, and demographic information to predict conversion from MCI to AD in an average 5-years interval. Initially, a 3D-CNN was developed based on fMRI single volumes of 266 samples from 81 subjects; then, we used neuron layers to combine clinical data with fMRI to improve the results. Results At first, the CNN model demonstrated an AUC of 87.67% and an accuracy of 85.7%, then after combining clinical and rs-fMRI features, we observed the following improved scores: an AUC of 91.72%, an accuracy of 87.6%, a sensitivity of 75.58% and a specificity of 92.57%. Conclusion Our developed algorithm managed to predict prognosis from MCI to AD with high levels of accuracy, proving the potential of DL approaches in solving the matter and the efficiency of integrating clinical information with imaging according to the proposed method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-022-02620-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-022-02587-2,Improving segmentation and classification of renal tumors in small sample 3D CT images using transfer learning with convolutional neural networks,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-022-02587-2,Springer,2022-07-01,"Purpose Computed tomography (CT) images can display internal organs of patients and are particularly suitable for preoperative surgical diagnoses. The increasing demands for computer-aided systems in recent years have facilitated the development of many automated algorithms, especially deep convolutional neural networks, to segment organs and tumors or identify diseases from CT images. However, performances of some systems are highly affected by the amount of training data, while the sizes of medical image data sets, especially three-dimensional (3D) data sets, are usually small. This condition limits the application of deep learning. Methods In this study, given a practical clinical data set that has 3D CT images of 20 patients with renal carcinoma, we designed a pipeline employing transfer learning to alleviate the detrimental effect of the small sample size. A dual-channel fine segmentation network (FS-Net) was constructed to segment kidney and tumor regions, with 210 publicly available 3D images from a competition employed during the training phase. We also built discriminative classifiers to classify the benign and malignant tumors based on the segmented regions, where both handcrafted and deep features were tested. Results Our experimental results showed that the Dice values of segmented kidney and tumor regions were 0.9662 and 0.7685, respectively, which were better than those of state-of-the-art methods. The classification model using radiomics features can classify most of the tumors correctly. Conclusions The designed FS-Net was demonstrated to be more effective than simply fine-tuning on the practical small size data set given that the model can borrow knowledge from large auxiliary data without diluting the signal in primary data. For the small data set, radiomics features outperformed deep features in the classification of benign and malignant tumors. This work highlights the importance of architecture design in transfer learning, and the proposed pipeline is anticipated to provide a reference and inspiration for small data analysis.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-022-02587-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12091-z,3DFCNN: real-time action recognition using 3D deep neural networks with raw depth information,Multimedia Tools and Applications,10.1007/s11042-022-12091-z,Springer,2022-07-01,"This work describes an end-to-end approach for real-time human action recognition from raw depth image-sequences. The proposal is based on a 3D fully convolutional neural network, named 3DFCNN, which automatically encodes spatio-temporal patterns from raw depth sequences. The described 3D-CNN allows actions classification from the spatial and temporal encoded information of depth sequences. The use of depth data ensures that action recognition is carried out protecting people’s privacy, since their identities can not be recognized from these data. The proposed 3DFCNN has been optimized to reach a good performance in terms of accuracy while working in real-time. Then, it has been evaluated and compared with other state-of-the-art systems in three widely used public datasets with different characteristics, demonstrating that 3DFCNN outperforms all the non-DNN-based state-of-the-art methods with a maximum accuracy of 83.6% and obtains results that are comparable to the DNN-based approaches, while maintaining a much lower computational cost of 1.09 seconds, what significantly increases its applicability in real-world environments.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12091-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13383-0,Study on no-reference video quality assessment method incorporating dual deep learning networks,Multimedia Tools and Applications,10.1007/s11042-022-13383-0,Springer,2022-06-30,"The quality assessment of user-generated content (UGC) videos is a challenge. Unlike synthetic videos, these videos are then susceptible to various distortions caused by the external environment during the generation process. This paper proposes a video quality assessment method (VQA) incorporating a dual-depth network architecture. First, the diversity of video acquisition information is ensured by global average pooling and global standard deviation pooling under the InceptionV3 network and ResNet50 network, and video frame quality scores are obtained under bidirectional GRU networks. Second, in the spatial-temporal domain, a temporal memory block is constructed by exploiting human temporal memory and content-dependent effects to obtain components of video quality. Meanwhile, a Gaussian distribution is also added to the spatial domain to reduce the effect of content variation. Finally, extensive experiments are conducted using the KoNViD-1 k and LIVEVQC databases. The experimental results show that the metrics Spearman’s rank-order correlation (SROCC) and Pearson’s linear correlation coefficient (PLCC) are 0.7786 and 0.7759 in the overall performance,which 2.87% and 0.52% higher than Tang, respectively. This verifies the validity of the model. In addition, the cross-validation experiments show that the present model also has strong generalization ability.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13383-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10044-022-01083-2,Visual attention-based deepfake video forgery detection,Pattern Analysis and Applications,10.1007/s10044-022-01083-2,Springer,2022-06-27,"The prime goal of creating synthetic digital data is to generate something very closer to real ones when the original data are scarce. However, the trustworthiness of such digital content is dipping potentially in society owing to malicious users. Deepfake method that uses computer graphics and computer vision techniques to replace the face of one person with the face of a different person is becoming an area of big concern. Such techniques can easily be used to hide the identity of a person. Therefore, a method is needed to verify the originality of such face images/videos. To this end, we design a deep learning model enhanced with visual attention technique to differentiate manipulated videos/images (generated by deepfake methods) from real ones. At first, we extract the face region from video frames and then pass the same through the pre-trained Xception model to obtain the feature maps. Next, with the help of the visual attention mechanism, we mainly try to focus on the deepfake video manipulation leftover artifacts. We evaluate our model on two publicly available datasets, namely FaceForensics++ and Celeb-DF (V2), and our model outperforms many state-of-the-art methods tested on these two datasets. Source code of the proposed method can be found at: https://github.com/tre3x/Deepfake-Video-Forgery-Detection .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10044-022-01083-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41550-022-01701-3,Efficient labelling of solar flux evolution videos by a deep learning model,Nature Astronomy,10.1038/s41550-022-01701-3,Nature,2022-06-27,"Big-data labelling is critical to harness the power of supervised machine learning in astronomy. Neural networks applied to the solar flux emergence problem considerably reduce the manual labelling burden and easily extract higher-level information. Machine learning is becoming a critical tool for the interrogation of large, complex data. Labelling, defined as the process of adding meaningful annotations, is a crucial step of supervised machine learning. However, labelling datasets is time consuming. Here we show that convolutional neural networks (CNNs) trained on crudely labelled astronomical videos can be leveraged to improve the quality of data labelling and reduce the need for human intervention. We use videos of the solar magnetic field that are divided into two classes—emergence or non-emergence of bipolar magnetic regions (BMRs)—on the basis of their first detection on the solar disk. We train CNNs using crude labels, manually verify, correct disagreements between the labelling and CNN, and repeat this process until convergence is reached. Traditionally, flux emergence labelling is done manually. We find that a high-quality labelled dataset derived through this iterative process reduces the necessary manual verification by 50%. Furthermore, by gradually masking the videos and looking for maximum changes in CNN inference, we locate BMR emergence time without retraining the CNN. This demonstrates the versatility of CNNs for simplifying the challenging task of labelling complex dynamic events.",https://www.nature.com/articles/s41550-022-01701-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-022-02691-3,Open surgery tool classification and hand utilization using a multi-camera system,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-022-02691-3,Springer,2022-06-27,"Purpose The goal of this work is to use multi-camera video to classify open surgery tools as well as identify which tool is held in each hand. Multi-camera systems help prevent occlusions in open surgery video data. Furthermore, combining multiple views such as a top-view camera covering the full operative field and a close-up camera focusing on hand motion and anatomy may provide a more comprehensive view of the surgical workflow. However, multi-camera data fusion poses a new challenge: A tool may be visible in one camera and not the other. Thus, we defined the global ground truth as the tools being used regardless their visibility. Therefore, tools that are out of the image should be remembered for extensive periods of time while the system responds quickly to changes visible in the video. Methods Participants ( n = 48) performed a simulated open bowel repair. A top-view and a close-up cameras were used. YOLOv5 was used for tool and hand detection. A high-frequency LSTM with a 1-second window at 30 frames per second (fps) and a low-frequency LSTM with a 40-second window at 3 fps were used for spatial, temporal, and multi-camera integration. Results The accuracy and F1 of the six systems were: top-view (0.88/0.88), close-up (0.81,0.83), both cameras (0.9/0.9), high-fps LSTM (0.92/0.93), low-fps LSTM (0.9/0.91), and our final architecture the multi-camera classifier(0.93/0.94). Conclusion Since each camera in a multi-camera system may have a partial view of the procedure, we defined a ’global ground truth.’ Defining this at the data labeling phase emphasized this requirement at the learning phase, eliminating the need for any heuristic decisions. By combining a system with a high fps and a low fps from the multiple camera array, we improved the classification abilities of the global ground truth.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-022-02691-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-14983-4,A general skull stripping of multiparametric brain MRIs using 3D convolutional neural network,Scientific Reports,10.1038/s41598-022-14983-4,Nature,2022-06-27,"Accurate skull stripping facilitates following neuro-image analysis. For computer-aided methods, the presence of brain skull in structural magnetic resonance imaging (MRI) impacts brain tissue identification, which could result in serious misjudgments, specifically for patients with brain tumors. Though there are several existing works on skull stripping in literature, most of them either focus on healthy brain MRIs or only apply for a single image modality. These methods may be not optimal for multiparametric MRI scans. In the paper, we propose an ensemble neural network (EnNet), a 3D convolutional neural network (3DCNN) based method, for brain extraction on multiparametric MRI scans (mpMRIs). We comprehensively investigate the skull stripping performance by using the proposed method on a total of 15 image modality combinations. The comparison shows that utilizing all modalities provides the best performance on skull stripping. We have collected a retrospective dataset of 815 cases with/without glioblastoma multiforme (GBM) at the University of Pittsburgh Medical Center (UPMC) and The Cancer Imaging Archive (TCIA). The ground truths of the skull stripping are verified by at least one qualified radiologist. The quantitative evaluation gives an average dice score coefficient and Hausdorff distance at the 95th percentile, respectively. We also compare the performance to the state-of-the-art methods/tools. The proposed method offers the best performance. The contributions of the work have five folds: first, the proposed method is a fully automatic end-to-end for skull stripping using a 3D deep learning method. Second, it is applicable for mpMRIs and is also easy to customize for any MRI modality combination. Third, the proposed method not only works for healthy brain mpMRIs but also pre-/post-operative brain mpMRIs with GBM. Fourth, the proposed method handles multicenter data. Finally, to the best of our knowledge, we are the first group to quantitatively compare the skull stripping performance using different modalities. All code and pre-trained model are available at: https://github.com/plmoer/skull_stripping_code_SR .",https://www.nature.com/articles/s41598-022-14983-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-022-03869-y,Content based video retrieval using deep learning feature extraction by modified VGG_16,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-022-03869-y,Springer,2022-06-26,"The recent challenge faced by the users from the multimedia area is to collect the relevant object or unique image from the collection of huge data. During the classification of semantics, the media was allowed to access the text by merging the media with the text or content before the emergence of content based retrieval. After its presence, media retrieval process is made easier than earlier stages by adding the attributes to the media in the database using multi-dimensional feature vectors which are termed as descriptors. The identification this features has become major challenges, so to overcome this issue this paper focuses on a deep learning techniques named as Modified Visual Geometry Group _16, and the result of this techniques have been compared with the existing other feature extraction techniques such as conventional histogram of oriented gradients (HOG), local binary patterns (LBP) and convolution neural network (CNN) methods. In this scheme the video frame image retrieval is performed by assigning the indexing to the all video files in the database in order to perform the system more efficiently. Thus the system produces the top result matches for the similar query in comparison with the existing techniques based on accuracy, precision, recall and F1 score in optimized video frame retrieval.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-022-03869-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00500-022-07295-2,Application of deep learning in video target tracking of soccer players,Soft Computing,10.1007/s00500-022-07295-2,Springer,2022-06-25,"Football matches have a high degree of attention and the analysis technology used for video contents has important practical significance and good application prospects. However, due to the diversity of conditions, i.e., football venues, clothing colors, etc., there is no universal tracker that can perfectly adapt to all scenarios. Due to its excellent feature extraction capabilities, deep learning technology has been widely used in the field of computer vision in recent years. The main objective of this article is the extraction of player’s trajectory in a football game, i.e., the path tracking of the player’s goal. To achieve this objective, deep learning technology is used for automatic extraction of the characteristic features of the player’s target in the context of target detection and tracking. The target detection method employed in this study is based on deep learning by forming new multi-scale features and modifying the generation rules of anchor points of the captured videos, making it more suitable for small target detection tasks in football match scenes. The generation rules are based on a complex decision support system for target tracking. This decision support system uses the method of constructing a similarity matrix to transform the multi-target tracking problem into a data association problem that can be solved by the Hungarian algorithm. The proposed approach is compared against state-of-the-art techniques in terms of area under the curve (AUC) value, track set scene distribution, number of frames, and other parameters. Based on the experimental results, the proposed approach outperforms these existing techniques with much better results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-022-07295-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-022-04649-3,A time sequence location method of long video violence based on improved C3D network,The Journal of Supercomputing,10.1007/s11227-022-04649-3,Springer,2022-06-25,"This paper mainly studies the retrieval and location of violence in long time sequence video. Aiming at the low accuracy of violence detection in long time sequence video, a two-stage violence time sequence location method based on DC3D network model is proposed in this paper. In the video preprocessing stage, this paper adopts the method of generating video at multiple scales. In the first stage, we first use a large number of labeled datasets for pre-training to obtain a C3D network model for generating candidate videos and finally filter out meaningless background videos. In the second stage, we use the deconvolution method to identify the candidate video accurately to the frame level, so as to determine the specific time of violence. In the first stage, this method can improve the overall accuracy by generating candidate videos. DC3D network model can accurately locate the time of violence to the level of frame. The experimental results show that the proposed method can quickly retrieve and locate the violent fighting behavior in the surveillance video under the long time sequence video. The research results of this paper can provide convenience for the surveillance personnel to quickly retrieve and locate the target segment in a large amount of video data.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-022-04649-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40747-022-00794-7,Architecture entropy sampling-based evolutionary neural architecture search and its application in osteoporosis diagnosis,Complex & Intelligent Systems,10.1007/s40747-022-00794-7,Springer,2022-06-25,"In recent years, neural architecture search (NAS) has achieved unprecedented development because of its ability to automatically achieve high-performance neural networks in various tasks. Among these, the evolutionary neural architecture search (ENAS) has impressed the researchers due to the excellent heuristic exploration capability. However, the evolutionary algorithm-based NAS are prone to the loss of population diversity in the search process, causing that the structure of the surviving individuals is exceedingly similar, which will lead to premature convergence and fail to explore the search space comprehensively and effectively. To address this issue, we propose a novel indicator, named architecture entropy, which is used to measure the architecture diversity of population. Based on this indicator, an effective sampling strategy is proposed to select the candidate individuals with the potential to maintain the population diversity for environmental selection. In addition, an unified encoding scheme of topological structure and computing operation is designed to efficiently express the search space, and the corresponding population update strategies are suggested to promote the convergence. The experimental results on several image classification benchmark datasets CIFAR-10 and CIFAR-100 demonstrate the superiority of our proposed method over the state-of-the-art comparison ones. To further validate the effectiveness of our method in real applications, our proposed NAS method is applied in the identification of lumbar spine X-ray images for osteoporosis diagnosis, and can achieve a better performance than the commonly used methods. Our source codes are available at https://github.com/LabyrinthineLeo/AEMONAS.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40747-022-00794-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s12859-022-04794-9,A state-of-the-art technique to perform cloud-based semantic segmentation using deep learning 3D U-Net architecture,BMC Bioinformatics,10.1186/s12859-022-04794-9,BioMed Central,2022-06-24,"Glioma is the most aggressive and dangerous primary brain tumor with a survival time of less than 14 months. Segmentation of tumors is a necessary task in the image processing of the gliomas and is important for its timely diagnosis and starting a treatment. Using 3D U-net architecture to perform semantic segmentation on brain tumor dataset is at the core of deep learning. In this paper, we present a unique cloud-based 3D U-Net method to perform brain tumor segmentation using BRATS dataset. The system was effectively trained by using Adam optimization solver by utilizing multiple hyper parameters. We got an average dice score of 95% which makes our method the first cloud-based method to achieve maximum accuracy. The dice score is calculated by using Sørensen-Dice similarity coefficient. We also performed an extensive literature review of the brain tumor segmentation methods implemented in the last five years to get a state-of-the-art picture of well-known methodologies with a higher dice score. In comparison to the already implemented architectures, our method ranks on top in terms of accuracy in using a cloud-based 3D U-Net framework for glioma segmentation.",https://www.biomedcentral.com/openurl?doi=10.1186/s12859-022-04794-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10845-022-01971-8,A new differentiable architecture search method for optimizing convolutional neural networks in the digital twin of intelligent robotic grasping,Journal of Intelligent Manufacturing,10.1007/s10845-022-01971-8,Springer,2022-06-24,"Convolutional neural networks (CNNs) have been widely used for object recognition and grasping posture planning in intelligent robotic grasping (IRG). Compared with the traditional usage of CNNs in image recognition, IRGs require high recognition accuracy and computational efficiency. However, the existing methodologies for CNN architecture design often rely on human experience and numerous trial-and-error attempts, which make it a very challenging task to obtain an optimal CNN for IRGs. To tackle this challenge, this paper develops a new differentiable architecture search (DARTS) method considering the floating-point operations (FLOPs) of CNNs, named the DARTS-F method, which converts the discrete CNN architecture search to a gradient-based continuous optimization problem and considers both the prediction accuracy and the computational cost of the CNN during the optimization. To efficiently identify the optimal neural network, this paper adopts a bilevel optimization, which first trains the neural network weights in the inner level and then optimizes the neural network architecture by fine-tuning the operational variables in the outer level. In addition, a new digital twin (DT) of IRG is developed considering the physics of realistic robotic grasping in the DT’s virtual space, which could not only improve the IRG accuracy but also avoid the expensive training time. In the experiments, the proposed DARTS-F method could generate an optimized CNN with higher prediction accuracy and lower FLOPs than those obtained by the original DARTS method. The DT framework improves the accuracy of real robotic grasping from 61 to 71%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10845-022-01971-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s10033-022-00760-x,Digital Twin for Integration of Design-Manufacturing-Maintenance: An Overview,Chinese Journal of Mechanical Engineering,10.1186/s10033-022-00760-x,Springer,2022-06-23,"Traditional design, manufacturing and maintenance are run and managed independently under their own rules and regulations in an increasingly time-and-cost ineffective manner. A unified platform for efficient and intelligent design-manufacturing-maintenance of mechanical equipment and systems is highly needed in this rapidly digitized world. In this work, the definition of digital twin and its research progress and associated challenges in the design, manufacturing and maintenance of engineering components and equipment were thoroughly reviewed. It is indicated that digital twin concept and associated technology provide a feasible solution for the integration of design-manufacturing-maintenance as it has behaved in the entire lifecycle of products. For this aim, a framework for information-physical combination, in which a more accurate design, a defect-free manufacturing, a more intelligent maintenance, and a more advanced sensing technology, is prospected.",http://link.springer.com/openurl/fulltext?id=doi:10.1186/s10033-022-00760-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13367-0,A deep learning-based pipeline for mosquito detection and classification from wingbeat sounds,Multimedia Tools and Applications,10.1007/s11042-022-13367-0,Springer,2022-06-22,"Mosquito vector-borne diseases such as malaria and dengue constitute some of the most serious public health burdens in tropical and sub-tropical countries. Effective targeting of disease control efforts requires accurate estimates of mosquito vector population density. The traditional, and still most common, approach to this involves the use of traps along with manual counting and classification of mosquito species. This process is costly and labor-intensive, which hinders its widespread use. In this paper we present a software pipeline for detection and classification of mosquito wingbeat sounds. Since our target platform is low-cost IoT devices, we explore the tradeoff between accuracy and efficiency. When a fast binary mosquito detector precedes the classifier, we can reduce the computational demand compared with use of the classifier alone by a factor of 10. While the accuracy of traditional machine learning model drops from 90% to 64% when reducing the sample rate from 96 kHz to 8 kHz, our deep-learning models maintain an accuracy of almost 83%, even when additionally reducing the bit depth from 24 to 16 bits. We conclude that the combination of an efficient mosquito detector with a convolutional neural network provides for an excellent trade-off between accuracy and efficiency to detect, classify and count mosquitoes.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13367-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13363-4,A voice-based real-time emotion detection technique using recurrent neural network empowered feature modelling,Multimedia Tools and Applications,10.1007/s11042-022-13363-4,Springer,2022-06-22,"The advancements of the Internet of Things (IoT) and voice-based multimedia applications have resulted in the generation of big data consisting of patterns, trends and associations capturing and representing many features of human behaviour. The latent representations of many aspects and the basis of human behaviour is naturally embedded within the expression of emotions found in human speech. This signifies the importance of mining audio data collected from human conversations for extracting human emotion. Ability to capture and represent human emotions will be an important feature in next-generation artificial intelligence, with the expectation of closer interaction with humans. Although the textual representations of human conversations have shown promising results for the extraction of emotions, the acoustic feature-based emotion detection from audio still lags behind in terms of accuracy. This paper proposes a novel approach for feature extraction consisting of Bag-of-Audio-Words (BoAW) based feature embeddings for conversational audio data. A Recurrent Neural Network (RNN) based state-of-the-art emotion detection model is proposed that captures the conversation-context and individual party states when making real-time categorical emotion predictions. The performance of the proposed approach and the model is evaluated using two benchmark datasets along with an empirical evaluation on real-time prediction capability. The proposed approach reported 60.87% weighted accuracy and 60.97% unweighted accuracy for six basic emotions for IEMOCAP dataset, significantly outperforming current state-of-the-art models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13363-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-022-01638-0,Structured Binary Neural Networks for Image Recognition,International Journal of Computer Vision,10.1007/s11263-022-01638-0,Springer,2022-06-22,"In this paper, we propose to train binarized convolutional neural networks (CNNs) that are of significant importance for deploying deep learning to mobile devices with limited power capacity and computing resources. Previous works on quantizing CNNs often seek to approximate the floating-point information of weights and/or activations using a set of discrete values. Such methods, termed value approximation here, typically are built on the same network architecture of the full-precision counterpart. Instead, we take a new “structured approximation” view for network quantization — it is possible and valuable to exploit flexible architecture transformation when learning low-bit networks, which can achieve even better performance than the original networks in some cases. In particular, we propose a “group decomposition” strategy, termed GroupNet, which divides a network into desired groups. Interestingly, with our GroupNet strategy, each full-precision group can be effectively reconstructed by aggregating a set of homogeneous binary branches. We also propose to learn effective connections among groups to improve the representation capability. To improve the model capacity, we propose to dynamically execute sparse binary branches conditioned on input features while preserving the computational cost. More importantly, the proposed GroupNet shows strong flexibility for a few vision tasks. For instance, we extend the GroupNet for accurate semantic segmentation by embedding the rich context into the binary structure. The proposed GroupNet also shows strong performance on object detection. Experiments on image classification, semantic segmentation, and object detection tasks demonstrate the superior performance of the proposed methods over various quantized networks in the literature. Moreover, the speedup and runtime memory cost evaluation comparing with related quantization strategies is analyzed on GPU platforms, which serves as a strong benchmark for further research.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-022-01638-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-022-03775-y,Principal views selection based on growing graph convolution network for multi-view 3D model recognition,Applied Intelligence,10.1007/s10489-022-03775-y,Springer,2022-06-22,"With the development of 3D technologies, 3D model recognition has attracted substantial attention in various areas, such as automatic driving, virtual/augmented reality, and computer-aided design. Many researchers are devoted to 3D model recognition and obtain some achievements in research. However, the abundant structure information of the 3D model also brings a huge challenge in model representation. In recent years, many researchers focus on classical computer vision technologies, which are utilized to represent the multi-view information of the 3D model. However, redundant visual information also brings a new challenge in model representation. In this paper, we focus on the multi-view 3D model data and propose a novel growing graph convolution network (GGCN) to handle the principal views selection problem, which can guarantee the performance of 3D model representation and effectively reduce the cost time. The proposed method mainly includes two modules: 1) principal views selection module: we utilize the selected views to describe the 3D model, which can effectively remove the redundant information and reduce computational complexity. 2) growing GCN module: we propose an effective growing GCN model, which focuses on gathering nodes that were less related to each other to ensure the result of multi-view fusion. It can indirectly retain the structure information and also reduce redundant information. In the process of graph growing, the GGCN model gradually adds view information to make up for the lack of characterization and guarantee the final performance. More specially, these two modules can guide each other to improve the performance of the principal views module and indirectly increase the final recognition accuracy. To evaluate the effectiveness of our proposed method, we test the classification accuracy and retrieval performance on the ModelNet40 dataset and ShapeNet dataset. The experimental results demonstrate the superiority of our proposed method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-022-03775-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07480-2,ENCVIDC: an innovative approach for encoded video content classification,Neural Computing and Applications,10.1007/s00521-022-07480-2,Springer,2022-06-21,"With the increase in the sum of online video viewers on the internet every day, the video service providers are getting interested to know about the nature of the content being viewed through the supplied network in order to accomplish their business associated objectives that may include the user’s internet behavior profile, etc. Due to the widespread use of encoded video streaming techniques, the network video traffic classification has turned out to be a challenging task. As devoid of the authentic decryption key, it is impossible to comprehend the actual content viewed by the user. However, the current advances in machine learning have demonstrated the fact that encryption can also lead to certain information leak which yields promising results in determining the actual transmitted content between the two communicating parties. This research proposes a classifier for determining the encrypted video content over different streaming sites such as YouTube, Netflix and Dailymotion. We demonstrated that an eavesdropper can determine the stream video content even if the traffic is encrypted by identifiable patterns extracted from the captured traffic. We used different machine algorithms for the task and conducted a series of tests, demonstrating that our classification based on Random Forest showed accuracy greater than 98% and has the ability to execute all the network-related business objectives of any enterprise network.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07480-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10845-022-01973-6,Knowledge-embedded machine learning and its applications in smart manufacturing,Journal of Intelligent Manufacturing,10.1007/s10845-022-01973-6,Springer,2022-06-21,"Demands for more accurate machine learning models have given rise to rethinking current modeling approaches that were deemed unsuitable, primarily due to their computational complexity and the lack of availability and accessibility to representative data. In Industry 4.0, rapid advancements in Digital Twin (DT) technologies and the pervasiveness of cost-effective sensor technologies have pushed the incorporation of artificial intelligence, particularly data-driven machine learning models, for use in smart manufacturing. However, the persistent issue with such models is their high sensitivity to the training data and the lack of interpretability in the outcomes, at times generating unrealistic results. The incorporation of knowledge into the machine learning pipeline has been earmarked as the most promising approach to address such issues. This paper aims to answer this call through a Knowledge-embedded Machine Learning (KML) framework for smart manufacturing, which embeds knowledge from experience and, or physics information into the machine learning pipeline, thus making the outcomes from these models more representative of real applications. The merits of KML were then presented through comparative studies showing its capability to outperform knowledge-based and data-driven models. This promising outcome led to the development of frameworks that can potentially incorporate KML for smart manufacturing applications such as Prognostics and Health Management (PHM) and DT, further supporting the usefulness of the proposed KML framework.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10845-022-01973-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00330-022-08952-8,Deep-learning-based 3D super-resolution MRI radiomics model: superior predictive performance in preoperative T-staging of rectal cancer,European Radiology,10.1007/s00330-022-08952-8,Springer,2022-06-21,"Objectives To investigate the feasibility and efficacy of a deep-learning (DL)-based three-dimensional (3D) super-resolution (SR) MRI radiomics model for preoperative T-staging prediction in rectal cancer (RC). Methods Seven hundred six eligible RC patients (T1/2 = 287, T3/4 = 419) were retrospectively enrolled in this study and chronologically allocated into a training cohort ( n = 565) and a validation cohort ( n = 141). We conducted a deep-transfer-learning network on high-resolution (HR) T2-weighted imaging (T2WI) to enhance the z -resolution of the images and acquired the preoperative SRT2WI. The radiomics models named model_HRT2 and model_SRT2 were respectively constructed with high-dimensional quantitative features extracted from manually segmented volume of interests of HRT2WI and SRT2WI through the Least Absolute Shrinkage and Selection Operator method. The performances of the models were evaluated by ROC, calibration, and decision curves. Results Model_SRT2 outperformed model_HRT2 (AUC 0.869, sensitivity 71.1%, specificity 93.1%, and accuracy 83.3% vs. AUC 0.810, sensitivity 89.5%, specificity 70.1%, and accuracy 77.3%) in distinguishing T1/2 and T3/4 RC with significant difference ( p < 0.05). Both radiomics models achieved higher AUCs than the expert radiologists (0.685, 95% confidence interval 0.595–0.775, p < 0.05). The calibration curves confirmed high goodness of fit, and the decision curve analysis revealed the clinical value. Conclusions Model_SRT2 yielded superior predictive performance in preoperative RC T-staging by comparison with model_HRT2 and expert radiologists’ visual assessments. Key Points • For the first time, DL-based 3D SR images were applied in radiomics analysis for clinical utility. • Compared with the visual assessment of expert radiologists and the conventional radiomics model based on HRT2WI, the SR radiomics model showed a more favorable capability in helping clinicians assess the invasion depth of RC preoperatively. • This is the largest radiomics study for T-staging prediction in RC.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00330-022-08952-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-022-09979-4,Acoustic modelling using deep learning for Quran recitation assistance,International Journal of Speech Technology,10.1007/s10772-022-09979-4,Springer,2022-06-21,"Recent advancements in computer science and engineering have significantly boosted interest in and success rate of speech recognition systems. Sophisticated speech recognition systems are being developed for a variety of languages having wide-spread applications. Arabic being one of the most widely used languages in the world, augmented by religious importance, is a potential candidate in this domain, however rich vocabulary and multiple dialects open up new challenges for development of Arabic speech recognition processes. This paper focuses on development of an Arabic isolated speech recognition system for the vocabulary of Holy Quran using Mel frequency cepstral coefficients and deep neural networks. The proposed system is capable of identifying individual words from a recited verse with reasonable accuracy. It targets the 362 unique words of the first and the last 19 chapters of the Holy Quran and uses 14 hours of audio data to demonstrate the working prototype of the system. A user-friendly, web-based application has also been developed for transcription of recitations. The software is specified to make an effort in spreading the right way of recitation of the Holy Quran. The presented architecture can be extended to a complete automatic speech recognition system to help people practice recitations, mapping speech to transcription and then matching the transcribed utterance with standard templates of correct recitation to identify reciter’s short comings.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-022-09979-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11747-022-00868-5,Voice bots on the frontline: Voice-based interfaces enhance flow-like consumer experiences & boost service outcomes,Journal of the Academy of Marketing Science,10.1007/s11747-022-00868-5,Springer,2022-06-21,"Voice-based interfaces provide new opportunities for firms to interact with consumers along the customer journey. The current work demonstrates across four studies that voice-based (as opposed to text-based) interfaces promote more flow-like user experiences, resulting in more positively-valenced service experiences, and ultimately more favorable behavioral firm outcomes (i.e., contract renewal, conversion rates, and consumer sentiment). Moreover, we also provide evidence for two important boundary conditions that reduce such flow-like user experiences in voice-based interfaces (i.e., semantic disfluency and the amount of conversational turns). The findings of this research highlight how fundamental theories of human communication can be harnessed to create more experiential service experiences with positive downstream consequences for consumers and firms. These findings have important practical implications for firms that aim at leveraging the potential of voice-based interfaces to improve consumers’ service experiences and the theory-driven “conversational design” of voice-based interfaces.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11747-022-00868-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-022-04184-2,Retraction Note to: IoT using machine learning security enhancement in video steganography allocation for Raspberry Pi,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-022-04184-2,Springer,2022-06-20,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-022-04184-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s12968-022-00870-4,Highlights of the Virtual Society for Cardiovascular Magnetic Resonance 2022 Scientific Conference: CMR: improving cardiovascular care around the world,Journal of Cardiovascular Magnetic Resonance,10.1186/s12968-022-00870-4,BioMed Central,2022-06-20,"The 25th Society for Cardiovascular Magnetic Resonance (SCMR) Annual Scientific Sessions saw 1524 registered participants from more than 50 countries attending the meeting virtually. Supporting the theme “CMR: Improving Cardiovascular Care Around the World”, the meeting included 179 invited talks, 52 sessions including 3 plenary sessions, 2 keynote talks, and a total of 93 cases and 416 posters. The sessions were designed so as to showcase the multifaceted role of cardiovascular magnetic resonance (CMR) in identifying and prognosticating various myocardial pathologies. Additionally, various social networking sessions as well as fun activities were organized. The major areas of focus for the future are likely to be rapid efficient and high value CMR exams, automated and quantitative acquisition and post-processing using artificial intelligence and machine learning, multi-contrast imaging and advanced vascular imaging including 4D flow.",https://www.biomedcentral.com/openurl?doi=10.1186/s12968-022-00870-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11790-3,Tomato Leaf Disease Detection System Based on FC-SNDPN,Multimedia Tools and Applications,10.1007/s11042-021-11790-3,Springer,2022-06-18,"In this study, taking the common diseases in tomato leaves, which are typical crops in southern China, as the research object, a FC-SNDPN (Fully Convolutional – Switchable Normalization Dual Path Networks) -based method for automatic identification and detection of crop leaf diseases is proposed to solve the problem that traditional image identification methods for crop diseases and insect pests heavily rely on artificial feature extraction and have a poor generalization ability for image recognition with a complex background. In order to reduce the influence of the complicated background on the recognition of crops diseases and insect pests image, A full convolutional network (FCN) algorithm based on VGG-16 model is used to segment the target crop image. Then an improved DPN (Dual-Path Networks) model is proposed to improve the ability of feature extraction. SNDPN combines the connection method between Desnet and Resnet layers, forms a neural network by using SN layer, and adaptively optimizes the parameters of the dual-path neural network by switching the normalized layer, which improves the versatility of the network for different types of diseases and insect pests and the training speed of the network. Finally, the identification accuracy of the proposed method of using FCN for foreground segmentation and SNDPN for identification is 97.59% on the augmentation data set, the result proves the effectiveness of our method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11790-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10851-022-01105-y,Learning-Based Non-rigid Video Depth Estimation Using Invariants to Generalized Bas-Relief Transformations,Journal of Mathematical Imaging and Vision,10.1007/s10851-022-01105-y,Springer,2022-06-18,"We present a method to locally reconstruct dense video depth maps of a non-rigidly deformable object directly from a video sequence acquired by a static orthographic camera. The estimation of depth is performed locally on spatiotemporal patches of the video, and then, the full depth video is recovered by combining them together. Since the geometric complexity of a local spatiotemporal patch of a deforming non-rigid object is often simple enough to be faithfully represented with a parametric model, we artificially generate a database of small deforming rectangular meshes rendered with different material properties and light conditions, along with their corresponding depth videos, and use such data to train a convolutional neural network. Since the database images are rendered with an orthographic camera model, linear deformations along the optical axis cannot be recovered from the training images. These are known in the literature as generalized bas-relief (GBR) transformations. We address this ambiguity problem by employing the invariant-theoretic normalization procedure in order to obtain complete invariants with respect to this group of transformations, and use them in the loss function of a neural network. We tested our method on both synthetic and Kinect data and experimentally observed that the reconstruction error is significantly lower than the one obtained using conventional non-rigid structure from motion approaches and state-of-the-art video depth estimation techniques.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10851-022-01105-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11063-022-10919-1,BTSwin-Unet: 3D U-shaped Symmetrical Swin Transformer-based Network for Brain Tumor Segmentation with Self-supervised Pre-training,Neural Processing Letters,10.1007/s11063-022-10919-1,Springer,2022-06-17,"Medical image automatic segmentation plays an important role in Computer-Aided Diagnosis system. Although convolution-based network has achieved great performance in medical image segmentation, it has limitations in modeling long-range contextual interactions and spatial dependencies. Due to the powerful ability of long-range information interaction of Vision Transformer, Vision Transformer have achieved advanced performance in several downstream tasks via self-supervised learning. In this paper, motivative by Swin Transformer, we proposed BTSwin-Unet, which is a 3D U-shaped symmetrical Swin Transformer-based network for brain tumor segmentation. Moreover, we construct a self-supervised learning framework to pre-train the model encoder through the reconstruction task. Extensive experiments on tumor segmentation tasks validated the performance of our proposed model, and our results consistently demonstrate favorable benchmarks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11063-022-10919-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-022-02269-1,A novel U-Net with dense block for drum signal separation from polyphonic music signal mixture,"Signal, Image and Video Processing",10.1007/s11760-022-02269-1,Springer,2022-06-16,"Deep neural network algorithms have shown promising results for music source signal separation. Most existing methods rely on deep networks, where billions of parameters need to be trained. In this paper, we propose a novel autoencoder framework with a reduced number of parameters to separate the drum signal component from a music signal mixture. A denoising autoencoder with a U-Net architecture and direct skip connections was employed. A dense block is included in the bottleneck of the autoencoder stage. This technique was tested on both demixing secret data (DSD) and the MUSDB database. The source-to-distortion ratio (SDR) for the proposed method was at par with that of other state-of-the-art methods, whereas the number of parameters required was quite low, making it computationally more efficient. The experiment performed using the proposed method to separate drum signal yielded an average SDR of 5.71 on DSD and 6.45 on MUSDB database while using only 0.32 million parameters.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-022-02269-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-022-03581-6,Domestic pig sound classification based on TransformerCNN,Applied Intelligence,10.1007/s10489-022-03581-6,Springer,2022-06-16,"Excellent performance has been demonstrated in implementing challenging agricultural production processes using modern information technology, especially in the use of artificial intelligence methods to improve modern production environments. However, most of the existing work uses visual methods to train models that extract image features of organisms to analyze their behavior, and it may not be truly intelligent. Because vocal animals transmit information through grunts, the information obtained directly from the grunts of pigs is more useful to understand their behavior and emotional state, which is important for monitoring and predicting the health conditions and abnormal behavior of pigs. We propose a sound classification model called TransformerCNN, which combines the advantages of CNN spatial feature representation and the Transformer sequence coding to form a powerful global feature perception and local feature extraction capability. Through detailed qualitative and quantitative evaluations and by comparing state-of-the-art traditional animal sound recognition methods with deep learning methods, we demonstrate the advantages of our approach for classifying domestic pig sounds. The scores for domestic pig sound recognition accuracy, AUC and recall were 96.05%, 98.37% and 90.52%, respectively, all higher than the comparison model. In addition, it has good robustness and generalization capability with low variation in performance for different input features.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-022-03581-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07421-z,Dominant motion identification of multi-particle system using deep learning from video,Neural Computing and Applications,10.1007/s00521-022-07421-z,Springer,2022-06-15,"Identifying underlying governing equations and relevant information from high-dimensional observable data has always been a challenge in physical sciences. With the recent advances in sensing technology and available datasets, various machine learning techniques have made it possible to distill underlying mathematical models from sufficiently clean and usable datasets. However, most of these techniques rely on prior knowledge of the system and noise free data obtained by simulation of physical system or by direct measurements of the signals. Hence, the inference obtained by using these techniques is often unreliable to be used in the real world where observed data are noisy and require feature engineering to extract relevant features. In this work, we provide a deep-learning framework that extracts relevant information from real-world videos of highly stochastic systems, with no prior knowledge and distills the underlying governing equation representing the system. We demonstrate this approach on videos of confined multi-agent/particle systems of ants, termites, fishes as well as a simulated confined multi-particle system with elastic collision interactions. Furthermore, we explore how these seemingly diverse systems have predictable underlying behavior. In this study, we have used motion tracking to extract spatial trajectories of individual agents/particles in a system, and by using LSTM VAE we projected these features on to a low-dimensional latent space from which the underlying differential equation representing the data was extracted using SINDy framework.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07421-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-022-03813-9,Video prediction for driving scenes with a memory differential motion network model,Applied Intelligence,10.1007/s10489-022-03813-9,Springer,2022-06-15,"Accurate video prediction is critical for human drivers and self-driving cars to make informed decisions. To overcome the limitations of existing deep learning-based video prediction methods in driving scenes, such as inaccurate relative position prediction, high latency, and blurred multi-frame image prediction, we propose a Memory Differential Motion Network (MDMNet) model with a convolutional auto-encoder and a spatiotemporal information learning module, based on an improved Memory In Memory Network (MIM). One prominent characteristic of this newly constructed model is that in each time step, it can learn richer spatiotemporal representation features in each frame, and each layer computes the attention weight of the global spatiotemporal information collected, recalling and focusing on the crucial component. First, inspired by the notion of inter-frame differentiation, the network model incorporates motion filters and differential operations inside the unit, which makes it easier to extract the complex relative motion changes and trends of multiple objects in the scene. Second, an innovative global attention module between pictures and states is implemented to ensure global motion information integrity. Finally, to improve the quality of generated pictures, an adaptive convolutional auto-encoder is created. The MDMNet model outperforms other baseline prediction models in Mean Squared Error (MSE), Mean Absolute Error (MAE), Structural Similarity (SSIM), and Peak Signal to Noise Ratio (PSNR) when tested on D^2-city and BDD100K datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-022-03813-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-022-03813-9,Video prediction for driving scenes with a memory differential motion network model,Applied Intelligence,10.1007/s10489-022-03813-9,Springer,2022-06-15,"Accurate video prediction is critical for human drivers and self-driving cars to make informed decisions. To overcome the limitations of existing deep learning-based video prediction methods in driving scenes, such as inaccurate relative position prediction, high latency, and blurred multi-frame image prediction, we propose a Memory Differential Motion Network (MDMNet) model with a convolutional auto-encoder and a spatiotemporal information learning module, based on an improved Memory In Memory Network (MIM). One prominent characteristic of this newly constructed model is that in each time step, it can learn richer spatiotemporal representation features in each frame, and each layer computes the attention weight of the global spatiotemporal information collected, recalling and focusing on the crucial component. First, inspired by the notion of inter-frame differentiation, the network model incorporates motion filters and differential operations inside the unit, which makes it easier to extract the complex relative motion changes and trends of multiple objects in the scene. Second, an innovative global attention module between pictures and states is implemented to ensure global motion information integrity. Finally, to improve the quality of generated pictures, an adaptive convolutional auto-encoder is created. The MDMNet model outperforms other baseline prediction models in Mean Squared Error (MSE), Mean Absolute Error (MAE), Structural Similarity (SSIM), and Peak Signal to Noise Ratio (PSNR) when tested on D^2-city and BDD100K datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-022-03813-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13358-1,Tri-integrated convolutional neural network for audio image classification using Mel-frequency spectrograms,Multimedia Tools and Applications,10.1007/s11042-022-13358-1,Springer,2022-06-15,"Emotion is a state which encompasses a variety of physiological phenomena. Classification of emotions has many applications in fields like customer review, product evaluation, national security, etc., thus making it a prominent area of research. The state-of-art methodologies have used either text or audio files to classify emotions which is in contrast to the proposed work which utilizes the Mel-frequency spectrograms. An integrated methodology TiCNN (Tri integrated Convolutional Neural Network) has been proposed for classifying emotions into eight different classes. Three models namely VGG16, VGG19, and a proposed CNN architecture have been integrated and trained on the RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) dataset. The proposed integrated TiCNN approach classifies emotions into eight different classes with an accuracy of 93.27%. Precision, recall and F1-Score of 0.93, 0.92 and 0.92 have also been used as metrics to evaluate the performance of the proposed model. Further, for model validation, the efficiency and efficacy of the proposed methodology have been compared and analysed with the EMO-DB (Berlin Database of Emotional Speech) dataset. The proposed TiCNN model gives an accuracy of 92.78% on the EMO-DB dataset. Empirical evaluation of the proposed methodology has been compared with conventional transfer learning models and state-of-the-art methodologies, where it has shown its superiority over others.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13358-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-022-02688-y,HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-022-02688-y,Springer,2022-06-14,"Purpose The success or failure of modern computer-assisted surgery procedures hinges on the precise six-degree-of-freedom (6DoF) position and orientation (pose) estimation of tracked instruments and tissue. In this paper, we present HMD-EgoPose, a single-shot learning-based approach to hand and object pose estimation and demonstrate state-of-the-art performance on a benchmark dataset for monocular red-green-blue (RGB) 6DoF marker-less hand and surgical instrument pose tracking. Further, we reveal the capacity of our HMD-EgoPose framework for performant 6DoF pose estimation on a commercially available optical see-through head-mounted display (OST-HMD) through a low-latency streaming approach. Methods Our framework utilized an efficient convolutional neural network (CNN) backbone for multi-scale feature extraction and a set of subnetworks to jointly learn the 6DoF pose representation of the rigid surgical drill instrument and the grasping orientation of the hand of a user. To make our approach accessible to a commercially available OST-HMD, the Microsoft HoloLens 2, we created a pipeline for low-latency video and data communication with a high-performance computing workstation capable of optimized network inference. Results HMD-EgoPose outperformed current state-of-the-art approaches on a benchmark dataset for surgical tool pose estimation, achieving an average tool 3D vertex error of 11.0 mm on real data and furthering the progress towards a clinically viable marker-free tracking strategy. Through our low-latency streaming approach, we achieved a round trip latency of 199.1 ms for pose estimation and augmented visualization of the tracked model when integrated with the OST-HMD. Conclusion Our single-shot learned approach, which optimized 6DoF pose based on the joint interaction between the hand of a user and a rigid surgical drill, was robust to occlusion and complex surfaces and improved on current state-of-the-art approaches to marker-less tool and hand pose estimation. Further, we presented the feasibility of our approach for 6DoF object tracking on a commercially available OST-HMD.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-022-02688-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13375-0,Correction to: Crowd abnormality detection in video sequences using supervised convolutional neural network,Multimedia Tools and Applications,10.1007/s11042-022-13375-0,Springer,2022-06-14,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13375-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-022-04630-0,Augmentation dataset of a two-dimensional neural network model for use in the car parts segmentation and car classification of three dimensions,The Journal of Supercomputing,10.1007/s11227-022-04630-0,Springer,2022-06-14,"In this study, three-dimensional (3D) spatial data, two-dimensional (2D) texture information, and automatic marking processes were used for the detection and classification of car parts. The automatic marking processes involved automatic car part segmentation and classification, car part detection and classification on the basis of images of 2D textures, and car part segmentation and car identification on the basis of a 3D point cloud. The 2D image processing system identifies car parts and generates numerous car texture images, which are subjected to three stages of processing. In the first stage of processing, automated segmentation technology is used to segment images; in the second stage, different types of training images with various backgrounds are generated; and in the third stage, the You Only Look Once v3 model is used to identify car parts on the basis of the generated training images. The adopted 3D model conducts processing over two stages. In the first stage, a 3D triangular grid is combined with a texture image to achieve car part identification at a grid point by employing a PointNet model trained using ground truth data. In the second stage, the trained PointNet model is used for detecting the parts of a 3D car model on the basis of 3D triangular mesh data. The precision of fine part segmentation from texture images was considerably higher than that of simple part segmentation. In car part detection and classification experiments on texture images, the mean intersection over union (mIoU) and mean average percentage both exceeded 70%. In a 3D car model experiment conducted using the ShapeNet dataset, the average mIoU and accuracy were 73.66% and 90.2%, respectively. When the developed PointNet model was trained using the Train-2000 dataset, the mIoU and accuracy of the model were 40.6% and 56.64%, respectively. When the developed PointNet model was trained using the Train dataset, the mIoU and accuracy of the model were 40.73% and 61.61%, respectively. The developed model achieved a 4.97% higher accuracy when training it with the Train dataset than when training it with the Train-2000 dataset. However, the mIoU of this model was only 0.13% higher when it was trained with the Train dataset than when it was trained with the Train-2000 dataset. When 3DNetWeight-2 was used as the initial parameter of the developed model in the final training, the mIoU and accuracy of the model were 44.27% and 61.98%, respectively—3.54% and 0.97% higher than those obtained without transfer learning, respectively. In sum, the proposed method can identify different parts of objects, and the findings serve as a reference for the design of simulation systems for autonomous vehicles. There are also applied to automated automobile management, medical technology, military training, aerospace technology, and disaster response systems.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-022-04630-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40494-022-00723-4,High-resolution micro-CT with 3D image analysis for porosity characterization of historic bricks,Heritage Science,10.1186/s40494-022-00723-4,Springer,2022-06-13,"The study of pores in historic bricks is important for characterizing and comparing brick materials, evaluating the degree of deterioration, predicting behavior in future weathering conditions, studying the effectiveness of protective measures, and analyzing the potential effects of cleaning treatments. High-resolution micro-CT coupled with 3D image analysis is a promising new approach for studying porosity and pore systems in bricks. In this technique, hundreds or even thousands of X-ray projection images are acquired at 360 degrees around a sample. The X-radiation passing through the sample is absorbed, with radiation attenuated to varying degrees depending on the varying densities of phases within the object. The 3D volume is reconstructed by a computer algorithm, producing images where each voxel has a grayscale intensity value associated with the component it represents. Recent new instrument designs allow fast scanning with good spatial resolution. In this research, we present a set of protocols for creating optimal images of brick pores in micro-CT scans and for conducting 3D image analysis to extract both qualitative and quantitative data from those scans. Small samples give better spatial resolution for imaging of pores, so given the typical heterogeneity of bricks, scanning multiple samples from each brick ensures that the results are more likely to be representative. Machine learning and deep learning with convolutional neural networks were found to be important tools for better distinguishing pores from the surrounding matrix in the segmentation process, especially at the very limits of spatial resolution. Statistical analyses revealed which of the many parameters that can be measured are potentially most significant for characterizing the pore systems of bricks. These significant pore variables came from a multi-staged image analysis approach and include the total volume percent occupied by pores, the percentage of those pores accessible to the surface versus isolated interior ones, a variety of statistical properties of individual pores related to their size and shape, the average number of connections that pores have to other pores, and the length, diameter, and directness of those connections. Graphical Abstract ",https://www.biomedcentral.com/openurl?doi=10.1186/s40494-022-00723-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40032-022-00835-7,Machine Learning Model Selection for Performance Prediction in 3D Printing,Journal of The Institution of Engineers (India): Series C,10.1007/s40032-022-00835-7,Springer,2022-06-13,"Prediction of elongation, roughness, tensile strength yield of a 3D printed product is important to boost the quality of the printing process. This paper inspects and compares most machine learning regression strategies to build a robust representation that best models the 3D printing process. 3D printing process is affected by nine primary input factors such as layer height, wall thickness, infill density, infill pattern, nozzle temperature, bed temperature, print speed, material and fan speed. These factors influence the quality of the 3D printed product. Multiple machine learning models are run based on the experiments and the best model is selected based on the prediction performance. Among the various machine learning strategies, the best is identified which best predicts the output factors. It was inferred that “radial basis function regressor” algorithm was able to best predict the results for elongation with a mean absolute error of 0.3421 and root mean square error of 0.3421. “Additive regression” was able to best predict the results for elongation with a mean absolute error of and 28.2969 and root mean square error of 39.2895. “Random committee regression” was able to best predict the results for tensile strength with a mean absolute error of 4.112 and root mean square error of 5.2789.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40032-022-00835-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00034-022-02068-6,Speaker Adversarial Neural Network (SANN) for Speaker-independent Speech Emotion Recognition,"Circuits, Systems, and Signal Processing",10.1007/s00034-022-02068-6,Springer,2022-06-13,"Recently, domain adversarial neural networks (DANN) have delivered promising results for out of domain data. This paper exploits DANN for speaker independent emotion recognition, where the domain corresponds to speakers, i.e. the training and testing datasets contain different speakers. The result is a speaker adversarial neural network (SANN). The proposed SANN is used for extracting speaker-invariant and emotion-specific discriminative features for the task of speech emotion recognition. To extract speaker-invariant features, multi-tasking adversarial training of a deep neural network (DNN) is employed. The DNN framework consists of two sub-networks: one for emotion classification (primary task) and the other for speaker classification (secondary task). The gradient reversal layer (GRL) was introduced between (a) the layer common to both the primary and auxiliary classifiers and (b) the auxiliary classifier. The objective of the GRL layer is to reduce the variance among speakers by maximizing the speaker classification loss. The proposed framework jointly optimizes the above two sub-networks to minimize the emotion classification loss and mini-maximize the speaker classification loss. The proposed network was evaluated on the IEMOCAP and EMODB datasets. A total of 1582 features were extracted from the standard library openSMILE. A subset of these features was eventually selected using a genetic algorithm approach. On the IEMOCAP dataset, the proposed SANN model achieved relative improvements of +6.025% (weighted accuracy) and +5.62% (unweighted accuracy) over the baseline system. Similar results were observed for the EMODB dataset. Further, in spite of differences with respect to models and features with state-of-the-art methods, significant improvement in accuracy values was also obtained over them.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00034-022-02068-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s13636-022-00244-9,Language agnostic missing subtitle detection,"EURASIP Journal on Audio, Speech, and Music Processing",10.1186/s13636-022-00244-9,Springer,2022-06-11,"Subtitles are a crucial component of Digital Entertainment Content (DEC such as movies and TV shows) localization. With ever increasing catalog (≈ 2M titles) and localization expansion (30+ languages), automated subtitle quality checks becomes paramount. Being a manual creation process, subtitles can have errors such as missing transcriptions, out-of-sync subtitle blocks with the audio and incorrect translations. Such erroneous subtitles result in an unpleasant viewing experience and impact the viewership. Moreover, manual correction is laborious, highly costly and requires expertise of audio and subtitle languages. A typical subtitle correction process consists of (1) linear watch of the movie, (2) identification of time stamps associated with erroneous subtitle blocks, and (3) correcting procedure. Among the three, time taken to watch the entire movie by a human expert is the most time consuming step. This paper discusses the problem of missing transcription, where the subtitle blocks corresponding to some speech segments in the DEC are non-existent. We present a solution to augment human correction process by automatically identifying the timings associated with the non-transcribed dialogues in a language agnostic manner. The correction step can then be performed by either human-in-the-loop mechanism or automatically using neural transcription (speech-to-text in same language) and translation (text-to-text in different languages) engines. Our method uses a language agnostic neural voice activity detector (VAD) and an audio classifier (AC) trained explicitly on DEC corpora for better generalization. The method consists of three steps: first, we use VAD to identify the timings associated with dialogues (predicted speech blocks). Second, we refine those timings using the AC module by removing the timings associated with the leading and trailing non-speech segments identified as speech by VAD. Finally, we compare the predicted dialogue timings to the dialogue timings present in the subtitle file (subtitle speech blocks) and flag the missing transcriptions. We empirically demonstrate that the proposed method (a) reduces incorrect predicted missing subtitle timings by 10%, (b) improves the predicted missing subtitle timings by 2.5%, (c) reduces false positive rate (FPR) of overextending the predicted timings by 77%, and (d) improves the predicted speech block-level precision by a 119% over VAD baseline on a human-annotated dataset of missing subtitle speech blocks.",https://www.biomedcentral.com/openurl?doi=10.1186/s13636-022-00244-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11276-022-03000-1,Strategy for improving the football teaching quality by AI and metaverse-empowered in mobile internet environment,Wireless Networks,10.1007/s11276-022-03000-1,Springer,2022-06-11,"The metaverse has influenced the development of science and technology since it was proposed. While artificial intelligence (AI) is still one of the important technologies to solve problems in metaverse. The success of AI in various fields and the development of information technology make the integration of sports industry and AI an inevitable trend. Currently, the traditional physical education is experiencing qualitative changes, and the demand of integrating AI and metaverse into physical education is becoming more and more obvious. Football teaching is an important part of physical education, and Virtual Reality (VR) has the characteristics of immersion, interaction and imagination, which can build virtual and realistic football teaching process. In this study, through 360-degree panoramic VR football teaching videos empowered by the metaverse and K-means algorithm based on machine learning under AI, we study the strategies for improving the quality of football teaching in the mobile Internet environment. Therefore, we propose K-means based optimized 360-degree panoramic VR football teaching video delivery strategy. In addition, we conduct simulation experiments under content delivery networks simulator, and the simulation results show that the proposed strategy is superior to the baselines in terms of proxy server hit ratio, byte hit ratio, mean response time and students quality of experience. Moreover, through the 360-degree panoramic VR football teaching videos learning, students can intuitively analyze the actions and improve the teaching quality. The reconstruction of football teaching environment is beneficial to promote the combination of football teaching and smart learning.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11276-022-03000-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00530-022-00957-z,Facial expression recognition of online learners from real-time videos using a novel deep learning model,Multimedia Systems,10.1007/s00530-022-00957-z,Springer,2022-06-10,"In every learning setting, in classrooms or online, a student's emotions throughout course involvement play a critical role. It employs disturbing, excite, and eye and head movement patterns to infer important information about a student's mood in an e-learning environment. Researchers from numerous disciplines have been focusing on emotion detection technologies to better understand user engagement, efficacy, and utility of systems that have been established or are being deployed. The goal of this study is to see if students' facial expressions can be used by lecturers to understand students' comprehension levels in a virtual classroom, as well as to determine the influence of facial expressions during lectures and the degree of comprehension displayed by these emotions. The objective is to determine which facial physical behaviours are associated with emotional states and then to determine how these emotional states are related to student understanding. The major purpose of this work is to plan and develop a new deep learning-oriented facial expression recognition (FER) of online learners from real-time videos. For the frames of online learners from real-time videos, the Viola–Jones Face detection algorithm is employed for face detection. Further, the pattern extraction is performed by the optimized local directional Texture Pattern (LDTP) using the hybrid Coyote Optimization Algorithm (COA), and Deer Hunting Optimization Algorithm (DHOA) referred as Coyote–Deer Hunting Optimization (C-DHO). These pattern images are inputted to the convolutional neural network (CNN) for deep feature extraction. Furthermore, the heuristically modified recurrent neural network (HM-RNN) using the same C-DHO is used for the expression recognition. The experimental research reveals that the suggested method aids in the identification of emotions as well as the classification of student participation and interest in the topic, all of which are displayed as feedback to the teacher in terms of improving the learner experience.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-022-00957-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00330-022-08877-2,Comparison of utility of deep learning reconstruction on 3D MRCPs obtained with three different k-space data acquisitions in patients with IPMN,European Radiology,10.1007/s00330-022-08877-2,Springer,2022-06-10,"Objective To compare the utility of deep learning reconstruction (DLR) for improving acquisition time, image quality, and intraductal papillary mucinous neoplasm (IPMN) evaluation for 3D MRCP obtained with parallel imaging (PI), multiple k-space data acquisition for each repetition time (TR) technique (Fast 3D mode multiple: Fast 3Dm) and compressed sensing (CS) with PI. Materials and methods A total of 32 IPMN patients who had undergone 3D MRCPs obtained with PI, Fast 3Dm, and CS with PI and reconstructed with and without DLR were retrospectively included in this study. Acquisition time, signal-to-noise ratio (SNR), and contrast-to-noise ratio (CNR) obtained with all protocols were compared using Tukey’s HSD test. Results of endoscopic ultrasound, ERCP, surgery, or pathological examination were determined as standard reference, and distribution classifications were compared among all 3D MRCP protocols by McNemar’s test. Results Acquisition times of Fast 3Dm and CS with PI with and without DLR were significantly shorter than those of PI with and without DLR ( p < 0.05). Each MRCP sequence with DLR showed significantly higher SNRs and CNRs than those without DLR ( p < 0.05). IPMN distribution accuracy of PI with and without DLR and Fast 3Dm with DLR was significantly higher than that of Fast 3Dm without DLR and CS with PI without DLR ( p < 0.05). Conclusion DLR is useful for improving image quality and IPMN evaluation capability on 3D MRCP obtained with PI, Fast 3Dm, or CS with PI. Moreover, Fast 3Dm and CS with PI may play as substitution to PI for MRCP in patients with IPMN. Key Points • Mean examination times of multiple k-space data acquisitions for each TR and compressed sensing with parallel imaging were significantly shorter than that of parallel imaging (p < 0.0001). • When comparing image quality of 3D MRCPs with and without deep learning reconstruction, deep learning reconstruction significantly improved signal-to-noise ratio and contrast-to-noise ratio (p < 0.05). • IPMN distribution accuracies of parallel imaging with and without deep learning reconstruction (with vs. without: 88.0% vs. 88.0%) and multiple k-space data acquisitions for each TR with deep learning reconstruction (86.0%) were significantly higher than those of others (p < 0.05).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00330-022-08877-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12720-7,A coarse-to-fine temporal action detection method combining light and heavy networks,Multimedia Tools and Applications,10.1007/s11042-022-12720-7,Springer,2022-06-10,"Temporal action detection aims to judge whether there existing a certain number of action instances in a long untrimmed videos and to locate the start and end time of each action. Even though the existing action detection methods have shown promising results in recent years with the widespread application of Convolutional Neural Network (CNN), it is still a challenging problem to accurately locate each action segment while ensuring real-time performance. In order to achieve a good tradeoff between detection efficiency and accuracy, we present a coarse-to-fine hierarchical temporal action detection method by using multi-scale sliding window mechanism. Since the complexity of the convolution operator is proportional to the number and the size of the input video clips, the idea of our proposed method is to first determine candidate action proposals and then perform the detection task on these candidate action proposals only with a view to reducing the overall complexity of the detection method. By making full use of the spatio-temporal information of video clips, a lightweight 3D-CNN classifier is first used to quickly determine whether the video clip is a candidate action proposal, avoiding the re-detection of a large number of non-action video clips by the heavyweight deep network. A heavyweight detector is designed to further improve the accuracy of action positioning by considering both boundary regression loss and category loss in the target loss function. In addition, the Non-Maximum Suppression (NMS) is performed to eliminate redundant detection results among the overlapping proposals. The mean Average Precision (mAP) is 40.6%, 51.7% and 20.4% on THUMOS14, ActivityNet and MPII Cooking dataset when the Intersection-over-Union (tIoU) threshold is set to 0.5, respectively. Experimental results show the superior performance of the proposed method on three challenging temporal activity detection datasets while achieving real-time speed. At the same time, our method can generate proposals for unseen action classes with high recalls.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12720-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00530-022-00952-4,Automated brain tumor malignancy detection via 3D MRI using adaptive-3-D U-Net and heuristic-based deep neural network,Multimedia Systems,10.1007/s00530-022-00952-4,Springer,2022-06-08,"Using the 3D image from public benchmark sources, the experiment is initiated with pre-processing using skull stripping and contrast enhancement. Further, the segmentation of tumor region is performed by the Adaptive-3-D U-Net (A-3D-U-Net) utilized for the hybridized Butterfly Optimization Algorithm (BOA), and Tunicate Swarm Algorithm (TSA) termed to as Butterfly–Tunicate Swarm Algorithm (B-TSA). The optimal segmentation of tumors is based on solving the multi-objective solution concerning “Structured Similarity Index (SSIM), Mean Square Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Dice Coefficient”. From the segmented tumor region, the numerical features such as “mean, standard deviation, entropy, skewness, kurtosis, energy, contrast, inverse difference moment, directional moment, correlation, coarseness, and texture features like Local Ternary Pattern (LTP), and Local Tetra Pattern (LTrP)” are extracted. In the final stage, the detection of malignancy is performed by heuristic-based deep neural network (HDNN) using the same proposed B-TSA for the parameter optimization. The findings of applying the suggested methodology to 3D-MRI images from the Decathlon dataset demonstrate that the suggested technique is comparable to conventional methods for brain tumor segmentation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-022-00952-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07443-7,Topical collections on machine learning based semantic representation and analytics for multimedia application,Neural Computing and Applications,10.1007/s00521-022-07443-7,Springer,2022-06-08,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07443-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s12859-022-04762-3,Sfcnn: a novel scoring function based on 3D convolutional neural network for accurate and stable protein–ligand affinity prediction,BMC Bioinformatics,10.1186/s12859-022-04762-3,BioMed Central,2022-06-08,"Background Computer-aided drug design provides an effective method of identifying lead compounds. However, success rates are significantly bottlenecked by the lack of accurate and reliable scoring functions needed to evaluate binding affinities of protein–ligand complexes. Therefore, many scoring functions based on machine learning or deep learning have been developed to improve prediction accuracies in recent years. In this work, we proposed a novel featurization method, generating a new scoring function model based on 3D convolutional neural network. Results This work showed the results from testing four architectures and three featurization methods, and outlined the development of a novel deep 3D convolutional neural network scoring function model. This model simplified feature engineering, and in combination with Grad-CAM made the intermediate layers of the neural network more interpretable. This model was evaluated and compared with other scoring functions on multiple independent datasets. The Pearson correlation coefficients between the predicted binding affinities by our model and the experimental data achieved 0.7928, 0.7946, 0.6758, and 0.6474 on CASF-2016 dataset, CASF-2013 dataset, CSAR_HiQ_NRC_set, and Astex_diverse_set, respectively. Overall, our model performed accurately and stably enough in the scoring power to predict the binding affinity of a protein–ligand complex. Conclusions These results indicate our model is an excellent scoring function, and performs well in scoring power for accurately and stably predicting the protein–ligand affinity. Our model will contribute towards improving the success rate of virtual screening, thus will accelerate the development of potential drugs or novel biologically active lead compounds.",https://www.biomedcentral.com/openurl?doi=10.1186/s12859-022-04762-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-021-09956-3,A comparison of neural-based visual recognisers for speech activity detection,International Journal of Speech Technology,10.1007/s10772-021-09956-3,Springer,2022-06-08,"Existing literature on speech activity detection (SAD) highlights different approaches within neural networks but does not provide a comprehensive comparison to these methods. This is important because such neural approaches often require hardware-intensive resources. In this article, we provide a comparative analysis of three different approaches: classification with still images (CNN model), classification based on previous images (CRNN model), and classification of sequences of images (Seq2Seq model). Our experimental results using the Vid-TIMIT dataset show that the CNN model can achieve an accuracy of 97% whereas the CRNN and Seq2Seq models increase the classification to 99%. Further experiments show that the CRNN model is almost as accurate as the Seq2Seq model (99.1% vs. 99.6% of classification accuracy, respectively) but 57% faster to train (326 vs. 761 secs. per epoch).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-021-09956-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40684-022-00444-w,Systematic Literature Review on Augmented Reality-Based Maintenance Applications in Manufacturing Centered on Operator Needs,International Journal of Precision Engineering and Manufacturing-Green Technology,10.1007/s40684-022-00444-w,Springer,2022-06-07,"Smart manufacturing supported by emerging Industry 4.0 technologies is a key driver to realize mass product customizations. Augmented reality (AR) has been commonly applied to facilitate manual operations with ambient intelligence by overlaying virtual information on physical scenes. In most modern factories, maintenance remains an indispensable process that is difficult or yet to be fully automated. Several studies have previously reviewed AR-based maintenance across all industrial sectors, whereas those specific to manufacturing did not necessarily involve maintenance. Hence, this paper presents a systematic literature review on AR-assisted maintenance in manufacturing with a focus on the operator’s needs. A generic process has been proposed to classify the maintenance operations examined in the past studies into four sequential steps and to analyze the classification results based on the geographical location, maintenance type, AR technical elements, and integrated external sensors. The findings thus derived are expected to provide design guidelines for implementing AR applications with practical values to aid manual maintenance in future smart manufacturing environments.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40684-022-00444-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-022-03766-z,"Deepfakes generation and detection: state-of-the-art, open challenges, countermeasures, and way forward",Applied Intelligence,10.1007/s10489-022-03766-z,Springer,2022-06-04,"Easy access to audio-visual content on social media, combined with the availability of modern tools such as Tensorflow or Keras, and open-source trained models, along with economical computing infrastructure, and the rapid evolution of deep-learning (DL) methods have heralded a new and frightening trend. Particularly, the advent of easily available and ready to use Generative Adversarial Networks (GANs), have made it possible to generate deepfakes media partially or completely fabricated with the intent to deceive to disseminate disinformation and revenge porn, to perpetrate financial frauds and other hoaxes, and to disrupt government functioning. Existing surveys have mainly focused on the detection of deepfake images and videos; this paper provides a comprehensive review and detailed analysis of existing tools and machine learning (ML) based approaches for deepfake generation, and the methodologies used to detect such manipulations in both audio and video. For each category of deepfake, we discuss information related to manipulation approaches, current public datasets, and key standards for the evaluation of the performance of deepfake detection techniques, along with their results. Additionally, we also discuss open challenges and enumerate future directions to guide researchers on issues which need to be considered in order to improve the domains of both deepfake generation and detection. This work is expected to assist readers in understanding how deepfakes are created and detected, along with their current limitations and where future research may lead.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-022-03766-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-022-03714-x,Self-supervised method for 3D human pose estimation with consistent shape and viewpoint factorization,Applied Intelligence,10.1007/s10489-022-03714-x,Springer,2022-06-03,"3D human pose estimation from monocular images has shown great success due to the sophisticated deep network architectures and large 3D human pose datasets. However, it is still an open problem when such datasets are unavailable. Estimating 3D human poses from monocular images is an ill-posed inverse problem. In our work, we propose a novel self-supervised method, which effectively trains a 3D human pose estimation network without any extra 3D pose annotations. Different from the commonly used GAN-based technique, our method overcomes the projection ambiguity problem by fully disentangling the camera viewpoint information from the 3D human shape. Specifically, we design a factorization network to predict the coefficients of canonical 3D human pose and camera viewpoint in two separate channels. Here, we represent the canonical 3D human pose as a combination of pose basis from a dictionary. To guarantee consistent factorization, we design a simple yet effective loss function taking advantage of multi-view information. Besides, in order to generate robust canonical reconstruction from the 3D pose coefficient, we exploit the underlying 3D geometry of human poses to learn a novel hierarchical dictionary from 2D poses. The hierarchical dictionary has stronger 3D pose expressibility than the traditional single-level dictionary. We comprehensively evaluate the proposed method on two public 3D human pose datasets, Human3.6M and MPI-INF-3DHP. The experimental results show that our method can maximally disentangle 3D human shapes and camera viewpoints, as well as reconstruct 3D human poses accurately. Moreover, our method achieves state-of-the-art results compared with recent weakly/self-supervised methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-022-03714-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-13220-2,Automated soccer head impact exposure tracking using video and deep learning,Scientific Reports,10.1038/s41598-022-13220-2,Nature,2022-06-03,"Head impacts are highly prevalent in sports and there is a pressing need to investigate the potential link between head impact exposure and brain injury risk. Wearable impact sensors and manual video analysis have been utilized to collect impact exposure data. However, wearable sensors suffer from high deployment cost and limited accuracy, while manual video analysis is a long and resource-intensive task. Here we develop and apply DeepImpact, a computer vision algorithm to automatically detect soccer headers using soccer game videos. Our data-driven pipeline uses two deep learning networks including an object detection algorithm and temporal shift module to extract visual and temporal features of video segments and classify the segments as header or nonheader events. The networks were trained and validated using a large-scale professional-level soccer video dataset, with labeled ground truth header events. The algorithm achieved 95.3% sensitivity and 96.0% precision in cross-validation, and 92.9% sensitivity and 21.1% precision in an independent test that included videos of five professional soccer games. Video segments identified as headers in the test data set correspond to 3.5 min of total film time, which can be reviewed through additional manual video verification to eliminate false positives. DeepImpact streamlines the process of manual video analysis and can help to collect large-scale soccer head impact exposure datasets for brain injury research. The fully video-based solution is a low-cost alternative for head impact exposure monitoring and may also be expanded to other sports in future work.",https://www.nature.com/articles/s41598-022-13220-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1557/s43577-022-00327-0,Physics informs machine learning for crack-free printing of metals,MRS Bulletin,10.1557/s43577-022-00327-0,Springer,2022-06-03,,http://link.springer.com/openurl/fulltext?id=doi:10.1557/s43577-022-00327-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41524-022-00803-w,AutoPhaseNN: unsupervised physics-aware deep learning of 3D nanoscale Bragg coherent diffraction imaging,npj Computational Materials,10.1038/s41524-022-00803-w,Nature,2022-06-03,"The problem of phase retrieval underlies various imaging methods from astronomy to nanoscale imaging. Traditional phase retrieval methods are iterative and are therefore computationally expensive. Deep learning (DL) models have been developed to either provide learned priors or completely replace phase retrieval. However, such models require vast amounts of labeled data, which can only be obtained through simulation or performing computationally prohibitive phase retrieval on experimental datasets. Using 3D X-ray Bragg coherent diffraction imaging (BCDI) as a representative technique, we demonstrate AutoPhaseNN, a DL-based approach which learns to solve the phase problem without labeled data. By incorporating the imaging physics into the DL model during training, AutoPhaseNN learns to invert 3D BCDI data in a single shot without ever being shown real space images. Once trained, AutoPhaseNN can be effectively used in the 3D BCDI data inversion about 100× faster than iterative phase retrieval methods while providing comparable image quality.",https://www.nature.com/articles/s41524-022-00803-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12065-022-00733-y,AI-based English teaching cross-cultural fusion mechanism,Evolutionary Intelligence,10.1007/s12065-022-00733-y,Springer,2022-06-03,"Due to the differences between eastern and western cultures, many students often face difficulties in language learning caused by cultural differences in learning English. Adaptability in cross-cultural communication and fluency in language learning are the highest realm of learning English and the highest pursuit for every student’s learning. To push the dialogue contents for students, this paper proposes a recognition model of multi-input feature extraction based on convolutional neural networks (CNN) for English emotion recognition. The model adopts an end-to-end training method for learning, which can simplify the workflow and improve the overall recognition accuracy. At first, a speech is transformed into a form more suitable for model learning: spectrogram and Mel spectrogram. Then, it is input into the model for automatic feature extraction. In the model design, the information of spectrogram in time-domain and frequency-domain is considered at the same time. Finally, the high-level features of the image are fused, which greatly improves the effect of speech emotion recognition. Experimental results prove that the proposed speech emotion recognition fusion model has high recognition accuracy and quality of experience.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12065-022-00733-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40537-022-00628-w,Comparative analysis of deep learning based Afaan Oromo hate speech detection,Journal of Big Data,10.1186/s40537-022-00628-w,Springer,2022-06-02,"Social media platforms like Facebook, YouTube, and Twitter are banking on developing machine learning models to help stop the spread of hateful speech on their platforms. The idea is that machine learning models that utilize natural language processing will detect hate speech faster and better than people can. Despite numerous progress has been made for resource reach language, only a few attempts have been made for Ethiopian Languages such as Afaan Oromo. This paper examines the viability of deep learning models for Afaan Oromo hate speech recognition. Toward this, the biggest dataset of hate speech was collected and annotated by the language experts. Variations of profound deep learning models such as CNN, LSTMs, BiLSTMs, LSTM, GRU, and CNN-LSTM are examined to evaluate their viability in identifying Afaan Oromo Hate speeches. The result uncovers that the model dependent on CNN and Bi-LSTM outperforms all the other investigated models with an average F1-score of 87%.",https://www.biomedcentral.com/openurl?doi=10.1186/s40537-022-00628-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11660-y,E^2-PV-RCNN: improving 3D object detection via enhancing keypoint features,Multimedia Tools and Applications,10.1007/s11042-021-11660-y,Springer,2022-06-02,"3D object detection plays a vital role and exerts a growing important effect in many applications, such as 3D scene understanding and autonomous driving. In this paper, we present a high-performance 3D object detection network, keypoint enhancement-pointvoxel-RCNN (E^2-PV-RCNN). In the proposed network, the whole scene is encoded by a set of keypoints, and two modules are proposed to enhance keypoint features. One is local feature enhancement module, which learns and enhances keypoint features by deeply fusing rich local spatial information, and keep comprehensive and representative features. The other one is keypoint weight enhancement module, it introduces raw point-wise supervision to enhance the learning process of keypoint weights, and an auxiliary centre regression task is carried out in training stage to achieve better performance. With the enhanced keypoint features, more accurate predictions can be obtained. Experiment results shows that the proposed E^2-PV-RCNN achieves state-of-the-art performance on KITTI official 3D detection benchmark for cars, which ranks 1^st among results with published works until submission. It also significantly lifts the 3D detection performance for cyclists in moderate and hard subsets by 3.93% and 2.86%, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11660-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s43154-022-00073-w,Applications for Augmented and Virtual Reality in Robot-Assisted Spine Surgery,Current Robotics Reports,10.1007/s43154-022-00073-w,Springer,2022-06-01,"Surgical innovation is at an all-time high with technologies such as robotics, advanced image-guidance, augmented reality, virtual reality, and artificial intelligence aiming to shape the future of spine surgery. The aim of this review is to analyze these emerging technologies. Combining these technologies with minimally invasive techniques may serve to further minimize disruption of normal anatomy, blood loss, postoperative pain, narcotic requirements, recovery time, and complications. Specific to spine surgery, these advancements may provide for enhanced accuracy of spinal implant placement, optimized surgical ergonomics, and decreased radiation exposure and further facilitate resident and fellow education. Combining augmented and virtual reality with current image-guidance systems and robotics adds a further layer of data for real-time intraoperative feedback, pre-operative planning, and trainee education. This review discusses the current concepts of augmented and virtual reality and their applications to robot-assisted spine surgery.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s43154-022-00073-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02263-7,3D hand pose estimation using RGBD images and hybrid deep learning networks,The Visual Computer,10.1007/s00371-021-02263-7,Springer,2022-06-01,"Hand pose estimation is one of the most attractive research areas for image processing. Among the human body parts, hands are particularly important for human–machine interactions. The advent of commercial depth cameras along with the rapid growth of deep learning has made great progress in all image processing fields, especially in hand pose estimation. In this study, using depth data, we introduce two hybrid deep neural networks to estimate 3D hand poses with fewer computations and higher accuracy compared with their counterparts. Due to the fact that the dimensions of data are reduced while passing through successive layers of networks, which causes data to be lost, we use the concept of residual network to compensate this phenomenon. By incorporating data from several views, the estimated poses are more robust in the occlusions. Evaluation results show the superiority of the proposed networks in terms of accuracy and implementation complexity.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02263-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10162-022-00846-2,Harnessing the Power of Artificial Intelligence in Otolaryngology and the Communication Sciences,Journal of the Association for Research in Otolaryngology,10.1007/s10162-022-00846-2,Springer,2022-06-01,"Use of artificial intelligence (AI) is a burgeoning field in otolaryngology and the communication sciences. A virtual symposium on the topic was convened from Duke University on October 26, 2020, and was attended by more than 170 participants worldwide. This review presents summaries of all but one of the talks presented during the symposium; recordings of all the talks, along with the discussions for the talks, are available at https://www.youtube.com/watch?v=ktfewrXvEFg and https://www.youtube.com/watch?v=-gQ5qX2v3rg . Each of the summaries is about 2500 words in length and each summary includes two figures. This level of detail far exceeds the brief summaries presented in traditional reviews and thus provides a more-informed glimpse into the power and diversity of current AI applications in otolaryngology and the communication sciences and how to harness that power for future applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10162-022-00846-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12532-9,Violence detection in videos using interest frame extraction and 3D convolutional neural network,Multimedia Tools and Applications,10.1007/s11042-022-12532-9,Springer,2022-06-01,"With the rapid development of detecting violent behaviors in surveillance cameras, requests on systems that automatically recognize violent events are expanded. Nowadays, violence detection has become an active research field in image processing and machine learning. The relevant works in such a field are classified into hand-crafted and deep learning methods. Despite the effectiveness of hand-crafted ones, their computational cost may be suppressive for practical applications. Additionally, deep learning techniques usually exploit 3D Convolutional Networks (3D ConvNets) to do this task. To improve the accuracy of these networks, meaningful regions and temporal changes in videos should be considered. Consequently, the performance of a 3D ConvNet can be reinforced by selecting significant temporal information and noticing to special regions in two spatial dimensions. In this work, we propose a novel 3D ConvNet along with a technique for extracting interest frames. The Structural Similarity Index Measure (SSIM) is exploited to extract interest frames as significant temporal information. Indeed, the SSIM uses the statistical features of two consecutive frames for this reason. In this way, sixteen video frames with the smallest SSIM are considered as dominant motion frames, which are then sent to a 3D CNN for classification. Moreover, a spatial attention module is exploited to make attention on the specific regions. Furthermore, three benchmark datasets are employed to evaluate the performance of the proposed method. The results show that in terms of accuracy, our scheme outperforms existing approaches.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12532-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-06968-1,A context aware-based deep neural network approach for simultaneous speech denoising and dereverberation,Neural Computing and Applications,10.1007/s00521-022-06968-1,Springer,2022-06-01,"Generally, the recorded speech signal is corrupted by both room reverberation and background noise leading to a reduced speech quality and intelligibility. In order to deal with the distortions caused by the joint effect of noise and reverberation, we propose a context-aware-based deep neural network (DNN) approach for simultaneous speech denoising and dereverberation. The proposed system consists of two stages such as denoising stage and the dereverberation stage. In the denoising stage, the additive noise is suppressed by estimating a phase-sensitive mask using DNN. Then, the noise-free reverberant speech is processed through the dereverberation stage. In the dereverberation stage, a reverberation-time-aware DNN-based model is used to perform dereverberation by adopting two reverberation time-dependent parameters such as frameshift size and acoustic context size to get the benefits of the characteristics of the superposition and frame-wise temporal correlations in different reverberation circumstances. Finally, we integrate both the modules and employ the integrated module for joint training using a multi objective loss function to further optimize both the denoising and dereverberation stages. Experimental results show that the proposed approach has shown significant performance improvements over prevalent benchmark dereverberation algorithms on IEEE corpus, REVERB challenge, and TIMIT corpus datasets under several reverberation circumstances.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-06968-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11554-022-01202-6,Lightweight convolutional neural network for real-time 3D object detection in road and railway environments,Journal of Real-Time Image Processing,10.1007/s11554-022-01202-6,Springer,2022-06-01,"For smart mobility, and autonomous vehicles (AV), it is necessary to have a very precise perception of the environment to guarantee reliable decision-making, and to be able to extend the results obtained for the road sector to other areas such as rail. To this end, we introduce a new single-stage monocular real-time 3D object detection convolutional neural network (CNN) based on YOLOv5, dedicated to smart mobility applications for both road and rail environments. To perform the 3D parameter regression, we replace YOLOv5’s anchor boxes with our hybrid anchor boxes. Our method is available in different model sizes such as YOLOv5: small, medium, and large. The new model that we propose is optimized for real-time embedded constraints (lightweight, speed, and accuracy) that takes advantage of the improvement brought by split attention (SA) convolutions called small split attention model (Small-SA). To validate our CNN model, we also introduce a new virtual dataset for both road and rail environments by leveraging the video game Grand Theft Auto V (GTAV). We provide extensive results of our different models on both KITTI and our own GTAV datasets. Through our results, we show that our method is the fastest available 3D object detection with accuracy results close to state-of-the-art methods on the KITTI road dataset. We further demonstrate that the pre-training process on our GTAV virtual dataset improves the accuracy on real datasets such as KITTI, thus allowing our method to obtain an even greater accuracy than state-of-the-art approaches with 16.16% 3D average precision on hard car detection with inference time of 11.1 ms/image on an RTX 3080 GPU.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-022-01202-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-022-01604-w,Sparse Black-Box Video Attack with Reinforcement Learning,International Journal of Computer Vision,10.1007/s11263-022-01604-w,Springer,2022-06-01,"Adversarial attacks on video recognition models have been explored recently. However, most existing works treat each video frame equally and ignore their temporal interactions. To overcome this drawback, a few methods try to select some key frames and then perform attacks based on them. Unfortunately, their selection strategy is independent of the attacking step, therefore the resulting performance is limited. Instead, we argue the frame selection phase is closely relevant with the attacking phase. The key frames should be adjusted according to the attacking results. For that, we formulate the black-box video attacks into a Reinforcement Learning (RL) framework. Specifically, the environment in RL is set as the recognition model, and the agent in RL plays the role of frame selecting. By continuously querying the recognition models and receiving the attacking feedback, the agent gradually adjusts its frame selection strategy and adversarial perturbations become smaller and smaller. We conduct a series of experiments with two mainstream video recognition models: C3D and LRCN on the public UCF-101 and HMDB-51 datasets. The results demonstrate that the proposed method can significantly reduce the adversarial perturbations with efficient query times.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-022-01604-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10586-021-03350-z,Artificial intelligent system for multimedia services in smart home environments,Cluster Computing,10.1007/s10586-021-03350-z,Springer,2022-06-01,"Internet of Things (IoT) has introduced new applications and environments. Smart Home provides new ways of communication and service consumption. In addition, Artificial Intelligence (AI) and deep learning have improved different services and tasks by automatizing them. In this field, reinforcement learning (RL) provides an unsupervised way to learn from the environment. In this paper, a new intelligent system based on RL and deep learning is proposed for Smart Home environments to guarantee good levels of QoE, focused on multimedia services. This system is aimed to reduce the impact on user experience when the classifying system achieves a low accuracy. The experiments performed show that the deep learning model proposed achieves better accuracy than the KNN algorithm and that the RL system increases the QoE of the user up to 3.8 on a scale of 10.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10586-021-03350-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00466-022-02152-3,A digital-twin and machine-learning framework for precise heat and energy management of data-centers,Computational Mechanics,10.1007/s00466-022-02152-3,Springer,2022-06-01,"The massive growth in data-centers has led to increased interest and regulations for management of waste heat and its utilization. This work seeks to develop a combined Digital-Twin and Machine-Learning framework to optimize such systems by controlling both the ventilation and the cooling of the bases of data units/processors in the system. This framework ascertains optimal cooling strategies to deliver a target temperature in the system using a minimum amount of energy. A model problem is constructed for a data-center, where the design variables are the flow rates and air-cooling at multiple ventilation ports and ground-level conduction-based base-cooling of processors. A thermo-fluid model, based on the Navier–Stokes equations and the first law of thermodynamics, for the data-center is constructed and a rapid, stencil-based, iterative solution method is developed. This is then combined with a genomic-based machine-learning algorithm to develop a digital-twin (digital-replica) of the system that can run in real-time or faster than the actual physical system, making it suitable as either a design tool or an adaptive controller. Numerical examples are provided to illustrate the framework.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00466-022-02152-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10462-021-10104-1,A comprehensive review of the video-to-text problem,Artificial Intelligence Review,10.1007/s10462-021-10104-1,Springer,2022-06-01,"Research in the Vision and Language area encompasses challenging topics that seek to connect visual and textual information. When the visual information is related to videos, this takes us into Video-Text Research, which includes several challenging tasks such as video question answering, video summarization with natural language, and video-to-text and text-to-video conversion. This paper reviews the video-to-text problem, in which the goal is to associate an input video with its textual description. This association can be mainly made by retrieving the most relevant descriptions from a corpus or generating a new one given a context video. These two ways represent essential tasks for Computer Vision and Natural Language Processing communities, called text retrieval from video task and video captioning/description task . These two tasks are substantially more complex than predicting or retrieving a single sentence from an image. The spatiotemporal information present in videos introduces diversity and complexity regarding the visual content and the structure of associated language descriptions. This review categorizes and describes the state-of-the-art techniques for the video-to-text problem. It covers the main video-to-text methods and the ways to evaluate their performance. We analyze twenty-six benchmark datasets, showing their drawbacks and strengths for the problem requirements. We also show the progress that researchers have made on each dataset, we cover the challenges in the field, and we discuss future research directions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10462-021-10104-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1134/S0005117922060042,Method for Reducing the Feature Space Dimension in Speech Emotion Recognition Using Convolutional Neural Networks,Automation and Remote Control,10.1134/S0005117922060042,Springer,2022-06-01,"Abstract We consider the architectures of convolutional neural networks used to assess the emotional state of a person by their speech. The problem of increasing the efficiency of emotion recognition by reducing the computational complexity of this process is solved. To this end, we propose a method transforming the input data into a form suitable for machine learning algorithms.",http://link.springer.com/openurl/fulltext?id=doi:10.1134/S0005117922060042,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-020-05310-x,Multiclass classification of nutrients deficiency of apple using deep neural network,Neural Computing and Applications,10.1007/s00521-020-05310-x,Springer,2022-06-01,"Agriculture industry is the foundation of Indian economy where quality fruit production plays an important role. Apple or pome fruits are always in demand because of rich nutrients in it. Hence, to analyze and recognize the nutrients deficiency in fruits, a deep neural-based model is being proposed. This model automatically classifies and recognizes the type of deficiency present in apple. In this paper, a database has been created for four major types of nutrients deficiency in apples and used for training and validation of the proposed deep convolutional network. The model is tuned with k -fold cross-validation. The hyper-parameters such as epoch are set at 100 and batch size kept at 5. Finally, the model is tested with the testing data and achieved an average accuracy of 98.24% with k -fold cross-validation set to 15. The model accuracy depends on the hyper-parameters. The process of features optimization reduces the risk of overfitting of the model. Hence, careful selection of hyper-parameters is important for the convergence of cost function to the global minima that results in minimum misclassification.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-020-05310-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-021-02846-w,FATALRead - Fooling visual speech recognition models,Applied Intelligence,10.1007/s10489-021-02846-w,Springer,2022-06-01,"Visual speech recognition is essential in understanding speech in several real-world applications such as surveillance systems and aiding differently-abled. It proliferates the research in the realm of visual speech recognition, also known as Automatic Lip Reading (ALR). In recent years, Deep Learning (DL) methods are being utilised for developing ALR systems. DL models tend to be vulnerable to adversarial attacks. Studying these attacks creates new research directions in designing robust DL systems. Existing attacks on images and videos classification models are not directly applicable to ALR systems. Since the ALR systems encompass temporal information, attacking these systems is comparatively more challenging and strenuous than attacking image classification models. Similarly, compared to other video classification tasks, the region-of-interest is smaller in the case of ALR systems. Despite these factors, our proposed method, Fooling AuTomAtic Lip Reading (FATALRead), can successfully perform adversarial attacks on state-of-the-art ALR systems. To the best of our knowledge, we are the first to successfully fool ALR systems for the word recognition task. We further demonstrate that the success of the attack is increased by incorporating logits instead of probabilities in the loss function. Our extensive experiments on a publicly available dataset, show that our attack successfully circumvents the well-known transformation based defences.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02846-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40747-022-00665-1,Computational intelligence in processing of speech acoustics: a survey,Complex & Intelligent Systems,10.1007/s40747-022-00665-1,Springer,2022-06-01,"Speech recognition of a language is a key area in the field of pattern recognition. This paper presents a comprehensive survey on the speech recognition techniques for non-Indian and Indian languages, and compiled some of the computational models used for processing speech acoustics. An immense number of frameworks are available for speech processing and recognition for languages persisting around the globe. However, a limited number of automatic speech recognition systems are available for commercial use. The gap between the languages being spoken around the globe and the technical support available to these languages are very few. This paper examined major challenges for speech recognition for different languages. Analysis of the literature shows that lack of standard databases availability of minority languages hinder the research recognition research across the globe. When compared with non-Indian languages, the research on speech recognition of Indian languages (except Hindi) has not achieved the expected milestone yet. Combination of MFCC and DNN–HMM classifier is most commonly used system for developing ASR minority languages, whereas in some of the majority languages, researchers are using much advance algorithms of DNN. It has also been observed that the research in this field is quite thin and still more research needs to be carried out, particularly in the case of minority languages.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40747-022-00665-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-021-09952-7,Automatic annotation method of VR speech corpus based on artificial intelligence,International Journal of Speech Technology,10.1007/s10772-021-09952-7,Springer,2022-06-01,"With the rapid development of the Internet and artificial intelligence, the demand for data annotation becomes more and more urgent. In order to meet the needs of data annotation, the automatic annotation method of VR speech corpus based on artificial intelligence is designed. The existing annotation methods use word, excel and other text forms, or develop a special web page system to organize the annotation corpus. Then the taggers annotate the corpus in the form of text or web pages. The problems of the existing annotation methods are as follows: the taggers do their own things, label their own data, and there are differences in the annotation standards among taggers; tagging and R&D process are independent of each other and cannot be developed cooperatively; for the labeling errors of the labeling personnel, either they can not be corrected, or they can only be corrected by secondary labeling. These problems limit the efficiency and quality of labeling and R&D.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-021-09952-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-022-09976-7,Soft-computation based speech recognition system for Sylheti language,International Journal of Speech Technology,10.1007/s10772-022-09976-7,Springer,2022-06-01,"The encouraging trend of usage of human machine interfaces in diverse areas has driven the evolution of Automatic Speech Recognition (ASR) systems during last two decades. Lately, the inclination has been towards the use of machine learning techniques for under-resourced human languages primarily to focus on designing of voice activated digital tool for a sizable portion of computer illiterate speakers. A vast majority of the works in this field have employed shallow models like conventional Artificial Neural Network and Hidden Markov Model in combination with Mel Frequency Cepstral Coefficients and other relevant features for the applications of speech recognition systems. Although these shallow models are found effective, but to minimize human intervention from the approach and also to yield the better system performance, recent research has focused to incorporate deep learning models for ASR applications especially for under-resourced languages. Sylheti language, a member of Indo-Aryan language group, is an under resourced language which has more than 10 million Sylheti speakers living across the world mostly in India and Bangladesh. Focusing on the need of an ASR model for Sylheti, this work aims to design a robust ASR model for an under resourced language Sylheti by employing state-of-the-art deep learning technique Convolutional Neural Network (CNN). To find out the best and suitable ASR model for Sylheti, certain ASR approaches are formulated and trained by Sylheti isolated and connected words. The specially configured ASR model based on CNN is trained with clean, and noisy speech data which are necessary for training and making the system robust. Thereafter, a comparative analysis is presented by configuring the ASR model by some shallow models like Feed-forward neural network, Recurrent neural Network, Hidden Markov model and Time Delay neural Network. Experimental results indicate that the proposed CNN based ASR system works well for Sylheti language and the performance accuracy obtained by the system is found to be satisfactory despite the system demonstrating certain training latency.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-022-09976-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40313-021-00868-w,Automated Violence Detection in Video Crowd Using Spider Monkey-Grasshopper Optimization Oriented Optimal Feature Selection and Deep Neural Network,"Journal of Control, Automation and Electrical Systems",10.1007/s40313-021-00868-w,Springer,2022-06-01,"There is an increasing demand for automated violence detection with a wide range of threats in society and less manpower to monitor them. Especially, detecting violence in crowded scenes is challenging because of the rapid movement, overlapping features due to occlusion, and cluttered backgrounds. This paper plans to implement the enhanced model for video violence detection with the aid of intelligent approaches. The proposed model covers different phases like (a) pre-processing, (b) feature extraction, (c) optimal feature selection, and (d) classification. Initially, the video frames are split, and the pre-processing of the frames is carried out by the Gaussian filter. Next, the feature extraction procedure is undergone, in which the Motion Boundary Scale Invariant Feature Transform (MoBSIFT), Histogram of oriented Gradients (HoG), and Motion Weber Local Descriptor (MoWLD) are used. Further, the optimal feature selection is adopted. The hybridization of two well-performing algorithms like Spider Monkey Optimization (SMO), and Grasshopper Optimisation Algorithm (GOA), namely Spider Monkey-Grasshopper Optimization algorithm (SM-GOA) is used for optimal feature selection with the intention of solving a multi-objective function. Then, the classification of violence and non-violence video frames is done by the Deep Neural Network (DNN), in which the training algorithm is enhanced by the same SM-GOA. The proposed SM-GOA-based DNN had achieved less False Positive Rate (FPR), False Negative Rate (FNR), and False Detection Rate (FDR) values compared to the existing methods proving less variance to the negative performance and thus has shown good overall performance on the violence flow dataset. Experimental results on diverse benchmark datasets have demonstrated the superior performance of the proposed approach over the state-of-the-art.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40313-021-00868-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11034-4,A hybridized modified densenet deep architecture with CLAHE algorithm for humpback whale identification and recognition,Multimedia Tools and Applications,10.1007/s11042-021-11034-4,Springer,2022-06-01,"Authenticity in multimedia information retrieval system is one such parameter which is of utmost in demand. One of such parameters is the biometric based authentication system, as seen in case of human the system proves its importance in almost each and every field. But in case of animals it becomes quite complex to recognize them using unique biometric parameter. This paper presents a modified 10 layered DenseNet deep learning framework which is implemented for the identification of Humpback whale, in an online challenge conducted by the kaggle, using biometric parameter. The proposed dense model minimizes the traditional limitation of vanishing gradient problem and outperforms as compared with other methods exists in the literature. This paper also proposed an hand free flow algorithm for ROI segmentation. In addition, the proposed methodology also uses CLAHE as a preprocessing method and focused on the feature re-usability with an optimized number of parameters used for recognition. During the experimentation, validation of the proposed model is found satisfactory with an accuracy of 93.21%. The test set results are provided by the kaggle with an AUC-ROC result over public and private scoreboard with an accuracy of 94.47% and 92.54% respectively. The comparative result with other deep methodologies suggests that the highest accuracy gained is 95.55% by Se-ResNet50 by some other learderboard and proposed model is found at second place.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11034-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12510-1,Survey for person re-identification based on coarse-to-fine feature learning,Multimedia Tools and Applications,10.1007/s11042-022-12510-1,Springer,2022-06-01,"Person re-identification (Re-ID), aiming to retrieve interested people through multiple non-overlapping cameras, has caused concerns in pattern recognition communities and computer vision in recent years. With the continuous promotion of deep learning, the research on person Re-ID is more and more extensive. In this paper, we conduct a comprehensive review of the advanced methods and divide them into three categories from coarse to fine: (1) global-based methods, which are based on whole images to obtain discriminative features; (2) part-based methods, which focus on image regions to extract detailed information; (3) multiple granularities-based methods, which combine advantages of the above two categories. For each category, we further classify it according to popular research tools. Then, we give the evaluation of some typical models on a set of benchmark datasets and compare them in detail. We also introduce some widely used training tricks. The methods mentioned in this paper were published in 2011-2021. By discussing their advantages and limitations, we provide a reference for future works.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12510-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s41095-021-0238-4,Joint 3D facial shape reconstruction and texture completion from a single image,Computational Visual Media,10.1007/s41095-021-0238-4,Springer,2022-06-01,"Recent years have witnessed significant progress in image-based 3D face reconstruction using deep convolutional neural networks. However, current reconstruction methods often perform improperly in self-occluded regions and can lead to inaccurate correspondences between a 2D input image and a 3D face template, hindering use in real applications. To address these problems, we propose a deep shape reconstruction and texture completion network, SRTC-Net, which jointly reconstructs 3D facial geometry and completes texture with correspondences from a single input face image. In SRTC-Net, we leverage the geometric cues from completed 3D texture to reconstruct detailed structures of 3D shapes. The SRTC-Net pipeline has three stages. The first introduces a correspondence network to identify pixel-wise correspondence between the input 2D image and a 3D template model, and transfers the input 2D image to a U - V texture map. Then we complete the invisible and occluded areas in the U - V texture map using an inpainting network. To get the 3D facial geometries, we predict coarse shape ( U - V position maps) from the segmented face from the correspondence network using a shape network, and then refine the 3D coarse shape by regressing the U - V displacement map from the completed U - V texture map in a pixel-to-pixel way. We examine our methods on 3D reconstruction tasks as well as face frontalization and pose invariant face recognition tasks, using both in-the-lab datasets (MICC, MultiPIE) and in-the-wild datasets (CFP). The qualitative and quantitative results demonstrate the effectiveness of our methods on inferring 3D facial geometry and complete texture; they outperform or are comparable to the state-of-the-art.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41095-021-0238-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11053-022-10064-5,Constraints on the Geometry and Gold Distribution in the Black Reef Formation of South Africa Using 3D Reflection Seismic Data and Micro-X-ray Computed Tomography,Natural Resources Research,10.1007/s11053-022-10064-5,Springer,2022-06-01,"Geological and geophysical models are essential for developing reliable mine designs and mineral processing flowsheets. For mineral resource assessment, mine planning, and mineral processing, a deeper understanding of the orebody's features, geology, mineralogy, and variability is required. We investigated the gold-bearing Black Reef Formation in the West Rand and Carletonville goldfields of South Africa using approaches that are components of a transitional framework toward fully digitized mining: (1) high-resolution 3D reflection seismic data to model the orebody; (2) petrography to characterize Au and associated ore constituents (e.g., pyrite); and (3) 3D micro-X-ray computed tomography (µCT) and machine learning to determine mineral association and composition. Reflection seismic reveals that the Black Reef Formation is a planar horizon that dips < 10° and has a well-preserved and uneven paleotopography. Several large-scale faults and dikes (most dipping between 65° and 90°) crosscut the Black Reef Formation. Petrography reveals that gold is commonly associated with pyrite, implying that µCT can be used to assess gold grades using pyrite as a proxy. Moreover, we demonstrate that machine learning can be used to discriminate between pyrite and gold based on physical characteristics. The approaches in this study are intended to supplement rather than replace traditional methodologies. In this study, we demonstrated that they permit novel integration of micro-scale observations into macro-scale modeling, thus permitting better orebody assessment for exploration, resource estimation, mining, and metallurgical purposes. We envision that such integrated approaches will become a key component of future geometallurgical frameworks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11053-022-10064-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00784-022-04475-0,Personalized workflows in reconstructive dentistry—current possibilities and future opportunities,Clinical Oral Investigations,10.1007/s00784-022-04475-0,Springer,2022-06-01,"Objectives The increasing collection of health data coupled with continuous IT advances have enabled precision medicine with personalized workflows. Traditionally, dentistry has lagged behind general medicine in the integration of new technologies: So what is the status quo of precision dentistry? The primary focus of this review is to provide a current overview of personalized workflows in the discipline of reconstructive dentistry (prosthodontics) and to highlight the disruptive potential of novel technologies for dentistry; the possible impact on society is also critically discussed. Material and methods Narrative literature review. Results Narrative literature review. Conclusions In the near future, artificial intelligence (AI) will increase diagnostic accuracy, simplify treatment planning, and thus contribute to the development of personalized reconstructive workflows by analyzing e-health data to promote decision-making on an individual patient basis. Dental education will also benefit from AI systems for personalized curricula considering the individual students’ skills. Augmented reality (AR) will facilitate communication with patients and improve clinical workflows through the use of visually guided protocols. Tele-dentistry will enable opportunities for remote contact among dental professionals and facilitate remote patient consultations and post-treatment follow-up using digital devices. Finally, a personalized digital dental passport encoded using blockchain technology could enable prosthetic rehabilitation using 3D-printed dental biomaterials. Clinical significance Overall, AI can be seen as the door-opener and driving force for the evolution from evidence-based prosthodontics to personalized reconstructive dentistry encompassing a synoptic approach with prosthetic and implant workflows. Nevertheless, ethical concerns need to be solved and international guidelines for data management and computing power must be established prior to a widespread routine implementation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00784-022-04475-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11665-021-06535-0,Machine Learning-Based Mechanical Behavior Optimization of 3D Print Constructs Manufactured Via the FFF Process,Journal of Materials Engineering and Performance,10.1007/s11665-021-06535-0,Springer,2022-06-01,"Fused filament fabrication (FFF) is one of the fastest-growing additive manufacturing processes due to its low operational cost and the capability to rapidly construct prototypes with complex geometrical structures. Albeit the fact that FFF technology has been widely studied, the printing conditions that offer the optimum mechanical behavior exploiting numerical tools have not been systematically studied yet. The main goal of the study is to introduce machine learning-based models that predict the tensile strength of 3D printed parts and the development of an optimization tool, which infers the appropriate printing conditions according to the requirements of the user. The current paper deals with a computational and experimental study over the effect of specific process-related parameters and their impact on the mechanical behavior of an object manufactured via the FFF procedure. The data for the development of the predictive models and the numerical optimization module were acquired by manufacturing specimens with varying printing conditions. The results demonstrate that by adapting the suggested values on the printing parameters from the machine learning models and the optimization module, a remarkable enhancement on the mechanical properties of the printed parts can be achieved. Finally, the present work is the first to utilize machine learning-based regression models coupled with optimization techniques in order to improve the mechanical behavior of products manufactured via the FFF process.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11665-021-06535-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13735-022-00227-8,Anomaly detection using edge computing in video surveillance system: review,International Journal of Multimedia Information Retrieval,10.1007/s13735-022-00227-8,Springer,2022-06-01,"The current concept of smart cities influences urban planners and researchers to provide modern, secured and sustainable infrastructure and gives a decent quality of life to its residents. To fulfill this need, video surveillance cameras have been deployed to enhance the safety and well-being of the citizens. Despite technical developments in modern science, abnormal event detection in surveillance video systems is challenging and requires exhaustive human efforts. In this paper, we focus on evolution of anomaly detection followed by survey of various methodologies developed to detect anomalies in intelligent video surveillance. Further, we revisit the surveys on anomaly detection in the last decade. We then present a systematic categorization of methodologies for anomaly detection. As the notion of anomaly depends on context, we identify different objects-of-interest and publicly available datasets in anomaly detection. Since anomaly detection is a time-critical application of computer vision, we explore the anomaly detection using edge devices and approaches explicitly designed for them. The confluence of edge computing and anomaly detection for real-time and intelligent surveillance applications is also explored. Further, we discuss the challenges and opportunities involved in anomaly detection using the edge devices.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13735-022-00227-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-06936-9,Astrocytes mediate analogous memory in a multi-layer neuron–astrocyte network,Neural Computing and Applications,10.1007/s00521-022-06936-9,Springer,2022-06-01,"Modeling the neuronal processes underlying short-term working memory remains the focus of many theoretical studies in neuroscience. In this paper, we propose a mathematical model of a spiking neural network (SNN) which simulates the way a fragment of information is maintained as a robust activity pattern for several seconds and the way it completely disappears if no other stimuli are fed to the system. Such short-term memory traces are preserved due to the activation of astrocytes accompanying the SNN. The astrocytes exhibit calcium transients at a time scale of seconds. These transients further modulate the efficiency of synaptic transmission and, hence, the firing rate of neighboring neurons at diverse timescales through gliotransmitter release. We demonstrate how such transients continuously encode frequencies of neuronal discharges and provide robust short-term storage of analogous information. This kind of short-term memory can store relevant information for seconds and then completely forget it to avoid overlapping with forthcoming patterns. The SNN is inter-connected with the astrocytic layer by local inter-cellular diffusive connections. The astrocytes are activated only when the neighboring neurons fire synchronously, e.g., when an information pattern is loaded. For illustration, we took grayscale photographs of people’s faces where the shades of gray correspond to the level of applied current which stimulates the neurons. The astrocyte feedback modulates (facilitates) synaptic transmission by varying the frequency of neuronal firing. We show how arbitrary patterns can be loaded, then stored for a certain interval of time, and retrieved if the appropriate clue pattern is applied to the input.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-06936-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40860-021-00151-4,Underwater estimation of audio signal prediction using fruit fly algorithm and hybrid wavelet neural network,Journal of Reliable Intelligent Environments,10.1007/s40860-021-00151-4,Springer,2022-06-01,"Underwater acoustic signal is one of its most vital features in oceanography for predicting the local variants, helps to process the signal, and also assists to discover non-stationary signals. wavelet neural network (WNN) architecture acquires a huge benefit from the artificial neural network (ANN) and time-frequency localizing features of wavelet analysis (WA). However, the system could train using the ANN model, it has tendency of falling into local minima and generating convergence. A wavelet-aided neural network is integrated with a meta-heuristic optimizer named fruit fly algorithm (FFA) that aids our proposed system in which reduces the computational complexity and improves performance measures. The main objective of the proposed work is of modelling an optimized wavelet neural network for efficient measurement of the mean absolute error and the root mean square error. The simulation of the sound signals has been performed with the use of optimized neural networks that has predicted the unknown signal. The proposed model is assessed by comparing the outcomes with the complements of it, i.e. the traditional neural networks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40860-021-00151-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-022-09960-1,English language teaching based on big data analytics in augmentative and alternative communication system,International Journal of Speech Technology,10.1007/s10772-022-09960-1,Springer,2022-06-01,"The tremendous growth in the education sector has given rise to several developments focused on teaching and training. The Augmentative and Alternative Communication (AAC) method has helped people with neurological disabilities to learn for years. AAC faces significant challenges that affect the level of language learning skills, mainly English. Artificial intelligence on AAC mechanism for leveling the English language because it trains to have dataset processing. The system processing trains and tests datasets by intelligent thought values to enough output of English communication. In this paper, Big Data Integrated Artificial Intelligence for AAC (BDIAI-AAC) has been proposed to train people in English with neural disorders. Here, BDIAI-AAC is speech recognition trained with a network of animated videos. Artificial Intelligence (AI) trained network works on three layers. The input layer is the speech recognition model, which converts speech to string said by the educator. The hidden layer processes the string data as well as matches with the corresponding video animation. Artificial intelligence works on three layers of conversions like input layer, hidden layer, and output layer. The hidden layer verifies to produce a matched string as predefined dataset values on video animation. The hidden process comprises image processing, recurrent networks, and memory unit for storing data. Finally, the Output layer displays the animated video along with a sentence using AAC. Thus, English sentences are converted into respective videos or animations using AI-trained networks and AAC models. The comparative analysis of the proposed method BDIAI-AAC with technological advancements has shown that the method reaches 98.01% of word recognition rate and 97.89% of prediction rate, high efficiency (95.34%), performance (96.45%), accuracy (95.14%), stimulus (94.2%), disorder identification rate (91.12%) when compared to other methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-022-09960-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00500-022-07093-w,A joint method for Chinese word segmentation and part-of-speech labeling based on deep neural network,Soft Computing,10.1007/s00500-022-07093-w,Springer,2022-06-01,"Aiming at the sequential tasks of Chinese word segmentation and part-of-speech labeling, this paper proposes a parallel model for word segmentation and part-of-speech labeling that combines BERT model, bidirectional long-short memory model, and conditional random field model, Markov family model (MFM) or Tree Probability (TLP). In part-of-speech labeling combined with MFM or TLP, the part-of-speech of the current word is not only related to the part-of-speech of the previous word, but also related to the current word itself. The use of the joint method helps to use part-of-speech information to achieve word segmentation, and organically combining the two is beneficial to eliminate ambiguity and improve the accuracy of part-of-speech labeling or word segmentation tasks. Experimental data shows that the joint model for part-of-speech labeling and Chinese word segmentation proposed in this paper can significantly enhances the precision of Chinese word segmentation and the accuracy of part-of-speech labeling.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-022-07093-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40477-021-00560-4,Impact of scan quality on AI assessment of hip dysplasia ultrasound,Journal of Ultrasound,10.1007/s40477-021-00560-4,Springer,2022-06-01,"Aims Early diagnosis of developmental dysplasia of the hip (DDH) using ultrasound (US) is safe, effective and inexpensive, but requires high-quality scans. The effect of scan quality on diagnostic accuracy is not well understood, especially as artificial intelligence (AI) begins to automate such diagnosis. In this paper, we developed a 10-point scoring system for reporting DDH US scan quality, evaluated its inter-rater agreement and examined its effect on automated assessment by an AI system—MEDO-Hip. Methods Scoring was based on iliac wing straightness and angulation; visibility of labrum, os ischium and femoral head; motion; and other artifacts. Four readers from novice to expert separately scored the quality of 107 scans with this 10-point scale and with holistic grading on a scale of 1–5. MEDO-Hip interpreted the same scans, providing a diagnostic category or identifying the scan as uninterpretable. Results Inter-rater agreement for the 10-point scale was significantly higher than holistic scoring ICC 0.68 vs 0.93, p  < 0.05. Inter-rater agreement on the categorisation of individual features, by Cohen’s kappa, was highest for os ischium (0.67 ± 0.06), femoral head (0.65 ± 0.07) and iliac wing (0.49 ± 0.12) indices, and lower for the presence of labrum (0.21 ± 0.19). MEDO-Hip interpreted all images of a quality > 7 and flagged 13/107 as uninterpretable. These were low-quality images (3 ± 1.2 vs. 7 ± 1.8 in others, p  < 0.05), with poor visualization of the os ischium and noticeable motion. AI accuracy in cases with quality scores <  = 7 was 57% vs. 89% on other cases, p  < 0.01. Conclusion This study validates that our scoring system reliably characterises scan quality, and identifies cases likely to be misinterpreted by AI. This could lead to more accurate use of AI in DDH diagnosis by flagging low-quality scans likely to provide poor diagnosis up front.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40477-021-00560-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12065-020-00435-3,Video super-resolution using hybrid support vector regression–Actor Critic Neural Network model,Evolutionary Intelligence,10.1007/s12065-020-00435-3,Springer,2022-06-01,"The aim of video super-resolution (SR) is to produce a high-resolution (HR) video frame from numerous successive low-resolution (LR) frames. Even though there are a large number of techniques employed for the video SR, all these existing techniques face a hectic challenge at various conditions. Thus, this paper proposes an effective video resolution strategy using the hybrid Support vector regression–Actor Critic Neural Network (SVR–ACNN) model for video enhancement. The SR images formed using the individual SVR model and ACNN are integrated using the weighted average concept. The ACNN is tuned optimally by the proposed Fractional-based Sine Cosine algorithm (F-SCA), which is responsible for the global optimal convergence. The experimentation of the proposed method utilizes three videos taken from the Cambridge-driving Labeled Video Database (CamVid), and the results are analyzed for three scaling factors. The results prove that the proposed model offers a better SR image with a better PSNR, SSIM, and SDME of 33.6447 dB, 0.9398, and 45.2779, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12065-020-00435-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-022-03603-3,Learning a spatial-temporal symmetry network for video super-resolution,Applied Intelligence,10.1007/s10489-022-03603-3,Springer,2022-06-01,"The video super-resolution (VSR) method is designed to estimate and restore high-resolution (HR) sequences from low-resolution (LR) input. For the past few years, many VSR methods with machine learning have been proposed that combine both the convolutional neural network (CNN) and motion compensation. Most mainstream approaches are based on optical flow or deformation convolution, and both need accurate estimates for motion compensation. However, most previous methods have not been able to fully utilize the spatial-temporal symmetrical information from input sequences. Moreover, much computation is consumed by aligning every neighbouring frame to the reference frame separately. Furthermore, many methods reconstruct HR results on only a single scale, which limits the reconstruction accuracy of the network and its performance in complex scenes. In this study, we propose a spatial-temporal symmetry network (STSN) to solve the above deficiencies. STSN includes four parts: prefusion, alignment, postfusion and reconstruction. First, a two-stage fusion strategy is applied to reduce the computation consumption of the network. Furthermore, ConvGRU is utilized in the prefusion module, the redundant features between neighbouring frames are eliminated, and several neighbouring frames are fused and condensed into two parts. To generate accurate offset maps, we present a spatial-temporal symmetry attention block (STSAB). This component exploits the symmetry of spatial-temporal combined spatial attention. In the reconstruction module, we propose an SR multiscale residual block (SR-MSRB) to enhance reconstruction performance. Abundant experimental results that test several datasets show that our method possesses better effects and efficiency in both quantitative and qualitative measurement indices compared with state-of-the-art methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-022-03603-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-021-02938-7,High capacity speech steganography for the G723.1 coder based on quantised line spectral pairs interpolation and CNN auto-encoding,Applied Intelligence,10.1007/s10489-021-02938-7,Springer,2022-06-01,"In this paper, a novel steganographic method for Voice over IP applications —called Steganography-based Interpolation and Auto-Encoding (SIAE)— is proposed. The aim of the proposed scheme is to securely transmit a secret speech hidden within another (cover) speech coded with a G723.1 coder. SIAE embeds the steganograms in four interpolated and quantised line spectral pairs (QLSP) vectors. In order to minimize the changes in the cover speech, the proposed approach uses a 1D auto-encoder to compress the payload, and this scheme only requires embedding eight bits in about 30% of the packets. At the receiver side, the secret data can be successfully expanded to its original size upon decoding. This represents a significant reduction in the number of modified bits compared to state-of-the-art schemes, and results in enhanced undetectability and decreased steganographic quality loss. The results show that the proposed auto-encoder scheme has a very high performance since it can compress the embedded data up to 80 times from its original size, leading to a steganographic capacity that exceeds one kilobit per second (kpbs). In terms of imperceptibility, which is a relevant property for speech-in-speech steganography, the proposed SIAE method entails a very imperceptible distortion, with an average steganographic quality loss not greater than 0.19 in terms of mean opinion scores (MOS). Last but not least, the proposed method evades steganalysis specifically targeted at speech steganography. The tested steganalytic methods fail in detecting the steganographic content produced with the proposed SIAE method, yielding classification results that are indistinguishable from random guessing.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02938-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-021-02896-0,A semi-automated system for person re-identification adaptation to cross-outfit and cross-posture scenarios,Applied Intelligence,10.1007/s10489-021-02896-0,Springer,2022-06-01,"Person re-identification (ReID) algorithms are often trained on multi-camera snapshots of individuals taken on the same day, wearing the same outfits. Models trained with such protocols often fail in many long-term, indoor applications where person matching must be done across days, necessitating that algorithms be able to adapt to changing clothing and body postures. This study presents a simple, yet effective, system to overcome this challenge in realistic settings. We collected a new dataset capturing the natural variations of office worker appearances across days. To teach a ReID algorithm to adapt, we designed a semi-automated identity labeling system that requires only a small set of identification inputs from human labelers. The system utilized instance segmentation algorithms to detect people and one-shot video segmentation algorithms to track individuals across frames. Identified footages are then fed into the image repository to continually fine-tune the ReID network. These experiments demonstrate the applicability of our proposed method in helping the ReID algorithm overcome the challenges of varied clothing and postures. Our system improves the performance (measured by mAP) compared to pre-trained benchmark by 2.46% for the standard ReID condition, by 18.19% for cross-outfit re-identification, by 22.94% for cross-posture re-identification, and by 19.17% for the cross-posture and cross-outfit setting. As such, we anticipate this method may be beneficial towards the multitude of applications that utilize machine vision to automatically recognize human subjects.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02896-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1134/S1054661822020067,Detection of Appearance and Behavior Anomalies in Stationary Camera Videos Using Convolutional Neural Networks,Pattern Recognition and Image Analysis,10.1134/S1054661822020067,Springer,2022-06-01,"Abstract The automatic detection and tracking of appearance and behavior anomalies in video surveillance systems is one of the promising areas for the development and implementation of artificial intelligence. In this paper, we present a formalization of these problems. Based on the proposed generalization, a detection and tracking algorithm that uses the tracking-by-detection paradigm and convolutional neural networks (CNNs) is developed. At the first stage, people are detected using the YOLOv5 CNN and are marked with bounding boxes. Then, their faces in the selected regions are detected and the presence or absence of face masks is determined. Our approach to face-mask detection also uses YOLOv5 as a detector and classifier. For this problem, we generate a training dataset by combining the Kaggle dataset and a modified Wider Face dataset, in which face masks were superimposed on half of the images. To ensure a high accuracy of tracking and trajectory construction, the CNN features of the images are included in a composite descriptor, which also contains geometric and color features, to describe each person detected in the current frame and compare this person with all people detected in the next frame. The results of the experiments are presented, including some examples of frames from processed video sequences with visualized trajectories for loitering and falls.",http://link.springer.com/openurl/fulltext?id=doi:10.1134/S1054661822020067,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07410-2,Bidirectional parallel echo state network for speech emotion recognition,Neural Computing and Applications,10.1007/s00521-022-07410-2,Springer,2022-05-31,"Speech is an effective way for communicating and exchanging complex information between humans. Speech signal has involved a great attention in human-computer interaction. Therefore, emotion recognition from speech has become a hot research topic in the field of interacting machines with humans. In this paper, we proposed a novel speech emotion recognition system by adopting multivariate time series handcrafted feature representation from speech signals. Bidirectional echo state network with two parallel reservoir layers has been applied to capture additional independent information. The parallel reservoirs produce multiple representations for each direction from the bidirectional data with two stages of concatenation. The sparse random projection approach has been adopted to reduce the high-dimensional sparse output for each direction separately from both reservoirs. Random over-sampling and random under-sampling methods are used to overcome the imbalanced nature of the used speech emotion datasets. The performance of the proposed parallel ESN model is evaluated from the speaker-independent experiments on EMO-DB, SAVEE, RAVDESS, and FAU Aibo datasets. The results show that the proposed SER model is superior to the single reservoir and the state-of-the-art studies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07410-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12065-022-00729-8,Adult content image recognition by Boltzmann machine limited and deep learning,Evolutionary Intelligence,10.1007/s12065-022-00729-8,Springer,2022-05-30,"Adult content images have a detrimental effect on Internet users, a significant number of whom are minors. Therefore, it is essential to control and detecting adult content images using multimedia processing and computer vision techniques. Previous studies have typically focused on manual-engineered visual features that may be difficult to detect and analyze. This paper presents a new model that employs deep convolutional neural networks within a Gaussian-Bernoulli limited-time, for adult content image recognition of a wide variety in a precise and effective manner. There are various layers within Convolutional Neural Networks for feature extraction and classification. Gaussian-Bernoulli limited-time was used for feature extraction to describe the images, and these features were summarized using the Boltzmann machine limited in the feature summary phase. The benefit of such an approach is convenience in carrying out feature extraction. Additionally, when tested on the most modern criterion dataset, this finding is believed to be more precise compared to other state-of-the-art approaches. The results obtained prove that the proposed approach leads to a higher efficiency.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12065-022-00729-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-022-02681-5,Video-based assessment of intraoperative surgical skill,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-022-02681-5,Springer,2022-05-30,"Purpose Surgeons’ skill in the operating room is a major determinant of patient outcomes. Assessment of surgeons’ skill is necessary to improve patient outcomes and quality of care through surgical training and coaching. Methods for video-based assessment of surgical skill can provide objective and efficient tools for surgeons. Our work introduces a new method based on attention mechanisms and provides a comprehensive comparative analysis of state-of-the-art methods for video-based assessment of surgical skill in the operating room. Methods Using a dataset of 99 videos of capsulorhexis, a critical step in cataract surgery, we evaluated image feature-based methods and two deep learning methods to assess skill using RGB videos. In the first method, we predict instrument tips as keypoints and predict surgical skill using temporal convolutional neural networks. In the second method, we propose a frame-wise encoder (2D convolutional neural network) followed by a temporal model (recurrent neural network), both of which are augmented by visual attention mechanisms. We computed the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, and predictive values through fivefold cross-validation. Results To classify a binary skill label (expert vs. novice), the range of AUC estimates was 0.49 (95% confidence interval; CI = 0.37 to 0.60) to 0.76 (95% CI = 0.66 to 0.85) for image feature-based methods. The sensitivity and specificity were consistently high for none of the methods. For the deep learning methods, the AUC was 0.79 (95% CI = 0.70 to 0.88) using keypoints alone, 0.78 (95% CI = 0.69 to 0.88) and 0.75 (95% CI = 0.65 to 0.85) with and without attention mechanisms, respectively. Conclusion Deep learning methods are necessary for video-based assessment of surgical skill in the operating room. Attention mechanisms improved discrimination ability of the network. Our findings should be evaluated for external validity in other datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-022-02681-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s41870-022-00996-9,Language-independent hyperparameter optimization based speech emotion recognition system,International Journal of Information Technology,10.1007/s41870-022-00996-9,Springer,2022-05-29,"Speech emotion recognition is challenging due to substantially overlapping regions of emotions. Extracting desired features that influence emotions in a speech and categorizing these emotions is a tedious task. We intend to develop an effective and robust speech emotion recognition system capable of classifying ambiguous and overlapping emotions through this manuscript. Three feature sets Spectral, Prosodic, and Discrete Wavelet Transform are extracted and further processed to reduce the required combination of features. The use of hyper-parameter optimization in the machine learning model has been done to tune the support vector machine classifier parameter for the Speech emotion recognition system. The suggested model is also verified with two different language datasets: ‘SAVEE’ and ‘EmoDB’ resulting in a language-independent emotion recognition system from speech. The performance result achieved by employing the proposed technique in EmoDB with 535 samples and SAVEE with 480 samples in seven different emotion types is 90.02% and 71.66%, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41870-022-00996-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12008-022-00926-w,Autonomous lemon grading system by using machine learning and traditional image processing,International Journal on Interactive Design and Manufacturing (IJIDeM),10.1007/s12008-022-00926-w,Springer,2022-05-29,"Fruits like lemons are always harvested in huge quantities. However, currently, the assessment of lemon quality as best, good and bad, which based on the color change in the surface of the lemon, is still being used manually by humans. This leads to low productivity as well as uneven grading quality. This study will present an effective method that can help the classifying has high accuracy and fast grading time that could meet industrial needs by using a conveyor belt which makes lemons move and rotate at the same time and machine learning, traditional image processing. By moving and rotating continuously on the conveyor belt, the machine learning algorithm using Yolov4 network and traditional image processing will examine the whole surface of the lemon to classify the current state of the lemon. This methodology has the advantage of being able to check the entire lemon surface and the number of lemons to be classified at the same time is not affected too much by the image processing time. Beside the number of image collection for YoloV4 also reduced significantly. These advantage are very important in real implement at the factory. The efficiency of the system will be proven through series of experiments on real machine by comparing the performance of the current machine learning, traditional method with the proposed method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12008-022-00926-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s41403-022-00337-z,Virtual Sinter^®: Digital Twin for Integrated Sinter Plants,Transactions of the Indian National Academy of Engineering,10.1007/s41403-022-00337-z,Springer,2022-05-28,"Sinter Plants, that produce iron ore sinter, are important units in an integrated steel plant. They face several operational challenges due to frequent changes in input raw materials, the inability to instrument key equipment in the plant, and the absence of real-time sinter quality measurement. To address these challenges, Virtual Sinter^®, a digital twin of the integrated sinter plant is developed and presented here. Virtual Sinter^® is a software tool that utilizes physics-based and data-driven models to simulate the operation of all the unit operations in the sinter plant and predict sinter quality in real time using raw material data and process parameters as inputs. The physics-based models in Virtual Sinter^® also act as soft sensors and estimate parameters such as granule size distributions, green bed voidage, and sinter bed temperatures that are difficult to measure online, thereby bringing additional visibility into the operation of key units. Virtual Sinter^® also monitors and detects deviations in key process and quality parameters and diagnoses the root causes of such deviations in real time to enable operators take corrective actions at the right time. It also identifies, through process optimization, optimal settings for improving plant productivity, sinter yield and quality, and minimizing fuel consumption, emissions, and overall cost of operation. Virtual Sinter^® can also be utilized for designing raw material blends and as an operator training simulator.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41403-022-00337-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-022-03886-x,A computational framework to support the treatment of bedsores during COVID-19 diffusion,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-022-03886-x,Springer,2022-05-27,"The treatment of pressure ulcers, also known as bedsores, is a complex process that requires to employ specialized field workforce assisting patients in their houses. In the period of COVID-19 or during any other non-trivial emergency, reaching the patients in their own house is impossible. Therefore, as well as in the other sectors, the adoption of digital technologies is invoked to solve, or at least mitigate, the problem. In particular, during the COVID-19, the social distances should be maintained in order to decrease the risk of contagion. The Project Health Management Systems proposes a complete framework, based on Deep Learning, Augmented Reality. Pattern Matching, Image Segmentation and Edge Detection approaches, to support the treatment of bedsores without increasing the risk of contagion, i.e., improving the remote aiding of specialized operators and physicians and involving inexperienced familiars in the process.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-022-03886-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-12503-y,Integration of thermal imaging and neural networks for mechanical strength analysis and fracture prediction in 3D-printed plastic parts,Scientific Reports,10.1038/s41598-022-12503-y,Nature,2022-05-27,"Additive manufacturing demonstrates tremendous progress and is expected to play an important role in the creation of construction materials and final products. Contactless (remote) mechanical testing of the materials and 3D printed parts is a critical limitation since the amount of collected data and corresponding structure/strength correlations need to be acquired. In this work, an efficient approach for coupling mechanical tests with thermographic analysis is described. Experiments were performed to find relationships between mechanical and thermographic data. Mechanical tests of 3D-printed samples were carried out on a universal testing machine, and the fixation of thermal changes during testing was performed with a thermal imaging camera. As a proof of concept for the use of machine learning as a method for data analysis, a neural network for fracture prediction was constructed. Analysis of the measured data led to the development of thermographic markers to enhance the thermal properties of the materials. A combination of artificial intelligence with contactless nondestructive thermal analysis opens new opportunities for the remote supervision of materials and constructions.",https://www.nature.com/articles/s41598-022-12503-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11265-022-01772-5,Efficient Neuromorphic Signal Processing with Resonator Neurons,Journal of Signal Processing Systems,10.1007/s11265-022-01772-5,Springer,2022-05-26,"The biologically inspired spiking neurons used in neuromorphic computing are nonlinear filters with dynamic state variables, which is distinct from the stateless neuron models used in deep learning. The new version of Intel’s neuromorphic research processor, Loihi 2, supports an extended range of stateful spiking neuron models with programmable dynamics. Here, we showcase advanced neuron models that can be used to efficiently process streaming data in simulation experiments on emulated Loihi 2 hardware. In one example, Resonate-and-Fire (RF) neurons are used to compute the Short Time Fourier Transform (STFT) with similar computational complexity but 47x less output bandwidth than the conventional STFT. In another example, we describe an algorithm for optical flow estimation using spatiotemporal RF neurons that requires over 90x fewer operations than a conventional DNN-based solution. We also demonstrate backpropagation methods to train non-linear spiking RF neurons for audio classification tasks, suitable for efficient execution on Loihi 2. We conclude with another application of nonlinear filtering showing a cascade of Hopf resonators exhibiting computational properties seen in the cochlea, such as self-normalization. Taken together, this work presents new techniques for an efficient spike-based spectrogram encoder that can be used for signal processing applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11265-022-01772-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40747-022-00764-z,Diagnosis of rotor demagnetization and eccentricity faults for IPMSM based on deep CNN and image recognition,Complex & Intelligent Systems,10.1007/s40747-022-00764-z,Springer,2022-05-26,"Permanent magnet synchronous motors (PMSMs) have many advantages attributed to the structure of the permanent magnet, which connects the influences of each type of fault. However, diagnostic methods are not capable of monitoring multitype faults with one index or method, due to closed-loop control or non-stationarity. There are a few diagnostic methods based on convolutional neural networks (CNNs) and vibration analysis; however, they are limited by the installed sensor locations and are affected by various factors of the driven system. This paper proposes a method to diagnose eccentricity and demagnetization in an interior PMSM (IPMSM) with image recognition based on a deep CNN and motor stator current analysis. To extract fault features from the IPMSM’s stator current with a deep CNN, a gray image transformation algorithm that uses the autocorrelation matrix is proposed to enhance the feature representations of stator currents, in which currents data are processed recursively and timely. The testing accuracy of 98.74% of both designed model pyramidal Resnet-9 and Resnet-15 indicates that the proposed method is capable of monitoring multitype faults and is immune to speeds and loads.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40747-022-00764-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10845-022-01957-6,A systematic literature review on recent trends of machine learning applications in additive manufacturing,Journal of Intelligent Manufacturing,10.1007/s10845-022-01957-6,Springer,2022-05-25,"Additive manufacturing (AM) offers the advantage of producing complex parts more efficiently and in a lesser production cycle time as compared to conventional subtractive manufacturing processes. It also provides higher flexibility for diverse applications by facilitating the use of a variety of materials and different processing technologies. With the exceptional growth of computing capability, researchers are extensively using machine learning (ML) techniques to control the performance of every phase of AM processes, such as design, process parameters modeling, process monitoring and control, quality inspection, and validation. Also, ML methods have made it possible to develop cybermanufacturing for AM systems and thus revolutionized Industry 4.0. This paper presents the state-of-the-art applications of ML in solving numerous problems related to AM processes. We give an overview of the research trends in this domain through a systematic literature review of relevant journal articles and conference papers. We summarize recent development and existing challenges to point out the direction of future research scope. This paper can provide AM researchers and practitioners with the latest information consequential for further development.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10845-022-01957-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-022-03613-1,Attention-based residual autoencoder for video anomaly detection,Applied Intelligence,10.1007/s10489-022-03613-1,Springer,2022-05-25,"Automatic anomaly detection is a crucial task in video surveillance system intensively used for public safety and others. The present system adopts a spatial branch and a temporal branch in a unified network that exploits both spatial and temporal information effectively. The network has a residual autoencoder architecture, consisting of a deep convolutional neural network-based encoder and a multi-stage channel attention-based decoder, trained in an unsupervised manner. The temporal shift method is used for exploiting the temporal feature, whereas the contextual dependency is extracted by channel attention modules. System performance is evaluated using three standard benchmark datasets. Result suggests that our network outperforms the state-of-the-art methods, achieving 97.4% for UCSD Ped2, 86.7% for CUHK Avenue, and 73.6% for ShanghaiTech dataset in term of Area Under Curve, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-022-03613-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s12938-022-01001-x,Fast machine learning annotation in the medical domain: a semi-automated video annotation tool for gastroenterologists,BioMedical Engineering OnLine,10.1186/s12938-022-01001-x,BioMed Central,2022-05-25,"Background Machine learning, especially deep learning, is becoming more and more relevant in research and development in the medical domain. For all the supervised deep learning applications, data is the most critical factor in securing successful implementation and sustaining the progress of the machine learning model. Especially gastroenterological data, which often involves endoscopic videos, are cumbersome to annotate. Domain experts are needed to interpret and annotate the videos. To support those domain experts, we generated a framework. With this framework, instead of annotating every frame in the video sequence, experts are just performing key annotations at the beginning and the end of sequences with pathologies, e.g., visible polyps. Subsequently, non-expert annotators supported by machine learning add the missing annotations for the frames in-between. Methods In our framework, an expert reviews the video and annotates a few video frames to verify the object’s annotations for the non-expert. In a second step, a non-expert has visual confirmation of the given object and can annotate all following and preceding frames with AI assistance. After the expert has finished, relevant frames will be selected and passed on to an AI model. This information allows the AI model to detect and mark the desired object on all following and preceding frames with an annotation. Therefore, the non-expert can adjust and modify the AI predictions and export the results, which can then be used to train the AI model. Results Using this framework, we were able to reduce workload of domain experts on average by a factor of 20 on our data. This is primarily due to the structure of the framework, which is designed to minimize the workload of the domain expert. Pairing this framework with a state-of-the-art semi-automated AI model enhances the annotation speed further. Through a prospective study with 10 participants, we show that semi-automated annotation using our tool doubles the annotation speed of non-expert annotators compared to a well-known state-of-the-art annotation tool. Conclusion In summary, we introduce a framework for fast expert annotation for gastroenterologists, which reduces the workload of the domain expert considerably while maintaining a very high annotation quality. The framework incorporates a semi-automated annotation system utilizing trained object detection models. The software and framework are open-source.",https://www.biomedcentral.com/openurl?doi=10.1186/s12938-022-01001-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10844-022-00714-8,SentiCode: A new paradigm for one-time training and global prediction in multilingual sentiment analysis,Journal of Intelligent Information Systems,10.1007/s10844-022-00714-8,Springer,2022-05-25,"The main objective of multilingual sentiment analysis is to analyze reviews regardless of the original language in which they are written. Switching from one language to another is very common on social media platforms. Analyzing these multilingual reviews is a challenge since each language is different in terms of syntax, grammar, etc. This paper presents a new language-independent representation approach for sentiment analysis, SentiCode. Unlike previous work in multilingual sentiment analysis, the proposed approach does not rely on machine translation to bridge the gap between different languages. Instead, it exploits common features of languages, such as part-of-speech tags used in Universal Dependencies. Equally important, SentiCode enables sentiment analysis in multi-language and multi-domain environments simultaneously. Several experiments were conducted using machine/deep learning techniques to evaluate the performance of SentiCode in multilingual (English, French, German, Arabic, and Russian) and multi-domain environments. In addition, the vocabulary proposed by SentiCode and the effect of each token were evaluated by the ablation method. The results highlight the 70% accuracy of SentiCode, with the best trade-off between efficiency and computing time (training and testing) in a total of about 0.67 seconds, which is very convenient for real-time applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10844-022-00714-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00530-022-00948-0,Deep learning in multimedia healthcare applications: a review,Multimedia Systems,10.1007/s00530-022-00948-0,Springer,2022-05-24,"The increase in chronic diseases has affected the countries’ health system and economy. With the recent COVID-19 virus, humanity has experienced a great challenge, which has led to make efforts to detect it and prevent its spread. Hence, it is necessary to develop new solutions that are based on technology and low cost, to satisfy the citizens’ needs. Deep learning techniques is a technological solution that has been used in healthcare lately. Nowadays, with the increase in chips processing capabilities, increase size of data, and the progress in deep learning research, healthcare applications have been proposed to provide citizens’ health needs. In addition, a big amount of data is generated every day. Development in Internet of Things, gadgets, and phones has allowed the access to multimedia data. Data such as images, video, audio and text are used as input of applications based on deep learning methods to support healthcare system to diagnose, predict, or treat patients. This review pretends to give an overview of proposed healthcare solutions based on deep learning techniques using multimedia data. We show the use of deep learning in healthcare, explain the different types of multimedia data, show some relevant deep learning multimedia applications in healthcare, and highlight some challenges in this research area.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-022-00948-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-12096-6,Using deep learning to identify maturity and 3D distance in pineapple fields,Scientific Reports,10.1038/s41598-022-12096-6,Nature,2022-05-24,"Pineapples are an important agricultural economic crop in Taiwan. Considerable human resources are required to protect pineapples from excessive solar radiation, which could otherwise lead to overheating and subsequent deterioration. Note that simple covering all of the fruit with a paper bag is not a viable solution, due to the fact that it makes it impossible to determine whether the fruit is ripe. This paper proposes a system by which to automate the detection of ripe pineapples. The proposed deep learning architecture enables detection regardless of lighting conditions, achieving accuracy of more than 99.27% with error of less than 2% at distances of 300 ~ 800 mm. This proposed system using an Nvidia TX2 is capable of 15 frames per second, thereby making it possible to mount the device on machines that move at walking speed.",https://www.nature.com/articles/s41598-022-12096-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13102-9,Identify videos with facial manipulations based on convolution neural network and dynamic texture,Multimedia Tools and Applications,10.1007/s11042-022-13102-9,Springer,2022-05-24,"Recent facial manipulation techniques based on deep learning can create a highly realistic face by changing expression, attributes, identity, or creating an entire face synthesis, that called recently Deep-Fake. With the rapid appearance of such applications, they have raised great security concerns. Therefore, corresponding forensic techniques are proposed to tackle this issue. However, existing techniques are either based on complex deep networks with a binary classification that are unable to distinguish between facial manipulation types or rely on fragile hand-crafted features with unsatisfactory results. To overcome these issues, we propose a learning-based detection method by creating an uncomplicated CNN network called FMD-Net relying on the dynamic textures as input. Moreover, it is able to distinguish between facial manipulation types such as Deepfake, Face2Face, FaceSwap, and NeuralTexture. By using dynamic textures of each video shot, motion and appearance features are combined which helped the network learn manipulation artifacts and provides a robust performance at various compression levels. We conduct extensive experiments on various benchmark datasets (FaceForensics++, DFDC, and Celeb-DF) to empirically demonstrate the superiority and effectiveness of the proposed method with both binary classification and multi-classification against the state-of-the-art methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13102-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s42979-022-01189-8,Hate Speech Detection on Code-Mixed Dataset Using a Fusion of Custom and Pre-trained Models with Profanity Vector Augmentation,SN Computer Science,10.1007/s42979-022-01189-8,Nature,2022-05-24,"With the increase in user-generated content on social media networks, hate speech and offensive language content are also increasing. From the perspective of computer science, automatic detection of such hate speech and offensive language content is an interesting problem to solve. The natural language community has taken a step to identify such content via automated hate speech and offensive content detection. The hate speech content is generated mostly on social media, and automatic hate speech and offensive language detection face many challenges due to non-standard spelling and grammar variations. Specifically, in a multilingual community, the hate content would be in code-mixed form, making the task further challenging. In this article, we propose a model for code-mixed hate speech detection. This model embeds the knowledge from both user-trained and multilingual pre-trained models. The proposed method also calculates the profanity word list and augments it. Experimental results on code-mixed hate speech and offensive language detection benchmarks show that our method outperforms the existing baselines.",https://www.nature.com/articles/s42979-022-01189-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12411-3,Emotion detection from multilingual audio using deep analysis,Multimedia Tools and Applications,10.1007/s11042-022-12411-3,Springer,2022-05-20,"Human emotion detection from multiple languages is a very challenging job. In this work, we have used language emotional databases of various languages such as – Ryerson-Audio-Visual database (RAVDESS), Berlin Database (EmoDb) and Italian Database (Emo-Vo) which are in English, German and Italian languages respectively. The proposed model extract MFCC, chroma, Tonnetz, Contrast from the raw audio file, which is further taken as input in the CNN model to identify emotions correctly. We are not using any visual representation of sound only direct from natural sound data. An extensive comparison is made with some of the previous approaches on emotion detection from speech. The experimental result shows that; the proposed model has successfully worked with all the selected databases with higher accuracy. The same also has been tested with the augmented database. We secure 70.46% for RAVDESS, 70.37% Emo-Db and 73.47% for Emo-Vo in the initial database and best model work in the augmented database. However, test with Original test dataset, secured 96.53% in RAVDESS 96.22% in Emo-Db and Emo-Vo 96.11% respectively. Multilingual Emotion detection, a state of art model, has been discussed with an accuracy of 97.89%. The proposed model is a speaker-independent as well as language-independent emotion detection system .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12411-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07330-1,Spatiotemporal context-aware network for video salient object detection,Neural Computing and Applications,10.1007/s00521-022-07330-1,Springer,2022-05-20,"It has been witnessed that there is an increasing interest in video salient object detection (VSOD) in computer vision field. Different from image salient object detection (ISOD), VSOD not only requires appearance information but also needs motion cues. Thus, it is essential to exploit spatiotemporal information to generate accurate saliency results. Existing VSOD models mainly combine an ISOD model with long short-term memory (LSTM) or flow-estimation modules to integrate saliency cues estimated from spatial and temporal domain. However, flow-estimation modules heavily rely on optical flow images; the generation process of which is rather time-consuming and severely limits its applications in practice. Besides, the LSTM can only exploit motion cues via a step-by-step propagation in the time domain and is hard to realize the multi-scale spatiotemporal interaction. In this paper, we propose the SCANet to solve the above problems. Specifically, we develop the pyramid dilated 3D convolutional (PD3C) module to generate rich temporal features by leveraging context information. Besides, a feature aggregation module is designed to effectively integrate spatial and temporal features. Equipped with these modules, the SCANet is capable of generating high-quality saliency maps at more than real-time inference speed (41 FPS on a single Titan Xp GPU). Extensive experimental results on six widely used benchmark datasets prove that SCANet outperforms state-of-the-art methods in terms of three standard evaluation metrics. Our code will be publicly available at https://github.com/clelouch/SCANet .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07330-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40537-022-00619-x,Detection of fake news and hate speech for Ethiopian languages: a systematic review of the approaches,Journal of Big Data,10.1186/s40537-022-00619-x,Springer,2022-05-19,"With the proliferation of social media platforms that provide anonymity, easy access, online community development, and online debate, detecting and tracking hate speech has become a major concern for society, individuals, policymakers, and researchers. Combating hate speech and fake news are the most pressing societal issues. It is difficult to expose false claims before they cause significant harm. Automatic fact or claim verification has recently piqued the interest of various research communities. Despite efforts to use automatic approaches for detection and monitoring, their results are still unsatisfactory, and that requires more research work in the area. Fake news and hate speech messages are any messages on social media platforms that spread negativity in society about sex, caste, religion, politics, race, disability, sexual orientation, and so on. Thus, the type of massage is extremely difficult to detect and combat. This work aims to analyze the optimal approaches for this kind of problem, as well as the relationship between the approaches, dataset type, size, and accuracy. Finally, based on the analysis results of the implemented approaches, deep learning (DL) approaches have been recommended for other Ethiopian languages to increase the performance of all evaluation metrics from different social media platforms. Additionally, as the review results indicate, the combination of DL and machine learning (ML) approaches with a balanced dataset can improve the detection and combating performance of the system.",https://www.biomedcentral.com/openurl?doi=10.1186/s40537-022-00619-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13042-022-01572-0,A multi-stage fusion instance learning method for anomalous event detection in videos,International Journal of Machine Learning and Cybernetics,10.1007/s13042-022-01572-0,Springer,2022-05-18,"Anomalous event detection in giant amount surveillance footage in real world is currently an active research area. Variery and rareness of the anomaly events is still a thorny challenge to deal with. In this paper, we propose a multi-stage fusion instance learning method (MFIL) for inferring anomalous event pattern and predicting anomaly appearance in videos. We propose object-aware model and action-aware model to represent regularities of human objects and actions among frames exploiting cascaded deep network models. Furthermore we improve and represent fusion instance learning method for fetching and maximizing anomaly scores via object and action regularities in anomalous sequences from videos. We validate the performance of MFIL on action movie and UCF-Crime respectively, both contain anomalous and violent events. Experimental results demonstrated that MFIL is effective for anomalous event detection in videos gathered from real world.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13042-022-01572-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-11549-2,Expert surgeons and deep learning models can predict the outcome of surgical hemorrhage from 1 min of video,Scientific Reports,10.1038/s41598-022-11549-2,Nature,2022-05-17,"Major vascular injury resulting in uncontrolled bleeding is a catastrophic and often fatal complication of minimally invasive surgery. At the outset of these events, surgeons do not know how much blood will be lost or whether they will successfully control the hemorrhage (achieve hemostasis). We evaluate the ability of a deep learning neural network (DNN) to predict hemostasis control ability using the first minute of surgical video and compare model performance with human experts viewing the same video. The publicly available SOCAL dataset contains 147 videos of attending and resident surgeons managing hemorrhage in a validated, high-fidelity cadaveric simulator. Videos are labeled with outcome and blood loss (mL). The first minute of 20 videos was shown to four, blinded, fellowship trained skull-base neurosurgery instructors, and to SOCALNet (a DNN trained on SOCAL videos). SOCALNet architecture included a convolutional network (ResNet) identifying spatial features and a recurrent network identifying temporal features (LSTM). Experts independently assessed surgeon skill, predicted outcome and blood loss (mL). Outcome and blood loss predictions were compared with SOCALNet. Expert inter-rater reliability was 0.95. Experts correctly predicted 14/20 trials (Sensitivity: 82%, Specificity: 55%, Positive Predictive Value (PPV): 69%, Negative Predictive Value (NPV): 71%). SOCALNet correctly predicted 17/20 trials (Sensitivity 100%, Specificity 66%, PPV 79%, NPV 100%) and correctly identified all successful attempts. Expert predictions of the highest and lowest skill surgeons and expert predictions reported with maximum confidence were more accurate. Experts systematically underestimated blood loss (mean error − 131 mL, RMSE 350 mL, R^2 0.70) and fewer than half of expert predictions identified blood loss > 500 mL (47.5%, 19/40). SOCALNet had superior performance (mean error − 57 mL, RMSE 295 mL, R^2 0.74) and detected most episodes of blood loss > 500 mL (80%, 8/10). In validation experiments, SOCALNet evaluation of a critical on-screen surgical maneuver and high/low-skill composite videos were concordant with expert evaluation. Using only the first minute of video, experts and SOCALNet can predict outcome and blood loss during surgical hemorrhage. Experts systematically underestimated blood loss, and SOCALNet had no false negatives. DNNs can provide accurate, meaningful assessments of surgical video. We call for the creation of datasets of surgical adverse events for quality improvement research.",https://www.nature.com/articles/s41598-022-11549-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-022-09975-8,Multi-agent based Arabic speech synthesis,International Journal of Speech Technology,10.1007/s10772-022-09975-8,Springer,2022-05-17,"Natural Language Processing (NLP) has many applications such as Speech recognition, Speech understanding, and Speech synthesis. Several approaches have been proposed in the literature in dealing with NLP. This paper describes an ongoing research project that tackles Speech Arabic Synthesis using multi-agent system techniques. The system consists of five modules (agents): the User Interface Agent (UIA), the Facilitator Agent (FA), the Preprocessing Agent (PPA), the Orthographic and Phonetic Transcription Agent, and the Speech Generation Agent. These agents are communicating with each other to construct agent sub societies representing the user input. All the agents are cognitive, work together, and communicate with the Knowledge-Base and the Sound Segments Database to generate Arabic speech signals. We used 800 Arabic sentences and asked 10 listeners with different levels of knowledge of the Arab language to accomplish the evaluation perception process. The system presents in general a Success Rate of 86% for the set of 800 tested sentences.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-022-09975-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s13636-022-00242-x,"An overview of machine learning and other data-based methods for spatial audio capture, processing, and reproduction","EURASIP Journal on Audio, Speech, and Music Processing",10.1186/s13636-022-00242-x,Springer,2022-05-16,"The domain of spatial audio comprises methods for capturing, processing, and reproducing audio content that contains spatial information. Data-based methods are those that operate directly on the spatial information carried by audio signals. This is in contrast to model-based methods, which impose spatial information from, for example, metadata like the intended position of a source onto signals that are otherwise free of spatial information. Signal processing has traditionally been at the core of spatial audio systems, and it continues to play a very important role. The irruption of deep learning in many closely related fields has put the focus on the potential of learning-based approaches for the development of data-based spatial audio applications. This article reviews the most important application domains of data-based spatial audio including well-established methods that employ conventional signal processing while paying special attention to the most recent achievements that make use of machine learning. Our review is organized based on the topology of the spatial audio pipeline that consist in capture, processing/manipulation, and reproduction. The literature on the three stages of the pipeline is discussed, as well as on the spatial audio representations that are used to transmit the content between them, highlighting the key references and elaborating on the underlying concepts. We reflect on the literature based on a juxtaposition of the prerequisites that made machine learning successful in domains other than spatial audio with those that are found in the domain of spatial audio as of today. Based on this, we identify routes that may facilitate future advancement.",https://www.biomedcentral.com/openurl?doi=10.1186/s13636-022-00242-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-022-09974-9,Analysis of influencing features with spectral feature extraction and multi-class classification using deep neural network for speech recognition system,International Journal of Speech Technology,10.1007/s10772-022-09974-9,Springer,2022-05-16,"There is a drastic need for extracting information from non-linguistic features of the audio sources. It leads to the eminent rise of speech technology over the past few decades. It is termed computational para-linguistics. This research concentrates on extracting and providing a robust feature that examines the characteristics of speech data. The factors are analysed in a spectral way which stimulates the auditory elements. The speech enhancement technological process is being initiated with pre-processing, feature extraction, and classification. Initially, the input data conversion is done with ADC of 16 kHz sampling frequency. The spectral features are extracted with minimal Mean Square Error to enhance the re-construction ability and eliminate the redundancy characteristics. Finally, the deep neural network is adopted for multi-class classification. The simulation is performed in MATLAB 2020a environment, and the empirical outcomes are evaluated with existing approaches. Here, metrics like Mean Square Error, accuracy, Signal-to-Noise ratio (SNR) and features retained are computed efficiently. The anticipated model shows a trade-off in contrast to prevailing approaches. The outcomes demonstrate a better recognition rate and offer significant characteristics in selecting the most influencing features.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-022-09974-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13001-z,Video splicing detection and localization based on multi-level deep feature fusion and reinforcement learning,Multimedia Tools and Applications,10.1007/s11042-022-13001-z,Springer,2022-05-16,"Splicing forgery refers to copying some regions of a video or an image to another video/image. Although image splicing detection has been studied for many years, video splicing detection has attracted relatively much less attention. In this paper, we proposed a novel framework for video splicing detection by modeling this forensic task as a video object segmentation problem. Based on the nature of this forgery operation, discontinuous noise distribution and object contours are adopted as traces to guide the localization results. The method consists of three modules: EXIF-consistency prediction, suspected region tracking, and semantic segmentation. To bridge the gap between sensor-level and semantic-level features, three modules in our framework are integrated for final tampered areas detection. Firstly, we use the EXIF-consistency prediction module to extract sensor-level traces from tampered areas. Then, we employ a deep reinforcement learning-based method for tracking suspected regions. Finally, a semantic segmentation module is adopted to localize the final results of the tampered regions. Compared with several state-of-the-art forensic approaches, our method demonstrates superiority in publicly available datasets. In terms of F1 score, our method achieves 0.623 in GRIP dataset.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13001-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s42001-022-00170-y,VisualCommunity: a platform for archiving and studying communities,Journal of Computational Social Science,10.1007/s42001-022-00170-y,Springer,2022-05-16,"VisualCommunity is a platform designed to support community or neighborhood scale research. The platform integrates mobile, AI, visualization techniques, along with tools to help domain researchers, practitioners, and students collecting and working with spatialized video and geo-narratives. These data, which provide granular spatialized imagery and associated context gained through expert commentary have previously provided value in understanding various community-scale challenges. This paper further enhances this work AI-based image processing and speech transcription tools available in VisualCommunity, allowing for the easy exploration of the acquired semantic and visual information about the area under investigation. In this paper we describe the specific advances through use case examples including COVID-19 related scenarios.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s42001-022-00170-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00415-022-11148-1,Predicting clinical scores in Huntington’s disease: a lightweight speech test,Journal of Neurology,10.1007/s00415-022-11148-1,Springer,2022-05-14,"Objectives Using brief samples of speech recordings, we aimed at predicting, through machine learning, the clinical performance in Huntington’s Disease (HD), an inherited Neurodegenerative disease (NDD). Methods We collected and analyzed 126 samples of audio recordings of both forward and backward counting from 103 Huntington’s disease gene carriers [87 manifest and 16 premanifest; mean age 50.6 (SD 11.2), range (27–88) years] from three multicenter prospective studies in France and Belgium (MIG-HD (ClinicalTrials.gov NCT00190450); BIO-HD (ClinicalTrials.gov NCT00190450) and Repair-HD (ClinicalTrials.gov NCT00190450). We pre-registered all of our methods before running any analyses, in order to avoid inflated results. We automatically extracted 60 speech features from blindly annotated samples. We used machine learning models to combine multiple speech features in order to make predictions at individual levels of the clinical markers. We trained machine learning models on 86% of the samples, the remaining 14% constituted the independent test set. We combined speech features with demographics variables (age, sex, CAG repeats, and burden score) to predict cognitive, motor, and functional scores of the Unified Huntington’s disease rating scale. We provided correlation between speech variables and striatal volumes. Results Speech features combined with demographics allowed the prediction of the individual cognitive, motor, and functional scores with a relative error from 12.7 to 20.0% which is better than predictions using demographics and genetic information. Both mean and standard deviation of pause durations during backward recitation and clinical scores correlated with striatal atrophy (Spearman 0.6 and 0.5–0.6, respectively). Interpretation Brief and examiner-free speech recording and analysis may become in the future an efficient method for remote evaluation of the individual condition in HD and likely in other NDD.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00415-022-11148-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10278-022-00644-5,Deep Reinforcement Learning with Automated Label Extraction from Clinical Reports Accurately Classifies 3D MRI Brain Volumes,Journal of Digital Imaging,10.1007/s10278-022-00644-5,Springer,2022-05-13,"Image classification is probably the most fundamental task in radiology artificial intelligence. To reduce the burden of acquiring and labeling data sets, we employed a two-pronged strategy. We automatically extracted labels from radiology reports in Part 1. In Part 2, we used the labels to train a data-efficient reinforcement learning (RL) classifier. We applied the approach to a small set of patient images and radiology reports from our institution. For Part 1, we trained sentence-BERT (SBERT) on 90 radiology reports. In Part 2, we used the labels from the trained SBERT to train an RL-based classifier. We trained the classifier on a training set of $$40$$ 40 images. We tested on a separate collection of $$24$$ 24 images. For comparison, we also trained and tested a supervised deep learning (SDL) classification network on the same set of training and testing images using the same labels. Part 1: The trained SBERT model improved from 82 to $$100\%$$ 100 % accuracy. Part 2: Using Part 1’s computed labels, SDL quickly overfitted the small training set. Whereas SDL showed the worst possible testing set accuracy of 50%, RL achieved $$100\%$$ 100 % testing set accuracy, with a $$p$$ p -value of $$4.9\times {10}^{-4}$$ 4.9 × 10 - 4 . We have shown the proof-of-principle application of automated label extraction from radiological reports. Additionally, we have built on prior work applying RL to classification using these labels, extending from 2D slices to entire 3D image volumes. RL has again demonstrated a remarkable ability to train effectively, in a generalized manner, and based on small training sets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10278-022-00644-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12898-w,Machine learning based analysis of learner-centric teaching of punjabi grammar with multimedia tools in rural indian environment,Multimedia Tools and Applications,10.1007/s11042-022-12898-w,Springer,2022-05-13,"The advent of multimedia and its reach to everyone has made a massive change in life. Multimedia content enhances the learning trend and is playing a pivotal role in making the teaching and learning process more learners centric. In the present manuscript, authors use machine learning methods to find the impact of multimedia led teaching in government schools of Punjab, India. They promote the use of computer-based teaching in Punjabi for teaching the syllabus in schools. The secondary class students of Patiala and Mohali government schools affiliated to Punjab School Education Board are participants of this study. The students are divided in two groups. Twenty two topics of Punjabi grammar syllabus are taught to two student groups separately using different instructional strategies (multimedia presentations and traditional lectures). Achievement test, before and after the teaching, are conducted for all participants. The results support the hypothesis that multimedia does make a difference in the overall learning of the students. The descriptive statistics of achievement score shows the improvement in marks obtained by students after technology driven teaching. The average marks scored by students taught through multimedia system is 52.24% more than taught through traditional method. Nine machine learning models are used to find effectiveness of multimedia-based learning. Results shows that AdaBoost performs well with an accuracy of 99.8% respective to others based on student learning strategies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12898-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s13049-022-01020-6,Artificial intelligence in Emergency Medical Services dispatching: assessing the potential impact of an automatic speech recognition software on stroke detection taking the Capital Region of Denmark as case in point,"Scandinavian Journal of Trauma, Resuscitation and Emergency Medicine",10.1186/s13049-022-01020-6,BioMed Central,2022-05-12,"Background and purpose Stroke recognition at the Emergency Medical Services (EMS) impacts the stroke treatment and thus the related health outcome. At the EMS Copenhagen 66.2% of strokes are detected by the Emergency Medical Dispatcher (EMD) and in Denmark approximately 50% of stroke patients arrive at the hospital within the time-to-treatment. An automatic speech recognition software (ASR) can increase the recognition of Out-of-Hospital cardiac arrest (OHCA) at the EMS by 16%. This research aims to analyse the potential impact an ASR could have on stroke recognition at the EMS Copenhagen and the related treatment. Methods Stroke patient data ( n  = 9049) from the years 2016–2018 were analysed retrospectively, regarding correlations between stroke detection at the EMS and stroke specific, as well as personal characteristics such as stroke type, sex, age, weekday, time of day, year, EMS number contacted, and treatment. The possible increase in stroke detection through an ASR and the effect on stroke treatment was calculated based on the impact of an existing ASR to detect OHCA from CORTI AI. Results The Chi-Square test with the respective post-hoc test identified a negative correlation between stroke detection and females, the 1813-Medical Helpline, as well as weekends, and a positive correlation between stroke detection and treatment and thrombolysis. While the association analysis showed a moderate correlation between stroke detection and treatment the correlation to the other treatment options was weak or very weak. A potential increase in stroke detection to 61.19% with an ASR and hence an increase of thrombolysis by 5% in stroke patients calling within time-to-treatment was predicted. Conclusions An ASR can potentially improve stroke recognition by EMDs and subsequent stroke treatment at the EMS Copenhagen. Based on the analysis results improvement of stroke recognition is particularly relevant for females, younger stroke patients, calls received through the 1813-Medical Helpline, and on weekends. Trial registration This study was registered at the Danish Data Protection Agency (PVH-2014-002) and the Danish Patient Safety Authority (R-21013122).",https://www.biomedcentral.com/openurl?doi=10.1186/s13049-022-01020-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40644-022-00457-3,Real-time automatic prediction of treatment response to transcatheter arterial chemoembolization in patients with hepatocellular carcinoma using deep learning based on digital subtraction angiography videos,Cancer Imaging,10.1186/s40644-022-00457-3,BioMed Central,2022-05-12,"Background Transcatheter arterial chemoembolization (TACE) is the mainstay of therapy for intermediate-stage hepatocellular carcinoma (HCC); yet its efficacy varies between patients with the same tumor stage. Accurate prediction of TACE response remains a major concern to avoid overtreatment. Thus, we aimed to develop and validate an artificial intelligence system for real-time automatic prediction of TACE response in HCC patients based on digital subtraction angiography (DSA) videos via a deep learning approach. Methods This retrospective cohort study included a total of 605 patients with intermediate-stage HCC who received TACE as their initial therapy. A fully automated framework (i.e., DSA-Net) contained a U-net model for automatic tumor segmentation (Model 1) and a ResNet model for the prediction of treatment response to the first TACE (Model 2). The two models were trained in 360 patients, internally validated in 124 patients, and externally validated in 121 patients. Dice coefficient and receiver operating characteristic curves were used to evaluate the performance of Models 1 and 2, respectively. Results Model 1 yielded a Dice coefficient of 0.75 (95% confidence interval [CI]: 0.73–0.78) and 0.73 (95% CI: 0.71–0.75) for the internal validation and external validation cohorts, respectively. Integrating the DSA videos, segmentation results, and clinical variables (mainly demographics and liver function parameters), Model 2 predicted treatment response to first TACE with an accuracy of 78.2% (95%CI: 74.2–82.3), sensitivity of 77.6% (95%CI: 70.7–84.0), and specificity of 78.7% (95%CI: 72.9–84.1) for the internal validation cohort, and accuracy of 75.1% (95% CI: 73.1–81.7), sensitivity of 50.5% (95%CI: 40.0–61.5), and specificity of 83.5% (95%CI: 79.2–87.7) for the external validation cohort. Kaplan-Meier curves showed a significant difference in progression-free survival between the responders and non-responders divided by Model 2 ( p  = 0.002). Conclusions Our multi-task deep learning framework provided a real-time effective approach for decoding DSA videos and can offer clinical-decision support for TACE treatment in intermediate-stage HCC patients in real-world settings.",https://www.biomedcentral.com/openurl?doi=10.1186/s40644-022-00457-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-11506-z,The feasibility to use artificial intelligence to aid detecting focal liver lesions in real-time ultrasound: a preliminary study based on videos,Scientific Reports,10.1038/s41598-022-11506-z,Nature,2022-05-11,"Despite the wide availability of ultrasound machines for hepatocellular carcinoma surveillance, an inadequate number of expert radiologists performing ultrasounds in remote areas remains a primary barrier for surveillance. We demonstrated feasibility of artificial intelligence (AI) to aid in the detection of focal liver lesions (FLLs) during ultrasound. An AI system for FLL detection in ultrasound videos was developed. Data in this study were prospectively collected at a university hospital. We applied a two-step training strategy for developing the AI system by using a large collection of ultrasound snapshot images and frames from full-length ultrasound videos. Detection performance of the AI system was evaluated and then compared to detection performance by 25 physicians including 16 non-radiologist physicians and 9 radiologists. Our dataset contained 446 videos (273 videos with 387 FLLs and 173 videos without FLLs) from 334 patients. The videos yielded 172,035 frames with FLLs and 1,427,595 frames without FLLs for training on the AI system. The AI system achieved an overall detection rate of 89.8% (95%CI: 84.5–95.0) which was significantly higher than that achieved by non-radiologist physicians (29.1%, 95%CI: 21.2–37.0, p  < 0.001) and radiologists (70.9%, 95%CI: 63.0–78.8, p  < 0.001). Median false positive detection rate by the AI system was 0.7% (IQR: 1.3%). AI system operation speed reached 30–34 frames per second, showing real-time feasibility. A further study to demonstrate whether the AI system can assist operators during ultrasound examinations is warranted.",https://www.nature.com/articles/s41598-022-11506-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07244-y,Memory-enhanced deep reinforcement learning for UAV navigation in 3D environment,Neural Computing and Applications,10.1007/s00521-022-07244-y,Springer,2022-05-10,"It is a long-term challenging task to develop an intelligent agent that is able to navigate in 3D environment using only visual input in an end-to-end manner. In this paper, we introduce a goal-conditioned reinforcement learning framework for vision-based UAV navigation, and then develop a Memory Enhanced DRL agent with dynamic relative goal, extra action penalty and non-sparse reward to tackle the UAV navigation problem. This enables the agent to escape from the objective-obstacle dilemma. By performing experimental evaluations in high-fidelity visual environments simulated by Airsim, we show that our proposed memory-enhanced model can achieve higher success rate with less training steps compared to the DRL agents without memories.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07244-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13735-022-00232-x,RGBD deep multi-scale network for background subtraction,International Journal of Multimedia Information Retrieval,10.1007/s13735-022-00232-x,Springer,2022-05-10,"This paper proposes a novel deep learning model called deep multi-scale network (DMSN) for background subtraction. This convolutional neural network is built to use RGB color channels and Depth maps as inputs with which it can fuse semantic and spatial information. In comparison with previous deep learning background subtraction techniques that lack information due to its use of only RGB channels, our RGBD version is able to overcome most of the drawbacks, especially in some particular kinds of challenges. Further, this paper introduces a new protocol for the SBM-RGBD dataset, concerning scene-independent evaluation, dedicated to Deep Learning methods to set up a competitive platform that includes more challenging situations. The proposed method proved its efficiency in solving the background subtraction in complex situations at different levels. The experimental results verify that the proposed work outperforms the state of the art on SBM-RGBD and GSM datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13735-022-00232-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12901-4,Spatiotemporal two-stream LSTM network for unsupervised video summarization,Multimedia Tools and Applications,10.1007/s11042-022-12901-4,Springer,2022-05-10,"Within user-created videos, the constantly changing content among neighboring images brings more challenge for the prior video summarization methods. Assuming the images’ critical features are refined, one can obtain promising accuracy of keyframes’ selection which is key in video summarization. In our work, we innovatively proposed a Spatiotemporal two-stream LSTM network-based (ST-LSTM) model to enhance the images’ critical features with the combination of spatial saliency and temporal semantic dependencies which is referred to as the two-stream method. Motivated by the fact that sizable and moving objects attract more visual attention, we newly design a Saliency-area-based attention network to filter irrelative non-attractive information. We use the latest attention-based Bi-LSTM network to extract the temporal dependency on the semantic features. Furthermore, a multi-feature-based reward function is presented to reinforce the ST-LSTM model by integrating diversity, representativeness, and storyness. Last, the Deep Deterministic Policy Gradient (DDPG) algorithm is adopted to do the unsupervised training for the proposed method. Extensive experiments on the public datasets demonstrate that our method outperforms the state-of-the-art.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12901-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40510-022-00410-x,Accuracy of deep learning-based integrated tooth models by merging intraoral scans and CBCT scans for 3D evaluation of root position during orthodontic treatment,Progress in Orthodontics,10.1186/s40510-022-00410-x,Springer,2022-05-09,"Objective This study aimed to evaluate the accuracy of deep learning-based integrated tooth models (ITMs) by merging intraoral scans and cone-beam computed tomography (CBCT) scans for three-dimensional (3D) evaluation of root position during orthodontic treatment and to compare the fabrication process of integrated tooth models (ITMs) with manual method. Material and methods Intraoral scans and corresponding CBCT scans before and after treatment were obtained from 15 patients who completed orthodontic treatment with premolar extraction. A total of 600 ITMs were generated using deep learning technology and manual methods by merging the intraoral scans and CBCT scans at pretreatment. Posttreatment intraoral scans were integrated into the tooth model, and the resulting estimated root positions were compared with the actual root position at posttreatment CBCT. Discrepancies between the estimated and actual root position including average surface differences, arch widths, inter-root distances, and root axis angles were obtained in both the deep learning and manual method, and these measurements were compared between the two methods. Results The average surface differences of estimated and actual ITMs in the manual method were 0.02 mm and 0.03 mm for the maxillary and mandibular arches, respectively. In the deep learning method, the discrepancies were 0.07 mm and 0.08 mm for the maxillary and mandibular arches, respectively. For the measurements of arch widths, inter-root distances, and root axis angles, there were no significant differences between estimated and actual models both in the manual and in the deep learning methods, except for some measurements. Comparing the two methods, only three measurements showed significant differences. The procedure times taken to obtain the measurements were longer in the manual method than in the deep learning method. Conclusion Both deep learning and manual methods showed similar accuracy in the integration of intraoral scans and CBCT images. Considering time and efficiency, the deep learning automatic method for ITMs is highly recommended for clinical practice.",https://www.biomedcentral.com/openurl?doi=10.1186/s40510-022-00410-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00530-022-00937-3,A reference-based model using deep learning for image captioning,Multimedia Systems,10.1007/s00530-022-00937-3,Springer,2022-05-09,"Describing images in natural language is a challenging task for computer vision. Image captioning is the task of creating image descriptions. Deep learning architectures that use convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are beneficial in this task. However, traditional RNNs may cause problems, including exploding gradients, vanishing gradients, and non-descriptive sentences. To solve these problems, we propose a model based on the encoder–decoder structure, using CNNs to extract features from reference images and gated recurrent units (GRUs) to create the descriptions. Our model applies part-of-speech (PoS) analysis and the likelihood function to generate weights in GRU. This method also performs the knowledge transfer during a validation phase using the k-nearest neighbors (kNN) technique. Our experimental results using Flickr30k and MS-COCO datasets indicate that the proposed PoS-based model yields competitive scores compared to those of high-end models. The system predicts more descriptive captions and closely approximates the expected captions for both the predicted and kNN-selected captions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-022-00937-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12856-6,An information-rich sampling technique over spatio-temporal CNN for classification of human actions in videos,Multimedia Tools and Applications,10.1007/s11042-022-12856-6,Springer,2022-05-09,"We propose a novel video sampling scheme for human action recognition in videos, using Gaussian Weighing Function. Traditionally in deep learning-based human activity recognition approaches, either a few random frames or every k ^ t h frame of the video is considered for training the 3D CNN, where k is a small positive integer, like 4, 5, or 6. This kind of sampling reduces the volume of the input data, which speeds-up the training network and also avoids overfitting to some extent, thus enhancing the performance of the 3D CNN model. In the proposed video sampling technique, consecutive k frames of a video are aggregated into a single frame by computing a Gaussian-weighted summation of the k frames. The resulting frame preserves the information in a better way than the conventional approaches and experimentally shown to perform better. In this paper, a 3-Dimensional deep CNN is proposed to extract the spatio-temporal features and follows Long Short-Term Memory (LSTM) to recognize human actions. The proposed 3D CNN architecture is capable of handling the videos where the camera is placed at a distance from the performer. Experiments are performed with KTH, WEIZMANN, and CASIA-B Human Activity and Gait datasets, whereby it is shown to outperform state-of-the-art deep learning based techniques. We achieve 95.78%, 95.27%, and 95.27% over the KTH, WEIZMANN, and CASIA-B human action and gait recognition datasets, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12856-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-022-04512-5,3D map reconstruction using a monocular camera for smart cities,The Journal of Supercomputing,10.1007/s11227-022-04512-5,Springer,2022-05-07,"Large-scale high-resolution three-dimensional (3D) maps play a vital role in the development of smart cities. In this work, a novel deep learning-based multi-view-stereo method is proposed for reconstructing the 3D maps in large-scale urban environments by exploiting a monocular camera. Compared with other existing works, the proposed method can perform 3D depth estimation more efficiently in terms of computational complexity and graphics processing unit memory usage. As a result, the proposed method can practically perform depth estimation for each pixel before generating 3D maps for even large-scale scenes. Extensive experiments on the well-known DTU dataset and real-life data collected on our campus confirm the good performance of the proposed method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-022-04512-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-10886-6,3D Reconstruction of cellular images from microfabricated imagers using fully-adaptive deep neural networks,Scientific Reports,10.1038/s41598-022-10886-6,Nature,2022-05-04,"Millimeter-scale multi-cellular level imagers enable various applications, ranging from intraoperative surgical navigation to implantable sensors. However, the tradeoffs for miniaturization compromise resolution, making extracting 3D cell locations challenging—critical for tumor margin assessment and therapy monitoring. This work presents three machine-learning-based modules that extract spatial information from single image acquisitions using custom-made millimeter-scale imagers. The neural networks were trained on synthetically-generated (using Perlin noise) cell images. The first network is a convolutional neural network estimating the depth of a single layer of cells, the second is a deblurring module correcting for the point spread function (PSF). The final module extracts spatial information from a single image acquisition of a 3D specimen and reconstructs cross-sections, by providing a layered “map” of cell locations. The maximum depth error of the first module is 100 µm, with 87% test accuracy. The second module’s PSF correction achieves a least-square-error of only 4%. The third module generates a binary “cell” or “no cell” per-pixel labeling with an accuracy ranging from 89% to 85%. This work demonstrates the synergy between ultra-small silicon-based imagers that enable in vivo imaging but face a trade-off in spatial resolution, and the processing power of neural networks to achieve enhancements beyond conventional linear optimization techniques.",https://www.nature.com/articles/s41598-022-10886-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12938-5,Energy-efficient cooperative routing scheme with recurrent neural network based decision making system for wireless multimedia sensor networks,Multimedia Tools and Applications,10.1007/s11042-022-12938-5,Springer,2022-05-03,"“Wireless multimedia sensor networks (WMSNs)” are deployed in wider range of applications including video surveillance and area monitoring. However, due to the error-prone unreliable medium and application-based quality of service (QoS) requirements, routing in WMSNs becomes a serious issue. Thereby, this work intends to find the maximum energy cooperative route in WMSNs. Accordingly, Recurrent Neural Network (RNN) oriented decision making system is introduced for selecting the appropriate cooperative nodes with the knowledge of: (i) Tri-level energy utilization of nodes (ii) Reliability (iii) Delay to encounter the multimedia services in the network for transmitting the multimedia information. To make the precise decision on this, this paper intends to enhance the system model of RNN via optimizing the weights. For this optimization, a new Sea lion Adapted Grey Wolf Optimization (SA-GWA) is introduced, which is the hybridization of both Sea lion Optimization (SLnO) and Grey Wolf Optimizer (GWO). Finally, the superiority of the proposed model is validated over existing models in terms of reliability, residual energy and delay analysis.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12938-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s42836-022-00119-6,Artificial intelligence in knee arthroplasty: current concept of the available clinical applications,Arthroplasty,10.1186/s42836-022-00119-6,BioMed Central,2022-05-02,"Background Artificial intelligence (AI) is defined as the study of algorithms that allow machines to reason and perform cognitive functions such as problem-solving, objects, images, word recognition, and decision-making. This study aimed to review the published articles and the comprehensive clinical relevance of AI-based tools used before, during, and after knee arthroplasty. Methods The search was conducted through PubMed, EMBASE, and MEDLINE databases from 2000 to 2021 using the 2009 Preferred Reporting Items for Systematic Reviews and Meta-Analyses Protocol (PRISMA). Results A total of 731 potential articles were reviewed, and 132 were included based on the inclusion criteria and exclusion criteria. Some steps of the knee arthroplasty procedure were assisted and improved by using AI-based tools. Before surgery, machine learning was used to aid surgeons in optimizing decision-making. During surgery, the robotic-assisted systems improved the accuracy of knee alignment, implant positioning, and ligamentous balance. After surgery, remote patient monitoring platforms helped to capture patients’ functional data. Conclusion In knee arthroplasty, the AI-based tools improve the decision-making process, surgical planning, accuracy, and repeatability of surgical procedures.",https://www.biomedcentral.com/openurl?doi=10.1186/s42836-022-00119-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12073-1,Multi-viewport based 3D convolutional neural network for 360-degree video quality assessment,Multimedia Tools and Applications,10.1007/s11042-022-12073-1,Springer,2022-05-01,"360-degree videos, also known as omnidirectional or panoramic videos, provide the user an immersive experience that 2D videos cannot provide. It is crucial to access the perceived quality of the 360-degree video. 2D video quality assessment (VQA) methods are unsuitable for 360-degree videos. There are few 360-degree video quality assessment (360VQA) methods. This paper proposes a multi-viewport based 3D convolutional neural network for 360VQA (3D-360VQA). First, it is easy to divide the 2D planar video into rectangular blocks as video patches in order to adapt to a deep neural network. The way to form the video patch in a 2D planar video is unsuitable for a 360-degree video. Thus, a multiple viewports based video patch forming method is proposed. Second, although the deep neural networks have achieved great success in image quality assessment (IQA), there are few deep neural networks for 360VQA. A 3D convolution based deep neural network is proposed to predict the perceived quality of 360-degree videos. The publicly available 360-degree videos datasets are used to evaluate the proposed method. The experimental results show that the proposed method is suitable for the 360-degree video and outperforms other existing methods, which verifies the effectiveness of our network architecture.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12073-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06875-x,A comparison of deep learning models for end-to-end face-based video retrieval in unconstrained videos,Neural Computing and Applications,10.1007/s00521-021-06875-x,Springer,2022-05-01,"Face-based video retrieval (FBVR) is the task of retrieving videos that containing the same face shown in the query image. In this article, we present the first end-to-end FBVR pipeline that is able to operate on large datasets of unconstrained, multi-shot, multi-person videos. We adapt an existing audiovisual recognition dataset to the task of FBVR and use it to evaluate our proposed pipeline. We compare a number of deep learning models for shot detection, face detection, and face feature extraction as part of our pipeline on a validation dataset made of more than 4000 videos. We obtain 97.25% mean average precision on an independent test set, composed of more than 1000 videos. The pipeline is able to extract features from videos at $$\sim $$ ∼ 7 times the real-time speed, and it is able to perform a query on thousands of videos in less than 0.5 s.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06875-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-022-01601-z,RIConv++: Effective Rotation Invariant Convolutions for 3D Point Clouds Deep Learning,International Journal of Computer Vision,10.1007/s11263-022-01601-z,Springer,2022-05-01,"3D point clouds deep learning is a promising field of research that allows a neural network to learn features of point clouds directly, making it a robust tool for solving 3D scene understanding tasks. While recent works show that point cloud convolutions can be invariant to translation and point permutation, investigations of the rotation invariance property for point cloud convolution has been so far scarce. Some existing methods perform point cloud convolutions with rotation-invariant features, existing methods generally do not perform as well as translation-invariant only counterpart. In this work, we argue that a key reason is that compared to point coordinates, rotation-invariant features consumed by point cloud convolution are not as distinctive. To address this problem, we propose a simple yet effective convolution operator that enhances feature distinction by designing powerful rotation invariant features from the local regions. We consider the relationship between the point of interest and its neighbors as well as the internal relationship of the neighbors to largely improve the feature descriptiveness. Our network architecture can capture both local and global context by simply tuning the neighborhood size in each convolution layer. We conduct several experiments on synthetic and real-world point cloud classifications, part segmentation, and shape retrieval to evaluate our method, which achieves the state-of-the-art accuracy under challenging rotations.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-022-01601-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06072-w,Video analysis of intelligent teaching based on machine learning and virtual reality technology,Neural Computing and Applications,10.1007/s00521-021-06072-w,Springer,2022-05-01,"In this paper, we study and analyse the teaching video of oil painting art through machine learning combined with virtual reality computing. Since the current oil painting, image acquisition method cannot meet the user's demand for multi-dimensional image description, and at the same time the retrieval method is too simple to perform high flexibility retrieval, we try to adopt the deep learning-based object extraction fusion method. Also, objects with poor performance quality are not suitable for further interception and cutting. We first pre-process the images in the image library to filter out the relatively high-quality images and then filter out the objects whose saliency and clarity do not reach the queue value by judging the saliency and clarity values of the images. Next, a series of aesthetic criteria, such as visual balance, visual triangulation, and centrosymmetric diagonal composition criteria, used to further filter the objects with relatively poor quality and low ratings. Then, we expand the areas with high saliency, match the contours of the segmented image elements with the contours of the user-drawn image to return an optimal matching value, and finally improve the quality and naturalness of the image by learning the deeper features of the image based on the style migration. The experimental framework based on TensorFlow is a new application of deep learning in the field of image synthesis, which has a very good improvement in the implementation efficiency compared with the traditional method. Using virtual reality technology to carry out teaching practice and analyse the effect of teaching practice, students can immerse themselves in art appreciation teaching activities, accept multiculturalism, learn through experience, and improve aesthetic quality.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06072-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-021-04274-6,Player target tracking and detection in football game video using edge computing and deep learning,The Journal of Supercomputing,10.1007/s11227-021-04274-6,Springer,2022-05-01,"The purpose is to explore the player detection and motion tracking in football game video based on edge computing and deep learning (DL), thus improving the detection effect of player trajectory in different scenes. First, the basic technology of player target tracking and detection task is analyzed based on the Histograms of Oriented Gradients feature. Then, the neural network structure in DL is combined with the target tracking method to improve the miss detection problem of the Faster R-CNN (FRCN) algorithm in detecting small targets. Edge computing places massive computing nodes close to the terminal devices to meet the high computing and low latency requirements of DL on edge devices. After the occlusion problem in the football game is analyzed, the optimized algorithm is applied to the public dataset OTB2013 and the football game dataset containing 80 motion trajectories. After testing, the target tracking accuracy of the improved FRCN is 89.1%, the target tracking success rate is 64.5%, and the running frame rate is still about 25 fps. The high confidence of FRCN algorithm also avoids template pollution. In the ordinary scene, the FRCN algorithm basically does not lose the target. The area under curve value of the proposed FRCN algorithm decreases slightly in the scene where the target is occluded. The FRCN algorithm based on DL technology can achieve the target tracking of players in football game video and has strong robustness to the situation of players occlusion. The designed target detection algorithm is applied to the football game video, which can better analyze the technical characteristics of players, promote the development of football technology, bring different viewing experiences to the audience, drive the development of economic products derived from football games, and promote the dissemination and promotion of football.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-021-04274-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06058-8,Image recognition algorithm based on artificial intelligence,Neural Computing and Applications,10.1007/s00521-021-06058-8,Springer,2022-05-01,"Convolutional neural networks also encountered some problems in the development of image recognition. The most prominent problem is that it is costly and time-consuming to collect data sets and train models. Limited data sets will cause the trained models to overfit. This paper proposes two methods to reduce overfitting based on the residual neural network architecture. The first type of method proposes a method of cross-combining waivers, reducing the size of the convolution kernel, and reducing the number of convolution kernels. The fitting method uses cross-combination to make the accuracy of Kaggle cat and dog data on the validation data set reach 95.37% and 90.31% on 30 types of engineering practice verification data set. The second method is based on the finetune residual neural network. A method of recurrent finetune residual neural network is proposed to improve the accuracy of the model. The accuracy of the finetune residual neural network on the Kaggle cat and dog validation dataset is 99.37%, and the accuracy of the dataset is verified in 30 types of engineering practice. The accuracy is 99.30%. The residual neural network method achieves 99.68% accuracy in the Kaggle cat and dog validation dataset and 99.61% in the validation dataset for 30 types of engineering practice.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06058-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11831-021-09639-x,"Machine Learning and Deep Learning Based Computational Approaches in Automatic Microorganisms Image Recognition: Methodologies, Challenges, and Developments",Archives of Computational Methods in Engineering,10.1007/s11831-021-09639-x,Springer,2022-05-01,"Microorganisms or microbes comprise majority of the diversity on earth and are extremely important to human life. They are also integral to processes in the ecosystem. The process of their recognition is highly tedious, but very much essential in microbiology to carry out different experimentation. To overcome certain challenges, machine learning techniques assist microbiologists in automating the entire process. This paper presents a systematic review of research done using machine learning (ML) and deep leaning techniques in image recognition of different microorganisms. This review investigates certain research questions to analyze the studies concerning image pre-processing, feature extraction, classification techniques, evaluation measures, methodological limitations and technical development over a period of time. In addition to this, this paper also addresses the certain challenges faced by researchers in this field. Total of 100 research publications in the chronological order of their appearance have been considered for the time period 1995–2021. This review will be extremely beneficial to the researchers due to the detailed analysis of different methodologies and comprehensive overview of effectiveness of different ML techniques being applied in microorganism image recognition field.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11831-021-09639-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12707-4,Blind stereoscopic image quality assessment using 3D saliency selected binocular perception and 3D convolutional neural network,Multimedia Tools and Applications,10.1007/s11042-022-12707-4,Springer,2022-05-01,"The purpose of stereoscopic image quality assessment (SIQA) is to design an objective evaluation algorithm to automatically evaluate the quality of stereoscopic image. In this paper, we propose a blind SIQA method via 3D saliency selected binocular perception and 3D convolutional neural network (CNN). Given a pair of stereoscopic images, we first generate 3D saliency map by weighted average of 2D saliency map and depth saliency map. Then, when the value of 3D saliency map patches is higher than the setting threshold, these patches from left and right images are selected to feed to 3D-CNN to predict the perceived quality. Finally, the score of the distorted stereoscopic image is computed by the weighted average of the quality scores of these saliency image patches. Experimental results on LIVE 3D Phase I and Phase II databases show that our proposed method is robust and competitive with the state-of-the-art NR SIQA methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12707-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00464-021-08602-y,"Does your team know how to respond safely to an operating room fire? Outcomes of a virtual reality, AI-enhanced simulation training",Surgical Endoscopy,10.1007/s00464-021-08602-y,Springer,2022-05-01,"Background Operating room (OR) fires are rare but devastating events requiring immediate and effective response. Virtual Reality (VR) simulation training can provide a safe environment for practice of skills in such highly stressful situation. This study assessed interprofessional participants’ ability to respond to VR-simulated OR fire scenarios, attitudes, numbers of attempt of the VR simulation do participants need to successfully respond to OR fires and does prior experience, confidence level, or professional role predict the number of attempts needed to demonstrate safety and pass the simulation. Methods 180 surgical team members volunteered to participate in this study at Beth Israel Deaconess Medical Center, Boston, MA. Each participant completed five VR OR simulation trials; the final two trials incorporated AI assistance. Primary outcomes were performance scores, number of attempts needed to pass, and pre- and post-survey results describing participant confidence and experiences. Differences across professional or training role were assessed using chi-square tests and analyses of variance. Differences in pass rates over time were assessed using repeated measures logistic regression. Results One hundred eighty participants completed simulation testing; 170 (94.4%) completed surveys. Participants included surgeons (17.2%), anesthesiologists (10.0%), allied health professionals (41.7%), and medical trainees (31.1%). Prior to training, 45.4% of participants reported feeling moderately or very confident in their ability to respond to an OR fire. Eight participants (4.4%) responded safely on the first simulation attempt. Forty-three participants (23.9%) passed by the third attempt (VR only); an additional 97 participants (53.9%) passed within the 4–5th attempt (VR with AI assistance). Conclusions Providers are unprepared to respond to OR fires. VR-based simulation training provides a practical platform for individuals to improve their knowledge and performance in the management of OR fires with a 79% pass rate in our study. A VR AI approach to teaching this essential skill is innovative, feasible, and effective.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00464-021-08602-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-022-01592-x,Unsupervised Multi-View CNN for Salient View Selection and 3D Interest Point Detection,International Journal of Computer Vision,10.1007/s11263-022-01592-x,Springer,2022-05-01,"We present an unsupervised 3D deep learning framework based on a ubiquitously true proposition named by us view-object consistency as it states that a 3D object and its projected 2D views always belong to the same object class. To validate its effectiveness, we design a multi-view CNN instantiating it for salient view selection and interest point detection of 3D objects, which quintessentially cannot be handled by supervised learning due to the difficulty of collecting sufficient and consistent training data. Our unsupervised multi-view CNN, namely UMVCNN, branches off two channels which encode the knowledge within each 2D view and the 3D object respectively and also exploits both intra-view and inter-view knowledge of the object. It ends with a new loss layer which formulates the view-object consistency by impelling the two channels to generate consistent classification outcomes. The UMVCNN is then integrated with a global distinction adjustment scheme to incorporate global cues into salient view selection. We evaluate our method for salient view section both qualitatively and quantitatively, demonstrating its superiority over several state-of-the-art methods. In addition, we showcase that our method can be used to select salient views of 3D scenes containing multiple objects. We also develop a method based on the UMVCNN for 3D interest point detection and conduct comparative evaluations on a publicly available benchmark, which shows that the UMVCNN is amenable to different 3D shape understanding tasks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-022-01592-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00414-021-02746-1,Forensic bone age estimation of adolescent pelvis X-rays based on two-stage convolutional neural network,International Journal of Legal Medicine,10.1007/s00414-021-02746-1,Springer,2022-05-01,"In the forensic estimation of bone age, the pelvis is important for identifying the bone age of teenagers. However, studies on this topic remain insufficient as a result of lower accuracy due to the overlapping of pelvic organs in X-ray images. Segmentation networks have been used to automate the location of key pelvic areas and minimize restrictions like doubling images of pelvic organs to increase the accuracy of estimation. This study conducted a retrospective analysis of 2164 pelvis X-ray images of Chinese Han teenagers ranging from 11 to 21 years old. Key areas of the pelvis were detected with a U-Net segmentation network, and the findings were combined with the original X-ray image for regional augmentation. Bone age estimation was conducted with the enhanced and not enhanced pelvis X-ray images by separately using three convolutional neural networks (CNNs). The root mean square errors (RMSE) of the Inception-V3, Inception-ResNet-V2, and VGG19 convolutional neural networks were 0.93 years, 1.12 years, and 1.14 years, respectively, and the mean absolute errors (MAE) of these networks were 0.67 years, 0.77 years, and 0.88 years, respectively. For comparison, a network without segmentation was employed to conduct the estimation, and it was found that the RMSE of the three CNNs above became 1.22 years, 1.25 years, and 1.63 years, respectively, and the MAE became 0.93 years, 0.96 years, and 1.23 years. Bland–Altman plots and attention maps were also generated to provide a visual comparison. The proposed segmentation network can be used to reduce the influence of restrictions like image overlapping of organs and can thus increase the accuracy of pelvic bone age estimation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00414-021-02746-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-022-01600-0,Weakly Supervised Moment Localization with Decoupled Consistent Concept Prediction,International Journal of Computer Vision,10.1007/s11263-022-01600-0,Springer,2022-05-01,"Localizing moments in a video via natural language queries is a challenging task where models are trained to identify the start and the end timestamps of the moment in a video. However, it is labor intensive to obtain the temporal endpoint annotations. In this paper, we focus on a weakly supervised setting, where the temporal endpoints of moments are not available during training. We develop a decoupled consistent concept prediction (DCCP) framework to learn the relations between videos and query texts. Specifically, the atomic objects and actions are decoupled from the query text to facilitate the recognition of these concepts in videos. We introduce a concept pairing module to temporally localize the objects and actions in the video. The classification loss and the concept consistency loss are proposed to leverage the mutual benefits of object and action cues for building relations between languages and videos. Extensive experiments on DiDeMo , Charades-STA , and ActivityNet Captions demonstrate the effectiveness of our model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-022-01600-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-022-04314-9,Exploring intelligent image recognition technology of football robot using omnidirectional vision of internet of things,The Journal of Supercomputing,10.1007/s11227-022-04314-9,Springer,2022-05-01,"With the improvement and development of intelligent robot technology, such technology is gradually used to improve the intelligence of national sports fitness. The study aims to explore the intelligent image processing methods based on football robots. First, the moving dynamic image is recognized and processed, and computer vision is proposed to simulate animal vision and collect static and dynamic images of football based on the Internet of Things (IoT). Second, a football robot platform is designed to process the collected graphic dataset. Finally, the feature vector and combination matrix of the image in the dataset are calculated, and the obtained feature matrix is input into AdaBoost to obtain the recognition result of football images. The experimental results show that the system error of the auxiliary recognition technology based on IoT is 0.63, and the detection error rate of the auxiliary recognition technology based on AdaBoost is negatively correlated with the number of features. It is concluded that the more training the samples receive, the smaller the detection error rate of the algorithm is. Compared with similar algorithms, the recognition accuracy of the designed algorithm in different datasets is more than 80.1%, and the recognition result is better than that of similar algorithms. Therefore, the algorithm designed and the results obtained prove the feasibility of the proposed intelligent recognition technology in football image recognition. This study provides a reference for the application of artificial intelligence (AI) in the field of physical fitness.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-022-04314-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-021-04302-5,An improved algorithm based on deep learning network for road image redundancy removal,The Journal of Supercomputing,10.1007/s11227-021-04302-5,Springer,2022-05-01,"Road detection is defined as one of the core technology of Advanced Driving Assistance System (ADAS), and this problem is important for improving the recognition accuracy and speed. Though much work has been done concerning road detection, the related questions about non-road areas are not thoroughly considered. Understanding the question is of primary importance in ADAS, we proposed an improved algorithm based on deep learning network for road image redundancy removal. Compared with the most typical road recognition methods, the experimental results show that the proposed method improves the speed of road recognition greatly, while ensuring the accuracy of road recognition to high level.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-021-04302-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12273-021-0837-0,CoolVox: Advanced 3D convolutional neural network models for predicting solar radiation on building facades,Building Simulation,10.1007/s12273-021-0837-0,Springer,2022-05-01,"Data-driven models have become increasingly prominent in the building, architecture, and construction industries. One area ideally suited to exploit this powerful new technology is building performance simulation. Physics-based models have traditionally been used to estimate the energy flow, air movement, and heat balance of buildings. However, physics-based models require many assumptions, significant computational power, and a considerable amount of time to output predictions. Artificial neural networks (ANNs) with prefabricated or simulated data are likely to be a more feasible option for environmental analysis conducted by designers during the early design phase. Because ANNs require fewer inputs and shorter computation times and offer superior performance and potential for data augmentation, they have received increased attention for predicting the surface solar radiation on buildings. Furthermore, ANNs can provide innovative and quick design solutions, enabling designers to receive instantaneous feedback on the effects of a proposed change to a building’s design. This research introduces deep learning methods as a means of simulating the annual radiation intensities and exposure level of buildings without the need for physics-based engines. We propose the CoolVox model to demonstrate the feasibility of using 3D convolutional neural networks to predict the surface radiation on building facades. The CoolVox model accurately predicted the radiation intensities of building facades under different boundary conditions and performed better than ARINet (with average mean square errors of 0.01 and 0.036, respectively) in predicting the radiation intensity both with (validation error = 0.0165) and without (validation error = 0.0066) the presence of boundary buildings.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12273-021-0837-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12709-2,A multi-modal approach to detect inappropriate cartoon video contents using deep learning networks,Multimedia Tools and Applications,10.1007/s11042-022-12709-2,Springer,2022-05-01,"Children are more than ever exposed to all kinds of video contents on the Internet. Consequently, several studies have proposed different techniques to detect videos that can be harmful for children. However, we note so far, that no attention has been given to the cartoon characters and the underlying language used. To address this gap, we propose to evaluate the effectiveness of using actual images of cartoon characters and the language used in cartoons in categorising videos as being appropriate or inappropriate for children. We do so through the development of a multi modal classifier, which makes use of the output from two deep learning networks: LSTM for text analysis and VGGNet for image analysis. More specifically, the LSTM network is used to process user comments and closed captions associated with a video and the VGGNet network is used to recognize cartoon image characters. The LSTM model was trained and tested on a dataset comprising about 290,000 labelled text records, while the VGGNet model was trained and tested on a manually annotated image dataset of 6000 cartoon characters. A testing accuracy of 94% was obtained for the LSTM network while a testing accuracy of 99% was obtained for the VGGNet network. Our proposed approach was further evaluated using 50 actual videos intended for children from YouTube. Here also, a good accuracy of 72% was obtained using LSTM alone, while a better accuracy of 78% was obtained using VGGNet alone and an accuracy of 76% was obtained using the combined output from the LSTM and VGGNet networks. We conclude that closed captions, user comments and images of cartoon characters are all useful in detecting unsafe videos for children and can be considered as essential parameters to include when developing multimedia filtering tools.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12709-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-021-04221-5,Research into the application of AI robots in community home leisure interaction,The Journal of Supercomputing,10.1007/s11227-021-04221-5,Springer,2022-05-01,"This paper focuses on comprehensive application of artificial intelligence robots for community-based leisure interaction. We propose a multiple-layer perceptron network to design and implement the intelligent interactive home robot system, which includes establishment of an environment map, autonomous navigation, obstacle-avoidance control and human–machine interaction, to complete the positioning and perception functions required by the robot in the home environment. With this system, community residents use an interactive interface to manipulate robots remotely and create an environmental map. In order for the robot to adapt in this changing environment, the robot needs to have a completely autonomous navigation and obstacle-avoidance-control system. In this study, a long-distance obstacle-avoidance fuzzy system and a short-distance anti-fall obstacle-avoidance fuzzy system were used to enable the robot to accommodate unforeseen changes. This technology proved itself capable of navigating a home environment, ensuring that the robot could instantaneously dodge nearby obstacles and correcting the robot’s path of travel. At the same time, it could prevent the robot from falling off a high dropping point and thereby effectively control the robot’s movement trajectory. After combining the above-mentioned multi-sensor and image recognition functions, the intelligent interactive home robot showed that it clearly has the ability to integrate vision, perception and interaction, and we were able to verify that the robot has the necessary adaptability in changing environments and that the design of such interactive robots can be an asset in the home.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-021-04221-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00330-021-08408-5,Combined model-based and deep learning-based automated 3D zonal segmentation of the prostate on T2-weighted MR images: clinical evaluation,European Radiology,10.1007/s00330-021-08408-5,Springer,2022-05-01,"Objective To train and to test for prostate zonal segmentation an existing algorithm already trained for whole-gland segmentation. Methods The algorithm, combining model-based and deep learning–based approaches, was trained for zonal segmentation using the NCI-ISBI-2013 dataset and 70 T2-weighted datasets acquired at an academic centre. Test datasets were randomly selected among examinations performed at this centre on one of two scanners (General Electric, 1.5 T; Philips, 3 T) not used for training. Automated segmentations were corrected by two independent radiologists. When segmentation was initiated outside the prostate, images were cropped and segmentation repeated. Factors influencing the algorithm’s mean Dice similarity coefficient (DSC) and its precision were assessed using beta regression. Results Eighty-two test datasets were selected; one was excluded. In 13/81 datasets, segmentation started outside the prostate, but zonal segmentation was possible after image cropping. Depending on the radiologist chosen as reference, algorithm’s median DSCs were 96.4/97.4%, 91.8/93.0% and 79.9/89.6% for whole-gland, central gland and anterior fibromuscular stroma (AFMS) segmentations, respectively. DSCs comparing radiologists’ delineations were 95.8%, 93.6% and 81.7%, respectively. For all segmentation tasks, the scanner used for imaging significantly influenced the mean DSC and its precision, and the mean DSC was significantly lower in cases with initial segmentation outside the prostate. For central gland segmentation, the mean DSC was also significantly lower in larger prostates. The radiologist chosen as reference had no significant impact, except for AFMS segmentation. Conclusions The algorithm performance fell within the range of inter-reader variability but remained significantly impacted by the scanner used for imaging. Key Points • Median Dice similarity coefficients obtained by the algorithm fell within human inter-reader variability for the three segmentation tasks (whole gland, central gland, anterior fibromuscular stroma) . • The scanner used for imaging significantly impacted the performance of the automated segmentation for the three segmentation tasks . • The performance of the automated segmentation of the anterior fibromuscular stroma was highly variable across patients and showed also high variability across the two radiologists .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00330-021-08408-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11831-021-09647-x,Survey on Machine Learning in Speech Emotion Recognition and Vision Systems Using a Recurrent Neural Network (RNN),Archives of Computational Methods in Engineering,10.1007/s11831-021-09647-x,Springer,2022-05-01,"This is a survey paper that aims to give reviews about that finest architectures of machine learning, the use of algorithms and the applications of the system and speech and vision processes. The current technology poses vast areas of research in the field of architectures and algorithms which are used in machine learning, they can further be used for the planning new ideas and recreation of speech system and vision systems intelligently. The level of personal computing and its commercialization is at an all-time high. The machine learning can used for learning and training using large sets of sensor data and computing through clouds, also not to forget the mobile and embedded technology which is even more sophisticated. The survey is presented by giving a detailed background and the evolving nature of the most used models of deep learning which are used effectively in the vision and speech systems. The survey aims to give a perspective into the large scale research at the industrial level, it also highlights the efforts taken for future focus and upcoming demands of the intelligent use of speech and vision processes. The demand in a strong, robust and high intelligent system is that of lower latency and fidelity to be high. This is mostly seen in hardware devices with limited resources like automobiles, robots and mobile phones. These points are kept in mind and a detail of the major challenges and success rates especially in machine learning with platforms that have limited resources. The restrictions are in memory, life of the battery and the capacity of processing. The conclusion is based on the applications which are fast emerging based on the usage of speech and vision systems. Examples include effective evaluation, smart and quick system transportation and correct medicine prescriptions. This paper promise to deliver a comprehensive survey emphasizing on the demands of speech and vision systems with the view of both hardware and software systems. The technologies which are discussed in machine learning are fast gaining access and aim to revolutionize the areas of research and development in speech and vision systems.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11831-021-09647-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00264-022-05311-6,Subtalar axis determined by combining digital twins and artificial intelligence: influence of the orientation of this axis for hindfoot compensation of varus and valgus knees,International Orthopaedics,10.1007/s00264-022-05311-6,Springer,2022-05-01,"Purpose Previous studies evaluating hindfoot and knee alignment have suggested compensation between the knee and the hindfoot deformities. However, these studies did not investigate the influence of the orientation of the subtalar axis on the results. Material and methods Using computed tomography data of patients without osteoarthritis, digital twins, and artificial intelligence, we identified the orientation of the axis of the subtalar joint. Compensation was evaluated in the subtalar joint according to angular knee deformity and subtalar axis direction. Results With the inclination angle defined as the angle between the axis and the XY plane (horizontal) and the deviation angle defined as the angle between the projection of axis on the XZ plane, the inclination angle of the subtalar helical axis showed an average angle of 35.3° (range 5° to 48°). The mean deviation angle for the helical axis was 6.4° (range − 4° to + 12°). Our findings indicated that an increase of the inclination angle of the subtalar axis tends to limit adjustment in the hindfoot alignment toward re-balance of the whole lower limb toward a neutral weight-bearing axis when malalignment of the knee occurs. Conclusion Malalignment of the knee and different compensations in the hindfoot contribute to various combined deformities in the population: associated valgus or varus deformities and inverse associations of varus/valgus deformities.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00264-022-05311-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06781-2,InstaIndoor and multi-modal deep learning for indoor scene recognition,Neural Computing and Applications,10.1007/s00521-021-06781-2,Springer,2022-05-01,"Indoor scene recognition is a growing field with great potential for behaviour understanding, robot localization, and elderly monitoring, among others. In this study, we approach the task of scene recognition from a novel standpoint, using multi-modal learning and video data gathered from social media. The accessibility and variety of social media videos can provide realistic data for modern scene recognition techniques and applications. We propose a model based on fusion of transcribed speech to text and visual features, which is used for classification on a novel dataset of social media videos of indoor scenes named InstaIndoor. Our model achieves up to 70% accuracy and 0.7 F1-Score. Furthermore, we highlight the potential of our approach by benchmarking on a YouTube-8M subset of indoor scenes as well, where it achieves 74% accuracy and 0.74 F1-Score. We hope the contributions of this work pave the way to novel research in the challenging field of indoor scene recognition.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06781-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-021-04299-x,Deeply feature fused video super-resolution network using temporal grouping,The Journal of Supercomputing,10.1007/s11227-021-04299-x,Springer,2022-05-01,"The video super-resolution (VSR) task refers to the use of corresponding low-resolution frames and multiple neighboring frames to generate high-resolution (HR) frames. An important step in VSR is to fuse the features of the reference frame with the features of the supporting frame. Existing VSR methods do not take full advantage of the information provided by distant neighboring frames and usually fuse the information in a one-stage manner. In this paper, we propose a deep fusion video super-resolution network based on temporal grouping. We divide the input sequence into groups according to different frame rates to provide more accurate supplementary information. Our method aggregates temporal-spatial information at different fusion stages. Firstly, we group the input sequence. Then the temporal-spatial information is extracted and fused hierarchically, and these groups are used to recover the information lost in the reference frame. Secondly, integrate information within each group to generate group-wise features, and then perform multi-stage fusion. The information of the reference frame is fully utilized, resulting in a better recovery of video details. Finally, the upsampling module is used to generate HR frames. We conduct a comprehensive comparative experiment on Vid4, SPMC-11 and Vimeo-90K-T datasets. The results show that the proposed method achieves good performance compared with state-of-the-art methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-021-04299-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12558-z,Fast coding unit size decision based on deep reinforcement learning for versatile video coding,Multimedia Tools and Applications,10.1007/s11042-022-12558-z,Springer,2022-05-01,"Video coding has long been looking for a more available approach than the greedy method. The quad-tree with nested multi-type tree (QTMT) structure including quad-tree (QT) and multi-type tree (MTT) results in highly coding complexity in Versatile Video Coding (VVC). In addition, the rapid progress in deep learning (DL) is attracting increasing attention in the video coding community. Therefore, this paper proposes a fast Coding Unit (CU) splitting decision method based on Deep reinforcement learning (DRL) for VVC to decrease the coding complexity. Specifically, the 32 × 32 CU for splitting is considered as a Markov decision process (MDP), the CU splitting situations at a certain node as state, the splitting modes decision as actions, the reduction or increase in rate-distortion (RD) cost as the immediate rewards or punishments, and the encoder as an agent to make coding decisions successively. The simulation results demonstrate that the coding time reduction (CTR) of the proposed approach can lead to a reduction of about 54.38% while maintaining coding performance, which can realize a trade-off between the complexity reduction and coding efficiency.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12558-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02103-8,3D lidar point-cloud projection operator and transfer machine learning for effective road surface features detection and segmentation,The Visual Computer,10.1007/s00371-021-02103-8,Springer,2022-05-01,"The classification and extraction of road markings and lanes are of critical importance to infrastructure assessment, planning and road safety. We present a pipeline for the accurate segmentation and extraction of rural road surface objects in 3D lidar point-cloud, as well as a method to extract geometric parameters belonging to tar seal. To decrease the computational resources needed, the point-clouds were aggregated into a 2D image space before being transformed using affine transformations. The Mask R-CNN algorithm is then applied to the transformed image space to localize, segment and classify the road objects. The segmentation results for road surfaces and markings can then be used for geometric parameter estimation such as road widths estimation, while the segmentation results show that the efficacy of the existing Mask R-CNN to segment needle-type objects is improved by our proposed transformations.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02103-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12769-4,Significant full reference image segmentation evaluation: a survey in remote sensing field,Multimedia Tools and Applications,10.1007/s11042-022-12769-4,Springer,2022-05-01,"Image segmentation is a crucial step in remote sensing application, as it breaks down a larger image into smaller chunks, which contains useful information. Although there are several image segmentation algorithms available, evaluation of the algorithms is challenging. Furthermore, the evaluation of image segmentation can elucidate the best image segmentation algorithm for a single image or a group of image or a whole class of image. This paper explores and evaluates the benefits and the drawbacks of various qualitative and quantitative image segmentation evaluation metrics used in remote sensing applications. For all the metrics, a quantitative set of values for good and bad segmentation is provided. In addition, some image segmentation algorithms such as Multi Otsu Threshold, K Means clustering, Fuzzy C Means clustering, Improved K Means clustering (IFCM), Improved Fuzzy C Means clustering, Naïve Bayes classifier, K Nearest Neighbor, Decision Tree (DT) and Random Forest classifier are used in the experimental comparison of metrics. The qualitative and quantitative satellite image segmentation evaluation using the mentioned algorithms is measured. The results are analyzed to strengthen the impact of different metrics on the segmentation algorithms. In both qualitative and quantitative analysis, the IFCM outperformed the other unsupervised machine learning algorithms and the DT outperformed the other supervised machine learning algorithms. The effectiveness of the provided metrics in the remote sensing field is validated.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12769-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1631/FITEE.2000414,Shot classification and replay detection for sports video summarization,Frontiers of Information Technology & Electronic Engineering,10.1631/FITEE.2000414,Springer,2022-05-01,"由于摄像机、回放速度、光照条件、剪辑效果、比赛结构和类型等方面的差异, 体育视频摘要的自动分析具有挑战性。为了解决这些问题, 针对户外运动视频, 本文提出一种基于镜头分类和回放检测的有效视频摘要框架。准确的镜头分类对于更好地安排输入视频从而进行进一步处理 (关键事件或回放检测) 是必要的。因此, 提出一种基于轻量级卷积神经网络的镜头分类方法。该方法对每一个镜头进行回放检测; 特别地, 检测出从体育视频中识别出回放片段的连续标识转换帧。为此, 提出局部八元模式特征来表示视频帧, 并训练极限学习机分为回放或非回放两类。所提框架对于摄像机、回放速度、镜头速度、光照条件、比赛结构、运动类型、广播公司、标识设计和位置、帧转换和剪辑效果具有鲁棒性。基于YouTube体育视频集中的足球、棒球和板球运动对所提框架进行性能评估。实验结果证明所提框架能够可靠地用于户外运动视频摘要的镜头分类和回放检测。 Automated analysis of sports video summarization is challenging due to variations in cameras, replay speed, illumination conditions, editing effects, game structure, genre, etc. To address these challenges, we propose an effective video summarization framework based on shot classification and replay detection for field sports videos. Accurate shot classification is mandatory to better structure the input video for further processing, i.e., key events or replay detection. Therefore, we present a lightweight convolutional neural network based method for shot classification. Then we analyze each shot for replay detection and specifically detect the successive batch of logo transition frames that identify the replay segments from the sports videos. For this purpose, we propose local octa-pattern features to represent video frames and train the extreme learning machine for classification as replay or non-replay frames. The proposed framework is robust to variations in cameras, replay speed, shot speed, illumination conditions, game structure, sports genre, broadcasters, logo designs and placement, frame transitions, and editing effects. The performance of our framework is evaluated on a dataset containing diverse YouTube sports videos of soccer, baseball, and cricket. Experimental results demonstrate that the proposed framework can reliably be used for shot classification and replay detection to summarize field sports videos.",http://link.springer.com/openurl/fulltext?id=doi:10.1631/FITEE.2000414,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10115-022-01669-6,Multimedia ontology population through semantic analysis and hierarchical deep features extraction techniques,Knowledge and Information Systems,10.1007/s10115-022-01669-6,Springer,2022-05-01,"The rapid increase of available data in different complex contexts needs automatic tasks to manage and process contents. Semantic Web technologies represent the silver bullet in the digital Internet ecosystem to allow human and machine cooperation in achieving these goals. Specific technologies as ontologies are standard conceptual representations of this view. It aims to transform data into an interoperability format providing a common vocabulary for a given domain and defining, with different levels of formality, the meaning of informative objects and their possible relationships. In this work, we focus our attention on Ontology Population in the multimedia realm. An automatic and multi-modality framework for images ontology population is proposed and implemented. It allows the enrichment of a multimedia ontology with new informative content. Our multi-modality approach combines textual and visual information through natural language processing techniques, and convolutional neural network used the features extraction task. It is based on a hierarchical methodology using images descriptors and semantic ontology levels. The results evaluation shows the effectiveness of our proposed approach.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10115-022-01669-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12559-020-09817-2,SETransformer: Speech Enhancement Transformer,Cognitive Computation,10.1007/s12559-020-09817-2,Springer,2022-05-01,"Speech enhancement is a fundamental way to improve speech perception quality in adverse environment where the received speech is seriously corrupted by noise. In this paper, we propose a cognitive computing based speech enhancement model termed SETransformer which can improve the speech quality in unkown noisy environments. The proposed SETransformer takes advantages of LSTM and multi-head attention mechanism, both of which are inspired by the auditory perception principle of human beings. Specifically, the SETransformer pocesses the ability of characterizing the local structure implicated in the speech spectrum and has more lower computation complexity due to its distinctive parallelization perfermance. Experimental results show that, compared with the standard Transformer and the LSTM model, the proposed SETransformer model can consistently achieve better denoising performance in terms of speech quality (PESQ) and speech intelligibility (STOI) under unseen noise conditions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12559-020-09817-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12112-x,Speech Emotion Recognition using Time Distributed 2D-Convolution layers for CAPSULENETS,Multimedia Tools and Applications,10.1007/s11042-022-12112-x,Springer,2022-05-01,"Speech Emotion Recognition (SER) determines human emotions using linguistic and nonlinguistic features of the uttered speech. The nonlinguistic process is more suitable for applications where language is not a concern. In this paper, Capsule Network (CapsuleNets) with a combination of Time Distributed 2D-Convolution layers is used for classifying emotions using speech signals. CapsuleNets are specially designed to capture the spatial cues of the data but fail in considering temporal cues in time series data like speech. In order to capture the temporal cues, along with spatial cues, Time distributed 2D- convolution neural layers are introduced before the CapsuleNets. Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) and Interactive Emotional Dyadic Motion Capture (IEMOCAP) speech data sets are used for experimenting with the proposed network architecture. The log-mel spectrogram of the speech samples is extracted and used for training and testing of the proposed model. The combination of CapsuleNets with Time Distributed 2D-Convolution layers has achieved a classification accuracy of 92.6% on the RAVDESS dataset and 93.2% on the IEMOCAP dataset. These results are compared with the plain CapsuleNets model, and remarkable improvement is observed. Also, the proposed system has outperformed the existing models on the mentioned benchmarked datasets. The confusion matrix shows consistent improvement in the accuracy of every emotion, including sad and disgust in RAVDESS and angry in IEMOCAP, which are poorly classified by classifiers such as variants in CNN, RNN, LSTM.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12112-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40799-022-00577-2,Multi-Axis 3D Printing Defect Detecting by Machine Vision with Convolutional Neural Networks,Experimental Techniques,10.1007/s40799-022-00577-2,Springer,2022-04-29,"Part quality inspection is playing a critical role in Fused Deposition Modeling (FDM). It produces a part quality analysis report which can be adopted to further improve the overall part quality. However, the part quality inspection process puts heavy reliance on the engineer’s background and experience. This manual process suffers from both low efficiency and potential errors. The purpose of the paper is to look into a deep neural network towards a robust method for montoring of 3D priting Parts. However, there is little research on the detection of multi-axis FDM printing. Aiming at FDM printing defects, an offline detection method of multi-axis FDM printing based on machine vision and convolutional neural network (CNN) is proposed. The CCD (Charge Coupled Device) camera is used to capture the current layer, and the self-constructed CNN network is used to classify the defects. The interlayer stripping defect is taken as an example to verify the detection method’s feasibility. The data set of the defect is established, used as CNN’s input. The network’s structure is continuously improved through training and testing. Finally, the CNN with excellent performance is constructed. The classification results show that the CNN network’s classification accuracy for interlayer stripping and normal state is 83.1%, the sensitivity is 85.6% and 80.5% and the ROC curve area is 0.824, which proves the feasibility of the detection method. Moreover, the method can be extended to other print defect detection. It provides a new method to ensure the quality of FDM prints.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40799-022-00577-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07267-5,Correction to: Research on simulation of 3D human animation vision technology based on an enhanced machine learning algorithm,Neural Computing and Applications,10.1007/s00521-022-07267-5,Springer,2022-04-28,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07267-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-022-03529-w,Spatiotemporal tubelet feature aggregation and object linking for small object detection in videos,Applied Intelligence,10.1007/s10489-022-03529-w,Springer,2022-04-26,"This paper addresses the problem of exploiting spatiotemporal information to improve small object detection precision in video. We propose a two-stage object detector called FANet based on short-term spatiotemporal feature aggregation and long-term object linking to refine object detections. First, we generate a set of short tubelet proposals. Then, we aggregate RoI pooled deep features throughout the tubelet using a new temporal pooling operator that summarizes the information with a fixed output size independent of the tubelet length. In addition, we define a double head implementation that we feed with spatiotemporal information for spatiotemporal classification and with spatial information for object localization and spatial classification. Finally, a long-term linking method builds long tubes with the previously calculated short tubelets to overcome detection errors. The association strategy addresses the generally low overlap between instances of small objects in consecutive frames by reducing the influence of the overlap in the final linking score. We evaluated our model in three different datasets with small objects, outperforming previous state-of-the-art spatiotemporal object detectors and our spatial baseline.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-022-03529-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41746-022-00596-8,Continuous monitoring of surgical bimanual expertise using deep neural networks in virtual reality simulation,npj Digital Medicine,10.1038/s41746-022-00596-8,Nature,2022-04-26,"In procedural-based medicine, the technical ability can be a critical determinant of patient outcomes. Psychomotor performance occurs in real-time, hence a continuous assessment is necessary to provide action-oriented feedback and error avoidance guidance. We outline a deep learning application, the Intelligent Continuous Expertise Monitoring System (ICEMS), to assess surgical bimanual performance at 0.2-s intervals. A long-short term memory network was built using neurosurgeon and student performance in 156 virtually simulated tumor resection tasks. Algorithm predictive ability was tested separately on 144 procedures by scoring the performance of neurosurgical trainees who are at different training stages. The ICEMS successfully differentiated between neurosurgeons, senior trainees, junior trainees, and students. Trainee average performance score correlated with the year of training in neurosurgery. Furthermore, coaching and risk assessment for critical metrics were demonstrated. This work presents a comprehensive technical skill monitoring system with predictive validation throughout surgical residency training, with the ability to detect errors.",https://www.nature.com/articles/s41746-022-00596-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07249-7,Audiovisual speech recognition for Kannada language using feed forward neural network,Neural Computing and Applications,10.1007/s00521-022-07249-7,Springer,2022-04-25,"Audiovisual speech recognition is one of the promising technologies in a noisy environment. In this work, we develop the database for Kannada Language and develop an AVSR system for the same. The proposed work is categorized into three main components: a. Audio mechanism. b. Visual speech mechanism. c. Integration of audio and visual mechanisms. In the audio model, MFCC is used to extract the features and a one-dimensional convolutional neural network is used for classification. In the visual module, Dlib is used to extract the features and long short-term memory recurrent neural network is used for classification. Finally, integration of audio and visual module is done using feed forward neural network. Audio speech recognition of Kannada dataset training accuracy achieved is 93.86 and 91.07% for testing data using seventy epochs. Visual speech recognition for Kannada dataset training accuracy is 77.57%, and testing accuracy is 75%. After integration, audiovisual speech recognition for Kannada dataset train accuracy is 93.33% and for testing is 92.26%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07249-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s43681-022-00158-4,Agency in augmented reality: exploring the ethics of Facebook’s AI-powered predictive recommendation system,AI and Ethics,10.1007/s43681-022-00158-4,Springer,2022-04-25,"The development of predictive algorithms for personalized recommendations that prioritize ads, filter content, and tailor our decision-making processes will increasingly impact our society in the upcoming years. One example of what this future might hold was recently presented by Facebook Reality Labs (FRL) who work on augmented reality (AR) glasses powered by contextually aware AI that allows the user to “communicate, navigate, learn, share, and take action in the world” (Facebook Reality Labs 2021). A major feature of those glasses is “the intelligent click” that presents action prompts to the user based on their personal history and previous choices. The user can accept or decline those suggested action prompts depending on individual preferences. Facebook/Meta presents this technology as a gateway to “increased agency”. However, Facebook’s claim presumes a simplistic view of agency according to which our agentive capacities increase parallel to the ease in which our actions are carried out. Technologies that structure people’s lives need to be based on a deeper understanding of agency that serves as the conceptual basis in which predictive algorithms are developed. With the goal of mapping this emerging terrain, the aim of this paper is to offer a thorough analysis of the agency-limiting risks and the agency-enhancing potentials of Facebook’s “intelligent click” feature. Based on a concept of agency by Dignum (Responsible Artificial Intelligence: How to Develop and Use AI in a Responsible Way. Springer International Publishing, Cham, 2019), the three agential dimensions of autonomy (acting independently), adaptability (reacting to changes in the environment), and interactivity (interacting with other agents) are analyzed towards our ability to make self-determining choices.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s43681-022-00158-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-08179-z,Artificial intelligence deep learning for 3D IC reliability prediction,Scientific Reports,10.1038/s41598-022-08179-z,Nature,2022-04-25,"Three-dimensional integrated circuit (3D IC) technologies have been receiving much attention recently due to the near-ending of Moore’s law of minimization in 2D IC. However, the reliability of 3D IC, which is greatly influenced by voids and failure in interconnects during the fabrication processes, typically requires slow testing and relies on human’s judgement. Thus, the growing demand for 3D IC has generated considerable attention on the importance of reliability analysis and failure prediction. This research conducts 3D X-ray tomographic images combining with AI deep learning based on a convolutional neural network (CNN) for non-destructive analysis of solder interconnects. By training the AI machine using a reliable database of collected images, the AI can quickly detect and predict the interconnect operational faults of solder joints with an accuracy of up to 89.9% based on non-destructive 3D X-ray tomographic images. The important features which determine the “Good” or “Failure” condition for a reflowed microbump, such as area loss percentage at the middle cross-section, are also revealed.",https://www.nature.com/articles/s41598-022-08179-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10846-022-01603-6,"Continual Learning for Real-World Autonomous Systems: Algorithms, Challenges and Frameworks",Journal of Intelligent & Robotic Systems,10.1007/s10846-022-01603-6,Springer,2022-04-23,"Continual learning is essential for all real-world applications, as frozen pre-trained models cannot effectively deal with non-stationary data distributions. The purpose of this study is to review the state-of-the-art methods that allow continuous learning of computational models over time. We primarily focus on the learning algorithms that perform continuous learning in an online fashion from considerably large (or infinite) sequential data and require substantially low computational and memory resources. We critically analyze the key challenges associated with continual learning for autonomous real-world systems and compare current methods in terms of computations, memory, and network/model complexity. We also briefly describe the implementations of continuous learning algorithms under three main autonomous systems, i.e., self-driving vehicles, unmanned aerial vehicles, and urban robots. The learning methods of these autonomous systems and their strengths and limitations are extensively explored in this article.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10846-022-01603-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12125-6,Rigid and non-rigid 3D shape classification based on 3D Hahn moments neural networks model,Multimedia Tools and Applications,10.1007/s11042-022-12125-6,Springer,2022-04-23,"With the rapid development of 3D technology, 3D object classification has recently become a popular research topic in computer graphics and pattern recognition. Analyzing the shape of 3D models has become a common concern for researchers. A crucial technique for analyzing these 3D shapes is to capture features from 3D models that can sufficiently distinguish their shapes. While the 3D discrete orthogonal moments are powerful for the analysis and representation of 3D objects. It was proved that they have a better feature representation capability than the continuous orthogonal moments. This paper presents a new model for 3D shape classification named 3D Hahn moments neural networks (3DHMNNs) to improve the classification accuracy and decrease the computational complexity of 3D pattern recognition process. The proposed model is derived by introducing 3D Hahn moments as an input layer in neural network, largely utilized in various applications related to the pattern recognition. In fact, the advantages of image moments, especially discrete orthogonal moments have the property to extract relevant features from an object in lower orders and their robustness against image noise. Along with the efficiency of the neural networks, we can construct the proposed robust model. The main purpose of this work is to investigate the classification capabilities of the proposed model on rigid and non-rigid 3D shape datasets. The experiment simulations are conducted on the Articulated of McGill Princeton Shape Benchmark (PSB), SHREC’2011, ModelNet10 and ModelNet40 databases to assess the effectiveness of our proposed model. Two structures of the proposed model are constructed as noted 3 DHMNNs _6  layers  and 3 DHMNNs _7  layers . The obtained results indicate that the proposed model provide a reasonable performance in terms of classification accuracy by achieving 90.96% on ModelNet10, 83.87% on ModelNet40, 97.66% on SHREC’2011 and over 90.16% on Articulated McGill outperforming other existing methods as well as being more robust under noisy conditions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12125-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-022-03516-1,Staged cascaded network for monocular 3D human pose estimation,Applied Intelligence,10.1007/s10489-022-03516-1,Springer,2022-04-23,"The study of deep end-to-end representation learning for 2D to 3D monocular human pose estimation is a common yet challenging task in computer vision. However, current methods still face the problem that the recognized 3D key points are inconsistent with the actual joint positions. The strategy that trains 2D to 3D networks using 3D human poses with corresponding 2D projections to solve this problem is effective. On this basis, we build a cascaded monocular 3D human pose estimation network, which uses a hierarchical supervision network, and uses the proposed composite residual module (CRM) and enhanced fusion module (EFM) as the main components. In the cascaded network, CRMs are stacked to form cascaded modules. Compared with the traditional residual module, the proposed CRM expands the information flow channels. In addition, the proposed EFM is alternately placed with cascaded modules, which addresses the problems of reduced accuracy and low robustness caused by multi-level cascade. We test the proposed network on the standard benchmark Human3.6M dataset and MPI-INF-3DHP dataset. We compare the results under the fully-supervised methods with six algorithms and the results under the weakly-supervised methods with five algorithms. We use the mean per joint position error (MPJPE) in millimeters as the evaluation index and get the best results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-022-03516-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13169-4,Real time violence detection in surveillance videos using Convolutional Neural Networks,Multimedia Tools and Applications,10.1007/s11042-022-13169-4,Springer,2022-04-23,"Real-time violence detection with the use of surveillance is the process of using live videos to detect violent and irregular behavior. In organizations, they use some potential procedures for recognition the activity in which normal and abnormal activities can be found easily. In this research, multiple key challenges have been oncorporated with the existing work and the proposed work contrast. Firstly, violent objects can’t be defined manually and then the system needs to deal with the uncertainty. The second step is the availability of label dataset because manually annotation video is an expensive and labor-intensive task. There is no such approach for violence detection with low computation and high accuracy in surveillance environments so far. The Convolutional Neural Network’s (CNN) models have been evaluated with the proposed MobileNet model. The MobileNet model has been contrasted with AlexNet, VGG-16, and GoogleNet models. The simulations have been executed using Python from which the accuracy of AlexNet is 88.99 and the loss is 2.480 (%). The accuracy of VGG-16 is 96.49 and loss is 0.1669, the accuracy of GoogleNet is 94.99 and loss is 2.92416 (%). The proposed MobileNet model accuracy is 96.66 and loss is 0.1329 (%). The proposed MobileNet model has shown outstanding performance in the perspective of accuracy, loss, and computation time on the hockey fight dataset.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13169-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-022-02204-4,Fast multi-type tree partitioning for versatile video coding using machine learning,"Signal, Image and Video Processing",10.1007/s11760-022-02204-4,Springer,2022-04-22,"The emerging versatile video coding (VVC) standard adopted an innovated multi-type tree (MTT) versatile block structure, comprising binary trees (BTs) and ternary trees (TTs) pruning. This new tree structures’ flexibility, induced by the MTT module, significantly improved the compression performance. However, it dramatically increased the coding complexity due to the brute force search for rate distortion optimization (RDO). To cope with this issue, we proposed a fast decision approach using a lightweight neural network (LNN) with an early direction determination scheme to avoid redundant MTT pruning and hence, reduced considerable computing complexity. The I-frame processing significantly affected the coding efficiency. Thus, the main goal of the suggested LNN-based approach is to substitute the brute force RDO search, used to check all block decision candidates, without affecting the compression efficiency performance. Based on the BT RD cost, the TT splitting direction was selected in a first step. Subsequently, an adequate LNN-based model was applied to predict the corresponding VVC TT partition, which deeply optimized the VVC coding unit partition module. Experiments over various test sequences showed that the proposed method substantially decreased the total encoding time by up to 46% with negligible compression efficiency loss under the all-intra configuration.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-022-02204-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12796-1,LSTM model for visual speech recognition through facial expressions,Multimedia Tools and Applications,10.1007/s11042-022-12796-1,Springer,2022-04-21,"Hearing impaired persons are more expressive while speaking and expression is a salient feature in hearing impaired Visual Speech Recognition. Most Visual Speech Recognition systems focus only on the lip area for the recognition of speech or speaker. This work utilizes video data which includes information from both speech and facial expressions. As part of this study, we have developed a Malayalam audio-visual speech expression database of unimpaired people. The experiments were conducted on this newly developed Malayalam audio-visual speech database. The data has been collected from two people, 1 male, and 1 female. A combination of Convolutional Neural Network-Long Short Term Memory deep learning video processing model is applied for this system. The result demonstrate that, the classification accuracy is better for the features extracted using GoogleNet model compared to AlexNet and ResNet model. The system evaluation is carried out in both Speaker-dependent and speaker-independent domains. The recognition rate of the system for both speaker-dependent and speaker-independent experiments proves that facial expression analysis plays a crucial role in Visual Speech Recognition.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12796-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-10449-9,Deep learning-enabled mobile application for efficient and robust herb image recognition,Scientific Reports,10.1038/s41598-022-10449-9,Nature,2022-04-21,"With the increasing popularity of herbal medicine, high standards of the high quality control of herbs becomes a necessity, with the herb recognition as one of the great challenges. Due to the complicated processing procedure of the herbs, methods of manual recognition that require chemical materials and expert knowledge, such as fingerprint and experience, have been used. Automatic methods can partially alleviate the problem by deep learning based herb image recognition, but most studies require powerful and expensive computation hardware, which is not friendly to resource-limited settings. In this paper, we introduce a deep learning-enabled mobile application which can run entirely on common low-cost smartphones for efficient and robust herb image recognition with a quite competitive recognition accuracy in resource-limited situations. We hope this application can make contributions to the increasing accessibility of herbal medicine worldwide.",https://www.nature.com/articles/s41598-022-10449-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13025-5,Face tracking and recognition in video moving images based on convolutional neural network,Multimedia Tools and Applications,10.1007/s11042-022-13025-5,Springer,2022-04-21,"In order to accurately track and recognize faces in video moving images, a method for tracking and recognizing faces in video moving images based on convolutional neural networks is proposed. Establish feature maps through convolutional neural networks, extract deep convolutional features of face targets in video moving images, build a multi-feature fusion likelihood model of face targets in video moving images, determine the importance of features and update face targets State, according to the size of the weight, adaptively select a reasonable feature set to describe the current state of the target, so as to realize the tracking and recognition of the face target in the video moving image. The proposed method has good tracking results for face targets in video moving images, and can accurately track face targets in video moving images, and under occlusion conditions, the tracking results have low mean square error and average absolute error. Below 0.02, the tracking accuracy is higher and the recognition time is shorter, indicating that the convolutional neural network has a better application effect in face tracking and recognition in video moving images.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13025-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12466-2,Cloud based secure multimedia medical data using optimized convolutional neural network and cryptography mechanism,Multimedia Tools and Applications,10.1007/s11042-022-12466-2,Springer,2022-04-21,"Cloud storage system (CSS) is a significant service of cloud computing that permits the data owner to store their data in the cloud. Though the system is efficient, the system needs to provide privacy and security to the users. This paper presents a solution for securing multimedia data in the cloud using different techniques. The major phases of the proposed system are the data classification and data authentication phases. Initially, the acquired data is classified based on its sensitivity using the proposed optimized convolutional neural network (CNN-EEO). Then, the key pair is generated and encrypted using the proposed infinite elliptic curve cryptography with Merkle hash digest algorithm (IECC-MHDA). Finally, the classified data is encrypted using the proposed Kernel Homomorphic chaos encryption algorithm (KHCEA), which is decrypted at the user end. The simulation of the proposed scheme proved its excellence against the other existing schemes in terms of major metrics.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12466-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-022-02227-x,FaceMD: convolutional neural network-based spatiotemporal fusion facial manipulation detection,"Signal, Image and Video Processing",10.1007/s11760-022-02227-x,Springer,2022-04-21,"Digital videos have become essential to broadcast news that targets many audiences around the world, and it is therefore important to ensure the reliability of these broadcasted videos. Unfortunately, digital videos can be manipulated by replacing a person’s face or expressions with another person’s face or expressions without leaving visible traces. This facial manipulation is a challenging problem due to the lack of digital forensic techniques that can be used to verify the originality of video content. In this paper, we propose a novel approach, dubbed FaceMD, based on fusing three streams of convolutional neural networks to detect facial manipulation. The proposed FaceMD incorporates spatiotemporal information by fusing video frames, motion residuals, and 3D gradients to improve facial manipulation detection accuracy. We combine these three streams using different fusion methods and places to best use this spatiotemporal information, hence increasing detection performance. The experimental results show that the proposed FaceMD achieves state-of-the-art accuracy using two different facial manipulation data sets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-022-02227-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00500-022-07074-z,A rough set theory and deep learning-based predictive system for gender recognition using audio speech,Soft Computing,10.1007/s00500-022-07074-z,Springer,2022-04-20,"Speech is one of the most delicate medium through which gender of the speakers can easily be identified. Though the related research has shown very good progress in machine learning, but recently, deep learning has imparted a very good research area to explore the deficiency of gender discrimination using traditional machine learning techniques. In deep learning techniques, the speech features are automatically generated by the reinforcement learning from the raw data which have more discriminating power than the human-generated features. But in some practical situations like gender recognition, it is observed that combination of both types of features sometimes provides comparatively better performance. In the proposed work, we have initially extracted and selected some informative and precise acoustic features relevant to gender recognition using entropy-based information theory and Rough Set Theory (RST). Next, the audio speech signals are directly fed into the deep neural network model consisting of Convolution Neural Network (CNN) and Gated Recurrent Unit network (GRUN) for extracting features useful for gender recognition. The RST selects precise and informative features, CNN extracts the locally encoded important features, and GRUN reduces the vanishing gradient and exploding gradient problems. Finally, a hybrid gender recognition system is developed combining both generated feature vectors. The developed model has been tested with five bench mark and a simulated dataset to evaluate its performance, and it is observed that combined feature vector provides more effective gender recognition system specially when transgender is considered as a gender type together with male and female.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-022-07074-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13154-x,Implementation of smart social distancing for COVID-19 based on deep learning algorithm,Multimedia Tools and Applications,10.1007/s11042-022-13154-x,Springer,2022-04-20,"The first step to reducing the effect of viral disease is to prevent the spread which could be achieved by implementing social distancing (reducing the number of close physical interactions between peoples). Almost every viral disease whose means of communication is air, and enters through mouth or nose, definitely will affect our vocal organs which cause changes in features of our voice and could be traceable using feature analysis of voice using deep learning. The detection of an affected person using deep neural networks and tracking him would help us in the implementation of the social distancing rule in an area where it is needed. The aim of this paper is to study different solutions which help in enabling, encouraging, and even enforcing social distancing. In this paper, we implemented and analyzed scenarios on the basis of COVID-19 patient detection using cough and tracking him using smart cameras, or emerging wireless technologies with deep learning techniques for prediction and preventing the spread of disease. Thus these techniques are easy to be implemented in the initial stage of any pandemic as well and will help us in the implementation of smart social distancing (apply whenever needed).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13154-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-10480-w,Deep learning methodology for predicting time history of head angular kinematics from simulated crash videos,Scientific Reports,10.1038/s41598-022-10480-w,Nature,2022-04-20,"Head kinematics information is important as it is used to measure brain injury risk. Currently, head kinematics are measured using wearable devices or instrumentation mounted on the head. This paper evaluates the deep learning approach in predicting time history of head angular kinematics directly from videos without any instrumentation. To prove the concept, a deep learning model was developed for predicting time history of head angular velocities using finite element (FE) based crash simulation videos. This FE dataset was split into training, validation, and test datasets. A combined convolutional neural network and recurrent neural network based deep learning model was developed using the training and validations sets. The test (unseen) dataset was used to evaluate the predictive capability of the deep learning model. On the test dataset, correlation coefficient obtained between the actual and predicted peak angular velocities was 0.73, 0.85, and 0.92 for X, Y, and Z components respectively.",https://www.nature.com/articles/s41598-022-10480-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1208/s12249-022-02271-3,Investigation of Quantitative X-ray Microscopy for Assessment of API and Excipient Microstructure Evolution in Solid Dosage Processing,AAPS PharmSciTech,10.1208/s12249-022-02271-3,Springer,2022-04-19,"Assessment and understanding of changes in particle size of active pharmaceutical ingredients (API) and excipients as a function of solid dosage form processing is an important but under-investigated area that can impact drug product quality. In this study, X-ray microscopy (XRM) was investigated as a method for determining the in situ particle size distribution of API agglomerates and an excipient at different processing stages in tablet manufacturing. An artificial intelligence (AI)–facilitated XRM image analysis tool was applied for quantitative analysis of thousands of individual particles, both of the API and the major filler component of the formulation, microcrystalline cellulose (MCC). Domain size distributions for API and MCC were generated along with the calculation of the porosity of each respective component. The API domain size distributions correlated with laser diffraction measurements and sieve analysis of the API, formulation blend, and granulation. The XRM analysis demonstrated that attrition of the API agglomerates occurred secondary to the granulation stage. These results were corroborated by particle size distribution and sieve potency data which showed generation of an API fines fraction. Additionally, changes in the XRM-calculated size distribution of MCC particles in subsequent processing steps were rationalized based on the known plastic deformation mechanism of MCC. The XRM data indicated that size distribution of the primary MCC particles, which make up the larger functional MCC agglomerates, is conserved across the stages of processing. The results indicate that XRM can be successfully applied as a direct, non-invasive method to track API and excipient particle properties and microstructure for in-process control samples and in the final solid dosage form. The XRM and AI image analysis methodology provides a data-rich way to interrogate the impact of processing stresses on API and excipients for enhanced process understanding and utilization for Quality by Design (QbD). Graphical Abstract ",http://link.springer.com/openurl/fulltext?id=doi:10.1208/s12249-022-02271-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-022-02213-3,Violent activity classification with transferred deep features and 3d-Cnn,"Signal, Image and Video Processing",10.1007/s11760-022-02213-3,Springer,2022-04-18,"Recognition of violent activities is a sub-problem of activity recognition that is new and less studied, comparatively. This study proposes a method to classify violent activity videos by utilizing 3D Convolutional Neural Networks (CNN) and transfer learning. A 3D feature structure is constructed from deep features obtained from frames of the input video with transfer learning and classified with a 3D CNN classifier. The pre-trained AlexNet model is used for feature extraction. Extracted features are reshaped to a 2D structure and concatenated to build 3D feature volumes. These volumes are used in the 3D CNN model construction. The 3D CNN model can only process fixed-size inputs; thus, the volumes of deep features are resized with 3D interpolation. The proposed model is tested with Hockey Fight, Violent Flow, and Movies datasets and compared to the other studies. Higher classification accuracy is obtained compared with the temporal methods like Lstm and Bi-Lstm.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-022-02213-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12852-w,Identification and classification of wild animals from video sequences using hybrid deep residual convolutional neural network,Multimedia Tools and Applications,10.1007/s11042-022-12852-w,Springer,2022-04-18,"In recent decades, wild animal classification from the video sequence is considered the trending research domain. Existing techniques utilize image processing for wild animal classification; however, the video-based classification is not much concentrated on any other previous research. So, in this research, we have concentrated on the wild animal classification using many videos that may contain accurate details about the wild animals. For such classification, Deep Residual Convolutional Neural Network (DRCNN) is integrated with (TSO) algorithm to perform the video processing, accurately classifying the different classes of wild animals using the Serengeti dataset. The video sequences are initially converted into video frames to initiate the wild animal classification. Then, unwanted noise from each video frame is removed using the Fast Average Peer Group (FAPG) filter in the pre-processing stage. Before the filtering process, all the images are resized into the same size. Next to the pre-processing, the threshold-based segmentation process is performed to subtract the background portion from the video frame. Then, features like colour, texture, etc., are extracted from the segmented image to perform the classification process. Finally, the extracted features are given input to the hybrid DRCNN-TSO algorithm for class label prediction. The TSO algorithm achieves the hyperparameter tuning of DRCNN. The proposed method has been executed in the Python platform. Finally, the performance of the proposed methodology is evaluated using the performance metrics (i.e. accuracy, false alarm rate, sensitivity, precision, F1 score, and false discovery rate), which are calculated using true positive (TP), false positive (FP), true negative (TN), and false-negative (FN) values. The obtained results are compared with existing techniques. Further, the Specificity, F1 score, false alarm rate and false discovery rate are compared with filtering and without filtering o show the efficiency of the proposed methodology.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12852-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00034-022-02016-4,Correntropy-Based Multi-objective Multi-channel Speech Enhancement,"Circuits, Systems, and Signal Processing",10.1007/s00034-022-02016-4,Springer,2022-04-18,"Although deep learning-based methods have greatly advanced the speech enhancement, their performance is intensively degraded under the non-Gaussian noises. To combat the problem, a correntropy-based multi-objective multi-channel speech enhancement method is proposed. First, the log-power spectra (LPS) of multi-channel noisy speech are fed to the bidirectional long short-term memory network with the aim of predicting the intermediate log ideal ratio mask (LIRM) and LPS of clean speech in each channel. Then, the intermediate LPS and LIRM features obtained from each channel are separately integrated into a single-channel LPS and a single-channel LIRM by fusion layers. Next, the two single-channel features are further fused into a single-channel LPS and finally fed to the deep neural network to predict the LPS of clean speech. During training, a multi-loss function is constructed based on correntropy with the aim of reducing the impact of outliers and improving the performance of overall network. Experimental results show that the proposed method achieves significant improvements in suppressing non-Gaussian noises and reverberations and has good robustness to different noises, signal–noise ratios and source–array distances.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00034-022-02016-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13198-022-01661-x,Construction of smart medical assurance system based on virtual reality and GANs image recognition,International Journal of System Assurance Engineering and Management,10.1007/s13198-022-01661-x,Springer,2022-04-15,"With the rapid and recent development of computer technology and Internet technology, virtual reality technology gradually tends to be mature and perfect, and widely used in various fields of life. At present, virtual reality technology has touched telemedicine activities, and become another intuitive and effective form of service in telemedicine activities. Practitioners in the medical industry pay so much attention to the safety of medical services, which also shows that medical safety has been a major challenge for medical staff since ancient times. Due to the limitations of medical research and the complexity of medical work, it is very difficult to avoid medical errors and improve the safety level of patients. In this paper, virtual reality technology and GAN based image recognition technology are applied in the intelligent medical system. Combining medical image recognition with virtual reality system, an intelligent medical system is designed and implemented. Different from the traditional telemedicine system, the application of the system can achieve remote sharing of three-dimensional surgical images. The medical staff in different areas only need to operate through the network to realize the virtual operation environment, which can not only fully display the operation scene, but also observe from any direction and angle, so as to reduce medical costs and save time. The proposed model is implemented under different scenarios, and the comparison experimental analysis is conducted to validate the performance of the model. Through the simulation, it can be proven that the propose model is efficient, the accuracy is improved to more than 95%, as the system is improved from the perspectives of robustness and accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13198-022-01661-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12832-0,Moving object detection for unseen videos via truncated weighted robust principal component analysis and salience convolution neural network,Multimedia Tools and Applications,10.1007/s11042-022-12832-0,Springer,2022-04-15,"Moving object detection is a basic and important work in intelligent video analysis. Recently, a lot of methods have sprung up. Among them, the methods based on deep learning have achieved very amazing results. However, the methods based on deep learning rely on special annotated data to train the model. Thus they have weak generalization ability and can only deal with the data related to the training data. In order to handle this issue, this paper proposes a method based on Truncated Weighted Robust Principal Component Analysis and Salience Convolution Neural Network. Unlike other deep learning methods, the input of the proposed method does not contain the scene information. The proposed method uses the salient information obtained by the proposed Truncated Weighted Robust Principal Component Analysis as input. This improves the generalization ability of the proposed method. The experimental results show the superior performance of the proposed method for unseen videos on CDNET 2014 database.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12832-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12517-022-10057-x,Classification of pile foundation integrity based on convolutional neural network,Arabian Journal of Geosciences,10.1007/s12517-022-10057-x,Springer,2022-04-15,"Pile integrity is a comprehensive qualitative indicator reflecting the relative change of pile section size, the compactness, and continuity of pile material. The evaluation of pile integrity is a systematic and comprehensive evaluation. However, manual detection has some defects, such as cost high, low efficiency, and strong subjectivity. In order to realize the automatic classification of pile integrity, this paper proposes a method of pile integrity classification and recognition based on convolutional neural network (CNN), including one input layer, four convolutional layers, four pooling layers, two fully connected layers, and an output layer. The stochastic gradient descent algorithm and overfitting prevention technology are used to improve the model. The experimental results show that the proposed model can effectively achieve the classification of pile integrity, with an average accuracy of 98.58% on the test set, and has good robustness and generalization performance, which overcomes the shortcomings of complex operation, high cost, and strong subjectivity of artificial extraction feature.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12517-022-10057-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13080-y,Virtual reality in training artificial intelligence-based systems: a case study of fall detection,Multimedia Tools and Applications,10.1007/s11042-022-13080-y,Springer,2022-04-14,"Artificial Intelligent (AI) systems generally require training data of sufficient quantity and appropriate quality to perform efficiently. However, in many areas, such training data is simply not available or incredibly difficult to acquire. The recent developments in Virtual Reality (VR) have opened a new door for addressing this issue. This paper demonstrates the use of VR for generating training data for AI systems through a case study of human fall detection. Fall detection is a challenging problem in the public healthcare domain. Despite significant efforts devoted to introducing reliable and effective fall detection algorithms and enormous devices developed in the literature, minimal success has been achieved. The lack of recorded fall data and the data quality have been identified as major obstacles. To address this issue, this paper proposes an innovative approach to remove the afformentioned obstacle using VR technology. In this approach, a framework is, first, proposed to generate human fall data in virtual environments. The generated fall data is then tested with state-of-the-art visual-based fall detection algorithms to gauge its effectiveness. The results have indicated that the virtual human fall data generated using the proposed framework have sufficient quality to improve fall detection algorithms. Although the approach is proposed and verified in the context of human fall detection, it is applicable to other computer vision problems in different contexts, including human motion detection/recognition and self-driving vehicles.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13080-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00366-022-01648-z,SMA-Net: Deep learning-based identification and fitting of CAD models from point clouds,Engineering with Computers,10.1007/s00366-022-01648-z,Springer,2022-04-13,"Identification and fitting is an important task in reverse engineering and virtual/augmented reality. Compared to the traditional approaches, carrying out such tasks with a deep learning-based method have much room to exploit. This paper presents SMA-Net (Spatial Merge Attention Network), a novel deep learning-based end-to-end bottom-up architecture, specifically focused on fast identification and fitting of CAD models from point clouds. The network is composed of three parts whose strengths are clearly highlighted: voxel-based multi-resolution feature extractor, spatial merge attention mechanism and multi-task head. It is trained with both virtually-generated point clouds and as-scanned ones created from multiple instances of CAD models, themselves obtained with randomly generated parameter values. Using this data generation pipeline, the proposed approach is validated on two different data sets that have been made publicly available: robot data set for Industry 4.0 applications, and furniture data set for virtual/augmented reality. Experiments show that this reconstruction strategy achieves compelling and accurate results in a very high speed, and that it is very robust on real data obtained for instance by laser scanner and Kinect.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00366-022-01648-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-13091-9,"A multimodal emotion recognition model integrating speech, video and MoCAP",Multimedia Tools and Applications,10.1007/s11042-022-13091-9,Springer,2022-04-13,"As one of the core technologies in the field of human-computer interaction, emotion recognition focuses on the simulation of human emotion perception and understanding process. Emotion recognition is widely used in medical, education, life, transportation and other fields. At present, the emotion recognition is still a challenging topic. The accuracy of emotion recognition in multimodal is discussed, different emotion features are extracted from speech, video and motion capture (MoCAP) by using deep learning methods, and a matching emotion recognition model called facial motion speech emotion recognition (FM-SER) model is designed. Local and global information of speech, dual spectrograms are designed in audio mode to choose the time-domain and frequency-domain information, and convolutional neural networks (CNN), gated recurrent unit (GRU) and attention models are used to realize speech emotion recognition. A 3D CNN model based on attention mechanism is used in the video mode to capture the potential emotional expression. The sequential features of hand and head movements are extracted from MoCAP, and import into a bidirectional three-layer long short-term memory (LSTM) model with the attention mechanism. Based on the complementary relationship between multimodal, the decision level integrating scheme is designed with higher-precision, stronger generalization ability of emotion recognition. Through a lot of experiments, we compared the results of several popular emotion recognition models on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus. The results showed that the proposed method had higher recognition accuracies in single modality and multimodal, and the average accuracies of one modality and multimodal were improved by 16.3% and 9%. The effectiveness of FM-SER model in emotion recognition was proved.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-13091-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-022-03825-w,Prediction of Parkinson’s disease based on artificial neural networks using speech datasets,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-022-03825-w,Springer,2022-04-12,"Parkinson’s disease (PD) is a progressive disorder of the nervous system that affects movement. Early prediction of PD can increase the chances of earlier intervention and delay the onset of the disease. Vocal impairment is one of the most important signs in the early stages of PD. Therefore, PD detection based on speech analysis and vocal patterns has attracted significant attention recently. In this paper, we propose a vowel-based artificial neural network (ANN) model for PD prediction based on single vowel phonation. Firstly, we propose a novel multi-layer neural network based on speech features to predict PD. The speech samples from 48 PD patients and 20 healthy individuals are processed into four types: vowel, number, word, and short sentence. Secondly, we establish ANN models with single-type speech samples versus combinations of multi-type speech samples, respectively. Comparative experiments demonstrate that the single-type vowel model is superior to other single-type models as well as multi-type models. Finally, we build a vowel-based ANN model for PD prediction and evaluate its performance. Extensive experiments demonstrate that the proposed model has a prediction accuracy of 91%, sensitivity of 99%, specificity of 82%, and area under the receiver operating characteristic curve (AUC) of 91%, which is superior to the performance of previous methods. Overall, this study demonstrates that the proposed model can provide good classification accuracy for predicting PD and can improve the rate of early diagnosis.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-022-03825-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12931-y,Noise invariant feature pooling for the internet of audio things,Multimedia Tools and Applications,10.1007/s11042-022-12931-y,Springer,2022-04-11,"This manuscript discusses the robustness to noise of deep learning models for two audio classification tasks. The first task is a speaker recognition application, trying to identify five different speakers. The second one is a speech command identification where the goal is to classify ten voice commands. These two tasks are very important to make the communication between humans and smart devices as smooth and natural as possible. The emergence of smart home devices such as personal assistants and the deployment of audio based applications in noisy environments raise new challenges and reveal the weaknesses of existing speech recognition systems. Despite the advances of deep learning in audio tasks, most of the proposed architectures are computationally inefficient and very sensitive to noise. This research addresses these problems by proposing two neural architectures that incorporate a novel pooling operation, named entropy pooling. Entropy pooling is based on the principle of maximum entropy. A detailed ablation study is conducted to evaluate the performance of entropy pooling against the classic max and average pooling layers. The neural networks that are developed are based on two architectures, convolutional networks and residual ones. The study shows that entropy based feature pooling improves the robustness of these architectures in the presence of noise.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12931-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12315-2,A novel multi-modal depression detection approach based on mobile crowd sensing and task-based mechanisms,Multimedia Tools and Applications,10.1007/s11042-022-12315-2,Springer,2022-04-11,"Depression has become a global concern, and COVID-19 also has caused a big surge in its incidence. Broadly, there are two primary methods of detecting depression: Task-based and Mobile Crowd Sensing (MCS) based methods. These two approaches, when integrated, can complement each other. This paper proposes a novel approach for depression detection that combines real-time MCS and task-based mechanisms. We aim to design an end-to-end machine learning pipeline, which involves multimodal data collection, feature extraction, feature selection, fusion, and classification to distinguish between depressed and non-depressed subjects. For this purpose, we created a real-world dataset of depressed and non-depressed subjects. We experimented with: various features from multi-modalities, feature selection techniques, fused features, and machine learning classifiers such as Logistic Regression, Support Vector Machines (SVM), etc. for classification. Our findings suggest that combining features from multiple modalities perform better than any single data modality, and the best classification accuracy is achieved when features from all three data modalities are fused. Feature selection method based on Pearson’s correlation coefficients improved the accuracy in comparison with other methods. Also, SVM yielded the best accuracy of 86%. Our proposed approach was also applied on benchmarking dataset, and results demonstrated that the multimodal approach is advantageous in performance with state-of-the-art depression recognition techniques.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12315-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12351-y,Query based relevant music genre retrieval using adaptive artificial neural network for multimedia applications,Multimedia Tools and Applications,10.1007/s11042-022-12351-y,Springer,2022-04-11,"In the case of digital music industry, current major internet stores contain millions of tracks, which complicate search, retrieval and discovery of music relevant for a user. To facilitate the advancement in multimedia applications, an efficient Query based Music Genre Retrieval (Q-MGR) strategy constructed by AANN (Adaptive Artificial Neural Network) is followed in this paper. Here, the proposed Q-MGR approach is done in three steps. Firstly, the few relevant features that are capable of distinguishing variety of signals are extracted. In second step, the AANN is trained with few music query signals to produce the prediction model for enabling the query based music retrieval. Here, the AANN is modelled to develop dynamic prediction model using Grasshopper Optimization algorithm (GOA), where, the optimal number of hidden layers and its neurons are found. Finally, the retrieval step is done with the predicted network model. Moreover, the proposed methodology is implemented in the working platform of MATLAB and the results are analysed with the recent literature works.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12351-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10462-022-10176-7,Deep learning for video object segmentation: a review,Artificial Intelligence Review,10.1007/s10462-022-10176-7,Springer,2022-04-08,"As one of the fundamental problems in the field of video understanding, video object segmentation aims at segmenting objects of interest throughout the given video sequence. Recently, with the advancements of deep learning techniques, deep neural networks have shown outstanding performance improvements in many computer vision applications, with video object segmentation being one of the most advocated and intensively investigated. In this paper, we present a systematic review of the deep learning-based video segmentation literature, highlighting the pros and cons of each category of approaches. Concretely, we start by introducing the definition, background concepts and basic ideas of algorithms in this field. Subsequently, we summarise the datasets for training and testing a video object segmentation algorithm, as well as common challenges and evaluation metrics. Next, previous works are grouped and reviewed based on how they extract and use spatial and temporal features, where their architectures, contributions and the differences among each other are elaborated. At last, the quantitative and qualitative results of several representative methods on a dataset with many remaining challenges are provided and analysed, followed by further discussions on future research directions. This article is expected to serve as a tutorial and source of reference for learners intended to quickly grasp the current progress in this research area and practitioners interested in applying the video object segmentation methods to their problems. A public website is built to collect and track the related works in this field: https://github.com/gaomingqi/VOS-Review .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10462-022-10176-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12886-0,Fusing traditionally extracted features with deep learned features from the speech spectrogram for anger and stress detection using convolution neural network,Multimedia Tools and Applications,10.1007/s11042-022-12886-0,Springer,2022-04-08,"Stress and anger are two negative emotions that affect individuals both mentally and physically; there is a need to tackle them as soon as possible. Automated systems are highly required to monitor mental states and to detect early signs of emotional health issues. In the present work convolutional neural network is proposed for anger and stress detection using handcrafted features and deep learned features from the spectrogram. The objective of using a combined feature set is gathering information from two different representations of speech signals to obtain more prominent features and to boost the accuracy of recognition. The proposed method of emotion assessment is more computationally efficient than similar approaches used for emotion assessment. The preliminary results obtained on experimental evaluation of the proposed approach on three datasets Toronto Emotional Speech Set (TESS), Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), and Berlin Emotional Database (EMO-DB) indicate that categorical accuracy is boosted and cross-entropy loss is reduced to a considerable extent. The proposed convolutional neural network (CNN) obtains training (T) and validation (V) categorical accuracy of T = 93.7%, V = 95.6% for TESS, T = 97.5%, V = 95.6% for EMO-DB and T = 96.7%, V = 96.7% for RAVDESS dataset.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12886-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12873-5,Hand-crafted versus learned representations for audio event detection,Multimedia Tools and Applications,10.1007/s11042-022-12873-5,Springer,2022-04-07,"Audio Event Detection (AED) pertains to identifying the types of events in audio signals. AED is essential for applications requiring decisions based on audio signals, which can be critical, for example, for health, surveillance and security applications. Despite the proven benefits of deep learning in obtaining the best representation for solving a problem, AED studies still generally employ hand-crafted representations even when deep learning is used for solving the AED task. Intrigued by this, we investigate whether or not hand-crafted representations (i.e. spectogram, mel spectogram, log mel spectogram and mel frequency cepstral coefficients) are better than a representation learned using a Convolutional Autoencoder (CAE). To the best of our knowledge, our study is the first to ask this question and thoroughly compare feature representations for AED. To this end, we first find the best hop size and window size for each hand-crafted representation and compare the optimized hand-crafted representations with CAE-learned representations. Our extensive analyses on a subset of the AudioSet dataset confirm the common practice in that hand-crafted representations do perform better than learned features by a large margin ( ∼ $\sim $ 30 AP). Moreover, we show that the commonly used window and hop sizes do not provide the optimal performances for the hand-crafted representations.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12873-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10055-022-00644-4,Immersive machine learning for social attitude detection in virtual reality narrative games,Virtual Reality,10.1007/s10055-022-00644-4,Springer,2022-04-07,"People can understand how human interaction unfolds and can pinpoint social attitudes such as showing interest or social engagement with a conversational partner. However, summarising this with a set of rules is difficult, as our judgement is sometimes subtle and subconscious. Hence, it is challenging to program Non-Player Characters (NPCs) to react towards social signals appropriately, which is important for immersive narrative games in Virtual Reality (VR). We collaborated with two game studios to develop an immersive machine learning (ML) pipeline for detecting social engagement. We collected data from participants-NPC interaction in VR, which was then annotated in the same immersive environment. Game design is a creative process and it is vital to respect designer’s creative vision and judgement. We therefore view annotation as a key part of the creative process. We trained a reinforcement learning algorithm (PPO) with imitation learning rewards using raw data (e.g. head position) and socially meaningful derived data (e.g. proxemics); we compared different ML configurations including pre-training and a temporal memory (LSTM). The pre-training and LSTM configuration using derived data performed the best (84% F1-score, 83% accuracy). The models using raw data did not generalise. Overall, this work introduces an immersive ML pipeline for detecting social engagement and demonstrates how creatives could use ML and VR to expand their ability to design more engaging experiences. Given the pipeline’s results for social engagement detection, we generalise it for detecting human-defined social attitudes.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-022-00644-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12540-9,A tibetan-dependent speaker recognition method based on deep learning,Multimedia Tools and Applications,10.1007/s11042-022-12540-9,Springer,2022-04-07,"For the text content is known, the semantic information and speaker characteristics in the speech signal can be used for speech recognition and speaker verification respectively in text prompt speaker recognition, which solves the problem of forged recordings in the process of text association. In practical applications, by combining speech recognition and speaker recognition technologies, a double verification effect can be achieved, which also can effectively improve security. There are few studies on the combination of speaker recognition and speech recognition in Tibetan, mainly using non-end-to-end methods, and the performance of the model is not ideal. Based on the original research, this paper uses the mainstream end-to-end method to study the speaker verification part. The network model uses ResNet-34 and ResNet-50, and fine-tuned them. “Open set” speaker verification is essentially metric learning. The ideal embedding is to compress the frame-level features into a compact speech-level representation, thereby maximizing the inter-class distance and minimizing the intra-class distance. For the loss function, we use three classification objective loss functions and three metric learning objective loss functions to extensively evaluate the performance of the model. In order to further improve the performance of the model, we fused the two loss functions of Softmax and Angular Prototype. The experimental results show that the effect of Fast ResNet-50 is better than that of Fast ResNet-34, and the model effect of the Angular Prototype loss function is better than other single loss functions. The model with the fused loss function has the best performance, with an equal error rate of 4.25%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12540-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12612-w,Sound of guns: digital forensics of gun audio samples meets artificial intelligence,Multimedia Tools and Applications,10.1007/s11042-022-12612-w,Springer,2022-04-06,"Classifying a weapon based on its muzzle blast is a challenging task that has significant applications in various security and military fields. Most of the existing works rely on ad-hoc deployment of spatially diverse microphone sensors to capture multiple replicas of the same gunshot, which enables accurate detection and identification of the acoustic source. However, carefully controlled setups are difficult to obtain in scenarios such as crime scene forensics, making the aforementioned techniques inapplicable and impractical. We introduce a novel technique that requires zero knowledge about the recording setup and is completely agnostic to the relative positions of both the microphone and shooter. Our solution can identify the category, caliber, and model of the gun, reaching over 90% accuracy on a dataset composed of 3655 samples that are extracted from YouTube videos. Our results demonstrate the effectiveness and efficiency of applying Convolutional Neural Network (CNN) in gunshot classification eliminating the need for an ad-hoc setup while significantly improving the classification performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12612-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12299-z,Video based exercise recognition and correct pose detection,Multimedia Tools and Applications,10.1007/s11042-022-12299-z,Springer,2022-04-05,"Human pose estimation has gained significant attention from researchers of the present era. Personal exercise sessions can be monitored and supervised with the help of pose recognition. Existing work on exercise classification primarily relies on external or wearable sensors for recognizing poses. However, such sensors often fail to differentiate amongst similar exercises. Some essential extensions of human pose estimation are activity detection and activity prediction. In this paper, we first classify an individual’s exercises and then predict whether the pose corresponding to an exercise is correct or not. The tasks mentioned above are performed with the help of 2-dimensional pose coordinates. We have used an RGB camera to capture the poses during exercises performed by individuals. We formulate our model with 2D coordinates obtained from the 2D pose. We consider 2D coordinates of 18 joints of a human body as the primary features to classify different exercises and predict correctness about the poses. We have developed a benchmark dataset consisting of human subjects of various age groups with varying heights. An accuracy of 97.01% has been obtained, and it is better than existing work when tested on our dataset.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12299-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10044-022-01070-7,Multiple Regularization and Analysis of Deep Capsule Network,Pattern Analysis and Applications,10.1007/s10044-022-01070-7,Springer,2022-04-04,"With the increase of layers in deep capsule networks, the overfitting problem also becomes more serious. Capsule-based regularization methods are important to solve this problem. However, little attention has been paid to this field. To fill this gap, we propose five regularization methods from the following aspects. In capsules represented by vectors, two methods are proposed to modify the existence and properties of their activation vectors by disturbing the length and orientation of the vectors. In capsules represented by tensors, capsule-based layer normalization is proposed to improve dynamic routing. In the training strategy, a warm restart learning rate with probability is used to improve the efficiency of training. In reconstruction, a novel image decoder provides a better regularization effect by using multiscale information of images. These regularization methods are investigated on CIFAR10, CIFAR100, and SVHN. Experiments show that using these regularization methods can effectively improve the generalization performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10044-022-01070-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07190-9,Fuzzy rule-based neural network for high-speed train manufacturing system scheduling problem,Neural Computing and Applications,10.1007/s00521-022-07190-9,Springer,2022-04-03,"Our country has a vast territory, and rail transit is very important to the development of our country's national economy. In this paper, key technologies for a digital twin-based shop floor management and control system are investigated, and the concept is designed and implemented. By adding a digital twin between the business management layer and the production execution layer of the traditional workshop management and control system through the fuzzy rule neural network, a new workshop management and control system architecture on the basis of the virtual is formed, enabling intellectual management and control of the workshop. The results of the study found that the integration of the digital twin into the conventional shop floor management and control system led to changes in the composition, processes and information integration of the management and support system. For the purpose of comparing the system scheduling of the high-speed railway on the basis of the vague rule neural network with the traditional method, we made statistics on the system scheduling before and after the transformation. In terms of manufacturing volume, after the output exceeds 200, the speed of the traditional manufacturing method lags behind the fuzzy rule neural network by nearly 50%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07190-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10462-022-10147-y,Video super-resolution based on deep learning: a comprehensive survey,Artificial Intelligence Review,10.1007/s10462-022-10147-y,Springer,2022-04-01,"Video super-resolution (VSR) is reconstructing high-resolution videos from low resolution ones. Recently, the VSR methods based on deep neural networks have made great progress. However, there is rarely systematical review on these methods. In this survey, we comprehensively investigate 37 state-of-the-art VSR methods based on deep learning. It is well known that the leverage of information contained in video frames is important for video super-resolution. Thus we propose a taxonomy and classify the methods into seven sub-categories according to the ways of utilizing inter-frame information. Moreover, descriptions on the architecture design and implementation details are also included. Finally, we summarize and compare the performance of the representative VSR methods on some benchmark datasets. We also discuss the applications, and some challenges, which need to be further addressed by researchers in the community of VSR. To the best of our knowledge, this work is the first systematic review on VSR tasks, and it is expected to make a contribution to the development of recent studies in this area and potentially deepen our understanding of the VSR techniques based on deep learning.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10462-022-10147-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12954-5,Deep learning for multimedia signal processing and applications,Multimedia Tools and Applications,10.1007/s11042-022-12954-5,Springer,2022-04-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12954-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12366-5,Vision-based techniques for fall detection in 360^∘ videos using deep learning: Dataset and baseline results,Multimedia Tools and Applications,10.1007/s11042-022-12366-5,Springer,2022-04-01,"Alarming cases of falls in the elderly have triggered the rise of robust and cost-efficient systems for automated fall detection in humans. Although several potential solutions exist, they still have not achieved the desired level of robustness and acceptability. Lately, the proliferation of low-cost cameras coupled with deep learning techniques has transformed vision-based methods for fall detection. Motivated by this, in this paper, we present an alternate low-cost and efficient system for fall detection in 360^∘ videos using deep learning. Towards this, we first built a well-balanced video dataset named Fall360. The Fall360 dataset contains video clips of several falls and non-fall actions, captured by a 360^∘ camera mounted on the ceiling in a home-like environment. Secondly, we examined the performance of deep learning techniques that consist of several variants of hybrid CNN & LSTM, hybrid CNN & ConvLSTM, and 3D CNNs to test the effectiveness of the dataset in the fall detection task. Thirdly, to assess the performance of these techniques, we conducted an ablation study on a recently introduced multi-camera UP-Fall dataset. The deep learning models attained substantial improvement in recognition accuracy on both the fall datasets and have set the new state-of-the-art performance. Overall, our designed fall detection system using 360^∘ videos, in addition to providing a better perspective, bestows a more suitable and low-cost alternative for the existing multi-camera-based fall detection systems. To encourage more study, we will make our in-house Fall360 dataset publicly available to the research community.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12366-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10462-021-10061-9,Deep reinforcement learning in computer vision: a comprehensive survey,Artificial Intelligence Review,10.1007/s10462-021-10061-9,Springer,2022-04-01,"Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations . In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i) landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10462-021-10061-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-021-04190-9,Attention-based framework for weakly supervised video anomaly detection,The Journal of Supercomputing,10.1007/s11227-021-04190-9,Springer,2022-04-01,"Video anomaly detection automatically recognizes abnormal events in surveillance videos. Existing works have made advances in recognizing whether a video contains abnormal events; however, they cannot temporally localize the abnormal events within videos. This paper presents a novel anomaly attention-based framework for accurately temporally localize the abnormal events. Benefiting from the proposed framework, we can achieve frame-level VAD using video-level labels, which significantly reduces the burden of data annotation. Our method is an end-to-end deep neural network-based approach, which contains three modules: anomaly attention module (AAM), discriminative anomaly attention module (DAAM) and generative anomaly attention module (GAAM). Specifically, AAM is trained to generate the anomaly attention, which is used to measure the abnormal degree of each frame. Whereas, DAAM and GAAM are used to alternately augmenting AAM from two different aspects. On the one hand, DAAM enhancing AAM by optimizing the video-level video classification. On the other hand, GAAM adopts a conditional variational autoencoder to model the likelihood of each frame given the attention for refining AAM. As a result, AAM can generate higher anomaly scores for abnormal frames while lower anomaly scores for normal frames. Experimental results show that our proposed approach outperforms state-of-the-art methods, which validates the superiority of our AAVAD.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-021-04190-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-021-04251-z,An reinforcement learning-based speech censorship chatbot system,The Journal of Supercomputing,10.1007/s11227-021-04251-z,Springer,2022-04-01,"The rapid development of artificial intelligence (AI) technology has enabled large-scale AI applications to land in the market and practice. However, plenty of security issues have been exposed to society while AI technology has brought many conveniences to humankind, especially for the chatbot with online learning. This paper proposes a speech censorship chatbot system with reinforcement learning, which is mainly composed of two parts: the aggressive speech censorship model and the speech purification model. The aggressive speech censorship can combine the context of user input sentences to detect aggressive speech and respond to the rapid evolution of aggressive speech. According to the situation of the chatbot that is polluted by large numbers of aggressive speech, the speech purification model has the capacity to ""forget"" the learned malicious data through reinforcement learning rather than rolling back to the early versions. In addition, by integrating few-shot learning, the speed of speech purification is accelerated while reducing the influence on the quality of replies. The experimental results show that our proposed method reduces the probability of generating aggressive speeches and that the integration of the few-shot learning improves the training speed rapidly while effectively slowing down the decline in BLEU values.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-021-04251-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-021-02790-9,Video representation learning by identifying spatio-temporal transformations,Applied Intelligence,10.1007/s10489-021-02790-9,Springer,2022-04-01,"Self-supervised learning becomes a prevalent paradigm in both image and video domains due to the difficulty in obtaining a large amount of annotated data. In this paper, we adopt the self-supervised learning paradigm and propose to learn 3D video representations by identifying spatio-temporal transformations. Specifically, we choose a set of transformations and apply them to unlabelled videos to change the spatio-temporal structure of these videos. By identifying these spatio-temporal transformations, the network learns knowledge about both spatial appearance and temporal relation of video frames. In this paper, we choose the spatio-temporal rotations as the transformations. We conduct extensive experiments to validate the effectiveness of the proposed method. After fine-tuning on action recognition benchmarks, our model yields a remarkable gain of 29.6% on UCF101 and 25.1% on HMDB51 compared with models trained from scratch, which belongs to the current advanced method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02790-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10278-021-00574-8,Studierfenster: an Open Science Cloud-Based Medical Imaging Analysis Platform,Journal of Digital Imaging,10.1007/s10278-021-00574-8,Springer,2022-04-01,"Imaging modalities such as computed tomography (CT) and magnetic resonance imaging (MRI) are widely used in diagnostics, clinical studies, and treatment planning. Automatic algorithms for image analysis have thus become an invaluable tool in medicine. Examples of this are two- and three-dimensional visualizations, image segmentation, and the registration of all anatomical structure and pathology types. In this context, we introduce Studierfenster ( www.studierfenster.at ): a free, non-commercial open science client-server framework for (bio-)medical image analysis. Studierfenster offers a wide range of capabilities, including the visualization of medical data (CT, MRI, etc.) in two-dimensional (2D) and three-dimensional (3D) space in common web browsers, such as Google Chrome, Mozilla Firefox, Safari, or Microsoft Edge. Other functionalities are the calculation of medical metrics (dice score and Hausdorff distance), manual slice-by-slice outlining of structures in medical images, manual placing of (anatomical) landmarks in medical imaging data, visualization of medical data in virtual reality (VR), and a facial reconstruction and registration of medical data for augmented reality (AR). More sophisticated features include the automatic cranial implant design with a convolutional neural network (CNN), the inpainting of aortic dissections with a generative adversarial network, and a CNN for automatic aortic landmark detection in CT angiography images. A user study with medical and non-medical experts in medical image analysis was performed, to evaluate the usability and the manual functionalities of Studierfenster. When participants were asked about their overall impression of Studierfenster in an ISO standard (ISO-Norm) questionnaire, a mean of 6.3 out of 7.0 possible points were achieved. The evaluation also provided insights into the results achievable with Studierfenster in practice, by comparing these with two ground truth segmentations performed by a physician of the Medical University of Graz in Austria. In this contribution, we presented an online environment for (bio-)medical image analysis. In doing so, we established a client-server-based architecture, which is able to process medical data, especially 3D volumes. Our online environment is not limited to medical applications for humans. Rather, its underlying concept could be interesting for researchers from other fields, in applying the already existing functionalities or future additional implementations of further image processing applications. An example could be the processing of medical acquisitions like CT or MRI from animals [Clinical Pharmacology & Therapeutics, 84(4):448–456, 68 ], which get more and more common, as veterinary clinics and centers get more and more equipped with such imaging devices. Furthermore, applications in entirely non-medical research in which images/volumes need to be processed are also thinkable, such as those in optical measuring techniques, astronomy, or archaeology.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10278-021-00574-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-021-01555-8,Pre-Training Without Natural Images,International Journal of Computer Vision,10.1007/s11263-021-01555-8,Springer,2022-04-01,"Is it possible to use convolutional neural networks pre-trained without any natural images to assist natural image understanding? The paper proposes a novel concept, Formula-driven Supervised Learning (FDSL). We automatically generate image patterns and their category labels by assigning fractals, which are based on a natural law. Theoretically, the use of automatically generated images instead of natural images in the pre-training phase allows us to generate an infinitely large dataset of labeled images. The proposed framework is similar yet different from Self-Supervised Learning because the FDSL framework enables the creation of image patterns based on any mathematical formulas in addition to self-generated labels. Further, unlike pre-training with a synthetic image dataset, a dataset under the framework of FDSL is not required to define object categories, surface texture, lighting conditions, and camera viewpoint. In the experimental section, we find a better dataset configuration through an exploratory study, e.g., increase of #category/#instance, patch rendering, image coloring, and training epoch. Although models pre-trained with the proposed Fractal DataBase (FractalDB), a database without natural images, do not necessarily outperform models pre-trained with human annotated datasets in all settings, we are able to partially surpass the accuracy of ImageNet/Places pre-trained models. The FractalDB pre-trained CNN also outperforms other pre-trained models on auto-generated datasets based on FDSL such as Bezier curves and Perlin noise. This is reasonable since natural objects and scenes existing around us are constructed according to fractal geometry. Image representation with the proposed FractalDB captures a unique feature in the visualization of convolutional layers and attentions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-021-01555-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12206-6,Efficient vision-based multi-target augmented reality in the browser,Multimedia Tools and Applications,10.1007/s11042-022-12206-6,Springer,2022-04-01,"Augmented Reality (AR) has gained rising attention from both industry and academia as it enhances the way we interact with the physical world. Compared with native AR apps, implementing AR with web technologies (Web AR) can provide lightweight and universal cross-platform deployment that does not involve extra downloading and installation in advance. However, there are some challenges when developing Web AR apps, such as computational efficiency and networking. The limited capabilities of the browser, especially on mobile devices, make it more challenging to develop efficient web apps. Fortunately, several technical advances have emerged that could change the status of Web AR. This paper presents an efficient implementation of a vision-based and multi-target Web AR app that runs at real-time frame rates on standard web browsers on mobile devices and PCs. A method based on natural features tracking (NFT) is used, and several new web technologies are optimized to achieve specific tasks. The proposed implementation takes advantage of an efficient and lightweight class of convolutional neural networks (CNN) to classify image targets. It uses an image registration method that eliminates the need for a database of the feature points’ descriptors, which is usually used in natural feature tracking methods. Computation-intensive tasks, such as target extraction and pose estimation, were computed with separate threads. Thus, the main thread which handles the HTML rendering runs smoothly and is not blocked by these computation-intensive tasks. To evaluate the performance of the proposed architecture and validate its performance, a prototype app was developed. The findings demonstrate that the app can track multiple image targets with real-time frame rates and stable interaction.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12206-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11036-022-01931-9,Advanced Machine Learning Based Mobile Multimedia Application,Mobile Networks and Applications,10.1007/s11036-022-01931-9,Springer,2022-04-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11036-022-01931-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10845-021-01906-9,Improving automated visual fault inspection for semiconductor manufacturing using a hybrid multistage system of deep neural networks,Journal of Intelligent Manufacturing,10.1007/s10845-021-01906-9,Springer,2022-04-01,"In the semiconductor industry, automated visual inspection aims to improve the detection and recognition of manufacturing defects by leveraging the power of artificial intelligence and computer vision systems, enabling manufacturers to profit from an increased yield and reduced manufacturing costs. Previous domain-specific contributions often utilized classical computer vision approaches, whereas more novel systems deploy deep learning based ones. However, a persistent problem in the domain stems from the recognition of very small defect patterns which are often in the size of only a few $$\mu $$ μ m and pixels within vast amounts of high-resolution imagery. While these defect patterns occur on the significantly larger wafer surface, classical machine and deep learning solutions have problems in dealing with the complexity of this challenge. This contribution introduces a novel hybrid multistage system of stacked deep neural networks (SH-DNN) which allows the localization of the finest structures within pixel size via a classical computer vision pipeline, while the classification process is realized by deep neural networks. The proposed system draws the focus over the level of detail from its structures to more task-relevant areas of interest. As the created test environment shows, our SH-DNN-based multistage system surpasses current approaches of learning-based automated visual inspection. The system reaches a performance (F1-score) of up to 99.5%, corresponding to a relative improvement of the system’s fault detection capabilities by 8.6-fold. Moreover, by specifically selecting models for the given manufacturing chain, runtime constraints are satisfied while improving the detection capabilities of currently deployed approaches.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10845-021-01906-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13042-022-01540-8,A hierarchical reasoning graph neural network for the automatic scoring of answer transcriptions in video job interviews,International Journal of Machine Learning and Cybernetics,10.1007/s13042-022-01540-8,Springer,2022-04-01,"We address the task of automatically scoring the competency of candidates based on textual features, from the automatic speech recognition transcriptions in the asynchronous video job interviews. The key challenge is to construct the dependency relations and semantic level interaction over each question–answer (QA) pair. However, most recent studies focus on the representation of questions and answers, but ignore the dependency information and interaction between them, which is critical for QA evaluation. In this work, we propose a hierarchical reasoning graph neural network for the automatic assessment of question–answer pairs. Specifically, we construct a sentence-level relational graph neural network to capture the dependency information of sentences in or between the question and the answer. Based on these graphs, we employ a semantic-level reasoning graph attention network to model the interaction states of the current QA session. Finally, we propose a gated recurrent unit encoder to represent the temporal question–answer pairs for the final prediction. Empirical results on CHNAT (a real-world dataset) validate that our proposed model significantly outperforms matching-based benchmark models. Ablation studies and experimental results with 10 random seeds also show the effectiveness and stability of our models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13042-022-01540-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-021-02960-0,LSTM and CNN based ensemble approach for spoof detection task in automatic speaker verification systems,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-021-02960-0,Springer,2022-04-01,"Nowadays, fingerprint and retina scans are the most reliable and widely used biometric authentication systems. Another emerging biometric approach for authentication, though vulnerable, is speech-based systems. However, speech-based systems are more prone to spoofing attacks. Through such malicious attacks, an unauthentic person tries to present himself as legitimate user, in order to acquire illegitimate advantage. Therefore, these attacks, created by synthesis or conversion of speech, pose an enormous threat to the reliable functioning of automatic speaker verification (ASV) authentication systems. The work presented in this paper tries to address this problem by using deep learning (DL) methods and ensemble of different neural networks. The first model is a combination of time-distributed dense layers and long short-term memory (LSTM) layers. The other two deep neural networks (DNNs) are based on temporal convolution (TC) and spatial convolution (SC). Finally, an ensemble model comprising of these three DNNs has also been analysed. All these models are analysed with Mel frequency cepstral coefficients (MFCC), inverse Mel frequency cepstral coefficients (IMFCC) and constant Q cepstral coefficients (CQCC) at the frontend, where the proposed ensemble performs best with CQCC features. The proposed work uses ASVspoof 2015 and ASVspoof 2019 datasets for training and testing, with the evaluation set having speech synthesis (SS) and voice conversion (VC) attacked utterances. Performance of proposed system trained with ASVspoof 2015 dataset degrades with evaluation set of ASVspoof 2019 dataset, whereas performance of the same system improves when training is also done with the ASVspoof 2019 dataset. Also, a joint ASVspoof 1519 dataset is created to add more variations into the single dataset, and it has been observed that the trained ensemble with this joint dataset performs even better, while performing evaluation using single datasets. This research promotes the development of systems that can cope with completely unknown data in testing. It has been further observed that the models can reach promising results, paving the way for future research in this domain using DL.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-021-02960-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-022-04439-x,Full-convolution Siamese network algorithm under deep learning used in tracking of facial video image in newborns,The Journal of Supercomputing,10.1007/s11227-022-04439-x,Springer,2022-04-01,"This study was carried out with the aim of exploring the full-convolution Siamese network (SiamFC) in the application of neonatal facial video image tracking, achieving accurate recognition of neonatal pain and helping doctors evaluate neonatal emotions in an automatic manner. The current technology shows low accuracy on facial image recognition of newborns, so the SiamFC algorithm under the deep learning was optimized in this study. Besides, a newborn facial video image tracking model (FVIT model) was constructed based on the SiamFC algorithm in combination with the attention mechanism with face tracking algorithm, and the facial features of newborns were tracked and recognized. In addition, a newborn face database was constructed based on the adult face database to evaluate performance of the FVIT model. It was found that the accuracy of the improved algorithm is 0.889, higher by 0.036 in contrast to other models; the area under the curve (AUC) of success rate reaches 0.748, higher by 0.075 compared with other algorithms. What’s more, the improved algorithm shows good performance in tracking the facial occlusion, facial expression changes, and scale conversion of newborns. Therefore, the improved algorithm shows higher accuracy and success rate and has good effect in capturing and tracking the facial images of newborns, thereby providing an experimental basis for facial recognition and pain assessment of newborns in the later stage.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-022-04439-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00779-020-01442-y,Urban traffic accident risk prediction for knowledge-based mobile multimedia service,Personal and Ubiquitous Computing,10.1007/s00779-020-01442-y,Springer,2022-04-01,"Traditional accident prediction models have been mostly designed with statistical analysis that finds and analyzes the causal relationships between traffic accidents and a variety of human, road geometry, and environmental factors. However, these statistical methods have limitations in that they are based on assumptions about data distribution and function type. Therefore, this study suggests an accident prediction model using deep learning. This newly suggested risk prediction model is for predicting risk by reflecting static features of the road, such its length and the speed limit on it, and dynamic features of the road, such as traffic volume when driving on it, and the altitude and azimuth of the sun. For this purpose, 4470 accident cases, collected over 5 months from August to December 2018 in Seoul—the most complex, high-traffic, and accident-prone city in Korea—were analyzed. As a result of testing the model using such data, it was found to have an accuracy of 75% and recall of 81%. Based on testing results for the suggested risk prediction model, a system was developed to guide not only accident-prone regions predicted using statistical data but to also guide a risk level for the road. This level of risk is estimated based upon each given situation, so it can change even for the same road. This guide system can be used to provide a level of risk for each road segment and region but also to improve roads with recommendations, such as installation of safety features. In addition, it could support a mobile system that provides a driver with the optimized driving path for safety.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00779-020-01442-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10396-022-01193-8,Tongue model construction based on ultrasound images with image processing and deep learning method,Journal of Medical Ultrasonics,10.1007/s10396-022-01193-8,Springer,2022-04-01,"Purpose The purpose of this paper is to construct a 3D tongue model and to generate an animation of tongue movement for speech therapy in patients with lateral articulation (LA). Methods The 3D tongue model is generated based on ultrasound (US) images, which are widely used in many clinics. A tongue model is constructed by extracting the tongue surfaces from US images with the help of image processing techniques and a deep learning method. A reference tongue model is generated first using US images of a normal speaker, and a model of an LA patient is then constructed by modifying the reference tongue model. An animation of the tongue movement is generated by deforming the model according to a time sequence. Results The accuracy of the tongue surfaces estimated by a deep learning method were 22/45 = 49% and 29/45 = 64% for US images of a normal speaker and an LA patient, respectively. In addition, the maximum vertical errors between the ground truth and the estimated spline curves were 1.01 and 1.03 mm for US images of a normal speaker and an LA patient, respectively. Conclusion We have constructed a tongue model and generated a tongue movement animation of an LA patient using US images. The maximum vertical error between the ground truth and the estimated spline curves was only 1.03 mm, and we have confirmed that the generated tongue model is very useful for speech therapy in LA patients.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10396-022-01193-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-020-02154-9,Temperate fish detection and classification: a deep learning based approach,Applied Intelligence,10.1007/s10489-020-02154-9,Springer,2022-04-01,"A wide range of applications in marine ecology extensively uses underwater cameras. Still, to efficiently process the vast amount of data generated, we need to develop tools that can automatically detect and recognize species captured on film. Classifying fish species from videos and images in natural environments can be challenging because of noise and variation in illumination and the surrounding habitat. In this paper, we propose a two-step deep learning approach for the detection and classification of temperate fishes without pre-filtering. The first step is to detect each single fish in an image, independent of species and sex. For this purpose, we employ the You Only Look Once (YOLO) object detection technique. In the second step, we adopt a Convolutional Neural Network (CNN) with the Squeeze-and-Excitation (SE) architecture for classifying each fish in the image without pre-filtering. We apply transfer learning to overcome the limited training samples of temperate fishes and to improve the accuracy of the classification. This is done by training the object detection model with ImageNet and the fish classifier via a public dataset (Fish4Knowledge), whereupon both the object detection and classifier are updated with temperate fishes of interest. The weights obtained from pre-training are applied to post-training as a priori. Our solution achieves the state-of-the-art accuracy of 99.27% using the pre-training model. The accuracies using the post-training model are also high; 83.68% and 87.74% with and without image augmentation, respectively. This strongly indicates that the solution is viable with a more extensive dataset.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-020-02154-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12211-9,Few-data guided learning upon end-to-end point cloud network for 3D face recognition,Multimedia Tools and Applications,10.1007/s11042-022-12211-9,Springer,2022-04-01,"Deep-learning-based 3D face recognition methods have developed vigorously in recent years, while the potential of these methods is being exploited in more and more scenarios. In this paper, an end-to-end deep learning network entitled Sur3dNet-Face for point-cloud-based 3D face recognition is proposed. The method uses PointNet, which is a successful point cloud classification solution but performs unexpectedly in face recognition, as the backbone. To adapt the backbone to 3D face recognition, modifications in network architecture and a few-data guided learning framework based on Gaussian process morphable model is supplemented. Instead of mass data in multiple datasets for training, our method takes only Spring2003 subset of FRGC v2.0 for training which contains 943 facial scans and the network is well trained with such a small amount of real data. The processing time to generate face representation is less than 0.15 s. Without fine-tuning on the test set, the Rank-1 Recognition Rate (RR1) is achieved as follows: 98.85% on FRGC v2.0 dataset and 99.33% on Bosphorus dataset, which proves the effectiveness and the potentiality of our method. When facing scenarios with limited resource, the proposed method is expected to give a competitive performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12211-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11063-021-10673-w,3D Point Convolutional Network for Dense Scene Flow Estimation,Neural Processing Letters,10.1007/s11063-021-10673-w,Springer,2022-04-01,"Scene flow estimation is one of the most crucial components of many scene understanding tasks, which represents the complete 3D motion of objects in dynamic scene. Most of existing scene flow estimation approaches are usually based on joint learning framework, which cast the problem of scene flow estimation to dense prediction of optical flow and stereo matching. However, these approaches need to reconstruct 3D motion from optical flow and disparity using 2D stereo image pairs, which leads the estimation process to be indirect. Recently, FlowNet3D attempts to learn scene flow from 3D point clouds, which adopts element-wise max-pooling to aggregate features from different points. Nevertheless, max-pooling operation only can obtain the strongest activation on features across a local or global region, which may increase the loss of some useful detailed and contextual. Specifically, for dense estimation task, the ability to transfer information gradually from coarse to fine layers is important. To address this problem, we investigate a new deep architecture, 3D point convolutional network, to learn scene flow from 3D point clouds. This specific architecture uses multi-layer perceptron (MLP) to approximate the weight function and applies a density scale to re-weight the learned weight functions for each convolutional filter, which can make the network permutation-invariant and translation invariant on 3D space which is beneficial for feature aggregation. Extensive experimental results are conducted on FlyingThings3D and KITTI scene flow datasets and show the effectiveness of our proposed approach. Our algorithm can achieve state-of-the-art performance on FlyingThings3D and KITTI datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11063-021-10673-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1134/S0005117922040014,3D Recognition: State of the Art and Trends,Automation and Remote Control,10.1134/S0005117922040014,Springer,2022-04-01,"Abstract We consider the field of three-dimensional technical vision and in particular three-dimensional recognition. The problems of three-dimensional vision are singled out, and methods for obtaining and presenting three-dimensional data, as well as applications of three-dimensional vision, are reviewed. Deep learning methods in 3D recognition problems are surveyed. The main modern trends in this field are revealed. So far, quite a few neural network architectures, convolutional layers, sampling, pooling, and aggregation operations, and methods for representing and processing three-dimensional input data have been proposed. The field is under active development, with the greatest variety of methods being presented for point clouds.",http://link.springer.com/openurl/fulltext?id=doi:10.1134/S0005117922040014,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-021-04124-5,Understanding human emotions through speech spectrograms using deep neural network,The Journal of Supercomputing,10.1007/s11227-021-04124-5,Springer,2022-04-01,"This paper presents the analysis and classification of speech spectrograms for recognizing emotions in RAVDESS dataset. Feature extraction from speech utterances is performed using Mel-Frequency Cepstrum Coefficient. Thereafter, deep neural networks are employed to classify speech into six emotions (happy, sad, neutral, calm, disgust, and fear). Firstly, this paper presents a comprehensive comparative study on DNNs on prosodic features. The outcomes of all DNNs are presented in the paper. Secondly, the paper puts forward an analysis of Bag of Visual Words that uses speeded-up robust features (SURF) to cluster them using K-means and further classify them using support vector machine (SVM) into aforementioned emotions. Out of the five DNNs deployed, (i) Long Short-Term Memory (LSTM) on MFCC and, (ii) Multi-Layer Perceptron (MLP) classifier on MFCC, outperforms others, giving an accuracy score of 0.70 (in both cases). Further, the BoVW technique performed 53% of correct classification. Therefore, the proposed methodology constructs a Hybrid of Acoustic Features (HAF) and feeds them into an ensemble of bagged multi-layer perceptron classifier imparting an accuracy of 85%. Also, it achieves a precision score between 0.77 and 0.88 for the classification of six emotions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-021-04124-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-022-02561-y,Vertebrae segmentation in reduced radiation CT imaging for augmented reality applications,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-022-02561-y,Springer,2022-04-01,"Purpose There is growing evidence for the use of augmented reality (AR) navigation in spinal surgery to increase surgical accuracy and improve clinical outcomes. Recent research has employed AR techniques to create accurate auto-segmentations, the basis of patient registration, using reduced radiation dose intraoperative computed tomography images. In this study, we aimed to determine if spinal surgery AR applications can employ reduced radiation dose preoperative computed tomography (pCT) images. Methods We methodically decreased the imaging dose, with the addition of Gaussian noise, that was introduced into pCT images to determine the image quality threshold that was required for auto-segmentation. The Gaussian distribution’s standard deviation determined noise level, such that a scalar multiplier ( L : [0.00, 0.45], with steps of 0.03) simulated lower doses as L increased. We then enhanced the images with denoising algorithms to evaluate the effect on the segmentation. Results The pCT radiation dose was decreased to below the current lowest clinical threshold and the resulting images produced segmentations that were appropriate for input into AR applications. This held true at simulated dose L  = 0.06 (estimated 144 mAs) but not at L  = 0.09 (estimated 136 mAs). The application of denoising algorithms to the images resulted in increased artifacts and decreased bone density. Conclusions The pCT image quality that is required for AR auto-segmentation is lower than that which is currently employed in spinal surgery. We recommend a reduced radiation dose protocol of approximately 140 mAs. This has the potential to reduce the radiation experienced by patients in comparison to procedures without AR support. Future research is required to identify the specific, clinically relevant radiation dose thresholds required for surgical navigation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-022-02561-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00779-019-01296-z,Research of multi-object detection and tracking using machine learning based on knowledge for video surveillance system,Personal and Ubiquitous Computing,10.1007/s00779-019-01296-z,Springer,2022-04-01,"Recently, as the risk of crime and accidents increases, interest in security and surveillance of individuals and the public is increasing rapidly, and video surveillance system technology is continuously developing. Reliable object detection in the system is the basis of all elements using image information and it is used in various applications using the information, so accurate object detection and tracking are needed. Therefore, we propose a system for analyzing images with a knowledge-based deep learning technology for multi-object recognition and tracking enhancement. Algorithms for recognizing objects using existing convolution neural network (CNN) classifiers have a problem that it is difficult to process in real time because the processing time is increased when there are a lot of objects to be classified in the image. Therefore, we propose an algorithm that combines optical flow while maintaining the recognition performance through a knowledge-based CNN. An optical flow-based tracker can forecast the position of objects in the next frame based on the position of objects in the current frame. A CNN-based detector can detect the position of objects through a knowledge-based mining method between the two images. CNN-based detectors also carry out mining method on current frame information. This detector can select more capacity features based on the background to more accurately forecast the location of the tracked targets and targets. The fusion of the tracker and detector compensates for accumulated errors that can occur in the tracker and for drift from the detector. The experimental results show that the proposed algorithm combining CNN and optical flow can detect and trace multiple objects in a video stream, and can carry out robust detection and tracing even in a complex environment.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00779-019-01296-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.3758/s13428-021-01623-4,Automated evaluation of psychotherapy skills using speech and language technologies,Behavior Research Methods,10.3758/s13428-021-01623-4,Springer,2022-04-01,"With the growing prevalence of psychological interventions, it is vital to have measures which rate the effectiveness of psychological care to assist in training, supervision, and quality assurance of services. Traditionally, quality assessment is addressed by human raters who evaluate recorded sessions along specific dimensions, often codified through constructs relevant to the approach and domain. This is, however, a cost-prohibitive and time-consuming method that leads to poor feasibility and limited use in real-world settings. To facilitate this process, we have developed an automated competency rating tool able to process the raw recorded audio of a session, analyzing who spoke when, what they said, and how the health professional used language to provide therapy. Focusing on a use case of a specific type of psychotherapy called “motivational interviewing”, our system gives comprehensive feedback to the therapist, including information about the dynamics of the session (e.g., therapist’s vs. client’s talking time), low-level psychological language descriptors (e.g., type of questions asked), as well as other high-level behavioral constructs (e.g., the extent to which the therapist understands the clients’ perspective). We describe our platform and its performance using a dataset of more than 5000 recordings drawn from its deployment in a real-world clinical setting used to assist training of new therapists. Widespread use of automated psychotherapy rating tools may augment experts’ capabilities by providing an avenue for more effective training and skill improvement, eventually leading to more positive clinical outcomes.",http://link.springer.com/openurl/fulltext?id=doi:10.3758/s13428-021-01623-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12161-021-02179-x,Precise Hapten Design of Sulfonamides by Combining Machine Learning and 3D-QSAR Approaches,Food Analytical Methods,10.1007/s12161-021-02179-x,Springer,2022-04-01,"The cross-reactivity (CR) of monoclonal antibody (mAb) is mainly determined by chemical structure of hapten which is used to synthesize immunogen. Hapten structures are commonly designed based on experience and then verified via experiments. To save cost, a theoretical model is extremely necessary to guide hapten structure design. In this study, the relationship between sulfonamides (SAs) structures and CR of previously obtained broad-specific mAb 2G3 was analyzed to construct stable comparative molecular field assay (CoMFA) and precise machine learning models. A new hapten corresponded to mAb 3D1 with better properties was designed based on the prior knowledge from model analysis of mAb 2G3. The further models for mAb 3D1 were also built and analyzed. Finally, a complementary analysis approach was established to clarify the structural factors and physicochemical factors that affected the CR of mAb by analyzing the advantages and disadvantages of the respective models of mAb 2G3 and mAb 3D1. The proposed approach can provide valuable reference for further precise modification of hapten structure in other similar projects. Graphical abstract ",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12161-021-02179-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02142-1,Generative design of decorative architectural parts,The Visual Computer,10.1007/s00371-021-02142-1,Springer,2022-04-01,"This paper presents a method for generative design of decorative architectural parts such as corbel, moulding and panel, which usually have clear structure and aesthetic details. The method is composed of two components: offline learning and online generation. The offline learning trains a 2D CurveInfoGAN and a 3D VoxelVAE that learn the feature representations of the parts in a dataset. The online generation proceeds with an evolution procedure that evolves to product new generation of part components by selecting, crossing over and mutating features, followed by a feature-driven deformation that synthesizes the 3D mesh representation of new models. Built upon these technical components, a generative design tool is developed, which allows the user to input a decorative architectural model as a reference and then generates a set of new models that are “more of the same” as the reference and meanwhile exhibit some “surprising” elements. The experiments demonstrate the effectiveness of the method and also showcase the use of classic geometric modelling and advanced machine learning techniques in modelling of architectural parts.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02142-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1134/S1064230722020022,Frame-by-Frame Determination of Emotions in a Video Recording Using Multilayer Neural Networks,Journal of Computer and Systems Sciences International,10.1134/S1064230722020022,Springer,2022-04-01,"Abstract The architecture of a multilayer neural network is proposed for solving the problem of determining a person’s emotions in a video recording. The corresponding signs are formed. The emotions include fear, joy, sadness, etc. Well-nown and widely used datasets are used for training. With the help of combination in architecture, it is possible to complete the training of the neural network and increase the accuracy of its final version.",http://link.springer.com/openurl/fulltext?id=doi:10.1134/S1064230722020022,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02140-3,Virtual reality safety training using deep EEG-net and physiology data,The Visual Computer,10.1007/s00371-021-02140-3,Springer,2022-04-01,"Virtual reality (VR) safety training systems can enhance safety awareness while supporting health assessment in various work conditions. This paper proposes a novel VR system for construction safety training, which augments an individual’s functioning in VR via a brain–computer interface of electroencephalography (EEG) and physiology data such as blood pressure and heart rate. The use of VR aims to support high levels of interactions and immersion. Crucially, we apply novel clipping training algorithms to improve the performance of a deep EEG neural network, including batch normalization and ELU activation functions for real-time assessment. It significantly improves the system performance in time efficiency while maintaining high accuracy of over 80% on the testing datasets. For assessing workers’ competence under various construction environments, the risk assessment metrics are developed based on a statistical model and workers’ EEG data. One hundred and seventeen construction workers in Shanghai took part in the study. Nine of the participants’ EEG is identified with highly abnormal levels by the proposed evaluation metric. They have undergone further medical examinations, and among them, six are diagnosed with high-risk health conditions. It proves that our system plays a significant role in understanding workers’ physical condition, enhancing safety awareness, and reducing accidents.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02140-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40815-021-01195-7,Evolving Deep Convolutional Neural Networks by Extreme Learning Machine and Fuzzy Slime Mould Optimizer for Real-Time Sonar Image Recognition,International Journal of Fuzzy Systems,10.1007/s40815-021-01195-7,Springer,2022-04-01,"Due to the shortcomings of conventional machine hearing methods in tackling data with high-dimension search space, such as the need for initial configuration and feature extraction manually, high complexity, and long processing time, real-time processing is challenging in the field of sonar image processing. Therefore, this paper proposes a hierarchical three-stage deep learning (DL)-based approach for real-time accurate sonar image recognition. A conventional deep convolutional neural network (DCNN) is used as an automatic feature extractor in the first stage. In the second stage, ELM replaces the last fully connected layer to reduce tuning and testing time. Due to the uncertainty imposed via the addition of ELM to the model, in the third stage, the slime mould algorithm (SMA) will be used to tune the input weights and biases of the ELM. Finally, fuzzy maps are used to balance the relationship between the SMA’s exploration and exploitation phases. For evaluating the efficiency of the designed fuzzy SMA (FSMA), we first use 23 standard benchmark mathematical optimization functions. Subsequently, we employ three experimental sonar datasets to examine the efficiency of DCNN–ELM–FSMA in dealing with high-dimensional datasets. For a comprehensive investigation, we compare FSMA to the standard SMA, whale optimization algorithm (WOA), gray wolf optimizer (GWO), kalman filter (KF), henry gas solubility optimization (HGSO), Harris Hawks optimization (HHO), chimp optimization algorithm (ChOA), genetic algorithm (GA), and particle swarm optimization (PSO), with respect to convergence rate, entrapment in local minima, and detection accuracy. The results demonstrate that the proposed strategy performs better in detecting underwater anomaly targets by an average of 2.11 percent compared to the best benchmark model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40815-021-01195-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10439-022-02925-0,Convolutional Neural Network to Segment Laminae on 3D Ultrasound Spinal Images to Assist Cobb Angle Measurement,Annals of Biomedical Engineering,10.1007/s10439-022-02925-0,Springer,2022-04-01,"A recent innovation in scoliosis monitoring is the use of ultrasonography, which provides true 3D information in one scan and does not emit ionizing radiation. Measuring the severity of scoliosis on ultrasonographs requires identifying lamina pairs on the most tilted vertebrae, which is difficult and time-consuming. To expedite and automate measurement steps, this paper detailed an automatic convolutional neural network-based algorithm for identifying the laminae on 3D ultrasonographs. The predicted laminae were manually paired to measure the lateral spinal curvature on the coronal view, called the Cobb angle. In total, 130 spinal ultrasonographs of adolescents with idiopathic scoliosis recruited from a scoliosis clinic were selected, with 70 for training and 60 for testing. Data augmentation increased the effective training set size to 140 ultrasonographs. Semi-automatic Cobb measurements were compared to manual measurements on the same ultrasonographs. The semi-automatic measurements demonstrated good inter-method reliability (ICC_3,1 = 0.87) and performed better on thoracic (ICC_3,1 = 0.91) than lumbar curves (ICC_3,1 = 0.81). The mean absolute difference and standard deviation between semi-automatic and manual was 3.6° ± 3.0°. In conclusion, the semi-automatic method to measure the Cobb angle on ultrasonographs is feasible and accurate. This is the first algorithm that automates steps of Cobb angle measurement on ultrasonographs.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10439-022-02925-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-020-09609-8,Deep residual neural network based PointNet for 3D object part segmentation,Multimedia Tools and Applications,10.1007/s11042-020-09609-8,Springer,2022-04-01,"Point cloud segmentation is the premise and basis of many 3D perception tasks, such as intelligent driving, object detection and recognition, scene recognition and understanding. In this paper, we present an improved PointNet for 3D object part Segmentation, and named the proposed PointNet as Deep Residual Neural Network Based PointNet (DResNet-PointNet). The architecture of DResNet- PointNet was desigined based on the idea of residual networks. Residual networks can increase the depth of the DResNet-PointNet without network degradation. The depth of DResNet-PointNet is twice as deep as that of original PointNet model. Increasing the depth of DResNet-PointNet can improve its ability to express complex functions and generalization ability of complex classification problems, and achieve better approximation of complex functions, thus improving the accuracy of segmentation. The experimental results of part segmentation verify the feasibility and effectiveness of DResNet-PointNet.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-09609-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11247-7,D^2F: discriminative dense fusion of appearance and motion modalities for end-to-end video classification,Multimedia Tools and Applications,10.1007/s11042-021-11247-7,Springer,2022-04-01,"Recently, two-stream networks with multi-modality inputs have shown to be of vital importance for state-of-the-art video understanding. Previous deep systems typically employ a late fusion strategy, however, despite its simplicity and effectiveness, the late strategy might experience insufficient fusion due to that it performs fusion across modalities only once and treats each modality equally without discrimination. In this paper, we propose a Discriminative Dense Fusion (D^2F) network, addressing these limitations by densely inserting an attention-based fusion block at each layer. We experiment with two typical action classification benchmarks and three popular classification backbones, where our proposed module consistently outperforms state-of-the-art baselines by noticeable margins. Specifically, the two-stream VGG16, ResNet and I3D achieve accuracy of [93.5%, 69.2%], [94.6%, 70.5%], [94.1%, 72.3%] with D^2F on [UCF101, HMDB51], respectively, with absolute gains of [5.5%, 9.8%], [5.13%, 9.91%], and [0.7%, 5.9%] compared with their late fusion counterparts. The qualitative performance also demonstrates that our model can learn more informative complementary representation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11247-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06753-6,Open set task augmentation facilitates generalization of deep neural networks trained on small data sets,Neural Computing and Applications,10.1007/s00521-021-06753-6,Springer,2022-04-01,"Many application scenarios for image recognition require learning of deep networks from small sample sizes in the order of a few hundred samples per class. Then, avoiding overfitting is critical. Common techniques to address overfitting are transfer learning, reduction of model complexity and artificial enrichment of the available data by, e.g., data augmentation. A key idea proposed in this paper is to incorporate additional samples into the training that do not belong to the classes of the target task. This can be accomplished by formulating the original classification task as an open set classification task. While the original closed set classification task is not altered at inference time, the recast as open set classification task enables the inclusion of additional data during training. Hence, the original closed set classification task is augmented with an open set task during training. We therefore call the proposed approach open set task augmentation. In order to integrate additional task-unrelated samples into the training, we employ the entropic open set loss originally proposed for open set classification tasks and also show that similar results can be obtained with a modified sum of squared errors loss function. Learning with the proposed approach benefits from the integration of additional “unknown” samples, which are often available, e.g., from open data sets, and can then be easily integrated into the learning process. We show that this open set task augmentation can improve model performance even when these additional samples are rather few or far from the domain of the target task. The proposed approach is demonstrated on two exemplary scenarios based on subsets of the ImageNet and Food-101 data sets as well as with several network architectures and two loss functions. We further shed light on the impact of the entropic open set loss on the internal representations formed by the networks. Open set task augmentation is particularly valuable when no additional data from the target classes are available—a scenario often faced in practice.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06753-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10115-022-01652-1,Chinese adversarial examples generation approach with multi-strategy based on semantic,Knowledge and Information Systems,10.1007/s10115-022-01652-1,Springer,2022-04-01,"Recent studies have shown that after adding small perturbations that are imperceptible to humans, deep neural networks (DNNs) with good performance and popular application are likely to produce incorrect results. These processed samples are called adversarial examples. High-quality adversarial examples help to increase the accuracy of estimating the robustness of the network model, thereby reducing the security risks behind the unreal high accuracy of the model. And there are few existing researches on Chinese texts in this field, therefore, this paper proposes a Chinese adversarial examples generation approach with multi-strategy based on semantic called GreedyAttack. Based on the analysis of the characteristics of the Chinese version, the ranking of the influence of each word in the text is obtained according to the calculation formula of the word importance with the weighted part-of-speech. Next, five strategies including synonymous words, similar words of form, similar words of sound, pinyin rewriting, and phrase disassembly are combined to replace the original words, and then, the black box attack on the DNNs models is completed. The method is evaluated by attacking the BERT and ERNIE models on three data sets. The results indicate that the adversarial examples generated by the method can effectively reduce the accuracy of the model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10115-022-01652-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-06960-9,Literature review: efficient deep neural networks techniques for medical image analysis,Neural Computing and Applications,10.1007/s00521-022-06960-9,Springer,2022-04-01,"Significant evolution in deep learning took place in 2010, when software developers started using graphical processing units for general-purpose applications. From that date, the deep neural network (DNN) started progressive steps across different applications ranging from natural language processing to hyperspectral image processing. The convolutional neural network (CNN) mostly triggers the interest, as it is considered one of the most powerful ways to learn useful representations of images and other structured data. The revolution of DNNs in medical imaging (MI) came in 2012, when Li launched ImageNet, a free database of more than 14 million labeled medical images. This state-of-the-art work presents a comprehensive study for the recent DNNs research directions applied in MI analysis. Clinical and pathological analysis through a selected patch of most cited researches is introduced. It will be shown how DNNs are able to tackle medical problems: classification, detection, localization, segmentation, and automatic diagnosis. Datasets comprises a range of imaging technologies: X-Ray, MRI, CT, Ultrasound, PET, Fluorescene Angiography, and even photographic images. This work surveys different patterns of DNNs and focuses somehow on the CNN, which offers an outstanding percentage of solutions compared to other DNNs structures. CNN emphasizes image features and has well-known architectures. On the other hand, limitations beyond DNNs training and execution time will be explained. Problems related to data augmentation and image annotation will be analyzed among a multiple of high standard publications. Finally, a comparative study of existing software frameworks supporting DNNs and future research directions in the area will be presented. From all presented works it could be deduced that the use of DNNs in healthcare is still in its early stages, there are strong initiatives in academia and industry to pursue healthcare projects based on DNNs.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-06960-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00330-021-08329-3,Infratentorial lesions in multiple sclerosis patients: intra- and inter-rater variability in comparison to a fully automated segmentation using 3D convolutional neural networks,European Radiology,10.1007/s00330-021-08329-3,Springer,2022-04-01,"Objective Automated quantification of infratentorial multiple sclerosis lesions on magnetic resonance imaging is clinically relevant but challenging. To overcome some of these problems, we propose a fully automated lesion segmentation algorithm using 3D convolutional neural networks (CNNs). Methods The CNN was trained on a FLAIR image alone or on FLAIR and T1-weighted images from 1809 patients acquired on 156 different scanners. An additional training using an extra class for infratentorial lesions was implemented. Three experienced raters manually annotated three datasets from 123 MS patients from different scanners. Results The inter-rater sensitivity (SEN) was 80% for supratentorial lesions but only 62% for infratentorial lesions. There was no statistically significant difference between the inter-rater SEN and the SEN of the CNN with respect to the raters. For supratentorial lesions, the CNN featured an intra-rater intra-scanner SEN of 0.97 (R1 = 0.90, R2 = 0.84) and for infratentorial lesion a SEN of 0.93 (R1 = 0.61, R2 = 0.73). Conclusion The performance of the CNN improved significantly for infratentorial lesions when specifically trained on infratentorial lesions using a T1 image as an additional input and matches the detection performance of experienced raters. Furthermore, for infratentorial lesions the CNN was more robust against repeated scans than experienced raters. Key Points • A 3D convolutional neural network was trained on MRI data from 1809 patients (156 different scanners) for the quantification of supratentorial and infratentorial multiple sclerosis lesions. • Inter-rater variability was higher for infratentorial lesions than for supratentorial lesions. The performance of the 3D convolutional neural network (CNN) improved significantly for infratentorial lesions when specifically trained on infratentorial lesions using a T1 image as an additional input. • The detection performance of the CNN matches the detection performance of experienced raters.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00330-021-08329-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07077-9,Sports video athlete detection based on deep learning,Neural Computing and Applications,10.1007/s00521-022-07077-9,Springer,2022-03-31,"The deep fusion of sports and machine vision has become a research hot spot in sports video target detection, athlete state recovery and sports promotion. On the basis of in-depth study, it can detect a large number of sports videos, complete the drawing and analysis of human body detection model, and detect and evaluate the posture of corresponding athletes in the video, which can save a lot of costs and maximize the more professional training of athletes. In order to solve the above problems, this paper innovatively completes the automatic language description of sports video based on time-sharing memory algorithm. Its principle is to realize the accurate decomposition of athletes' sports data through the mapping relationship between the corresponding letter sequence and video sequence in time-sharing memory. In order to capture the key posture of athletes' sports video, this paper innovatively proposes an object extraction algorithm based on athletes' skeleton motion enhancement. In practical application, based on the key pose capture, it is necessary to train the depth selection network in time to extract the key pose of the skeleton. Based on this network, it can enhance the key posture of bone information and accurately express its related features. After extracting the actual athlete's bone information, we need to fine-tune the training network to realize the accurate recognition of key features. Based on the above key algorithms, this paper designs a sports video athlete detection system based on deep learning and makes an experimental research on the related sports video. The experimental results show that the detection accuracy of athletes' sports video is improved by nearly 10% compared with the traditional convolution network recognition algorithm, so the algorithm has obvious advantages in recognition accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07077-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-09293-8,A union of deep learning and swarm-based optimization for 3D human action recognition,Scientific Reports,10.1038/s41598-022-09293-8,Nature,2022-03-31,"Human Action Recognition (HAR) is a popular area of research in computer vision due to its wide range of applications such as surveillance, health care, and gaming, etc. Action recognition based on 3D skeleton data allows simplistic, cost-efficient models to be formed making it a widely used method. In this work, we propose DSwarm-Net , a framework that employs deep learning and swarm intelligence-based metaheuristic for HAR that uses 3D skeleton data for action classification. We extract four different types of features from the skeletal data namely: Distance, Distance Velocity, Angle, and Angle Velocity, which capture complementary information from the skeleton joints for encoding them into images. Encoding the skeleton data features into images is an alternative to the traditional video-processing approach and it helps in making the classification task less complex. The Distance and Distance Velocity encoded images have been stacked depth-wise and fed into a Convolutional Neural Network model which is a modified version of Inception-ResNet. Similarly, the Angle and Angle Velocity encoded images have been stacked depth-wise and fed into the same network. After training these models, deep features have been extracted from the pre-final layer of the networks, and the obtained feature representation is optimized by a nature-inspired metaheuristic, called Ant Lion Optimizer, to eliminate the non-informative or misleading features and to reduce the dimensionality of the feature set. DSwarm-Net has been evaluated on three publicly available HAR datasets, namely UTD-MHAD, HDM05, and NTU RGB+D 60 achieving competitive results, thus confirming the superiority of the proposed model compared to state-of-the-art models.",https://www.nature.com/articles/s41598-022-09293-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-09495-0,Machine learning to predict effective reaction rates in 3D porous media from pore structural features,Scientific Reports,10.1038/s41598-022-09495-0,Nature,2022-03-31,"Large discrepancies between well-mixed reaction rates and effective reactions rates estimated under fluid flow conditions have been a major issue for predicting reactive transport in porous media systems. In this study, we introduce a framework that accurately predicts effective reaction rates directly from pore structural features by combining 3D pore-scale numerical simulations with machine learning (ML). We first perform pore-scale reactive transport simulations with fluid–solid reactions in hundreds of porous media and calculate effective reaction rates from pore-scale concentration fields. We then train a Random Forests model with 11 pore structural features and effective reaction rates to quantify the importance of structural features in determining effective reaction rates. Based on the importance information, we train artificial neural networks with varying number of features and demonstrate that effective reaction rates can be accurately predicted with only three pore structural features, which are specific surface, pore sphericity, and coordination number. Finally, global sensitivity analyses using the ML model elucidates how the three structural features affect effective reaction rates. The proposed framework enables accurate predictions of effective reaction rates directly from a few measurable pore structural features, and the framework is readily applicable to a wide range of applications involving porous media flows.",https://www.nature.com/articles/s41598-022-09495-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12858-4,Adaptive recurrent nonnegative matrix factorization with phase compensation for Single-Channel speech enhancement,Multimedia Tools and Applications,10.1007/s11042-022-12858-4,Springer,2022-03-30,"The speech signals are affected by the background noise distortion that is unfavorable to both the intelligibility as well as the speech quality. Most of the speech processing algorithms function with the spectral magnitude without consideration of the spectral phase by leaving them unexplored and unstructured. The proposed single channel speech enhancement model called the Adaptive Recurrent Nonnegative Matrix Factorization (AR-NMF) is designed based on the phase compensation strategy with deep learning. The two major phases considered here are the training phase and the testing phase. During the process of training, the noisy speech signal is decomposed by the Hurst exponent-based Empirical Mode Decomposition (HEMD) and is converted into the frequency domain using Short Time Fourier Transform. Further, the new AR-NMF is used for denoising, where the tuning factor is optimally generated by the optimized RNN. Here, the hidden neurons are optimized using the proposed Adaptive Attack Power-based Sail Fish Optimization (AAP-SFO) with consideration of minimizing the Mean Absolute Error between the actual value and the predicted value. Finally, this phase compensated speech signal is given to the ISTFT that results in the final denoised clean speech signal. From the analysis, the CSED of AAP-SFO-AR-NMF for the street noise is 58.24%, 57.34%, 56.72%, and 77.37% more than RNMF, esHRNR, esTSNR, and Vuvuzela respectively. The performance of the proposed deep enhancement method is extensively evaluated and compared to diverse adverse noisy environments that describe the superiority of the proposed method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12858-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12833-z,Video analytics using deep learning for crowd analysis: a review,Multimedia Tools and Applications,10.1007/s11042-022-12833-z,Springer,2022-03-29,"Gathering a large number of people in a shared physical area is very common in urban culture. Although there are limitless examples of mega crowds, the Islamic religious ritual, the Hajj, is considered as one of the greatest crowd scenarios in the world. The Hajj is carried out once in a year with a congregation of millions of people when the Muslims visit the holy city of Makkah at a given time and date. Such a big crowd is always prone to public safety issues, and therefore requires proper measures to ensure safe and comfortable arrangement. Through the advances in computer vision based scene understanding, automatic analysis of crowd scenes is gaining popularity. However, existing crowd analysis algorithms might not be able to correctly interpret the video content in the context of the Hajj. This is because the Hajj is a unique congregation of millions of people crowded in a small area, which can overwhelm the use of existing video and computer vision based sophisticated algorithms. Through our studies on crowd analysis, crowd counting, density estimation, and the Hajj crowd behavior, we faced the need of a review work to get a research direction for abnormal behavior analysis of Hajj pilgrims. Therefore, this review aims to summarize the research works relevant to the broader field of video analytics using deep learning with a special focus on the visual surveillance in the Hajj. The review identifies the challenges and leading-edge techniques of visual surveillance in general, which may gracefully be adaptable to the applications of Hajj and Umrah. The paper presents detailed reviews on existing techniques and approaches employed for crowd analysis from crowd videos, specifically the techniques that use deep learning in detecting abnormal behavior. These observations give us the impetus to undertake a painstaking yet exhilarating journey on crowd analysis, classification and detection of any abnormal movement of the Hajj pilgrims. Furthermore, because the Hajj pilgrimage is the most crowded domain for video-related extensive research activities, this study motivates us to critically analyze the crowd on a large scale.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12833-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00138-022-01294-x,Convolutional neural network-based cross-corpus speech emotion recognition with data augmentation and features fusion,Machine Vision and Applications,10.1007/s00138-022-01294-x,Springer,2022-03-28,"Speech emotion recognition (SER) is one of the most challenging and active research topics in data science due to its wide range of applications in human–computer interaction, computer games, mobile services and psychological assessment. In the past, several studies have employed handcrafted features to classify emotions and achieved good classification accuracy. However, such features degrade the classification accuracy in complex scenarios. Thus, recent studies employed deep learning models to automatically extract the local representation from given audio signals. Though, automated feature engineering overcomes the issues of handcrafted feature extraction approach. However, still there is a need to further improve the performance of reported techniques. This is because, in reported techniques, single-layer and two-layer convolutional neural networks (CNNs) were used and these architectures are not capable of learning optimal features from complex speech signals. Thus, to overcome this limitation, this study proposed a novel SER framework, which applies data augmentation methods before extracting seven informative feature sets from each utterance. The extracted feature vector is used as input to the 1D CNN for emotions recognition using the EMO-DB, RAVDESS and SAVEE databases. Moreover, this study also proposed a cross-corpus SER model using the all audio files of common emotions of aforementioned databases. The experimental results showed that our proposed SER framework outperformed existing SER frameworks. Specifically, the proposed SER framework obtained 96.7% accuracy for EMO-DB with all utterances in seven emotions, 90.6% RAVDESS with all utterances in eight emotions, 93.2% for SAVEE with all utterances in seven emotions and 93.3% for cross-corpus with 1930 utterances in six emotions. We believe that our proposed framework will bring significant contribute to SER domain.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00138-022-01294-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07107-6,OtoXNet—automated identification of eardrum diseases from otoscope videos: a deep learning study for video-representing images,Neural Computing and Applications,10.1007/s00521-022-07107-6,Springer,2022-03-28,"The lack of an objective method to evaluate the eardrum is a critical barrier to an accurate diagnosis. Eardrum images are classified into normal or abnormal categories with machine learning techniques. If the input is an otoscopy video, a traditional approach requires great effort and expertise to manually determine the representative frame(s). In this paper, we propose a novel deep learning-based method, called OtoXNet, which automatically learns features for eardrum classification from otoscope video clips. We utilized multiple composite image generation methods to construct a highly representative version of otoscopy videos to diagnose three major eardrum diseases, i.e., otitis media with effusion, eardrum perforation, and tympanosclerosis versus normal (healthy). We compared the performance of OtoXNet against methods that either use a single composite image or a keyframe selected by an experienced human. Our dataset consists of 394 otoscopy videos from 312 patients and 765 composite images before augmentation. OtoXNet with multiple composite images achieved 84.8% class-weighted accuracy with 3.8% standard deviation, whereas with the human-selected keyframes and single composite images, the accuracies were respectively, 81.8% ± 5.0% and 80.1% ± 4.8% on multi-class eardrum video classification task using an eightfold cross-validation scheme. A paired t -test shows that there is a statistically significant difference ( p -value of 1.3 × 10^–2) between the performance values of OtoXNet (multiple composite images) and the human-selected keyframes. Contrarily, the difference in means of keyframe and single composites was not significant ( p  = 5.49 × 10^–1). OtoXNet surpasses the baseline approaches in qualitative results. The use of multiple composite images in analyzing eardrum abnormalities is advantageous compared to using single composite images or manual keyframe selection.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07107-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-022-02182-7,Machine learning-based fast transcoding for low-power video communication in Internet of Things,"Signal, Image and Video Processing",10.1007/s11760-022-02182-7,Springer,2022-03-27,"Aiming at the low-power video communication with low latency at resource-constrained terminals in the Internet of Things, a machine learning-based fast transcoding from distributed video coding (DVC) to high-efficiency video coding (HEVC) is put forward. In order to accelerate the transcoding, the DVC decoding information is efficiently exploited to reduce HEVC encoding complexity at both levels of coding unit (CU) and prediction unit (PU). First of all, the predictions of CU partition and PU modes are regarded as two binary classification tasks. Subsequently, the initial features are extracted from the DVC decoding information and the support vector machine (SVM)-recursive feature elimination algorithm is adopted to select feature vectors to construct the training data which is used to train the SVM classifiers for CU and PU, respectively. By means of the top-down division prediction method, the CU partition is first determined by the trained SVM classifier. For the CUs which are not further split, the PU modes will be predicted to terminate the quad-tree coding process of HEVC in advance, so that HEVC encoding complexity is reduced. Experimental results show that the proposed algorithm can reduce 57.64% computational complexity on average with Bjøntegaard delta bit-rate 2.43%. In terms of transcoding time efficiency, the proposed algorithm outperforms the state-of-the-art fast DVC to HEVC transcoding algorithms based on machine learning.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-022-02182-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13042-022-01539-1,Multi-distance metric network for few-shot learning,International Journal of Machine Learning and Cybernetics,10.1007/s13042-022-01539-1,Springer,2022-03-25,"Few-shot learning aims to make classification when few samples are available. In general, metric-based methods map images into a space by learning the embedding function. However, conventional metric-based methods rely on a single distance value, which does not pay attention to the shallow features. In this paper, we propose a multi-distance metric network (MDM-Net) by employing a multi-output embedding network to map samples into different feature spaces. In addition, we maximize the inter-class distance which is popular in metric learning field to improve the performance of few-shot classifier. Furthermore, we design a task-adaptive margin to adjust the distance between different sample pairs, and we found that the distance loss combined with cross-entropy loss is beneficial to achieve better results in meta-task training. The proposed method is verified by tests on miniImageNet and FC100 these two benchmarks for 5-way 1-shot classification task and 5-way 5-shot classification task with competitive results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13042-022-01539-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40747-022-00712-x,Interactive spatio-temporal feature learning network for video foreground detection,Complex & Intelligent Systems,10.1007/s40747-022-00712-x,Springer,2022-03-25,"Video foreground detection (VFD), as one of the basic pre-processing tasks, is very essential for subsequent target tracking and recognition. However, due to the interference of shadow, dynamic background, and camera jitter, constructing a suitable detection network is still challenging. Recently, convolution neural networks have proved its reliability in many fields with their powerful feature extraction ability. Therefore, an interactive spatio-temporal feature learning network (ISFLN) for VFD is proposed in this paper. First, we obtain the deep and shallow spatio-temporal information of two paths with multi-level and multi-scale. The deep feature is conducive to enhancing feature identification capabilities, while the shallow feature is dedicated to fine boundary segmentation. Specifically, an interactive multi-scale feature extraction module (IMFEM) is designed to facilitate the information transmission between different types of features. Then, a multi-level feature enhancement module (MFEM), which provides precise object knowledge for decoder, is proposed to guide the coding information of each layer by the fusion spatio-temporal difference characteristic. Experimental results on LASIESTA, CDnet2014, INO, and AICD datasets demonstrate that the proposed ISFLN is more effective than the existing advanced methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40747-022-00712-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07166-9,Control of adaptive running platform based on machine vision technologies and neural networks,Neural Computing and Applications,10.1007/s00521-022-07166-9,Springer,2022-03-25,"The paper considers the problem of selection of the optimal method for controlling an adaptive running platform for movement organization in virtual reality. The analysis of existing approaches for the control of such systems is carried out, a list of existing methods is formed, and new functions are developed. In order to solve the problem of user positioning within the framework of an adaptive running platform, two approaches, based on virtual reality trackers and using a machine vision, are considered and implemented. The problem of the study includes the choice of the optimal method of controlling an adaptive running platform to ensure maximum user comfort when walking on it. The study methodology includes the creation of a simplified model of human interaction with a running platform, the formalization of the assessment of the quality of movement, various methods of human positioning and platform control functions, followed by an experimental part on their comparison and the search for an optimal approach to running platform control. It was proved that machine vision technologies and neural networks allow positioning a person with sufficient accuracy, and they are deprived of the disadvantages of trackers. Moreover, comparative studies of six different control functions were carried out with the tracker-based positioning method and with the use of machine vision technologies. The most universal is the nonlinear one, a detailed zonal and proportional–differential functions are recommended for the tracker-based positioning method, linear and proportional–differential functions are recommended for positioning based on machine vision. The scientific novelty of the study consists in the formalization and comparison of various control methods of adaptive running platforms (based on linear and nonlinear functions, proportional differential law and neural networks), methods of positioning a person on them (using cameras and trackers), which will expand the area of knowledge about the optimal control functions of this class of devices. The practical significance of the research lies in a comprehensive description of the solution of the problems of organizing the control of adaptive running platforms and positioning a person by various methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07166-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07116-5,VDNet: video deinterlacing network based on coarse adaptive module and deformable recurrent residual network,Neural Computing and Applications,10.1007/s00521-022-07116-5,Springer,2022-03-24,"Interlacing is the technique that overlaps odd lines from an odd frame and even lines from an even frame to increase the perceived frame rate in TV displays without increasing bandwidth. On the other hand, since original frames are not stored, deinterlacing is the technique introduced for reversing this process and restore the original or progressive video. Existing deinterlacing approaches focus on restoring a single interlaced frame without optimally leveraging the temporal information available. In this paper, we propose VDNet , which to the best of our knowledge, is the first deep learning-based deinterlacing framework that considers inter-frame correlation. Our proposed method addresses deinterlacing by splitting the frames and regenerating the missing lines using a simple coarse method (base image sequence) before combining them with the residual image sequence for refinement. For the deinterlaced base image sequence, our data module uses spatial and temporal information to fill in the missing areas and leverages our coarse adaptive module to interpolate them. The residual module then leverages our proposed Deformable Recurrent Residual Network to optimally enhance and aggregate the features extracted from the interlaced video. Our method then refines the base image sequence using the residual image sequence generated from the residual module. After reconstructing the progressive frames, our proposed Spatial-Temporal Correlation Loss uses the information provided by the existing interlaced video to further smooth and boost the deinterlaced output. We perform extensive experiments to demonstrate our proposed VDNet ’s incredible quantitative performance. Moreover, we design our model to be lightweight and efficient.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07116-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00330-022-08708-4,Evaluation of deep learning reconstructed high-resolution 3D lumbar spine MRI,European Radiology,10.1007/s00330-022-08708-4,Springer,2022-03-24,"Objectives To compare interobserver agreement and image quality of 3D T2-weighted fast spin echo (T2w-FSE) L-spine MRI images processed with a deep learning reconstruction (DLRecon) against standard-of-care (SOC) reconstruction, as well as against 2D T2w-FSE images. The hypothesis was that DLRecon 3D T2w-FSE would afford improved image quality and similar interobserver agreement compared to both SOC 3D and 2D T2w-FSE. Methods Under IRB approval, patients who underwent routine 3-T lumbar spine (L-spine) MRI from August 17 to September 17, 2020, with both isotropic 3D and 2D T2w-FSE sequences, were retrospectively included. A DLRecon algorithm, with denoising and sharpening properties was applied to SOC 3D k-space to generate 3D DLRecon images. Four musculoskeletal radiologists blinded to reconstruction status evaluated randomized images for motion artifact, image quality, central/foraminal stenosis, disc degeneration, annular fissure, disc herniation, and presence of facet joint cysts. Inter-rater agreement for each graded variable was evaluated using Conger’s kappa ( κ ). Results Thirty-five patients (mean age 58 ± 19, 26 female) were evaluated. 3D DLRecon demonstrated statistically significant higher median image quality score (2.0/2) when compared to SOC 3D (1.0/2, p < 0.001), 2D axial (1.0/2, p < 0.001), and 2D sagittal sequences (1.0/2, p value < 0.001). κ ranges (and 95% CI) for foraminal stenosis were 0.55–0.76 (0.32–0.86) for 3D DLRecon, 0.56–0.73 (0.35–0.84) for SOC 3D, and 0.58–0.71 (0.33–0.84) for 2D. Mean κ (and 95% CI) for central stenosis at L4-5 were 0.98 (0.96–0.99), 0.97 (0.95–0.99), and 0.98 (0.96–0.99) for 3D DLRecon, 3D SOC and 2D, respectively. Conclusions DLRecon 3D T2w-FSE L-spine MRI demonstrated higher image quality and similar interobserver agreement for graded variables of interest when compared to 3D SOC and 2D imaging. Key Points • 3D DLRecon T2w-FSE isotropic lumbar spine MRI provides improved image quality when compared to 2D MRI, with similar interobserver agreement for clinical evaluation of pathology . • 3D DLRecon images demonstrated better image quality score (2.0/2) when compared to standard-of-care (SOC) 3D (1.0/2), p value < 0.001; 2D axial (1.0/2), p value < 0.001; and 2D sagittal sequences (1.0/2), p value < 0.001 . • Interobserver agreement for major variables of interest was similar among all sequences and reconstruction types. For foraminal stenosis, κ ranged from 0.55 to 0.76 (95% CI 0.32–0.86) for 3D DLRecon, 0.56–0.73 (95% CI 0.35–0.84) for standard-of-care (SOC) 3D, and 0.58–0.71 (95% CI 0.33–0.84) for 2D .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00330-022-08708-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40747-022-00713-w,A DCRNN-based ensemble classifier for speech emotion recognition in Odia language,Complex & Intelligent Systems,10.1007/s40747-022-00713-w,Springer,2022-03-24,"The Odia language is an old Eastern Indo-Aryan language, spoken by 46.8 million people across India. We have designed an ensemble classifier using Deep Convolutional Recurrent Neural Network for Speech Emotion Recognition (SER). This study presents a new approach for SER tasks motivated by recent research on speech emotion recognition. Initially, we extract utterance-level log Mel-spectrograms and their first and second derivative (Static, Delta, and Delta-delta), represented as 3-D log Mel-spectrograms. We utilize deep convolutional neural networks deep convolutional neural networks to extract the deep features from 3-D log Mel-spectrograms. Then a bi-directional-gated recurrent unit network is applied to express long-term temporal dependency out of all features to produce utterance-level emotion. Finally, we use ensemble classifiers using Softmax and Support Vector Machine classifier to improve the final recognition rate. In this way, our proposed framework is trained and tested on Odia (Seven emotional states) and RAVDESS (Eight emotional states) dataset. The experimental results reveal that an ensemble classifier performs better instead of a single classifier. The accuracy levels reached are 85.31% and 77.54%, outperforming some state-of-the-art frameworks on the Odia and RAVDESS datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40747-022-00713-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11063-022-10796-8,Improving Target-driven Visual Navigation with Attention on 3D Spatial Relationships,Neural Processing Letters,10.1007/s11063-022-10796-8,Springer,2022-03-23,"Embodied Artificial Intelligence has become popular in recent years. Its task shifts from focusing on internet images to active settings, involving an embodied agent to perceive and act within 3D environments. In this paper, we study the Target-driven Visual Navigation (TDVN) in 3D indoor scenes using deep reinforcement learning techniques. The generalization of TDVN is a long-standing ill-posed issue, where the agent is expected to transfer intelligent knowledge from training domains to unseen domains. To address this issue, we propose a model that combines visual and relational graph features to learn the navigation policy. Graph convolutional networks are used to obtain graph features, which encodes spatial relations between objects. We also adopt a Target Skill Extension module to generate sub-targets, in order to allow the agent to learn from its failures. For evaluation, we perform experiments in the AI2-THOR. Experimental results show that our proposed model outperforms baselines under various metrics.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11063-022-10796-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41467-022-29236-1,Deep-learning two-photon fiberscopy for video-rate brain imaging in freely-behaving mice,Nature Communications,10.1038/s41467-022-29236-1,Nature,2022-03-22,"Scanning two-photon (2P) fiberscopes (also termed endomicroscopes) have the potential to transform our understanding of how discrete neural activity patterns result in distinct behaviors, as they are capable of high resolution, sub cellular imaging yet small and light enough to allow free movement of mice. However, their acquisition speed is currently suboptimal, due to opto-mechanical size and weight constraints. Here we demonstrate significant advances in 2P fiberscopy that allow high resolution imaging at high speeds (26 fps) in freely-behaving mice. A high-speed scanner and a down-sampling scheme are developed to boost imaging speed, and a deep learning (DL) algorithm is introduced to recover image quality. For the DL algorithm, a two-stage learning transfer strategy is established to generate proper training datasets for enhancing the quality of in vivo images. Implementation enables video-rate imaging at ~26 fps, representing 10-fold improvement in imaging speed over the previous 2P fiberscopy technology while maintaining a high signal-to-noise ratio and imaging resolution. This DL-assisted 2P fiberscope is capable of imaging the arousal-induced activity changes in populations of layer2/3 pyramidal neurons in the primary motor cortex of freely-behaving mice, providing opportunities to define the neural basis of behavior. The acquisition speed of two-photon fiberscopes is currently suboptimal. Here the authors report advances, including a high-speed scanner and down-sampling scheme as well as a two-stage deep learning (DL) algorithm, to allow high-speed, high-resolution imaging in freely moving mice.",https://www.nature.com/articles/s41467-022-29236-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00330-022-08687-6,﻿Highly accelerated 3D MPRAGE using deep neural network–based reconstruction for brain imaging in children and young adults,European Radiology,10.1007/s00330-022-08687-6,Springer,2022-03-22,"Objectives This study aimed to accelerate the 3D magnetization–prepared rapid gradient-echo (MPRAGE) sequence for brain imaging through the deep neural network (DNN). Methods This retrospective study used the k-space data of 240 scans (160 for the training set, mean ± standard deviation age, 93 ± 80 months, 94 males; 80 for the test set, 106 ± 83 months, 44 males) of conventional MPRAGE (C-MPRAGE) and 102 scans (77 ± 74 months, 52 males) of both C-MPRAGE and accelerated MPRAGE. All scans were acquired with 3T scanners. DNN was developed with simulated-acceleration data generated by under-sampling. Quantitative error metrics were compared between images reconstructed with DNN, GRAPPA, and E-SPIRIT using the paired t -test. Qualitative image quality was compared between C-MPRAGE and accelerated MPRAGE reconstructed with DNN (DNN-MPRAGE) by two readers. Lesions were segmented and the agreement between C-MPRAGE and DNN-MPRAGE was assessed using linear regression. Results Accelerated MPRAGE reduced scan times by 38% compared to C-MPRAGE (142 s vs. 320 s). For quantitative error metrics, DNN showed better performance than GRAPPA and E-SPIRIT ( p < 0.001). For qualitative evaluation, overall image quality of DNN-MPRAGE was comparable ( p > 0.999) or better ( p = 0.025) than C-MPRAGE, depending on the reader. Pixelation was reduced in DNN-MPRAGE ( p < 0.001). Other qualitative parameters were comparable ( p > 0.05). Lesions in C-MPRAGE and DNN-MPRAGE showed good agreement for the dice similarity coefficient (= 0.68) and linear regression ( R ^2 = 0.97; p < 0.001). Conclusions DNN-MPRAGE reduced acquisition time by 38% and revealed comparable image quality to C-MPRAGE. Key Points • DNN-MPRAGE reduced acquisition times by 38% . • DNN-MPRAGE outperformed conventional reconstruction on accelerated scans (SSIM of DNN-MPRAGE = 0.96, GRAPPA = 0.43, E-SPIRIT = 0.88; p < 0.001) . • Compared to C-MPRAGE scans, DNN-MPRAGE showed improved mean scores for overall image quality (2.46 vs. 2.52; p < 0.001) or comparable perceived SNR (2.56 vs. 2.58; p = 0.08).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00330-022-08687-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s12967-022-03335-5,Computer-assisted three-dimensional quantitation of programmed death-ligand 1 in non-small cell lung cancer using tissue clearing technology,Journal of Translational Medicine,10.1186/s12967-022-03335-5,BioMed Central,2022-03-16,"Immune checkpoint blockade therapy has revolutionized non-small cell lung cancer treatment. However, not all patients respond to this therapy. Assessing the tumor expression of immune checkpoint molecules, including programmed death-ligand 1 (PD-L1), is the current standard in predicting treatment response. However, the correlation between PD-L1 expression and anti-PD-1/PD-L1 treatment response is not perfect. This is partly caused by tumor heterogeneity and the common practice of assessing PD-L1 expression based on limited biopsy material. To overcome this problem, we developed a novel method that can make formalin-fixed, paraffin-embedded tissue translucent, allowing three-dimensional (3D) imaging. Our protocol can process tissues up to 150 μm in thickness, allowing anti-PD-L1 staining of the entire tissue and producing high resolution 3D images. Compared to a traditional 4 μm section, our 3D image provides 30 times more coverage of the specimen, assessing PD-L1 expression of approximately 10 times more cells. We further developed a computer-assisted PD-L1 quantitation method to analyze these images, and we found marked variation of PD-L1 expression in 3D. In 5 of 33 needle-biopsy-sized specimens (15.2%), the PD-L1 tumor proportion score (TPS) varied by greater than 10% at different depth levels. In 14 cases (42.4%), the TPS at different depth levels fell into different categories (< 1%, 1–49%, or ≥ 50%), which can potentially influence treatment decisions. Importantly, our technology permits recovery of the processed tissue for subsequent analysis, including histology examination, immunohistochemistry, and mutation analysis. In conclusion, our novel method has the potential to increase the accuracy of tumor PD-L1 expression assessment and enable precise deployment of cancer immunotherapy.",https://www.biomedcentral.com/openurl?doi=10.1186/s12967-022-03335-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07083-x,Research on simulation of 3D human animation vision technology based on an enhanced machine learning algorithm,Neural Computing and Applications,10.1007/s00521-022-07083-x,Springer,2022-03-14,"This paper provides an in-depth analysis and study of the simulation of 3D human animation visualization techniques by enhancing machine learning algorithms. Based on the statistical analysis of the data obtained from different measurement methods, the extraction of human body feature parameters based on millimeter-wave point cloud data is realized, and the 3D reconstruction and simulation of the human body are realized using parametric human modeling software. In video-based action recognition, most methods are data-driven and use deep networks to automatically learn features of the entire video image. In this process, specific research on human actions is not included or reflected. However, human action recognition is a processing of the semantic level of video content. Realizing universal human action recognition requires a semantic understanding of human behavior. Firstly, the geometric feature analysis of the 3D scanned human model is performed to extract the human body shape characteristic parameters, and the research on the analysis and estimation methods of body shape characteristic parameters is carried out to establish the human body shape parameter relationship model; then, the millimeter-wave point cloud is calculated and measured, the Li group features extracted using the group skeletal representation model with high data dimensionality, to be able to process the high-dimensional data, while reducing the complexity of the recognition process and speeding up the computation, feature learning and classification are performed with convolutional neural networks. To verify the better library portability and robustness of the method in this paper, the method was tested on a self-built human action database in the laboratory, and an average recognition rate of 97.26% was achieved. Meanwhile, this paper investigates the natural interaction application of virtual characters in a virtual learning environment based on human action recognition. Four testers tested the virtual human–computer interaction system of this paper, respectively, and the final test results show that the system has flexibility and stability.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07083-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40510-022-00402-x,"TIP, TORQUE & ROTATIONS: How accurately do digital superimposition software packages quantify tooth movement?",Progress in Orthodontics,10.1186/s40510-022-00402-x,Springer,2022-03-14,"Background To investigate the accuracy of three different 3D digital model registration software for tip, torque and rotation measurements, with reference to a 3D digital virtual setup. Twenty maxillary and mandibular pre-treatment scans of patients undergoing clear aligner therapy were used. Digital setups were generated from pre-treatment scans using a tooth movement software. Both the pretreatment digital scans (T1) and digital setups (T2) were converted to STL files to be exported to the 3 studied software that employed: (1) Semiautomatic best fit registration (S-BF), (2) Interactive surface-based registration (I-SB), and (3) Automatic best fit registration (A-BF) respectively. Changes in tip, torque and rotation were calculated for all the registered pairs. Results The change in tooth position was compared between the calculated tooth movement using each of the registration software packages versus the actual generated tooth movement from the digital setups. Continuous data was expressed as mean and standard deviation. Intra Class Correlation Coefficient for agreement between digital simulation and each software was used. Intra and Inter-examiner reliabilities were also assessed using Intra Class Correlation Coefficient. Significance of the obtained results was expressed at p  ≤ 0.01. Semiautomatic best fit registration software showed excellent agreement (> 0.90) for all tooth movements, except for good agreement for torque (0.808). Interactive surface-based registration software showed moderate agreement for all measurements (0.50 and < 0.75), except for good agreement for rotation (0.783). Automatic best fit registration software demonstrated excellent agreement (> 0.90) for rotation, good agreement for tip (0.890) and moderate agreement for torque (0.740). Conclusions Overall, semiautomatic best fit registration software consistently showed excellent agreement in superimpositions compared to other software types. Automatic best fit registration software consistently demonstrated better agreement for mandibular superimpositions, compared to others. Accuracy of digital model superimpositions for tooth movements studied in superimposition studies, can be attributed to the algorithm employed for quantification.",https://www.biomedcentral.com/openurl?doi=10.1186/s40510-022-00402-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00779-022-01672-2,Personalized healthcare museum exhibition system design based on VR and deep learning driven multimedia and multimodal sensing,Personal and Ubiquitous Computing,10.1007/s00779-022-01672-2,Springer,2022-03-11,"With the vigorous development of the Internet, multimedia technology, and virtual reality technology, digital museum has been born and gradually entered the stage of rapid development. As a typical application of 3D virtual scene in the field of traditional culture protection and communication, 3D virtual museum has become an important trend in the development of museums in recent years. Three-dimensional virtual museum can maximize the development of museum functions and better play the museum’s cultural communication and heritage protection and other functions. Faced with such rich cultural relic resources, how to construct 3D virtual museum quickly becomes an important research problem. Virtual reality technology integrates the latest research and development achievements of digital image graphics processing technology, computer technology, artificial intelligence, multimedia technology, sensor technology, and other information technologies, which provides powerful support and help for us to create and experience virtual world. The combination of virtual reality technology and cultural heritage protection has created a new research field. Based on this, this paper studies personalized healthcare museum exhibition system design based on the VR and deep learning-driven multimedia and multimodal sensing. The novel computational model is designed to improve the systematic performance of the VR framework, and the application scenarios are tested. Through the verification, the performance is tested.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00779-022-01672-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10462-022-10165-w,3DDACNN: 3D dense attention convolutional neural network for point cloud based object recognition,Artificial Intelligence Review,10.1007/s10462-022-10165-w,Springer,2022-03-11,"Recently, deep CNN-based methods have achieved significant success in solving various 2D computer vision issues. However, directly processing 3D point clouds with CNNs remains a challenging problem due to their irregular characteristic, which results in the comprehensive performance far from optimal. In this paper, we propose a novel trainable architecture for 3D point cloud based object recognition from the perspective of depth of network and attention mechanism for the first time. We first transform the input point cloud into regular volumetric representation using binary occupancy grid strategy. The output is then fed into our proposed 3D Dense-Attention CNN framework, dubbed as $$\mathbf{3DDACNN }$$ 3 DDACNN , to obtain features with enhanced representation power. Extensive experiments on highly challenging datasets demonstrate the effectiveness of our proposed model, which can achieve remarkable performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10462-022-10165-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07079-7,A deep learning algorithm for fast motion video sequences based on improved codebook model,Neural Computing and Applications,10.1007/s00521-022-07079-7,Springer,2022-03-09,"Fast motion video sequence processing is quite difficult. In order to improve the effect of fast motion video sequence processing, this paper improves the traditional codebook model algorithm and proposes an improved codebook model algorithm. Moreover, this paper analyzes and summarizes the development and application of background modeling-based methods in moving target detection, and points out the applicability and limitations of traditional methods, which lays the foundation for the further research of moving target detection based on background modeling in the complex background. In addition, this paper analyzes the characteristics of fast motion videos, and combines deep learning algorithms to improve the feature recognition effect of fast motion video sequences. Finally, this paper verifies the effect of this method through experimental research. Through experimental research, we know that the improved algorithm proposed in this paper can realize effective processing of fast motion video, and can improve the feature recognition effect of motion video frames.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07079-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-07872-3,Encouraging bystander helping behaviour in a violent incident: a virtual reality study using reinforcement learning,Scientific Reports,10.1038/s41598-022-07872-3,Nature,2022-03-09,"Virtual reality (VR) affords the study of the behaviour of people in social situations that would be logistically difficult or ethically problematic in reality. The laboratory-controlled setup makes it straightforward to collect multi-modal data and compare the responses across different experimental conditions. However, the scenario is typically fixed and the resulting data are usually analysed only once the VR experience has ended. Here we describe a method that allows adaptation of the environment to the behaviours of participants and where data is collected and processed during the experience. The goal was to examine the extent to which helping behaviour of participants towards the victim of a violent aggression might be encouraged, with the use of reinforcement learning (RL). In the scenario, a virtual human character represented as a supporter of the Arsenal Football Club, was attacked by another with the aggression escalating over time. (In some countries football is referred to as ‘soccer’, but we will use ‘football’ throughout). Each participant, a bystander in the scene, might intervene to help the victim or do nothing. By varying the extent to which some actions of the virtual characters during the scenario were determined by the RL we were able to examine whether the RL resulted in a greater number of helping interventions. Forty five participants took part in the study divided into three groups: with no RL, a medium level of RL, or full operation of the RL. The results show that the greater extent to which the RL operated the greater the number of interventions. We suggest that this methodology could be an alternative to full multi-factorial experimental designs, and more importantly as a way to produce adaptive VR scenarios that encourage participants towards a particular line of action.",https://www.nature.com/articles/s41598-022-07872-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11277-022-09602-4,Deep Neural Network Routing with Dynamic Space Division for 3D UAV FANETs,Wireless Personal Communications,10.1007/s11277-022-09602-4,Springer,2022-03-07,"With unmanned aerial vehicles (UAVs) being widely used, the rapidly changing network topology and vertical height changes of UAVs have been bottlenecks for many wild applications, such as battlefield communication. These problems lead to the frequent communication interruptions and poor stability of 3D UAV networks. Facing these challenges, we propose deep neural network routing (DNNR) that is characterized by a dynamic 3D two-subspace division (i.e., vertical-axis cylinder and horizontal-plane divisions) and deep neural network (DNN) forwarding. With the trajectories of base station and nodes changing, vertical-axis cylinder and horizontal-plane divisions also change dynamically according to the broadcast information. Different from multi subspace division, this kind of two subspace divisions could reduce the complexity of routing discovery and make full use of the dynamic adaptability of 3D space division against the rapidly changing network topology. Due to the DNN flexibility, DNN forwarding is a promising scheme to improve the probability of recognizing the available links and select the rational next-hop node. We implement four compared protocols and DNNR in NS3 network simulator and test them for various application scenarios, when changing base station speed, node speed, horizontal plane size, and vertical height. Comparing with four protocols, DNNR achieves better performance in terms of packet delivery rate and energy-saving performance. These indicate that 3D space division is a concise and feasible scheme in flight ad hoc networks which may be extended to other fields. Besides, owing to the flexibility and prevalent availability, machine learning routing protocols are becoming a popular technology.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11277-022-09602-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13592-022-00918-5,Image recognition using convolutional neural networks for classification of honey bee subspecies,Apidologie,10.1007/s13592-022-00918-5,Springer,2022-03-07,"Four models based on convolutional neural networks were used to investigate whether image recognition techniques applied to honey bee wings could be used to discriminate among honey bee subspecies. A dataset consisting of 9887 wing images belonging to 7 subspecies and one hybrid was analysed with ResNet 50, MobileNet V2, Inception Net V3, and Inception ResNet V2. Accuracy values of classification of individual wings were over 0.92, and all models outperformed traditional morphometric evaluation. The Inception models achieved the highest accuracies and higher scores of precision and recall for most classes. When wing images were grouped by colony, almost all wings in the colony samples were labelled with the same class. We conclude that automatic image recognition and machine learning applied to honey bee wings can reliably discriminate among the European subspecies and could thus represent a useful tool for fast classification of honey bee subspecies for breeding and conservation aims.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13592-022-00918-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-022-02174-7,Motion-aware future frame prediction for video anomaly detection based on saliency perception,"Signal, Image and Video Processing",10.1007/s11760-022-02174-7,Springer,2022-03-05,"The anomaly in videos can be considered as a deviation from regular video sequences. Most existing approaches neglect the imbalanced information distribution between the foreground and the background during the process of reconstruction or prediction. To address this problem, we propose a motion-aware future frame prediction network consisting of a frame prediction branch and a saliency perception branch. In particular, the saliency perception branch is designed to predict the most salient targets in the video frame, and the frame prediction branch is used to predict the RGB future frame with the guidance of saliency perception. Besides, a motion-aware attention module is bridged in the frame prediction branch to improve the representation ability of moving targets. Furthermore, a saliency prediction loss and a saliency-guided appearance loss are designed to optimize saliency prediction frames and constrain the weight of foreground. Experiments on three challenging benchmarks demonstrate our competitive performance with the state-of-the-art approaches.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-022-02174-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-022-03763-7,Improving detection accuracy of politically motivated cyber-hate using heterogeneous stacked ensemble (HSE) approach,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-022-03763-7,Springer,2022-03-04,"The surge in cyber-hate crimes is largely fuelled by the popularization of social media platforms. On that note, cyber-hate has become an increasing concern for most countries, especially those that are practising democracy. Studies on the influence of social media (SM) on political discourse have now become an important research area due to the rising trends of SM politics. It becomes necessary to address this problem using automated social intelligence. To tackle this concern, the researchers built a novel heterogeneous stacked ensemble (HSE) classifier for detecting politically motivated cyber-hate on Twitter. We constructed a heterogeneous stacked ensemble with eight baseline estimators. In the proposed methodology, the researchers employed TF-IDF for feature vectorisation. The researchers used Twitter API for data scraping to harvest tweets during a gubernatorial election in Nigeria for the training and evaluation of the stacked ensemble model. A total of 15,502 tweets were collected and after some preliminary cleaning, 5876 tweets were manually labelled as hate (1) or non-hate (0). The coded tweets contain 16.87% hate and 83.13% non-hate tweets. This article has three contributions – a critical review of literature on the detection of politically motivated cyber-hate, the building of a new dataset and the proposed stacked ensemble method. Two other public datasets (Kaggle and HASOC) were used to test the performance of our method. The F1-score metric was employed for comparison. Our method is better by 12% on the Kaggle and 4% on the HASOC datasets. We are working on more data for deep learning experiments.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-022-03763-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40336-022-00487-8,State-of-the-art techniques using pre-operative brain MRI scans for survival prediction of glioblastoma multiforme patients and future research directions,Clinical and Translational Imaging,10.1007/s40336-022-00487-8,Springer,2022-03-03,"Objective Glioblastoma multiforme (GBM) is a grade IV brain tumour with very low life expectancy. Physicians and oncologists urgently require automated techniques in clinics for brain tumour segmentation (BTS) and survival prediction (SP) of GBM patients to perform precise surgery followed by chemotherapy treatment. Methods This study aims at examining the recent methodologies developed using automated learning and radiomics to automate the process of SP. Automated techniques use pre-operative raw magnetic resonance imaging (MRI) scans and clinical data related to GBM patients. All SP methods submitted for the multimodal brain tumour segmentation (BraTS) challenge are examined to extract the generic workflow for SP. Results The maximum accuracies achieved by 21 state-of-the-art different SP techniques reviewed in this study are 65.5 and 61.7% using the validation and testing subsets of the BraTS dataset, respectively. The comparisons based on segmentation architectures, SP models, training parameters and hardware configurations have been made. Conclusion The limited accuracies achieved in the literature led us to review the various automated methodologies and evaluation metrics to find out the research gaps and other findings related to the survival prognosis of GBM patients so that these accuracies can be improved in future. Finally, the paper provides the most promising future research directions to improve the performance of automated SP techniques and increase their clinical relevance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40336-022-00487-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-07026-6,Basketball motion video target tracking algorithm based on improved gray neural network,Neural Computing and Applications,10.1007/s00521-022-07026-6,Springer,2022-03-03,"This article takes the basketball game video with high attention in sports video as an example to analyze the feature extraction of basketball game video, improve the gray neural network algorithm, and disassemble the basketball video. Moreover, this paper takes basketball, basket, and athletes as the feature extraction objects. Considering that the basketball is a round sphere and the object in the image is a circle, as well as the edge is added to the original image and saved. In addition, this paper combines the improved gray neural network algorithm to construct a basketball motion video target tracking algorithm. Finally, this paper designs experiments to verify the performance of this method. The experimental test results show that this method can effectively recognize basketball gestures with high recognition accuracy, which provides a new method for basketball posture recognition.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-07026-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s13634-022-00851-w,Regression-based beam training for UAV mmWave communications,EURASIP Journal on Advances in Signal Processing,10.1186/s13634-022-00851-w,Springer,2022-03-03,"For unmanned aerial vehicle (UAV) millimeter-wave (mmWave) communication systems, efficient and accurate beam training is urgently required to overcome beam misalignment. By taking into account the mmWave propagation environment, a three-dimensional (3D) intelligent beam training strategy that leverages the polynomial regression (PR) model and optimized beam patterns is proposed in this paper. We treat mmWave beam selection as a PR problem. By using machine learning (ML), the regression function is determined. The training dataset applied in the ML method consists of measured power and estimated angles and is obtained by carefully designed beam patterns. Furthermore, a noise suppression method involving the use of a denoising autoencoder (DAE) is developed to overcome the noise sensitivity of the proposed regression model. The numerical simulation results demonstrate that our proposed beam training strategy is capable of obtaining the same precision as an exhaustive search with a shorter time.",https://www.biomedcentral.com/openurl?doi=10.1186/s13634-022-00851-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10353-022-00747-x,"eSurgery—digital transformation in surgery, surgical education and training: survey analysis of the status quo in Germany",European Surgery,10.1007/s10353-022-00747-x,Springer,2022-03-03,"Background In surgery, electronic healthcare systems offer numerous options to improve patient care. The aim of this study was to analyse the current status of digitalisation and its influence in surgery, with a special focus on surgical education and training. Methods An individually created questionnaire was used to analyse the subjective assessment of the digitalisation processes in clinical surgery. The online questionnaire consisted of 16 questions regarding the importance and the corresponding implementation of the teaching content: big data, health apps, messenger apps, telemedicine, data protection/IT security, ethics, simulator training, economics and e‑Learning were included. The participation link was sent to members of the German Society of Surgery via the e‑Mail distribution list. Results In total, 119 surgeons (response rate = 19.8%) took part in the survey: 18.5% of them were trainees (TR), and 81.5% had already completed specialist training (SP). Overall, 66.4% confirmed a positive influence of digitalisation on the quality of patient care. The presence of a surgical robot was confirmed by 47.9% of the participants. A further 22.0% ( n  = 26) of the participants confirmed the possibility of using virtual simulators. According to 79.0% of the participants, the integration of digital technologies in surgical education for basic and advanced stage surgeons should be aimed for. Data protection (1.7) and e‑Learning (1.7) were rated as the most important teaching content. The greatest discrepancy between importance and implementation was seen in the teaching content of big data (mean: 2.2–3.8). Conclusion The results of the survey reveal the particular importance of digitalisation content for surgery, surgical education and training. At the same time, the results underline the desire for increased integration of digital competence teaching. The data also show an overall more progressive and optimistic perception of TR. In order to meet the challenges of the digital transformation, the implementation of suitable curricula, including virtual simulation-based training and blended-learning teaching concepts, should be emphasised.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10353-022-00747-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10796-021-10234-5,Design Principles for User Interfaces in AI-Based Decision Support Systems: The Case of Explainable Hate Speech Detection,Information Systems Frontiers,10.1007/s10796-021-10234-5,Springer,2022-03-02,"Hate speech in social media is an increasing problem that can negatively affect individuals and society as a whole. Moderators on social media platforms need to be technologically supported to detect problematic content and react accordingly. In this article, we develop and discuss the design principles that are best suited for creating efficient user interfaces for decision support systems that use artificial intelligence (AI) to assist human moderators. We qualitatively and quantitatively evaluated various design options over three design cycles with a total of 641 participants. Besides measuring perceived ease of use, perceived usefulness, and intention to use, we also conducted an experiment to prove the significant influence of AI explainability on end users’ perceived cognitive efforts, perceived informativeness, mental model, and trustworthiness in AI. Finally, we tested the acquired design knowledge with software developers, who rated the reusability of the proposed design principles as high.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10796-021-10234-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10515-022-00323-3,UAV surveillance for violence detection and individual identification,Automated Software Engineering,10.1007/s10515-022-00323-3,Springer,2022-03-02,"Violence detection and face recognition of the individuals involved in the violence has an influence that’s noticeable on the development of automated video surveillance research. With increasing risks in society and insufficient staff to monitor them, there is an expanding demand for drones square measure and computerized video surveillance. Violence detection is expeditious and can be utilized as the method to selectively filter the surveillance videos, and identify or take note of the individual who is creating the anomaly. Individual identification from drone surveillance videos in a crowded area is difficult because of the expeditious movement, overlapping features, and bestrew backgrounds. The goal is to come with a better drone surveillance system that recognizes the violent individuals that are implicated in violence and evoke a distress signal so that fast help can be offered. This paper uses the currently developed techniques based on deep learning and proposed the concept of transfer learning using deep learning-based different hybrid models with LSTM for violence detection. Identifying individuals incriminated in violence from drone-captured images involves major issues in variations of human facial appearance, hence the paper uses a CNN model combined with image processing techniques. For testing, the drone captured video dataset is developed for an unconstrained environment. Ultimately, the features extracted from a hybrid of inception modules and residual blocks, with LSTM architecture yielded an accuracy of 97.33% and thereby proved to be noteworthy and thereby, demonstrating its superiority over other models that have been tested. For the individual identification module, the best accuracy of 99.20% obtained on our dataset, is a CNN model with residual blocks trained for face identification.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10515-022-00323-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11277-021-09159-8,Content-Based Image Recognition and Tagging by Deep Learning Methods,Wireless Personal Communications,10.1007/s11277-021-09159-8,Springer,2022-03-01,"Deep learning techniques is growing wider day by day in the process of Content based Image retrieval (CBIR). The recognition of the image is based on its shape, attributes, and tag. It is challenging to establish the connection between semantic ideas in the vast real-world applications. The social media is dominating the globe by throwing its wide range of features where the people are finding difficulty in choosing their suitable objects or images because of any redundancy. So the proposed method is based on Content-based image recognition and tagging by using deep learning techniques. The tagging of the image is used here for easy identification of the objects. The Geon similarity model is used to extract the maximum similarity of the different images by its accurate and rapid computation methods. The modified grey wolf optimization method and the novelty based convolution neural network, ResNet-50, is applied here as a hashing technique and classifier to get high recognition rate and accuracy values when compared to the state of art methods. The performance attributes of the modified convolutional neural network give a high value of precision, accuracy, recall, and mAP.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11277-021-09159-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10055-021-00550-1,Democratizing AI in biomedical image classification using virtual reality,Virtual Reality,10.1007/s10055-021-00550-1,Springer,2022-03-01,"Artificial intelligence models can produce powerful predictive computer vision tools for healthcare. However, their development simultaneously requires computational skill as well as biomedical expertise. This barrier often impedes the wider utilization of AI in professional environments since biomedical experts often lack software development skills. We present the first development environment where a user with no prior training can build near-expert level convolutional neural network classifiers on real-world datasets. Our key contribution is a simplified environment in virtual reality where the user can build, compute, and critique a model. Through a controlled user study, we show that our software enables biomedical researchers and healthcare professionals with no AI development experience to build AI models with near-expert performance. We conclude that the potential role for AI in the biomedical domain can be realized more effectively by making its development more intuitive for non-technical domain experts using novel modes of interaction.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-021-00550-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12787-2,Deep learning techniques for infrared image/video understanding,Multimedia Tools and Applications,10.1007/s11042-022-12787-2,Springer,2022-03-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12787-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-021-01551-y,Learning Scene Dynamics from Point Cloud Sequences,International Journal of Computer Vision,10.1007/s11263-021-01551-y,Springer,2022-03-01,"Understanding 3D scenes is a critical prerequisite for autonomous agents. Recently, LiDAR and other sensors have made large amounts of data available in the form of temporal sequences of point cloud frames. In this work, we propose a novel problem— sequential scene flow estimation (SSFE)—that aims to predict 3D scene flow for all pairs of point clouds in a given sequence. This is unlike the previously studied problem of scene flow estimation which focuses on two frames. We introduce the SPCM-Net architecture, which solves this problem by computing multi-scale spatiotemporal correlations between neighboring point clouds and then aggregating the correlation across time with an order-invariant recurrent unit. Our experimental evaluation confirms that recurrent processing of point cloud sequences results in significantly better SSFE compared to using only two frames. Additionally, we demonstrate that this approach can be effectively modified for sequential point cloud forecasting (SPF), a related problem that demands forecasting future point cloud frames. Our experimental results are evaluated using a new benchmark for both SSFE and SPF consisting of synthetic and real datasets. Previously, datasets for scene flow estimation have been limited to two frames. We provide non-trivial extensions to these datasets for multi-frame estimation and prediction. Due to the difficulty of obtaining ground truth motion for real-world datasets, we use self-supervised training and evaluation metrics. We believe that this benchmark will be pivotal to future research in this area. All code for benchmark and models will be made accessible at ( https://github.com/BestSonny/SPCM ).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-021-01551-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-021-09885-1,Exploiting variable length segments with coarticulation effect in online speech recognition based on deep bidirectional recurrent neural network and context-sensitive segment,International Journal of Speech Technology,10.1007/s10772-021-09885-1,Springer,2022-03-01,"Deep bidirectional recurrent network (DBRNN) is a powerful acoustic model that can capture the dynamics and coarticulation effect of speech signal. It can model the temporal sequences that depend on left and right contexts, whereas deep unidirectional recurrent neural network (or deep recurrent neural network) can model the temporal sequences that usually depend only on past information. When traditional DBRNNs are used, context-sensitive segments with carefully selected fixed length are exploited to balance recognition accuracy and latency for online speech recognition because the ASR decoder results in recognition latency, depending on the whole input sequence in each evaluation. On the other hand, acoustical realization of phoneme depends not only on the left-sided phoneme, but also on the right-sided phoneme, which should be considered in acoustic modeling for speech recognition. In this paper, we propose a DBRNN-based online speech recognition method that selects and exploits variable length chunks to take into account coarticulation effects appearing in speech production. In order to select variable length segments with the coarticulation effects, the vowel identification points predicted by a deep unidirectional recurrent neural network are used, and such variable length segments are used for training of DBRNN for online recognition. The deep unidirectional recurent neural network for predicting variable length segments is trained using the connectionist temporal classification (CTC) method. We show that the online recognizable DBRNN acoustic model constructed using variable length chunks with coarticulation effect in experiments on Korean speech recognition effectively limits recognition latency, resulting in performance comparable to traditional offline DBRNN, and provides improved performance than online recognition based on fixed-length context-sensitive chunks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-021-09885-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10288-021-00473-2,Controlling the emotional expressiveness of synthetic speech: a deep learning approach,4OR,10.1007/s10288-021-00473-2,Springer,2022-03-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10288-021-00473-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10845-020-01699-3,Digital twin of functional gating system in 3D printed molds for sand casting using a neural network,Journal of Intelligent Manufacturing,10.1007/s10845-020-01699-3,Springer,2022-03-01,"The filling stage is a critical phenomenon in sand casting for making reliable castings. Latest research has demonstrated that for most liquid engineering alloys, the critical meniscus velocity of the melt at the ingate is in the range of 0.4–0.6 m s^−1. The work described in this research paper is to use neural network (NN) technology to propose digital twin approach for gating system design that allow to understand and model its performances faster and more reliable than traditional methods. This approach was applied in the case of sand casting of liquid aluminum alloy (EN AC-44200). The approach is based first on a digital representation of filling process to perform the melt flow simulations using a combination of the gating system design parameters, selected as a training cases from Taguchi orthogonal array (OA). The second step of the approach is the data capture of functional gating design system to train up the feed-forward back-propagation NN model. The validation of the well-trained NN model is assessed by interrogating predicted ingate velocity to it and making reliable predictions with high accuracy. The claim is that such digital twin approach is an effective solution to recognize the functional design parameters from the entire filling systems used during casting process.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10845-020-01699-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10055-021-00572-9,Immersive virtual reality as an empirical research tool: exploring the capability of a machine learning model for predicting construction workers’ safety behaviour,Virtual Reality,10.1007/s10055-021-00572-9,Springer,2022-03-01,"In recent years, research has found that people have stable predispositions to engage in certain behavioural patterns to work safely or unsafely, which vary among individuals as a function of their personality features. In this regard, an innovative machine learning model has been recently developed to predict workers’ behavioural tendency based on personality factors. This paper presents an empirical evaluation of the model’s prediction performance (i.e. the degree to which the model can generate similar results compared to reality) to address the issue of the model’s usability before it is implemented in real situations. As virtual reality allows a good grip on fidelity resembling real-world situations, it can stimulate more natural behaviour responses from participants to increase ecological validity of experimental results. Thus, we implemented a virtual reality experimentation environment to assess workers’ safety behaviour. The model’s prediction capability was then evaluated by comparing the model prediction results and workers’ safety behaviour as assessed in virtual reality. The comparison results showed that the model predictions on two dimensions of workers’ safety behaviour (i.e. task and contextual performance) were in good agreement with the virtual reality experimental results, with Spearman correlation coefficients of 79.7% and 87.8%, respectively. The machine learning model thus proved to have good prediction capability, which allows the model to help identify vulnerable workers who are prone to undertake unsafe behaviours. The findings also suggest that virtual reality is a promising method for measuring workers’ safety behaviour as it can provide a realistic and safe environment for experimentation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-021-00572-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12132-7,Exploiting epistemic uncertainty of the deep learning models to generate adversarial samples,Multimedia Tools and Applications,10.1007/s11042-022-12132-7,Springer,2022-03-01,"Deep neural network (DNN) architectures are considered to be robust to random perturbations. Nevertheless, it was shown that they could be severely vulnerable to slight but carefully crafted perturbations of the input, termed as adversarial samples. In recent years, numerous studies have been conducted in this new area called ``Adversarial Machine Learning” to devise new adversarial attacks and to defend against these attacks with more robust DNN architectures. However, most of the current research has concentrated on utilising model loss function to craft adversarial examples or to create robust models. This study explores the usage of quantified epistemic uncertainty obtained from Monte-Carlo Dropout Sampling for adversarial attack purposes by which we perturb the input to the shifted-domain regions where the model has not been trained on. We proposed new attack ideas by exploiting the difficulty of the target model to discriminate between samples drawn from original and shifted versions of the training data distribution by utilizing epistemic uncertainty of the model. Our results show that our proposed hybrid attack approach increases the attack success rates from 82.59% to 85.14%, 82.96% to 90.13% and 89.44% to 91.06% on MNIST Digit, MNIST Fashion and CIFAR-10 datasets, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12132-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-022-09961-0,Boosting subjective quality of Arabic text-to-speech (TTS) using end-to-end deep architecture,International Journal of Speech Technology,10.1007/s10772-022-09961-0,Springer,2022-03-01,"End-to-end speech synthesis methods managed to achieve nearly natural and human-like speech. They are prone to some synthesis errors such as missing or repeating words, or incomplete synthesis. We may argue this is mainly due to the local information preference between text input and the learned acoustic features of a conditional autoregressive (CAR) model. The local information preference prevents the model from depending on text input when predicting acoustic features. It contributes to synthesis errors during inference time. In this work, we are comparing two modified architectures based on Tacotron2 to generate Arabic speech. The first architecture replaces the WaveNet vocoder with a flow-based implementation of WaveGlow. The second architecture, influenced by InfoGan, maximizes the mutual information between text input and predicted acoustic features (mel-spectrogram) to eliminate the local information preference. The training objective has been also changed by adding a CTC loss term. The training objective could be considered as a metric of local information preference between text input and predicted acoustic features. We carried the experiments on Nawar Halabi’s dataset ( http://en.arabicspeechcorpus.com/ ) which contains about 2.41 h of Arabic speech. Our experiments show that maximizing mutual information between predicted acoustic features and conditional text input as well as changing the training objective can enhance the subjective quality of generated speech and reduce the utterance error rate.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-022-09961-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40192-021-00244-1,3D Grain Shape Generation in Polycrystals Using Generative Adversarial Networks,Integrating Materials and Manufacturing Innovation,10.1007/s40192-021-00244-1,Springer,2022-03-01,"This paper presents a generative adversarial network (GAN) capable of producing realistic microstructure morphology features and demonstrates its capabilities on a dataset of crystalline titanium grain shapes. Alongside this, we present an approach to train deep learning networks to understand material-specific descriptor features, such as grain shapes, based on existing conceptual relationships with established learning spaces, such as functional object shapes. A style-based GAN with Wasserstein loss, called M-GAN, was first trained to recognize distributions of morphology features from function objects in the ShapeNet dataset and was then applied to grain morphologies from a 3D crystallographic dataset of Ti–6Al–4V. Evaluation of feature recognition on objects showed comparable or better performance than state-of-the-art voxel-based network approaches. When applied to experimental data, M-GAN generated realistic grain morphologies comparable to those seen in Ti–6Al–4V. A quantitative comparison of moment invariant distributions showed that the generated grains were similar in shape and structure to the ground truth, but scale invariance learned from object recognition led to difficulty in distinguishing between the physical features of small grains and spatial resolution artifacts. The physical implications of M-GAN’s learning capabilities are discussed, as well as the extensibility of this approach to other material characteristics related to grain morphology.",https://www.biomedcentral.com/openurl?doi=10.1007/s40192-021-00244-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12304-5,Feature-based hybrid strategies for gradient descent optimization in end-to-end speech recognition,Multimedia Tools and Applications,10.1007/s11042-022-12304-5,Springer,2022-03-01,"With the increasing popularity of deep learning, deep learning architectures are being utilized in speech recognition. Deep learning based speech recognition became the state-of-the-art method for speech recognition tasks due to their outstanding performance over other methods. Generally, deep learning architectures are trained with a variant of gradient descent optimization. Mini-batch gradient descent is a variant of gradient descent optimization which updates network parameters after traversing a number of training instances. One limitation of mini-batch gradient descent is the random selection of mini-batch samples from training set. This situation is not preferred in speech recognition which requires training features to collapse all possible variations in speech databases. In this study, to overcome this limitation, hybrid mini-batch sample selection strategies are proposed. The proposed hybrid strategies use gender and accent features of speech databases in a hybrid way to select mini-batch samples when training deep learning architectures. Experimental results justify that using hybrid of gender and accent features is more successful in terms of speech recognition performance than using only one feature. The proposed hybrid mini-batch sample selection strategies would benefit other application areas that have metadata information, including image recognition and machine vision.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12304-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12028-6,Sound based alarming based video surveillance system design,Multimedia Tools and Applications,10.1007/s11042-022-12028-6,Springer,2022-03-01,"Modern video surveillance systems consist of a network of many video cameras. Constantly video camera systems are being installed for security reasons in prisons, elevators, automatic teller machines and more. Usually, video cameras are connected to a display screen from which security personnel monitor suspicious activity. As security personnel monitor multiple locations simultaneously, this manual task is labor intensive and inefficient. These camera systems have some other drawbacks such that they have limited coverage and security personnel cannot see all the points even though they are looking at the camera. Therefore, most of the time, some other sensors should accompany to video cameras. Although audio surveillance is in its early stage, there has been considerable amount of work in this area in the last decade. On the other hand, currently, there are no practical audio surveillance solutions for security on the market. In this paper, audio surveillance is integrated to current video surveillance systems using deep learning. We develop a complete system and show a working prototype. It is encouraging to see that the system is good enough and can be used in real life.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12028-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10816-021-09518-6,"Artificial Intelligence, 3D Documentation, and Rock Art—Approaching and Reflecting on the Automation of Identification and Classification of Rock Art Images",Journal of Archaeological Method and Theory,10.1007/s10816-021-09518-6,Springer,2022-03-01,"Rock art carvings, which are best described as petroglyphs, were produced by removing parts of the rock surface to create a negative relief. This tradition was particularly strong during the Nordic Bronze Age (1700–550 BC) in southern Scandinavia with over 20,000 boats and thousands of humans, animals, wagons, etc . This vivid and highly engaging material provides quantitative data of high potential to understand Bronze Age social structures and ideologies. The ability to provide the technically best possible documentation and to automate identification and classification of images would help to take full advantage of the research potential of petroglyphs in southern Scandinavia and elsewhere. We, therefore, attempted to train a model that locates and classifies image objects using faster region-based convolutional neural network (Faster-RCNN) based on data produced by a novel method to improve visualizing the content of 3D documentations. A newly created layer of 3D rock art documentation provides the best data currently available and has reduced inscribed bias compared to older methods. Several models were trained based on input images annotated with bounding boxes produced with different parameters to find the best solution. The data included 4305 individual images in 408 scans of rock art sites. To enhance the models and enrich the training data, we used data augmentation and transfer learning. The successful models perform exceptionally well on boats and circles, as well as with human figures and wheels. This work was an interdisciplinary undertaking which led to important reflections about archaeology, digital humanities, and artificial intelligence. The reflections and the success represented by the trained models open novel avenues for future research on rock art.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10816-021-09518-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02056-y,HybNet: a hybrid network structure for pain intensity estimation,The Visual Computer,10.1007/s00371-021-02056-y,Springer,2022-03-01,"Automatic pain intensity estimation has great potential in current rehabilitation medicine, and patients’ health status information can be obtained through the analysis of facial images. At present, deep convolutional neural networks (CNNs) have made great progress in many fields, including natural language processing, image classification and action recognition. Motivated by the current achievements, a novel end-to-end hybrid network is proposed to extract multidimensional features from image sequences, which is composed of 3D convolution, 2D convolution and 1D convolution. Specifically, the 3D convolutional neural network (3D CNN) is designed to capture the spatiotemporal features, and the 2D convolutional neural network (2D CNN) is designed to capture the spatial features, while the 1D convolutional neural network (1D CNN) is mainly used to capture the geometric information from facial landmarks. Finally, the features obtained by the three different networks are fused together for regression. The proposed HybNet is evaluated on UNBC-McMaster Shoulder Pain Expression Archive Database, and the experimental results show that it can effectively extract the discriminative high-level features and can achieve competitive performance with the state-of-the-art methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02056-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13198-021-01540-x,Research on digital media animation control technology based on recurrent neural network using speech technology,International Journal of System Assurance Engineering and Management,10.1007/s13198-021-01540-x,Springer,2022-03-01,"A vivid and lifelike virtual speaker can attract the user's attention, and the construction of a lifelike virtual speaker not only requires a beautiful static appearance, but also has mouth movements, facial expressions and body movements that are truly synchronized with the voice. Virtual speaker refers to a technology in which a computer generates an animated facial image that can speak. In order to add special effects such as image editing and beautification in the broadcast screen. This paper proposes a voice-driven facial animation synthesis method based on deep BLSTM. A Neural Network BLSTM-RNN Using Audio-Visual Dual Modal Information Training of Speakers, uses the active appearance model to model the face image, and uses the AAM model parameters as Network output, to study the influence of network structure and input of different voice features on the effect of animation synthesis. The experimental results based on the LIPS2008 standard evaluation library show that the network effect with BLSTM layer is obviously better than that of forward network, and the three-layer model structure based on BLSTM—forward- BLSTM 256 node (BFB256) is the best. FBank, fundamental frequency and energy combination can further improve animation synthesis effect. The main aim of this paper is to study the method of speech-driven facial animation synthesis based on deep BLSTM-RNN, and tries the synthesis effect of different neural network structures and different speech features.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13198-021-01540-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11015-022-01276-4,Experience in Developing Digital Twins Of Melting Processes in EAF for Solving Technological Problems of Producing a Semi-Finished Product with Required Quality Characteristics,Metallurgist,10.1007/s11015-022-01276-4,Springer,2022-03-01,"An approach to the development of models for the digital description of the process of smelting a semi-finished product in an electric-arc furnace using machine-learning methods is presented. The capabilities and advantages of the models developed are shown considering, as an example, the oxidation (reduction) of metal impurities during smelting.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11015-022-01276-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s42486-021-00076-0,An efficient and low power deep learning framework for image recognition on mobile devices,CCF Transactions on Pervasive Computing and Interaction,10.1007/s42486-021-00076-0,Springer,2022-03-01,"Image classification on mobile devices can provide convenient and secure services for users when using various social software. The traditional classification method mainly relies on the user’s manual marking, but the accuracy of automatic classification has some defects. With the development of convolutional neural network(CNN), the design of lightweight neural network has become a hot topic. However, the state-of-the-art studies always sacrifice classification accuracy for network lightweight, which greatly frustrates usability. In this paper, a new neural network framework, named MobVi, is proposed to enhance the precision of lightweight neural network by solution space division. MobVi is including image solution space division and judgment class. The former uses clustering method based on deep learning to distinguish which small solution space the image belongs to, while the latter uses lightweight neural network customized for the solution space to judge the class. In order to reduce the amount of model parameters and calculations, we designed a customized CNN module. Finally, we propose an energy prediction model to measure whether the model can be successfully implemented on mobile devices. A series of experiments have proved that MobVi has better performance than most existing models for mobile devices. Our model achieves 83.5% accuracy on CIFAR-10 data set, and the parameter quantity is only 2.0 M.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s42486-021-00076-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12205-021-0972-2,SEMA: A Site Equipment Management Assistant for Construction Management,KSCE Journal of Civil Engineering,10.1007/s12205-021-0972-2,Springer,2022-03-01,"Collecting construction equipment information such as the site equipment enter and exit date-time, driver’s name, type, and quantity is essential in construction management. Most construction projects use paper to record the equipment access history. However, manual recording is always labour-intensive and time-consuming. Therefore, this research aims to develop an assistant system, Site Equipment Management Assistant (SEMA), to automate the site equipment management processes. With the introduction of image recognition and multiple objects tracking technologies, the proposed system can extract equipment-related information from raw videos. SEMA is designed as a chatbot system that contains three major modules: data acquisition, information extraction, and information delivery. A deep learning-based model was first trained to automatically recognize and track construction equipment passing by the site monitor. Information such as equipment entering and exiting date-time, type, and quantity would be extracted and stored in a database. A chatbot interface was developed for users to obtain data from the database through an intuitive and easy-to-use interface. A system evaluation and usability test were conducted. The results showed that the system could effectively improve the construction equipment management process. SEMA can save 60.7% of users’ operation time on obtaining related information.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12205-021-0972-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13369-021-06297-w,A Deep Learning Framework for Audio Deepfake Detection,Arabian Journal for Science and Engineering,10.1007/s13369-021-06297-w,Springer,2022-03-01,"Audio deepfakes have been increasingly emerging as a potential source of deceit, with the development of avant-garde methods of synthetic speech generation. Hence, differentiating fake audio from the real one is becoming even more difficult owing to the increasing accuracy of text-to-speech models, posing a serious threat to speaker verification systems. Within the domain of audio deepfake detection, a majority of experiments have been based on the ASVSpoof or the AVSpoof dataset using various machine learning and deep learning approaches. In this work, experiments were performed on a more recent dataset, the Fake or Real (FoR) dataset which contains data generated using some of the best text to speech models. Two approaches have been adopted to the solve problem: feature-based approach and image-based approach. The feature-based approach involves converting audio data into a dataset consisting of various spectral features of the audio samples, which are fed to the machine learning algorithms for the classification of audio as fake or real. While in the image-based approach audio samples are converted into melspectrograms which are input into deep learning algorithms, namely Temporal Convolutional Network (TCN) and Spatial Transformer Network (STN). TCN has been implemented because it is a sequential model and has been shown to give good results on sequential data. A comparison between the performances of both the approaches has been made, and it is observed that deep learning algorithms, particularly TCN, outperforms the machine learning algorithms by a significant margin, with a 92 percent test accuracy. This solution presents a model for audio deepfake classification which has an accuracy comparable to the traditional CNN models like VGG16, XceptionNet, etc.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13369-021-06297-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-021-02527-8,Primitive-contrastive network: data-efficient self-supervised learning from robot demonstration videos,Applied Intelligence,10.1007/s10489-021-02527-8,Springer,2022-03-01,"Due to the costly collection of expert demonstrations for robots, robot imitation learning suffers from the demonstration-insufficiency problem. A promising solution to this problem is self-supervised learning that leverages pretext tasks to extract general and high-level features from a relatively small amount of data. Since imitation learning tasks are typically composed of primitives (e.g., primary skills, such as grasping and reaching), learning representations of these primitives is crucial. However, existing methods have a weak ability to represent primitive, leading to unsatisfactory generalizability to learning scenarios with few data. To address this problem, we propose a novel primitive-contrastive network (PCN) and pretext task that optimizes the distances between pseudo-primitive distributions as a learning objective. Experimental results show that the proposed PCN can learn a more discriminative embedding space of primitives than existing self-supervised learning methods. Four representative robot manipulation experiments are conducted to demonstrate the superior data efficiency of the proposed method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02527-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00345-021-03820-4,Percutaneous puncture during PCNL: new perspective for the future with virtual imaging guidance,World Journal of Urology,10.1007/s00345-021-03820-4,Springer,2022-03-01,"Context Large and complex renal stones are usually treated with percutaneous nephrolithotomy (PCNL). One of the crucial steps in this procedure is the access to the collecting system with the percutaneous puncture and this maneuver leads to a risk of vascular and neighboring organs’ injury. In the last years, the application of virtual image-guided surgery has gained wide diffusion even in this specific field. Objectives To provide a short overview of the most recent evidence on current applications of virtual imaging guidance for PCNL. Evidence acquisition A non-systematic review of the literature was performed. Medline, PubMed, the Cochrane Database and Embase were screened for studies regarding the use virtual imaging guidance for PCNL. Evidence synthesis 3D virtual navigation technology for PCNL was first used in urology with the purpose of surgical training and surgical planning; subsequently, the field of surgical navigation with different modalities (from cognitive to augmented reality or mixed reality) had been explored. Finally, anecdotal preliminary experiences explored the potential application of artificial intelligence guidance for percutaneous puncture. Conclusion Nowadays, many experiences proved the potential benefit of virtual guidance for surgical simulation and training. Focusing on surgery, this tool revealed to be useful both for surgical planning, allowed to achieve a better surgical performance, and for surgical navigation by using augmented reality and mixed reality systems aimed to assist the surgeon in real time during the intervention.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00345-021-03820-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-021-01569-2,From Individual to Whole: Reducing Intra-class Variance by Feature Aggregation,International Journal of Computer Vision,10.1007/s11263-021-01569-2,Springer,2022-03-01,"The recording process of observation is influenced by multiple factors, such as viewpoint, illumination, and state of the object-of-interest etc.Thus, the image observation of the same object may vary a lot under different conditions. This leads to severe intra-class variance which greatly challenges the discrimination ability of the vision model. However, the current prevailing softmax loss for visual recognition only pursues perfect inter-class separation in the feature space. Without considering the intra-class compactness, the learned model easily collapses when it encounters the instances that deviate a lot from their class centroid. To resist the intra-class variance, we start by organizing the input instances as a graph. From this viewpoint, we find that the normalized cut on the graph is a favorable surrogate metric of the intra-class variance within the training batch. Inspired by the equivalence between the normalized cut and random walk, we propose a feature aggregation scheme using transition probabilities as guidance. By imposing supervision on the aggregated features, we can constrain the transition probabilities to form a graph partition consistent with the given labels. Thus, the normalized cut as well as intra-class variance can be well suppressed. To validate the effectiveness of this idea, we instantiate it in spatial, temporal, and spatial-temporal scenarios. Experimental results on corresponding benchmarks demonstrate that the proposed feature aggregation leads to significant improvement in performance. Our method is on par with, or even better than current state-of-the-arts in both tasks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-021-01569-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11766-3,STCNet: spatiotemporal cross network for industrial smoke detection,Multimedia Tools and Applications,10.1007/s11042-021-11766-3,Springer,2022-03-01,"Industrial smoke emissions present a serious threat to natural ecosystems and human health. Prior works have shown that using computer vision techniques to identify smoke is a low-cost and convenient method. However, translucent smoke detection is a challenging task because of the irregular contours and complex motion state. To overcome these problems, we propose a novel spatiotemporal cross network (STCNet) to recognize industrial smoke emissions. The proposed STCNet involves a spatial pathway to extract appearance features and a temporal pathway to capture smoke motion information. Our STCNet is more targeted and goal oriented for dealing with translucent, nonrigid smoke objects. The spatial path can easily recognize obvious nonsmoking objects such as trees and buildings, and the temporal path can highlight the obscure traces of motion smoke. Our STCNet achieves the mutual guidance of multilevel spatiotemporal information by bidirectional feature fusion on multilevel feature maps. Extensive experiments on public datasets show that our STCNet achieves clear improvements against the best competitors by 6.2%. We also perform in-depth ablation studies on STCNet to explore the impacts of different feature fusion methods for the entire model. The code will be available at https://github.com/Caoyichao/STCNet .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11766-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13369-021-06306-y,Dual-Branch Enhanced Network for Change Detection,Arabian Journal for Science and Engineering,10.1007/s13369-021-06306-y,Springer,2022-03-01,"Change detection is an essential task in intelligent monitoring, and the accuracy of detection is of central importance for subsequent target tracking and recognition. However, a series of challenges such as illumination change, severe weather, shadow, and camera jitter have brought great troubles. To reduce the impact of these factors, we propose a novel model, called dual-branch enhanced network (DBEN), which can simultaneously extract enough spatial features and context information. Specifically, we design a recurrent gated bottleneck module to get high-level features, and build the global attention module as an auxiliary branch to obtain fine resolution details. Moreover, we also propose a gated residual dense module to enhance feature expression by reconstructing the combined information. Meanwhile, a weighted loss function is designed to optimize the network. The proposed DBEN is verified on CDnet2014, DAVIS and AICD, which are three large-scale change detection datasets. Experimental results show that the proposed model is competitive in overall performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13369-021-06306-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-021-02631-9,A novel deep pixel restoration video prediction algorithm integrating attention mechanism,Applied Intelligence,10.1007/s10489-021-02631-9,Springer,2022-03-01,"With the rapid development of deep learning, in recent years, many excellent deep learning models have been developed to solve the problem of video frame prediction. Among them, most models directly generate predicted target frames. However, the predicted frames obtained in this way are often fuzzy and not realistic enough. In order to solve this problem, this paper first attempts to integrate the attention mechanism with Convolutional Long Short-Term Memory, and correspondingly proposes a new deep learning model, abbreviated as AttConvLSTM. One prominent and original characteristic of this newly constructed model is that, each of its layer calculates the attention weight of the obtained information, focusing on the information key part. Although the proposed AttConvLSTM model effectively improves the prediction accuracy, it still does not solve the problem that the prediction frames directly generated by classical deep learning models are often fuzzy and not realistic. Therefore, inspired by the concept of optical flow, this work further develops a novel Deep Pixel Restoration AttConvLSTM (DPRAConvLSTM) model. This model cleverly uses the input frames and the end-to-end characteristics of deep learning. We innovatively restore the pixels of the input frames to the predicted frames, thereby avoiding the defects that typical deep learning models can easily cause, when directly generating the predicted frames. The experimental results effectively confirm that the finally formed DPRAConvLSTM model can not only improve the accuracy of prediction, but also obtain clearer and more realistic prediction frames.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02631-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-020-02110-7,Critical direction projection networks for few-shot learning,Applied Intelligence,10.1007/s10489-020-02110-7,Springer,2022-03-01,"With the development of deep learning, visual systems perform better than human beings in many classification tasks. However, the scarcity of labelled data is the most critical problem in such visual systems. Few-shot learning is adopted to tackle this problem, wherein a classifier should acquire the ability to identify some class is not in the training data when given only a few examples. In this paper, critical direction projection (CDP) networks are proposed for few-shot learning. Basically, two crucial steps are involved in CDP: The first step is to find the critical directions for each category in the embedding space, and the second step is to measure the similarity between samples and critical directions according to the projection length. It emerges that CDP networks can be effectively compatible with existing classification networks and achieve state-of-the-art performance on several benchmark datasets. Moreover, CDP achieves outstanding performance both on 2D image and 3D object classification. This study is a new attempt to achieve 3D object classification in a few-shot learning scenario. To summarize, our major research contributions are as follows: 1) a novel metric learning method, CDP, is proposed; 2) a new feature extraction module, EffNet, is introduced; and 3) a benchmark for few-shot 3D object classification is provided.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-020-02110-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-021-01564-7,RePCD-Net: Feature-Aware Recurrent Point Cloud Denoising Network,International Journal of Computer Vision,10.1007/s11263-021-01564-7,Springer,2022-03-01,"The captured 3D point clouds by depth cameras and 3D scanners are often corrupted by noise, so point cloud denoising is typically required for downstream applications. We observe that: (i) the scale of the local neighborhood has a significant effect on the denoising performance against different noise levels, point intensities, as well as various kinds of local details; (ii) non-iteratively evolving a noisy input to its noise-free version is non-trivial; (iii) both traditional geometric methods and learning-based methods often lose geometric features with denoising iterations, and (iv) most objects can be regarded as piece-wise smooth surfaces with a small number of features. Motivated by these observations, we propose a novel and task-specific point cloud denoising network, named RePCD-Net, which consists of four key modules: (i) a recurrent network architecture to effectively remove noise; (ii) an RNN-based multi-scale feature aggregation module to extract adaptive features in different denoising stage; (iii) a recurrent propagation layer to enhance the geometric feature perception across stages; and (iv) a feature-aware CD loss to regularize the predictions towards multi-scale geometric details. Extensive qualitative and quantitative evaluations demonstrate the effectiveness and superiority of our method over state-of-the-arts, in terms of noise removal and feature preservation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-021-01564-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06678-0,Development of accurate automated language identification model using polymer pattern and tent maximum absolute pooling techniques,Neural Computing and Applications,10.1007/s00521-021-06678-0,Springer,2022-03-01,"Various language identification tools and methods have been used in the real world. These applications can detect language using text or images. However, there is no speech-based language automated identification tool available. Therefore, many studies have been presented to overcome this problem. This work presents an automated high accurate language identification model and developed a new corpus for language identification. The developed language identification model uses two novel methods: (i) polymer pattern (PP) and (ii) tent maximum absolute pooling (TMAP). These methods help to extract both low- and high-frequency features. In order to choose the most informative features, a threshold-based iterative feature selector is presented. The proposed PP- and TMAP-based model has attained an accuracy of 97.87% and 99.70% using our newly developed and VoxForge datasets, respectively, with kNN classifier with tenfold cross-validation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06678-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-020-09757-0,Hindi speech recognition using time delay neural network acoustic modeling with i-vector adaptation,International Journal of Speech Technology,10.1007/s10772-020-09757-0,Springer,2022-03-01,"It is a need of time to build an Automatic Speech Recognition (ASR) system for low and limited resource languages. Usually, statistical techniques such as Hidden Markov Models (HMM) have been applied for Indian language ASR systems for the last two decades. In this work, we have selected the Time-delay Neural Network (TDNN) based acoustic modeling with i-vector adaptation for limited resource Hindi ASR. The TDNN can capture the extended temporal context of acoustic events. To reduce the training time, we used sub-sampling based TDNN architecture in this work. Further, data augmentation techniques have been applied to extend the size of training data developed by TIFR, Mumbai. The results show that data augmentation significantly improves the performance of the Hindi ASR. Further, $$\approx$$ ≈ 4% average improvement has been recorded by applying i-vector adaptation in this work. We found the best system accuracy of 89.9% with TDNN based acoustic modeling with i-vector adaptation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-020-09757-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12145-021-00731-1,Hyperspectral image classification based on optimized convolutional neural networks with 3D stacked blocks,Earth Science Informatics,10.1007/s12145-021-00731-1,Springer,2022-03-01,"3D convolution can fully utilize the spectral-spatial characteristics of hyperspectral image (HSI), and stacked blocks with deep layers are capable of extracting hidden features and utilizing discriminant information for classification. Naturally, a 3D convolutional neural network (CNN) based on stacked blocks named SB-3D-CNN is presented for HSI classification. Moreover, the proposed network introduces the attention mechanism before the fully connected layer, which can filter out interfering information effectively. Then we optimized the architecture to obtain optimal results on three commonly used datasets of Indian Pines, Salinas and Pavia University. Experimental results demonstrate that the optimized architecture achieves better classification rates than related recent works. Because the classification accuracies on the three datasets have reached saturation, we transferred the optimized architecture to a more complex dataset adopting the airborne hyperspectral data, which obtains from Guangxi province in south China. The results show that the optimized architecture achieves superior classification accuracies compared with other state-of-the-art methods. These results also demonstrate the optimized SB-3D-CNN has the advantages of validity and portability to more complex data.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12145-021-00731-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-021-09950-9,Noise-robust speech recognition in mobile network based on convolution neural networks,International Journal of Speech Technology,10.1007/s10772-021-09950-9,Springer,2022-03-01,"The performance of Continuous Automatic Speech Recognition Systems (CASRS) in networks communications degrades rapidly in the presence of speech signal variability such as noisy environment, channel communication, and speech codec. There are several techniques proposed to improve recognition accuracy. The ASR consists of two main processing steps: feature extraction (Front-End) and classification (Back-End). We are motivated to develop speech separation algorithms (feature enhancement) to improve the intelligibility of noisy speech and the accuracy of ASR. We use non-negative matrix factorization and Ideal Binary Mask, which are estimated by a deep neural network (DNN) to use the Spectro-temporal structures of magnitude spectrograms for supervised speech separation. The ASR is based on the convolution neural network where the input is the Log Mel Cepstrum features. The system was trained using 440 sentences of 20 speakers encoded AMR-NB database and contaminated with various levels of signal-to-noise ratio (0 dB, 5 dB and 10 dB).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-021-09950-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12242-2,BG-3DM2F: Bidirectional gated 3D multi-scale feature fusion for Alzheimer’s disease diagnosis,Multimedia Tools and Applications,10.1007/s11042-022-12242-2,Springer,2022-03-01,"A computer-aided diagnosis system is one of the crucial decision support tools under the medical imaging scope. It has recently emerged as a powerful way to diagnose Alzheimer’s Disease (AD) from structural magnetic resonance imaging scans. However, due to the deficit of recognition memory in the Mild Cognitive Impairment (MCI) stage, semantic feature ambiguity, and high inter-class visual similarities problems, computer-aided diagnosis of AD remains challenging. To bridge these gaps, this paper proposed a hippocampus analysis method based on a novel 3D convolutional neural network fusion strategy, called Bidirectional Gated 3D Multi-scale Feature Fusion (BG-3DM2F). The suggested BG-3DM2F framework consists of two modules: 3D Multi-Scale Chained Network (3DMS-ChaineNet) and Bidirectional Gated Recurrent Fusion Unit (Bi-GRFU). The 3DMS-ChaineNet architecture is introduced to design the subtle features and capture the variations in hippocampal atrophy, while the Bi-GRFU scheme is investigated to store 3DMS-ChaineNet levels in the forward and backward fashion and retain them in the decision-making process. For validation, our solution is completely evaluated on the public Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset. Practically, we conducted empirical evaluations to verify the effect of BG-3DM2F components. In comparison with the current state-of-the-art methods, the experiments show that our proposed approach provides efficient results, achieving the accuracies of 98.12%, 95.26%, and 96.97% for binary classification of Normal Control (NC) versus AD, AD versus MCI, and NC versus MCI, respectively. Therefore, we can conclude that our proposed BG-3DM2F system has the potential to dramatically improve the conventional classification methods for assisting clinical decision-making.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12242-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13198-021-01415-1,Development of image recognition software based on artificial intelligence algorithm for the efficient sorting of apple fruit,International Journal of System Assurance Engineering and Management,10.1007/s13198-021-01415-1,Springer,2022-03-01,"Manual sorting of fruits was considered as a significant challenging for agricultural sector as it is a laborious task and may also lead to inconsistency in the classification. In order to improve the apple sorting efficiency and realize the non-destructive testing of apple, the machine vision technology integrated with artificial intelligence was introduced in this article for the design of apple sorting system. This article provides a low budget alternative solution for intelligent grading and sorting of apple fruit employing the deep learning-based approach. The automatic grading of apple was realized according to the determined apple grading standard by applying various stages of artificial intelligence platform like grayscale processing, binarization, enhancement processing, feature extraction and so on. The proposed end-to-end low-cost machine vision system provides an automated sorting of apple and significantly reduces the labor cost and provides a time-effective solution for medium and large-scale enterprises. In order to verify the feasibility of the scheme, the image recognition system of apple sorting machine is tested and the average accuracy of 99.70% is achieved while observing the recognition accuracy 99.38% for the CNN based apple sorting system. The results show that the sorting image recognition system can successfully sort apples according to the perimeter characteristics. It realizes the non-destructive testing and grade classification of apple and provides an important reference value for the research and development of fruit automatic sorting system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13198-021-01415-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-11946-9,Improving machine learning recognition of colorectal cancer using 3D GLCM applied to different color spaces,Multimedia Tools and Applications,10.1007/s11042-022-11946-9,Springer,2022-03-01,"Colorectal cancer (CRC) is one of the widely happening cancers among men and women. This cancer, which is also known as bowel cancer, affects the human large intestine, especially the rectum or colon. Therefore, providing new techniques with high accuracy to detect CRC cancer leads to providing an early and successful plan to treat it. In this research, we proposed a method to classify colorectal cancer using different machine learning algorithms. The method uses extracted features from 3D Gray Level Cooccurrence Matrix (GLCM) matrices of three different color spaces namely RGB, HSV, and L*A*B colors spaces. The 3D GLCM matrices of the used images were calculated and evaluated using a training dataset of 3504 images and a testing dataset of 1496 images. The five different widely used machine learning algorithms, which are Support Vector Machine (SVM), Artificial Neural Network (ANN), K-Nearest Neighbor (KNN), Quadratic Discriminant Analysis (QDA), and Classification Decision Tree (CDT). The results show that the proposed methodology can detect CRC with a high-performance rate. This higher rate is due to combining texture features from all color space channels. The best performance rate for the used machine learning models was greater than 97% for the training and testing sets using QDA using RG. The obtained results show that the proposed methodology can be used efficiently to detect CRC with high performance compared to all previous methods since texture features from the three color space channels. This research represents the first of its kind in the current research trend.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-11946-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s42380-021-00105-7,"AI Content Moderation, Racism and (de)Coloniality",International Journal of Bullying Prevention,10.1007/s42380-021-00105-7,Springer,2022-03-01,"The article develops a critical approach to AI in content moderation adopting a decolonial perspective. In particular, the article asks: to what extent does the current AI moderation system of platforms address racist hate speech and discrimination? Based on a critical reading of publicly available materials and publications on AI in content moderation, we argue that racialised people have no significant input in the definitions and decision making processes on racist hate speech and are also exploited as their unpaid labour is used to clean up platforms and to train AI systems. The disregard of the knowledge and experiences of racialised people and the expropriation of their labour with no compensation reproduce rather than eradicate racism. In theoretically making sense of this, we draw influences from Anibal Quijano’s theory of the coloniality of power and the centrality of race, concluding that in its current iteration, AI in content moderation is a technology in the service of coloniality. Finally, the article develops a sketch for a decolonial approach to AI in content moderation, which aims to centre the voices of racialised communities and to reorient content moderation towards repairing, educating and sustaining communities.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s42380-021-00105-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1134/S1054661822010035,Object Detection in Video Surveillance Based on Multiscale Frame Representation and Block Processing by a Convolutional Neural Network,Pattern Recognition and Image Analysis,10.1134/S1054661822010035,Springer,2022-03-01,"Abstract A method for detecting objects in high-resolution images is proposed that is based on representing an image as a set of its copies of decreasing scale, splitting it into blocks with overlap at each level of the image pyramid except for the top one, detecting objects in the blocks, and analyzing objects at the boundaries of adjacent blocks to merge them. The number of pyramid layers is determined by the size of the image and the input layer of the convolutional neural network (CNN). At all levels except for the top one, a block splitting is performed, and the use of overlap allows one to improve the correct classification of objects, which are divided into fragments and located in adjacent blocks. The decision to merge such fragments is made based on the analysis of the metric of intersection over union and membership in the same class. The proposed approach is evaluated for 4K and 8K images. To carry out experiments, a database is prepared with objects of two classes, person and vehicle, marked in such images. Networks of the You Only Look Once (YOLO) family of the third and fourth versions are used as CNNs. A quantitative assessment of the detection efficiency of objects is performed using the mAP metric for various combinations of parameters such as the degree of threshold confidence of the CNN and the percentage of intersection of blocks in the hierarchical representation of images. The results of the investigations are presented.",http://link.springer.com/openurl/fulltext?id=doi:10.1134/S1054661822010035,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12378-1,A two-channel speech emotion recognition model based on raw stacked waveform,Multimedia Tools and Applications,10.1007/s11042-022-12378-1,Springer,2022-03-01,"To improve the accuracy and efficiency of speech emotion recognition (SER), the acoustic feature set and speech emotion recognition model was designed based on the original speech signal, and explored the nonlinear relationship between acoustic features, the speech emotion recognition model, and the recognition task. Moreover, the original features of speech signals were studied rather than the traditional statistical features. A joint two-channel model was proposed based on the raw stacked waveform. To model raw waveform features, the convolutional recurrent neural network (CRNN) and bi-directional long short-term memory (BiLSTM) were introduced. An attention mechanism was integrated into the model to ensure that a single channel could learn the expression of the salient local region and global emotion features. Through these channels, the perception ability of speech acoustic features in multi-scale is improved, and the internal correlation between salient region and convolutional neural network is explored. The time domain and frequency domain features of speech are prominent, and the local expression of emotion is emphasized. Based on the preprocessing strategy of background separation and dimension unification, the convolutional recurrent neural network is used to extract global information. The proposed joint model could effectively integrate the advantages of the two channels. Several comparative experiments were conducted on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) database. The experiments results showed that the proposed two-channel SER model could improve recognition accuracy (UA) by 5.1% and the convergence period was shortened by 58%, compared with the popular models. Furthermore, it performed best in solving data skew and improving efficiency, which proved the importance of having features and models based on the raw waveform.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12378-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12123-8,Content-based encrypted speech retrieval scheme with deep hashing,Multimedia Tools and Applications,10.1007/s11042-022-12123-8,Springer,2022-03-01,"In order to improve the limitations of manual features and poor feature semantics in the feature extraction process of existing content-based encrypted speech retrieval methods, and as well as improve retrieval accuracy and retrieval efficiency, a content-based encrypted speech retrieval scheme with deep hashing was proposed. Firstly, the original speech file is encrypted by using Henon mapping chaotic encryption to construct encrypted speech library. Secondly, adopting secondary feature extraction method to extract the spectrogram feature, and using the spectrogram as the input of the designed convolutional neural network (CNN) for model training and deep hashing feature learning, to obtain the deep hash binary code of original speech, and upload it to the deep hash index table in the cloud. In addition, the batch normalization (BN) method is introduced to improve robustness and generalization ability of the model. Finally, establish a one-to-one mapping relationship between the encrypt speech in the encrypted speech library and the hash sequence in the deep hash index table. When retrieving for speech users, the normalized Hamming distance algorithm is used for retrieve matching. The experimental results show that the deep hash binary code constructed by the proposed method has strong discriminability and robustness, and it still has high recall rate, precision rate and retrieval efficiency under various general content preserving operations.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12123-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12284-6,Anthropometric salient points and convolutional neural network (CNN) for 3D human body classification,Multimedia Tools and Applications,10.1007/s11042-022-12284-6,Springer,2022-03-01,"In this paper, we introduce a 3D body shape biometric for recognizing a person as one of C possible individuals stored in a database in 3D free form (scatter of 3D data points) using a couple of canonical images (front and side) taken of that individual. The first step is to reconstruct the full body 3D shape model of the individual based on their frontal and profile silhouette images. This is done by using a 3D generic model that gets morphed in accordance with the canonical images of the individual. Starting with a small set of anthropometric interconnected ordered intrinsic control points residing on the silhouette boundary of the projections of the generic model onto the frontal and profile image spaces, corresponding control points on two canonical images of the person are automatically found. This imports equivalent saliency between the two sets. The positions of these control points on the canonical images are attained using deep convolutional neural networks (CNNs) that have been trained offline on a set of images of different individuals. Further equivalent saliencies between the projected points from the generic model and the canonical images are established through loop-subdivision. To personalize the generic model, points on the generic model are morphed to be consistent with their equivalent points on the canonical images. The 3D reconstruction yields sub-resolution errors when tested on the CAESAR data set with 700 different individuals. Classification based on the error between salient points with identical anthropometric meaning residing on nested sets of boundaries in the frontal and side projections, achieves an accuracy of 96%. This is to be compared to an accuracy of 72% when using KNN nearest distance point classification between test and base models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12284-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00170-021-08542-w,3D temperature field prediction in direct energy deposition of metals using physics informed neural network,The International Journal of Advanced Manufacturing Technology,10.1007/s00170-021-08542-w,Springer,2022-03-01,"Predicting the temperature field during the direct energy deposition (DED) process is vital for the microstructure control and property tuning of fabricated metals. The widely used data-driven machine learning method for accurate temperature prediction, however, is impractical and computation-intensive due to its sole reliance on large datasets; also being a black-box model in nature, it lacks interpretability. We propose a physics informed neural network (PINN) model, which adopts a novel physics-data hybrid method by embedding the heat transfer law into the loss function of the neural network, to model the temperature field in both single-layer and multi-layer DED. The results show that the PINN-based models with additional extrapolation ability can accurately predict temperatures with a mean relative error of 4.83%, and achieve identical prediction accuracy with only 20% of the labeled data required for training the data-driven deep neural network. The proposed model is more explainable in terms of the physics of the DED process and is also applicable for the DED of different metals.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00170-021-08542-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00603-021-02748-w,Rock Discontinuities Identification from 3D Point Clouds Using Artificial Neural Network,Rock Mechanics and Rock Engineering,10.1007/s00603-021-02748-w,Springer,2022-03-01,"An artificial neural network was created by machine learning to detect group discontinuities from point clouds. Point coordinate, normal, curvature, and density were considered in input layers of the artificial neural network. A clustering algorithm was employed to subdivide group discontinuities into single discontinuities. Both efficiency and accuracy of the discontinuity detection was improved by the proposed approach. An APP and full codes of the proposed method were freely made available to the engineering community. Rock discontinuities fundamentally impact the mechanical and hydraulic behaviors of a rock mass, and thus it is a critically important task to characterize the geometrical parameters of these rock discontinuities. To measure the discontinuity orientation more accurately and efficiently, two well-known point clouds were taken as cases (a cube and a road cut), and an artificial neural network (ANN)-an machine learning algorithm-was established to identify discontinuities from point clouds through learning a small number of training samples, which had been manually selected from the raw point clouds. Four attributes associated with geometrical features of point clouds were specified as input parameters, namely, point XYZ-coordinates, point normal, point curvature, and point density. Two main groups-discontinuity and non-discontinuity-were produced in the output layer, and the number of the discontinuity groups greatly depended on the sets of discontinuities in the real situation. Using principal component analysis (PCA) and density-based spatial clustering of applications with noise (DBSCAN), single discontinuities were extracted from the group discontinuities which were obtained using ANN, and the corresponding orientations were calculated. The results obtained with the proposed method in this study matched the field surveys and results calculated by a modified region-growing algorithm. The computational efficiency was significantly enhanced using the proposed method, only taking several seconds to process a huge data. More importantly, the accuracy of discontinuity detection was greatly improved by specifying the noise data as the non-discontinuity groups during training samples selection in ANN. The ANN approach does not require the engineers have a strong professional background in computer programming, which simplified the detection and characterization process of rock discontinuity. Furthermore, an APP-named DisDetANN-was developed to implement the rock discontinuity detection based on the proposed ANN model, and the full code of the DisDetANN has been freely shared on GitHub.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00603-021-02748-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02064-y,DTR-HAR: deep temporal residual representation for human activity recognition,The Visual Computer,10.1007/s00371-021-02064-y,Springer,2022-03-01,"Human activity recognition (HAR) is a highly prized application in the pattern recognition and the computer vision fields. Up till now, deep neural networks have acquired big attention in computer studies and image processing fields, and have generated significant results. In this paper, we propose a deep temporal residual system for daily living activity recognition that aims to enhance spatiotemporal feature representation in order to improve the HAR system performance. To this end, we adopt a deep residual convolutional neural network (RCN) to retain discriminative visual features relayed to appearance and long short-term memory neural network to capture the long-term temporal evolution of actions. The latter was considered to implement time dependencies occurring when carrying out the activity to enhance features extracted from the RCN network by adding time information to address the dynamic activity recognition problem as a sequence labeling job. The deep temporal residual model for human activity recognition system is performed on two benchmark publicly available datasets: MSRDailyActivity3D and CAD-60. the proposed system achieves very competitive results when compared to others from the state of the art.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02064-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s41315-021-00180-5,Digital twins: artificial intelligence and the IoT cyber-physical systems in Industry 4.0,International Journal of Intelligent Robotics and Applications,10.1007/s41315-021-00180-5,Springer,2022-03-01,"This paper presents a summary of mechanisms for the evolution of artificial intelligence in ‘internet of things’ networks. Firstly, the paper investigates how the use of new technologies in industrial systems improves organisational resilience supporting both a technical and human level. Secondly, the paper reports empirical results that correlate academic literature with Industry 4.0 interdependencies between edge components to both external and internal services and systems. The novelty of the paper is a new approach for creating a virtual representation operating as a real-time digital counterpart of a physical object or process (i.e., digital twin) outlined in a conceptual diagram. The methodology applied in this paper resembled a grounded theory analysis of complex interconnected and coupled systems. By connecting the human–computer interactions in different information knowledge management systems, this paper presents a summary of mechanisms for the evolution of artificial intelligence in internet of things networks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41315-021-00180-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s43390-021-00421-4,Semi-automatic ultrasound curve angle measurement for adolescent idiopathic scoliosis,Spine Deformity,10.1007/s43390-021-00421-4,Springer,2022-03-01,"Purpose Using X-ray to evaluate adolescent idiopathic scoliosis (AIS) conditions is the clinical gold standard, with potential radiation hazards. 3D ultrasound has demonstrated its validity and reliability of estimating X-ray Cobb angle (XCA) using spinous process angle (SPA), which can be automatically measured. While angle measurement with ultrasound using spine transverse process-related landmarks (UCA) shows better agreed with XCA, its automatic measurement is challenging and not available yet. This research aimed to analyze and measure scoliotic angles through a novel semi-automatic UCA method. Methods 100 AIS subjects (age: 15.0 ± 1.9 years, gender: 19 M and 81 F, Cobb: 25.5 ± 9.6°) underwent both 3D ultrasound and X-ray scanning on the same day. Scoliotic angles with XCA and UCA methods were measured manually; and transverse process-related features were identified/drawn for the semi-automatic UCA method. The semi-automatic method measured the spinal curvature with pairs of thoracic transverse processes and lumbar lumps in respective regions. Results The new semi-automatic UCA method showed excellent correlations with manual XCA ( R ^2 = 0.815: thoracic angles R ^2 = 0.857, lumbar angles R ^2 = 0.787); and excellent correlations with manual UCA ( R ^2 = 0.866: thoracic angles R ^2 = 0.921, lumbar angles R ^2 = 0.780). The Bland–Altman plot also showed a good agreement against manual UCA/XCA. The MADs of semi-automatic UCA against XCA were less than 5°, which is clinically insignificant. Conclusion The semi-automatic UCA method had demonstrated the possibilities of estimating manual XCA and UCA. Further advancement in image processing to detect the vertebral landmarks in ultrasound images could help building a fully automated measurement method. Level of evidence Level III.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s43390-021-00421-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11747-6,Effective network intrusion detection by addressing class imbalance with deep neural networks multimedia tools and applications,Multimedia Tools and Applications,10.1007/s11042-021-11747-6,Springer,2022-03-01,"The Intrusion Detection System plays a significant role in discovering malicious activities and provides better network security solutions than other conventional defense techniques such as firewalls. With the aid of machine learning-based techniques, such systems can detect attacks more accurately by identifying the relevant data patterns. However, the nature of network data, time-varying environment, and unknown occurrence of attacks made the learning task very complex. We propose a deep neural network that utilizes the classifier-level class imbalance solution to solve this problem effectively. Initially, the network data is preprocessed through data conversion followed by the min-max normalization method. Then, normalized data is fed to neural network where the cross-entropy function is modified to address the class imbalance problem. It is achieved by weighting the classes while training the classifier. The extensive experiments are performed on two challenging datasets, namely NSL-KDD and UNSW-NB15, to establish the superiority of the proposed approach. It includes comparisons with commonly employed imbalance approaches such as under-sampling, over-sampling, and bagging as well as existing works. The proposed approach attains 85.56% and 90.76% classification accuracy on NSL-KDD and UNSW-NB15 datasets, respectively. These outcomes outperformed data-level imbalance methods and existing works that validate the need to incorporate class imbalance for network traffic categorization.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11747-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1134/S1054661822010023,Mobile ID Document Recognition–Coarse-to-Fine Approach,Pattern Recognition and Image Analysis,10.1134/S1054661822010023,Springer,2022-03-01,"Abstract Automatic optical recognition of documents is a traditional function of modern document processing systems. In this context, recognition represents a complex process which includes image processing, segmentation, classification, and linguistic analysis. Although the idea of using mobile devices for recognition of paper documents is not new, direct usage of existing software solutions for scanned images recognition yields low recognition precision on images obtained using a mobile device. This is due, first of all, to perspective distortions and lower effective resolution in the latter case. In this paper, we present an original approach and a set of algorithms for recognition of video frame sequence containing a document image, which is suitable for mobile implementation. It is based on a coarse-to-fine methodology, where template matching and fields localization are performed on the image with lowered resolution, followed by lazy processing of parts of the images only corresponding to the fields which are not recognized yet. Video stream is utilized as a source of noise reduction both in coordinates of the fields and optical character recognition classifiers outputs. The algorithm based on the proposed approach is suitable for running on the device itself and can operate even when none of the video frames contain a document image of sufficient quality by themselves.",http://link.springer.com/openurl/fulltext?id=doi:10.1134/S1054661822010023,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s41095-021-0223-y,Learning conditional photometric stereo with high-resolution features,Computational Visual Media,10.1007/s41095-021-0223-y,Springer,2022-03-01,"Photometric stereo aims to reconstruct 3D geometry by recovering the dense surface orientation of a 3D object from multiple images under differing illumination. Traditional methods normally adopt simplified reflectance models to make the surface orientation computable. However, the real reflectances of surfaces greatly limit applicability of such methods to real-world objects. While deep neural networks have been employed to handle non-Lambertian surfaces, these methods are subject to blurring and errors, especially in high-frequency regions (such as crinkles and edges), caused by spectral bias: neural networks favor low-frequency representations so exhibit a bias towards smooth functions. In this paper, therefore, we propose a self-learning conditional network with multi-scale features for photometric stereo, avoiding blurred reconstruction in such regions. Our explorations include: (i) a multi-scale feature fusion architecture, which keeps high-resolution representations and deep feature extraction, simultaneously, and (ii) an improved gradient-motivated conditionally parameterized convolution (GM-CondConv) in our photometric stereo network, with different combinations of convolution kernels for varying surfaces. Extensive experiments on public benchmark datasets show that our calibrated photometric stereo method outperforms the state-of-the-art.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41095-021-0223-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00158-022-03194-0,Accelerated topology optimization design of 3D structures based on deep learning,Structural and Multidisciplinary Optimization,10.1007/s00158-022-03194-0,Springer,2022-02-28,"Topology optimization is computationally demanding for times of finite element analysis during iteration, especially for 3D structure optimization in intelligent design and construction, where a large amount of concept designs are executed. To this end, this paper proposes a deep Convolutional Neural Network (CNN) for 3D structural topology optimization, which can predict a near-optimal structure in negligible time without using any iterative scheme. To train the neural network, the Simplified Isotropic Material with Penalization (SIMP) method was adopted to generate the dataset of the optimized structures under the randomness loading conditions and volume fractions. The performance evaluation results show that compared with the SIMP method, the proposed method achieved a significant reduction in computation cost with little sacrifice on the performance of design solutions. Furthermore, the generalization ability of the proposed method trained on single boundary condition as well as multiple boundary conditions are explored. Finally, a stress-based topology optimization of the L-shape design domain is performed, which shows the portability of the proposed method to other more complex constraints.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00158-022-03194-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11830-y,An AI-based Approach for Improved Sign Language Recognition using Multiple Videos,Multimedia Tools and Applications,10.1007/s11042-021-11830-y,Springer,2022-02-28,"People with hearing and speaking disabilities face significant hurdles in communication. The knowledge of sign language can help mitigate these hurdles, but most people without disabilities, including relatives, friends, and care providers, cannot understand sign language. The availability of automated tools can allow people with disabilities and those around them to communicate ubiquitously and in a variety of situations with non-signers. There are currently two main approaches for recognizing sign language gestures. The first is a hardware-based approach, involving gloves or other hardware to track hand position and determine gestures. The second is a software-based approach, where a video is taken of the hands and gestures are classified using computer vision techniques. However, some hardware, such as a phone's internal sensor or a device worn on the arm to track muscle data, is less accurate, and wearing them can be cumbersome or uncomfortable. The software-based approach, on the other hand, is dependent on the lighting conditions and on the contrast between the hands and the background. We propose a hybrid approach that takes advantage of low-cost sensory hardware and combines it with a smart sign-recognition algorithm with the goal of developing a more efficient gesture recognition system. The Myo band-based approach using the Support Vector Machine method achieves an accuracy of only 49%. The software-based approach uses the Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) methods to train the Myo-based module and achieves an accuracy of over 80% in our experiments. Our method combines the two approaches and shows the potential for improvement. Our experiments are done with a dataset of nine gestures generated from multiple videos, each repeated five times for a total of 45 trials for both the software-based and hardware-based modules. Apart from showing the performance of each approach, our results show that with a more improved hardware module, the accuracy of the combined approach can be significantly improved.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11830-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s12880-022-00760-2,Grayscale medical image segmentation method based on 2D&3D object detection with deep learning,BMC Medical Imaging,10.1186/s12880-022-00760-2,BioMed Central,2022-02-27,"Background Grayscale medical image segmentation is the key step in clinical computer-aided diagnosis. Model-driven and data-driven image segmentation methods are widely used for their less computational complexity and more accurate feature extraction. However, model-driven methods like thresholding usually suffer from wrong segmentation and noises regions because different grayscale images have distinct intensity distribution property thus pre-processing is always demanded. While data-driven methods with deep learning like encoder-decoder networks always are always accompanied by complex architectures which require amounts of training data. Methods Combining thresholding method and deep learning, this paper presents a novel method by using 2D&3D object detection technologies. First, interest regions contain segmented object are determined with fine-tuning 2D object detection network. Then, pixels in cropped images are turned as point cloud according to their positions and grayscale values. Finally, 3D object detection network is applied to obtain bounding boxes with target points and boxes’ bottoms and tops represent thresholding values for segmentation. After projecting to 2D images, these target points could composite the segmented object. Results Three groups of grayscale medical images are used to evaluate the proposed image segmentation method. We obtain the IoU (DSC) scores of 0.92 (0.96), 0.88 (0.94) and 0.94 (0.94) for segmentation accuracy on different datasets respectively. Also, compared with five state of the arts and clinically performed well models, our method achieves higher scores and better performance. Conclusions The prominent segmentation results demonstrate that the built method based on 2D&3D object detection with deep learning is workable and promising for segmentation task of grayscale medical images.",https://www.biomedcentral.com/openurl?doi=10.1186/s12880-022-00760-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00530-022-00902-0,Image and audio caps: automated captioning of background sounds and images using deep learning,Multimedia Systems,10.1007/s00530-022-00902-0,Springer,2022-02-26,"Image recognition based on computers is something human beings have been working on for many years. It is one of the most difficult tasks in the field of computer science, and improvements to this system are made when we speak. In this paper, we propose a methodology to automatically propose an appropriate title and add a specific sound to the image. Two models have been extensively trained and combined to achieve this effect. Sounds are recommended based on the image scene and the headings are generated using a combination of natural language processing and state-of-the-art computer vision models. A Top 5 accuracy of 67% and a Top 1 accuracy of 53% have been achieved. It is also worth mentioning that this is also the first model of its kind to make this forecast.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-022-00902-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-022-12178-7,Multi-modal active learning with deep reinforcement learning for target feature extraction in multi-media image processing applications,Multimedia Tools and Applications,10.1007/s11042-022-12178-7,Springer,2022-02-25,"The advancement in on demand Multimedia Streaming Applications (MAS) enables faster video transmission as per the user request in various fields. This system suffers from poor speed, flexibility and efficiency in accessing and presenting the multimedia contents from the archive. It mostly undergoes delay, packet loss and congestion during data delivery. Hence, the requirement of manual annotation is required for access and retrieval but it suffers from poor retrieval accuracy over large databases. The need of automatic annotation in MAS takes the lead for increased retrieval accuracy on most similar image retrieval systems based on various low-level features. Thus, it eliminates the gap between the high-level semantics and low-level feature representation. The approach on automated annotation of images is considered dependent on the accuracy of a model while detecting edges, color, texture, shape and spatial information. In this paper, we develop an automated annotation model that retrieves visually similar images from online multimedia streams with optimal feature extraction. The automated annotation model is designed with a Multi-modal Active Learning (MAL) that uses Convolutional Recurrent Neural Network (CRNN) for automatic annotation of labels based on visually similar contents or features like edges, color, texture, shape and spatial information. Further, a Deep Reinforcement Learning (DRL) algorithm is used that increases the performance of the retrieval engine based on validating the visually extracted features. The simulation of MAL-CNN is conducted over large online streaming databases and it is then validated by DRL on an online real-time streaming. The performance is validated in terms of its retrieval accuracy, sensitivity, specificity, f-measure, geometric mean and mean absolute percentage error (MAPE). The results confirm the accuracy of the proposed MAL-DRL model against conventional machine learning, reinforcement learning and deep learning automatic annotation models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-022-12178-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.3758/s13428-021-01747-7,Measuring national mood with music: using machine learning to construct a measure of national valence from audio data,Behavior Research Methods,10.3758/s13428-021-01747-7,Springer,2022-02-25,We propose a new measure of national valence based on the emotional content of a country’s most popular songs. We first trained a machine learning model using 191 different audio features embedded within music and use this model to construct a long-run valence index for the UK. This index correlates strongly and significantly with survey-based life satisfaction and outperforms an equivalent text-based measure. Our methods have the potential to be applied widely and to provide a solution to the severe lack of historical time-series data on psychological well-being.,http://link.springer.com/openurl/fulltext?id=doi:10.3758/s13428-021-01747-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10579-022-09586-4,Semi-automation of gesture annotation by machine learning and human collaboration,Language Resources and Evaluation,10.1007/s10579-022-09586-4,Springer,2022-02-25,"Gesture and multimodal communication researchers typically annotate video data manually, even though this can be a very time-consuming task. In the present work, a method to detect gestures is proposed as a fundamental step towards a semi-automatic gesture annotation tool. The proposed method can be applied to RGB videos and requires annotations of part of a video as input. The technique deploys a pose estimation method and active learning. In the experiment, it is shown that if about 27% of the video is annotated, the remaining parts of the video can be annotated automatically with an F-score of at least 0.85. Users can run this tool with a small number of annotations first. If the predicted annotations for the remainder of the video are not satisfactory, users can add further annotations and run the tool again. The code has been released so that other researchers and practitioners can use the results of this research. This tool has been confirmed to work in conjunction with ELAN.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10579-022-09586-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s41870-022-00907-y,Combining audio and visual speech recognition using LSTM and deep convolutional neural network,International Journal of Information Technology,10.1007/s41870-022-00907-y,Springer,2022-02-24,"Human speech is bimodal, whereas audio speech relates to the speaker's acoustic waveform. Lip motions are referred to as visual speech. Audiovisual Speech Recognition is one of the emerging fields of research, particularly when audio is corrupted by noise. In the proposed AVSR system, a custom dataset was designed for English Language. Mel Frequency Cepstral Coefficients technique was used for audio processing and the Long Short-Term Memory (LSTM) method for visual speech recognition. Finally, integrate the audio and visual into a single platform using a deep neural network. From the result, it was evident that the accuracy was 90% for audio speech recognition, 71% for visual speech recognition, and 91% for audiovisual speech recognition, the result was better than the existing approaches. Ultimately model was skilled at enchanting many suitable decisions while forecasting the spoken word for the dataset that was used.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41870-022-00907-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s13244-022-01163-1,Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network,Insights into Imaging,10.1186/s13244-022-01163-1,Springer,2022-02-24,"Objective We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images. Methods This retrospective study evaluated an automated method using a deep neural network that was trained, validated, and tested with 367, 157, and 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff distance (HD), and volume ratio (RV) were used to quantitatively measure the accuracy of segmentation. The time consumed for model and manual segmentation was also compared. In addition, the model was applied to 100 consecutive cases from real clinical scenario for a qualitative evaluation and indirect evaluation. Results In quantitative evaluation, the model achieved high accuracy for DSC, MSD, HD and RV (0.920, 3.34, 3.61 and 1.01, respectively). Compared to manual segmentation, the automated method reduced the segmentation time from 26 min to 8 s. In qualitative evaluation, the segmentation quality was rated as good in 79% of the cases, moderate in 15% and poor in 6%. In indirect evaluation, 93.4% (99/106) of lesions could be assigned to the correct segment by only referring to the results from automated segmentation. Conclusion The proposed model may serve as an effective tool for automated anatomical region annotation of the liver on MRI images.",http://link.springer.com/openurl/fulltext?id=doi:10.1186/s13244-022-01163-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-022-02167-6,3D gesture segmentation for word-level Arabic sign language using large-scale RGB video sequences and autoencoder convolutional networks,"Signal, Image and Video Processing",10.1007/s11760-022-02167-6,Springer,2022-02-23,"Sign languages use hands, body movements, and facial expressions to deliver a message. Developing a communication environment for the deaf community is a social and economical necessity. Research has been conducted on the segmentation of gestures to develop methods capable of identifying a given sequence of signs and understanding their meaning. However, the variety of hand shapes and the complexity of gestures remain a challenge. In this paper, we propose a novel model called 3D gesture segmentation network (3D GS-Net) from video sequences for word-level Arabic sign language (ArSL) with a small number of features. To efficiently process and analyze the frame sequences, annotation and normalization are applied to the dataset. During the training phase, the preprocessed data are fed into the 3D GS-Net model using an autoencoder convolutional network architecture designed as a two-branch network that is merged at the final layer to produce the final predictive segmentation output. The proposed 3D GS-Net has been experimented with RGB videos of the Moroccan sign language (MoSL) dataset. Our obtained results have been compared with existing approaches and demonstrate the effectiveness and efficiency of our 3D GS-Net approach in the segmentation of gestures through different evaluation metrics .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-022-02167-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s12891-022-05126-x,"Musculoskeletal care – at the confluence of data science, sensors, engineering, and computation",BMC Musculoskeletal Disorders,10.1186/s12891-022-05126-x,BioMed Central,2022-02-22,"Data has always been integral to modern medicine in almost all aspects of patient care and the recent proliferation of data has opened up innumerable opportunities for all the stakeholders in trying to improve the quality of care and health outcomes including quality of life and rehabilitation. Greater usage and adoption of digital technologies have led to the convergence of health data in different forms – clinical, self-reported, electronic health records social media, etc. The application and utilization of patient data set continue to get broadened each day with greater availability and access. These are empowering newer cutting-edge solutions such as connected care and artificial intelligence, 3D printing and real-life mimicking prosthetics. The availability of data at micro and macro levels has the potential to act as a catalyst for personalized care based on behavioral, cultural, genetic, and psychological needs for patients with musculoskeletal disorders. Realistic algorithms coupled with biomarkers which can identify relevant interventions and alert the care providers regarding any deterioration. Although in the nascent stage currently, 3D printing, exoskeletons, and virtual rehabilitation hold tremendous potential of cost-effective, precise interventions for the patients.",https://www.biomedcentral.com/openurl?doi=10.1186/s12891-022-05126-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s13634-022-00848-5,3D reconstruction from structured-light profilometry with dual-path hybrid network,EURASIP Journal on Advances in Signal Processing,10.1186/s13634-022-00848-5,Springer,2022-02-21,"With the rapid development of high-speed image sensors and optical imaging technology, these have effectively promoted the improvement of non-contact 3D shape measurement. Among them, striped structured-light technology has been widely used because of its high measurement accuracy. Compared with classical methods such as Fourier transform profilometry, many deep neural networks are utilized to restore 3D shape from single-shot structured light. In actual engineering deployments, the number of learnable parameters of convolution neural network (CNN) is huge, especially for high-resolution structured-light patterns. To this end, we proposed a dual-path hybrid network based on UNet, which eliminates the deepest convolution layers to reduce the number of learnable parameters, and a swin transformer path is additionally built on the decoder to improve the global perception of this network. The experimental results show that the learnable parameters of the model are reduced by 60% compared with the UNet, and the measurement accuracy is not degraded at the same time. The proposed dual-path hybrid network provides an effective solution for structured-light 3D reconstruction and its practice in engineering.",https://www.biomedcentral.com/openurl?doi=10.1186/s13634-022-00848-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41467-022-28526-y,Inverse design of 3d molecular structures with conditional generative neural networks,Nature Communications,10.1038/s41467-022-28526-y,Nature,2022-02-21,"The targeted discovery of molecules with specific structural and chemical properties is an open challenge in computational chemistry. Here, the authors propose a conditional generative neural network for the inverse design of 3d molecular structures. The rational design of molecules with desired properties is a long-standing challenge in chemistry. Generative neural networks have emerged as a powerful approach to sample novel molecules from a learned distribution. Here, we propose a conditional generative neural network for 3d molecular structures with specified chemical and structural properties. This approach is agnostic to chemical bonding and enables targeted sampling of novel molecules from conditional distributions, even in domains where reference calculations are sparse. We demonstrate the utility of our method for inverse design by generating molecules with specified motifs or composition, discovering particularly stable molecules, and jointly targeting multiple electronic properties beyond the training regime.",https://www.nature.com/articles/s41467-022-28526-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s13007-022-00857-3,Segmentation of structural parts of rosebush plants with 3D point-based deep learning methods,Plant Methods,10.1186/s13007-022-00857-3,BioMed Central,2022-02-20,"Background Segmentation of structural parts of 3D models of plants is an important step for plant phenotyping, especially for monitoring architectural and morphological traits. Current state-of-the art approaches rely on hand-crafted 3D local features for modeling geometric variations in plant structures. While recent advancements in deep learning on point clouds have the potential of extracting relevant local and global characteristics, the scarcity of labeled 3D plant data impedes the exploration of this potential. Results We adapted six recent point-based deep learning architectures (PointNet, PointNet++, DGCNN, PointCNN, ShellNet, RIConv) for segmentation of structural parts of rosebush models. We generated 3D synthetic rosebush models to provide adequate amount of labeled data for modification and pre-training of these architectures. To evaluate their performance on real rosebush plants, we used the ROSE-X data set of fully annotated point cloud models. We provided experiments with and without the incorporation of synthetic data to demonstrate the potential of point-based deep learning techniques even with limited labeled data of real plants. Conclusion The experimental results show that PointNet++ produces the highest segmentation accuracy among the six point-based deep learning methods. The advantage of PointNet++ is that it provides a flexibility in the scales of the hierarchical organization of the point cloud data. Pre-training with synthetic 3D models boosted the performance of all architectures, except for PointNet.",https://www.biomedcentral.com/openurl?doi=10.1186/s13007-022-00857-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s12880-022-00753-1,The application research of AI image recognition and processing technology in the early diagnosis of the COVID-19,BMC Medical Imaging,10.1186/s12880-022-00753-1,BioMed Central,2022-02-17,"Background This study intends to establish a combined prediction model that integrates the clinical symptoms,the lung lesion volume, and the radiomics features of patients with COVID-19, resulting in a new model to predict the severity of COVID-19. Methods The clinical data of 386 patients with COVID-19 at several hospitals, as well as images of certain patients during their hospitalization, were collected retrospectively to create a database of patients with COVID-19 pneumonia. The contour of lungs and lesion locations may be retrieved from CT scans using a CT-image-based quantitative discrimination and trend analysis method for COVID-19 and the Mask R-CNN deep neural network model to create 3D data of lung lesions. The quantitative COVID-19 factors were then determined, on which the diagnosis of the development of the patients' symptoms could be established. Then, using an artificial neural network, a prediction model of the severity of COVID-19 was constructed by combining characteristic imaging features on CT slices with clinical factors. ANN neural network was used for training, and tenfold cross-validation was used to verify the prediction model. The diagnostic performance of this model is verified by the receiver operating characteristic (ROC) curve. Results CT radiomics features extraction and analysis based on a deep neural network can detect COVID-19 patients with an 86% sensitivity and an 85% specificity. According to the ROC curve, the constructed severity prediction model indicates that the AUC of patients with severe COVID-19 is 0.761, with sensitivity and specificity of 79.1% and 73.1%, respectively. Conclusions The combined prediction model for severe COVID-19 pneumonia, which is based on deep learning and integrates clinical aspects, pulmonary lesion volume, and radiomics features of patients, has a remarkable differential ability for predicting the course of disease in COVID-19 patients. This may assist in the early prevention of severe COVID-19 symptoms.",https://www.biomedcentral.com/openurl?doi=10.1186/s12880-022-00753-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-06190-y,Comprehensive deep learning model for 3D color holography,Scientific Reports,10.1038/s41598-022-06190-y,Nature,2022-02-15,"Holography is a vital tool used in various applications from microscopy, solar energy, imaging, display to information encryption. Generation of a holographic image and reconstruction of object/hologram information from a holographic image using the current algorithms are time-consuming processes. Versatile, fast in the meantime, accurate methodologies are required to compute holograms performing color imaging at multiple observation planes and reconstruct object/sample information from a holographic image for widely accommodating optical holograms. Here, we focus on design of optical holograms for generation of holographic images at multiple observation planes and colors via a deep learning model, the CHoloNet. The CHoloNet produces optical holograms which show multitasking performance as multiplexing color holographic image planes by tuning holographic structures. Furthermore, our deep learning model retrieves an object/hologram information from an intensity holographic image without requiring phase and amplitude information from the intensity image. We show that reconstructed objects/holograms show excellent agreement with the ground-truth images. The CHoloNet does not need iteratively reconstruction of object/hologram information while conventional object/hologram recovery methods rely on multiple holographic images at various observation planes along with the iterative algorithms. We openly share the fast and efficient framework that we develop in order to contribute to the design and implementation of optical holograms, and we believe that the CHoloNet based object/hologram reconstruction and generation of holographic images will speed up wide-area implementation of optical holography in microscopy, data encryption, and communication technologies.",https://www.nature.com/articles/s41598-022-06190-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40537-022-00569-4,Semantic context driven language descriptions of videos using deep neural network,Journal of Big Data,10.1186/s40537-022-00569-4,Springer,2022-02-10,"The massive addition of data to the internet in text, images, and videos made computer vision-based tasks challenging in the big data domain. Recent exploration of video data and progress in visual information captioning has been an arduous task in computer vision. Visual captioning is attributable to integrating visual information with natural language descriptions. This paper proposes an encoder-decoder framework with a 2D-Convolutional Neural Network (CNN) model and layered Long Short Term Memory (LSTM) as the encoder and an LSTM model integrated with an attention mechanism working as the decoder with a hybrid loss function. Visual feature vectors extracted from the video frames using a 2D-CNN model capture spatial features. Specifically, the visual feature vectors are fed into the layered LSTM to capture the temporal information. The attention mechanism enables the decoder to perceive and focus on relevant objects and correlate the visual context and language content for producing semantically correct captions. The visual features and GloVe word embeddings are input into the decoder to generate natural semantic descriptions for the videos. The performance of the proposed framework is evaluated on the video captioning benchmark dataset Microsoft Video Description (MSVD) using various well-known evaluation metrics. The experimental findings indicate that the suggested framework outperforms state-of-the-art techniques. Compared to the state-of-the-art research methods, the proposed model significantly increased all measures, B@1, B@2, B@3, B@4, METEOR, and CIDEr, with the score of 78.4, 64.8, 54.2, and 43.7, 32.3, and 70.7, respectively. The progression in all scores indicates a more excellent grasp of the context of the inputs, which results in more accurate caption prediction.",https://www.biomedcentral.com/openurl?doi=10.1186/s40537-022-00569-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10921-022-00843-8,POD Evaluation: The Key Performance Indicator for NDE 4.0,Journal of Nondestructive Evaluation,10.1007/s10921-022-00843-8,Springer,2022-02-10,"Reliability evaluations of modern test systems under the Industry 4.0 technologies, play a vital role in the successful transformation to NDE 4.0. This is due to the fact that NDE 4.0 is mainly based on the interconnection between the cyber-physical systems. When the individual reliability of the various important technologies from the Industry 4.0 such as the digital twin, digital thread, Industrial Internet of Things (IIoT), artificial intelligence (AI), data fusion, digitization, etc. is high, then it is possible to obtain the reliability beyond the intrinsic capability of the test system. In this paper, the significance of the reliability evaluation is reviewed under the vision of NDE 4.0, including examples of data fusion concepts as well as the importance of algorithms (like explainable artificial intelligence), the practical use is discussed and elaborated accordingly.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10921-022-00843-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00202-022-01501-y,Enhanced particle swarm optimization-based convolution neural network hyperparameters tuning for transformer failure diagnosis under complex data sources,Electrical Engineering,10.1007/s00202-022-01501-y,Springer,2022-02-09,"Measurement and recognition of partial discharge (PD) is a fundamental tool for condition monitoring and fault diagnosis of high-voltage (HV) transformers. Several machine learning (ML) approaches were used for PD classification in the past decades. However, ML techniques mainly depend upon feature extraction by human experts to train the model and clean PD data quantified in the HV laboratory. Still, in real-time circumstances, PD may occur in single and multiple PD patterns along with external interference. The deep convolution neural network (CNN) method is currently applicable for image classification. However, the challenging issue in the traditional CNN approaches is fine-tuning the hyperparameters with specific architecture. Thus, enhanced particle swarm optimization for hyperparameters tuning will reduce experts' manual trial and error method. Simultaneously, random minority and majority sampling approaches can overwhelm an imbalanced dataset problem. Results obtained by the ZFNet architecture compared with various pre-trained models like VGG19, AlexNet, GoogleNet, and ResNet, whose hyperparameters were determined manually. The result shows that the proposed model achieves a 7.6% higher recognition rate than various deep learning approaches under a noisy environment.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00202-022-01501-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-022-09958-9,Intelligent automobile auxiliary propagation system based on speech recognition and AI driven feature extraction techniques,International Journal of Speech Technology,10.1007/s10772-022-09958-9,Springer,2022-02-08,"In the recent years, with the rapid development of China's national economy, people's living standards have been greatly improved. People's consumption demand is constantly increasing, and the consumption structure is constantly upgrading. In the automotive industry, which is increasingly related to residents' travel, consumers' demand for cars also presents a rising trend. The production and sales of domestic automobile market in China have increased greatly, and it has become the largest automobile consumer market in the world for eight consecutive years. In the information age, the speed of information interaction is faster and faster. Today, 5G technology has entered the commercial era, and the era of interconnection of everything has come. According to the work report of the Chinese government, the future city should be a smart city. In the background of Internet of things, smart city will greatly facilitate people's life in the way of ecosystem. As part of the future smart city ecosystem, intelligent cars must be self driving vehicles that meet the needs of smart city ecosystems. Therefore, the marketing strategy of smart cars is very important. Voice has always been one of the most concerned research contents in the field of human–computer communication and interaction. The main purpose of automatic speech recognition is to enable the computer to ""understand"" human speech and convert speech waveform signal into text. Speech recognition technology is one of the key technologies to realize intelligent human–computer interaction. The application of voice, the most natural way of interaction between human and machine, can effectively improve the input efficiency, error prone and other shortcomings of traditional interaction methods. This paper studies the intelligent vehicle marketing strategy based on speech recognition and artificial intelligence driven feature extraction technology. Through the modelling and the comparison simulations, the performance of the designed model is verified.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-022-09958-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-022-09962-z,To deploy trained speech with DNN-LSTM framework for controlling a smart wheeled-robot in limited learning circumstance,International Journal of Speech Technology,10.1007/s10772-022-09962-z,Springer,2022-02-08,"The assessment to a trained speech controller with deep neural network long-short term memory (DNN-LSTM) framework adopted as the commander to control a smart wheeled-robot is implemented in the article. Accordingly, the deployment is implemented in recognition to control remotely a smart wheeled-robot which has been completed previously by other project of authors’ research group. Based on the machine learning skill a framework established with the DNN-LSTM model is embedded into the smart wheeled-robot prototype. Apart from, the control commands are designed over limited learning circumstance where constrained single-track (ST) and double-track (DT) speeches, and only are including 4 Chinese speech commands, “forward” (Chinese”前進”), “backward” (Chinese”後退”), “turn left” (Chinese”左轉”), and “turn right” (Chinese”右轉”). Though, there are just 4 simple speeches collected for data training, the investigation to the accurate ratio is deployed in 3 separated persons training work and each with 1000 to 5000 training times. There are just three parameters (this why “ Limited Learning Circumstance ” is referred as the article name) considered as the dominators for the performance evaluation of the speech controlled wheeled-robot. The results from the testing cases clearly show that the set with DT has the higher accurate comparison with the set of ST. The best outcomes form the performance of testing and validation happens at the case of DT channel, hereafter, the accurate and loss rate are obtained as 0.673 and 0.018 with 50% dropout, respectively. However, the ratio of dropout has been discovered definitely to dominate the accurate and loss rate when it is deployed during the process of training step. Eventually, the trained and developed model of speech command sets are uploaded into a micro-controller after accuracy analyzed, and embedded into the smart wheeled-robot plays as remotely pilot scheme.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-022-09962-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-06939-6,SPINet: self-supervised point cloud frame interpolation network,Neural Computing and Applications,10.1007/s00521-022-06939-6,Springer,2022-02-07,"For autonomous vehicles, the acquisition frequency difference between LiDAR (10–20 Hz) and camera (over 100 Hz) makes simultaneous update of two perceptive systems (2D/3D) less efficient. Nowadays, frame interpolation is in urgent need for increasing frame rate of point cloud sequences obtained by LiDAR. However, a major limitation of current full supervised methods is that high frame rate ground truth sequences are hard to access. We propose a novel Self-supervised Point Cloud Frame Interpolation Network (SPINet) accommodating with variable motion situation, retaining geometric consistency, but without the necessity of utilizing G.T. data. Extensive experiments show that our proposed SPINet outperforms the current full supervised methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-06939-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11036-021-01890-7,Ignoring Encrypted Protocols: Cross-layer Prediction of Video Streaming QoE Metrics,Mobile Networks and Applications,10.1007/s11036-021-01890-7,Springer,2022-02-05,"Nowadays, the widely used end-to-end encryptions have prevented the Deep Packet Inspect (DPI) method applied in Quality of Experience (QoE) prediction and optimization. Thus, for the Internet Service Providers (ISPs), it has become more difficult to monitor and improve user QoE when users are watching videos or using other video application services. To overcome the above problem, a Cross-layer Predicting framework of Video Streaming QoE (CPVS) is proposed to monitor and predict video indicators that affect user QoE, such as startup delay, stalling rate and stalling events. The most prominent advantage of the CPVS is that it can predict the video QoE indicators without the prior information of transmission protocols, whether it is encrypted or not. Furthermore, the general framework, CPVS, shows high predicting accuracy both in a time granularity as small as one second and in video-session prediction, even can perform well with only 6 features. For the session-based prediction, the best weighted f1 score achieves 85.71% about the startup delay prediction, and achieves 84.64% about the stalling rate classification. For the real-time prediction, the best weighted f1 score can achieve 79.25%. Besides, by exploring the 4G network features, we find out the 6 most critical ones that not only make the CPVS retain high accuracy but also save 35.02% running time to improve the real-time prediction performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11036-021-01890-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00138-022-01278-x,Contextual Guided Segmentation Framework for Semi-supervised Video Instance Segmentation,Machine Vision and Applications,10.1007/s00138-022-01278-x,Springer,2022-02-03,"In this paper, we propose contextual guided segmentation (CGS) framework for video instance segmentation in three passes. In the first pass, i.e .,preview segmentation, we propose Instance Re-Identification Flow to estimate main properties of each instance (i.e., human/non-human, rigid/deformable, known/unknown category) by propagating its preview mask to other frames. In the second pass, i.e .,contextual segmentation, we introduce multiple contextual segmentation schemes. For human instance, we develop skeleton-guided segmentation in a frame along with object flow to correct and refine the result across frames. For non-human instance, if the instance has a wide variation in appearance and belongs to known categories (which can be inferred from the initial mask), we adopt instance segmentation. If the non-human instance is nearly rigid, we train FCNs on synthesized images from the first frame of a video sequence. In the final pass, i.e .,guided segmentation, we develop a novel fined-grained segmentation method on non-rectangular regions of interest (ROIs). The natural-shaped ROI is generated by applying guided attention from the neighbor frames of the current one to reduce the ambiguity in the segmentation of different overlapping instances. Forward mask propagation is followed by backward mask propagation to further restore missing instance fragments due to re-appeared instances, fast motion, occlusion, or heavy deformation. Finally, instances in each frame are merged based on their depth values, together with human and non-human object interaction and rare instance priority. Experiments conducted on the DAVIS Test-Challenge dataset demonstrate the effectiveness of our proposed framework. We achieved the 3rd consistently in the DAVIS Challenges 2017–2019 with 75.4%, 72.4%, and 78.4% in terms of global score, region similarity, and contour accuracy, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00138-022-01278-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-022-05966-6,Automated recognition of the cricket batting backlift technique in video footage using deep learning architectures,Scientific Reports,10.1038/s41598-022-05966-6,Nature,2022-02-03,"There have been limited studies demonstrating the validation of batting techniques in cricket using machine learning. This study demonstrates how the batting backlift technique in cricket can be automatically recognised in video footage and compares the performance of popular deep learning architectures, namely, AlexNet, Inception V3, Inception Resnet V2, and Xception. A dataset is created containing the lateral and straight backlift classes and assessed according to standard machine learning metrics. The architectures had similar performance with one false positive in the lateral class and a precision score of 100%, along with a recall score of 95%, and an f1-score of 98% for each architecture, respectively. The AlexNet architecture performed the worst out of the four architectures as it incorrectly classified four images that were supposed to be in the straight class. The architecture that is best suited for the problem domain is the Xception architecture with a loss of 0.03 and 98.2.5% accuracy, thus demonstrating its capability in differentiating between lateral and straight backlifts. This study provides a way forward in the automatic recognition of player patterns and motion capture, making it less challenging for sports scientists, biomechanists and video analysts working in the field.",https://www.nature.com/articles/s41598-022-05966-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11276-022-02902-4,Deep learning to mobile hypermedia and multimedia,Wireless Networks,10.1007/s11276-022-02902-4,Springer,2022-02-02,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11276-022-02902-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00530-021-00880-9,Position constrained network for 3D human pose estimation,Multimedia Systems,10.1007/s00530-021-00880-9,Springer,2022-02-02,"Human pose estimation is a challenging research task in the field of computer vision. The current mainstream works have made great progress in pose estimation, but these works still have weakness in two aspects: first, the feature extraction module is not competent for representation learning; second, the training process does not take fully advantage of the projection model from 3D space to 2D plane. In this work, we propose a human pose estimation framework which exploits 3D root coordinates as subordinate input to 2D joint coordinates to provide positional reference to the recovered 3D joint coordinates, and employs inner camera parameters to construct additional projection constraints for recovering 3D joint coordinates. Moreover, we enhance the feature learning through residual branch. We tested our method on two benchmark datasets for human pose estimation, and the experimental results show that the proposed model is superior to current mainstream algorithms.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-021-00880-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00464-021-08381-6,Deep learning-based automatic surgical step recognition in intraoperative videos for transanal total mesorectal excision,Surgical Endoscopy,10.1007/s00464-021-08381-6,Springer,2022-02-01,"Background Dividing a surgical procedure into a sequence of identifiable and meaningful steps facilitates intraoperative video data acquisition and storage. These efforts are especially valuable for technically challenging procedures that require intraoperative video analysis, such as transanal total mesorectal excision (TaTME); however, manual video indexing is time-consuming. Thus, in this study, we constructed an annotated video dataset for TaTME with surgical step information and evaluated the performance of a deep learning model in recognizing the surgical steps in TaTME. Methods This was a single-institutional retrospective feasibility study. All TaTME intraoperative videos were divided into frames. Each frame was manually annotated as one of the following major steps: (1) purse-string closure; (2) full thickness transection of the rectal wall; (3) down-to-up dissection; (4) dissection after rendezvous; and (5) purse-string suture for stapled anastomosis. Steps 3 and 4 were each further classified into four sub-steps, specifically, for dissection of the anterior, posterior, right, and left planes. A convolutional neural network-based deep learning model, Xception, was utilized for the surgical step classification task. Results Our dataset containing 50 TaTME videos was randomly divided into two subsets for training and testing with 40 and 10 videos, respectively. The overall accuracy obtained for all classification steps was 93.2%. By contrast, when sub-step classification was included in the performance analysis, a mean accuracy (± standard deviation) of 78% (± 5%), with a maximum accuracy of 85%, was obtained. Conclusions To the best of our knowledge, this is the first study based on automatic surgical step classification for TaTME. Our deep learning model self-learned and recognized the classification steps in TaTME videos with high accuracy after training. Thus, our model can be applied to a system for intraoperative guidance or for postoperative video indexing and analysis in TaTME procedures.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00464-021-08381-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10586-021-03391-4,Towards the availability of video communication in artificial intelligence-based computer vision systems utilizing a multi-objective function,Cluster Computing,10.1007/s10586-021-03391-4,Springer,2022-02-01,"Availability is one of the three main goals of information security. This paper contributes to systems’ availability by introducing an optimization model for the adaptation (controlling the capturing, coding, and sending features of the video communication system) of live broadcasting of video to limited and varied network bandwidth and/or limited power sources such as wireless and mobile network cases. We first, analyzed the bitrate-accuracy and bitrate-power characteristics of various video transmission techniques for adapting video communication in Artificial Intelligence-based Systems. To optimize resources for live video streaming, we analyze various video parameter settings for adapting the stream to available resources. We consider the object detection accuracy, the bandwidth, and power consumption requirement. The results showed that setting SNR and spatial video encoding features (with upscaling the frames at the destination) are the best techniques that maximizing the object detection accuracy while minimizing the bandwidth and the consumed energy requirements. In addition, we analyze the effectiveness of combining SNR and spatial video encoding features with upscaling and find that we can increase the performance of the streaming system by combining these two techniques. We presented a multi-objective function for determining the parameter or parameters’ pairing that provides the optimal object detection’s accuracy, power consumption, and bit rate. Results are reported based on more than 15,000 experiments utilizing standard datasets for short video segments and a collected dataset of 300 videos from YouTube. We evaluated results based on the detection index, false-positive index, power consumption, and bandwidth requirements metrics. For a single adaptive parameter, the analysis of the experiment’s outcome demonstrate that the multi-objective function achieves object detection accuracy as high as the best while drastically reducing bandwidth requirements and energy consumption. For multiple adaptive parameters, the analysis of the experiment’s outcome demonstrate the significant benefits of effective pairings (pairs) of adaptive parameters. For example, by combining the signal-to-noise ratio (SNR) with the spatial feature in H.264, a certain optimal parameter setting can be reached where the power consumption can be reduced to $$20\%$$ 20 % , and the bandwidth requirements to $$2\%$$ 2 % from the original, while keeping the Object Detection Accuracy ( ODA ) within 10% less of the highest ODA .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10586-021-03391-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06551-0,Video surveillance image enhancement via a convolutional neural network and stacked denoising autoencoder,Neural Computing and Applications,10.1007/s00521-021-06551-0,Springer,2022-02-01,"In an extensive-scale surveillance system, the quality of the surveillance camera installed varies. This variation of surveillance camera produces different image quality in terms of resolution, illumination, and noise. The quality of the captured image depends on the surveillance camera hardware, placement and orientation, and the surrounding light. A pixelated, low illumination and noisy image produced by a low-quality surveillance camera causes critical issues for video surveillance face recognition systems. To address these issues, a deep learning image enhancement (DLIE) model is proposed. By utilizing a deep learning architecture such as a convolutional neural network (CNN) and a denoising autoencoder, the image quality can be enhanced. The DLIE model is able to improve image resolution and illumination and reduce noise in an image. There are two deep learning blocks (DLB) in the DLIE model, which are DLB1 and DLB2. Both DLBs are arranged in parallel so that all the stated problems can be addressed simultaneously. DLB1 is proposed to address the occurrence of pixelated images by reconstructing a low-resolution image into a high-resolution image using a CNN. DLB2 used the capability of a denoising autoencoder to reconstruct the corrupted image into a clean image by enhancing the dark and noisy images. The output of each DLB is fused using image fusion to obtain the optimum image quality. The image is evaluated using the peak to signal noise ratio (PSNR) and structural similarity index (SSIM). The enhanced image from the DLIE model exhibits superior quality compared to the original image ranging from 13.3625 to 22.7728 for PSNR and 0.6207 to 0.8155 for SSIM.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06551-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00530-021-00838-x,FPPN: fast pixel purification network for single-image super-resolution,Multimedia Systems,10.1007/s00530-021-00838-x,Springer,2022-02-01,"With the development of deep learning, convolution neural networks (CNNs) have drawn increasing attention in the field of single-image super-resolution (SISR). The previous methods have achieved excellent performance. However, in order to boost the performance, researchers often stack the basic blocks and deepen the networks, which leads to training difficulties, memory consumption, slow running, and other problems. The main goal of our work is to design a lightweight convolutional neural network to make a trade-off between performance and network capacity, thus relieving the aforementioned problems. On the one hand, we elaborately design a lightweight fast pixel purification block (FPPB), which is used to extract deep pixel features through multi-step pixel attention. On the other hand, our network adopts the structure of two path. One path takes LR images as input while the other takes gradient images as input. Because gradient images, which are rich in high-frequency information, can promote the recovery and reconstruction of image edges without the introduction of additional parameters. Experiments demonstrate that our method achieves superior SR performance while maintaining a small number of parameters and the computations.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-021-00838-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11722-1,"A study on video semantics; overview, challenges, and applications",Multimedia Tools and Applications,10.1007/s11042-021-11722-1,Springer,2022-02-01,"Due to the increase in surveillance systems, there is a massive increase in surveillance data. As of now, the key challenge for video surveillance systems is analyzing these large video clips. It, therefore, has an enormous demand for intelligent video analysis systems capable of identifying activities and events. Since many researchers have emphasized the role of contextual knowledge and how the performance of video content analysis has improved in several ways, we have looked at different approaches in this study that can extract semantic information to human-level perception in the video. We also addressed open problems in semantics that come from event detection and irregular activity detection. Most methods/models are too coarse to accurately extract a complete set of information. Thus, we need to use a machine-readable format to view, process, store and extract meaningful information from the video data. In this paper, we discussed the methods/approaches for extracting low-level features, mid-level features, and high-level video features and their representation using Semantic Technologies. A taxonomy of hierarchical feature generation approaches is also provided. Some evaluation metrics for evaluating video activity and measuring the performance of the extraction features are explored. Community-approved benchmark datasets are also thoroughly surveyed and presented. The paper provides a complete framework of video research to develop an intelligent surveillance system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11722-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13369-021-06167-5,Isolated Video-Based Arabic Sign Language Recognition Using Convolutional and Recursive Neural Networks,Arabian Journal for Science and Engineering,10.1007/s13369-021-06167-5,Springer,2022-02-01,"For natural and meaningful communication between the deaf community and the hearing population, sign language is very important. Most of the Arab sign recognition studies have focused on the identification of the sign action based on the descriptor of the feature. However, the limitation of this traditional method is the need to choose which features are important in each particular sequence. To address this issue, we propose a novel approach based on a deep learning architecture to classify video sequences of Arabic sign language, especially Moroccan sign language. Two methods of classification are applied, namely 2D convolutional recurring neural network (2DCRNN) and 3D convolutional neural network (3DCNN). Concerning the first method, a 2DCRNN model is used to extract features with a recurring network pattern to detect the relationship between frames. The second method uses a 3DCNN model learning the spatiotemporal features out of small patches. After 2DCRNN and the 3DCNN models extracted feature, the video data are classified into various classes, using a fully connected network. The proposed approach is trained over a collection of 224 videos of five individuals performing 56 different signs. The results achieved through the fourfold cross-validation technique demonstrate the performance of the proposed approach in terms of recall, F1 score, and AUROC, with the level accuracy of 92% for 2DCRNN and 99% for 3DCNN.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13369-021-06167-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11265-021-01683-x,Compute and Memory Efficient Universal Sound Source Separation,Journal of Signal Processing Systems,10.1007/s11265-021-01683-x,Springer,2022-02-01,"Recent progress in audio source separation led by deep learning has enabled many neural network models to provide robust solutions to this fundamental estimation problem. In this study, we provide a family of efficient neural network architectures for general purpose audio source separation while focusing on multiple computational aspects that hinder the application of neural networks in real-world scenarios. The backbone structure of this convolutional network is the SUccessive DOwnsampling and Resampling of Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is performed through simple one-dimensional convolutions. This mechanism enables our models to obtain high fidelity signal separation in a wide variety of settings where a variable number of sources are present and with limited computational resources (e.g. floating point operations, memory footprint, number of parameters and latency). Our experiments show that SuDoRM-RF models perform comparably and even surpass several state-of-the-art benchmarks with significantly higher computational resource requirements. The causal variation of SuDoRM-RF is able to obtain competitive performance in real-time speech separation of around 10dB scale-invariant signal-to-distortion ratio improvement (SI-SDRi) while remaining up to 20 times faster than real-time on a laptop device.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11265-021-01683-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-021-01970-x,A high-performance approach to detecting small targets in long-range low-quality infrared videos,"Signal, Image and Video Processing",10.1007/s11760-021-01970-x,Springer,2022-02-01,"Since targets are small in long-range infrared (IR) videos, it is challenging to accurately detect targets in those videos. In this paper, we propose a high-performance approach to detecting small targets in long-range and low-quality infrared videos. Our approach consists of a video resolution enhancement module, a proven small target detector based on local intensity and gradient (LIG), a connected component (CC) analysis module, and a track association module known as Simple Online and Real-time Tracking (SORT) to connect detections from multiple frames. Extensive experiments using actual mid-wave infrared (MWIR) videos in range between 3500 and 5000 m from a benchmark dataset clearly demonstrated the efficacy of the proposed approach. In the 5000 m case, the F1 score has been improved from 0.936 without SORT to 0.977 with SORT.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-021-01970-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11612-6,A nonlinear prediction model for Chinese speech signal based on RBF neural network,Multimedia Tools and Applications,10.1007/s11042-021-11612-6,Springer,2022-02-01,"A novel method for Chinese speech time series prediction model is proposed. In order to reconstruct the phase space of Chinese speech signal, the delay time and embedding dimension are calculated by C–C method and false nearest neighbor algorithm. The maximum lyapunov exponent and correlation dimension of Chinese speech phoneme are calculated by wolf algorithm and genetic programming algorithm. The numerical results show that there exists nonlinear characteristics in Chinese speech signal. Based on the analysis method of RBF neural network and the nonlinear characteristic parameters such as the delay time and embedding dimension, a nonlinear prediction model is designed. In order to further verify the prediction performance of the designed prediction model, waveform comparison and four evaluation indexes are used. It is shown that compared with the linear prediction model and back propagation neural network nonlinear prediction model, prediction error of the RBF neural network nonlinear prediction model is significantly reduced, and the model has higher prediction accuracy and prediction performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11612-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11650-0,Machine learning model for mapping of music mood and human emotion based on physiological signals,Multimedia Tools and Applications,10.1007/s11042-021-11650-0,Springer,2022-02-01,"Emotion is considered a physiological state that appears whenever a transformation is observed by an individual in their environment or body. While studying the literature, it has been observed that combining the electrical activity of the brain, along with other physiological signals for the accurate analysis of human emotions is yet to be explored in greater depth. On the basis of physiological signals, this work has proposed a model using machine learning approaches for the calibration of music mood and human emotion. The proposed model consists of three phases (a) prediction of the mood of the song based on audio signals, (b) prediction of the emotion of the human-based on physiological signals using EEG, GSR, ECG, Pulse Detector, and finally, (c) the mapping has been done between the music mood and the human emotion and classifies them in real-time. Extensive experimentations have been conducted on the different music mood datasets and human emotion for influential feature extraction, training, testing and performance evaluation. An effort has been made to observe and measure the human emotions up to a certain degree of accuracy and efficiency by recording a person’s bio- signals in response to music. Further, to test the applicability of the proposed work, playlists are generated based on the user’s real-time emotion determined using features generated from different physiological sensors and mood depicted by musical excerpts. This work could prove to be helpful for improving mental and physical health by scientifically analyzing the physiological signals.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11650-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-021-01987-2,No-reference stereoscopic image quality assessment using 3D visual saliency maps fused with three-channel convolutional neural network,"Signal, Image and Video Processing",10.1007/s11760-021-01987-2,Springer,2022-02-01,"In this paper, we present a depth-perceived 3D visual saliency map and propose a no-reference stereoscopic image quality assessment (NR SIQA) algorithm using 3D visual saliency maps and convolutional neural network (CNN). Firstly, the 2D salient region of stereoscopic image is generated, and the depth saliency map is calculated, and then, they are combined to compute 3D visual saliency map by linear weighted method, which can better use depth and disparity information of 3D image. Finally, 3D visual saliency map, together with distorted stereoscopic pairs, is fed into a three-channel CNN to learn human subjective perception. We call proposed depth perception and CNN-based SIQA method DPCNN. The performances of DPCNN are evaluated over the popular LIVE 3D Phase I and LIVE 3D Phase II databases, which demonstrates to be competitive with the state-of-the-art NR SIQA algorithms.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-021-01987-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41467-022-28153-7,Automatic mapping of multiplexed social receptive fields by deep learning and GPU-accelerated 3D videography,Nature Communications,10.1038/s41467-022-28153-7,Nature,2022-02-01,"Social interactions powerfully impact the brain and the body, but high-resolution descriptions of these important physical interactions and their neural correlates are lacking. Currently, most studies rely on labor-intensive methods such as manual annotation. Scalable and objective tracking methods are required to understand the neural circuits underlying social behavior. Here we describe a hardware/software system and analysis pipeline that combines 3D videography, deep learning, physical modeling, and GPU-accelerated robust optimization, with automatic analysis of neuronal receptive fields recorded in interacting mice. Our system (“3DDD Social Mouse Tracker”) is capable of fully automatic multi-animal tracking with minimal errors (including in complete darkness) during complex, spontaneous social encounters, together with simultaneous electrophysiological recordings. We capture posture dynamics of multiple unmarked mice with high spatiotemporal precision (~2 mm, 60 frames/s). A statistical model that relates 3D behavior and neural activity reveals multiplexed ‘social receptive fields’ of neurons in barrel cortex. Our approach could be broadly useful for neurobehavioral studies of multiple animals interacting in complex low-light environments. High resolution descriptions of social interactions and their neural correlates are lacking. Here the authors report a pipeline enabling fully automatic multi-animal tracking during social encounters, together with simultaneous electrophysiological recordings, and show this works in low-light settings.",https://www.nature.com/articles/s41467-022-28153-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s41870-021-00850-4,A low resource 3D U-Net based deep learning model for medical image analysis,International Journal of Information Technology,10.1007/s41870-021-00850-4,Springer,2022-02-01,"The success of deep learning, a subfield of Artificial Intelligence technologies in the field of image analysis and computer can be leveraged for building better decision support systems for clinical radiological settings. Detecting and segmenting tumorous tissues in brain region using deep learning and artificial intelligence is one such scenario, where radiologists can benefit from the computer based second opinion or decision support, for detecting the severity of disease, and survival of the subject with an accurate and timely clinical diagnosis. Gliomas are the aggressive form of brain tumors having irregular shape and ambiguous boundaries, making them one of the hardest tumors to detect, and often require a combined analysis of different types of radiological scans to make an accurate detection. In this paper, we present a fully automatic deep learning method for brain tumor segmentation in multimodal multi-contrast magnetic resonance image scans. The proposed approach is based on light weight UNET architecture, consisting of a multimodal CNN encoder-decoder based computational model. Using the publicly available Brain Tumor Segmentation (BraTS) Challenge 2018 dataset, available from the Medical Image Computing and Computer Assisted Intervention (MICCAI) society, our novel approach based on proposed light-weight UNet model, with no data augmentation requirements and without use of heavy computational resources, has resulted in an improved performance, as compared to the previous models in the challenge task that used heavy computational architectures and resources and with different data augmentation approaches. This makes the model proposed in this work more suitable for remote, extreme and low resource health care settings.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41870-021-00850-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12650-021-00783-x,DanceVis: toward better understanding of online cheer and dance training,Journal of Visualization,10.1007/s12650-021-00783-x,Springer,2022-02-01,"Abstract In online cheer and dance education, teacher needs to evaluate students’ performance manually based on their uploaded training videos. However, conventional training evaluation models suffer from the problems of strong subjection, low efficiency, low accuracy, and fuzzy training paths. To address these problems, we closely collaborate with domain experts and characterize requirements to design a comprehensive visualization system DanceVis, which has the characteristics of objective evaluation, fine-grained analysis, high efficiency, high accuracy and clear training paths, so as to track the dynamic changes of groups and individuals from coarse-to-fine granularity, the global-to-local dimension, and the time dimension. In terms of dimensional analysis, we divide the overall cheerleading dance performance of one student into nine dimensions, and these scores are calculated to a visual quantitative score that can replace the expert score. Simultaneously, we track the individual performance change through the dimension scores of in-class and after-class. In terms of group analysis, a nonlinear dimensionality reduction and clustering method is proposed to classify trainees and further build group portraits which help propose training paths for each group. In terms of individual analysis, we use human pose estimation method to automatically analyze videos, which improves the analysis efficiency, and obtains individual global performance curves. We invite experts to conduct with DanceVis, and demonstrate the usability of the system through expert interviews. The results show that DanceVis can fully make up for the shortcomings of existing training evaluation models, and greatly improve the efficiency and accuracy of online cheer and dance training. Graphical abstract ",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12650-021-00783-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00530-021-00816-3,Attention based video captioning framework for Hindi,Multimedia Systems,10.1007/s00530-021-00816-3,Springer,2022-02-01,"In recent times, active research is going on for bridging the gap between computer vision and natural language. In this paper, we attempt to address the problem of Hindi video captioning. In a linguistically diverse country like India, it is important to provide a means which can help in understanding the visual entities in native languages. In this work, we employ a hybrid attention mechanism by extending the soft temporal attention mechanism with a semantic attention to make the system able to decide when to focus on visual context vector and semantic input. The visual context vector of the input video is extracted using 3D convolutional neural network (3D CNN) and a Long Short-Term Memory (LSTM) recurrent network with attention module is used for decoding the encoded context vector. We experimented on a dataset built in-house for Hindi video captioning by translating $$MSR-VTT$$ M S R - V T T dataset followed by post-editing. Our system achieves 0.369 CIDEr score and 0.393 METEOR score and outperformed other baseline models including RMN (Reasoning Module Networks)-based model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-021-00816-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00034-021-01820-8,Detecting Anomalies in Videos using Perception Generative Adversarial Network,"Circuits, Systems, and Signal Processing",10.1007/s00034-021-01820-8,Springer,2022-02-01,"This paper presents a novel end-to-end unsupervised deep learning approach for video anomaly detection. We propose to utilize the Perception Generative Adversarial Net (Perception-GAN), which is trained using the initial portion of the video. The generator of the perceptual-GAN learns how to generate events similar to the normal events, while the discriminator of the perceptual-GAN learns how to distinguish the abnormal events from the normal events. At testing time, only the discriminator is used to solve our discriminative task (abnormality detection). Through combining the generative adversarial loss and the proposed perceptual adversarial loss, these two networks can be trained alternately to solve the anomaly detection task. A two-stream networks framework and an update strategy is employed to improve the detection result. We test our approach on three popular benchmarks and the experimental results verify the superiority of our method compared to the state of the arts.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00034-021-01820-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11801-3,Monocular 3D object detection via estimation of paired keypoints for autonomous driving,Multimedia Tools and Applications,10.1007/s11042-021-11801-3,Springer,2022-02-01,"3D objection detection is a key task in autonomous driving. Because 3D structure information is lost during perspective projection, 3D localization of an object from monocular images is challenging. We herein present a monocular 3D object detection method that formulates the 3D object localization as a paired keypoints regression problem. Our method exploits 2D bounding box priors to predict the projection of paired 3D keypoints on the image plane for each object, and the object localization is recovered via an inverse projection. A fast keypoint regression network is proposed to predict the projection of keypoints and to generate the initial 3D bounding box. Furthermore, to obtain more accurate 3D detection results, we leverage a light-weight cascaded refinement module to rectify the initial 3D box, which takes the instance point cloud converted from the monocular depth prediction as input. Experiments on the KITTI dataset demonstrate that our method exhibits state-of-the-art performance solely via monocular images. Our method achieves 15.97, 10.42, and 7.91 3D AP on the three difficulty levels on the KITTI test set, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11801-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06027-1,Recognition and extraction of high-resolution satellite remote sensing image buildings based on deep learning,Neural Computing and Applications,10.1007/s00521-021-06027-1,Springer,2022-02-01,"Extracting and recognizing buildings from high-resolution remote sensing images faces many problems due to the complexity of the buildings on the surface. The purpose is to improve the recognition and extraction capabilities of remote sensing satellite images. The Gao Fen-2 (GF-2) high-resolution remote sensing satellite is taken as the research object. The deep convolutional neural network (CNN) serves as the core of image feature extraction, and PCA (principal component analysis) is adopted to reduce the dimensionality of the data. A correction neural network model, that is, boundary regulated network (BR-Net) is proposed. The features of remote sensing images are extracted through convolution, pooling, and classification. Different data collection models are utilized for comparative analysis to verify the performance of the proposed model. Results demonstrate that when using CNN to recognize remote sensing images, the recognition accuracy is much higher than that of traditional image recognition models, which can reach 95.3%. Compared with the newly researched models, the performance is improved by 15%, and the recognition speed is increased by 20%. When extracting buildings with higher accuracy, the proposed model can also ensure clear boundaries, thereby obtaining a complete building image. Therefore, using deep learning technology to identify and extract buildings from high-resolution satellite remote sensing images is of great significance for advancing the deep learning applications in image recognition.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06027-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10845-021-01856-2,Transformation of a rolling mill aggregate to a cyber physical production system: from sensor retrofitting to machine learning,Journal of Intelligent Manufacturing,10.1007/s10845-021-01856-2,Springer,2022-02-01,"This paper describes the transformation of a rolling mill aggregate from a stand-alone solution to a fully integrated cyber physical production system. Within this process, already existing load cells were substituted and additional inductive and magnetic displacement sensors were applied. After calibration, those were fully integrated into a six-layer digitalization architecture at the Smart Forming Lab at the Chair of Metal Forming (Montanuniversitaet Leoben). Within this framework, two front end human machine interfaces were designed, where the first one serves as a condition monitoring system during the rolling process. The second user interface visualizes the result of a resilient machine learning algorithm, which was designed using Python and is not just able to predict and adapt the resulting rolling schedule of a defined metal sheet, but also to learn from additional rolling mill schedules carried out. This algorithm was created on the basis of a black box approach, using data from more than 1900 milling steps with varying roll gap height, sheet width and friction conditions. As a result, the developed program is able to interpolate and extrapolate between these parameters as well as different initial sheet thicknesses, serving as a digital twin for data-based recommendations on schedule changes between different rolling process steps. Furthermore, via the second user interface, it is possible to visualize the influence of this parameters on the result of the milling process. As the whole layer system runs on an internal server at the university, students and other interested parties are able to access the visualization and can therefore use the environment to deepen their knowledge within the characteristics and influence of the sheet metal rolling process as well as data science and especially fundamentals of machine learning. This algorithm also serves as a basis for further integration of materials science based data for the prediction of the influence of different materials on the rolling result. To do so, the rolled specimens were also analyzed regarding the influence of the plastic strain path on their mechanical properties, including anisotropy and materials’ strength.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10845-021-01856-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-022-03732-0,CR19: a framework for preliminary detection of COVID-19 in cough audio signals using machine learning algorithms for automated medical diagnosis applications,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-022-03732-0,Springer,2022-02-01,"Today, there is a level of panic and chaos dominating the entire world due to the massive outbreak in the second wave of COVID-19 disease. As the disease has numerous symptoms ranging from a simple fever to the inability to breathe, which may lead to death. One of these symptoms is a cough which is considered one of the most common symptoms for COVID-19 disease. Recent research shows that the cough of a COVID-19 patient has distinct features that are different from other diseases. Consequently, the cough sound can be detected and classified to be used as a preliminary diagnosis of the COVID-19, which will help in reducing the spreading of that disease. The artificial intelligence (AI) engine can diagnose COVID-19 diseases by executing differential analysis of its inherent characteristics and comparing it to other non-COVID-19 coughs. However, the diagnosis of a COVID-19 infection by cough alone is an extremely challenging multidisciplinary problem. Therefore, this paper proposes a hybrid framework for efficiently COVID-19 detection and diagnosis using various ML algorithms from cough audio signals. The accuracy of this framework is improved with the utilization of the genetic algorithm with the ML techniques. We also assess the proposed system called CR19 for diagnosis on metrics such as precision, recall, F-measure. The results proved that the hybrid (GA-ML) technique provides superior results based on different evaluation metrics compared with ML approaches such as LR, LDA, KNN, CART, NB, and SVM. The proposed framework achieve an accuracy equal to 92.19%, 94.32%, 97.87%, 92.19%, 91.48%, and 93.61% in compared with the ML are 90.78, 92.90, 95.74, 87.94, 81.56, and 92.198 for LR, LDA, KNN, CART, NB, and SVM respectively. The proposed framework will efficiently help the physicians provide a proper medical decision regarding the COVID-19 analysis, thereby saving more lives. Therefore, this CR19 framework can be a clinical decision assistance tool used to channel clinical testing and treatment to those who need it the most, thereby saving more lives.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-022-03732-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-021-02940-4,Three-stream spatio-temporal attention network for first-person action and interaction recognition,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-021-02940-4,Springer,2022-02-01,"The problem of action and interaction recognition of human activities from the perspective of first-person view-point is an interesting area of research in the field of human action recognition (HAR). This paper presents a data-driven spatio-temporal network to combine different modalities computed from first-person videos using a temporal attention mechanism. First, our proposed approach uses three-stream inflated 3D ConvNet (I3D) to extract low-level features from RGB frame difference (FD), optical flow (OF) and magnitude-orientation (MO) streams. An I3D network has the advantage to directly learn spatio-temporal features over short video snippets (like 16 frames). Second, the extracted features are fused together and fed to a Bidirectional long short-term memory (BiLSTM) network to model high-level temporal feature sequences. Third, we propose to incorporate attention mechanism with our BiLSTM network to automatically select the most relevant temporal snippets in the given video sequence. Finally, we conducted extensive experiments and achieve state-of-the-art results on JPL (98.5%), NUS (84.1%), UTK (91.5%) and DogCentric (83.3%) datasets. These results show that features extracted from three-stream network are complementary to each other, and attention mechanism further improves the results by a large margin than previous attempts based on handcrafted and deep features.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-021-02940-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11761-8,Pain fingerprinting using multimodal sensing: pilot study,Multimedia Tools and Applications,10.1007/s11042-021-11761-8,Springer,2022-02-01,"Pain is a complex phenomenon, the experience of which varies widely across individuals. At worst, chronic pain can lead to anxiety and depression. Cost-effective strategies are urgently needed to improve the treatment of pain, and thus we propose a novel home-based pain measurement system for the longitudinal monitoring of pain experience and variation in different patients with chronic low back pain. The autonomous nervous system and audio-visual features are analyzed from heart rate signals, voice characteristics and facial expressions using a unique measurement protocol. Self-reporting is utilized for the follow-up of changes in pain intensity, induced by well-designed physical maneuvers, and for studying the consecutive trends in pain. We describe the study protocol, including hospital measurements and questionnaires and the implementation of the home measurement devices. We also present different methods for analyzing the multimodal data: electroencephalography, audio, video and heart rate. Our intention is to provide new insights using technical methodologies that will be beneficial in the future not only for patients with low back pain but also patients suffering from any chronic pain.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11761-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13239-021-00552-9,Personalized Pre- and Post-Operative Hemodynamic Assessment of Aortic Coarctation from 3D Rotational Angiography,Cardiovascular Engineering and Technology,10.1007/s13239-021-00552-9,Springer,2022-02-01,"Purpose Coarctation of Aorta (CoA) is a congenital disease consisting of a narrowing that obstructs the systemic blood flow. This proof-of-concept study aimed to develop a framework for automatically and robustly personalizing aortic hemodynamic computations for the assessment of pre- and post-intervention CoA patients from 3D rotational angiography (3DRA) data. Methods We propose a framework that combines hemodynamic modelling and machine learning (ML) based techniques, and rely on 3DRA data for non-invasive pressure computation in CoA patients. The key features of our framework are a parameter estimation method for calibrating inlet and outlet boundary conditions, and regional mechanical wall properties, to ensure that the computational results match the patient-specific measurements, and an improved ML based pressure drop model capable of predicting the instantaneous pressure drop for a wide range of flow conditions and anatomical CoA variations. Results We evaluated the framework by investigating 6 patient datasets, under pre- and post-operative setting, and, since all calibration procedures converged successfully, the proposed approach is deemed robust. We compared the peak-to-peak and the cycle-averaged pressure drop computed using the reduced-order hemodynamic model with the catheter based measurements, before and after virtual and actual stenting. The mean absolute error for the peak-to-peak pressure drop, which is the most relevant measure for clinical decision making, was 2.98 mmHg for the pre- and 2.11 mmHg for the post-operative setting. Moreover, the proposed method is computationally efficient: the average execution time was of only $$2.1 \pm 0.8$$ 2.1 ± 0.8 minutes on a standard hardware configuration. Conclusion The use of 3DRA for hemodynamic modelling could allow for a complete hemodynamic assessment, as well as virtual interventions or surgeries and predictive modeling. However, before such an approach can be used routinely, significant advancements are required for automating the workflow.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13239-021-00552-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s41204-021-00185-2,Research hotspots of current interest and trend of artificial intelligence in intelligent speech processing and social sciences – visualized comparison research based on CiteSpace,Nanotechnology for Environmental Engineering,10.1007/s41204-021-00185-2,Springer,2022-02-01,"With the in-depth development of artificial intelligence (AI), more and more research is conducted on it. In this paper, the bibliometric software CiteSpace is used to analyze the status quo and topic of current interest of the research on AI in the field of intelligent speech processing and social sciences. With AI as the theme, the SSCI database of the core database of Web of Science and the Chinese CSSCI database from 2010 to 2020 are searched. Next, the status quo of research on AI in the field of intelligent speech processing and social sciences is explored through author co-occurrence analysis, institution co-occurrence analysis and journal co-citation analysis, and the topic of current interest of the research on AI is explored through keyword co-occurrence network and cluster analysis. Besides, the possible future research directions are discussed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41204-021-00185-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11781-4,Crowd abnormality detection in video sequences using supervised convolutional neural network,Multimedia Tools and Applications,10.1007/s11042-021-11781-4,Springer,2022-02-01,"In this paper, a Convolutional Neural Network (CNN) based crowd abnormality detection model in video sequences is proposed. The model has two convolution layers, two Fully Connected (FC) layers in which 1st FC layer uses Rectified Linear Unit (ReLU) and the 2nd uses sigmoid function as activation functions. Both convolution layers consist of a convolution operator followed by ReLU and the max-pooling layer. Intermediate convolutional layers produce features that are used to detect the abnormality in the video frame. The performance of the proposed model has been evaluated based on three parameters – Receiver Operator Characteristic (ROC) curve, Area Under the Curve (AUC), and Equal Error Rate (EER). Three scientific datasets have been used, consisting of several video sequences with various normal and abnormal activities. Experimental results show that the proposed CNN model performs better for all datasets compared with other similar methods in literature and achieves a maximum of near 100% sensitivity through the ROC curve for one dataset. Further, the average AUC value for the UCSD dataset (Ped1 and Ped2) is close to 98% and 95% for the Avenue dataset. The average EER for the UCSD dataset (Ped1 and Ped2) is near 10% and 11.5% for the Avenue dataset. Moreover, the model has also been evaluated for a couple of random YouTube videos of abnormal behavior, and it gives satisfactory results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11781-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11554-021-01161-4,Efficient binary 3D convolutional neural network and hardware accelerator,Journal of Real-Time Image Processing,10.1007/s11554-021-01161-4,Springer,2022-02-01,"The three-dimensional convolutional neural networks have abundant parameters and computational costs. It is urgent to compress the three-dimensional convolutional neural network. In this paper, an efficient and simple binary three-dimensional convolutional neural network architecture is proposed, in which the weight and activation are constrained to 0 or 1 instead of the common + 1 or – 1. Binary weight and activation are first applied to the three-dimensional convolutional neural networks. The proposed binary three-dimensional convolutional neural network has less computational complexity and memory consumption than standard convolution, and it is more appropriate for digital hardware design. Furthermore, an optimized convolution operation is proposed, in which case one input pixel is only required to be read once. A distributed storage approach is proposed to support the proposed convolution operation. With the proposed methods, a hardware accelerator for the binary three-dimensional convolutional neural network on the field programmable gate array platform is designed. The experimental results show that the presented accelerator is excellent in terms of computational resources and power efficiency. By jointly optimizing the algorithm and hardware, the accelerator achieves 89.2% accuracy and 384 frames per second on the KTH dataset.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-021-01161-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-10553-4,Speech emotion recognition based on multi‐feature and multi‐lingual fusion,Multimedia Tools and Applications,10.1007/s11042-021-10553-4,Springer,2022-02-01,"A speech emotion recognition algorithm based on multi-feature and Multi-lingual fusion is proposed in order to resolve low recognition accuracy caused bylack of large speech dataset and low robustness of acoustic features in the recognition of speech emotion. First, handcrafted and deep automatic features are extractedfrom existing data in Chinese and English speech emotions. Then, the various features are fused respectively. Finally, the fused features of different languages are fused again and trained in a classification model. Distinguishing the fused features with the unfused ones, the results manifest that the fused features significantly enhance the accuracy of speech emotion recognition algorithm. The proposedsolution is evaluated on the two Chinese corpus and two English corpus, and isshown to provide more accurate predictions compared to original solution. As a result of this study, the multi-feature and Multi-lingual fusion algorithm can significantly improve the speech emotion recognition accuracy when the dataset is small.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-10553-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11265-021-01727-2,Architecture of a Low Latency H.264/AVC Video Codec for Robust ML based Image Classification,Journal of Signal Processing Systems,10.1007/s11265-021-01727-2,Springer,2022-01-31,"The use of neural networks is considered as the state of the art in the field of image classification. A large number of different networks are available for this purpose, which, appropriately trained, permit a high level of classification accuracy. Typically, these networks are applied to uncompressed image data, since a corresponding training was also carried out using image data of similar high quality. However, if image data contains image errors, the classification accuracy deteriorates drastically. This applies in particular to coding artifacts which occur due to image and video compression. Typical application scenarios for video compression are narrowband transmission channels for which video coding is required but a subsequent classification is to be carried out on the receiver side. In this paper we present a special H.264/Advanced Video Codec (AVC) based video codec that allows certain regions of a picture to be coded with near constant picture quality in order to allow a reliable classification using neural networks, whereas the remaining image will be coded using constant bit rate. We have combined this feature with the ability to run with lowest latency properties, which is usually also required in remote control applications scenarios. The codec has been implemented as a fully hardwired High Definition video capable hardware architecture which is suitable for Field Programmable Gate Arrays.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11265-021-01727-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s13014-022-01985-9,Dosimetric impact of deep learning-based CT auto-segmentation on radiation therapy treatment planning for prostate cancer,Radiation Oncology,10.1186/s13014-022-01985-9,BioMed Central,2022-01-31,"Background The evaluation of automatic segmentation algorithms is commonly performed using geometric metrics. An analysis based on dosimetric parameters might be more relevant in clinical practice but is often lacking in the literature. The aim of this study was to investigate the impact of state-of-the-art 3D U-Net-generated organ delineations on dose optimization in radiation therapy (RT) for prostate cancer patients. Methods A database of 69 computed tomography images with prostate, bladder, and rectum delineations was used for single-label 3D U-Net training with dice similarity coefficient (DSC)-based loss. Volumetric modulated arc therapy (VMAT) plans have been generated for both manual and automatic segmentations with the same optimization settings. These were chosen to give consistent plans when applying perturbations to the manual segmentations. Contours were evaluated in terms of DSC, average and 95% Hausdorff distance (HD). Dose distributions were evaluated with the manual segmentation as reference using dose volume histogram (DVH) parameters and a 3%/3 mm gamma-criterion with 10% dose cut-off. A Pearson correlation coefficient between DSC and dosimetric metrics, i.e. gamma index and DVH parameters, has been calculated. Results 3D U-Net-based segmentation achieved a DSC of 0.87 (0.03) for prostate, 0.97 (0.01) for bladder and 0.89 (0.04) for rectum. The mean and 95% HD were below 1.6 (0.4) and below 5 (4) mm, respectively. The DVH parameters, V $$_{60/65/70\,{\mathrm{Gy}}}$$ 60 / 65 / 70 Gy for the bladder and V $$_{50/65/70\,{\mathrm{Gy}}}$$ 50 / 65 / 70 Gy for the rectum, showed agreement between dose distributions within $$\pm \, 5\%$$ ± 5 % and $$\pm \,2\%$$ ± 2 % , respectively. The D $$_{98/2\%}$$ 98 / 2 % and V $$_{95\%}$$ 95 % , for prostate and its 3 mm expansion (surrogate clinical target volume) showed agreement with the reference dose distribution within 2% and 3 Gy with the exception of one case. The average gamma pass-rate was 85%. The comparison between geometric and dosimetric metrics showed no strong statistically significant correlation. Conclusions The 3D U-Net developed for this work achieved state-of-the-art geometrical performance. Analysis based on clinically relevant DVH parameters of VMAT plans demonstrated neither excessive dose increase to OARs nor substantial under/over-dosage of the target in all but one case. Yet the gamma analysis indicated several cases with low pass rates. The study highlighted the importance of adding dosimetric analysis to the standard geometric evaluation.",https://www.biomedcentral.com/openurl?doi=10.1186/s13014-022-01985-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10846-021-01568-y,Double Critic Deep Reinforcement Learning for Mapless 3D Navigation of Unmanned Aerial Vehicles,Journal of Intelligent & Robotic Systems,10.1007/s10846-021-01568-y,Springer,2022-01-31,"This paper presents a novel deep reinforcement learning-based system for 3D mapless navigation for Unmanned Aerial Vehicles (UAVs). Instead of using an image-based sensing approach, we propose a simple learning system that uses only a few sparse range data from a distance sensor to train a learning agent. We based our approaches on two state-of-art double critics Deep-RL models: Twin Delayed Deep Deterministic Policy Gradient (TD3) and Soft Actor-Critic (SAC). We show that our two approaches manage to outperform an approach based on the Deep Deterministic Policy Gradient (DDPG) technique and the BUG2 algorithm. Also, our new Deep-RL structure based on Recurrent Neural Networks (RNNs) outperforms the current structure used to perform mapless navigation of mobile robots. Overall, we conclude that Deep-RL approaches based on double critic with Recurrent Neural Networks (RNNs) are better suited to perform mapless navigation and obstacle avoidance of UAVs.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10846-021-01568-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10479-021-04409-1,Automatic detection technology for sports players based on image recognition technology: the significance of big data technology in China’s sports field,Annals of Operations Research,10.1007/s10479-021-04409-1,Springer,2022-01-31,"In this research, players are investigated physically for the actual analysis to improve the recognition of motion effects based on image recognition technology. In this paper, the incorporation of the status of image recognition science has been tested on the athletes through image processing using artificial intelligence technology (IPAIT). Furthermore, the segmentation of gradient procedure has been validated using image segmentation techniques with big data assistance. In AIT, enhancing the traditional method of a grayscale image and obtaining the reconstructed image segmentation algorithm has been designed and developed. Furthermore, big data-assisted Gaussian background and IPAIT modeling are used to identify the target for the feature extraction of the human body and use morphological operators to deal with noise. The simulation findings demonstrate that the proposed IPAIT model enhances the recognition ratio of 98.8%, a performance ratio of 97.7%, and increases the accuracy ratio by 95.9% compared to other existing models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10479-021-04409-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10055-021-00616-0,Deep-learning-based real-time silent speech recognition using facial electromyogram recorded around eyes for hands-free interfacing in a virtual reality environment,Virtual Reality,10.1007/s10055-021-00616-0,Springer,2022-01-30,"Speech recognition technology is a promising hands-free interfacing modality for virtual reality (VR) applications. However, it has several drawbacks, such as limited usability in a noisy environment or a public place and limited accessibility to those who cannot generate loud and clear voices. These limitations may be overcome by employing a silent speech recognition (SSR) technology utilizing facial electromyograms (fEMGs) in a VR environment. In the conventional SSR systems, however, fEMG electrodes were attached around the user’s lips and neck, thereby creating new practical issues, such as the requirement of an additional wearable system besides the VR headset, necessity of a complex and time-consuming procedure for attaching the fEMG electrodes, and discomfort and limited facial muscle movements of the user. To solve these problems, we propose an SSR system using fEMGs measured by a few electrodes attached around the eyes of a user, which can also be easily incorporated into available VR headsets. To enhance the accuracy of classifying the fEMG signals recorded from limited recording locations relatively far from the phonatory organs, a deep neural network-based classification method was developed using similar fEMG data previously collected from other individuals and then transformed by dynamic positional warping. In the experiments, the proposed SSR system could classify six different fEMG patterns generated by six silently spoken words with an accuracy of 92.53%. To further demonstrate that our SSR system can be used as a hands-free control interface in practical VR applications, an online SSR system was implemented.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-021-00616-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11854-4,Deep learning-based automatic annotation and online classification of remote multimedia images,Multimedia Tools and Applications,10.1007/s11042-021-11854-4,Springer,2022-01-29,"In this paper, based on in-depth analysis of remote multimedia images, the automatic annotation and classification of graphics are tested and analyzed by algorithms of deep learning. To reduce the time of remote multimedia image labeling and online classification, and improve efficiency, we study the use of deep learning methods to automate annotation and online classification of remote multimedia images. An image is re-labeling algorithm based on modeling the correlation of hidden feature dimensions is proposed to improve the effect of hidden feature models by modeling the correlation between hid feature dimensions. The algorithm constructs the correlation between each pair of dimensions in the hidden features through the outer product operation to form a two-dimensional interactive graph. The information in the interaction graph is refined layer by layer by using the ability of the convolutional neural network to model local features, and finally, a representation of the correlation of all dimensions in the hidden features is formed to realize the re-labeling of social images. The experimental results show that this method can utilize the hidden feature information more effectively and improve the image re-labeling results. The light-weight feature extraction network significantly reduces the number of model parameters at the expense of a small amount of detection accuracy, and the FPN pyramid structure enhances the feature characterization ability of the feature extraction network. The performance is close to that of the Yolo algorithm.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11854-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-022-06940-z,Virtual reality and machine learning in the automatic photoparoxysmal response detection,Neural Computing and Applications,10.1007/s00521-022-06940-z,Springer,2022-01-29,"Photosensitivity, in relation to epilepsy, is a genetically determined condition in which patients have epileptic seizures of different severity provoked by visual stimuli. It can be diagnosed by detecting epileptiform discharges in their electroencephalogram (EEG), known as photoparoxysmal responses (PPR). The most accepted PPR detection method—a manual method—considered as the standard one, consists in submitting the subject to intermittent photic stimulation (IPS), i.e. a flashing light stimulation at increasing and decreasing flickering frequencies in a hospital room under controlled ambient conditions, while at the same time recording her/his brain response by means of EEG signals. This research focuses on introducing virtual reality (VR) in this context, adding, to the conventional infrastructure a more flexible one that can be programmed and that will allow developing a much wider and richer set of experiments in order to detect neurological illnesses, and to study subjects’ behaviours automatically. The loop includes the subject, the VR device, the EEG infrastructure and a computer to analyse and monitor the EEG signal and, in some cases, provide feedback to the VR. As will be shown, AI modelling will be needed in the automatic detection of PPR, but it would also be used in extending the functionality of this system with more advanced features. This system is currently in study with subjects at Burgos University Hospital, Spain.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-022-06940-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00530-021-00881-8,An object detection-based few-shot learning approach for multimedia quality assessment,Multimedia Systems,10.1007/s00530-021-00881-8,Springer,2022-01-29,"A large portion of the global population generates various multimedia data such as texts, images, videos, etc. One of the most common categories which influences the public at large is visual multimedia content. Due to the different social media platforms (e.g., Whatsapp, Twitter, Facebook, Instagram, and YouTube), these materials are passed without censorship and national boundaries. Multimedia data containing any violent or vulgar objects could trigger public unrest, and thus, it is a serious threat to the law and order of the land. Children and teenagers use social media like never before in previous generations and create lots of multimedia data. It is important to assess the quality of multimedia content without any bias and prejudices. Although the mainstream social media platforms use different filters and moderation using human experts, it is impossible to verify the terabytes of uploaded images and videos. Thus, it is inevitable to automate the content assessment phase without incurring an increase in upload time. This study aims to prevent uploading or to tag an image/video with a reasonable percentage of a gun as content. In this paper, object detection architectures such as Faster RCNN, EfficientDet, and YOLOv5 have been used to demonstrate how these techniques can efficiently detect human faces and different types of guns in given multimedia data (images/videos). The models are tested on various test images and video clips. A comparative analysis has also been discussed based on mean average precision and frames per second metric. The YOLOv5 provides the best-performing results as high as 80.39% and 35.22% at $$\text{mAP}_{0.5}$$ mAP 0.5 and $$\text{mAP}_{[0.50:0.95]}$$ mAP [ 0.50 : 0.95 ] , respectively. A face recognition task requires thousands of samples and the usual deep learning models are data-driven. On the contrary, a few-shot learning approach has been implemented to recognize the detected faces categorizing the content as real or reel.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-021-00881-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40430-022-03371-8,Review on key technologies of space intelligent grasping robot,Journal of the Brazilian Society of Mechanical Sciences and Engineering,10.1007/s40430-022-03371-8,Springer,2022-01-29,"Humankind’s yearning for universe has never stopped. Over the past decades, various spacecrafts have played an important role in space exploration. Combined with the development of AI, the space intelligent grasping robot, a new in-orbit service spacecraft, will bring new power to space exploration. This paper reviews the technologies related to configuration design, dynamic modeling and control algorithm. A four-stage task planning is proposed. Firstly, the means of the detection stage are put forward; secondly, the image recognition and visual tracking are described in the docking stage; thirdly, the cooperative control of robot in the capture stage is emphatically discussed, several important problems, such as coupling, path planning and motion control, multi-objective optimization, collision, are proposed. In order to solve these problems, the current research progress is summarized, and some new problem-solving technologies based on AI are analyzed in detail; lastly, the stability and control of the combination are briefly introduced.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40430-022-03371-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-022-03702-6,Real-world malicious event recognition in CCTV recording using Quasi-3D network,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-022-03702-6,Springer,2022-01-28,"Identification of exact malicious instant in lengthy CCTV recordings depends solely on Auto activity cognizance. The 3D CNN has previously been explored for the analysis of motion in video streams. Studies exhibit that, using separate filters for encoding spatial and temporal information has the same level of efficiency as that of 3D convolution filters. This study presents a novel approach through introduction of independent filters for event recognition in videos. This aims at learning extended Spatio-temporal features utilizing modified ResNet architecture. A novel 2D block termed as Quasi-3D (Q3D) decouples 3D information by combining 2D filters. The proposed Quasi-3D block encodes not only the spatial information in each frame but also the relative motion of objects along the x -axis and y -axis in a set of frames. Three variations of Quasi-3D block have been introduced to emphasize more on the features for further enhancing performance. A multi-class malicious activity recognition video dataset CrimesScene ( https://drive:google:com/file/d/1omiQG9sxx375HjL97DqXxIX9nnfW3oQ/view?usp=sharing ) inclusive of annotated video segments from 4 different classes of volume crimes has been developed. Results exhibit that the proposed Q3D ResNet model outperforms all other variants by achieving the overall detection accuracy of $$94.9\%$$ 94.9 % and $$93.07\%$$ 93.07 % on Hockey Fight and CrimesScene datasets, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-022-03702-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1140/epjs/s11734-022-00449-1,S-Box-based video stenography application of variable-order fractional hopfield neural network (VFHNN),The European Physical Journal Special Topics,10.1140/epjs/s11734-022-00449-1,Springer,2022-01-28,"With the increasing usage of the Internet nowadays, information security has become critical. Data hiding strategies are one approach used to ensure information security. Many different methods for data hiding are reported in the literature. In this study, an substitution-box (S-Box) based video stenography algorithm is suggested. In the design of the developed system, the variable-order fractional hopfield neural network (VFHNN) chaotic system is used and the analysis of the system is carried out. A random number generation algorithm is designed using the VFHNN system, and the randomness of the generated numbers is tested with NIST-800-22 tests. A new S-Box generation algorithm has been developed using the numbers obtained from the random number generator. Performance tests are applied to test the cryptological robustness of the produced S-Boxes. Then, the proposed algorithm is introduced to provide high-capacity and secure data hiding processes. Using four different S-Boxes, random determination of the pixels on the video file that will be data hiding is performed. To determine the security and performance of the proposed algorithm, data hiding processes are performed with different video files and analyses are carried out. The results obtained are compared with the studies in the literature, and it is shown that the proposed system achieved high-capacity and secure data hiding.",http://link.springer.com/openurl/fulltext?id=doi:10.1140/epjs/s11734-022-00449-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02366-1,LungSeek: 3D Selective Kernel residual network for pulmonary nodule diagnosis,The Visual Computer,10.1007/s00371-021-02366-1,Springer,2022-01-27,"Early detection and diagnosis of pulmonary nodules is the most promising way to improve the survival chances of lung cancer patients. This paper proposes an automatic pulmonary cancer diagnosis system, LungSeek. LungSeek is mainly divided into two modules: (1) Nodule detection, which detects all suspicious nodules from computed tomography (CT) scan; (2) Nodule Classification, classifies nodules as benign or malignant. Specifically, a 3D Selective Kernel residual network (SK-ResNet) based on the Selective Kernel Network and 3D residual network is located. A deep 3D region proposal network with SK-ResNet is designed for detection of pulmonary nodules while a multi-scale feature fusion network is designed for the nodule classification. Both networks use the SK-Net module to obtain different receptive field information, thereby effectively learning nodule features and improving diagnostic performance. Our method has been verified on the luna16 data set, reaching 89.06, 94.53% and 97.72% when the average number of false positives is 1, 2 and 4, respectively. Meanwhile, its performance is better than the state-of-the-art method and other similar networks and experienced doctors. This method has the ability to adaptively adjust the receptive field according to multiple scales of the input information, so as to better detect nodules of various sizes. The framework of LungSeek based on 3D SK-ResNet is proposed for nodule detection and nodule classification from chest CT. Our experimental results demonstrate the effectiveness of the proposed method in the diagnosis of pulmonary nodules.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02366-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-021-03640-9,Sentence2SignGesture: a hybrid neural machine translation network for sign language video generation,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-021-03640-9,Springer,2022-01-26,"The development of Neural Machine Translation (NMT) systems has attained prominent position in language translation tasks. However, it faces huge challenges in translating the new words and out-of-vocabularies. This problem is identified as a major drawback of conventional NMT systems in language translation results more copied outputs. In addition to that, it places the risks in understanding multilingual language structures and word relationships. In this paper, we propose novel deep stacked GRU algorithm based NMT System to address the aforementioned challenges and handles multilingual sentences based translation tasks efficiently. We aimed to develop the proposed model for translating the spoken sentences into sign words. The generated sign words (glosses) are mapped with sign gesture images to automate the sign gesture video generation process using deep generative models. The proposed hybrid NMT model has been evaluated qualitatively and quantitatively using different benchmark sign language datasets. The improved BLEU Score shows the outperformance of our model compared with earlier approaches. We also evaluated the proposed model using our self created Indian sign language corpus (ISL-CSLTR). The final result shows the achievement of greater translation results with minimal processing cost.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-021-03640-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40537-022-00561-y,Part of speech tagging: a systematic review of deep learning and machine learning approaches,Journal of Big Data,10.1186/s40537-022-00561-y,Springer,2022-01-24,"Natural language processing (NLP) tools have sparked a great deal of interest due to rapid improvements in information and communications technologies. As a result, many different NLP tools are being produced. However, there are many challenges for developing efficient and effective NLP tools that accurately process natural languages. One such tool is part of speech (POS) tagging, which tags a particular sentence or words in a paragraph by looking at the context of the sentence/words inside the paragraph. Despite enormous efforts by researchers, POS tagging still faces challenges in improving accuracy while reducing false-positive rates and in tagging unknown words. Furthermore, the presence of ambiguity when tagging terms with different contextual meanings inside a sentence cannot be overlooked. Recently, Deep learning (DL) and Machine learning (ML)-based POS taggers are being implemented as potential solutions to efficiently identify words in a given sentence across a paragraph. This article first clarifies the concept of part of speech POS tagging. It then provides the broad categorization based on the famous ML and DL techniques employed in designing and implementing part of speech taggers. A comprehensive review of the latest POS tagging articles is provided by discussing the weakness and strengths of the proposed approaches. Then, recent trends and advancements of DL and ML-based part-of-speech-taggers are presented in terms of the proposed approaches deployed and their performance evaluation metrics. Using the limitations of the proposed approaches, we emphasized various research gaps and presented future recommendations for the research in advancing DL and ML-based POS tagging.",https://www.biomedcentral.com/openurl?doi=10.1186/s40537-022-00561-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00500-021-06603-6,Task-specific image summaries using semantic information and self-supervision,Soft Computing,10.1007/s00500-021-06603-6,Springer,2022-01-21,"Large annotated datasets are needed for successful Deep Learning methodologies to achieve human-level performance. These needs restrict the impact of Deep Learning and build the necessity to create smaller and richer representative datasets that can offer a potential solution to this problem. In this paper, we propose task-specific image corpus summarization using semantic information and self-supervision. Our methodology makes use of GAN for the generation of features and leverages rotational invariance for employing self-supervision. All these objectives are facilitated on features from Resnet34. A summary can be obtained efficiently by using k -means clustering on the semantic embedding space and then selecting examples nearest to centroids. In comparison to end-to-end trained models, the proposed model does not require retraining to obtain summaries of different lengths. We also test our model by extensive qualitative and quantitative experiments.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-021-06603-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-021-04287-4,"Automated grading of enlarged perivascular spaces in clinical imaging data of an acute stroke cohort using an interpretable, 3D deep learning framework",Scientific Reports,10.1038/s41598-021-04287-4,Nature,2022-01-17,"Enlarged perivascular spaces (EPVS), specifically in stroke patients, has been shown to strongly correlate with other measures of small vessel disease and cognitive impairment at 1 year follow-up. Typical grading of EPVS is often challenging and time consuming and is usually based on a subjective visual rating scale. The purpose of the current study was to develop an interpretable, 3D neural network for grading enlarged perivascular spaces (EPVS) severity at the level of the basal ganglia using clinical-grade imaging in a heterogenous acute stroke cohort, in the context of total cerebral small vessel disease (CSVD) burden. T2-weighted images from a retrospective cohort of 262 acute stroke patients, collected in 2015 from 5 regional medical centers, were used for analyses. Patients were given a label of 0 for none-to-mild EPVS (< 10) and 1 for moderate-to-severe EPVS (≥ 10). A three-dimensional residual network of 152 layers (3D-ResNet-152) was created to predict EPVS severity and 3D gradient class activation mapping (3DGradCAM) was used for visual interpretation of results. Our model achieved an accuracy 0.897 and area-under-the-curve of 0.879 on a hold-out test set of 15% of the total cohort (n = 39). 3DGradCAM showed areas of focus that were in physiologically valid locations, including other prevalent areas for EPVS. These maps also suggested that distribution of class activation values is indicative of the confidence in the model’s decision. Potential clinical implications of our results include: (1) support for feasibility of automated of EPVS scoring using clinical-grade neuroimaging data, potentially alleviating rater subjectivity and improving confidence of visual rating scales, and (2) demonstration that explainable models are critical for clinical translation.",https://www.nature.com/articles/s41598-021-04287-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02347-4,A detailed analysis of image and video forgery detection techniques,The Visual Computer,10.1007/s00371-021-02347-4,Springer,2022-01-13,"With the recent advancement in modern technology, one can easily manipulate a digital image or video using computer software or a mobile application. The purpose of editing visual media could be as simple as to look good before sharing to the social networking site’s or can be as malicious as to defame or hurt one’s reputation in the real world through such morphed visual imagery. Identity theft is one of the examples where one’s identity get stolen by some impersonator who can access the personal and financial information of an innocent person. To avoid such drastic situations, law enforcement authorities must use some automatic tools and techniques to find out whether a person is innocent or the culprit. One major question that arises here is how and what parts of visual imagery can be manipulated or edited. The answer to this question is important to distinguish the authentic images/videos from the doctored multimedia. This survey provides a detailed analysis of image and video manipulation types, popular visual imagery manipulation methods, and state-of-the-art image and video forgery detection techniques. It also surveys different fake image and video datasets used in tampering. The goal is to develop a sense of privacy and security in the research community. Finally, it focuses to motivate researchers to develop generalized methods to capture artificial visual imagery which is capable of detecting any type of manipulation in given visual imagery.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02347-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11036-021-01873-8,3D Convolutional Neural Network for Human Behavior Analysis in Intelligent Sensor Network,Mobile Networks and Applications,10.1007/s11036-021-01873-8,Springer,2022-01-13,"The intelligent recognition of human behavior and action in massive video data is the key application direction in the field of artificial intelligence. With the development of intelligent communication network, multimedia communication has become a hot spot in the field of video analysis. 3D convolution is an efficient deep learning model. It can learn the temporal and spatial features of target images at the same time. A 3D max residual feature map convolution network (3D-MRCNN) is proposed in this paper. Problems can be solved by the proposed model that the deficiencies of the network degradation and gradient disappearance caused by convolution calculation. The proposed model is preprocessed by 2D convolution firstly. A learning network including 3D-max feature map (3D-MFM) and residual structure is established after the convolution splitting is completed. Finally, the output vectors corresponding to the two different inputs are connected and fused into the support vector machine (SVM) classification. The accuracy of 3D-MRCNN can achieve 85.7% by experimenting on the representative UCF101 data set. And it has higher accuracy and operating efficiency compared with the models which have strong correlation with 3D-MRCNN.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11036-021-01873-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-021-04286-5,Rapid video-based deep learning of cognate versus non-cognate T cell-dendritic cell interactions,Scientific Reports,10.1038/s41598-021-04286-5,Nature,2022-01-11,"Identification of cognate interactions between antigen-specific T cells and dendritic cells (DCs) is essential to understanding immunity and tolerance, and for developing therapies for cancer and autoimmune diseases. Conventional techniques for selecting antigen-specific T cells are time-consuming and limited to pre-defined antigenic peptide sequences. Here, we demonstrate the ability to use deep learning to rapidly classify videos of antigen-specific CD8^+ T cells. The trained model distinguishes distinct interaction dynamics (in motility and morphology) between cognate and non-cognate T cells and DCs over 20 to 80 min. The model classified high affinity antigen-specific CD8^+ T cells from OT-I mice with an area under the curve (AUC) of 0.91, and generalized well to other types of high and low affinity CD8^+ T cells. The classification accuracy achieved by the model was consistently higher than simple image analysis techniques, and conventional metrics used to differentiate between cognate and non-cognate T cells, such as speed. Also, we demonstrated that experimental addition of anti-CD40 antibodies improved model prediction. Overall, this method demonstrates the potential of video-based deep learning to rapidly classify cognate T cell-DC interactions, which may also be potentially integrated into high-throughput methods for selecting antigen-specific T cells in the future.",https://www.nature.com/articles/s41598-021-04286-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11831-021-09705-4,3D Face Reconstruction in Deep Learning Era: A Survey,Archives of Computational Methods in Engineering,10.1007/s11831-021-09705-4,Springer,2022-01-10,"3D face reconstruction is the most captivating topic in biometrics with the advent of deep learning and readily available graphical processing units. This paper explores the various aspects of 3D face reconstruction techniques. Five techniques have been discussed, namely, deep learning, epipolar geometry, one-shot learning, 3D morphable model, and shape from shading methods. This paper provides an in-depth analysis of 3D face reconstruction using deep learning techniques. The performance analysis of different face reconstruction techniques has been discussed in terms of software, hardware, pros and cons. The challenges and future scope of 3d face reconstruction techniques have also been discussed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11831-021-09705-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-021-04048-3,A novel deep learning-based 3D cell segmentation framework for future image-based disease detection,Scientific Reports,10.1038/s41598-021-04048-3,Nature,2022-01-10,"Cell segmentation plays a crucial role in understanding, diagnosing, and treating diseases. Despite the recent success of deep learning-based cell segmentation methods, it remains challenging to accurately segment densely packed cells in 3D cell membrane images. Existing approaches also require fine-tuning multiple manually selected hyperparameters on the new datasets. We develop a deep learning-based 3D cell segmentation pipeline, 3DCellSeg, to address these challenges. Compared to the existing methods, our approach carries the following novelties: (1) a robust two-stage pipeline, requiring only one hyperparameter; (2) a light-weight deep convolutional neural network (3DCellSegNet) to efficiently output voxel-wise masks; (3) a custom loss function (3DCellSeg Loss) to tackle the clumped cell problem; and (4) an efficient touching area-based clustering algorithm (TASCAN) to separate 3D cells from the foreground masks. Cell segmentation experiments conducted on four different cell datasets show that 3DCellSeg outperforms the baseline models on the ATAS (plant), HMS (animal), and LRP (plant) datasets with an overall accuracy of 95.6%, 76.4%, and 74.7%, respectively, while achieving an accuracy comparable to the baselines on the Ovules (plant) dataset with an overall accuracy of 82.2%. Ablation studies show that the individual improvements in accuracy is attributable to 3DCellSegNet, 3DCellSeg Loss, and TASCAN, with the 3DCellSeg demonstrating robustness across different datasets and cell shapes. Our results suggest that 3DCellSeg can serve a powerful biomedical and clinical tool, such as histo-pathological image analysis, for cancer diagnosis and grading.",https://www.nature.com/articles/s41598-021-04048-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-021-04395-1,Non-invasive scoring of cellular atypia in keratinocyte cancers in 3D LC-OCT images using Deep Learning,Scientific Reports,10.1038/s41598-021-04395-1,Nature,2022-01-10,"Diagnosis based on histopathology for skin cancer detection is today’s gold standard and relies on the presence or absence of biomarkers and cellular atypia. However it suffers drawbacks: it requires a strong expertise and is time-consuming. Moreover the notion of atypia or dysplasia of the visible cells used for diagnosis is very subjective, with poor inter-rater agreement reported in the literature. Lastly, histology requires a biopsy which is an invasive procedure and only captures a small sample of the lesion, which is insufficient in the context of large fields of cancerization. Here we demonstrate that the notion of cellular atypia can be objectively defined and quantified with a non-invasive in-vivo approach in three dimensions (3D). A Deep Learning (DL) algorithm is trained to segment keratinocyte (KC) nuclei from Line-field Confocal Optical Coherence Tomography (LC-OCT) 3D images. Based on these segmentations, a series of quantitative, reproducible and biologically relevant metrics is derived to describe KC nuclei individually. We show that, using those metrics, simple and more complex definitions of atypia can be derived to discriminate between healthy and pathological skins, achieving Area Under the ROC Curve (AUC) scores superior than 0.965, largely outperforming medical experts on the same task with an AUC of 0.766. All together, our approach and findings open the door to a precise quantitative monitoring of skin lesions and treatments, offering a promising non-invasive tool for clinical studies to demonstrate the effects of a treatment and for clinicians to assess the severity of a lesion and follow the evolution of pre-cancerous lesions over time.",https://www.nature.com/articles/s41598-021-04395-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s42979-021-01007-7,Explorative Application of Fusion Techniques for Multimodal Hate Speech Detection,SN Computer Science,10.1007/s42979-021-01007-7,Nature,2022-01-10,"Hate speech detection is an important research area owing to the severe effects of hate speech on the society. Hence automated hate speech detection based on textual data assumed a pivotal role among the research groups. Moreover, the exponential growth of multimodal content on social media like hateful memes poses the need for building efficient machine learning which can handle such content. In this work, we explore different fusion techniques and compare their performance for the multimodal hate speech identification task. In particular, we test new combinations of fusing textual and visual models to improve the performance of the models on the MMHS150K dataset. We apply the corresponding preprocessing techniques for the text and images of tweets. Then, we use a pre-trained BERT model for textual feature extraction and Inceptionv3, Inception ResNet, ResNext to extract features from the images. We apply different early fusion techniques like concatenation and product rule and late fusion techniques namely distribution summation, performance weighting, logarithmic opinion polling, rules learned from training on probabilities to efficiently fuse vision and text modalities. We also employ the SMOTE oversampling technique and random undersampling to deal with the class imbalance in the MMHS150K dataset. Our proposed model has achieved an accuracy of 67.7% which is comparable to the state-of-the-art.",https://www.nature.com/articles/s42979-021-01007-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02356-3,Motion pattern-based crowd scene classification using histogram of angular deviations of trajectories,The Visual Computer,10.1007/s00371-021-02356-3,Springer,2022-01-10,"Automated crowd behaviour analysis and monitoring is a challenging task due to the unpredictable nature of the crowd within a particular scene and across different scenes. The prior knowledge of the type of scene under consideration is a crucial mid-level information, which could be utilized to develop robust crowd behaviour analysis systems. In this paper, we propose an approach to automatically detect the type of a crowded scene based on the global motion patterns of the objects within the scene. Three different types of scenes whose global motion pattern characteristics vary from uniform to non-uniform are considered in this work, namely structured, semi-structured, and unstructured scenes, respectively. To capture the global motion pattern characteristics of an input crowd scene, we first extract the motion information in the form of trajectories using a key-point tracker and then compute the average angular orientation feature of each trajectory. This paper utilizes these angular features to introduce a novel feature vector, termed as Histogram of Angular Deviations (HAD), which depicts the distribution of the pair-wise angular deviation values for each trajectory vector. Since angular deviation information is resistant to changes in scene perspectives, we consider it as a key feature for distinguishing the scene types. To evaluate the effectiveness of the proposed HAD-based feature vector in classifying the crowded scenes, we build a crowd scene classification model by training the classical machine learning algorithms on the publicly available Collective Motion Database. The experimental results demonstrate the superior crowd classification performance of the proposed approach as compared to the existing methods. In addition to this, we propose a technique based on quantizing the angular deviation values to reduce the feature dimension and subsequently introduce a novel crowd scene structuredness index to quantify the structuredness of an input crowded scene based on its HAD.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02356-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s42979-022-01014-2,Controlling by Showing: i-Mimic: A Video-Based Method to Control Robotic Arms,SN Computer Science,10.1007/s42979-022-01014-2,Nature,2022-01-10,"A novel concept of vision-based intelligent control of robotic arms is developed here in this work. This work enables the controlling of robotic arm motion only with visual input, that is, controlling by showing the videos of correct movements. This work can broadly be sub-divided into two segments. The first part of this work is to develop an unsupervised vision-based method to control robotic arms in 2-D plane, and the second one is with deep convolutional neural network (CNN) in the same task in 3-D plane. The first method is unsupervised, where our aim is to perform mimicking of human arm motion in real-time by a manipulator. We developed a network, namely the vision-to-motion optical network (DON). Given the input of a video stream containing the hand movements of human on the DON, the velocity and torque information of the hand movements shown in the video would be generated as the output. The output information of the DON is then fed to the robotic arm by enabling it to generate motion according to the real hand videos. The method has been tested on both live-stream video feeds as well as on recorded video obtained from a monocular camera even by intelligently predicting the trajectory of the human hand when it gets occluded. This is why the mimicry of the arm incorporates some intelligence to it and becomes an intelligent mimic (i- mimic). Furthermore, to enhance the performance of DON and make it applicable to mimic multi-joint movements with n-link manipulator, a deep network, namely, CNN has been used along with a refiner network as the predecessor of DON. Refiner network has been used to overcome the limitations of inadequate labelled data. Both the proposed methods are validated with off-line as well as with on-line video datasets in real-time. The entire methodology is validated with real-time 1-link and simulated n-link manipulators (an arm with n number of different joints) along with suitable comparisons.",https://www.nature.com/articles/s42979-022-01014-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02339-4,3D human pose and shape estimation with dense correspondence from a single depth image,The Visual Computer,10.1007/s00371-021-02339-4,Springer,2022-01-07,"We propose a novel approach to estimate the 3D pose and shape of human bodies with dense correspondence from a single depth image. In contrast to most current 3D body model recovery methods from depth images that employ motion information of depth sequences to compute point correspondences, we reconstruct 3D human body models from a single depth image by combining the correspondence learning and the parametric model fitting. Specifically, a novel multi-view coarse-to-fine correspondence network is proposed by projecting a 3D template model into multi-view depth images. The proposed correspondence network can predict 2D flows of the input depth relative to each projected depth in a coarse-to-fine manner. The predicted multi-view flows are then aggregated to establish accurate dense point correspondences between the 3D template and the input depth with the known 3D-to-2D projection. Based on the learnt correspondences, the 3D human pose and shape represented by a parametric 3D body model are recovered through a model fitting method that incorporates an adversarial prior. We conduct extensive experiments on SURREAL, Human3.6M, DFAUST, and real depth data of human bodies. The experimental results demonstrate that the proposed method outperforms the state-of-the-art methods in terms of reconstruction accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02339-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06754-5,DeepClassRooms: a deep learning based digital twin framework for on-campus class rooms,Neural Computing and Applications,10.1007/s00521-021-06754-5,Springer,2022-01-07,"A lot of different methods are being opted for improving the educational standards through monitoring of the classrooms. The developed world uses Smart classrooms to enhance faculty efficiency based on accumulated learning outcomes and interests. Smart classroom boards, audio-visual aids, and multimedia are directly related to the Smart classroom environment. Along with these facilities, more effort is required to monitor and analyze students’ outcomes, teachers’ performance, attendance records, and contents delivery in on-campus classrooms. One can achieve more improvement in quality teaching and learning outcomes by developing digital twins in on-campus classrooms. In this article, we have proposed DeepClass-Rooms, a digital twin framework for attendance and course contents monitoring for the public sector schools of Punjab, Pakistan. DeepClassRooms is cost-effective and requires RFID readers and high-edge computing devices at the Fog layer for attendance monitoring and content matching, using convolution neural network for on-campus and online classes.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06754-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-021-03453-y,Contactless facial video recording with deep learning models for the detection of atrial fibrillation,Scientific Reports,10.1038/s41598-021-03453-y,Nature,2022-01-07,"Atrial fibrillation (AF) is often asymptomatic and paroxysmal. Screening and monitoring are needed especially for people at high risk. This study sought to use camera-based remote photoplethysmography (rPPG) with a deep convolutional neural network (DCNN) learning model for AF detection. All participants were classified into groups of AF, normal sinus rhythm (NSR) and other abnormality based on 12-lead ECG. They then underwent facial video recording for 10 min with rPPG signals extracted and segmented into 30-s clips as inputs of the training of DCNN models. Using voting algorithm, the participant would be predicted as AF if > 50% of their rPPG segments were determined as AF rhythm by the model. Of the 453 participants (mean age, 69.3 ± 13.0 years, women, 46%), a total of 7320 segments (1969 AF, 1604 NSR & 3747others) were analyzed by DCNN models. The accuracy rate of rPPG with deep learning model for discriminating AF from NSR and other abnormalities was 90.0% and 97.1% in 30-s and 10-min recording, respectively. This contactless, camera-based rPPG technique with a deep-learning model achieved significantly high accuracy to discriminate AF from non-AF and may enable a feasible way for a large-scale screening or monitoring in the future.",https://www.nature.com/articles/s41598-021-03453-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s12984-021-00978-1,Feasibility of a virtual reality-based exercise intervention and low-cost motion tracking method for estimation of motor proficiency in youth with autism spectrum disorder,Journal of NeuroEngineering and Rehabilitation,10.1186/s12984-021-00978-1,BioMed Central,2022-01-07,"Background Motor impairment is widely acknowledged as a core feature in children with autism spectrum disorder (ASD), which can affect adaptive behavior and increase severity of symptoms. Low-cost motion capture and virtual reality (VR) game technologies hold a great deal of promise for providing personalized approaches to motor intervention in ASD. The present study explored the feasibility, acceptability and potential efficacy of a custom-designed VR game-based intervention (GaitWayXR™) for improving gross motor skills in youth with ASD. Methods Ten children and adolescents (10–17 years) completed six, 20-min VR-based motor training sessions over 2 weeks while whole-body movement was tracked with a low-cost motion capture system. We developed a methodology for using motion tracking data to quantify whole-body movement in terms of efficiency, synchrony and symmetry. We then studied the relationships of the above quantities with standardized measures of motor skill and cognitive flexibility. Results Our results supported our presumption that the VR intervention is safe, with no adverse events and very few minor to moderate side-effects, while a large proportion of parents said they would use the VR game at home, the most prohibitive reasons for adopting the system for home therapy were cost and space. Although there was little evidence of any benefits of the GaitWayXR™ intervention in improving gross motor skills, we showed several positive correlations between the standardized measures of gross motor skills in ASD and our measures of efficiency, symmetry and synchrony from low-cost motion capture. Conclusions These findings, though preliminary and limited by small sample size, suggest that low-cost motion capture of children with ASD is feasible with movement exercises in a VR-based game environment. Based on these preliminary findings, we recommend conducting larger-scale studies with methods for improving adherence to VR gaming interventions over longer periods.",https://www.biomedcentral.com/openurl?doi=10.1186/s12984-021-00978-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06703-2,Human motion tracking and 3D motion track detection technology based on visual information features and machine learning,Neural Computing and Applications,10.1007/s00521-021-06703-2,Springer,2022-01-07,"Human motion detection is a major subject of investigation in the field of machine visualization and synthetic integration. It serves a wide and important spectrum of applications in terms of visual surveillance, cross-functional simulation, movement acquisition, and high-level man-computer interface. In order to improve the accuracy of human tracking, this paper starts from the geometric flow characteristics of the image, and proposes a Gaussian algorithm to process human motion images, and applies it to the video human motion tracking of machine learning methods. Using the statistical features of the optimized transformation as the features of the image, regression learning and prediction of the three-dimensional human body movement posture in the monocular video image. First, the optimal parameters and additional statistical features of the transformation used in the extraction of human image features are verified through experiments, and then various regression methods are used for parameter learning, and the prediction performance and human tracking tests are carried out. The final test results found that the accuracy of the method based on visual information features and machine learning is 5% higher than the previous method, and the recognition rate of human motion tracking is as high as 95%. The processing capability of the detection technology of 3D motion trajectory has also increased by nearly 7%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06703-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11578-5,Spoofing detection system for e-health digital twin using EfficientNet Convolution Neural Network,Multimedia Tools and Applications,10.1007/s11042-021-11578-5,Springer,2022-01-06,"Digital Twin is the mirror image of any living or non-living objects. Digital Twin and Cyber-physical system (CPS) provides a new era for industries especially in the healthcare sector that keeps track of health data of individuals to provide on-demand, fast and efficient services to the users. In the suggested system, various health parameters of the patients are collected through different health instruments, wearable devices that communicate data to the primary database; used for analysis purposes for better diagnosis and training for automated systems. The primary database in a physical object and parallelly maintain virtual object/digital twin of the same in order of analyzing, summarize and mine data for diagnosis, monitoring the patient in real-time. The e-health cloud data need to be protected from unauthorized access by biometric authentication using iris biometric trait. The proposed paper suggested two phases EfficientNet Convolution Neural Network-based framework for identifying the real or spoofed user sample. The proposed system is trained using EfficientNet Convolution Neural Network on different datasets of spoofed and actual iris biometric samples to discriminate the original and spoofed one.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11578-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40747-021-00608-2,ETHOS: a multi-label hate speech detection dataset,Complex & Intelligent Systems,10.1007/s40747-021-00608-2,Springer,2022-01-04,"Online hate speech is a recent problem in our society that is rising at a steady pace by leveraging the vulnerabilities of the corresponding regimes that characterise most social media platforms. This phenomenon is primarily fostered by offensive comments, either during user interaction or in the form of a posted multimedia context. Nowadays, giant corporations own platforms where millions of users log in every day, and protection from exposure to similar phenomena appears to be necessary to comply with the corresponding legislation and maintain a high level of service quality. A robust and reliable system for detecting and preventing the uploading of relevant content will have a significant impact on our digitally interconnected society. Several aspects of our daily lives are undeniably linked to our social profiles, making us vulnerable to abusive behaviours. As a result, the lack of accurate hate speech detection mechanisms would severely degrade the overall user experience, although its erroneous operation would pose many ethical concerns. In this paper, we present ‘ETHOS’ (multi-labEl haTe speecH detectiOn dataSet), a textual dataset with two variants: binary and multi-label, based on YouTube and Reddit comments validated using the Figure-Eight crowdsourcing platform. Furthermore, we present the annotation protocol used to create this dataset: an active sampling procedure for balancing our data in relation to the various aspects defined. Our key assumption is that, even gaining a small amount of labelled data from such a time-consuming process, we can guarantee hate speech occurrences in the examined material.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40747-021-00608-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-92054-8_3,Artificial Intelligence,Intelligent Cyber-Physical Systems for Autonomous Transportation,10.1007/978-3-030-92054-8_3,Springer,2022-01-01,"The Autonomous transportation systems will become the mainstream in the future; it is particularly important to design effectively automatic control modules. This calls for the algorithms with highly adaptive ability of perception and computation. By actively interacting with the surroundings, optimal control decision strategies can be automatically calculated. Although research efforts have been devoted to this domain for many years, the general mathematical optimization methods can only find the approximated solutions inside systems, which makes the performance of the transportation systems difficult to achieve perfectly optimal. In this context, introduction of intelligent technologies represented by artificial intelligence has been regarded as a promising perspective. Artificial intelligence (AI), a branch of computer science, attempts to understand the essence of intelligence and simulates a novel intelligent machine that can respond in the ways similar to human intelligence. Since the birth of AI, its theory and technology have become increasingly mature, expanding applications in many fields such as robotics, language recognition, image recognition, natural language processing, and expert systems. It can be imagined that the scientific and technological products brought by artificial intelligence in the future will be the “container” of human intelligence. Generalized into transportation systems, the AI abstracts the complex system processes as black boxes and then uses the idea of statistical learning to model the complex system processes. By discovering potential and unobservable patterns, the AI provides more opportunities to solve the uncertainty problems hidden in the transportation systems. Predictably, the AI technology will be the key breakthrough in the evolution from conventional transportation systems to autonomous transportation systems. Therefore, this chapter is organized via three aspects of contents: (1) Overview of AI; (2) Need and Evolution of AI; and (3) AI for Transportation Systems. In this chapter, the wide AI conception is described by dividing it into three branches: cognitive AI, machine learning AI, and deep learning AI. Further, the need and evolution of AI are surveyed via three parts. The first part describes origin and development of AI, the second part describes common approaches and technologies about AI, and the third part describes successive cross-field applications of AI. Finally, application of AI for transportation systems is introduced via three parts: motivation, application status, and challenges.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92054-8_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-82196-8_57,A Survey on the Semi Supervised Learning Paradigm in the Context of Speech Emotion Recognition,Intelligent Systems and Applications,10.1007/978-3-030-82196-8_57,Springer,2022-01-01,"The area of Automatic Speech Emotion Recognition has been a hot topic for researchers for quite some time now. The recent breakthroughs on technology in the field of Machine Learning open up doors for multiple approaches of many kinds. However, some concerns have been persistent throughout the years where we highlight the design and collection of data. Proper annotation of data can be quite expensive and sometimes not even viable, as specialists are often needed for such a complex task as emotion recognition. The evolution of the semi supervised learning paradigm tries to drag down the high dependency on labelled data, potentially facilitating the design of a proper pipeline of tasks, single or multi modal, towards the final objective of the recognition of the human emotional mental state. In this paper, a review of the current single modal (audio) Semi Supervised Learning state of art is explored as a possible solution to the bottlenecking issues mentioned, as a way of helping and guiding future researchers when getting to the planning phase of such task, where many positive aspects from each piece of work can be drawn and combined.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-82196-8_57,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-81007-8_74,Research on the Robotic Aiming System with Image Recognition and Deep Learning,Advances in Intelligent Automation and Soft Computing,10.1007/978-3-030-81007-8_74,Springer,2022-01-01,"This paper introduces a robotic aiming system with the function of image classification and target detection, which can be deployed to lightweight and mobile devices. The construction and training process of this model is described. The construction of the entire model is based on TensorFlow2 and structure of it is mainly based on SSD-MobileNetV2. Two methodologies for constructing and evaluating the system are presented. The transfer learning approach used other trained model’s weight and gained superior results than the base approach. This model is evaluated through TensorBoard and is able to achieve the accuracy of 95% in real world when deployed to Raspberry Pi.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-81007-8_74,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-2008-9_13,Modern Transfer Learning-Based Preliminary Diagnosis of COVID-19 Using Forced Cough Recordings with Mel-Frequency Cepstral Coefficients,Applied Information Processing Systems,10.1007/978-981-16-2008-9_13,Springer,2022-01-01,"Researchers have used forced recordings of coughs to diagnose conditions like asthma and pneumonia accurately. Research to analyze COVID-19 using audio recordings of cough is still in its infancy. Our paper proposes a novel kind of COVID-19 test using forced cough recordings. Our model architecture enables a nearly cost-free, real-time solution for COVID-19 testing. The model uses forced cough recordings to recognize whether the patient is COVID-19 positive or not. Readily available tests can help check the outbreak of this novel virus and gradually ensure a COVID-19-free world Machine Learning and Deep Learning approaches and K-Nearest Neighbors, Support Vector Machine, Decision Trees, and Random Forest classifiers were employed to address this issue. At the same time, disparate sorts of Convolutional Neural Networks were used under the Deep Learning approach.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-2008-9_13,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-1677-9_39,A Novel Deep Learning Based Nepali Speech Recognition,Innovations in Electrical and Electronic Engineering,10.1007/978-981-19-1677-9_39,Springer,2022-01-01,Automatic speech recognition has allowed human beings to use their voices to speak with a computer interface. Nepali speech recognition involves conversion of Nepali language to corresponding text in Devanagari lipi. This work proposes a novel approach for developing Nepali Speech recognition model based using CNN-GRU. The data is collected from the Librispeech. The collected data is pre-processed and MFCC is applied on it for feature extraction. CNN-GRU model is responsible for extraction of the features and development of the acoustic model. CTC is responsible for decoding. The performance of the developed model has been assessed using Word Error Rate of the transcribed text.,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-1677-9_39,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9012-9_11,Hate Speech Detection Using Machine Learning Techniques,Sustainable Advanced Computing,10.1007/978-981-16-9012-9_11,Springer,2022-01-01,"Hate speech is an issue to most governments and the public’s concern due to the increased emergence of different social media applications and the increasing use of such media to disseminate hate speech to individuals or groups of persons, communities, or race. Therefore, without detection and hate speech analysis, it is impossible to believe that there is not any malicious information on social media. This paper strives to provide a survey of hate speech detection using different approaches and their comparisons while focusing on aspects like machine learning models, different features put to use, and datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9012-9_11,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0604-6_17,Applications of Deep Learning Approaches in Speech Recognition: A Survey,Proceedings of International Conference on Computing and Communication Networks,10.1007/978-981-19-0604-6_17,Springer,2022-01-01,"Automated speech recognition (ASR) appeared to be a driving force for a variety of machine learning (ML) techniques, include to ubiquitously utilized discriminative learning, Bayesian learning, hidden Markov model, adaptive learning, and structured sequence learning. Although machine learning utilize ASR as a large scale, it can reasonable application to thoroughly test viability for a given procedure and to motivate unused issues emerging from intrinsically consecutive and discourse energetic nature. Also, although ASR is accessible commercially for a few applications used in this research through the limitation and research gaps that the researcher try to access high accuracy of these systems. The advance technology from new ML techniques appears incredible guarantee to progress the literature review in ASR innovation. This study gives reader with a diagram of present-day ML methods as used within the relevant and current as significant for ASR future systems and research. The study goal is to promote advanced cross-pollination between ML and ASR communities more than has hither to occurred.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0604-6_17,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-05484-6_51,Multi-feature Fusion Image Recognition System Based on Machine Learning Algorithm,Application of Intelligent Systems in Multi-modal Information Analytics,10.1007/978-3-031-05484-6_51,Springer,2022-01-01,"The branch BP neural network of machine learning can recognize image information, so it can effectively improve the application effect of artificial intelligence. Compared with humans, machine learning has faster data processing speed, storage information and the ability to extract features. Machine learning models that deal with image recognition and cognitive abilities have a profound impact on applications such as massive image classification, image recognition positioning, image-text conversion, and image segmentation. This paper uses BP neural network to design a multi-feature fusion image recognition system. After using this system to extract the color and texture feature vector values of the image, the image recognition accuracy is calculated. Research shows that the system has the highest recognition accuracy of the image in the forest, reaching 93.68. %, the recognition accuracy of grass is the lowest, only 9.52%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-05484-6_51,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9756-2_33,Deep Learning in Human Activity Recognition from Videos: A Survey,Advances in Computational Intelligence and Communication Technology,10.1007/978-981-16-9756-2_33,Springer,2022-01-01,"Human activity recognition is a trending area of research in current times. It has variety of applications especially from security point of view. Recognizing human actions from video is a tedious task in computer vision due to largely invariant nature of scenarios that occur and also the infinite amount of data that needs to be worked upon that too accurately and fast. Activities are classified into two major groups. The simple activities come under single-layer group wherein simple activities like sitting, walking, running are performed by one or two subjects. Another group is called hierarchical group that consists of much complex activities or activities in sequence being performed by one or many subjects at a time. This paper discusses some papers associated with human activity recognition from videos. We examined each paper to highlight the method used for feature extraction and classification. We also noted the method used and accuracies achieved by the authors on various public datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9756-2_33,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-2406-3_64,Speech Recognizing Comparisons Between Web Speech API and FPT.AI API,Proceedings of the 12th National Technical Seminar on Unmanned System Technology 2020,10.1007/978-981-16-2406-3_64,Springer,2022-01-01,"Nowadays, people use speech recognition services for many purposes in their daily lives, such as learning foreign languages, communicating, etc. Therefore, they need to decide which ones to use. High accuracy and short processing time speech recognition service will help improve the work effectively as the time to re-check output results and the delay time between recognition tasks. For Vietnamese speech recognition, Web Speech API and FPT.AI API are popular. Web Speech API supports multiple languages, while FPT.AI API focuses on Vietnamese as FPT.AI’s products are developed exclusively for the Vietnamese market. In order to assist people in choosing a suitable Vietnamese speech recognition service, in this paper, the speech recognizing accuracy and processing time between Web Speech API and FPT.AI API has been compared. 307 audio files containing Vietnamese speeches which are obtained from FPT Open Speech Dataset were chosen to test the accuracy and the processing time of both APIs. For the accuracy test, FPT.AI API was 0.57% more precise than Web Speech API. However, in the processing time test, Web Speech API was 50.99% faster than FPT.AI API. For Web Speech API, it was mostly accurate to process 12–14-second-long audio files, while FPT.AI API did best when process 2–4-second-long audio files. The audio files with duration values between 2 and 8 seconds are optimal for both APIs to proceed with STT conversions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-2406-3_64,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-2422-3_7,Speech Recognition Using Artificial Neural Network,Intelligent Sustainable Systems,10.1007/978-981-16-2422-3_7,Springer,2022-01-01,"Hussain, Shoeb Nazir, Ronaq Javeed, Urooj Khan, Shoaib Sofi, Rumaisa Speech recognition can be an important tool in today’s society for hand-free or voice-driven implementation. Using simple commands or triggers, it is possible for speech impaired human beings to communicate with increased ease of understanding. With the advent of various soft computing methods, a large class of nonlinearities can be handled. Artificial Neural Networks (ANN) have been applied in finding the solution for speech recognition. A lot of work is going on in this regard and mostly positive results have been achieved. Now, the research is being done to minimize the rate of error in obtaining the solution. In this paper, a comprehensive study of use of artificial neural networks in speech recognition is studied and proposes methods for training of the neural network so that an appropriate neural output can be obtained which is as close to the desired output. The paper demonstrates that ANN can indeed form the basis for a general-purpose speech recognition and neural network offers clear advantages over conventional methods. MATLAB simulation has been carried out to validate the results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-2422-3_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-2448-4_3,Image Recognition Methods Based on Deep Learning,3D Imaging—Multidimensional Signal Processing and Deep Learning,10.1007/978-981-19-2448-4_3,Springer,2022-01-01,"With the development of science and technology worldwide, computer technology has gradually been widely used in various fields like the military and biomedical domains. In this situation, people’s requirements for image recognition are increasing. However, the efficiency of manual image recognition is very low, and image recognition has always been a weak field in computer technology. This paper will introduce various deep learning models such as feedforward neural networks, convolutional neural networks, and recurrent neural networks. We will talk about how these models work and then compare and analyze these different networks. In this paper, we will try to train a convolutional neural network from a data set. The “dogs-vs-cats” data set from Kaggle will be used. In the “dogs-vs-cats” experiment part, some basic tasks in image processing will be discussed, such as feature extraction method, which is one of the most important steps in each link of image processing. Finally, the machine should be able to distinguish between pictures of dogs and cats. Moreover, we will also discuss some preliminary applications of deep learning in image recognition.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-2448-4_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-74614-8_87,Explainable AI for Entertainment: Issues on Video on Demand Platforms,Proceedings of the 21st Congress of the International Ergonomics Association (IEA 2021),10.1007/978-3-030-74614-8_87,Springer,2022-01-01,"With the proliferation of Artificial Intelligence-based systems, several questions arise involving ethical principles. In addition, the human-centered approach takes the focus on the user experience with these systems and studies user needs. A growing issue is the relationship between the transparency of these systems and the trust of users, since most systems are considered black-boxes. In this scenario, the Explainable Artificial Intelligence (XAI) emerges, with the proposal to explain the rationale of the decision making of the algorithms. XAI then starts to gain space in systems that involve high risk, such as health. Our research aims to discuss the importance of transparency to improve the user experience with recommendation mechanisms for entertainment, such as Video on Demand (VoD) platforms. In addition, we intent to raise the adjacent consequences of including XAI, such as improving the control and trust of VoD platforms. For this, we conducted an exploratory research method named Directed Storytelling. The study was conducted with thirty-one participants, all users of VoD platforms, regardless of time and frequency of use of this kind of systems. We note that people understand that there is an automated mechanism making recommendations for content in a personalized way for them, based on their browsing history, but the rules are not explicit. Thus, many users are suspicious of being manipulated by the system’s recommendations and resort to external recommendations, such as tips from third parties or Internet searches through specialized channels.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-74614-8_87,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8558-3_11,Deep Learning 3D Convolutional Neural Networks for Predicting Alzheimer’s Disease (ALD),New Approaches for Multidimensional Signal Processing,10.1007/978-981-16-8558-3_11,Springer,2022-01-01,"Deep learning is a power machine learning algorithm in classification while extracting high-level features. This paper is proposed to predict Alzheimer’s disease (AlD) with a deep 3D convolutional neural network (3D-CNN), which can learn generic features capturing AlD biomarkers, classify Alzheimer’s brain from normal healthy brain based on Magnetic Resonance Imaging (MRI) scans of the brain. In this work, we successfully classified MRI data of Alzheimer’s subjects from normal controls using the Alzheimer’s disease Neuroimaging Initiative (ADNI) data set using 3,013 scans where the accuracy for training data reached 96.5% and for test data reached 80.6%. This experiment suggests us the shift- and scale-invariant features extracted by 3D-CNN followed by deep learning classification which is the most powerful method to distinguish clinical data from healthy data in MRI. This approach also enables us to expand our methodology to predict more complicated systems.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8558-3_11,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-3690-5_134,Personalized Emotion Recognition Utilizing Speech Signal and Linguistic Clues,ICDSMLA 2020,10.1007/978-981-16-3690-5_134,Springer,2022-01-01,"In this project, non-intrusive videos are taken in real-time, which detect the emotional state of an individual by analyzing the facial expression. We present the Convolution Neural Network (CNN), a machine learning algorithm used for automatic image classification. This supervised classification technique analyses and trains the classifier on the labeled images and extracts the features of the classifier. By using the learned information of the training, the newly provided image will be classified based on the features observed in the image. The prototype which we are going to develop detects whether a person is happy or sad or angry or surprise.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-3690-5_134,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-4369-9_4,Prosodic Speech Synthesis of Narratives Depicting Emotional Diversity Using Deep Learning,Advanced Computational Paradigms and Hybrid Intelligent Computing,10.1007/978-981-16-4369-9_4,Springer,2022-01-01,"Shah, Shloka S. Gupta, Rishika A. Jardosh, Parth M. Nimkar, Anant V. Emotions are an essential part of speech or communication, which is why they cannot be neglected. The existing text-to-speech systems are not the most appropriate at conveying the emotions present behind the text. The systems can speak out the text monotonically lacking expressiveness. In this paper, an Expressive Text-to-Speech Synthesis System (ETSSS) is proposed which considers the dominant emotions in the text provided. ETSSS works in two parts: first, it identifies the label behind the text, and second produces expressive speech. In the first part, the input text is given an emotional label. Later, this label is used to generate expressive and prosodic speech. Labeling emotions in ETSSS is carried out using BERT which has an accuracy of 94%, 90%, and 90% for disgust, amused, and anger, respectively. The speech synthesis with the emotion module of ETSSS achieves a good MOS of 3.8 for anger, 3.5 for disgust, and 3.2 for amused.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-4369-9_4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9701-2_28,Evaluation of Emotions Generated in Audio-Branding Strategies Using a Deep Learning Model with a Central Affinity Autoencoder Structure with Mixed Learning,"Advances in Tourism, Technology and Systems",10.1007/978-981-16-9701-2_28,Springer,2022-01-01,"The new approach to brands has streamlined the creation of marketing strategies and customer relationships to achieve emotionally strong insights. Audio-branding is hardly used and is difficult to measure its help in creating customer connection with brands. To close this gap, we propose a deep learning model with mixed learning and central affinity, for the characterization of emotions in audio-branding by compressing electroencephalographic (EEG) signals obtained from the brain activation of an individual subjected to auditory brand strategies used for its branding. The results show that the proposed model allowed the emotional characterization of audio-branding for five brands (Microsoft Windows, Valve, MasterCard, THX, and XBOX) taking as a reference a set of 20 basic sounds, facilitating the iterative construction of identity and brand management, helping to strengthen or readjust brand marketing strategies. This neural model, given its capacity of adaptation and learning, can be applied in other spheres of action as an innovative solution to the characterization of emotions from auditory images.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9701-2_28,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-98112-9_9,Current Trends of Semiconductor Systems and Physical Design,SoC Physical Design,10.1007/978-3-030-98112-9_9,Springer,2022-01-01,This chapter discusses current trends and design challenges of SoCs. It covers different converging technologies that enable advancements in innovative SoC architectures. This is a short chapter to motivate engineers to help push the envelope of technological advancement in future system development.,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98112-9_9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-99584-3_2,Text Detection and Recognition Using Augmented Reality and Deep Learning,Advanced Information Networking and Applications,10.1007/978-3-030-99584-3_2,Springer,2022-01-01,"In recent years, the detection and recognition of text in natural images has become a very attractive and important subject for researchers. Many applications were developed for text detection and recognition and the majority of them are based on deep learning (DL) and augmented reality (AR). In this article, we propose a perfect solution based on both deep learning and augmented reality in order to make the text reading process more efficient, clear and safer. The system purpose is to help visually impaired people read a text from natural images. First of all, the user has to hover his smartphone’s camera over the image of the text present in his environment. Then, the system executes the detection and recognition module using the DL model. Finally, the system displays the associated graphical data augmented on the identified text on the screen of the smartphone using the AR method. AR method is used to improve the visualization of the detected and recognized word so that the user can read that text more efficiently. This mobile application has the highest-level visual features to improve the reading process of the detected and recognized text. To validate the system performance, the application is tested on a group of people who answer a questionnaire that reflects their experience with our proposed approach. In addition, user study test is performed to test user friendliness and satisfaction.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-99584-3_2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-92127-9_68,Application of Digital Twin Theory for Improvement of Natural Gas Treatment Unit,"11th International Conference on Theory and Application of Soft Computing, Computing with Words and Perceptions and Artificial Intelligence - ICSCCW-2021",10.1007/978-3-030-92127-9_68,Springer,2022-01-01,"This paper describes fundamental principles of Digital Twins theory and provides exact investigation results in application of Digital Twins theory in upstream branch of oil and gas industry, namely based on example of natural gas treatment plant’s performance increase. As one of key process units of natural gas treatment which allows to implement most powerful functions of digital twin gas sweetening unit with set of membranes is considered as object of investigation. On the base of membrane technology manipulated variables are defined as inputs to digital twin model. Some theoretical results as well as real references of model’s engine calculations are reflected in the paper. Details of technical dashboards to visualize calculated results of running model based on manipulated variables are presented including monthly key performance indicators report dashboard, process flow diagram dashboard and high-level management dashboard. Paper also demonstrates data flow between digital twin model and real process unit and also inside digital twin model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92127-9_68,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8129-5_97,Remote Spoken Arabic Digits Recognition Using CNN,"Proceedings of the 11th International Conference on Robotics, Vision, Signal Processing and Power Applications",10.1007/978-981-16-8129-5_97,Springer,2022-01-01,"While Arabic is the fifth most spoken language in the world, it still lacks powerful speech recognition systems. Moreover, well-resourced languages have benefited greatly from recent developments in artificial intelligence and In deep learning, while the development of Arabic speech recognizers is still modest. In this paper, we contribute to bridging the gap between the resources of Arabic and those of the well-resourced languages, in particular in the context of remote communications. Since the unavailability of dedicated corpora is the most problematic that prevents the development of Arabic products, we built a corpus composed of the ten Arabic digits. The corpus was collected remotely, over-connected smartphone apps, and social media. A convolutional neural network algorithm is then trained to recognize the digits. Initial experiments faced an overfitting problem due to insufficient data. To overcome this problem, we artificially expanded the training dataset with audio data augmentation techniques. In further experiments, we explored cross-corpora speech recognition. The final model achieved a recognition rate of 95.94% on training subset and 77.30% on test subset. The results showed the potential of the proposed approach in remote speech recognition conditions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8129-5_97,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-98355-0_53,Reinforcement Learning-Based Interactive Video Search,MultiMedia Modeling,10.1007/978-3-030-98355-0_53,Springer,2022-01-01,"Despite the rapid progress in text-to-video search due to the advancement of cross-modal representation learning, the existing techniques still fall short in helping users to rapidly identify the search targets. Particularly, in the situation that a system suggests a long list of similar candidates, the user needs to painstakingly inspect every search result. The experience is frustrated with repeated watching of similar clips, and more frustratingly, the search targets may be overlooked due to mental tiredness. This paper explores reinforcement learning-based (RL) searching to relieve the user from the burden of brute force inspection. Specifically, the system maintains a graph connecting shots based on their temporal and semantic relationship. Using the navigation paths outlined by the graph, an RL agent learns to seek a path that maximizes the reward based on the continuous user feedback. In each round of interaction, the system will recommend one most likely video candidate for users to inspect. In addition to RL, two incremental changes are introduced to improve VIREO search engine. First, the dual-task cross-modal representation learning has been revised to index phrases and model user query and unlikelihood relationship more effectively. Second, two more deep features extracted from SlowFast and Swin-Transformer, respectively, are involved in dual-task model training. Substantial improvement is noticed for the automatic Ad-hoc search (AVS) task on the V3C1 dataset.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98355-0_53,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-00828-3_19,An Improved Convolutional Neural Network for Speech Emotion Recognition,Recent Advances in Soft Computing and Data Mining,10.1007/978-3-031-00828-3_19,Springer,2022-01-01,"The speech emotion recognition is a challenging and an exigent task in the field of data science. Existing studies have only focused on one-dimensional Convolutional Neural Network (CNN) architecture for speech emotion recognition. This one-dimensional architecture’s speech recognition accuracy is low when dealt with RAVDESS, TESS and URDU datasets using non-optimal parameters. To overcome this problem, this research work proposed an efficient two-dimensional CNN architecture with an optimized combination of parameters to achieve better accuracy. The proposed method is compared with Support Vector Machine (SVM) and one-dimensional CNN using RAVDESS, TESS and URDU datasets based on accuracy. Based on the conducted experiments, it can be seen that, the proposed method has outperformed with an accuracy of 76.08% and 99.68%, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-00828-3_19,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-5301-8_34,Gender and Age Recognition Using Audio Data—Artificial Neural Networks,Soft Computing for Security Applications,10.1007/978-981-16-5301-8_34,Springer,2022-01-01,"Human voice is a result of movement of air in our vocal cords, and as we grow, different voice features tend to vary according to age and gender. In many criminal cases, the evidence is sometimes collected as voice samples which we can use to identify the speaker’s features like age and gender. Hence, in our model, we are focusing on recognizing age and gender from audio samples using different machine learning algorithms and deep learning models. In the existing models, mostly work has been done using machine learning algorithms, but here we have tried to implement the same along with different dimensionality reduction techniques and deep learning models to figure the best one out of all. The main objective of this project is to identify age and gender by producing a robust feature and to design a good classifier that will improve the accuracy and performance of our system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5301-8_34,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9416-5_39,A Deep Learning Approach for Splicing Detection in Digital Audios,Congress on Intelligent Systems,10.1007/978-981-16-9416-5_39,Springer,2022-01-01,"The authenticity of digital audios has a crucial role when presented as evidence in the court of law or forensic investigations. Fake or doctored audios are commonly used for manipulation of facts and causing false implications. To facilitate passive-blind detection of forgery, the current paper presents a deep learning approach for detecting splicing in digital audios. It aims to eliminate the process of feature extraction from the digital audios by taking the deep learning route to expose forgery. A customized dataset of 4200 spliced audios is created for the purpose, using the publicly available Free Spoken Digit Dataset (FSDD). Unlike the other related approaches, the splicing is carried out at a random location in the audio clip that spans 1–3 s. Spectrograms corresponding to audios are used to train a deep convolutional neural network that classifies the audios as original or forged. Experimental results show that the model can classify the audios correctly with 93.05% classification accuracy. Moreover, the proposed deep learning approach also overcomes the drawbacks of feature engineering and reduces manual intervention significantly.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9416-5_39,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-02444-3_30,Image Registration Between Real Image and Virtual Image Based on Self-supervised Keypoint Learning,Pattern Recognition,10.1007/978-3-031-02444-3_30,Springer,2022-01-01,"A digital twin is a next-generation technology that connects virtual and physical environments into a single world. Although the virtual environment of a digital twin models the real world, the technology used to match the real world with the virtual environment has yet to be studied. The existing deep-learning-based image registration methods aim to extract feature points and descriptors and show a good registration performance in real images. However, these methods are difficult to apply in an actual digital twin environment because 3D and real 2D images have a significant difference in terms of the external and physical characteristics of the image itself. In this paper, we propose a deep learning model that self-learns the difference between virtual and real environments using a generative-adversarial network and self-supervised learning. Image registration between virtual environments with real-world images is a new method that has not been previously achieved, and we have demonstrated experimentally that the proposed method is applicable to various virtual environments and real-world image matching.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-02444-3_30,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-97874-7_21,Application of Data Informatization Deep Learning Technology in 3D Clothing Design,Cyber Security Intelligence and Analytics,10.1007/978-3-030-97874-7_21,Springer,2022-01-01,"Clothing is an important part of human social culture and an important manifestation of human spiritual civilization. In recent years, with the rapid development of society, economy and culture, people’s requirements for clothing not only stay in the aspects of warmth and fit, but also the pursuit of individuality and artistry. In view of the key technologies and research difficulties in 3D digital clothing design, this paper focuses on human body feature recognition and 3D clothing digital modeling.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-97874-7_21,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-99188-3_8,Deep Learning Based Video Compression,Intelligent Technologies for Interactive Entertainment,10.1007/978-3-030-99188-3_8,Springer,2022-01-01,"Our goal is to test the capability of deep learning for compressing the size of video files, e.g., for sending them over digital networks. This is done by extracting keypoint and affine transformation tensors, using a pre-trained face model and then reducing the data by quantization and compression. This minimal information is sent through a network together with full source images used as starting frames for our approach. The receiver device then reconstructs the video with a generator and a keypoint detector, by transforming and animating the keypoints of the source image according to the video keypoints. We minimized the required data by using LZMA2 compression and a quantization factor of 10 000 for keypoints and 1 000 for transformations. Lastly, we determined limitations of this approach and found that in regard to file size reduction, our approach was noticeably better, while the quality of the resulting video in comparison to the original one was only half as good.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-99188-3_8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-4103-9_7,Deep Learning for Video Based Human Activity Recognition: Review and Recent Developments,Proceedings of International Conference on Computational Intelligence and Emerging Power System,10.1007/978-981-16-4103-9_7,Springer,2022-01-01,"The enormous applicability of advanced human-computer interaction technology in daily life has facilitated the interest of the researchers toward the development of more intelligent autonomous systems. These human-computer interaction systems can be successful in real-life by addressing the gap among the existing techniques. This research work focuses on one of the prominent human-computer interaction applications of human activity recognition. Human activity recognition (HAR) is the process to detect human gestures, actions, and different types of interactions. The HAR process requires competent knowledge about the day to day human activities and advanced technology to recognize their activities. The conventional pattern recognition techniques are significant to recognize the human activities using the machine learning techniques but only within the controlled environment for the recognition of limited actions. In recent years, deep learning techniques are developed that can learn the deep attributes of the problem application and determine the outcomes with promising performance. The present paper has presented a systematic review of the deep learning models for video-based human activity recognition. The work describes the recent developments in the field for the analysis of different models. The paper also discusses the process of human activity recognition and the eminent datasets available for experimentation. The summarization of the work is illustrated with the future directions in the field.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-4103-9_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-5157-1_25,Analysis of Car Damage for Personal Auto Claim Using CNN,Sentimental Analysis and Deep Learning,10.1007/978-981-16-5157-1_25,Springer,2022-01-01,"In the world of vehicle insurance and rental industries, detecting damages on vehicles is one of the most important activities. These damages are identified and inspected by the drivers and insurance companies to determine the suitable monetary compensation, and by vehicle rental companies to assign the responsibility to guilty customers. Since the current system is time consuming, wherein the inspectors have to manually inspect the damages before appraising, this identification can be performed by object recognition systems. The intricacy of these systems rests in the image feature determination and extraction techniques. A more novel approach to detecting the severity of damage and predicting the repair costs is using 2D image recognition. This way, the driver would not have to wait for the appraisal of insurance companies to determine the rough estimate of cost of repairs. After a picture is uploaded to the framework, the picture is then processed and the vehicular damage is identified. The image is then classified into relevant damage severity classes. Afterward, the damage severity that is detected in the image, is then mapped to the approximate cost values. Then finally, the user is presented with a report of the vehicular damage severity classification as well as an average expense cost report from which the vehicle can then be recuperated from the damages.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5157-1_25,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-96908-0_111,Design and Implementation of English Speech Scoring Data System Based on Neural Network Algorithm,Cyber Security Intelligence and Analytics,10.1007/978-3-030-96908-0_111,Springer,2022-01-01,"With the improvement of the performance of the Internet and other mobile terminals, English speech scoring technology based on neural network algorithms has become more and more favored by business development and daily life. Therefore, how to provide a voice scoring data system with high recognition performance and strong robustness has become more and more obvious, and it is also particularly important. The purpose of this paper is to study the design of English speech scoring data system based on neural network algorithm. This article makes full use of the freedom of neural network learning and the adaptability of multiple data domains. This paper has developed a set of data system for scoring spoken English. Students can use the system to simulate film dubbing, and the system can evaluate the multi-index data in the recordings submitted by the students. Experimental research shows that the improved BP algorithm used in this paper has a higher oral English recognition rate than the traditional BP algorithm by about 10%, and the optimized algorithm training performance and recognition rate are significantly better than the traditional algorithm.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96908-0_111,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-98253-9_4,Automatic Segmentation of Head and Neck (H&N) Primary Tumors in PET and CT Images Using 3D-Inception-ResNet Model,Head and Neck Tumor Segmentation and Outcome Prediction,10.1007/978-3-030-98253-9_4,Springer,2022-01-01,"In many computer vision areas, deep learning-based models achieved state-of-the-art performances and started to catch the attention in the context of medical imaging. The emergence of deep learning is significantly cutting-edge the state-of-the-art in medical image segmentation. For generalization and better optimization of current deep learning models for head and neck segmentation problems, head and neck tumor segmentation and outcome prediction in PET/CT images (HECKTOR21) challenge offered the opportunity to participants to develop automatic bi-modal approaches for the 3D segmentation of Head and Neck (H&N) tumors in PET/CT scans, focusing on oropharyngeal cancers. In this paper, a 3D Inception ResNet-based deep learning model (3D-Inception-ResNet) for head and neck tumor segmentation has been proposed. The 3D-Inception module has been introduced at the encoder side and the 3D ResNet module has been proposed at the decoder side. The 3D squeeze and excitation (SE) module is also inserted in each encoder block of the proposed model. The 3D depth-wise convolutional layer has been used in 3D inception and 3D ResNet module to get the optimized performance. The proposed model produced optimal Dice coefficients (DC) and HD95 scores and could be useful for the segmentation of head and neck tumors in PET/CT images. The code is publicly available ( https://github.com/RespectKnowledge/HeadandNeck21_3D_Segmentation ).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98253-9_4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-021-02494-y,3D spatial priors for semi-supervised organ segmentation with deep convolutional neural networks,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-021-02494-y,Springer,2022-01-01,"Purpose Fully Convolutional neural Networks (FCNs) are the most popular models for medical image segmentation. However, they do not explicitly integrate spatial organ positions, which can be crucial for proper labeling in challenging contexts. Methods In this work, we propose a method that combines a model representing prior probabilities of an organ position in 3D with visual FCN predictions by means of a generalized prior-driven prediction function. The prior is also used in a self-labeling process to handle low-data regimes, in order to improve the quality of the pseudo-label selection. Results Experiments carried out on CT scans from the public TCIA pancreas segmentation dataset reveal that the resulting STIPPLE model can significantly increase performances compared to the FCN baseline, especially with few training images. We also show that STIPPLE outperforms state-of-the-art semi-supervised segmentation methods by leveraging the spatial prior information. Conclusions STIPPLE provides a segmentation method effective with few labeled examples, which is crucial in the medical domain. It offers an intuitive way to incorporate absolute position information by mimicking expert annotators.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-021-02494-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-67241-6_15,Small Parts Recognition by Convolutional Neural Networks with Implementation to Virtual Reality Devices for Assisted Assembly Tasks,5th EAI International Conference on Management of Manufacturing Systems,10.1007/978-3-030-67241-6_15,Springer,2022-01-01,"The paper presents a new approach to recognize small parts from high-resolution 4K images grabbed by the embedded camera with trained convolutional network models. The main principle is a combination of standard machine vision techniques (OpenCV library) to identify blobs in the high-resolution image and parse to a suitable set of regions of interest, which are later recognized by trained convolutional neural network model (Tensorflow framework). The convolutional neural network model with single-shot detection was selected to identify the position of the object. 3D virtual models of parts with autogenerated orientations are used for simpler input data generation. Two embedded devices were selected for use in the experimental system. The first is NVidia Xavier dev board with 4K camera and integrated APU and the second is Raspberry PI 4 with external APU Intel Neural Compute Stick 2. The last part of the paper shows the implementation of the presented principle to the virtual device (HTC Vive Pro) for assisted assembly tasks to train and check workers during the teaching process.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-67241-6_15,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-87059-1_1,Artificial Intelligence-based Internet of Things for Industry 5.0,Artificial Intelligence-based Internet of Things Systems,10.1007/978-3-030-87059-1_1,Springer,2022-01-01,"In Industry 5.0 paradigm, AI-based systems are the essential component for the Internet of Things. In most applications, Industry 5.0 showed a significant connection between intelligent systems and humans through accurate manufacturing automation with critical thinking skills. Also, Industry 5.0 brings several competent tools that help organizations work inexpensively and instantly change without any principal investment. In recent years, smart devices, wireless communication, and sensor nodes have made considerable advances, making the Internet of Things (IoT) ecosystems. The Internet of Things (IoT) is a network of interconnected, Internet-connected devices that can capture and transmit data without the need for human interaction over a wireless network. With the introduction of IoT, human beings take some relaxed and unburden feelings. IoT devices enable users to obtain information even in rural surroundings and prepare reports without restrictions. In addition, they accurately guide human beings with intelligent judgments via communication, as mentioned earlier, technologies. Numerous connected devices collect a considerable quantity of raw sensed data, where it needs pre-processing. However, only it turns into something valuable because of IoT devices’ adequate resources entailed to edge computing. Artificial intelligence-based algorithms are the necessary means for information inference in edge computing. Besides, the sensed data accumulated from IoT applications are generally unstructured and need further analysis, where AI-based models help extract relevant information. Furthermore, transmitting data from device to device, there is a chance for malicious attacks. Hence, this chapter explores Industry 5.0, IoT architecture, and AI-based IoT; we analyze IoT network’s technical details; communication enables technologies. Then we discussed various AI-based technologies integrated into IoT, AI-based tools, edge computing, and trust models for IoT appliances.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-87059-1_1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-0739-4_83,Machine Learning-Based Security Authentication for Wireless Multimedia Network,Information and Communication Technology for Competitive Strategies (ICTCS 2020),10.1007/978-981-16-0739-4_83,Springer,2022-01-01,"For supporting various services in wireless multimedia networks, security is one of the major concerns. The traditional upper-layer authentication system and conventional security solution lack efficiency and does not solve the emerging security problems and ignores physical layer protection. To overcome these challenges, this paper presents a lightweight physical layer authentication system developed by understanding certain physical layer attributes in the wireless multimedia network through supervised machine learning algorithm. Support vector classifier (SVC) is used for classification. Classification can also be performed by other supervised algorithm and select based on each detection rate. Experimental analysis proved that the proposed system could protect wireless multimedia networks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-0739-4_83,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-1476-7_23,Detection of Imagery Vowel Speech Using Deep Learning,Advances in Energy Technology,10.1007/978-981-16-1476-7_23,Springer,2022-01-01,"The recent advances in brain–computer interface (BCI) systems have resulted in successful implementations of multiple application domains using different paradigms. The imaginary speech paradigm has received less focus when compared to other paradigms. This study proposes deep learning models to classify and detect the imaginary pronunciations of vowels ‘a’ and ‘u’ using EEG data. The data was captured from three subjects with no psychological or neurological disorders. Channels Fz, Cz, C3, and C4 were used for analyzing the data. A bandpass filter with a frequency range of 1–45 Hz was used to filter the data. Three deep learning models are proposed in this study. These models are the convolution neural network (CNN) model, the long short-term memory (LSTM) model, and a model based on the combination of CNN and LSTM. The model containing combinations of CNN and LSTM performed best with the average classification accuracy of 84%. The CNN model and LSTM models yielded average classification accuracies of 51% and 63%, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-1476-7_23,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-658-36295-9_6,Deep Learning Frameworks Applied For Audio-Visual Scene Classification,Data Science – Analytics and Applications,10.1007/978-3-658-36295-9_6,Springer,2022-01-01,"In this paper, we present deep learning frameworks for audio-visual scene classification (SC) and indicate how individual visual, audio features as well as their combination affect SC performance. Our extensive experiments are conducted on DCASE 2021 (IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events) Task 1B Development and Evaluation datasets. Our results on Development dataset achieve the best classification accuracy of 82.2%, 91.1%, and 93.9% with audio input only, visual input only, and both audio-visual input, respectively. The highest classification accuracy of 93.9%, obtained from an ensemble of audio-based and visual-based frameworks, shows an improvement of 16.5% compared with DCASE 2021 baseline. Our best results on Evaluation dataset is 91.5%, outperforming DCASE baseline of 77.1%",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-658-36295-9_6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-6285-0_1,CNN Based Feature Extraction for Visual Speech Recognition in Malayalam,Proceedings of Data Analytics and Management,10.1007/978-981-16-6285-0_1,Springer,2022-01-01,"Bhaskar, Shabina Thasleema, T. M. Visual speech recognition is the technique of recognizing speech by using visual cues obtained during speech. The current research in this area makes use of visual cues such as mouth or lip movement for the recognition of speech. This paper introduced a method of visual speech recognition from the face by giving importance to the facial expressions while a person speaking. Facial expression is an important feature for hearing impaired speech recognition because they are more expressive while speaking. As an initial study in this area, we have introduced a Malayalam audio-visual speech emotion database of hearing people. The experimental studies in this database prove that facial expressions play a pivotal role in visual speech recognition.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-6285-0_1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-0739-4_43,Early Childhood Anxiety and Depression Detection Based on Speech Using Machine Learning Analysis,Information and Communication Technology for Competitive Strategies (ICTCS 2020),10.1007/978-981-16-0739-4_43,Springer,2022-01-01,"Uneasiness and wretchedness in youngsters habitually go undiscovered. On the off chance that these conditions are not treated at the ideal time, generally known as disguising issues, they bring about long-haul negative results like medication misuse and expanded self-destruction hazard. This paper gives an appropriate technique to perceive small kids with incapacities being disguised utilizing a three-minute discourse task. We present that AI examination of sound information from the errand can be utilized to characterize youngsters with an incapacity that disguises. The most significant attributes of disguising issue are concentrated inside and out, indicating that influenced youngsters have low-pitch voices, with repeatable discourse emphases and content, and a shrill reaction to startling control improvements. The current procedure relies upon clinician’s audit of the patient. These techniques are abstract, done on meet and rely upon reports by the patient. With increment in the downturn, some programmed and dependable methods for gloom acknowledgment are required. Endeavors are being made to survey and distinguish despondency through PC vision and AI. Subsequently, in this paper, we present our techniques for text, sound, and video highlight extraction, trailed by choice level combination which helps in recognizing discouraged kids with exactness.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-0739-4_43,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9605-3_61,Comparison of Machine Learning Algorithms for Hate and Offensive Speech Detection,Evolutionary Computing and Mobile Sustainable Networks,10.1007/978-981-16-9605-3_61,Springer,2022-01-01,"Hate speech is not uncommon and is likely practiced almost on every networking platform. In recent times, due to exponential increase in Internet users and events such as the unprecedented pandemic and lockdown, it showed increased usage of social platforms for communicating thoughts, opinions, and ideas. Hate speech has a strong impact on people’s lives and is one of the reasons for suicidal events. There is certainly a strong need to make progress toward the mitigation of hate speech. Detection is the primary step to eradicate hate speech. In the following paper, the comparative analysis of different machine learning algorithms to detect hate speech was shown. Data from the Twitter social platform was considered. From the analysis, it was found that the long short-term memory method is a highly performant machine learning algorithm.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9605-3_61,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0745-6_10,Comparative Study on Sentiment Analysis of Human Speech using DNN and CNN,Topical Drifts in Intelligent Computing,10.1007/978-981-19-0745-6_10,Springer,2022-01-01,"Communication is a way of exchanging thoughts through emotions. In this paper, we have proposed a method where human speech is converted into digital input. The digitized sound is then fed into the proposed models, and the voice of every person is classified into discrete emotional characteristics by its pitch, intensity, timbre, speech rate, and pauses. In the proposed method, we have drawn a comparative study between sentiment analysis of human speech using deep convolutional neural network (CNN) and dense deep neural networks (DNNs). In this method, multiscale area attention is applied in deep CNN as well as dense DNN to obtain emotional characteristics with wide range of granularities and therefore, the classifier can predict a wide range of emotions on a broad scale classification.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0745-6_10,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-78307-5_14,Data-Driven Artificial Intelligence and Predictive Analytics for the Maintenance of Industrial Machinery with Hybrid and Cognitive Digital Twins,Technologies and Applications for Big Data Value,10.1007/978-3-030-78307-5_14,Springer,2022-01-01,"This chapter presents a Digital Twin Pipeline Framework of the COGNITWIN project that supports Hybrid and Cognitive Digital Twins, through four Big Data and AI pipeline steps adapted for Digital Twins. The pipeline steps are Data Acquisition, Data Representation, AI/Machine learning, and Visualisation and Control. Big Data and AI Technology selections of the Digital Twin system are related to the different technology areas in the BDV Reference Model. A Hybrid Digital Twin is defined as a combination of a data-driven Digital Twin with First-order Physical models. The chapter illustrates the use of a Hybrid Digital Twin approach by describing an application example of Spiral Welded Steel Industrial Machinery maintenance, with a focus on the Digital Twin support for Predictive Maintenance. A further extension is in progress to support Cognitive Digital Twins includes support for learning, understanding, and planning, including the use of domain and human knowledge. By using digital, hybrid, and cognitive twins, the project’s presented pilot aims to reduce energy consumption and average duration of machine downtimes. Data-driven artificial intelligence methods and predictive analytics models that are deployed in the Digital Twin pipeline have been detailed with a focus on decreasing the machinery’s unplanned downtime. We conclude that the presented pipeline can be used for similar cases in the process industry.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-78307-5_14,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-96308-8_63,Combining Bert Representation and POS Tagger for Arabic Word Sense Disambiguation,Intelligent Systems Design and Applications,10.1007/978-3-030-96308-8_63,Springer,2022-01-01,"In this paper, we address the problem of Word Sense Disambiguation of Arabic language where the objective is to determine an ambiguous word’s meaning. We model the problem as a supervised sequence to sequence learning. We propose recurrent neural network models, BERT based models and combined POS-BERT models for Arabic word sense Disambiguation. We achieve 96.33% accuracy with the POS-BERT system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96308-8_63,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-91479-0_1,Introduction,Deep Learning Based Speech Quality Prediction,10.1007/978-3-030-91479-0_1,Springer,2022-01-01,"This chapter provides a brief introduction to subjective and objective speech quality assessment. At first, the work on deep learning based speech quality prediction models is motivated with a summary of current state-of-the-art speech quality prediction models and their drawbacks. The two objectives and four research questions that are further investigated in this book are then presented with a following outline of the book.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-91479-0_1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9488-2_30,Hate and Offensive Language Identification from Social Media: A Machine Learning Approach,Electronic Systems and Intelligent Computing,10.1007/978-981-16-9488-2_30,Springer,2022-01-01,"Social media has evolved into a valuable tool for disseminating information while also ensuring that everyone on the site has complete freedom of expression. This ability of social media to openly express oneself to the world has resulted in major issues such as hate speech, cyberbullying, aggression, and general profanity, all of which harm society's well-being. It's critical to pay attention to these issues as it has a significant impact on an individual's life and, in some situations, be suicidal, negatively impacting the victim's mental health. Our key aim is that our approach will automate and quicken the detection of offensive content that has been posted, making it easier to take appropriate steps and moderate these offenses. In this work, we have implemented several popular machine learning classifiers with character N-gram TF-IDF features. Along with this, we have also proposed dense neural network (DNN) and convolutional neural network (CNN) models for the identification of hate speech. In the case of DNN, character N-gram TF-IDF features wherein CNN, word embedding features are used. In the extensive experiments, we found AdaBoost classifiers performed best with the weighted F1-score of 0.86 in the identification of hate speech.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9488-2_30,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-91479-0_8,Conclusions,Deep Learning Based Speech Quality Prediction,10.1007/978-3-030-91479-0_8,Springer,2022-01-01,"In this chapter the book is concluded by answering the four research questions and evaluating the book’s objectives. To answer the first research question, it was investigated which neural network architectures give the best performance for speech quality prediction. The second research question deals with double-ended speech quality prediction and how the clean reference can be incorporated into the neural network. To answer the third research question, it was analysed whether multi-task learning with speech quality dimensions improves the prediction performance. Lastly, it was answered how the model can be trained from multiple datasets that are exposed to subjective biases. Finally, the results of the presented speech quality model NISQA are discussed and potential future work is outlined.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-91479-0_8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-7466-2_101,Large Data Recognition System for Speech Outliers Based on Deep Learning,2021 International Conference on Big Data Analytics for Cyber-Physical System in Smart City,10.1007/978-981-16-7466-2_101,Springer,2022-01-01,"In order to effectively improve the recognition efficiency of large data speech outliers, a large data recognition system for speech outliers is designed based on deep learning theory. With the recognition hardware developed by Stratix IV GX series FPGA as the core, a speech recognition device is designed to parse large data network voice data. Based on the characteristics of traditional classifier and voice data, the traditional deep trust network is improved. The output layer of BP neural network is changed to KNN nearest neighbor classifier. The voice classification network is designed. The input vector and corresponding input link of voice data are set up. The data in the voice network are read into the system, and the deep learning recognition is established with the restriction of Boltzmann machine unit as the core. The system realizes the recognition of large data of speech outliers through the training of nerve layer. The experimental data show that, compared with the traditional speech big data recognition system, the system has higher recognition efficiency, the rate of increase is more than 35%, which meets the requirements of the current environment of large data explosion, and is feasible.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7466-2_101,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ReferenceWorkEntry,doi:10.1007/978-3-030-73206-6_26,NDE 4.0: Image and Sound Recognition,Handbook of Nondestructive Evaluation 4.0,10.1007/978-3-030-73206-6_26,Springer,2022-01-01,"Advances powered by Artificial Intelligence (AI) centric technologies have enveloped nearly every aspect of our lives. Of the many aspects of AI, seven patterns have been classified, with the most common being the recognition pattern (Walch, Kathleen-Contributor, Cognitive Word-Contributor Group. September 17, 2019. The seven patterns of AI. https://www.forbes.com/sites/cognitiveworld/2019/09/17/the-seven-patterns-of-ai/?sh=71056b2b12d0 ) . This chapter focuses on pattern recognition with a subset emphasis on image and sound as it may relate to NDE 4.0. Optical character recognition (OCR) leveraged image recognition for the past decade in document conversion and computer-assisted check deposit may present precedence for AI-assisted flaw detection systems for radiographic images. Computer vision (CV) is the base building block for extraction of data from an image and can recognize objects using algorithms and machine learning concepts (Brownlee, Jason. May 22, 2019 (updated January 27, 2021). Deep Learning for Computer Vision. A Gentle Introduction to Object Recognition with Deep Learning. https://machinelearningmastery.com/object-recognition-with-deep-learning/ ). Computer vision has been integral in detection, segmentation, classification, monitoring, and prediction of radiographs in the medical community and has applicability in visual and radiographic inspection in industry and the NDE community. Sound recognition has a large portion of defect formations and flaw mechanical movements release energy in the method of elastic waves with the broad frequency spectrum. Typically, these signals are digitized and converted into amplitude time series. Regardless of their frequency content, these digital acoustics, or sound, signals can be analyzed and classified by any method that applies to time series data, including those developed specifically for audible sound signals such as deep learning algorithms.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-73206-6_26,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ReferenceWorkEntry,doi:10.1007/978-3-030-64573-1_188,AIM in Dermatology,Artificial Intelligence in Medicine,10.1007/978-3-030-64573-1_188,Springer,2022-01-01,"In medicine, dermatology is a promising pioneer for the use of artificial intelligence (AI). In dermatological practice, the recognition of visual patterns (morphology) has always been fundamental for making a diagnosis, so artificial intelligence has great potential here. The collection of clinical data, especially image data, is playing an increasingly important role in the diagnosis and therapy of skin disease. Existing analog data archives are being digitized and restructured with great effort, and new data sets are often captured and labeled directly in digital form. During the last years, a growing number of studies have demonstrated AI’s benefits in research settings, and first applications are already used clinically. Particularly in the detection of skin cancer and for the quantification of chronic inflammatory skin diseases, artificial intelligence is supporting doctors as well as patients to find the right diagnosis and treatment. The purpose of this book chapter is to discuss the potential applications of artificial intelligence in various areas of dermatology.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-64573-1_188,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-10464-0_11,A Review of Unsupervised Machine Learning Frameworks for Anomaly Detection in Industrial Applications,Intelligent Computing,10.1007/978-3-031-10464-0_11,Springer,2022-01-01,"Unsupervised learning, also known as unsupervised machine learning, analyzes and clusters unlabeled data utlizing machine learning techniques. Without human input, these algorithms discover patterns or groupings in the data. In the domain of abuse and network intrusion detection, interesting objects are often short bursts of activity rather than rare objects. Anomaly detection is a difficult task that requires familiarity and a good understanding of the data and the pattern does not correspond to the common statistical definition of an outlier as an odd item. The traditional algorithms need data preparations while unsupervised algorithms can be prepared so that they can handle the data in war format. Anomaly detection, sometimes referred to as outlier analysis is a data mining procedure that detects events, data points, and observations that deviates from the expected behaviour of a dataset. The unsupervised machine learning approaches have shown potential in static data modeling applications such as computer vision, and their use in anomaly detection is gaining attention. A typical data might reveal critical flaws, such as a software defect, or prospective possibilities, such as a shift in consumer behavior. Currently, academic literature does not really cover the topic of unsupervised machine learning techniques for anomaly detection. This paper provides an overview of the current deep learning and unsupervised machine learning techniques for anomaly detection and discusses the fundamental challenges in anomaly detection.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-10464-0_11,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8826-3_9,Human Fall Detection Analysis with Image Recognition Using Convolutional Neural Network Approach,Proceedings of Trends in Electronics and Health Informatics,10.1007/978-981-16-8826-3_9,Springer,2022-01-01,"Human falling may cause injuries and sometimes may lead to deadly conditions. Therefore, in recent decade, the systems used for monitoring of human falling and non-falling are receiving attention among research community for its diversified features and social benefits. These systems solve the problem of falling and gets activated to avert the likely incident with an alarm message, and uses fall recognition classifiers. System helps to identify the human in the intended regions, and classifiers are trained using the information available in the images. The lack of massive scale datasets and human errors limits the generalization of models in terms of robustness and efficiency to invisible regions. In the proposed work, an automatic fall detection using deep learning is modeled using dataset of falling and non-falling images. The sensitive information available in the original images is kept secure and private to maintain the safety and protection by the presented work. The experiments were conducted using real-world fall datasets having both types of human images, i.e., falling and non-falling, and the results obtained clearly indicate system enhancement for falling and non-falling image recognition using convolutional neural network (CNN) algorithm and achieving higher accuracy and reduced loss with a trained dataset which finds the optimal performance from real-time environments.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8826-3_9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-89508-2_82,Handwritten Font Image Design System Based on Deep Learning Algorithm,The 2021 International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy,10.1007/978-3-030-89508-2_82,Springer,2022-01-01,"Deep learning technology is an important sub-discipline in the field of machine learning technology today, and the predecessor of its development is neural networks. In recent years, due to huge breakthroughs in information technology, deep learning has achieved good development results in computer vision, natural language processing, machine translation, unmanned driving, and artificial intelligence technology closely related to robotics. Like other deep learning fields, it has to face the two problems of overfitting and computational efficiency. This article aims to study a handwritten font image design system based on deep learning algorithms. Based on the analysis of factors affecting recognition rate, the concept of deep learning, the classification of deep learning algorithms, and the classification of recognition results, a handwriting based on convolutional neural network is constructed. The font recognition network model is trained on the built model through the MNIST data set, and the model is subjected to standardized recognition tests on the test set created by me. Experimental test results show that the system design basically meets the practical requirements of handwritten font recognition.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-89508-2_82,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-2422-3_41,Comprehensive View of Low Light Image/Video Enhancement Centred on Deep Learning,Intelligent Sustainable Systems,10.1007/978-981-16-2422-3_41,Springer,2022-01-01,"In computer vision, enhancing the image/video quality is strongly influenced by darkness and the noise is considered as an important challenge. Currently, different methods are used for performing image/video enhancement. However, deep learning-based methodologies have introduced huge changes in image/video quality improvement. This paper introduces more classic image/video enhancement network in recent years. Additionally, the elementary understanding of CNN components, current challenges, comparison of traditional enhancement with deep learning methods along with their benefits and drawbacks are analysed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-2422-3_41,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-94182-6_21,Fall Behavior Recognition Algorithm in Video Surveillance Based on Feature and Deep Learning,IoT and Big Data Technologies for Health Care,10.1007/978-3-030-94182-6_21,Springer,2022-01-01,"Aiming at the problem of low recognition rate and accuracy rate of human fall behavior recognition algorithm in current video surveillance, the human behavior feature extraction module is relatively backward. To solve this problem, a fall behavior recognition algorithm based on feature and deep learning is designed. The video image preprocessing is completed by dilation and erosion. The covariance matrix of image features is constructed to extract the features of human fall behavior. The standard image database is constructed, and the deep learning algorithm and neural network are used to complete the human fall behavior recognition in video surveillance. So far, the design of human fall behavior recognition algorithm in video surveillance based on feature and deep learning is completed. Through the example test, the application effect of the feature algorithm is better than that of the traditional algorithm.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-94182-6_21,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9709-8_20,Characterizing the Performance of Deep Learning Inference for Edge Video Analytics,Big Data,10.1007/978-981-16-9709-8_20,Springer,2022-01-01,"Real-time video analytics is a killer application for edge computing, however, it has not been fully understood how much edge resource is required to support compute-intensive video analytics tasks at the edge. In this paper, we conduct an in-depth measurement study to unveil the resource requirement of deep learning (DL) inference for edge video analytics (EVA). We measure the DL inference time under various resource combinations, with different physical resources (e.g., processor types, core numbers, memory), video resolutions, and DL inference models. It is observed that the relationship between DL inference time and resource configuration is complicated, which cannot be captured by a simple model. Considering the coupling effects, we model the resource requirements of the DL inference by utilizing tensor factorization (TF) method. Our TF-based DL inference model can match well with the observations from field measurements. Different from previous models, our model is completely explainable and all the components have their physical meanings. Especially, it is possible to match the top three components extracted from the TF-based model with the practical DL functional layers, namely, convolutional layer, pooling layer, and fully-connected layer. The measurement results show that the top three components contribute approximately 50%, 25%, and 10% time for DL inference execution, which provides clear instruction on fine-grained DL task scheduling for edge video analytics applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9709-8_20,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-86165-0_18,Feature Extraction by Rework Image Recognition (RIR) Learning Model,"International Conference on Computing, Communication, Electrical and Biomedical Systems",10.1007/978-3-030-86165-0_18,Springer,2022-01-01,"Image recognition is a widely adopted feature extraction technique in the field of deep learning. Convolutional neural network is one of the algorithms of deep learning used in the field of image analysis. Most of the software under development are prone to fault because of code moderation, so rework-related feature of software development needs to maintain fault-related dataset for future prediction. Rework is the major problem related to time and cost estimation after the software delivery. This paper is used to identify the rework effort of Organization-A, which is to be used for finding rework effort estimation in future projects. In most of the organization, historical dataset can be available in either document format or image format. This paper proposed a new model called RIR (Rework Image Recognition) learning model, which is used to generate the CSV file as the result of extracting the feature from Rework report image of an Organization-A. The model is generated by using a deep learning algorithmic technique called convolutional neural network. This paper also shows the performance of the RIR (Rework Image Recognition) learning model in the base of accuracy and loss level, and the future use of the rework result is also presented.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-86165-0_18,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-85365-5_17,Robust Model for Rural Education Using Deep Learning and Robotics,"Advances in Deep Learning, Artificial Intelligence and Robotics",10.1007/978-3-030-85365-5_17,Springer,2022-01-01,"Rural Education is important for the overall development of villages. National Achievement Survey (NAS) has surveyed and reported in many States of India consistent decline in the learning levels of students in mathematics, language and science from class III to class VIII studying in the government school system. Smart Villages is only possible if the literacy level and infrastructure improves considerably. This paper aims to perform a reality check of the situation by comparing different rural areas of various countries including India and study of related work done. The paper proposes a Robust Model for Rural Education by developing an intelligent humanoid robot using the Deep Learning approach, Human recognition, Object Recognition and Speech Recognition. The data set consists of Primary and Secondary Student data of around 10,000 Students (5 years) from 5 villages. The Proposed Model would be compared with existing models on the parameters of Learnability, Decision making, Flexibility and Cost-effectiveness. The implementation of this Model will help in decreasing the drop out rate, evaluate Students and give them a Learning platform based on their characteristics, increase adaptive and self paced learning. This Model can also be executed for Rural Adult Education and Skill building so that the Smart Village concept can become a reality.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85365-5_17,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-4369-9_56,Mycobacterium Tuberculosis Detection Using CNN Ranking Approach,Advanced Computational Paradigms and Hybrid Intelligent Computing,10.1007/978-981-16-4369-9_56,Springer,2022-01-01,"The main reason behind the large number of mortality among homo-sapiens is due to Mycobacterium tuberculosis (TB). Majorly, TB can be classified into two categories named as active and latent tuberculosis. Tuberculosis diagnosis is a critical area and needs a high extent of accuracy for working. Minor errors in diagnosis can result in disastrous repercussions. The ultimate goal of this paper is to detect TB at right time and help combat the increasing number of cases by early treatment. In this paper, we implement CNNs of different configurations on a dataset for binary classification and mainly focus on the objective function value obtained from different CNN models with an LVCEL, MVA, and TT. The objective function can be a useful tool for physicians and our medical community for correctly identifying a TB patient. Experimental evaluation of the best architecture shows that a maximum objective function value of 6.503, with a validation accuracy score of 0.9671 and an AUC of 0.9733 in the receiver-operating characteristics (ROC) curve, is achieved to correctly identify whether TB is present or not.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-4369-9_56,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-05039-8_12,Integrating Machine Learning with Augmented Reality for Accessible Assistive Technologies,Universal Access in Human-Computer Interaction. User and Context Diversity,10.1007/978-3-031-05039-8_12,Springer,2022-01-01,"Augmented Reality (AR) is a technology which enhances physical environments by superimposing digital data on top of a real-world view. AR has multiple applications and use cases, bringing digital data into the physical world enabling experiences such as training staff on complicated machinery without the risks that come with such activities. Numerous other uses have been developed including for entertainment, with AR games and cultural experiences now emerging. Recently, AR has been used for developing assistive technologies, with applications across a range of disabilities. To achieve the high-quality interactions expected by users, there has been increasing integration of AR with Machine Learning (ML) algorithms. This integration offers additional functionality to increase the scope of AR applications. In this paper we present the potential of integrating AR with ML algorithms for developing assistive technologies, for the use case of locating objects in the home context.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-05039-8_12,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-95419-2_9,A Deep Analysis on the Role of Deep Learning Models Using Generative Adversarial Networks,Blockchain and Deep Learning,10.1007/978-3-030-95419-2_9,Springer,2022-01-01,"A comparatively novel advance field of deep learning is the Generative Adversarial Network called GAN. These different types of networks if they start working in line with each other and work not to get the better of each other but start working keeping arm to arm connected to each other's world will be different. GAN is a category of machine learning frameworks intended for generation of images in which neural networks challenge each other in format of playing game. One network generates metaphors also called the generator and an additional network attempt to differentiate between the fake and real from the data set called as the discriminator. If suppose a training set is presented this procedure guides to manufacture a innovative information comparable to training set. Pictures created from GAN are same metaphors giving the notion of genuine to any observer having real features. GAN networks work on supervised, unsupervised and for reinforcement learning but earlier it applies or used only unsupervised learning only. This generative network produces candidate and on the other hand, the discriminative network evaluate them. Here producer is a complex neural network and differentiator is a convolution neural network. The GAN network divided into three categories to produce generative model learns and generation of data by probabilistic ideas. Next training of model can be completed in contradictory state. Lastly for training using neural networks, deep learning, and artificial intelligence methods. If generative networks used deep leaning methods then deep learning models can employ a very large amount of dataset, heavily dependent on high-end Machines, tries to solves problem from end to end machines, takes longer time to train means the results are better after getting trained on the other hand takes lesser time to test the data. Applications of GAN networks are increasing day by day as it is touching every sphere of our day today life. Some of the benefits of the deep learning are to creating artificial Intelligence function which mimics the mechanism of human brain in handing out data for decision making. Deep learning if combined with artificial intelligence can be capable of learning data which is considered unlabeled and unstructured. The chapter will relate different models of deep learning and their efficiency can be measured by studying different methods, models and simulation techniques.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95419-2_9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-91479-0_3,Neural Network Architectures for Speech Quality Prediction,Deep Learning Based Speech Quality Prediction,10.1007/978-3-030-91479-0_3,Springer,2022-01-01,"In this chapter, different neural network architectures are investigated for their suitability to predict speech quality. The neural network architectures are divided into three different stages. First, a frame-based neural network calculates features for each time step. The resulting feature sequence is then modelled by a time-dependency neural network. Finally, a pooling stage aggregates the sequence of features over time to estimate the overall speech quality. It will be shown that the combination of a CNN for per-frame modelling, a self-attention network for time-dependency modelling, and an attention-pooling network for pooling yields the best overall performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-91479-0_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-90087-8_21,Detecting Fake News on COVID-19 Vaccine from YouTube Videos Using Advanced Machine Learning Approaches,Combating Fake News with Computational Intelligence Techniques,10.1007/978-3-030-90087-8_21,Springer,2022-01-01,"Fake news is considered a massive threat to many internet users, with the heavy usage of social media networks. Many news agencies develop their platforms to publish and share their news articles. Also, an ordinary user on the social media network has an account where the content can be posted and shared. Some users share fake news or rumors to achieve personal goals and benefits. Fake news is considered to be the most visible challenge on social media networks. It creates a threat to individuals and society while creating a negative impact. Many research works tackle this issue using propagation-based, content-based, and meta-data analysis approaches. This book chapter proposes a model to detect fake news about the COVID-19 vaccine on YouTube videos using a sentiment analysis approach through machine learn and deep learning approaches focusing on the Arabic language of middle-east people. The process started with building a dataset through the collected textual data using the comments that were later annotated into two classes. They are fake and real news. Two experiments have been conducted using machine learning classifiers and deep learning models. Through these experiments, the performance level of the model has reached 94% in terms of accuracy. In the deep learning approach, it has reached 99%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-90087-8_21,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-85540-6_103,Machine Learning and Digital Twin for Production Line Simulation: A Real Use Case,"Human Interaction, Emerging Technologies and Future Systems V",10.1007/978-3-030-85540-6_103,Springer,2022-01-01,"The advent of Industry 4.0 has boosted the usage of innovative technologies to promote the digital transformation of manufacturing realities, especially exploiting the possibilities offered by cyber physical systems and virtual environments (VEs). Digital Twins (DTs) have been widely adopted to virtually reproduce the physical world for training activities and simulations, and today they can also leverage on the integration of Machine Learning (ML), which is considered a relevant technology for industry 4.0. This paper investigates the usage of a combination of DT and ML technologies in the context of a real production environment, specifically on the creation of a DT enhanced with YOLO (You only look once), a state-of-the-art, real-time object detection algorithm. The ML system has been trained with synthetic data automatically generated and labelled and its performance enables its usage in the VE for real-time users training.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85540-6_103,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-96429-0_17,Machine Learning for Green Smart Video Surveillance,Computational Intelligence Techniques for Green Smart Cities,10.1007/978-3-030-96429-0_17,Springer,2022-01-01,"The forthcoming smart environments will have advanced technology for capturing, processing, and analysing high-resolution and high-quality visual information. In future green smart cities, advanced video surveillance with dense deployment of video sensors is definitely necessary to build intelligent and secure urban environments. Since large-scale compression of high-quality video with low bit rate is computationally expensive, a significant impact on energy consumption is inevitable, and green computing approaches become increasingly relevant. The best candidate for video compression is the recently approved Versatile Video Coding (VVC) standard, but its huge computational power requirements have been under investigation to achieve low-energy consumption by means of reducing its algorithmic complexity. This chapter addresses the main sources of computational complexity in the VVC and presents a thorough review of machine learning approaches recently investigated for reducing this burden. Some recent research results, specifically targeted for omnidirectional video surveillance systems, are also presented, including future research directions towards green computing for video coding.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96429-0_17,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-96293-7_59,End-to-End License Plate Recognition System for an Efficient Deployment in Surveillance Scenarios,Information Technology and Systems,10.1007/978-3-030-96293-7_59,Springer,2022-01-01,"The automatic recognition of license plates has been a widely research area, in special for surveillance scenarios. With the evolution of vigilance cameras and embedded devices, many computer vision models are being used to identify license plates by recognizing its characters. In this work, we present a computer vision model based on convolutional and recurrent neural networks to solve the character recognition task. Artificial data generation is used to balance the dataset, composed by images of alphabetical and numerical characters. A weighted loss function is used in the training stage to enable a robust generalization of the model in the real-world. Our approach reaches an average recognition accuracy of 95% at 35 fps when tested on 60 real surveillance videos.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96293-7_59,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0840-8_17,Deep Learning Framework Based on Audio–Visual Features for Video Summarization,Advanced Machine Intelligence and Signal Processing,10.1007/978-981-19-0840-8_17,Springer,2022-01-01,"The techniques of video summarization (VS) has garnered immense interests in current generation leading to enormous applications in different computer vision domains, such as video extraction, image captioning, indexing, and browsing. By the addition of high-quality features and clusters to pick representative visual elements, conventional VS studies often aim at the success of the VS algorithms. Many of the existing VS mechanisms only take into consideration the visual aspect of the video input, thereby ignoring the influence of audio features in the generated summary. To cope with such issues, we propose an efficient video summarization technique that processes both visual and audio content while extracting key frames from the raw video input. Structural similarity index is used to check similarity between the frames, while mel-frequency cepstral coefficient (MFCC) helps in extracting features from the corresponding audio signals. By combining the previous two features, the redundant frames of the video are removed. The resultant key frames are refined using a deep convolution neural network (CNN) model to retrieve a list of candidate key frames which finally constitute the summarization of the data. The proposed system is experimented on video datasets from YouTube that contain events within them which helps in better understanding the video summary. Experimental observations indicate that with the inclusion of audio features and an efficient refinement technique, followed by an optimization function, provides better summary results as compared to standard VS techniques.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0840-8_17,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ReferenceWorkEntry,doi:10.1007/978-3-030-64573-1_164,AIM in Endoscopy Procedures,Artificial Intelligence in Medicine,10.1007/978-3-030-64573-1_164,Springer,2022-01-01,"Artificial intelligence (AI) is revolutionizing the way medicine is practiced. In this context, the application of AI algorithms in endoscopy is gaining increasing attention so that modern endoscopy is moving towards more and more assisted/automatic solutions. Several approaches have been carried out in order to improve accuracy in diagnosis and surgical procedures. In this chapter, a general overview of the main contributions in the field is surveyed. Four main categories of applications were identified, namely, (i) detection and diagnosis during endoscopic procedure, (ii) informative frame selection, (iii) mosaicking and surface reconstruction, (iv) augmented reality systems for intraoperative assistance and surgeon training. Discussions on future research directions and implementation in clinical practice are provided.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-64573-1_164,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-6460-1_9,Emotion Recognition in Speech Using Convolutional Neural Networks,Data Intelligence and Cognitive Informatics,10.1007/978-981-16-6460-1_9,Springer,2022-01-01,"This paper aims to implement and analyse the performance of Convolutional Neural Networks (CNNs) in detecting and labelling emotion in speech based on the features used to describe the speech. CNNs are often associated with natural language processing, and this paper compares the results of a CNN model on two datasets with the speech in different languages. This paper thus presents the suitability of CNNs as language-agnostic speech-based emotion recognition models, along with the accuracies obtained using different feature sets, with other varying hyperparameters like the batch size. The emotions considered are happiness, sadness, anger, fear and neutrality. The features experimented with are Mel-frequency Cepstrum Coefficient (MFCC), pitch and the log of filterbank energy (LFBE). The datasets in consideration are the ‘Indian Institute of Technology Kharagpur (IIT-KGP)’ Simulated Emotion Hindi Speech Corpus (SEHSC), as well as the Berlin Database of Emotional Speech. Improving speech-based emotion recognition systems would enable them to complement other visual and textual systems to perfectly understand the emotional state of people. This could be highly useful in advertising, reading review sentiment and in the analysis of interviews, speeches and even in the mental-healthcare industry.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-6460-1_9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-3880-0_27,2Be3-Net: Combining 2D and 3D Convolutional Neural Networks for 3D PET Scans Predictions,Proceedings of 2021 International Conference on Medical Imaging and Computer-Aided Diagnosis (MICAD 2021),10.1007/978-981-16-3880-0_27,Springer,2022-01-01,"Radiomics - high-dimensional features extracted from clinical images - is the main approach used to develop predictive models based on 3D Positron Emission Tomography (PET) scans of patients suffering from cancer. Radiomics extraction relies on an accurate segmentation of the tumoral region, which is a time consuming task subject to inter-observer variability. On the other hand, data driven approaches such as deep convolutional neural networks (CNN) struggle to achieve great performances on PET images due to the absence of available large PET datasets combined to the size of 3D networks. In this paper, we assemble several public datasets to create a PET dataset large of 2800 scans and propose a deep learning architecture named “2Be3-Net” associating a 2D feature extractor to a 3D CNN predictor. First, we take advantage of a 2D pre-trained model to extract feature maps out of 2D PET slices. Then we apply a 3D CNN on top of the concatenation of the previously extracted feature maps to compute patient-wise predictions. Experiments suggest that 2Be3-Net has an improved ability to exploit spatial information compared to 2D or 3D-only CNN solutions. We also evaluate our network on the prediction of clinical outcomes of head-and-neck cancer. The proposed pipeline outperforms PET radiomics approaches on the prediction of loco-regional recurrences and overall survival. Innovative deep learning architectures combining a pre-trained network with a 3D CNN could therefore be a great alternative to traditional CNN and radiomics approaches while empowering small and medium sized datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-3880-0_27,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-99616-1_30,"Information Age, Artificial Intelligence and Virtual Reality Technology are Integrated with Logistics Teaching Reform",Forthcoming Networks and Sustainability in the IoT Era,10.1007/978-3-030-99616-1_30,Springer,2022-01-01,"This research mainly discusses the integration of artificial intelligence and VR in the information age and the logistics teaching reform. In educational and teaching activities, VR is used to construct a highly interactive, real-time, and spatially realistic virtual teaching environment. In artificial intelligence logistics teaching, students are often required to use different ways and methods to experience and feel the same thing, so as to choose the best solution strategy. The application of VR to artificial intelligence logistics teaching has created a new teaching model and learning method for the teaching of this course, enriched teaching methods and methods, and promoted the diversification of teaching practice forms. Nearly 54% of logistics teachers believe that logistics teaching is very important in artificial intelligence teaching, and nearly 46% of teachers still fail to realize the important role of logistics teaching in artificial intelligence teaching. This research will help promote logistics teaching reform.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-99616-1_30,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-7466-2_132,The Role of Artificial Intelligence and Virtual Reality Technology in the Training Mode of Rehabilitation Professionals,2021 International Conference on Big Data Analytics for Cyber-Physical System in Smart City,10.1007/978-981-16-7466-2_132,Springer,2022-01-01,"Rehabilitation therapeutics started late and developed slowly in China, and there are many shortcomings in the talent training mode. The trained rehabilitation talents cannot fully meet people’s increasingly diverse needs for rehabilitation services. In order to better promote the development of rehabilitation medicine, we urgently need to reform and innovate the training mode of rehabilitation talents, and cultivate professional talents with rehabilitation specialty characteristics. The introduction of artificial intelligence and virtual reality technology is the key to achieve this goal.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7466-2_132,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9423-3_30,Classroom Speech Emotion Recognition Based on Multi-channel Convolution and SEnet Network,Artificial Intelligence in China,10.1007/978-981-16-9423-3_30,Springer,2022-01-01,"Speech emotion recognition is one of the research hotspots in the current artificial intelligence background. It has applications in many scenarios and fields. In order to solve the problems of low accuracy and complex training parameters of traditional speech emotion recognition models. This paper intends to introduce speech emotion into classroom teaching, and proposes a multi-channel convolution combined with SEnet network as an emotion recognition model by extracting the emotional characteristics of speech. In the self-made emotional data set, the accuracy rate, accuracy rate, F1 value, and recall rate all have good performance. The effectiveness of the model is verified through comparative experiments.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9423-3_30,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0604-6_5,Ensemble Learning with CNN–LSTM Combination for Speech Emotion Recognition,Proceedings of International Conference on Computing and Communication Networks,10.1007/978-981-19-0604-6_5,Springer,2022-01-01,"Speech plays the most significant role in communication between people. The voice enables a speaker’s unique characteristics to be mapped with biometric properties as well as carrying emotions. Emotion contains many non-linguistic signals to express ourselves as humans. Emotion recognition in human speech is a challenging task in different applications in fields such as healthcare, services, telecommunications, video conferencing, and human–computer interaction (HCI). Deep learning techniques are becoming a significant focus in recent research in the speech emotion recognition (SER) domain. In this paper, we present an ensemble learning approach based on various combinations of CNN and LSTM networks to address the limitations of the existing SER models. The proposed system is evaluated using the RAVDESS dataset. More specifically, the LSTM, CNN, and CNN and LSTM models achieved an accuracy rate of 0.64, 0.73, and 0.71, respectively. The simulation outcomes confirm that ensemble learning of the three deep model combinations contributes to the effectiveness of SER.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0604-6_5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-09037-0_31,Multi-view Monocular Depth and Uncertainty Prediction with Deep SfM in Dynamic Environments,Pattern Recognition and Artificial Intelligence,10.1007/978-3-031-09037-0_31,Springer,2022-01-01,"3D reconstruction of depth and motion from monocular video in dynamic environments is a highly ill-posed problem due to scale ambiguities when projecting to the 2D image domain. In this work, we investigate the performance of the current State-of-the-Art (SotA) deep multi-view systems in such environments. We find that current supervised methods work surprisingly well despite not modelling individual object motions, but make systematic errors due to a lack of dense ground truth data. To detect such errors during usage, we extend the cost volume based Deep Video to Depth (DeepV2D) framework [ 26 ] with a learned uncertainty. Our resulting Deep Video to certain Depth (DeepV2cD) model allows (i) to perform en par or better with current SotA and (ii) achieve a better uncertainty measure than the naive Shannon entropy. Our experiments show that we can significantly reduce systematic errors by taking the uncertainty into account. This results in more accurate reconstructions both on static and dynamic parts of the scene.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-09037-0_31,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-2543-5_10,Image Caption Generation Using Neural Network Models and LSTM Hierarchical Structure,Computational Intelligence in Pattern Recognition,10.1007/978-981-16-2543-5_10,Springer,2022-01-01,"The caption generation is nothing but the generation of textual information from images. For this, objects from images are extracted and classified among predefined classes. The logical objects from the image are extracted and transformed into natural sentences. The recognizing process requires an iterative task that incorporates image recognition as well as machine vision. The process must define relations among objects, persons, and animals and create the textual description of these relations. The paper is about the study of deep learning techniques to discover, identify and produce good captions for a source image. The process of making explanations in the form of sentences for a source image is image captioning, which involves machine vision and natural sentence forming techniques. For these processes, recent models have used deep learning techniques to acquire a great improvement in performance. Second, a more advanced trend is set in utilizing attention-dependent structure for captioning. Recent interpreters use a process of attention for each produced term containing seen term and unseen term. However, these unseen terms are effortlessly detected by considering a model for language in the absence of taking seen indicators, but unseen words could cause and a give bad performance for visual captioning. Taking these issues into consideration, the hierarchy of LSTM [Long–Short-Term Memory] with adaptive attention approach for the creation of captions for images and videos is presented.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-2543-5_10,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-95498-7_32,Classifying Simulated Driving Scenarios from Automated Cars,"Applications in Electronics Pervading Industry, Environment and Society",10.1007/978-3-030-95498-7_32,Springer,2022-01-01,"Detection of driving scenarios is getting ever more importance for assessment and control of automated driving functions. This paper investigates the performance of two versions of a high-end 3D convolutional network for scenario classification. The first one uses fully 3D kernels, the second one separates, in each constituting block, the 2D spatial convolution from the temporal convolution, (2 + 1)D. We made the tests on a synthetic dataset created by specifying scenarios in OpenScenario and running them in the CarLA 3D simulator. We focused our analysis on three main performance profiles: at different frame per second rates, different video clip lengths, and different weather conditions. Results show an overall robustness of the 3D predictors, and seem to suggest two different use cases: (2 + 1)D looks more suited when the scenario changes quickly or a low latency is required, while the plain 3D solution is better for slow-changing scenarios and when FPS can be low.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95498-7_32,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-1866-6_42,Obstacle Avoidance and Ranging Using LIDAR,Mobile Computing and Sustainable Informatics,10.1007/978-981-16-1866-6_42,Springer,2022-01-01,"For many years, engineers are searching solutions for car accidents that are caused by human error due to drowsiness or by a person who comes out of nowhere in front of the vehicle. In such a situation, vehicle accident may occur. So, there is a need to develop a vehicle that saves human life from a dangerous accident. Autonomous vehicle have emerged with cameras and sensors to avoid the strategic accidents of human error. Autonomous vehicle systems get information about other vehicles, pedestrians, and other immediate surroundings that is necessary to detect the presence of pedestrians, vehicles and other related entity objects. In this paper, novel LIDAR technologies for automotive applications and the perception algorithms based on field of view (FOV) are used for obstacle avoidance, and ranging of object or vehicle nearby to avoid collision of data in crucial conditions is discussed. This paper is structured based on a vehicle detection and processing of 2D LIDAR sensor to avoid traffic accidents and save life of the people with the help of deep learning algorithm concept. The LIDAR serves as image sensor, and the output response is a steering response, based on the image decisions taken either stop or turn the vehicle depending on the object in front of the vehicle based on road boundary detection.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-1866-6_42,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-7466-2_125,Application of AI Video Image Technology in Soft Ladder Strength Teaching and Training System,2021 International Conference on Big Data Analytics for Cyber-Physical System in Smart City,10.1007/978-981-16-7466-2_125,Springer,2022-01-01,"Ladder strength training is an important content of athletes’ daily training, is the focus of athletes’ teaching, and one of the more difficult techniques to master. Today’s soft ladder strength teaching training is mainly based on subjective judgments and experience-based traditional teaching methods. Teachers rely on subjective observations to guide, correct and objectively evaluate students’ technical movements. They cannot communicate teaching content to students intuitively, vividly and vividly. Exploring a set of practice methods suitable for enthusiasts of ladder strength training and teaching methods suitable for teachers is the research direction of ladder strength training workers. In this paper, 24 male students in a domestic university are used as the research entity, through the use of traditional decomposition teaching mode combined with AI video image technology, a comparative study with traditional teaching methods, and related experiments on 24 students. Experiments show that AI video image technology combined with traditional teaching mode is of great help to the improvement of students’ ladder strength training performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7466-2_125,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11277-021-08899-x,Dysarthric Speech Recognition Using Variational Mode Decomposition and Convolutional Neural Networks,Wireless Personal Communications,10.1007/s11277-021-08899-x,Springer,2022-01-01,"Dysarthric speech recognition requires a learning technique that is able to capture dysarthric speech specific features. Dysarthric speech is considered as speech with source distortion or noisy speech. Hence, as a first step speech enhancement is performed using variational mode decomposition (VMD) and wavelet thresholding. The reconstructed signals are then fed as input to convolutional neural networks. These networks learn dysarthric speech specific features and generate a speech model that supports dysarthric speech recognition. The performance of the proposed method is evaluated using UA-Speech database. The average accuracy values obtained by the proposed method for speakers with different intelligibility levels with VMD based enhancement and without enhancement are 95.95 and 91.80% respectively. The proposed method also provides an increased accuracy value compared to existing methods that are based on generative models and artificial neural networks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11277-021-08899-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-85577-2_37,Bilingual Speech Emotion Recognition Using Neural Networks: A Case Study for Turkish and English Languages,Intelligent and Fuzzy Techniques for Emerging Conditions and Digital Transformation,10.1007/978-3-030-85577-2_37,Springer,2022-01-01,"Emotion extraction and detection are considered as complex tasks due to the nature of data and subjects involved in the acquisition of sentiments. Speech analysis becomes a critical gateway in deep learning where the acoustic features would be trained to obtain more accurate descriptors to disentangle sentiments, customs in natural language. Speech feature extraction varies by the quality of audio records and linguistic properties. The speech nature is handled through a broad spectrum of emotions regarding the age, the gender and the social effects of subjects. Speech emotion analysis is fostered in English and German languages through multilevel corpus. The emotion features disseminate the acoustic analysis in videos or texts. In this study, we propose a multilingual analysis of emotion extraction using Turkish and English languages. MFCC (Mel-Frequency Cepstrum Coefficients), Mel Spectrogram, Linear Predictive Coding (LPC) and PLP-RASTA techniques are used to extract acoustic features. Three different data sets are analyzed using feed forward neural network hierarchy. Different emotion states such as happy, calm, sad and angry are compared in bilingual speech records. The accuracy and precision metrics are reached at level higher than 80%. Turkish language emotion classification is concluded to be more accurate regarding speech features.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85577-2_37,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06015-1_21,AI-AR for Bridge Inspection by Drone,"Virtual, Augmented and Mixed Reality: Applications in Education, Aviation and Industry",10.1007/978-3-031-06015-1_21,Springer,2022-01-01,"Good and regular inspections of transportation infrastructures such as bridges and overpasses are necessary to maintain the safety of the public who uses them and the integrity of the structures. Until recently, these inspections were done entirely manually by using mainly visual inspection to detect defects on the structure. In the last few years, inspection by drone is an emerging way of achieving inspection that allows more efficient access to the structure. This paper describes a human-in-the-loop system that combines AI and AR for bridge inspection by drone.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06015-1_21,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-2211-4_25,Emotion Recognition of Speech by Audio Analysis using Machine Learning and Deep Learning Techniques,Data Science and Security,10.1007/978-981-19-2211-4_25,Springer,2022-01-01,"Verbal communication is key for understanding any human being. Verbal communication even shows various emotions of a person. During pandemic, online learning systems are used everywhere that need to be understands the emotions of learner for happy learning. Many applications introduced for speech analysis. Deep learning applications are growing faster as time in various fields like image recognition, health care, natural language processing, machine and language translation, and many more. People never imagined that such applications will give produce Alexa like virtual assistants. In similar concepts, emotion recognition is also nowadays one of the important factors to be considered for further research. An audio analysis is a field that works on digital signal processing, tagging, music classification, speech synthesis, and automatic speech recognition like techniques. Information is extracted for the virtual assistants from the audio signals only. For the same work, it is mandatory to analyze audio data; in this paper, how a tool can be built that understands and explores data for emotion recognition using machine and deep learning platforms is mentioned and described.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-2211-4_25,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-5652-1_24,A Review on Video Tampering Analysis and Digital Forensic,"Proceedings of International Conference on Deep Learning, Computing and Intelligence",10.1007/978-981-16-5652-1_24,Springer,2022-01-01,"Digital evidence collection and analysis have become an increasing tool to solve crimes and prepare courts’ cases over the last two decades, undergoing major changes in the area of IT. Crime is a major problem every day, so that computer forensics are avoided and protected from crime. More information is created, stored and accessed with increasingly portable and powerful technology. Mobile systems may serve as large personal knowledge archives in a wallet still accessible through a hand or phrase. The advantage is obvious by having ample information in order to obtain judgments, but the collection and admissibility of digital proof should be balanced with the privacy concerns of law enforcement and other parties to criminal law. The need of validating the honesty of digital video content ranges from a person to associations, obstacles and security arrangements to law authorization/organizations’. With video and image changing, the change tools have made it simple to modify media content. Therefore, it is necessary to investigate viable methods for video falsification.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5652-1_24,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-93709-6_41,Multi-channel Convolutional Neural Network for Hate Speech Detection in Social Media,Advances of Science and Technology,10.1007/978-3-030-93709-6_41,Springer,2022-01-01,"As online social media content continues to grow, so does the spread of hate speech. Hate speech has devastating consequences unless it is detected and monitored early. Recently, deep neural network-based hate speech detection models, particularly conventional single-channel Convolutional Neural Network (CNN), have achieved remarkable performance. However, the effectiveness of the models depends on the type of language they are trained on and the training data size. We argue that the effectiveness of the models could further be enhanced if we use multi-channel CNN models even for under-resourced languages that have limited training data size. This is because the single-channel CNN might fail to consider the potential effect of multiple channels to generate better features, which is not well investigated for hate speech detection. Therefore, in this work, we explore the use of multi-channel CNN to extract better features from different channels in an end-to-end manner on top of a word2vec embedding layer. Tested on a new small-scale Amharic hate speech dataset containing 2000 annotated social media comments, the experimental results show that the proposed multi-channel CNN model outperforms the single-channel CNN models but underperform from the baseline Support Vector Machine (SVM) with an average F-score of 81.3%, 78.2%, and 92.5% respectively. The finding of the study implies that the proposed MC-CNN model can be used as an alternative solution for hate speech detection using a deep learning approach when dataset scarcity is an issue.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-93709-6_41,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-981-19-1223-8_4,Machine Learning in Medical Imaging – Clinical Applications and Challenges in Computer Vision,Artificial Intelligence in Medicine,10.1007/978-981-19-1223-8_4,Springer,2022-01-01,"Applications for Machine Learning in Healthcare have rapidly increased in recent years. In particular, the analysis of images using machine learning and computer vision is one of the most important domains in the area. The idea that machines can outperform the human eye in recognizing subtle patterns is not new, but it is now gaining momentum with large financial investments and the arrival of many startups with a focus in this area. Several examples illustrate that machine learning has enabled us to detect more diffuse patterns that are difficult to detect by non-experts. This chapter provides a state-of-the-art review of machine learning and computer vision in medical image analysis. We start with a brief introduction to computer vision and an overview of deep learning architectures. We proceed to highlight relevant progress in clinical development and translation across various medical specialties of dermatology, pathology, ophthalmology, radiology, and cardiology, focusing on the domains of computer vision and machine learning. Furthermore, we introduce some of the challenges that the disciplines of computer vision and machine learning face within a traditional regulatory environment. This chapter highlights the developments of computer vision and machine learning in medicine by displaying a breadth of powerful examples that give the reader an understanding of the potential impact and challenges that computer vision and machine learning can play in the clinical environment.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-1223-8_4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-6320-8_43,Audio Signal Fault Diagnosis Based on Convolutional Neural Network,Proceedings of 2021 Chinese Intelligent Systems Conference,10.1007/978-981-16-6320-8_43,Springer,2022-01-01,"Monitoring the audio signal of each link of the broadcast transmission is not only to control the broadcast quality, but also the basis for fault diagnosis. Broadcast transmission has problems such as wrong signal, lost signal, downtime, interference signal, etc. To prevent these problems, traditional monitoring system uses audio contrast technology. This technique requires signal synchronization for delayed signals. What’s more, it is also impossible to classify and diagnose audio signals. As a feed forward neural network, convolutional neural network is one of the typical algorithms of deep learning. This article mainly discusses how to use convolutional neural networks to preprocess audio signals, extract features and diagnose faults. This article also summarizes the advantages of convolutional neural networks for audio fault diagnosis.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-6320-8_43,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-83864-5_32,The Role of Technology and Innovation,Global Cardiac Surgery Capacity Development in Low and Middle Income Countries,10.1007/978-3-030-83864-5_32,Springer,2022-01-01,"Technology Technology holds a significant role in the past, present, and future of cardiac surgical care capacity in low-to-middle income countries (LMICs): a role that holds positive value overall in opportunity and outcome, despite recurrent and emerging practical and ethical challenges to different stakeholder Stakeholders groups. Aided by case studies, this chapter discusses the value of both creating technologies to benefit cardiac surgical care directly, and creating environments Environments for technological innovation Innovation . This is addressed by examining the value of technology Technology for, and within, human and physical resource Physical resource development Development . Human resource Human resource development, consisting of cognitive, technical, and reflective components, is developed through two complementary approaches: increasing the minimum quality Quality standard achieved by healthcare professionals, and increasing the maximum quality Quality standard achievable. Both of these strategies are hugely helped by the implementation Implementation of technology Technology to facilitate new training Training methods, relationships, and career opportunities. The discussion of physical resource Physical resource development is guided by the journey of development and implementation taken by individual technologies, identifying key forms of support required to combat potential issues and increase the chance of success. Technology Technology requires significant resource investment by a varied portfolio of stakeholder Stakeholders groups to deliver on its current achievable, and future potential promise to contribute significantly towards cardiac surgical capacity development Development in LMICs.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-83864-5_32,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8826-3_3,A Review of Speech Sentiment Analysis Using Machine Learning,Proceedings of Trends in Electronics and Health Informatics,10.1007/978-981-16-8826-3_3,Springer,2022-01-01,"Kumar, Tapesh Mahrishi, Mehul Nawaz, Sarfaraz Over recent decades, sentiment analysis has been progressive. A lot of effort focused on text analysis using techniques of text mining. However, audio sentiment analysis is still in the developing phase in the academic community. In this recommended study, we examine different transcripts of speech sentiment also with speaker recognition to assess speakers’ emotions. The study of the sentiment of speech in many businesses, such as customer service, health care, and education, represents a major issue.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8826-3_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-15-8155-7_365,Research on Infrared and Images Identification Technology of Small and Weak Targets with Different Resolutions at Sea,"Advances in Guidance, Navigation and Control",10.1007/978-981-15-8155-7_365,Springer,2022-01-01,"Aiming at the problems that the same recognition method for sea surface targets could not be used both day and night under visible light condition and the infrared images of weak and small targets were not highly distinguishable from the background, this paper proposed an algorithm which was compatible with identifying both infrared and visible light video information. This algorithm uses the methods of background suppression and frame correlation infrared sequence to distinguish the gray level information between targets and background, and to constitute a complete image. Besides, this algorithm takes into account the influence of changes in the marine environment on imaging effects, and the video of real boats with different resolution was selected for recognition test. By comparing the recognition results, the reliability of the algorithm was verified. The single target recognition rate under simulation conditions is greater than 90%, and is not less than 60% under the condition of real boat test. The recognition time is less than 100 ms.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-8155-7_365,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-7466-2_148,Location Prediction and Image Recognition of Asian Hornet Based on Deep Learning,2021 International Conference on Big Data Analytics for Cyber-Physical System in Smart City,10.1007/978-981-16-7466-2_148,Springer,2022-01-01,"In recent years, the frequent occurrence of alien species invasion not only reduces the diversity of local species, but also greatly affects the production and life of human beings. To help the government solve the problem of Asian bumblebee invasion in the problem, this paper establishes a migration learning model of Long Short Term Memory neural network, convolution neural network and neural network to solve the location prediction of Asian bumblebee propagation and the recognition of Asian bumblebee in bee pictures. To determine the next propagation location of bumblebee in Asia, this paper first filters and sorts the attached data to obtain the time, latitude and longitude information Positive ID by 14 Lab State fields arranged in chronological order. LSTM neural network suitable for high precision prediction of small data sets is selected to solve the problem of small and 2 dimensional data sets. According to the data set size, the sliding window size is four, the first three data of each window are used as the training set, and the last data of the window is the most tested set. A LSTM neural network with a scale of 120 ~ 120 ~ 2 is established to train and learn the data set after grouping. The correct probability of prediction results is 98.41, so we think that the model can solve the problem of location prediction of Asian hornet propagation over time. Using the training results of the last three data as the prediction results, the latitude and longitude of the next propagation position is obtained as [48.94857702, −122.581335]. The distance between the predicted position and the latest data position is 3.44 km, which is smaller than the maximum active radius of Asian bumblebee 8 km, so it is considered that the predicted data are reasonable. To identify Asian bumblebees in bee pictures, this paper filters the attached data and obtains pictures with Positive ID and Negative ID fields. Since there are only 14 Positive ID data sets, CNN networks with high image recognition accuracy are selected. Then separate the two types of pictures and number them separately, and the image of 128 * 128 into 128 * 128 * 3 pixel matrix, enter into the neurons of the CNN network for training and learning, 10 rounds of training, 120 sessions per round, each time 20 samples were randomly selected for training, of which 80% were randomly selected as training data, 20% for test data. Finally, the accuracy of the training set is 99.67%, the accuracy of the test set is 99.55. Therefore, we think that the model can solve the problem of Asian bumblebee recognition in bee pictures.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7466-2_148,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9576-6_14,Trojan Attacks and Defense for Speech Recognition,Mobile Internet Security,10.1007/978-981-16-9576-6_14,Springer,2022-01-01,"Mobile devices commonly employ speech recognition (SR) techniques to facilitate user interaction. Typical voice assistants on mobile devices detect a wake word or phrase before allowing users to use voice commands. While the core functionality of contemporary SR systems relies on deep learning, researchers have shown that deep learning suffers from various security issues. Among these security threats, Trojan attacks in particular have attracted great interest in the research community. To conduct a Trojan attack, an adversary must stealthily modify a target model, such that the compromised model will output a predefined label whenever presented with a trigger. Most work in the literature has focused on Trojan attacks for image recognition, and there is limited work in the SR domain. Due to the increasing use of SR systems in daily devices, such as mobile phones, Trojan attacks for SR pose a great threat to the public and is therefore an important topic of concern to mobile internet security. Despite its growing importance, there has not been an extensive review conducted on Trojan attacks for SR. This paper fills this gap by presenting an overview of existing techniques of conducting Trojan attacks and defending against them for SR. The purpose is to provide researchers with an in-depth comparison of current methods and the challenges faced in this important research area.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9576-6_14,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-92245-0_1,Machine Learning for Business Analytics: Case Studies and Open Research Problems,Artificial Intelligence for Data Science in Theory and Practice,10.1007/978-3-030-92245-0_1,Springer,2022-01-01,"Business analytics (BA) refers to the process of organizing, processing, and examining business data for the purpose of gaining useful information that can be used to resolve problems and enhance the efficiency, productivity, and revenue. Applications of machine learning (ML) within BA have proliferated in recent years and have revolutionized the process of business decision-making, despite concerns that implementation of BA functions could lead to job loss. This chapter describes the different ML techniques being currently used in business. Several case studies in which ML is used for business purposes are presented. Open research problems in the areas of ML in BA are discussed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92245-0_1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9650-3_39,"Deepfake Images, Videos Generation, and Detection Techniques Using Deep Learning",Machine Intelligence and Smart Systems,10.1007/978-981-16-9650-3_39,Springer,2022-01-01,"Deep learning is utilized as a deepfake detection and creation method. Deepfake produces fake videos and images, which are difficult to distinguish between authentic images or videos. A free software learning platform has allowed the development of credible faceswaps in videos in recent months, which leaves only a small residue of manipulation in so-called deepfake videos. GANs create deepfakes with two computer models coming out. One is trained on the dataset, and the other aims to explain deepfakes faults. Forger produces fakes until another model does not identify forgery. Deepfakes create fake news that can contribute to financial and social fraud, videos, photographs, and terrorism. Religions, organizations, citizens and societies, environment, protection and democracy are increasingly being influenced. These realistic fake videos cause political distress, challenging someone, or fake terrorist activities are easily conceived. When deep videos and photographs expand in social media, people disregard the facts. This analysis discussed available datasets, deepfake development, deepfake issues, and fake video detection techniques, including fake video detection using GANs. Besides this, deep learning techniques for deepfake detection detailing are provided in this paper.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9650-3_39,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-03918-8_23,Predicting the Intention to Use Audi and Video Teaching Styles: An Empirical Study with PLS-SEM and Machine Learning Models,The 8th International Conference on Advanced Machine Learning and Technologies and Applications (AMLTA2022),10.1007/978-3-031-03918-8_23,Springer,2022-01-01,"Lately, there has been an increasing trend of using audio and video content on different online platforms. This trend of incorporation of audio and video content in online interaction was specifically beneficial for teachers and students who experienced better teaching and learning owing to the audio and visual data that offered them a richer interactive environment. The educational domain was positively affected by this practice. It is important to note that there is a dearth of empirical studies that use a conceptual model based on the factor of acceptance to offer insight into the acceptance of this video and audio content. Fortunately, this dearth in literature has been addressed by the current study that contributes in exploring the factors like speed and vividness, perceived concentration, perceived ease of use, perceived usefulness and sheds light on the impact of these factors on the acceptance of audio-video material. This study used the survey method for data collection and subsequently evaluated the research model. The Partial least squares-structural equation modelling (PLS-SEM) and machine learning models were used for this study. The study used a conceptual model to reveal the intentions of the students for the incorporation of audio-visual content on online platforms. The acceptance of technology by students was investigated concerning TAM constructs as well as the predictors like perceived concentration, speed, and vividness. Lastly, the study suggested theoretical and practical implications in the recommendations section. The recommendations were specifically associated with technology users, designers, and change managers.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-03918-8_23,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9573-5_44,A Deep Convolutional Neural Network-Based Speech-to-Text Conversion for Multilingual Languages,Computational Vision and Bio-Inspired Computing,10.1007/978-981-16-9573-5_44,Springer,2022-01-01,"Designers have been processing speech for decades for a wide variety of applications, from mobile communications to automatic reading machines, among others. By eliminating the need for alternative communication methods, speech recognition saves time and money. In the world of electronics and computers, speech is rarely employed because of the complexity and variety of voice signals and noises. Today's technologies allow us to process speech signals quickly and accurately and recognize the text. A real-time translation of speech into written language requires specific techniques, as it must be extremely rapid and nearly error-free to make sense. A person's speech is the most natural and important form of communication. This system converts human speech into a string of words using the speech-to-text (STT) technology. This system's goal is to extract, classify, and recognize information about speech in a variety of ways. Using convolutional neural networks (CNNs) for voice classification, the proposed system is developed. The input signals are classified by CNN on its own since it is a self-optimizing neural network. In addition, high-level features are extracted by convolutional and pooling layer, where the data is classified using fully connected (FC) layer. A database contains pre-recorded speech. Testing and training are the two key aspects of the database. In the training phase, samples from the training database are run through a series of tests to determine their characteristics. Each sample's features are combined to create a feature vector that is stored for future reference. When a sample is supplied to the system for analysis, its features are extracted. There is a comparison between these features and the reference feature vector, and the words with highest similarity are output. MATLAB (V2018a) environment is used to design the system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9573-5_44,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-96908-0_129,Design of English Multimedia Intelligent Translation System Based on Neural Network Algorithm,Cyber Security Intelligence and Analytics,10.1007/978-3-030-96908-0_129,Springer,2022-01-01,"Due to the vigorous development of network information technology and the internationalization of the market economy, communication in various fields is becoming more and more common, and the requirements for cross-language communication are increasing. As a simple and efficient tool, the machine translation system can achieve peer-to-peer translation between various sentences while maintaining the original semantic structure, which is very critical in practice. This paper aims to study the design of English multimedia intelligent translation system based on neural network algorithm. Based on the analysis of common neural network translation models, neural network learning rules and system non-functional requirements, the English multimedia intelligent translation system level is designed. In order to test the translation performance of the designed system, the system in this paper is compared with other systems. The test results show that the performance of the translation system in this paper is optimal regardless of the length of the interval, and as the sentence length gradually increases, the translation performance gradually increases.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96908-0_129,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-6369-7_51,Network Slicing in Software-Defined Networks for Resource Optimization,Intelligent Sustainable Systems,10.1007/978-981-16-6369-7_51,Springer,2022-01-01,"The Internet users’ heterogeneous requirements pose a challenge to the service provider for catering to the network infrastructure needs to provide the different services. The partitioning of the shared physical network is proposed using a network slicing mechanism to meet this challenge. The classification of the services is carried out based on the attributes in the flow features. The flow features are extracted in real-time for traffic classification. The traffic classification is implemented using supervised learning models for different network services. The combination of traffic classification and network slicing is proposed for the dynamic allocation of resources. In this paper, the testbed to demonstrate the network slicing concept based on the bandwidth requirement is presented using FlowVisor and the OpenFlow protocol in software-defined networks. The intelligence is embedded into networks by integrating decision making using a supervised learning model for classification followed by network slicing for optimal resource allocation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-6369-7_51,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-88892-3_27,Artificial Intelligence in Pediatric Cardiology,Modelling Congenital Heart Disease,10.1007/978-3-030-88892-3_27,Springer,2022-01-01,"The exponential growth in healthcare data as a whole and in particular the breadth of advanced imaging studies and data unique to pediatric cardiology welcomes more innovative methods for analysis, interpretation, and translation of these data into clinically useful information. As it stands today, physicians are inefficiently only acting on a small portion of the data available to them. Implementation of artificial intelligence (AI) into the healthcare model offers promise to improve the quality of patient care in clinical practice with significant potential in pediatric cardiology to lead the transition from a “one-size-fits-all” approach to management of congenital heart disease (CHD) to patient-specific tailored therapies that acknowledge that subtle variations in CHD lesions merit individualized treatment strategies. This chapter reviews the history of AI, provides a primer on the aspects of AI and machine learning in pediatric cardiology, discusses current applications of these methods, and explores how pediatric cardiology can incorporate AI in the future.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-88892-3_27,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-75123-4_8,Artificial Intelligence Trends: Insights for Digital Economy Policymakers,Information and Knowledge in Internet of Things,10.1007/978-3-030-75123-4_8,Springer,2022-01-01,"Artificial intelligence (AI) is reshaping the economy, redefining the industry and the services sector. It is a disruptive technology, contributing to the development of different practices, new business models, and more efficient industrial processes. The literature highlights that AI will be included in all areas of organizations and the personal lives of citizens, and such dynamics are still significantly unstudied. The purpose of this research is to highlight the latest research about AI through a bibliometric analysis, which will include an up-to-date overview of artificial intelligence research on the goals, results, methodologies, and geographic distribution of studies during 2015–2019. AI applications to the industry and services have been growing over the last 5 years, as the research results show. The results include that most studies have concentrated on the effect of artificial intelligence on industries’ productivity potential. Robotics was the area of subject matter most researched. The findings show that the most common AI technologies are predictive analysis, machine learning, and robotics.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-75123-4_8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-97281-3_16,Self-supervised Medical Out-of-Distribution Using U-Net Vision Transformers,"Biomedical Image Registration, Domain Generalisation and Out-of-Distribution Analysis",10.1007/978-3-030-97281-3_16,Springer,2022-01-01,"Mitigating out-of-distribution inaccuracies due to insufficient labeled data is a widespread problem in the medical AI field. To tackle this medical out-of-distribution problem, through the medical out-of-distribution challenge [ 1 ], our team utilized self-supervised learning with UNETR [ 2 ]. UNETR is a 3D UNET model where the encoder incorporates Vision Transformers [ 3 ]. Abnormal samples were generated from normal samples using a 3D extension to the cutout method described by [ 4 ], allowing for self-supervised learning. The input size was chosen to be one eighth of the original size due to the large computational cost associated with 3D image segmentation. This results in 3D images that are $$128 \times 128 \times 128$$ 128 × 128 × 128 pixels for brain samples and $$256 \times 256 \times 256$$ 256 × 256 × 256 pixels for abdominal samples. Abdominal samples were further divided into 8 different patches, with all 8 patches being used for training. The model produces a voxel-wise abnormality prediction ranging between 0 and 1, where 1 represents an abnormal voxel. We then further extrapolate this prediction to generate a sample-wise prediction by taking the maximum value.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-97281-3_16,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-77185-0_10,Machine Learning for Hate Speech Detection in Arabic Social Media,Computational Intelligence in Recent Communication Networks,10.1007/978-3-030-77185-0_10,Springer,2022-01-01,"( WARNING: This paper may contain some offensive words) Over the past few years, abusive language and cyberbullying have known a great increase on social media in general. This phenomenon has encouraged efforts to propose solutions able to detect and prohibit such behavior. Most of these solutions are dedicated to English, but the ones that can handle Arabic are, to the best of our knowledge, rare. Many reasons lie behind this situation including the informality and ambiguity of the Arabic dialects, as well as the use of Arabic/Arabizi combinations. In this paper, we will use a collection of Arabic YouTube comments that are annotated as either “hateful” or “inoffensive” to compare the ability of five machine learning algorithms to perform correct classification on hateful Arabic comments. The algorithms are Logistic Regression, Naïve Bayes, Random Forests, Support Vector Machines, and Long Short-Term Memory. The performance metrics are Accuracy, F1-Score, Precision, and Recall.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-77185-0_10,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-85540-6_109,Detecting a Coronavirus Through Breathing Using 3D Modeling and Artificial Intelligence,"Human Interaction, Emerging Technologies and Future Systems V",10.1007/978-3-030-85540-6_109,Springer,2022-01-01,"A coronavirus called COVID-19 appeared in Wuhan, China in December 2019. By early 2020 it caused a worldwide pandemic of a respiratory illness. This virus crisis affected the economy in the world in 2020 and will affect it in 2021, a lot of companies lost $ billions, especially the airlines, and because of that a lot of people lost their jobs; it also caused deaths of many people and till now it causes deaths of thousands of people in the world every day, a lot of people have died and till now a lot of people are dying every day because of a late detection of the virus in their bodies. The coronavirus can spread from an infected person’s mouth or nose in small liquid particles, having different sizes, ranging from larger ‘respiratory droplets’ to smaller ‘aerosols’, when they cough, sneeze, speak, sing or breathe heavily, which means that it can be spread through breathing from an infected person to another person. There are different types (shapes) of COVID-19. This article aims to introduce a new approach that allows us to detect a coronavirus immediately using the artificial intelligence techniques and 3D modeling. The basic idea is to use 3D modeling technology to prepare the geometric data for different shapes of COVID-19 and AI technique, which can detect a coronavirus immediately through breathing by comparing the respiratory droplets or smaller aerosols with the shapes of COVID-19 specified by the 3D model .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85540-6_109,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-93639-6_25,Use of Artificial Intelligence in Mining: An Indian Overview,International Congress and Workshop on Industrial AI 2021,10.1007/978-3-030-93639-6_25,Springer,2022-01-01,"In Mining in India contributes over 2% to its GDP. The growing population had a direct impact on the need for more electricity, steel, cement, fertilizers etc. Most of these consumables are mining products. Thus expansion and modernization of mining activities had always gone for introducing new initiatives, innovations, safe and cost cutting areas. In order to make operations safe and economical an attempt has been made to introduce Artificial Intelligence in most mining activities. Operations attached to mining areas like transportation, shovelling, beneficiation etc. are manpower intensive and unsafe having considerable health related issues. By adopting Artificial Intelligence most complex operations, computations, analysis becomes easier and accurate. Although, global mining companies have made significant advances in AI applications, in India it has been in nascent stage. The areas of AI applications has been identified and this paper highlights the roadmap of the future.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-93639-6_25,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-91479-0_5,Prediction of Speech Quality Dimensions with Multi-Task Learning,Deep Learning Based Speech Quality Prediction,10.1007/978-3-030-91479-0_5,Springer,2022-01-01,"In this chapter, machine learning based methods for single-ended prediction of the overall quality and additionally, the four following speech quality dimensions are presented: Noisiness, Coloration, Discontinuity, and Loudness. The resulting dimension scores serve as degradation decomposition and help to understand the underlying reason for a low MOS score. The subjective ground truth values of these scores are perceptual speech quality dimensions. Because the model aims to predict the overall MOS and additionally dimension scores from the same speech signal, the prediction can be seen as a Multi-Task-Learning (MTL) problem.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-91479-0_5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-94554-1_10,Research on Coding Method of Microscopic Video Signal Based on Machine Learning,Advanced Hybrid Information Processing,10.1007/978-3-030-94554-1_10,Springer,2022-01-01,"At present, the commonly used microscopic video signal coding methods have poor processing ability and low coding accuracy. Therefore, this paper proposes a new micro video coding method based on machine learning technology. Firstly, the video coding processing architecture is established by intra prediction, inter prediction, transformation, quantization, entropy coding and loop filtering, and then the coding processing is realized by image segmentation, intra prediction and inter prediction, and the depth decision method is used for depth analysis. The experimental results show that this method can effectively improve the processing ability of microscopic video signal coding, and at the same time enhance the coding accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-94554-1_10,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-95419-2_8,Application of Machine Learning Algorithms to Disordered Speech,Blockchain and Deep Learning,10.1007/978-3-030-95419-2_8,Springer,2022-01-01,"Aphasia is an acquired neurogenic language/speech disorder that can be assessed with one of the well-known assessment standards, such as the Chinese Rehabilitation Research Center Aphasia Examination (CRRCAE, for Chinese-dialects speaking patients), Aachen Aphasia Test (AAT, for German-speaking patients) and Boston Diagnostic Aphasia Examination (BDAE, for English-speaking patients). These standards are utilized by a skilled speech-language pathologist (SLP) to assess patients with aphasia (PWA). Generally, there are three aphasia assessment tasks where an SLP performs a comprehensive examination of the patient’s communication abilities, including speaking, expressing ideas, understanding language, and reading and writing. These tasks are the discrimination between healthy and aphasic speech, the degree of severity of impairment for aphasic patients’ assessment and the classification of aphasia syndromes (such as Global aphasia, Broca’s aphasia, Wernicke’s aphasia, and amnesic aphasia). Conventional methods of aphasia assessment and rehabilitation are resource-intensive processes that require the presence of an SLP. Due to the vast number of PWAs, the big data involves in the aphasia assessment and the financial difficulties, it is complex to fulfil this requirement manually. Therefore, automation of the aphasia assessment process is essential to attract researchers’ attention. There are several research efforts in automating the aphasia and speech disorder assessment process using machine learning (ML) algorithms. This chapter presents the application of classical machine learning (CML) and deep neural networks (DNN) to aphasic speech assessment. Moreover, the challenges and limitations facing ML algorithms over disordered speech datasets are presented. Furthermore, the chapter provides recommendations on the suitable ML framework for aphasia assessment based on comparative performance results between various CML’s classifiers and CNN algorithm over aphasic speech. This chapter also presents two practical examples of the ML application to aphasia assessment.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95419-2_8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-99948-3_5,Drugtionary: Drug Pill Image Detection and Recognition Based on Deep Learning,Proceedings of the 18th International Conference on Computing and Information Technology (IC2IT 2022),10.1007/978-3-030-99948-3_5,Springer,2022-01-01,"Drugtionary, which is a mobile application, is developed to support people who lack medical understanding and avoid taking the wrong drug. It consists of four main features including (i) sign-up, (ii) managing profile and medication history, (iii) viewing medication information, and (iv) managing the schedule. For viewing medication information, there are three ways to retrieve the drug information–(i) text search, (ii) chatbot, and image search. We use string search and DialogFlow for text search and chatbot, respectively, whereas deep learning technique for image detection and recognition is used to search the given drug pill image. The experimental result shows that the model generated from the CenterNet method is suitable when compared to the Faster-RCNN, RetinaNet, Yolo, and SSD on our drug pill dataset. Moreover, our application is constructed by using React and React Native technology. All data are stored in the MongoDB database.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-99948-3_5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-15-8155-7_366,Study on the Recognition of Visible Image at Sea Based on YOLOv2-Network Model,"Advances in Guidance, Navigation and Control",10.1007/978-981-15-8155-7_366,Springer,2022-01-01,"With the continuous maturity and improvement of machine learning, an intelligent maritime monitoring system is constructed for the identification and tracking of dynamic fuzzy targets in maritime video images. After studying the characteristics of the Marine target environment, it is found that the imaging effect is poor due to the violent movement of the offshore imaging platform, and the field of view of the ship is relatively small, so it is difficult to extract the characteristic information of the corresponding target. Depending on these reasons above, under the CAFFE framework of deep learning, the original and improved yolov2-a network model, SVM support vector machine, HOG feature, multi-scale transformation and multi-thread technology are adopted to identify multiple targets. The fast and effective intelligent automatic detection and recognition algorithm for fuzzy image is completed, which supports a variety of standards and definition. The final research results can detect the weak floating, small boats, speedboats, cruise ships and other targets, the identification accuracy reached more than 95%, for hd video image, real-time up to 30 frames/second.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-15-8155-7_366,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-81576-9_6,"Health Monitoring, Machine Learning, and Digital Twin for LED Degradation Analysis",Reliability of Organic Compounds in Microelectronics and Optoelectronics,10.1007/978-3-030-81576-9_6,Springer,2022-01-01,"Light-emitting diodes (LEDs) are among the key innovations that have revolutionized the lighting industry, due to their versatility in applications, higher reliability, longer lifetime, and higher efficiency compared with other light sources. The demand for increased lifetime and higher reliability has attracted a significant number of research studies on the prognostics and lifetime estimation of LEDs, ranging from the traditional failure data analysis to the latest degradation analysis and machine learning-based approaches over the past couple of years. However, there have been few reviews that systematically address the currently evolving machine learning (ML) algorithms and methods for fault detection, diagnostics, and lifetime prediction of LEDs. To address these deficiencies, we provide a review on the diagnostic and prognostic methods and algorithms based on machine learning that helps to improve system performance, reliability, and lifetime assessment of LEDs. And the emerging trend in the application of digital twins for PHM with the focus on LEDs is also discussed. Finally, a case study on UV LED radiation degradation modeling with different machine learning methods is provided.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-81576-9_6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-67936-1_1,Introduction,Acoustic Emission Testing,10.1007/978-3-030-67936-1_1,Springer,2022-01-01,"This chapter is devoted to the comparison of acoustic emission techniques to other NDT techniques and to relate the data processing techniques to those developed and applied in seismology. Moreover, new developments are described that are currently under development which opening new fields in future for applications using acoustic emission technologies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-67936-1_1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8546-0_33,Content-Based Video Retrieval Based on Security Using Enhanced Video Retrieval System with Region-Based Neural Network (EVRS-RNN) and K-Means Classification,International Conference on Artificial Intelligence and Sustainable Engineering,10.1007/978-981-16-8546-0_33,Springer,2022-01-01,"The recent development in video data requires more prominent and capable techniques in retrieval of video which might improve the searching quality of video as well as can understand the content of video in higher standard, whereas to search relevant secured video from a document is a major setback for the digital research community. However, the major problem is the external information brought from digital resources. Therefore, this research proposed a secure CBVR framework directly select features from the ciphertext domain on the cloud server side. Therefore, high secure data has been stored in cloud using enhanced video retrieval system with region-based neural network (EVRS-RNN). From the resulted retrieval data, accuracy had been improved using ensemble classifier linear discriminant analysis (LDA) and relevant video frames retrieval using K-means classification technique. The simulation performance shows efficiency in security with obtaining parameters and improves the accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8546-0_33,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-6289-8_39,Artificial Intelligence in Healthcare: Diabetic Retinopathy,Proceedings of Data Analytics and Management,10.1007/978-981-16-6289-8_39,Springer,2022-01-01,"Artificial Intelligence (AI) has had a major influence on the medical sector. It has the potential to be used for mass screening and may also aid in accurate diagnosis. AI technology is primarily used in optometry and ophthalmology to treat diseases with a high prevalence, such as Retinal Vein Occlusion, Cataract, Age-related Macular Degeneration (AMD), Retinopathy of Prematurity, Diabetic Retinopathy (DR), and Glaucoma. Diabetic Retinopathy is becoming more common among these. It can result in permanent blindness if not diagnosed in a timely manner. As a result, any technologies that can aid in rapid screening, while reducing the need for qualified human resources will likely be beneficial to both patients and ophthalmologists. This paper represents how AI can help with diabetic retinopathy by using Image Recognition technique.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-6289-8_39,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-97652-1_32,Recommendation System Using MixPMF,ITNG 2022 19th International Conference on Information Technology-New Generations,10.1007/978-3-030-97652-1_32,Springer,2022-01-01,"The objective of this paper is to the use of the Probabilistic Matrix Factorization (PMF) model which scales linearly with the number of observations and performs well on the large, sparse, and imbalanced music/movie dataset. In this project, we compare various PMF-based models and apply them to the recommendation system. We design and develop a Mix Probabilistic Matrix Factorization (MixPMF) model for music recommendation. This new model will take advantage of user network mapping and artist tag information and forms the effective rating matrix and thus will be efficient in recommending music/movies to new users. Simulation results show the advantage of our model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-97652-1_32,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-97549-4_52,Multi-lingual Emotion Classification Using Convolutional Neural Networks,Large-Scale Scientific Computing,10.1007/978-3-030-97549-4_52,Springer,2022-01-01,"Emotions play a central role in human interaction. Interestingly, different cultures have different ways to express the same emotion, and this motivates the need to study the way emotions are expressed in different parts of the world to understand these differences better. This paper aims to compare 4 emotions namely, anger, happiness, sadness, and neutral as expressed by speakers from 4 different languages (Canadian French, Italian, North American English and German - Berlin Deutsche) using modern digital signal processing methods and convolutional neural networks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-97549-4_52,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-89508-2_135,Short Video Audience Identification Data Recommended by Multiple Neural Network Algorithms,The 2021 International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy,10.1007/978-3-030-89508-2_135,Springer,2022-01-01,"Short Video is a hot new media in recent years. It has broad prospects for development, and the users are increasing rapidly. Therefore, it is significant to research and analyze mobile and audience identification data. Now, it is the Internet age, we get the useful information by collecting a large number of user behavior records. Traditional media platforms have problems such as serious information overload or slow transmission speed. So, in the big data era, how to generate interest in the short video industry through the analysis of precise audiences has become an urgent problem to be solved. The authors use survey and audience identification data recommended by multiple neural network algorithms to analysis. The research shows that the young and middle-aged people are mainly users and producers. The users’ interest, high-quality educational content and entertainment are the potential factors affecting the popularity of video.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-89508-2_135,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-3089-8_23,Audio Driven Artificial Video Face Synthesis Using GAN and Machine Learning Approaches,Computational Intelligence in Pattern Recognition,10.1007/978-981-19-3089-8_23,Springer,2022-01-01,"Now-a-days a large number of people share their opinion in either audio or video format through internet. Some of them are real videos and some are fake. So, we need to find out the differences between these two with the help of digital forensics. In this paper, the authors will discuss the different types of artificial face synthesis methods and after that authors will analyze the deepfake videos using machine learning methods. In artificial face synthesis, based on an incoming audio stream in any language, a face image or source video of a single person is animated with full lip synchronization and synthesized expression. For full lip synchronization, GAN can also be used to train the generative models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-3089-8_23,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-2109-3_14,Speech Separation Using Deep Learning with MATLAB,Smart and Intelligent Systems,10.1007/978-981-16-2109-3_14,Springer,2022-01-01,"The essential aspect in the field of speech processing is to resolve the Cocktail Party Problem and to realize speech-based human–machine interaction. For speaker source separation, Deep Learning is used as a potent means in order to isolate the speakers using the binary masking method for two speakers. The Mask is estimated with a set of training data and validation data and subsequently applied for test data. The latter method is based on separating three speaker sources utilizing the same binary masking method which involves various threshold techniques utilized to isolate speakers. This article presents how masking is applied to two speaker model and extended to three speaker model. The threshold techniques are implemented in the real environment to verify the efficiency.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-2109-3_14,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-07012-9_29,Deep Learning Application of Image Recognition Based on Self-driving Vehicle,Emerging Technologies in Computer Engineering: Cognitive Computing and Intelligent IoT,10.1007/978-3-031-07012-9_29,Springer,2022-01-01,"A CNN (Convolutional Neural Network) is an artificial neural network used to evaluate visual pictures. It is used for visual image processing and is categorised as a deep neural network in deep learning. So, using real-time image processing, an AI autonomous driving model was built using a road crossing picture as an impediment. Based on the CNN model, we created a low-cost approach that can realistically perform autonomous driving. An end-to-end model is applied to the most widely used deep neural network technology for autonomous driving. It was shown that viable lane identification and maintaining techniques may be used to train and self-drive on a virtual road.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-07012-9_29,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-93119-3_3,Deep Learning Image Recognition for Non-images,Integrating Artificial Intelligence and Visualization for Visual Knowledge Discovery,10.1007/978-3-030-93119-3_3,Springer,2022-01-01,"Powerful deep learning algorithms open an opportunity for solving non-image Machine Learning (ML) problems by transforming these problems into the image recognition problems. The CPC-R algorithm presented in this chapter converts non-image data into images by visualizing non-image data. Then deep learning CNN algorithms solve the learning problems on these images. The design of the CPC-R algorithm allows preserving all high-dimensional information in 2-D images. The use of pair values mapping instead of single value mapping used in the alternative approaches allows encoding each n-D point with 2 times fewer visual elements. The attributes of an n-D point are divided into pairs of its values and each pair is visualized as 2-D points in the same 2-D Cartesian coordinates. Next, grey scale or color intensity values are assigned to each pair to encode the order of pairs. This is resulted in the heatmap image. The computational experiments with CPC-R are conducted for different CNN architectures, and methods to optimize the CPC-R images showing that the combined CPC-R and deep learning CNN algorithms are able to solve non-image ML problems reaching high accuracy on the benchmark datasets. This chapter expands our prior work by adding more experiments to test accuracy of classification, exploring saliency and informativeness of discovered features to test their interpretability, and generalizing the approach.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-93119-3_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-6601-8_28,Applications of AI Techniques like Machine Learning Methods and Deep Learning Models (ANNs) in Emerging Areas: A Review,Proceedings of International Conference on Wireless Communication,10.1007/978-981-16-6601-8_28,Springer,2022-01-01,"In today’s world, we come across numerous advancements in various fields of technology. In key areas, such as communication networks, microstrip antennas, signal and image processing (like speech and character recognition), Internet of things and embedded systems etc., there have been major breakthroughs aided and made possible through the use of artificial intelligence techniques such as machine learning and deep learning models (primarily including artificial neural networks). Some important developments propelled by the use of these techniques and its applications in the respective fields is what has been analysed in this paper.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-6601-8_28,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-76239-1_37,The Neural Network Model for a Multimedia Reinterpretation of the Modern Movement. The Representation of Transversal and Coherent Relationships in the Artistic and Architectural Productions of the De Stijl Movement,Digital Modernism Heritage Lexicon,10.1007/978-3-030-76239-1_37,Springer,2022-01-01,"The changed relationship of sharing of ways and thoughts, but also the different way of looking at reality, raises the interesting question of trying to imagine new ways to represent not so much the individual works of architects, but the relationships that have been established among different movements and among different disciplines. The ability, today, to use multimedia as a superimposition , juxtaposition and contrast of different media, allows to organize representation more on the flows and currents of thought than on a simple description of the works that characterized the period. It becomes interesting, then, in the light of all the artistic experiments produced by contemporaneity, to borrow ways and means to bend them to new ways of representation. It is through the complex drawing, borrowed from Migliari + Moles that we derive a methodology of representation of flows and relationships which, then, we propose to apply by way of example to the De Stijl Movement. Movement that, in the wider world of the Modern Movement, stands out for its formal “integralism”. Therefore, it will be simpler and more direct to experience the method of representation by analyzing a less complex movement but still representative of the Modern Movement in its entirety.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-76239-1_37,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-981-19-1076-0_22,Immersive Technologies in the Healthcare Space,Augmented Intelligence in Healthcare: A Pragmatic and Integrated Analysis,10.1007/978-981-19-1076-0_22,Springer,2022-01-01,"The field of life sciences and healthcare sector have always benefited from various digital innovations. One of the most beneficial technologies for healthcare is immersive technology. Immersive technologies such as VR, AR, and MR provide a full immersive experience by blending real and virtual worlds. Extended Reality (XR) is the new term to represent all immersive technologies. Currently, these technologies are used in almost all fields with the convergence of other relevant technologies. The role of these immersive technologies is inevitable in the healthcare sector as well. It provides many use cases with the support of other digital technologies and software techniques such as AI-based software methods, smart devices, sensors, robots, high performance computers (quantum computers), 6G networks and other compatible technologies. This chapter aimed to relate the state of the art in Immersive technologies related to healthcare by conducting a systematic review of the recent literature. With the support of digital technologies and artificial intelligence (AI), immersive technologies can offer significant assistance to medical professionals, surgeons, and even medical students in various fields. For instance, it can help a surgeon plan a complex surgery procedure by allowing the doctor to analyze layer by layer of a 3D holographic image of the patient. Without any physical limitation, medical students can study anatomy by examining a holographic body. Importantly, immersive technologies challenge age-related declines, especially by increasing morale. In conclusion, immersive technologies have already developed a strong identity in the healthcare sector and will be key components of healthcare in the future.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-1076-0_22,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-4538-9_18,A Review on Current IoT-Based Pasture Management Systems and Applications of Digital Twins in Farming,Proceedings of Third International Conference on Sustainable Computing,10.1007/978-981-16-4538-9_18,Springer,2022-01-01,"Smart farming concepts have transformed the farming sector. Smart farming allows enhanced management of farms using technologies like Internet of Things (IoT), Artificial Intelligence (AI) and robotics to increase the produce while minimizing human involvement. The evolution of IoT technology allows the use of intelligent sensors to collect and analyze farm data for improved management approach. IoT-based systems produce a huge amount of farm data that is been easily managed by the idea of big data. Big data provide easy collection of data with large storage capacity supplied on the cloud. The objective of implementing smart farming systems is to escalate productivity and sustainability. However, even though the idea of trying to ease the farm operations is a success, there are challenges that are brought by the use of these newer technological methods. With the focus on livestock farming, this paper reviews the current IoT pasture management systems implemented for better management of farm operations. The paper also reveals the applications of digital twins in farming to address the problems created by smart farming.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-4538-9_18,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-98883-8_17,Automatic Tuning of the Motion Control System of a Mobile Robot Along a Trajectory Based on the Reinforcement Learning Method,Pattern Recognition and Information Processing,10.1007/978-3-030-98883-8_17,Springer,2022-01-01,"The is a description of the development process of an adaptive motion controller for a two-wheeled mobile robot along a color-contrast line. The learning process of the controller took place on the basis of the digital twin of the indicated robot, with the use of reinforcement learning technology. The digital twin and reinforcement learning implemented in MATLAB/Simulink frameworks, for the latter, the Reinforcement Learning Toolbox library was used. The mobile robot equipped with a differential driver, and the PID-controller adjusted the angular speed of the rotation of both wheels. Therefore, the main purpose of the research work was to determine the coefficients of the PID-controller. The Twin-Delayed Deep Deterministic Policy Gradient Agents was used as a learning algorithm, which used a deterministic actor and a Q-value critic. As a function of reward, the minimization of the distance between the center of the robot and the edge of the nearest section of the color-contrast line was used, as well as the calculation of the angle (γ) between the tangent to the edge of the ellipse curve, where the robot should be located and guided at the current time. The developed environment, which would be influenced by the agent, as well as the policy of the agent, which provides a detailed diagram of the neural network for the actor and the critic. Different methods carried out for the comparison of the results, such as genetic algorithms and reinforcement learning by the TD3 algorithm. The experiments have shown that the founded coefficients of the PID-controller afford control the movement of the robot accurately, even on an unfamiliar track.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98883-8_17,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-94182-6_17,Design of Single Camera 3D Laser Scanning System Based on Artificial Intelligence,IoT and Big Data Technologies for Health Care,10.1007/978-3-030-94182-6_17,Springer,2022-01-01,"At present, the binocular/multi-visual 3D laser scanning system needs to register and match the acquired images, and needs to calibrate and calibrate frequently, which is easy to cause errors. In the hardware design, we choose the model and parameter of camera, laser, stepper motor and its driver. In the software design, we use octree structure to index the 3D data tree structure, preprocess the scanning data, introduce artificial intelligence scanning algorithm, design the optimal hyperplane based on SVM to complete the noise recognition, and finally extract the scanning image features to complete the design of the whole scanning process. Experimental results show that, compared with the traditional system, the light stripe obtained by the design system is complete, and the ranging result is more close to the actual distance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-94182-6_17,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-3930-2_17,Exploring a Large Dataset of Educational Videos Using Object Detection Analysis,"Ludic, Co-design and Tools Supporting Smart Learning Ecosystems and Smart Education",10.1007/978-981-16-3930-2_17,Springer,2022-01-01,"Processing data and analyzing it are very important these days and have many applications in real life. In the case of very large video datasets, it became counterproductive, if not impossible, to visualize, analyze, and label the videos manually, due to their sheer number and length. Thus, automatically explore video datasets to understand what the videos are depicting, both for having a general view of the videos and a per video understanding of the activities. For example, if they contain sensitive images, violence and other such activities the videos could be flagged for manual checking. We turned our attention toward understanding educational videos, due to the great potential of improving the educational process by understanding written data (such as forums, chats, documents), audio, and video data. In this paper, we continue our work regarding an exploratory analysis of videos collected using YouTube-8 M, with the “school” keyword in their metadata. Thus, we attempt to detect the type of activity in a video based on the number of unique people detected and tracked, and on the objects detected by YOLOv3. This allows us to detect the educational activities in the dataset and to estimate the distribution of the activities in videos uploaded worldwide.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-3930-2_17,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8403-6_36,Automatic Audio and Video Summarization Using TextRank Algorithm and Convolutional Neural Networks,"Advances in Data Computing, Communication and Security",10.1007/978-981-16-8403-6_36,Springer,2022-01-01,"There is a surge in data generation in last few years due to digitalization of daily activities, ubiquitous use of smartphone-based applications using Internet that include audio, text and video data. Summarization is the process of generating short and useful data from these large amounts of varied data. In this paper, an application is developed to automatically summarize the extensive and wordy individual speaker audios as well as videos into crisp and concise format. We extended the existing techniques of extractive document summarization using natural language processing (NLP) and neural networks to automatic audio and video summarization. The summarization is done by two approaches which are novel in terms of their implementation technique. One uses TextRank algorithm from genism, a powerful Python library used for performing NLP tasks. The other approach uses convolutional neural network (CNN)-based model that works on the word embedding of the sentences stated in the audio and video. We have used the BBC News Summary dataset for building the CNN model. For evaluating the automatically generated summary, we used ROUGE metric. The results illustrate that the NLP-based approach gave better results for both audio and video summarization.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8403-6_36,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-86165-0_23,Interactive Artificial Neural Network Model for UX Design,"International Conference on Computing, Communication, Electrical and Biomedical Systems",10.1007/978-3-030-86165-0_23,Springer,2022-01-01,"Video game platforms provide a variety of sensors to improve player experiences. The design of robust recognizers for player behaviour with sensors nevertheless provides developers with major challenges. In addition, sensor-based controls provide a small player customization relative to standard inputs. In past research into moving music systems, interactive machine learning (IML) techniques using interactive artificial neural network (IANN) have successfully been employed to allow developers and end users to customize sensor-based interfaces. Current standalone game creation and delivery platforms are therefore not suitable for use in IANN. We also developed an integrated IANN solution in the form of a visual node framework supporting classification, return and time series analysis of sensor data in order to facilitate more powerful and scalable use of sensors by developers and players.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-86165-0_23,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8862-1_5,"Development of Real-Time Violence Detection with Raspberry Pi
","Proceedings of Third International Conference on Communication, Computing and Electronics Systems",10.1007/978-981-16-8862-1_5,Springer,2022-01-01,"The detection of real-time object has drawn an increased interest in surveillance strategies, and it is one of the applications of CNNs. This project has focussed on the detection of fire and pistols in places that are tracked by cameras and fires in the home, business explosions, and wildfires are all major headaches that have negative implications for the environment. Mass shooting and violence caused by guns are also on the upward push in certain components of the sector. Such type of incidents is time touchy and may purpose a huge loss to lifestyles and assets. Hence, the proposed system is designed with YOLO v3 that detects perfectly by analysing the video or live feed frame to frame to discover such type of situations in real time and send an alert to the authorities through an email and mobile message. The model has been working well on data sets like IMFDB and Fire Net with accuracy more than 83%. Experimental output satisfies the aim of the proposed design is implemented on Raspberry Pi and that is tested with unique situations, and its detection is also very fast, and it can be installed indoor and outdoor.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8862-1_5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-05939-1_3,Photographic Composition Guide for Photo Acquisition on Augmented Reality Glasses,"Virtual, Augmented and Mixed Reality: Design and Development",10.1007/978-3-031-05939-1_3,Springer,2022-01-01,"Capturing meaningful moments by using cameras is a major application of Augmented Reality Glasses (AR Glasses). Taking photos with head-mounted cameras of AR Glasses bring a new photo acquisition experience to users because there are no viewfinders conventional cameras have. Users may experience difficulties to figure out the region the head-mounted camera will capture on AR Glasses. To address this issue, we propose a photographic composition guide for AR Glasses. The proposed method analyzes video streams from the camera and automatically determine the image region that has a high aesthetic quality score. Photos taken from the recommended position result in a better photographic composition. Our method achieved 4.03 in Mean-Opinion-Score (MOS) test, demonstrating that our method corresponds to human’s expectation on aesthetic quality of photos.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-05939-1_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-2406-3_55,Smart Waste Management System,Proceedings of the 12th National Technical Seminar on Unmanned System Technology 2020,10.1007/978-981-16-2406-3_55,Springer,2022-01-01,"The increasing amount of waste in landfill has created a serious environmental problem which demands a more reliable solution in handling the collection of wastes. To this date, recycling is one of the solutions to manage the waste as it collects and processes recyclable materials into new products instead of throwing the trash to the landfill. However, the consciousness of recycling in our society is still devastatingly lower than expected as people are faced with many challenges that impede them to recycle. One of the challenges is to segregate the waste according to its group. People are still having difficulty to clearly distinguish recyclable materials due to the lack of recycling knowledge. Thus, this paper aims to develop a system that can separate the waste automatically and channel them to the proper bins. To do that, a camera is used to capture the image of the waste. Then, image classification using deep learning model is used to classify different types of wastes. The developed model is then embedded in Raspberry Pi and a servo motor is used to direct the waste to the respective bins for real-world implementation. Experimental results show that the proposed system can identify the categories of waste within the accuracy of 77–85%. This system is expected to deliver the importance of recycling and cultivate recycling practices to the public and finally reduced waste generation on land.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-2406-3_55,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06368-8_4,A Deep Learning-Based Dessert Recognition System for Automated Dietary Assessment,Wireless Mobile Communication and Healthcare,10.1007/978-3-031-06368-8_4,Springer,2022-01-01,"Over the past few years, a significant part of scientific research has been focused on the assistance of patients who suffer from obesity or diabetes. Monitoring the food intake through self-report in diet control applications has been proven both time-consuming and non-practical and can be easily sidelined especially by children. In this paper, we propose the design and development of a novel system, which will assist obese or diabetic patients. We have implemented transfer learning as well as fine-tuning to different pre-trained CNN models to automatically distinguish dessert from non-dessert food images. For further training of these deep neural networks, a new dataset was constructed, which derived from the original Food-101 dataset. To be precise, 19 categories of desserts were used, which correspond to 19K images combined with 19K images of non-desserts. Google InceptionV3 architecture appeared to have the best performance, reaching a validation accuracy of 95.89%. To demonstrate feasibility of out platform and the independence of data biases, we constructed another data collection of food images, which was captured under challenging light and angle of capture conditions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06368-8_4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-97774-0_9,Defects Detection System of Medical Gloves Based on Deep Learning,Smart Computing and Communication,10.1007/978-3-030-97774-0_9,Springer,2022-01-01,"In industrial production, medical gloves with tear, stain and other defects will be produced. In traditional manual mode, the efficiency and accuracy of defect detection depend on the proficiency of spot-check workers, which results in uneven glove product quality. In this paper, a surface defect detection system of medical gloves based on deep learning is designed for the automatic detection with high efficiency and accuracy. According to the industrial requirements of high real-time, the system adopts a cache scheme to improve the data reading and writing speed, and an Open Neural Network Exchange (ONNX) to effectively improve the speed of model reasoning. For the demands of high detection accuracy, the system designs a dual model detection strategy, which divides texture detection and edge detection into two steps. The advantage of this strategy is to remove most useless information while ensuring the effective information of the image. Furthermore, two auxiliary models are used to promote the accuracy of detection based on classification methods. Finally, experiments are proposed to verify the functional indicators of the system. After the on-site test of the production line in the medical glove factory, the system has the ability to detect the gloves of two production lines with high real-time. The product missed detection rate is less than 2%, and the product mistakenly picked rate is less than 5/10000. Verified by the industry of gloves, the system can be put into production line.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-97774-0_9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-7985-8_2,A Literature Survey on Speech Enhancement Based on Deep Neural Network Technique,ICCCE 2021,10.1007/978-981-16-7985-8_2,Springer,2022-01-01,"Speech enhancement is the process of treating noisy speech signals to improve human perception as well as improve system understanding of the signal. To keep the signal undistorted while also reducing noise is a difficult task, which results in a limited performance of speech enhancement systems. For speech signals having medium or high signal to noise ratio value, the aim is to come up with subjectively practical signals, and for signals with low SNR, the aim is to reduce noise level while still retaining the intelligibility. Many noise reduction algorithms improve overall speech quality but little progress has been made to improve speech intelligibility. In this paper the necessity of speech enhancement, its different applications, overview of classification and various methods associated with it has been presented and a substantial literature review on such speech enhancement systems with various methods and platforms is done. Deep convolution neural network-based speech enhancement system is intended by optimizing the loss functions like Extended Short-Time Objective Intelligibility and Mean Square Error. The loss function required for training it are optimized using Harris Hawk Optimization.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7985-8_2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-4625-6_24,Speech Emotion Recognition Using Mel Frequency Log Spectrogram and Deep Convolutional Neural Network,Futuristic Communication and Network Technologies,10.1007/978-981-16-4625-6_24,Springer,2022-01-01,"In recent years, speech emotion recognition (SER) has engrossed more attention in speech processing because of its potential in various speech-based intelligent systems. In deep learning algorithms to capture discriminative features of the audio emotion samples, a large number of features are required, which increases the computational complexity of the network. This paper presents a three-layered sequential deep convolutional neural network (DCNN) based on mel frequency log spectrogram (MFLS) for emotion recognition. Mel frequency log spectrogram that confines the salient information from the emotion speech corpus and two-dimensional DCNN. Exploratory outcomes on the Berlin Emo-DB dataset show that the proposed method gives 95.68 and 96.07% accuracy for the speaker-dependent and speaker-independent approaches. The performance of the proposed method is compared with CNN and CNN-LSTM on the Berlin Emo-DB dataset and results in improved accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-4625-6_24,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-99200-2_37,Dual-Channel Speech Enhancement Using Neural Network Adaptive Beamforming,Communications and Networking,10.1007/978-3-030-99200-2_37,Springer,2022-01-01,"Dual-channel speech enhancement based on traditional beamforming is difficult to effectively suppress noise. In recent years, it is promising to replace beamforming with a neural network that learns spectral characteristic. This paper proposes a neural network adaptive beamforming end-to-end dual-channel model for speech enhancement task. First, the LSTM layer is used to directly process the original speech waveform to estimate the time-domain beamforming filter coefficients of each channel and convolve and sum it with the input speech. Second, we modified a fully-convolutional time-domain audio separation network (Conv-TasNet) into a network suitable for speech enhancement which is called Denoising-TasNet to further enhance the output of the beamforming. The experimental results show that the proposed method is better than convolutional recurrent network (CRN) model and several popular noise reduction methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-99200-2_37,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ReferenceWorkEntry,doi:10.1007/978-981-16-2210-6_2,Information and Communication Technology Application in the Asian Tourism Industry,Handbook of Technology Application in Tourism in Asia,10.1007/978-981-16-2210-6_2,Springer,2022-01-01,"Tourism becomes a major player in global trade and also a major income source for several developing countries. Tourism is one industry that has adopted the use of information and communication technology (ICT) in its services realizing its potential in enhancing the business prospects since mid of the last century. It was in 1946 that the first automated booking system was installed by the American Airlines. Since then, the use of ICT in tourism has diversified for providing large source of information to tourists and better and faster service to its customers, improving relationship between distribution channels, and enabling promotion and distribution of products directly to its customers. The advent of information technology during the years has seen tourism industry capitalizing the use of Internet, eBusiness applications, and even artificial intelligence. This has encouraged small players to launch business in tourism and depend on ICT for their product branding and marketing. The tourists themselves have online platforms to review the products and make the best choice of the destinations and packages. Robots have already taken up the role of waiters in hotels in many Asian countries including India. Recognizing the importance of ICT in tourism, the United Nations World Tourism Organization (UNWTO) had chosen the theme for World Tourism Day 2018 as “Tourism and the Digital Transformation.” This paper looks into the historic evolution of ICT in tourism, its application in different areas of the tourism industry especially in the Asian context, barriers in adopting to new technology, and the digital divide in tourism.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-2210-6_2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-981-16-5461-9_1,Thirty Years of Research on Application of Technology in Tourism and Hospitality Industry: A Systematic Literature Review,Technology Application in Tourism in Asia,10.1007/978-981-16-5461-9_1,Springer,2022-01-01,"In the fourth industrial revolution, the application or adoption of technology in the tourism and hospitality industry became mainstream and received significant attention from research scholars. But application of Systematic Literature Review (SLR) to elucidate the dynamics of technology in the tourism and hospitality industry has hardly been experimented in any prior research. Therefore, the literature gap pertaining to the nexus of technology and tourism industry is addressed in this chapter. Although the literature in tourism has extensively discussed technology application or adoption in tourism and hospitality industry, the research structure in this field remained scattered and fragmented. The current chapter is intended to bridge the gap and is attempted to achieve three goals. First, to understand the corpus of research on “Application and Adoption of technology in tourism and hospitality”, SLR is conducted only based on research articles extracted from Web of Science and Scopus on the specified topic using some specific search strings to unfold the corpus of research on use and adoption of technology in tourism and hospitality industry. To conduct the SLR, the study adopts four bibliometric analysis process named-co-citation, network visualization through co-occurrence data, multi-dimensional scaling, and hierarchical cluster analysis that shed light on the intellectual structure on technology application in tourism industry. Second, the chapter illuminates on how the technology shape tourism in a new form. Third, the chapter delineates the models and theories that were addressed in the previous studies to describe how technology is applied and adopted in the tourism industry. The findings of the study help the researchers and academicians to understand the research trends in this area and how knowledge structure has been evolved over the years.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5461-9_1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-82430-3_16,"Emerging Technologies for Health, Safety and Well-being in Construction Industry",Industry 4.0 for the Built Environment,10.1007/978-3-030-82430-3_16,Springer,2022-01-01,"New technologies brought about by digital transformation in the construction sector will inevitably lead to a change in the traditional ways of dealing with health and safety risk management as has occurred with other specialties such as architecture, structural, mechanical, electrical, heating and ventilation. Existing procedures will need to be adapted, and new routines are created. The technical and soft skills of employees will need to be updated, enhancing competitiveness and sustainability. Based upon ongoing research studies, this chapter aims to explain how health, safety and well-being (HSW) risk management and associated skills can be dealt with using technologies such as building information modelling, virtual and augmented reality, Internet of Things, teleoperation, soft skills, artificial intelligence applied to big data and cybersecurity. Fields where these key technological developments (KTD) can be applied include: technical training; documental/contractual management; hazards identification; on-site monitoring and implementation; emergency planning; and accident investigation. High-level risks including pedestrian/vehicle collisions and falls from heights will also be covered. Based on a review of the relevant literature, an analysis of the application of KTDs to the main fields indicated in EU directive 92/57/CE has been conducted. The main conclusions are that if correctly implemented, these new ways of handling risk management will enable a better perception of risks and enhanced preventive measures, allow improvement of training levels, logistic and financial management and, promote improved integration of prevention measures into work planning. Raising, the skills levels of HSW will be a key enabler for teams to increase their knowledge and understanding of KDT in HSW contexts.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-82430-3_16,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-981-16-5461-9_20,Information and Communication Technology Application in the Indian Tourism Industry,Technology Application in Tourism in Asia,10.1007/978-981-16-5461-9_20,Springer,2022-01-01,"Tourism has established as one of the fastest growing sectors in the world as the business volume of tourism is being surpassed only by the manufacturing sector. In 2019, the travel and tourism industry contributed to approximately 10.3% of the global GDP and generated 330 million jobs. As tourism has become a greater source of income and employment, many countries have increased their investment in the tourism sector and India is no exception. The investment in the sector was not limited only to mere development of physical infrastructure like hotels, airports etc., but also on the development and application of information and communication technology (ICT) to enhance the service quality in this sector. This has resulted in a spontaneous rise of foreign tourist arrivals to India in the millennium from 2.54 million in 2001 to 10.93 million in 2019. Tourism industry witnessed the use of ICT globally in its services since mid of last century. The use of ICT in Tourism has diversified from automated booking systems to providing large source of information to tourists, better and faster service to its customers, improve relationship between distribution channels and enabling promotion and distribution of products directly to its customers. With the growth and development of ICT, the tourism sector capitalised on the use of internet, artificial intelligence and other e-business applications. Tourists have also started depending on the information technology platforms to review the products and make the best choice of their destinations. This paper looks into the application of the ICT in different sectors of tourism industry especially in the Indian context.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5461-9_20,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06242-1_12,Capacity Estimation from Environmental Audio Signals Using Deep Learning,Artificial Intelligence in Neuroscience: Affective Analysis and Health Applications,10.1007/978-3-031-06242-1_12,Springer,2022-01-01,"Estimating the capacity of a room or venue is essential to avoid overcrowding that could compromise people’s safety. Having enough free space to guarantee a minimal safety distance between people is also essential for health reasons, as in the current COVID-19 pandemic. Already existing systems for automatic crowd counting are mostly based on image or video data, and some of them, using deep learning architectures. In this paper, we study the viability of already existing Deep Learning Crowd Counting systems and propose new alternatives based on new network architectures containing convolutional layers, exclusively based on the use of environmental audio signals. The proposed architecture is able to infer the actual capacity with a higher accuracy in comparison to previous proposals. Consequently, conclusions from the accuracy obtained with out approach are drawn and the possible scope of deep learning based crowd counting systems is discussed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06242-1_12,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ReferenceWorkEntry,doi:10.1007/978-981-13-6106-7_134-1,Future of Health Professions Education Curricula,Clinical Education for the Health Professions,10.1007/978-981-13-6106-7_134-1,Springer,2022-01-01,"This chapter discusses the future of health professions education (HPE) curricula using David Kern’s six steps of curriculum development as an organizing principle. It discusses several problems that future healthcare professionals (HCPs) will face including challenges of greater scope, less time, and less resources to promote health and provide patient care. It also discusses the transition from declarative knowledge and rote memorization to more application of information and problem-solving while leveraging the technology at HCPs’ fingertips. There will also be highlights of future technologies that enhance and transform patient care, the massive amounts of data that will be generated, and the future of technology-enabled learning, teaching, and assessment.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-13-6106-7_134-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ReferenceWorkEntry,doi:10.1007/978-3-030-60016-7_50,"Role of Digital Technologies, Robotics, and Augmented Realities",Handbook of Cultural Heritage Analysis,10.1007/978-3-030-60016-7_50,Springer,2022-01-01,"Several examples testify how the evolution in applied technologies to cultural heritage has improved significantly in the last decades in terms of accuracy and reliability of measurement, restitution, and management of the acquired data. This evolution is not always accompanied by the simplification of procedures, lowering of costs, and, more importantly, awareness-raising of the identity value of the investigated asset. In this article, some case studies where different types of technologies have been used are presented together with a recent study of applied robotics in cultural heritage. The aim is to construct an exhaustive picture of the technologies currently in use within the complex conservation process. To follow, a series of urgent reflections are introduced in order to stimulate, contribute, and increase the necessary interdisciplinary and intercultural debate on the issue of safeguarding cultural assets as a driving force for civil progress, paying particular attention to 3D digital survey. Above all, the paper expands the issue to a wider analysis of the general and progressive anthropological transformations produced on human society by the speed of technological innovation. Finally, the paper will attempt to verify how much and what role the identity values expressed by cultural heritage will still exercise on technologically globalized cities and societies of the twenty-first century.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-60016-7_50,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-98225-6_8,"Digital Transformation, Leadership, and Markets",Blockchain and Other Emerging Technologies for Digital Business Strategies,10.1007/978-3-030-98225-6_8,Springer,2022-01-01,"In the electronic era, the company environments are evolving into an insecure, composite, and indecisive atmosphere. This quick evolution can be blamed upon the rise of technology, competitive market, and legal and regulatory compliances. Teichert (Acta Univ Agriculturae Silviculturae Mendelianae Brunensis 67:1673–1687, [ 1 ]) observe this condition as the reason for forcing the business plans and policies of organizations to adapt to technological innovations. According to Gartner (no date), digital transformation could be termed as the evolution of technology as well as digital optimization including the discovery of advanced digital business models. Gartner also suggests labelling digital transformation as ‘digitization’ since nationalized establishments use the term digital transformation to mention normal IT practices like usage of online services. Vaughan [ 2 ] suggests the main four areas to consider while an organization plan on a digital transformation project as process transformation, business model transformation, domain transformation, cultural/organizational transformation. The benefit of a successful digital transformation according to the author is improved processes, fostering collaborations, broadening service options, and transforming the customer experience.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98225-6_8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-74648-3_1,Drive Towards 6G,Enabling 6G Mobile Networks,10.1007/978-3-030-74648-3_1,Springer,2022-01-01,"As fifth-generation (5G) mobile networks are being rolled out, the telecom industry and academia are now coordinating the 6G research effort towards defining the requirements and use cases for beyond 5G (B5G) or so-called sixth-generation (6G) mobile networks. 6G envisages an evolutionary communication platform based on complete network softwarisation, inclusive communications mediums including satellite, and ultra-dense networks to cater for the market demands that requires ultra-high speeds, tactile response time, and lower cost of network ownership by 2030. This chapter provides an overview of the use cases for B5G/6G systems, including holographic telepresence, digital twin, connected robotics, distributed artificial intelligence, and blockchain technologies. It further reviews the current standardisation and deployment status of 5G technology as a baseline and the drive towards 6G by identifying key enabling technologies, system requirements, and an overview on global B5G/6G activities.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-74648-3_1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-98249-2_4,Video Coding Impairment Models,Quality of Experience Modeling for Cloud Gaming Services,10.1007/978-3-030-98249-2_4,Springer,2022-01-01,"In this chapter, multiple video quality models that can be used to predict the video coding impairment factor of the proposed framework are described. In general, two approaches are taken into account; first a direct modeling approach, where the overall video coding impairment is directly predicted based on the developed model. The second approach uses the multidimensional approach where the sub-dimensions of video quality (cf. Sect. 2.5.5.2 ) are first predicted, then the overall video coding impairment is predicted based on the predicted sub-dimensions of video quality. The first figure in this chapter illustrates the two approaches as well as a short overview of where each proposed quality model is placed within the framework. The chapter is structured based on the type of video quality model according to the level of access to the video information (cf. Sect. 2.6 ), planning, bitstream-based, and signal-based video quality models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98249-2_4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-74032-0_21,Web Service for Point Cloud Supported Robot Programming Using Machine Learning,"Annals of Scientific Society for Assembly, Handling and Industrial Robotics 2021",10.1007/978-3-030-74032-0_21,Springer,2022-01-01,"In industrial automation, the use of robots is already standard. But there is still a lot of room for further automation. One such place where improvements can be made is in the adjustment of a production system to new and unknown products. Currently, this task includes the reprogramming of the robot and a readjustment of the image processing algorithms if sensors are involved. This takes time, effort, and a specialist, something especially small and middle-sized companies shy away from. We propose to represent a physical production line with a digital twin, using the simulated production system to generate labeled data to be used for training in a deep learning component. An artificial neural network will be trained to both recognize and localize the observed products. This allows the production line to handle both known and unknown products more flexible. The deep learning component itself is located in a cloud and can be accessed through a web service, allowing any member of the staff to initiate the training, regardless of their programming skills. In summary, our approach addresses not only further automation in manufacturing but also the use of synthesized data for deep learning.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-74032-0_21,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-5987-4_73,Real-Time Multi-Object Tracking of Pedestrians in a Video Using Convolution Neural Network and Deep SORT,ICT Systems and Sustainability,10.1007/978-981-16-5987-4_73,Springer,2022-01-01,"Multi-Object tracking in video processing is a challenging task in computer vision. Typically, tracking the objects in a video is performed using binocular vision or top-down camera movements. These methods encounter the problem of rigid boundaries on the counting area application scene. To overcome this problem, in this paper, a novel object detection algorithm is proposed by revamping YOLOv3 and real-time multi-object tracking is done by employing Deep SORT to track the targets by using movement representation and data association algorithms. It is a tracking-by-detection algorithm. The proposed algorithm is experimented on different public datasets and own dataset of videos of real scenes relevant to pedestrian movements. The results of experiments indicate that the proposed algorithm yields higher accuracy and exhibits better performance in real-time object tracking as compared to other object tracking techniques in the literature and hence is a reliable object-tracking algorithm.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5987-4_73,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-97610-1_31,A (2+1)D Attention Convolutional Neural Network for Video Prediction,Artificial Intelligence in Data and Big Data Processing,10.1007/978-3-030-97610-1_31,Springer,2022-01-01,"Video prediction is a challenging generative modeling task. The objective of algorithms is to generate future scenes from a given image sequence. However, instead of giving a ground-truth prediction, these methods often give blurry predictions. This is a major challenge to predictive models. The causes of the blurry predictions are these models didn’t capture the features of the context and the variation of motion from the input. In this paper, we introduce (2+1)D Convolutional Neural Network (CNN) combined with two attention modules named spatial and temporal attention. The (2+1)D CNN helps the model learn the spatial-temporal features better than 2D CNN. And two attention modules are to boost the representation power of CNNs. The experiment results have shown that our approach achieved state-of-the-art performance in video prediction on standard datasets in terms of both PSNR and SSIM compared to existing methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-97610-1_31,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-87664-7_3,Multimedia Forensics Before the Deep Learning Era,Handbook of Digital Face Manipulation and Detection,10.1007/978-3-030-87664-7_3,Springer,2022-01-01,"Image manipulation  Deep learning Multimedia forensics is as old as photography itself, and powerful media editing tools have been around for a long time. Using such conventional signal processing methods, it is possible to modify images and videos obtaining very realistic results. This chapter is devoted to describe the most effective strategies to detect the widespread manipulations that rely on traditional approaches and do not require a deep learning Deep learning strategy. In particular, we will focus on manipulations like adding, replicating, or removing objects and present the major lines of research in multimedia forensics Multimedia forensics  before the deep learning Deep learning  era and the rise of deepfakes DeepFake . The most popular approaches look for artifacts related to the in-camera processing chain (camera-based clues) or the out-camera processing history (editing-based clues). We will focus on methods that rely on the extraction of a camera fingerprint Camera fingerprint  and need some prior information on pristine data, for example, through a collection of images taken from the camera of interest. Then we will shift to blind methods that do not require any prior knowledge and reveal inconsistencies with respect to some well-defined hypotheses. We will also briefly review the most interesting features of machine learning Machine learning - based methods and finally present the major challenges in this area.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-87664-7_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-95531-1_23,SOUND OF(F): Contextual Storytelling Using Machine Learning Representations of Sound and Music,"ArtsIT, Interactivity and Game Creation",10.1007/978-3-030-95531-1_23,Springer,2022-01-01,"In dreams, one’s life experiences are jumbled together, so that characters can represent multiple people in your life and sounds can run together without sequential order. To show one’s memories in a dream in a more contextual way, we represent environments and sounds using machine learning approaches that take into account the totality of a complex dataset. The immersive environment uses machine learning to computationally cluster sounds in thematic scenes to allow audiences to grasp the dimensions of the complexity in a dream-like scenario. We applied the t-SNE algorithm to collections of music and voice sequences to explore the way interactions in immersive space can be used to convert temporal sound data into spatial interactions. We designed both 2D and 3D interactions, as well as headspace vs. controller interactions in two case studies, one on segmenting a single work of music and one on a collection of sound fragments, applying it to a Virtual Reality (VR) artwork about replaying memories in a dream. We found that audiences can enrich their experience of the story without necessarily gaining an understanding of the artwork through the machine-learning generated soundscapes. This provides a method for experiencing the temporal sound sequences in an environment spatially using nonlinear exploration in VR.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95531-1_23,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-4016-2_51,Human Motion Detection and Recognition from Video Surveillance Based on Machine Learning Approaches,Smart Trends in Computing and Communications,10.1007/978-981-16-4016-2_51,Springer,2022-01-01,"Detecting human and human motion through a video surveillance system is an important task. It helps in various fields, like crowd analysis, identifying abnormalities in human behavior in public places and identifying a person and their gender. To detect a human and its motion in a video, the first step is to detect the moving object correctly. Background subtraction method is an efficient way to detect the moving object efficiently on the foreground frame. Object detection and extraction could be performed in various ways. Once the object is found, then a classification method is applied to recognize the object and its motion. In this paper, first, the background subtraction method is applied to detect the person in a video input file. Then, this method is applied to track the human’s motion through the entire video. In the second step, the histogram of oriented gradient method (HOG) is used to extract the features from the input video file. And, in the last step, support vector machine (SVM) is used to classify the detected person’s identity and its motion. In this experiment, two different multi-class classifiers are used, SVM and decision tree (DT), to compare the performance of the classification models. Finally, a detailed comparison of each person and the motion class is performed to compare the classification rate of the two classifiers.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-4016-2_51,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-7610-9_3,Audio Denoising Using Deep Neural Networks,Intelligent Data Communication Technologies and Internet of Things,10.1007/978-981-16-7610-9_3,Springer,2022-01-01,"Improving speech quality is becoming a basic requirement with increasing interest in speech processing applications. A lot of speech enhancement techniques are developed to reduce or completely remove listeners fatigue from various devices like smartphones and also from online communication applications. Background noise often interrupts communication, and this was solved using a hardware physical device that normally emits a negative frequency of the incoming audio noise signal to cancel out the noise. Deep learning has recently made a break-through in the speech enhancement process. This paper proposes an audio denoising model which is built on a deep neural network architecture based on spectrograms (which is a hybrid between frequency domain and time domain). The proposed deep neural network model effectively predicts the negative noise frequency for given input incoming audio file with noise. After prediction, the predicted values are then removed from the original noise audio file to create the denoised audio output.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7610-9_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-95711-7_7,Deep Learning Approaches for Speech Analysis: A Critical Insight,Artificial Intelligence and Speech Technology,10.1007/978-3-030-95711-7_7,Springer,2022-01-01,"The main objective of speaker recognition is to identify the voice of an authenticated and authorized individual by extracting features from their voices. The number of published techniques for speaker recognition algorithms is text-dependent. On the other hand, text-independent speech recognition appears to be more advantageous since the user can freely interact with the system. Several scholars have suggested a variety of strategies for detecting speakers, although these systems were difficult and inaccurate. Relying on WOA and Bi-LSTM, this research suggested a text-independent speaker identification algorithm. In presence of various degradation and voice effects, the sample signals were obtained from a available dataset. Following that, MFCC features are extracted from these signals, but only the most important characteristics are chosen from the available features by utilizing WOA to build a single feature set. The Bi-LSTM network receives this feature set and uses it for training and testing. In the MATLAB simulation software, the proposed model’s performance is assessed and compared to that of the standard model. Various dependent factors, like accuracy, sensitivity, specificity, precision, recall, and Fscore, were used to calculate the simulated outputs. The findings showed that the suggested model is more efficient and precise at recognizing speaker voices.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95711-7_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0332-8_41,Emotion Recognition from Speech Using Deep Learning,Proceedings of International Joint Conference on Advances in Computational Intelligence,10.1007/978-981-19-0332-8_41,Springer,2022-01-01,"For more than a decade, emotion recognition from speech has been a major research topic, following in the footsteps of its “big brothers,” speech and speaker recognition. It’s currently a growing field of study targeted at improving human-machine interaction. Our main goal is to propose a method that allows computers to identify eight emotions in speech. Initially, the audio files are passed through a Noise Reduction algorithm based on Spectral gating. 15 different spectral & Timbre features are then extracted from this audio data. Finally, the method classifies the audio to a certain emotion class. This research contributes significantly by developing two models based on rigorous testing, fine-tuning, and analysis that use two of the most popular deep learning algorithms: Artificial Neural Network (ANN) and Long Short Term Memory (LSTM) Network. The ANN and LSTM models exhibited an average accuracy of 88.3 and 87.4% using five different datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0332-8_41,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06458-6_17,How far can Deep Learning Improve Arabic Part of Speech Tagging?,Business Intelligence,10.1007/978-3-031-06458-6_17,Springer,2022-01-01,"The use of Deep Learning (DL) in Natural Language Processing (NLP) has seen a significant growth in the last few years. Part of Speech (POS) tagging is an important element for many NLP applications, including machine translation, sentiment analysis, and text summarization. It consists in identifying the very likely tag (noun, verb or particle, adverbs, adjectives etc.) for each word in a given text. The goal of this research is to conduct a systematic literature review of deep learning methods applied to Arabic POS tagging for the last two decades. The review was conducted using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework. More than 4000 papers were reviewed to extract all DL approaches used to develop POS taggers for the Arabic language. After multiple exclusion steps 12 articles were selected for a full review. Results show that Long Short-Term Memory (LSTM) and its extension Bidirectional LSTM (Bi-LSTM) models are the most used DL techniques for Arabic POS tagging, and they give better results according to the reviewed papers.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06458-6_17,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-96302-6_45,Binary Emotion Classification of Music Using Deep Neural Networks,Proceedings of the 13th International Conference on Soft Computing and Pattern Recognition (SoCPaR 2021),10.1007/978-3-030-96302-6_45,Springer,2022-01-01,"Music emotion classification is an area of research that helps in identifying the emotions from the songs and labelling the songs with emotion classes by extracting the features from the songs and comparing the features. Audio of music includes a great deal of acoustic information. There are several types of features within music that can influence the emotions conveyed through the audio. This paper presents a classification approach using a deep neural network that relies on acoustic features, making it apt for emotional analysis. There are 109,269 song lyrics, audio metadata with more than 13 audio features in the Music4All dataset. In sum, the findings from this work are that Artificial Neural Networks (ANN) can effectively capture all acoustic indication of all emotional cues included in the music.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96302-6_45,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-5640-8_45,Facial Expression and Genre-Based Musical Classification Using Deep Learning,Pervasive Computing and Social Networking,10.1007/978-981-16-5640-8_45,Springer,2022-01-01,"Facial expression is one of the natural ways to express emotions, and it plays a vital role in extraction of human emotion. Manually, setting the playlists with an oversized assortment of songs is an intensive task that require more time. For automating the song playlist, various algorithms are proposed recently. Most of the existing algorithms are slow in performance and not up to the level of expectation by the customer, and some of the music players are higher in cost due to their usage of hardware like sensors. This paper proposes a deep learning approach for classifying human expressions and playing music tracks based on the detected expressions, thus saving time and labor in the task of manually creating the playlist. It also aims to improve the usability of the music system in terms of the user’s preferences. During this project, we have a tendency to propose a system that contains music analysis, where a sturdy approach that is  capable of classifying an audio stream into totally different genre and a picture analysis, wherever the popularity of the face expression is employed. The music analysis divides audio files into totally different genre supported by convolution neural network. The primary step of the image analysis is face detection using Haar feature, and also, the second step of the image analysis is that the emotion recognition using the saved convolution neural network model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5640-8_45,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-04245-4_29,3D Point Cloud Classification Based on Convolutional Neural Network,6GN for Future Wireless Networks,10.1007/978-3-031-04245-4_29,Springer,2022-01-01,"With the development of science and technology, the requirements for 3D point cloud classification are increasing. Methods that can directly process point cloud has the advantages of small calculation amount and high real-time performance. Hence, we proposed a novel convolutional neural network(CNN) method to directly extract features from point cloud for 3D object classification. We firstly train a pre-training model with ModelNet40 dataset. Then, we freeze the first five layers of our CNN model and adjust the learning rate to fine tune our CNN model. Finally, we evaluate our methods by ModelNet40 and the classification accuracy of our model can achieve 87.8% which is better than other traditional approaches. We also design some experiments to research the effect of T-Net proposed by Charles R. Qi et al. on 3D object classification. In the end, we find that T-Net has little effect on classification task and it is not necessary to apply in our CNN.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-04245-4_29,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-92245-0_10,Using a Bi-Directional Long Short-Term Memory Model with Attention Mechanism Trained on MIDI Data for Generating Unique Music,Artificial Intelligence for Data Science in Theory and Practice,10.1007/978-3-030-92245-0_10,Springer,2022-01-01,"Generating music is an interesting and challenging problem in the field of machine learning. Mimicking human creativity has been popular in recent years, especially in the field of computer vision and image processing. With the advent of generative adversarial neural networks, it is possible to generate new similar images, based on trained data. But this cannot similarly be done for music, as music is not a static image but rather has a temporal dimension. When building models that perform this generative task for music, the learning and generation aspect can make use of a high-level representation of the musical data, such as MIDI (Musical Instrument Digital Interface) or musical scores. This chapter proposes a bi-directional long short-term memory model with attention mechanism capable of generating music based on MIDI data. The music generated by the model follows the theme/style of the music the model is trained on. The MIDI format makes it convenient to adjust the tempo, instruments, and other parameters of the generated music. A survey involving 100 human listeners who ranked five randomly generated songs obtained the result that 80% of song rankings were average or above, while 50% were good or very good. We conclude that the model has good potential for generating pleasing music for commercial and recreational purposes.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92245-0_10,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-1642-6_11,Digital Twinning of the Battery Systems—A Review,Advances in Renewable Energy and Electric Vehicles,10.1007/978-981-16-1642-6_11,Springer,2022-01-01,"The share of renewable energy in the grid is ever increasing. Whenever excess energy is generated, it can be stored in large-scale battery systems to support the grid during peak loads, nullifying the need for gas/diesel power plants. Battery storage is remodelling the global electric grid and is a key element for the shift to sustainable energy. In order to extract optimum performance and to ensure reliable, safe working of battery system, its constant monitoring is a must. Digital twin architecture of the battery system offers digital services to different stakeholders starting from the manufacturing through its secondary usage until the end of its life. Digital twin helps in bridging the gap and offers an innovative approach for both manufacturing and usage of the product. This paper focuses on reviewing different architecture of digital twins along with communication protocols, thus providing readers with an insight into recent trends in digital twin architecture for the battery system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-1642-6_11,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-70601-2_347,Feature Analysis for Speech Emotion Classification,XXVII Brazilian Congress on Biomedical Engineering,10.1007/978-3-030-70601-2_347,Springer,2022-01-01,"The automated classification of speech emotions is a potential candidate for clinical applications or even for educational purposes in the training of students. A description of procedures to evaluate emotional states from voice recordings in different environments is presented, together with extraction and the selection of features such as Fundamental Frequency, energy, formants and Mel-Frequency Cepstrum Coefficients (MFCC). For comparison purposes three speech corpus, one made by actors using the German language, and two made by inducing emotions in English and Brazilian Portuguese. A number of 208 features were extracted, this number was reduced using selection and emotion classification was performed by the use of a Support Vector Machine algorithm. The objective of this work was to compare results from different databases based on the classification of emotions with supervised learning algorithms.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-70601-2_347,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-96634-8_1,Detection of Parkinson’s Disease Using Machine Learning,Modern Approaches in Machine Learning & Cognitive Science: A Walkthrough,10.1007/978-3-030-96634-8_1,Springer,2022-01-01,"In this paper, the dataset containing features from voice recordings has been used to predict Parkinson’s disease. Key traits have been extracted from the speech dataset using pairwise correlation and k means clustering methods. We have used a combination of 4 models for classification which are K Nearest Neighbor, SVM, Random Forest and XGBoost. The highest accuracy achieved was 99.2% using XGBoost and remaining models gave the accuracy of 93.2, 93.59, and 95.6% respectively. We have also implemented a web application which is used for both detection of Parkinson’s disease in patients using the back end machine learning model and monitoring their day to day progress. The data entered by patients is stored in this website and can be retrieved at later date for future research.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96634-8_1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-92964-0_31,Malay Cued Speech Recognition Using Image Analysis: A Review,Advanced Materials and Engineering Technologies,10.1007/978-3-030-92964-0_31,Springer,2022-01-01,"Automatic real-time translation of gestured languages for hearing-impaired would be a major advancement on disabled integration path. Cued speech (CS) is a specific visual hand gesture that complements oral languages lip-reading. Cued speech in Bahasa Malaysia (CSBM) is an adaptation of cued speech for use in the Malay language. The cued speech recognition system is capable of detecting all necessary parameters of CS (handshape, hand position, and hand movement) and translate it to text equivalent. The aim is to help the deaf learn and practice the basic of cued speech consonant and vowel using hand gesture. This paper looks into the existing researches involved in this area and also the sensors and methods they used. Due to the limited number of researches for cued speech, related researches such as sign language (SL) translator systems and hand gesture recognition are also reviewed. This paper gives a general overview on the implementation of cued speech recognition system that automatically recognize a succession of cued speech hand gestures in real time. A Malay cued speech recognition system using image analysis is proposed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92964-0_31,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-92905-3_54,Providing Voice to Susceptible Children: Depression and Anxiety Detected with the Help of Machine Learning,International Conference on Intelligent Emerging Methods of Artificial Intelligence & Cloud Computing,10.1007/978-3-030-92905-3_54,Springer,2022-01-01,"Anxiety about children and depression are often undiagnosed without diagnosis. Untreated by such circumstances, together termed as internalization disorder, longer period adverse effects include substances misuse and enhanced suicide risks are associated with these conditions. Close to 20% of people have depressive or anxious conditions during their childhood. Anxiety and depression are chronic diseases which can begin during pre-school and damage the socio-emotional development of a child. This can be called innerization disorders. This project offers a novel method to identify children with speech-related illnesses. We plan to carry out machine learning examination of audio information from a duty to recognize children by an internal disease. The most critical features of speech are investigated thoroughly, demonstrating that children that are impacted have particularly low pitched speeches, repetitive speech inflection and context, as well as enhanced pitch responses to astonishingly control stimuli. Speech analytics are used to sense clinically derivative depressive and anxiety analyses in children that are among the ages of 3–7 year during a three-minute speech and machine learning tasks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92905-3_54,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-96634-8_29,Music Recommendation System Using Speech-Based Emotion Recognition,Modern Approaches in Machine Learning & Cognitive Science: A Walkthrough,10.1007/978-3-030-96634-8_29,Springer,2022-01-01,"An increasing demand in online entertainment has led to the advent of music streaming platforms. With most online streaming platforms consisting of the same pool of music, there is a constant need for platforms to introduce more advanced features. To attract a larger number of users by improving user experience, platforms are attempting to include intelligent systems to recommend music. Psychologists have shown a clear correlation between moods and music. This paper applies intelligent systems using voice-based mood recognition to present a music recommendation system using speech-based emotion recognition. The emotion of a person is determined from their voice using a Multi-Layer Perceptron with a 5-fold cross validation accuracy of 85.3%. K-means clustering is then used to generate labels for the recommender. Finally, Light Gradient Boosted Machine Classifier with a 5-fold cross validation accuracy of 99.17% forms the recommender model. The overall model system can provide song recommendations based on the detected emotion accurately.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96634-8_29,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06424-1_28,Neural Network Visualization in Virtual Reality: A Use Case Analysis and Implementation,Human Interface and the Management of Information: Visual and Information Design,10.1007/978-3-031-06424-1_28,Springer,2022-01-01,"Software systems and components increasingly rely on machine learning methods such as Artificial Neural Networks (ANNs) to provide intelligent functionality. Therefore, software developers and machine learning users should have a basic understanding of such methods. However, since ANNs are complex and thus challenging to grasp, novel visualization approaches can contribute to a better comprehension. We conducted an online survey to identify use cases and requirements for visualizing ANNs. Based on our results we designed and implemented an ANN visualization in virtual reality (VR) specifically targeted at machine learning users. Our approach is particularly suitable for teaching purposes and machine learning novices or non-experts who want to get an impression of the general functionality of neural networks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06424-1_28,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-97020-8_18,A Feature Fusion Based Deep Learning Model for Deepfake Video Detection,Mathematics and its Applications in New Computer Systems,10.1007/978-3-030-97020-8_18,Springer,2022-01-01,"In the recent past, several tools developed for facial image manipulation utilizing techniques like FaceSwap and Deepfake with implausible success. Such tools enable to edit faces present in a video with a minimum effort and the outcomes are incredibly close to real faces. Despite their handful number of uses, they possess many unpleasant impacts like creating fake news and defaming celebrities on social media. Hence, pinpointing whether a video contains manipulated faces in it becomes a task of utmost importance. To this end, we have designed a face manipulation detection technique that can figure out whether a target video contains manipulated faces in it or not. To do this, we first crop out the target face from video frames. Next, we extract deep features using MesoInception from RGB as well as their YCbCr and HSV versions, which are then concatenated to form a single feature vector. On the top of this new feature vector, we add the classification layer to classify the images into fake and real. To evaluate our model performance, we use two publicly available video datasets, namely Celeb-DF (V2) and Faceforensis++. The experimental results lay out the effectiveness of the proposed model over some recently published models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-97020-8_18,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-08878-0_15,Red-Light Running Violation Detection of Vehicles in Video Using Deep Learning Methods,Industrial Networks and Intelligent Systems,10.1007/978-3-031-08878-0_15,Springer,2022-01-01,"Recently, Traffic Monitoring Systems (TMS) based on camera are widespread used in many large cities thanks to advances in artificial intelligence especially in deep learning and computer vision. Detection of traffic violation of vehicles is a critical problem for law enforcement in such TMS due to complicated trajectories of different vehicle types in road. Existing methods based on computer vision techniques for detecting, tracking vehicles and then applying violation rules on the perceived path of every vehicles. In this paper, we present a novel approach which is based on the flexible LSTM recurrent neural networks in addition to the traditional fixed rules to detect red-light running violation of vehicles. We also present our improvements on the existing DeepSort tracking algorithm for faster and more accurate ID matching. We evaluate our deep LSTM with attention mechanism on a dataset (Dataset and code are available here: https://github.com/namnv78/RunningRedlight ) of 108 traffic videos captured from three road intersections in Vietnam including 628 red-light running violated vehicles. Our method achieved a precision, recall and F1-score of more than 99% which is 3% higher than the traditional rule-based method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-08878-0_15,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-2109-3_12,Video-Based Facial Expression Recognition: A Deep Learning Approach,Smart and Intelligent Systems,10.1007/978-981-16-2109-3_12,Springer,2022-01-01,"Identifying human emotions through facial expressions exhibited in video sequences is a challenging and interesting problem. This paper presents a hybrid deep learning model for recognizing facial expressions in video sequences. The proposed strategy involves a spatial CNN and a temporal CNN, which are the two deep convolutional neural networks (CNNs). The spatial CNN processes static facial frames and learns spatial features on video sequences. The temporal CNN processes optical flow images and learns temporal features. The spatiotemporal characteristics are obtained by fine-tuning the two CNNs on target video-based datasets from a pre-trained model. These features are fused into a deep fusion network then. Discriminative spatiotemporal characteristics are learned with the deep fusion network and average pooling is carried out on the learned attributes to make a video feature representation having a predetermined length. Eventually, face expression classification is achieved by using a linear SVM.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-2109-3_12,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-5652-1_51,Video Based Human Gait Activity Recognition Using Fusion of Deep Learning Architectures,"Proceedings of International Conference on Deep Learning, Computing and Intelligence",10.1007/978-981-16-5652-1_51,Springer,2022-01-01,"Human activity recognition (HAR) plays a vital role in the fields like security, health analysis, gaming, video streaming, surveillances, etc. Many applications were developed based on HAR using video due to its user-friendly nature and affordable cost. We propose the fusion of convolutional layer and Long-Short-Term Memory deep architecture for HAR. In this model, three convolution layer, two BiLSTM layer is used. Convolution layer extracts local features of the frame data. Extracted features were flattened and feed into BiLSTM layers to process the frame sequence and handles the time dependencies. The performance of the model is evaluated using two public datasets namely UCF101 dataset and HMDB-51 dataset and obtained the average accuracy of 96% and 95%, respectively. Influence of hyperparameter was analysed and tuned parameter is used for implementation. The proposed model provides the better accuracy when compared with other deep learning architectures used for HAR using video.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5652-1_51,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06416-6,An efficient cybersecurity framework for facial video forensics detection based on multimodal deep learning,Neural Computing and Applications,10.1007/s00521-021-06416-6,Springer,2022-01-01,"In cloud services and Internet-of-Things (IoT) applications, the cybersecurity in video transmission technologies has drawn much attention in recent researches due to the rapid growth of cyber-risks on both individuals and institutions. Unfortunately, the spoofing attack, a kind of cyber-risk, has increased the number of cyber-criminals in data transfer applications without being detected, especially in smart cities. Several applications based on online video communications, such as online testing and video conferences, are involved in smart cities. The video displays various variations of a person, which makes face recognition an important concept in security implementation. The face spoofing attacks are mainly based on the person's face replication by replaying a video or by printed photos. Therefore, video forgery detection and related spoof attack detection have become a new topic in cybersecurity research. From this perspective, this paper presents a deep learning approach for video face forensic detection with a cyber facial spoofing attack using two methodologies. The first methodology is based on a convolutional neural network (CNN) to extract features from the input video frames. The model has five convolutional layers followed by five pooling layers. The second methodology is based on convolutional long short-term memory (ConvLSTM). This model comprises two pooling layers, two convolutional layers, and a convolutional LSTM layer. Each methodology includes a fully-connected layer to interconnect between the feature map resulting from the feature extraction process and the classification layer. A SoftMax layer performs the classification task in each method. This paper aims to achieve an optimum modality for face forensic detection to overcome spoofing attacks. Simulation results reveal that the ConvLSTM with CNN methodology achieves better classification results as the extracted features are more comprehensive than those of other conventional approaches. Also, it achieves an accuracy of 99%, and the works presented in the literature achieve an accuracy levels up to 95%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06416-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-89554-9_2,Understanding New Age of Intelligent Video Surveillance and Deeper Analysis on Deep Learning Techniques for Object Tracking,IoT for Sustainable Smart Cities and Society,10.1007/978-3-030-89554-9_2,Springer,2022-01-01,"Surveillance is an imminent part of a smart city model. The persistent possibility of terrorist attacks at public and secured locations raises the need for powerful monitoring systems with subsystems for embedded object tracking. Object tracking is one of machine vision’s basic challenges and has been actively researched for decades. Object tracking is a process to locate a moving object over time across a series of video frames. Object tracking powered with the Internet of Things (IoT) technology provides a broad range of applications such as smart camera surveillance, traffic video surveillance, event prediction and identification, motion detection, human-computer interaction, and perception of human behavior. Real-time visual tracking requires high-response time sensors, tracker speed performance, and large storage requirements. Researchers have ascertained and acknowledged that there is a significant change in the efficacy of drone-based surveillance systems towards object tracking with the inception of the deep learning technologies. Several tracking approaches and models have been proposed by researchers in the area of object tracking and have experienced major improvements with advancement in methods, but object tracking is still considered to be a hard problem to solve. This chapter explains state-of-the-art object tracking algorithms and presents views on current and future trends in object tracking and deep learning surveillance. It also provides an analytical discussion on multi-object tracking experiments based on various datasets available for surveillance and the corresponding results obtained from the research conducted in the near past. FairMOT, GNNMatch, MPNTrack, Lif T, GSDT, and Tracktor++ are among the methods investigated. For the MOT16 and MOT17 datasets, FairMOT generated accuracy of 74.9 and 73.7, respectively, whereas GSDT provided accuracy of 60.7 and 67.1 for the 2DMOT15 and MOT20 datasets. FairMOT is an efficient tracker among the models tested, while MPNTrack is significantly more stable and retains tracklet IDs intact across frames in a series. This concludes FairMOT being an efficient tracker and MPNTrack a stable one. It also discusses a case study on the application of IoT in multi-object tracking and future prospects in surveillance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-89554-9_2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-7305-4_25,An Intelligent Approach for Detecting COVID-19 Probability,"Applications of Networks, Sensors and Autonomous Systems Analytics",10.1007/978-981-16-7305-4_25,Springer,2022-01-01,"COVID-19 is a communicable disease caused by the recently discovered coronavirus. This new-fangled virus and disease were unknown ahead of the outbreak that began in Wuhan, China, in December 2019. The first case in India was recognized on 30 January. Supplementary cases started coming to radiance in the country from March onwards, and seeing that on 28 April, the total number of detailed cases was over 29,000 and still escalating quickly. This fact now is a real problem statement. The aim of this paper is to create a COVID-19 probability detector model by utilizing machine learning techniques with Python. It is a tailor's dummy model, which detects that if a person has the probability of being affected by coronavirus or not. It takes input from the user through the microphone and speaks the corresponding answers. In this pandemic period, a huge number of people are anxious to imagine if he or she has been affected by COVID-19 or not. Moreover, for being afraid of rumors or of being affected by corona, most of them do not feel safe to go outside for checkups in Doctors’ chamber or Hospitals. The objective of this article is to try to create an intelligent system by which people can know patients’ probability of being affected by COVID-19 at the home. Not only for COVID-19 but also for any kind of disease, this model can help to detect the probability of being affected easily.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7305-4_25,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-981-16-8012-0_24,Natural Language Processing for Small Businesses and Future Trends in Healthcare,Cyber Security in Intelligent Computing and Communications,10.1007/978-981-16-8012-0_24,Springer,2022-01-01,"The Natural Language Processing (NLP) a computational used for reducing the distance between human and the machine. It means NLP facilitate human to communicate with the machine easily. NLP is a subfield of linguistics and artificial intelligence. NLP can be defined as the automatic manipulation of natural language like speech and text by using python models. The conversation between humans and machines has never been much easier and it is bound to be better in the upcoming decade. The paper distinguishes four phases by discussing a brief introduction to NLP and components of NLP followed by current applications of NLP, effect of NLP models on small businesses and future scope of it in the field of healthcare. This paper also deals with the future trends and applications of NLP in the sectors such as Sales and Marketing, Healthcare, Digital Humanities, IT and Security among others. Deep learning models and classifiers both will be needed for smooth functioning of future applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8012-0_24,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9154-6_49,A Video-Based Uncertainty Technique for Human Action Recognition—A Deep Learning Approach,"Proceedings of the 3rd International Conference on Communication, Devices and Computing",10.1007/978-981-16-9154-6_49,Springer,2022-01-01,"The Human action recognition (HAR) has diverse application in many research areas and also a great challenge in the ground of computer perception and deep learning. Deep learning is a method which uses complex models to solve tasks difficult to implement by hand, like in computer vision. The purpose of this research work is to design deep learning-based uncertainty techniques that recognize various action recognitions (running, walking) of different human in videos with better accuracy. Bayesian method used here to maintain a general structure to evaluate unpredictability of human actions. A well-calibrated prediction in regression tasks is our ultimate desire (provide a credible interval of 95%). A simple procedure has been suggested for that, when applied to Bayesian models. This model produces accuracy of 92% for running and 96% for walking.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9154-6_49,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-85383-9_8,A Comprehensive Review of Recent Automatic Speech Summarization and Keyword Identification Techniques,Artificial Intelligence in Industrial Applications,10.1007/978-3-030-85383-9_8,Springer,2022-01-01,"Speech has been the most popular form of human communication. A keyboard or a mouse, on the other hand, is the most common way of entering data into a computer. It would be wonderful if computers could understand and carry out human commands. The method of obtaining the transcription (word sequence) of an utterance from the speech waveform is known as automatic speech recognition (ASR). Over the last few decades, speech technology and systems in human-computer interaction have progressed progressively and significantly. This chapter suggests a comprehensive review of automatic speech recognition systems (ASR) and their most recent developments. This research aims to outline and explain some of the popular approaches in speech recognition systems at various stages and highlight selected systems’ unique and innovative characteristics.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85383-9_8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-3440-7_12,Deep Learning on Special Processed Video Colonoscopy Datasets,Innovation in Medicine and Healthcare,10.1007/978-981-19-3440-7_12,Springer,2022-01-01,"Automated analysis of medical image databases enables accurate diagnosis, assists physicians and gives objective clues, relevant in disease treatment and prevention. Our research focused on a well selected image database, with annotated video colonoscopy frames, for training deep learning systems. Aside the pathologic aspects like polyps, diverticula or adenomas, it is also important to have an objective tool of detecting unwanted residues left on the membrane of the colon, to evaluate the colon cleansing degree. On our innovative automatically annotated image database, a deep learning semantic segmentation is applied, to classify three types of membrane areas. By training a deep learning network we obtained significant results, being able to segment the images and to highlight the residue-covered areas in colonoscopies. As an important aspect in establishing the validity of the medical exam, a mark, corresponding to the degree of patient preparation for colonoscopy, will be computed based on the cumulated covered areas detected by our method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-3440-7_12,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-2123-9_50,Video Summarization for Multiple Sports Using Deep Learning,Proceedings of the International e-Conference on Intelligent Systems and Signal Processing,10.1007/978-981-16-2123-9_50,Springer,2022-01-01,"This paper proposes a computationally inexpensive method for automatic key event extraction and subsequent summarization of sports videos using scoreboard detection. A database consisting of 1300 images was used to train a supervised learning-based object detection algorithm, You Only Look Once (YOLO). Then, for each frame of the video, once the scoreboard was detected using YOLO, the scoreboard was cropped out of the image. After this, image processing techniques were applied on the cropped scoreboard to reduce noise and false positives. Following that, the processed image was passed through an optical character recognizer (OCR) to get the score. A rule-based algorithm was run on the output of the OCR to generate the timestamps of key events based on the game. Finally, clips of length 30 s around every detected event (15 s before and after the event) were extracted from the video file and stitched together to form the highlights video. The proposed method is best suited for people who want to analyze the games and want precise timestamps of the occurrence of important events. The performance of the proposed design was tested on videos of Bundesliga, English Premier League, ICC WC 2019, IPL 2019, and Pro Kabaddi League. An average F1 score of 0.979 was achieved during the simulations. The model was also tested on different video formats, and these were .mp4, .avi, .mov, and .mkv. The algorithm was trained on five different classes of three separate games (soccer, cricket, and kabaddi). The design was implemented using Python 3.7.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-2123-9_50,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-07005-1_19,A Deep-Learning Based Automated COVID-19 Physical Distance Measurement System Using Surveillance Video,Recent Trends in Image Processing and Pattern Recognition,10.1007/978-3-031-07005-1_19,Springer,2022-01-01,"The contagious Corona Virus (COVID-19) transmission can be reduced by following and maintaining physical distancing (also known as COVID-19 social distance). The World Health Organisation (WHO) recommends it to prevent COVID-19 from spreading in public areas. On the other hand, people may not be maintaining the required 2-m physical distance as a mandated safety precaution in shopping malls and public places. The spread of the fatal disease may be slowed by an active monitoring system suitable for identifying distances between people and alerting them. This paper introduced a deep learning-based system for automatically detecting physical distance using video from security cameras. The proposed system employed the fine-tuning YOLO v4 for object detection and classification and Deepsort for tracking the detected people using bounding boxes from the video. Pairwise L2 vectorized normalization was utilized to generate a three-dimensional feature space for tracking physical distances and the violation index, determining the number of individuals who follow the distance rules. For training and testing, we use the MS COCO and Oxford Town Centre (OTC) datasets. We compared the proposed system to two well-known object detection models, YOLO v3 and Faster RCNN. Our method obtained a weighted mAP score of 87.8% and an FPS score of 28; both are computationally comparable.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-07005-1_19,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-86749-2_15,Technological Developments in Internet of Things Using Deep Learning,"Transforming Management with AI, Big-Data, and IoT",10.1007/978-3-030-86749-2_15,Springer,2022-01-01,"Internet of Things (IoT) has revolutionized different technological fields and applications. Many sensory devices are connected in the network, and an enormous amount of real-time data is associated with that in the form of audio, video and measured numerical data. Deep learning is an advanced field of machine learning that tries to make the computer understand from the input data, and data passes through multiple layers to make necessary prediction and classification. Higher-level features are extracted progressively from the raw input data from multilayer architectures. Integration of deep learning with IoT can efficiently manage these multidimensional large sensory data to extract information from the data, make new predictions, and have a more efficient control mechanism. Different aspects of IoT and deep learning are discussed in this chapter, with various applications in different fields in a comprehensive manner. Multiple applications and challenges are also discussed with current solutions and future directions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-86749-2_15,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-94551-0_37,Speech Signal Feature Extraction Method of Tibetan Speech Synthesis System Based on Machine Learning,Advanced Hybrid Information Processing,10.1007/978-3-030-94551-0_37,Springer,2022-01-01,"In order to improve the accuracy of Tibetan speech synthesis, a feature extraction method of Tibetan speech synthesis system based on machine learning is proposed. Based on the analysis of Tibetan speech text content, the construction of speech synthesis system is realized. By judging the level of Tibetan prosody, a synthetic encoder is designed to realize the feature extraction of Tibetan speech signal. According to the experimental results, under the condition of normal speaking speed and identical Tibetan speech content, the Tibetan speech synthesized by the speech signal feature extraction method of Tibetan speech synthesis system based on machine learning is more accurate.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-94551-0_37,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-81007-8_21,Research on Automatic Garbage Sorting System Based on Artificial Intelligence,Advances in Intelligent Automation and Soft Computing,10.1007/978-3-030-81007-8_21,Springer,2022-01-01,"In response to the national waste classification policy, environmental protection and resource utilization, an automatic waste sorting system is designed, which can be divided into four aspects: mechanical structure, detection system, mobile terminal control system and multifunctional practical system. The system is based on image recognition technology, using artificial intelligence equipment, combining hardware and software to realize the automatic classification of garbage. The product is applied in the community or street. After users put the garbage into the sorting platform, the system will automatically identify the type of garbage and transport it to the corresponding garbage can. Through the application of the above system, on the basis of realizing automatic sorting, the problem of environmental pollution and waste of resources caused by unable to determine the type of garbage thrown in the wrong place can be solved. On the other hand, it can promote the residents’ awareness of waste classification, improve the recycling rate of waste, avoid the occurrence of secondary pollution, and provide an efficient information management means for the government to solve the problem of waste treatment and build a green circular economy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-81007-8_21,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-3239-6_65,Image Recognition Using Unsupervised Learning Based Automatic Fuzzy Clustering Algorithm,Modern Mechanics and Applications,10.1007/978-981-16-3239-6_65,Springer,2022-01-01,"This article proposes a novel techniques for unsupervised learning in image recognition using automatic fuzzy clustering algorithm (AFCA) for discrete data. There are two main stages in order to recognize images in this study. First of all, new technique is shown to extract sixty four textural features from n images represented by a matrix n × 64 . Afterwards, we use the proposed method based on Hausdorff distance to simultaneously determine the appropriate number of clusters. At the end of the unsupervised clustering process, discrete data belonging to the same cluster converge to the same position, which represents the cluster’s center. After determining number of cluster, we have probability of assigning objects to the established clusters. The simulation result built by Matlab program shows the effectiveness of the proposed method using the corrected rand, the partition entropy, and the partition coefficients index. The experimental outcomes illustrate that the proposed method is better than the existing ones as Fuzzy C-mean. As a result, we believe that the proposed method is filled with a potential possibility which can be applied in practical realization.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-3239-6_65,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-94102-4_9,Modeling of Optimal Bidirectional LSTM Based Human Motion Recognition for Virtual Reality Environment,Virtual and Augmented Reality for Automobile Industry: Innovation Vision and Applications,10.1007/978-3-030-94102-4_9,Springer,2022-01-01,"Recently, virtual reality (VR) has received significant attention among researchers to accomplish realistic interaction between humans and computers. Among the different application areas of VR, multimedia human computer interaction requires accurately collect and identify the motions of human body in real time. For enabling highly realistic and efficient communication among humans and computers, design of human motion recognition system becomes necessary to determine diverse complex and distinct human actions. Therefore, this study designs a new optimal bidirectional long short term memory (BiLSTM) with fully convolution network (FCN), called OBiLSTM-FCN model for human motion recognition in VR environment. The proposed OBiLSTM-FCN model comprises different processes namely feature extraction, classification, and hyperparameter optimization. Primarily, kernel based linear discriminant analysis (LDA) approach is employed as a feature extraction technique. In addition, the BiLSTM-FCN technique is applied as a recognition model to determine human motions. Finally, the Adam optimizer is applied to optimally tune the hyperparameters involved in the BiLSTM-FCN model. The performance validation of the OBiLSTM-FCN model take place and the resultant values portrayed the improved performance over the other compared methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-94102-4_9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-05887-5_16,Learning Support and Evaluation of Weight-Shifting Skills for Novice Skiers Using Virtual Reality,Adaptive Instructional Systems,10.1007/978-3-031-05887-5_16,Springer,2022-01-01,"In this study, we propose a virtual reality learning support system designed to help train novice skiers. In previous research, we extracted the differences between the weight shifting movements and skiing postures of experts and beginners using deep learning. The obtained results showed weight shifting to be a more important feature than posture. Accordingly, we focused on supporting the weight-shifting technique. The support system provides real-time feedback to a user on their current weight-shifting status. We conducted an experiment to verify the effectiveness of the proposed approach, in which we defined evaluation criteria for a user’s level of skiing proficiency. The experimental results demonstrate that the system successfully facilitated participants’ acquisition of the weight-shifting skill.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-05887-5_16,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-5857-0_130,Image Recognition of Agricultural Plant Diseases and Insect Pests Based on Convolution Neural Network,International Conference on Cognitive based Information Processing and Applications (CIPA 2021),10.1007/978-981-16-5857-0_130,Springer,2022-01-01,"In order to identify pests quickly and accurately in agricultural environment, a deep learning model based on convolutional neural network is proposed to locate and identify pests. In this paper, by accelerating the color attenuation of the whole image, combining with the spatial influence between the hyperpixel regions, the saliency value of each super region is calculated, and then the potential area of the pest target is provided. Finally, the pest target location and segmentation are combined with grabcut algorithm. For the segmented pest target, the optimized convolution neural network is used to express and classify, and the structure of convolution neural network is further reduced.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5857-0_130,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-91581-0_27,Optimizing Convolutional Neural Network Architecture for Microscopy Image Recognition for Tuberculosis Diagnosis,"Advances in Neural Computation, Machine Learning, and Cognitive Research V",10.1007/978-3-030-91581-0_27,Springer,2022-01-01,"Globally, tuberculosis (TB) is the leading infectious killer in the world before pandemia. This paper presents the result of optimizing convolutional neural network architecture for the detection of acid-fast stained TB bacillus. The experimental set contains the segmentation results of microscopy images of the patients sputum stained by the Ziehl–Neelsen method. The authors constructed an experimental algorithm for optimizing the original convolutional neural network model, including optimizing the model dimension, data augmentation, adjusting the model parameters, and improving regularization. The authors built few models of convolutional neural networks (CNN) models to recognize TB bacillus, which showed the maximum value of metrics in the experiment.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-91581-0_27,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-93479-8_23,Research on Wheat Impurity Image Recognition Based on Convolutional Neural Network,"Broadband Communications, Networks, and Systems",10.1007/978-3-030-93479-8_23,Springer,2022-01-01,"The doping rate is one of the important indexes to evaluate the quality grade and price of wheat. In order to accurately and quickly recognize impurities (wheat husk) in wheat grains, images of doped wheat were collected and Convolutional Neural Network (CNN) was used to realize the classification and recognition of grains and impurities in wheat grains. In this study, image segmentation and image enhancement were used to preprocess the acquired images to establish the image database of wheat grains and impurities. According to the characteristics of image data, the classic CNN, VGGNet and ResNet network models for wheat impurity images recognition were established. Simulation analysis shows that, compared with the classical CNN and VGGNet network models, the ResNet network model has the best recognition performance. The recognition accuracy of the test set is 96.94%, the recognition time is 5.60 ms.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-93479-8_23,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00259-021-05483-0,"A 3D deep learning model to predict the diagnosis of dementia with Lewy bodies, Alzheimer’s disease, and mild cognitive impairment using brain 18F-FDG PET",European Journal of Nuclear Medicine and Molecular Imaging,10.1007/s00259-021-05483-0,Springer,2022-01-01,"Purpose The purpose of this study is to develop and validate a 3D deep learning model that predicts the final clinical diagnosis of Alzheimer’s disease (AD), dementia with Lewy bodies (DLB), mild cognitive impairment due to Alzheimer’s disease (MCI-AD), and cognitively normal (CN) using fluorine 18 fluorodeoxyglucose PET (18F-FDG PET) and compare model’s performance to that of multiple expert nuclear medicine physicians’ readers. Materials and methods Retrospective 18F-FDG PET scans for AD, MCI-AD, and CN were collected from Alzheimer’s disease neuroimaging initiative (556 patients from 2005 to 2020), and CN and DLB cases were from European DLB Consortium (201 patients from 2005 to 2018). The introduced 3D convolutional neural network was trained using 90% of the data and externally tested using 10% as well as comparison to human readers on the same independent test set. The model’s performance was analyzed with sensitivity, specificity, precision, F1 score, receiver operating characteristic (ROC). The regional metabolic changes driving classification were visualized using uniform manifold approximation and projection (UMAP) and network attention. Results The proposed model achieved area under the ROC curve of 96.2% (95% confidence interval: 90.6–100) on predicting the final diagnosis of DLB in the independent test set, 96.4% (92.7–100) in AD, 71.4% (51.6–91.2) in MCI-AD, and 94.7% (90–99.5) in CN, which in ROC space outperformed human readers performance. The network attention depicted the posterior cingulate cortex is important for each neurodegenerative disease, and the UMAP visualization of the extracted features by the proposed model demonstrates the reality of development of the given disorders. Conclusion Using only 18F-FDG PET of the brain, a 3D deep learning model could predict the final diagnosis of the most common neurodegenerative disorders which achieved a competitive performance compared to the human readers as well as their consensus.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00259-021-05483-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8774-7_42,Designing Deep Learning Architectures for Multiview 3D Shape Estimation Using Image Transformers,Emergent Converging Technologies and Biomedical Systems,10.1007/978-981-16-8774-7_42,Springer,2022-01-01,"The task of 3D shape generation for realistic data is an important challenge that needs to be addressed in the domain of computer vision, robotics, and graphics which serve as a building block for many real-time applications like autonomous driving or 3D modeling, etc. Estimating shapes from a few 2D images are fundamentally ill-posed as numerous 3D shapes can be explained by a few images. In the absence of complete information, recently, deep learning has been used to fill in the gap by leveraging data driven category level priors. In this work, we propose a novel 3D shape estimation network that uses an image transformer to better encode the shape features into a latent representation which is later decoded using a multilayer perceptron. Our experiments show that image transformers are better than convolution-based encoders due to their wide attention capability. We perform both qualitative and quantitative experiments to demonstrate the effect of new architecture on shape quality and detail.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8774-7_42,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-98253-9_7,PET Normalizations to Improve Deep Learning Auto-Segmentation of Head and Neck Tumors in 3D PET/CT,Head and Neck Tumor Segmentation and Outcome Prediction,10.1007/978-3-030-98253-9_7,Springer,2022-01-01,"Auto-segmentation of head and neck cancer (HNC) primary gross tumor volume (GTVt) is a necessary but challenging process for radiotherapy treatment planning and radiomics studies. The HEad and neCK TumOR Segmentation Challenge (HECKTOR) 2021 comprises two major tasks: auto-segmentation of GTVt in FDG-PET/CT images and the prediction of patient outcomes. In this paper, we focus on the segmentation part by proposing two PET normalization methods to mitigate impacts from intensity variances between PET scans for deep learning-based GTVt auto-segmentation. We also compared the performance of three popular hybrid loss functions. An ensemble of our proposed models achieved an average Dice Similarity Coefficient (DSC) of 0.779 and median 95% Hausdorff Distance (HD95) of 3.15 mm on the test set. Team: Aarhus_Oslo.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98253-9_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0619-0_12,2D and 3D Human Pose Estimation and Analysis Using Deep Learning,Advances in Information Communication Technology and Computing,10.1007/978-981-19-0619-0_12,Springer,2022-01-01,"3D human pose estimate has recently been made, a novel pipeline and architecture for 3D human posture estimation in in-the-wild pictures is a challenge. The major goal of this paper is to reduce keypoint re-projection loss, allowing the proposed model to be trained using in-the-wild pictures with only 2D ground truth annotations. In proposed model, several networks are employed at various phases of the process. COCO, a publicly available dataset, was utilized to train and validate results. Proposed pipeline model consists of yolo v3 to recognize and segment a person from an image, scale the picture without distorting the aspect ratio and inferring 17 essential points of a human body using PoseNet. Further, to measure the performance difficulties are highlighted with a subset of pictures where 2D keypoint recognition isn’t doing well, and proposed model is showing better performance that HMR and PoseNet.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0619-0_12,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-80458-9_6,Toward a Deep Learning Approach for Automatic Semantic Segmentation of 3D Lidar Point Clouds in Urban Areas,Geospatial Intelligence,10.1007/978-3-030-80458-9_6,Springer,2022-01-01,"Semantic segmentation of Lidar data using Deep Learning (DL) is a fundamental step for a deep and rigorous understanding of large-scale urban areas. Indeed, the increasing development of Lidar technology in terms of accuracy and spatial resolution offers a best opportunity for delivering a reliable semantic segmentation in large-scale urban environments. Significant progress has been reported in this direction. However, the literature lacks a deep comparison of the existing methods and algorithms in terms of strengths and weakness. The aim of the present paper is therefore to propose an objective review about these methods by highlighting their strengths and limitations. We then propose a new approach based on the combination of Lidar data and other sources in conjunction with a Deep Learning technique whose objective is to automatically extract semantic information from airborne Lidar point clouds by enhancing both accuracy and semantic precision compared to the existing methods. We finally present the first results of our approach.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-80458-9_6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-981-19-2374-6_8,Explainable AI for ICT: System and Software Architecture,Recent Advancements in ICT Infrastructure and Applications,10.1007/978-981-19-2374-6_8,Springer,2022-01-01,"Artificial Intelligence (AI) has become a revolution in the ICT domain due to the swift progress of analytical techniques and the availability of structured/unstructured data. With the indispensable role of AI in different applications, there are growing concerns over the lack of transparency and explainability. In addition, potential bias may affect the predictions of a model. This is where Explainable Artificial Intelligence (XAI) comes into the picture. XAI increases the trust placed in an AI system by researchers, medical practitioners, and others. Thus, it leads to widespread deployment of AI in healthcare, agriculture, online mart, and many more. The aim is to enlighten practitioners on the understandability and interpretability of EAI systems using a variety of techniques available which can be very advantageous in the ICT domain. In this chapter, we present two different techniques leveraging EAI where a user has to make the right choices based on his requirements. The software architecture of the first techniques is based on a medical diagnosis model where we need to be confident enough to treat a patient as instructed by a black-box model. Another approach presents an online Mart where a reliable pricing method can be developed by ML models that can read through historical sales data. The objective here is to match buyers and sellers, to weigh animals, and to oversee their sale. However, when AI models suggest or recommend a decision, that in itself does not reveal too much (i.e., it acts as a black box). Hence, a model capable of explaining the different factors that impact the price point is essential for the needs of a user.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-2374-6_8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-84760-9_61,A Novel Method of Transforming Real Images & Videos into Cartoon Images & Videos,Second International Conference on Image Processing and Capsule Networks,10.1007/978-3-030-84760-9_61,Springer,2022-01-01,"The primary objective of this research work is to develop a novel solution for transforming real images and videos into animated photos and videos. The existing transformation method requires complicated computer graphics & skills. The main approach of this article is based on designated images & videos, which is then converted into an art form such as painting. Among all the technique used, the proposed model accepts the input image and video, which is then followed by the extraction of edges, grey image and application of median blur with Bilateral Filter technique to convert the input image into cartoon image.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-84760-9_61,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-02447-4_20,Computational Analysis of Human Navigation in a VR Spatial Memory Locomotor Assessment Using Density-Based Clustering Algorithm of Applications with Noise DBSCAN,Digital Technologies and Applications,10.1007/978-3-031-02447-4_20,Springer,2022-01-01,"In this study, we explore human navigation as evaluated by the VR Magic Carpet TM (VMC) [ 1 ], a variation of the Corsi Block Tapping task (CBT) [ 2 , 3 ], employing Density-based spatial clustering of applications with noise (DBSCAN) [ 4 ]. As a result of the VMC, we acquired raw spatial data in 3D space, which we processed, analyzed, and turned into trajectories before evaluating them from a kinematic standpoint. Our previous research [ 5 ] revealed three unique groupings. However, the categorization remained ambiguous, showing clusters with diverse people (patients and healthy). We utilized DBSCAN to compare patients’ navigation behavior to healthy individuals, highlighting the most notable differences and assessing our existing classifiers. Our research aims to produce insights that may help clinicians and neuroscientists adopt machine learning, especially clustering algorithms, to identify cognitive impairments and analyze human navigation behavior.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-02447-4_20,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-92084-5_2,The Technological Revolution: The Rise of Machines,Making the Global Economy Work for Everyone,10.1007/978-3-030-92084-5_2,Springer,2022-01-01,"We are living in the most innovative period in all of human history. On the one hand, the technological and scientific revolution opens up unprecedented opportunities: accelerating human progress, improving the quality of life and overcoming many constraints. From an economic point of view, technology and science can increase productivity and expand the economy generating wealth, relieve humans from heavy and dangerous activities, and increase labour flexibility. On the other hand, some technologies can be used for criminal activities, terrorism, sabotage, restriction of personal freedoms, repression and other ethically controversial objectives. The risks also affect the sustainability of economic growth, the fabric of society and the future of labour, humanity as a whole and its role. Simply witnessing technological development as a mere spectator is not possible. New technologies influence the way we live, think, produce and work. Beyond the technical aspects, they introduce new mental, cultural and strategic models. The upcoming wave of innovation affects everyone. To ride this wave instead of being overwhelmed by it, one must grasp the main innovations and understand how they work and are connected to one another.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92084-5_2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-00805-4_4,The Role and Meaning of the Digital Twin Technology in the Process of Implementing Intelligent Collaborative Robots,Advances in Manufacturing III,10.1007/978-3-031-00805-4_4,Springer,2022-01-01,"In the last decade, the paradigms concerning designing and enhancing manufacturing systems were changed due to an uncommonly fast development of computerization, automation and robotization. Unknown information technologies, such as virtual reality, artificial intelligence-based solutions and robots enter the industrial reality to an increasingly great extent. Development trend analysis allows one to assume that in the nearest future collaborative robots, capable of intelligent cooperation with people will constitute an element of manufacturing systems. The objective of this article is to present conditions that ought to be met in order to introduce intelligent collaborative robots as well as to employ Digital Twin technology in the process of cobot adaptation in terms of a certain manufacturing system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-00805-4_4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-93052-3_9,Machine Learning for Identifying Abusive Content in Text Data,Advances in Selected Artificial Intelligence Areas,10.1007/978-3-030-93052-3_9,Springer,2022-01-01,"The proliferation of social media has created new norms in society. Incidents of abuse, hate, harassment and misogyny are widely spread across social media platforms. With the advancements in machine learning techniques, advanced text mining methods have been developed to analyse text data. Social media data poses additional challenges to these methods due to their nature of short content and the presence of ambiguity, errors and noises in content. In the past decade, machine learning researchers have focused on finding solutions dealing with these challenges. Outcomes of these methods boost the social media monitoring capability and can assist policymakers and governments to focus on key issues. This chapter will review various types of machine learning techniques including the currently popular deep learning methods that can be used in the analysis of social media data for identifying abusive content.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-93052-3_9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9089-1_13,Multi-model Emotion Recognition Using Hybrid Framework of Deep and Machine Learning,"Security, Privacy and Data Analytics",10.1007/978-981-16-9089-1_13,Springer,2022-01-01,"Automated emotion research has grown significantly over the last two decades, with direct application across domains, such as medical science and chat-bot, all requiring an understanding of emotional psychology. Emotion is identified by various sources of emotion such as text, images, gesture, and video. The fusion process of various sources of information is called multi-model emotion recognition . In the proposed framework, a multi-model deep neural network is trained and further the input data is passed with the learned (pre-trained) model to obtain the feature vector. Further, the machine learning classifiers (such as SVM, Decision tree, Random forest, and XGBoost) are used to develop an emotion recognition model using the deep learning-based features. The best weighted-accuracy (WA) and unweighted-accuracy (UWA) are 71 and 66%, respectively, using Random forest classifier with the combination speech, text, and mocap.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9089-1_13,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-04206-5_11,Assistive e-Learning Software Modules to Aid Education Process of Students with Visual and Hearing Impairment: A Case Study in North Macedonia,ICT Innovations 2021. Digital Transformation,10.1007/978-3-031-04206-5_11,Springer,2022-01-01,"This paper presents a technology4good initiative that integrates multiple breakthrough software modules with the aim to build a generic framework to aid the educational process for students with disabilities, such as, hearing and vision impairments, as well as various types of dyslexia. The purpose of the study is to apply various distinct researches among which the highlight is on the text-to-speech engine for the first time developed to support Macedonian language. Additionally, the framework integrates Macedonian sign language to guide the hearing impaired students through the contents of the educational framework. Also, there is a specially developed font (typeface) and color environment for students with specific reading difficulties (dyslexia). The methods that support the educational framework are developed by mix of social, special education and computer science experts. The methodology for developing the text-to-speech engine relies on the newest and most efficient principles in Machine Learning for Natural Language Processing - a Deep Learning approach. The framework has been tested on target group of students and the satisfaction has been measured by using the standard Likert scale.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-04206-5_11,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-08333-4_18,Speech Emotion Recognition from Earnings Conference Calls in Predicting Corporate Financial Distress,Artificial Intelligence Applications and Innovations,10.1007/978-3-031-08333-4_18,Springer,2022-01-01,"Sentiment and emotion analysis is attracting considerable interest from researchers in the field of finance due to its capacity to provide additional insight into opinions and intentions of investors and managers. A remarkable improvement in predicting corporate financial performance has been achieved by considering textual sentiments. However, little is known about whether managerial affective states influence changes in overall corporate financial performance. To overcome this problem, we propose a deep learning architecture that uses vocal cues extracted from earnings conference calls to detect managerial emotional states and exploits these states to identify firms that could be financially distressed. Our findings provide evidence on the role of managerial emotional states in the early detection of corporate financial distress. We also show that the proposed deep learning-based prediction model outperforms state-of-the-art financial distress prediction models based solely on financial indicators.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-08333-4_18,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-98876-0_13,BERT Model-Based Approach for Detecting Racism and Xenophobia on Twitter Data,Metadata and Semantic Research,10.1007/978-3-030-98876-0_13,Springer,2022-01-01,"The large amount of data generated on social networks makes the task of moderating textual content written by users complex and impossible to do manually. One of the most prominent problems on social networks is racism and xenophobia. Although there are studies of predictive models that make use of natural language processing techniques to detect racist or xenophobic texts, a lack of these has been observed in the Spanish language. In this paper we present a solution based on deep learning models and, more specifically, models based on transfer learning to detect racist and xenophobic messages in Spanish. For this purpose, a dataset obtained from the social network Twitter has been created using data mining techniques and, after a preprocessing, it has been labelled into racist messages and non-racist messages. The trained models are based on BERT and were called BETO and mBERT. Promising results were obtained showing 85.14% accuracy in the best performing model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98876-0_13,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-96305-7_26,Speech Enhancement Using Generative Adversarial Network (GAN),Hybrid Intelligent Systems,10.1007/978-3-030-96305-7_26,Springer,2022-01-01,"Most of the restoration techniques for loss of voice result in whispered and monotonous speech. In addition to intelligibility, this type of speech is poor in expressiveness and naturalness due to a) the lack of pitch resulting in whispered speech, and b) artificial pitch production resulting in monotone speech. This research work offers a neural network method for estimating a fully voiced speech waveform from alaryngeal whispering speech waveform. In this research paper a speech enhancement method using Generative Adversarial Networks (GANs) is implemented. The aim of this GAN implementation to perform whispered-to-voiced speech conversion and to handle speech reconstruction tasks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96305-7_26,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00034-021-01786-7,Speech Enhancement Based on Binaural Sound Source Localization and Cosh Measure Wiener Filtering,"Circuits, Systems, and Signal Processing",10.1007/s00034-021-01786-7,Springer,2022-01-01,"The existing speech enhancement algorithm has shown poor performance under low Signal Noise Ratios (SNRs). To resolve this problem, a speech enhancement algorithm based on binaural sound source localization and cosh measure filtering is proposed. Firstly, the algorithm uses a sound source localization algorithm based on head correlation functions and two-level deep learning to extract the spatial information of the binaural sound source and determine the spatial position of the sound source. The beamforming method is then used to remove the noises in different directions from the speech. Finally, the Wiener filtering of cosh measure based on logarithmic relation is used to remove the noise in the same direction as the speech to achieve speech enhancement. Experiments show that the proposed algorithm has better robustness and denoising ability than the contrast algorithms.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00034-021-01786-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-92563-5_56,Audio Signal Processing for Quantitative Moulding Material Regeneration,REWAS 2022: Developing Tomorrow’s Technical Cycles (Volume I),10.1007/978-3-030-92563-5_56,Springer,2022-01-01,"As a natural product with limited resources, sand is of existential importance for various industries. Foundry technology in particular requires considerable quantities worldwide for sand moulds and cores. Depending on the process Process technology , the sand is recycled and thus reused. However, especially in the case of inorganically bound moulding sand, it is still frequently not recycled. Existing regeneration methods include mechanical, pneumatic, or combined processes. These have been developed, but have reached their analytical optimisation limits, as the processes are not transparent and make in situ analyses of moulding materials Moulding material impossible. Within the scope of this research work, a methodology for the computer-aided processing of sound and image data with the help of convolutional neural networks Convolutional Neural Network (CNN) (CNNs) was developed, which is to evaluate the non-measurable changes of the moulding material Moulding material in the running process in real time via process acoustics. The aim is to optimise process control in terms of time, cost, and energy efficiency.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92563-5_56,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-98358-1_10,Rating-Aware Self-Organizing Maps,MultiMedia Modeling,10.1007/978-3-030-98358-1_10,Springer,2022-01-01,"Self-organizing maps (SOM) are one of the prominent paradigms for 2D data visualization. While aiming at preserving topological relations of high-dimensional data, they provide sufficiently organized view of objects and thus improve capability of users to explore displayed information. SOMs were also extensively utilized in visualizing results of multimedia information retrieval systems. However, for this task, SOM lacks the ability to adapt to the relevance scores induced by the underlying retrieval algorithm. Therefore, although exploration capability is enhanced, the capability to exploit the (best) results is severely limited. In order to cope with this problem, we propose a rating-aware modification of SOM algorithm that jointly optimizes for the preservation of both topological as well as relevance-based ordering of results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98358-1_10,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-08341-9_16,Deep Learning-Based Segmentation of the Atherosclerotic Carotid Plaque in Ultrasonic Images,Artificial Intelligence Applications and Innovations. AIAI 2022 IFIP WG 12.5 International Workshops,10.1007/978-3-031-08341-9_16,Springer,2022-01-01,"Early stroke risk stratification in individuals with carotid atherosclerosis is of great importance, especially in high-risk asymptomatic (AS) cases. In this study, we present a new computer-aided diagnostic (CAD) system for the automated segmentation of the atherosclerotic plaque in carotid ultrasound (US) images and the extraction of a refined set of ultrasonic features to robustly characterize plaques in carotid US images and videos (AS vs symptomatic (SY)). So far, we trained a UNet model (16 to 256 neurons in the contracting path; the reverse, for the expanding path), starting from a dataset of 201 (AS = 109 and SY = 92) carotid US videos of atherosclerotic plaques, from which their first frames were extracted to prepare three subsets, a training, an internal validation, and final evaluation set, with 150, 30 and 15 images, respectively. The automated segmentations were evaluated based on manual segmentations, performed by a vascular surgeon. To assess our model’s capacity to segment plaques in previously unseen images, we calculated 4 evaluation metrics (mean ± std). The evaluation of the proposed model yielded a 0.736 ± 0.10 Dice similarity score (DSC), a 0.583 ± 0.12 intersection of union (IoU), a 0.728 ± 0.10 Cohen’s Kappa coefficient (KI) and a 0.65 ± 0.19 Hausdorff distance. The proposed segmentation workflow will be further optimized and evaluated, using a larger dataset and more neurons in each UNet layer, as in the original model architecture. Our results are close to others published in relevant studies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-08341-9_16,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-79801-7_18,Real Time Multi Object Detection & Tracking on Urban Cameras,Advances in Road Infrastructure and Mobility,10.1007/978-3-030-79801-7_18,Springer,2022-01-01,"Processing video from urban video surveillance cameras requires the use of algorithms for multi-object detection and tracking on video in real-time. However, existing computer vision algorithms require the use of powerful equipment and are not sufficiently optimized to process multiple video streams simultaneously. This article proposes an approach to using the tracker in conjunction with the YoloV4 object detector for real-time video processing on medium-power equipment. Paper also presents the solution for difficulties that arise during work with optical flow. The results of the comparison of the accuracy and speed of image processing of the applied approach with such trackers as IOU17, SORT, KCF, and MOSSEE are also presented.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-79801-7_18,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06433-3_30,Forecasting Future Instance Segmentation with Learned Optical Flow and Warping,Image Analysis and Processing – ICIAP 2022,10.1007/978-3-031-06433-3_30,Springer,2022-01-01,For an autonomous vehicle it is essential to observe the ongoing dynamics of a scene and consequently predict imminent future scenarios to ensure safety to itself and others. This can be done using different sensors and modalities. In this paper we investigate the usage of optical flow for predicting future semantic segmentations. To do so we propose a model that forecasts flow fields autoregressively. Such predictions are then used to guide the inference of a learned warping function that moves instance segmentations on to future frames. Results on the Cityscapes dataset demonstrate the effectiveness of optical-flow methods.,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06433-3_30,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-95388-1_33,Self-adapted Frame Selection Module: Refine the Input Strategy for Video Saliency Detection,Algorithms and Architectures for Parallel Processing,10.1007/978-3-030-95388-1_33,Springer,2022-01-01,"Video saliency detection is intended to interpret the human visual system by modeling and predicting while observing a dynamic scene. This method is currently widely used in a variety of devices, including surveillance cameras and Internet-of-Things sensors. Traditionally, each video contains a large amount of redundancies in consecutive frames, while the common practices concentrate on extending the range of input frames to resist the uncertainty of input images. In order to overcome this problem, we propose Self-Adapted Frame Selection (SAFS) module that removes redundant information and selects frames that are highly informative. Furthermore, the module has high robustness and extensive application dealing with complex video contents, such as fast moving scene and images from different scenes. Since predicting the saliency map across multiple scenes is challenging, we establish a set of benchmarking videos for the scene change scenario. Specifically, our method combined with TASED-NET achieves significant improvements on the DHF1K dataset as well as the scene change dataset.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95388-1_33,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-89880-9_13,Multi-frame Abnormality Detection in Video Capsule Endoscopy,"Proceedings of the Future Technologies Conference (FTC) 2021, Volume 2",10.1007/978-3-030-89880-9_13,Springer,2022-01-01,"Video capsule endoscopy (VCE) provides a minimally invasive and highly effective screening mechanism for gut illness. Unfortunately, current methods require physicians to manually review the lengthy VCE recordings, which is both tedious and cost-prohibitive. Furthermore, available automation techniques are limited, and there is little publicly available data for developing such tools. This paper presents a novel multi-frame methodology for capsule localization and small bowel abnormality detection in VCE recordings. Unlike existing models, the deep-learning architecture incorporates contextual cues in adjacent frames to identify abnormalities. This approach corresponds with how physicians review VCE to identify disease and results in more efficient and effective models. To support model development, we developed a customized data collection platform and provide a mechanism for efficient annotation. The platform enabled rapid generation of annotated, full-length, VCE recordings. These annotated videos are in contrast to other publicly available VCE data which does not contain annotated video sequences with which we could train and test a multi-frame model. This platform can be easily replicated and provides a means to generate additional data to accelerate progress in machine-assisted VCE screening. our demonstration of the effectiveness of sequence-based architectures can readily be extended to other medical classification tasks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-89880-9_13,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06433-3_25,Spatial-Temporal Autoencoder with Attention Network for Video Compression,Image Analysis and Processing – ICIAP 2022,10.1007/978-3-031-06433-3_25,Springer,2022-01-01,"Deep learning-based approaches are now state of the art in numerous tasks, including video compression, and are having a revolutionary influence in video processing. Recently, learned video compression methods exhibit a fast development trend with promising results. In this paper, taking advantage of the powerful non-linear representation ability of neural networks, we replace each standard component of video compression with a neural network. We propose a spatial-temporal video compression network (STVC) using the spatial-temporal priors with an attention module (STPA). On the one hand, joint spatial-temporal priors are used for generating latent representations and reconstructing compressed outputs because efficient temporal and spatial information representation plays a crucial role in video coding. On the other hand, we also added an efficient and effective Attention module such that the model pays more effort on restoring the artifact-rich areas. Moreover, we formalize the rate-distortion optimization into a single loss function, in which the network learns to leverage the Spatial-temporal redundancy presented in the frames and decreases the bit rate while maintaining visual quality in the decoded frames. The experiment results show that our approach delivers the state-of-the-art learning video compression performance in terms of MS-SSIM and PSNR.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06433-3_25,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-981-16-5411-4_7,Strategies and Tools for Effective Suspicious Event Detection from Video: A Survey Perspective (COVID-19),Contactless Healthcare Facilitation and Commodity Delivery Management During COVID 19 Pandemic,10.1007/978-981-16-5411-4_7,Springer,2022-01-01,"Safety and health security are the major concerns in today's modern world. Most of the countries have adopted camera surveillance systems to achieve a secure environment, one such example is restriction imposed for the movement of people during the COVID-19 pandemic. Thus, surveillance systems serve the purpose of humans to identify intruders with suspicious behavior. Detecting these intrusions or any suspicious events in an early stage from surveillance systems is an important and challenging task. This can be done using Suspicious Event Detection Models (SEDM) and tools. In earlier systems, it is found that machine learning methods are proved to be efficient in predicting the suspicion activities. In this chapter, a survey of various SEDM Strategies and Tools that were developed earlier which captures the suspicious events in campuses or societies is discussed that provides a Full-proof secure environment. Few, earlier SEDM have also used deep learning approaches, IoT, and fuzzy logic techniques. Finally, an improved SEDM for campuses based on deep learning is suggested. The capability of deep learning (CNN) method is very influential in extraction of features from unstructured contents, especially from captured images of video. The efficiency of this suggested SEDM will be better when compared with earlier state-of-art-systems, which do not support alarming system by making use of GPS, and then extracting personal details.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5411-4_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-4943-1_1,Deep CNN Depth Decision in Intra Prediction,Proceedings of International Conference on Power Electronics and Renewable Energy Systems,10.1007/978-981-16-4943-1_1,Springer,2022-01-01,"The video and its compression become prominent with the emergence of digital video technology and common use of video acquisition devices. The traditional video compression needs upgradation with artificial intelligence, machine learning, neural network, and deep learning. Apart from normal signal processing the deep learning technologies are advantages as they can deal with content analysis than dealing only with neighboring pixels. The initial steps in video compression, intra/inter frame prediction provide a better percentage in overall compression. The computational complexity of existing intra prediction method is more. This paper proposes a deep learning based intra prediction method using CNN. This deep depth prediction algorithm trains the network to provide depth of the CTU with reduced computation and less time. The experimental results show a dip in the encoding time, about 71.3% compared to existing method.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-4943-1_1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-98358-1_46,Bi-attention Modal Separation Network for Multimodal Video Fusion,MultiMedia Modeling,10.1007/978-3-030-98358-1_46,Springer,2022-01-01,"With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal video understanding has received increasing attention from the scientific community. Video is usually composed of multimodal signals, such as video, text, image and audio, etc. The main method addressing this task is to develop powerful multimodal fusion techniques. Multimodal data fusion is to transform data from multiple single-mode representations to a compact multimodal representation. Effective multimodal fusion method should contain two key characteristics: the consistency and the difference. Previous studies mainly focused on applying different interaction methods to different modal fusion such as late fusion, early fusion, attention fusion, etc., but ignored the study of modal independence in the fusion process. In this paper, we introduce a fusion approach called bi-attention modal separation fusion network(BAMS) which can extract and integrate key information from various modalities and performs fusion and separation on modality representations. We conduct thorough ablation studies, and our experiments on datasets MOSI and MOSEI demonstrate significant gains over state-of-the-art models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98358-1_46,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-80119-9_54,Self-adaptive Perception Model for Action Segment Detection,Intelligent Computing,10.1007/978-3-030-80119-9_54,Springer,2022-01-01,"Action segment detection is an important yet challenging problem, since we need to localize the proposals which contain an action instance in a long untrimmed video with arbitrary length and random position. This task requires us not only to find the precise moment of starting and ending of an action instance, but also to detect action instances as many as possible. We propose a new model Self-Adaptive Perception to address this problem. We predict the action boundaries by classifying start and end of each action as separate components, allowing our model to predict the starting and ending boundaries roughly and generate candidate proposals. We evaluate each candidate proposals by a novel and flexible architecture called Discriminator. It can extract enough semantic information and generate precise confidence score of whether a proposal contains an action within its region, which benefit from the self-adaptive architecture. We conduct solid and rich experiments on large dataset Activity-Net, the result shows that our method achieves a competitive performance, outperforming most published state-of-the-art method in the field. And further experiments demonstrate the effect of each module of our model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-80119-9_54,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-5157-1_4,Video Summarization Using Fully Convolutional Residual Dense Network,Sentimental Analysis and Deep Learning,10.1007/978-981-16-5157-1_4,Springer,2022-01-01,"Video summarization is a keenly intellective video compression technique to select a subset of keyframes or keyshots which are combined to represent shorter and compendious summary of the original input video without losing the contextual semantics of the same. Previous work has shown that extracting rich contextual information from the input video frames is imperative for generating summary that is closer to human interpretation of the original input video. However, recent convolutional architectures were unable to account for the same. In this paper, we institute a novel relation between image super resolution and video summarization and introduce a novel architecture by adapting residual dense network (RDN), which fully exploits both local and global structural context. Experimental results indicate that introducing a modified RDN (SUM-RDN) unit significantly improves the performance over standard convolutional networks. Parihar, Anil Singh Mittal, Ritvik Himanshu Jain, Prashuk",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5157-1_4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-95405-5_9,Smart Online Exam Proctoring Assist for Cheating Detection,Advanced Data Mining and Applications,10.1007/978-3-030-95405-5_9,Springer,2022-01-01,"Online exams are the most preferred mode of exams in online learning environment. This mode of exam has been even more prevalent and a necessity in the event of a forced closure of face-to-face teaching such as the recent Covid-19 pandemic. Naturally, conducting online exams poses much greater challenge to preserving academic integrity compared to conducting on-site face-to-face exams. As there is no human proctor for policing the examinee on site, the chances of cheating are high. Various online exam proctoring tools are being used by educational institutes worldwide, which offer different solutions to reduce the chances of cheating. The most common technique followed by these tools is recording of video and audio of the examinee during the whole duration of exam. These videos can be analyzed later by human examiner to detect possible cheating case. However, viewing hours of exam videos for each student can be impractical for a large class and thus detecting cheating would be next to impossible. Although some AI-based tools are being used by some proctoring software to raise flags, they are not always very useful. In this paper we propose a cheating detection technique that analyzes an exam video to extract four types of event data, which are then fed to a pre-trained classification model for detecting cheating activity. We formulate the cheating detection problem as a multivariate time-series classification problem by transforming each video into a multivariate time-series representing the time-varying event data extracted from each frame of the video. We have developed a real dataset of cheating videos and conduct extensive experiments with varying video lengths, different deep learning and traditional machine learning models and feature sets, achieving prediction accuracy as high as 97.7%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95405-5_9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06527-9_17,Anomalous Trajectory Detection for Automated Traffic Video Surveillance,Bio-inspired Systems and Applications: from Robotics to Ambient Intelligence,10.1007/978-3-031-06527-9_17,Springer,2022-01-01,"Vehicle trajectories extracted from traffic video sequences can be helpful for many purposes. In particular, the analysis of detected anomalous trajectories may enhance drivers’ safety. This work proposes a methodology to detect anomalous vehicle trajectories by using a vehicle detection, a vehicle tracking and a processing of the tracking information steps. Once trajectories are detected, their velocity vectors are estimated and an anomaly value is computed for each trajectory by comparing its vector with those from its nearest neighbours. The management of these anomaly values allows considering which trajectories are suitable to be potentially anomalous considered. Real and synthetic videos have been included in the experiments to perform the goodness of the proposal.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06527-9_17,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06788-4_14,A Power Grid Equipment Fault Prediction Model Based on Faster RCNN and Video Streaming,Artificial Intelligence and Security,10.1007/978-3-031-06788-4_14,Springer,2022-01-01,"With the increasing proportion of power equipment in human life and production, the safe and stable operation of power equipment is related to the people's livelihood and national economy, and it is also very important to the sustainable development of the power industry. Grid equipment in long-term operation condition, the equipment of continuous monitoring and forecasting can guarantee the health of the whole grid system is running, the traditional way of artificial detection disadvantages such as low efficiency, high cost and difficult, gradually are replaced by the method of artificial intelligence. At present, power equipment detections mainly include two categories: image detection and video detection. Considering the real-time and image integrity of video stream detection, this article is based on the Faster RCNN model, using VGG16 network and ResNet50 network to detect video streams respectively, and comparing the efficiency and accuracy between them. According to the test results of the collected power grid video dataset, the performance of Resnet50 network is better than VGG16 network in terms of accuracy and recall rate, but VGG16 has more advantages in terms of processing speed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06788-4_14,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-981-16-3828-2_6,Realtime Accident Detection and Alarm Generation System Over IoT,"Multimedia Technologies in the Internet of Things Environment, Volume 2",10.1007/978-981-16-3828-2_6,Springer,2022-01-01,"In the recent scenario, there is a drastic improvement in transportation, infrastructure, and communication technology which increases the number of commercial as well as non-commercial vehicles. Therefore, there is also an increase in the number of accident incidence. This ultimately results in a high death rate due to a road accident. More than half of accident incidence results in death due to delayed medical aid to the victim. If medical aid or services received at the proper time, then the victim may survive. With the application of machine learning processes and communication advancements, there is scope for the development of a more accurate system. In this chapter, a model is presented based on IoT devices that can sense and predict the pre-accident/pre-collision state and generates an alarm message about the collision is going to occur. This model is designed to extracts image/video features to determine the possibility of occurrence of a collision. This model is also efficient for post-collision.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-3828-2_6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-87094-2_34,Fast and Accurate 3D Reconstruction of Plants Using MVSNet and Multi-View Images,Advances in Computational Intelligence Systems,10.1007/978-3-030-87094-2_34,Springer,2022-01-01,"Accurate 3D reconstruction of morphological structures, gro- wth processes is a prerequisite for image-based measurement of biological organisms. It is a particular challenge for digital plant and crop research. In this paper, multi-view images of structurally very different plants including Arabidopsis, Wheat and Physalis, were taken with a consumer-grade digital camera with a zoom lens and a turntable. Camera parameters were estimated using the SfM method (COLMAP), and then 3D point clouds were reconstructed using MVSNet and the camera parameters. The results show that the proposed method is able to quickly produce denser and complete 3D point cloud of plants. Compared with the existing methods, our method is an end-to-end framework and is more automatic and more promising for dense 3D reconstruction of plants, especially for plant phenotyping.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-87094-2_34,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-05814-1_8,Enhancing Autonomous Train Safety Through A Priori-Map Based Perception,"Reliability, Safety, and Security of Railway Systems. Modelling, Analysis, Verification, and Certification",10.1007/978-3-031-05814-1_8,Springer,2022-01-01,"Autonomous driving tends to increase use of perception as a tool for analyzing the environment before making a decision that could impact driving. However, recent techniques based on machine learning do not provide the necessary interpretability to ensure sufficient driving safety. Combining multiple sources, deterministic or not, allows results to be cross-referenced and therefore more reliable. In this paper, we propose a novel methodology that aligns an infrastructure mapping system and point cloud analysis for railway tracks and catenaries perception to ensure autonomous train’s safety. By using a deep learning model to recognize and classify rails with the implicit knowledge of the railway infrastructure, we exceed in performance all previous systems of infrastructure: 60.9% in mIoU for tracks segmentation and 9.27 points mMink for points alignment with ground-truth, at an interesting runtime of 20 Hz. Moreover, we propose an embedded solution for automatic monitoring which avoids hours of maintenance traffic on the railway tracks. This solution is used as acquisition system feeding map and perception in real-world data for autonomous trains.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-05814-1_8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-96299-9_17,Improving 3D Plankton Image Classification with C3D2 Architecture and Context Metadata,Innovations in Bio-Inspired Computing and Applications,10.1007/978-3-030-96299-9_17,Springer,2022-01-01,"Studying the variations of the submarine environment at the plankton level can significantly contribute to the preservation of the environment. In situ plankton imaging systems have known an important evolution giving large scale plankton data for organism classification and analysis. Automated classifiers based on Convolutional Neural network are identified as highly efficient methods for image classification but require careful configuration especially for 3D images. In this paper, we propose a CNN architecture for 3D image classification to classify 155 classes of plankton from TARA Oceans dataset in four levels of hierarchical classes. We experiment and compare our proposal denoted C3D2 with competitive CNNs already performed on the case of plankton recognition such as DenseNet and SparseConvNet. Furthermore, we design several methods to incorporate context metadata on CNN architectures in order to boost the performance of the classification model. Finally, we show that C3D2 is more precise than other models. We also show the impact of incorporating context metadata into CNN architecture on different levels of classes.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96299-9_17,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06464-y,Multi-features guidance network for partial-to-partial point cloud registration,Neural Computing and Applications,10.1007/s00521-021-06464-y,Springer,2022-01-01,"The recent extraction of hybrid features improves point cloud registration performance by emphasizing more integrated information. However, hybrid features ignore the large dimensional differences, big semantic gaps, and mutual interference between the shape features and spatial coordinates. This paper proposes a novel Multi-Features Guidance Network (MFGNet) for partial-to-partial point cloud registration to overcome the intrinsic flaws of hybrid features, which leverages the shape features and the spatial coordinates to account for correspondences searching independently. The proposed network mainly includes four parts: keypoints’ feature extraction, correspondences search, correspondences credibility computation, and singular value decomposition (SVD), among which correspondences search and correspondences credibility computation are the cores of the network. Specifically, the correspondences search module utilizes the shape features and the spatial coordinates to guide correspondences matching independently and fusing the matching results to obtain the final matching matrix. Moreover, based on the conflicted relationship between the two matching matrices, the correspondences credibility computation module scores each correspondence pair’s reliability, which can reduce the impact of mismatched or non-matched points significantly. Empirical experiments on the ModelNet40 dataset validate the effectiveness of the proposed MFGNet, which achieves 0.19 $$^\circ$$ ∘ , 0.24 $$^\circ$$ ∘ and 1.3 $$^\circ$$ ∘ mean absolute errors for rotation matrix and 0.0010, 0.0011, and 0.0068 mean absolute errors for translation vectors, respectively, under the settings of unseen point clouds, unseen categories, and Gaussian noise.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06464-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-6963-7_19,Compressed Channel Attention Mechanism for 3D Medical Image Segmentation of Liver,"The International Conference on Image, Vision and Intelligent Systems (ICIVIS 2021)",10.1007/978-981-16-6963-7_19,Springer,2022-01-01,"More and more Convolutional Neural Networks (CNNs) are used in computer vision, especially in the field of medical images. Since most of the medical data in clinical practice are three-dimensional, which is mainly obtained from imaging techniques such as MRI and CT, the use of the previous two-dimensional neural network becomes untimely. In this work, an end-to-end trained 3D image segmentation network combined with the Compressed Channel Attention Module (CCAM) is proposed to learn to predict the segmentation of the entire liver at one time. We embed the CCAM in the network structure of up-sampling and down-sampling. First, we divide the feature map obtained by down-sampling into two parts. The first part performs global average pooling and maximum pooling and then splicing, and the other part performs 1 × 1 convolution on the feature map. Then the two parts are merged, and finally spliced with the up-sampled feature map to realize the use of down-sampling features to monitor the up-sampled features, focusing on specific livers, and suppressing irrelevant areas in the input image. Experiments have been carried on the LITS dataset. Compared with the existing segmentation methods, the proposed method has better segmentation performance in both subjective and objective evaluation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-6963-7_19,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13042-021-01342-4,PointCSE: Context-sensitive encoders for efficient 3D object detection from point cloud,International Journal of Machine Learning and Cybernetics,10.1007/s13042-021-01342-4,Springer,2022-01-01,"Few modern 3D object detectors achieve fast inference speed and high accuracy at the same time. To achieve high performance, they usually directly operate on raw point clouds, or convert point clouds to 3D representation and then apply 3D convolution. However, those methods come with sizable computation overhead and complex operations. As for high-speed 2D-representation-based 3D detectors, their performance is still restricted. In this paper, we investigate how to leverage context knowledge to empower the 2D representation of point clouds for computation and memory-efficient 3D object detection with state-of-the-art performance. The proposed encoder has two parts: a context-sensitive point sampling network and a point set learning network. Specifically, our point sampling network samples points with dense localization information. With high-quality sampled points, we are allowed to utilize a deeper point set learning network to aggregate semantic details in a light manner. The proposed encoder is lightweight and very supportive of hardware acceleration like TensorRT and TVM. Extensive experiments on the KITTI benchmark show the proposed encoder called PointCSE outperforms prior real-time encoders by a large margin with 1.5 $$\times$$ × memory reduction; it also achieves state-of-the-art performance with 49 FPS inference speed ( 4 $$\times$$ × speedup on average compared to previous best methods).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13042-021-01342-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-08223-8_28,Brain Tumour Segmentation on 3D MRI Using Attention V-Net,Engineering Applications of Neural Networks,10.1007/978-3-031-08223-8_28,Springer,2022-01-01,"Brain tumour segmentation on 3D MRI imaging is one of the most critical deep learning applications. In this paper, for the segmentation of tumour sub-regions in brain MRI images, we study some popular architecture for medical imaging segmentation. We further, inspired by them, proposed an architecture that is an end-to-end trainable, fully convolutional neural network that uses attention block to learn localization of different features of the multiple sub-regions of a tumour. We also experiment with a combination of the weighted cross-entropy loss function and dice loss function on the model’s performance and the quality of the output segmented labels. The results of the evaluation of our model are received through BraTS’19 dataset challenge. The model can achieve a dice score of 0.80 for the whole tumour segmentation and dice scores of 0.639 and 0.536 for the other two sub-regions within the tumour on the validation dataset.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-08223-8_28,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06351-6,Glioma segmentation of optimized 3D U-net and prediction of multi-modal survival time,Neural Computing and Applications,10.1007/s00521-021-06351-6,Springer,2022-01-01,"It is difficult to segment Glioma and its internal structure because the Glioma boundaries have edemas and complex internal structures. This paper proposes a new optimized, integrated 3D U-Net network to achieve accurate segmentation of Glioma and internal subareas. The contribution of this paper is twofold, it studies the clinical path of patients with Glioma and constructs an optimized 3D U-Net deep learning algorithm by combining them with the radiologic feature set. The proposed model was validated in the published Glioma operation data set of multi-modal MRI resonance images and clinicians manual segmentation data. The model can accurately segment the MRI multi-modality images of Glioma and intra-tumour nodes and achieve the multi-modality prediction of the overall survival period of patients. The experimental results further indicated that the segmentation accuracy of the proposed method was higher than other sophisticated methods. The Dice similarity coefficients of the whole tumor (WT) region, the core tumor (CT) region, and the augmentation / enhanced tumor (ET) region, were 0.9632, 0.8763, and 0.8421, respectively, which are better than the clinical experts’ manual segmentation results. Hence, this research can effectively promote the development of deep learning clinical precise diagnosis and medical technology for Glioma.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06351-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-94893-1_3,CSG Tree Extraction from 3D Point Clouds and Meshes Using a Hybrid Approach,"Computer Vision, Imaging and Computer Graphics Theory and Applications",10.1007/978-3-030-94893-1_3,Springer,2022-01-01,"The problem of Constructive Solid Geometry (CSG) tree reconstruction from 3D point clouds or 3D triangle meshes is hard to solve. At first, the input data set (point cloud, triangle soup or triangle mesh) has to be segmented and geometric primitives (spheres, cylinders, ...) have to be fitted to each subset. Then, the size- and shape optimal CSG tree has to be extracted. We propose a pipeline for CSG reconstruction consisting of multiple stages: A primitive extraction step, which uses deep learning for primitive detection, a clustered variant of RANSAC for parameter fitting, and a Genetic Algorithm (GA) for convex polytope generation. It directly transforms 3D point clouds or triangle meshes into solid primitives. The filtered primitive set is then used as input for a GA-based CSG extraction stage. We evaluate two different CSG extraction methodologies and furthermore compare our pipeline to current state-of-the-art methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-94893-1_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-98253-9_13,Head and Neck Tumor Segmentation with Deeply-Supervised 3D UNet and Progression-Free Survival Prediction with Linear Model,Head and Neck Tumor Segmentation and Outcome Prediction,10.1007/978-3-030-98253-9_13,Springer,2022-01-01,"Accurate segmentation of Head and Neck (H&N) tumor has important clinical relevance in disease characterization, thereby holding a strong potential for better cancer treatment planning and optimized patient care. In recent times, the development in deep learning-based models has been able to effectively and accurately perform medical images segmentation task that eliminates the problems associated with manual annotation of region of interest (ROI) such as significant human efforts and inter-observer variability. For H&N tumors, FDG-PET and CT carry complementary information of metabolic and structural details of tumor and the fusion of those modalities were explored in this study, which led to significant enhancement in performance level with combined data. Furthermore, deep supervision technique was applied to the segmentation network, where the computation of loss occurs at multiple layers that allows for gradients to be injected deeper into the network and facilitates the training. Our proposed segmentation methods yield promising result, with a Dice Similarity Coefficient (DSC) of 0.731 in our cross-validation experiment. Finally, we developed a linear model for progression-free survival prediction using extracted imaging and non-imaging features.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98253-9_13,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9423-3_38,New Trend in Front-End Techniques of Visual SLAM: From Hand-Engineered Features to Deep-Learned Features,Artificial Intelligence in China,10.1007/978-981-16-9423-3_38,Springer,2022-01-01,"Visual simultaneous localization and mapping (V-SLAM) technique plays a key role in perception of autonomous mobile robots, augmented/mixed/virtual reality, as well as spatial AI applications. This paper gives a very concise survey about the front-end module of a V-SLAM system, which is charge of feature extraction, short/long-term data association with outlier rejection, as well as variable initialization. Visual features are mainly salient and repeatable points, called keypoints, their traditional extractors and matchers are hand-engineered and not robust to viewpoint/illumination/seasonal change, which is crucial for long-term autonomy of mobile robots. Therefore, new trend about deep-learning-based keypoint extractors and matchers are introduced to enhance the robustness of V-SLAM systems even under challenging conditions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9423-3_38,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-94141-3_8,Two-Stage Method of Speech Denoising by Long Short-Term Memory Neural Network,"High-Performance Computing Systems and Technologies in Scientific Research, Automation of Control and Production",10.1007/978-3-030-94141-3_8,Springer,2022-01-01,"This work is devoted to the development of a method for cleaning single-channel speech audio signals from additive noise. The main feature of the method is the application of a two-stage neural network. At the first stage, wideband processing of the input noisy signal is carried out, which allows the effective estimation of noise with a sophisticated spectral structure. The second stage of signal processing is a two-component neural network over the result of matrix representations that reveals the quasi-stationary characteristics of the clean and noise signal components in separate overlapping narrow frequency bands. The use of two components of noisy signals, which estimate complex masks of the clean and noise parts, makes it possible to improve useful information extraction about the formant structure of speech and effectively clean the input signal from noises of various nature. All of this together made it possible to develop a new method of speech enhancement, surpassing the best existing solutions in most quality metrics.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-94141-3_8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-95798-8_1,What Is Industry 4.0?,Innovation in Construction,10.1007/978-3-030-95798-8_1,Springer,2022-01-01,"This chapter briefly presents an introduction to key technological innovations in construction and their potential/current uses in the industry. Some of these technologies have been written about extensively; however, the summaries presented below are provided as a convenient introduction and reference for subsequent chapters.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95798-8_1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-07312-0_5,“Hey CAI” - Conversational AI Enabled User Interface for HPC Tools,High Performance Computing,10.1007/978-3-031-07312-0_5,Springer,2022-01-01,"HPC system users depend on profiling and analysis tools to obtain insights into the performance of their applications and tweak them. The complexity of modern HPC systems have necessitated advances in the associated HPC tools making them equally complex with various advanced features and complex user interfaces. While these interfaces are extensive and detailed, they require a steep learning curve even for expert users making them harder to use for novice users. While users are intuitively able to express what they are looking for in words or text (e.g., show me the process transmitting maximum data), they find it hard to quickly adapt to, navigate, and use the interface of advanced HPC tools to obtain desired insights. In this paper, we explore the challenges associated with designing a conversational (speech/text) interface for HPC tools. We use state-of-the-art AI models for speech and text and adapt it for use in the HPC arena by retraining them on a new HPC dataset we create. We demonstrate that our proposed model, retrained with an HPC specific dataset, can deliver higher accuracy than the existing state-of-the-art pre-trained language models. We also create an interface to convert speech/text data to commands for HPC tools and show how users can utilize the proposed interface to gain insights quicker leading to better productivity. To the best of our knowledge, this is the first effort aimed at designing a conversational interface for HPC tools using state-of-the-art AI techniques to enhance the productivity of novice and advanced users alike.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-07312-0_5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-981-19-2053-0_17,Emerging Technologies and Innovation to Reach Out to Vulnerable Populations in Nepal,Technology Entrepreneurship and Sustainable Development,10.1007/978-981-19-2053-0_17,Springer,2022-01-01,"In the dawn of the Fourth Industrial Revolution (Industry 4.0) shaped by Artificial Intelligence (AI) blockchain, Internet of Things, multiverse, and quantum computing, the emphasis on creativity and innovation, entrepreneurship, and technology development also affect the ways the so-called non-profit sectors design and implement development and humanitarian programmes in developing countries like Nepal. Following the 2015 Nepal Earthquake Response, World Vision International Nepal (WVI Nepal) has ideated, prototyped, and scaled up community-focused innovative solutions like SIKKA, KITAB Bazar, and Participatory Disaster Risk Assessment (PDRA) tools integrated into its development and humanitarian initiatives. SIKKA uses blockchain technology to ensure cash and voucher distributions are transparent and trackable, and the system has been used to assist 92,788 people during the COVID-19 response. Similarly, KITAB Bazar, an online marketplace platform, has served 53,412 children in 831 community schools by providing 133,008 supplementary reading books in local languages through the platform. Likewise, the PDRA tool capacitates communities to identify disaster-prone areas and engage stakeholders for response through mobile applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-2053-0_17,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06527-9_39,Deep Reinforcement Learning in Agents’ Training: Unity ML-Agents,Bio-inspired Systems and Applications: from Robotics to Ambient Intelligence,10.1007/978-3-031-06527-9_39,Springer,2022-01-01,"Video games are an area where Artificial Intelligence has multiple application scenarios, allowing to add improvements that can be applied to provide greater realism in the game experience, accelerate its development (even automate it) and save costs, among other benefits. Beyond the commercial vision and from a research point of view, different strategies and algorithms are applied in certain facets/applications that pose a significant challenge in terms of the development of these algorithms and their applicability (in this area and others). These applications include the creation of intelligent agents (which can be cooperate or adversarial), the automatic generation of content (structures, characters, scenarios, etc.), the modeling of player behavior and habits, and particular rendering techniques. This paper focuses on the use of the open source project Unity ML-Agents Toolkit to train different intelligent agents using Deep Reinforcement Learning techniques and associated learning algorithms applied to this scenario of Artificial Intelligence use.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06527-9_39,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0840-8_3,Human Emotion Classification Based on Speech Enhancement Using Neural Networks,Advanced Machine Intelligence and Signal Processing,10.1007/978-981-19-0840-8_3,Springer,2022-01-01,"The quality of speech signal is more concerned in the domain of speech processing. Impulsive noise in speech signal has a capability to deteriorate the overall performance and reliability of the speech signal. This paper presents an idea of speech enhancement by amputation of impulsive disruption (noise) with the aid of modifying the dual spectrum (noisy magnitude and phase spectrum) under short time Fourier transform (STFT) domain. The processing step involves signal segmentation, short-time Fourier transforms, spectrum modification, and signal concatenation. Initially, the samples of the signal that are noisy are broke down into a limited count of frames, and each of them are assessed with coefficients generated using hamming window. The noisy signal spectrum will be attained through fast Fourier transform, and noisy components are removed using spectral subtraction. After the restoring process, the filtered signal magnitude combined with signal phase and then inverse transform followed by signal concatenation will be done to reconstruct signal. This paper mainly brings forward a proposal for extraction of features by using Mel-frequency cepstral coefficients (MFCCs). Initially, it takes the input speech signal that is enhanced in terms of reducing noise followed by the classification of emotion from the resultant output using neural network training. The simulated result shows that the proposed filtering model is more accurate.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0840-8_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-7610-9_7,Novel Intelligent System for Medical Diagnostic Applications Using Artificial Neural Network,Intelligent Data Communication Technologies and Internet of Things,10.1007/978-981-16-7610-9_7,Springer,2022-01-01,"In recent years, the recognition of images with feature extraction in medical applications is a big challenge. It is a tough task for the Doctors to diagnose the diseases through image recognition with the scanned images or x-ray images. To enhance the image recognition with feature extraction for the medical applications, a novel intelligent system has been developed using artificial neural network. It gives high efficiency in recognizing the image with feature extraction compared over fuzzy logic system. The artificial neural network algorithm was used for the feature extraction from the scanned images of patients. The implementation has been carried out with the help of Tensor flow and Pytorch. The algorithms was tested over 200 sets of scanned images has been utilized for the classification and prediction of trained dataset images. The analysis on the data set and test cases has been performed successfully and acquired 81% of accuracy for the image recognition using artificial neural network algorithm. With the level of significance ( p  < 0.005), the resultant data depicts the reliability in independent sample t tests. The process of prediction of accuracy for the image recognition, through the ANN gives significantly better performance than the fuzzy logic system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7610-9_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8558-3_6,Image Recognition Technology Based Evaluation Index of Ship Navigation Risk in Bridge Area,New Approaches for Multidimensional Signal Processing,10.1007/978-981-16-8558-3_6,Springer,2022-01-01,"China has a vast territory, numerous inland river systems, with abundant water transport resources. More and more ships are traveling in inland waterways. Therefore, the risk of ship accidents in inland waterways is increasing year by year. This paper mainly studies the research and application of the navigation safety risk evaluation index system in bridge area based on image recognition technology. The convolutional neural network based detection model for inland river ships is proposed. Firstly, several common target detection algorithms are compared and analyzed in this paper, and a single-stage target detection algorithm with the best performance is selected, which is combined with the target detection algorithm according to the navigable environment characteristics of the bridge area. On the basis of ship track prediction, this paper studies the quantification of collision, grounding, hitting reef and collision risk and establishes the ship collision risk evaluation model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8558-3_6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-91581-0_28,Classification of Tree Species by Trunk Image Using Conventional Neural Network and Augmentation of the Training Sample Using a Telegram-Bot,"Advances in Neural Computation, Machine Learning, and Cognitive Research V",10.1007/978-3-030-91581-0_28,Springer,2022-01-01,"This paper considers the problem of creating a model of a convolutional neural network for recognizing tree species from the image of a trunk for ground-based lidar taxation of forest stands. To increase the probability of recognition, it is proposed to use a telegram bot for augmentation of the training set. Training, selection and comparison of convolutional neural network models was performed. A telegram bot has been created that allows you to automate the collection of images of the training sample. The study opens a cycle of works on modeling the carbon balance of forest plantations.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-91581-0_28,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-05014-5_8,Remote at Court,"Design, Operation and Evaluation of Mobile Communications",10.1007/978-3-031-05014-5_8,Springer,2022-01-01,"This article targets one of the fundamental changes in the judicial system induced by the severe limitations due to the absence of face-to-face meetings: the application of video conferencing in court sessions, an application with special requirements in this critical domain. A semi-structured literature review that we conducted revealed a lack of human-centered approaches. Potentials and challenges, mainly focused on the needs of judges, were also identified. These challenges were then transformed into requirements for designs of video conferencing systems in the judicial context. We ultimately developed a low-fidelity prototype of a system that incorporates a novel combination of three use-case-specific features: a solution to manage fatigue, a solution to manage user participation, and cognitive aid based on artificial intelligence (AI). The aim of the last feature was to reduce cognitive load while improving the moderation quality of court session leaders. Through a heuristic evaluation by human-computer interaction (HCI) and domain experts, the benefits of the basic design ideas, as well as potential areas for improvement, were identified. This paper presents the first systematic analyses of the potentials and limitations of video conferencing in German court sessions. It brings the enormous challenges of a critical domain in society, as well as human-centered and value-sensitive digitalization and AI adoption, under the spotlight.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-05014-5_8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10462-021-10047-7,Unsupervised learning in images and audio to produce neural receptive fields: a primer and accessible notebook,Artificial Intelligence Review,10.1007/s10462-021-10047-7,Springer,2022-01-01,"Sensory processing relies on efficient computation driven by a combination of low-level unsupervised, statistical structural learning, and high-level task-dependent learning. In the earliest stages of sensory processing, sparse and independent coding strategies are capable of modeling neural processing using the same coding strategy with only a change in the input (e.g., grayscale images, color images, and audio). We present a consolidated review of Independent Component Analysis (ICA) as an efficient neural coding scheme with the ability to model early visual and auditory neural processing. We created a self-contained, accessible Jupyter notebook using Python to demonstrate the efficient coding principle for different modalities following a consistent five-step strategy. For each modality, derived receptive field models from natural and non-natural inputs are contrasted, demonstrating how neural codes are not produced when the inputs sufficiently deviate from those animals were evolved to process. Additionally, the demonstration shows that ICA produces more neurally-appropriate receptive field models than those based on common compression strategies, such as Principal Component Analysis. The five-step strategy not only produces neural-like models but also promotes reuse of code to emphasize the input-agnostic nature where each modality can be modeled with only a change in inputs. This notebook can be used to readily observe the links between unsupervised machine learning strategies and early sensory neuroscience, improving our understanding of flexible data-driven neural development in nature and future applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10462-021-10047-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-1280-1_21,Path Optimization for Multi-material 3D Printing Using Self-organizing Maps,Computer-Aided Architectural Design. Design Imperatives: The Future is Now,10.1007/978-981-19-1280-1_21,Springer,2022-01-01,"S hape generation based on scalar fields opened up the space for new fabrication techniques bridging the digital and the physical through material computation. As an example, the development of voxelized methods for shape generation broadened the exploration of multi-material 3d printing and the use of Functionally Gradient Materials (FGM) through the creation of shapes based on their material properties known as Property representations (P-reps) as opposed to Boundary representations (B-reps) [ 1 ]. This paper proposes a novel approach for the fabrication of P-reps by generating optimized 3d printing paths by mapping shape internal stress into material distribution through a single optimized curve oriented to the fabrication of procedural shapes. By the use of a modified version of the traveling salesman problem (TSP), an optimized Spline is generated to map trajectories and material distribution into voxelized shape’s slices. As a result, we can obtain an optimized P-Rep G-code generation for multi-material 3d printing and explore the fabrication of P-Rep as FGMs based on material behavior.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-1280-1_21,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-94102-4_3,Autonomous Vehicle Assisted by Heads up Display (HUD) with Augmented Reality Based on Machine Learning Techniques,Virtual and Augmented Reality for Automobile Industry: Innovation Vision and Applications,10.1007/978-3-030-94102-4_3,Springer,2022-01-01,"The safety in driving is improved and driving workload is minimized, the provided information is understandably and the cognitive load on the driver is low. For the autonomous vehicle, machine learning-based AR-HUD (augmented reality based Head-up display) is used. In this paper the machine learning-based AR-HUD has been used for autonomous vehicles. The process of object detection and collected HUD data classification has been done by the proposed model. Determining the present state of vehicle has been validated based on the AR environment. The process of test and validation is an integral portion of a development cycle. Machine learning and deep neural network are used in this paper for lab & real-world T&V for ARHUD and autonomous vehicles. The results of simulation obtain the data gathered from the implementation of human and machine interface (HMI) to detect the object and to classify the objects in motion. Accuracy, precision, recall and F-1 score are the analyzed parameters for machine learning-based ARHUD. The simulation results obtained are accuracy of 98%, precision 94%, recall 92.3% and F-1 score 86% in comparison with CNN, ANN and SVM.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-94102-4_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-89906-6_3,On the Design of Therapy Tracker: A Cross-platform Medication Management Application with Augmented Reality and Machine Learning,"Proceedings of the Future Technologies Conference (FTC) 2021, Volume 1",10.1007/978-3-030-89906-6_3,Springer,2022-01-01,"In this research a prototype of a virtual assistant mobile application, called Therapy Tracker is introduced. The application detects and captures prescription information and provides automated assistance in scheduling, reminding and tracking medications by keeping a list of medications, dosages, refill information, and prescription documentation. Despite the rapid growth and recent successes, creating such auto-assistant systems remains a challenging task. Augmented Reality technologies are used in our app to provide virtual information that combines real world situations, such as detecting and highlighting the correct medication from a group. The virtual assistant also guides the user through both visual and auditory alerts. In this paper we present Therapy Tracker , the interface design, architectural framework we designed and developed to support the system, and the results of a usability study conducted from real field studies. Our system was tested with 30 participants and achieved a mean SUS usability score of 84.5% (high).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-89906-6_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-76724-2_25,Virtual Reality: A Possibility for Training Operator 4.0,Industrial Engineering in the Internet-of-Things World,10.1007/978-3-030-76724-2_25,Springer,2022-01-01,"As technological advancements have led to the onset of industry 4.0, there must be a shift in the organizational frameworks, wherein manufacturing propensities and the traditional workplaces must transform to adopt a system wherein humans and machines can work together towards a rise in productivity and flexibility. These recent developments call for an up-gradation of the traditional operator to Operator 4.0 or Smart Operator as there has been a rise in a typical, unique work relationships which will require interactions between the operators and machines. These are challenging circumstances for engineers and operators as they must enhance their skills and abilities to keep up with the changing trend. Smart operators will also need to update their cognitive skills parallel with the innovations in the field of virtual reality and wearable technology equipment, as the equipment’s augment the abilities of the Operator 4.0. This article aims to (1) Highlight the cognitive ergonomics skills required by operator 4.0 for Virtual Reality VR applications (2) Demonstrate how VR can be used to train future operator 4.0 to do his future tasks (3) Suggest how VR application in manufacturing can be improved for operator 4.0. The methodology in this research involves the usage of Virtual Reality to simulate a factory setup emulating Industry 4.0 factory. Based on the feedback of the participants, the efficiency of VR in training workers in the manufacturing industry can be determined. Also, this exercise can aid designers in avoiding Graphical User Interface issues and further develop the system to maximize efficiency and to provide a positive user experience.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-76724-2_25,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-020-08896-5,A closed-loop healthcare processing approach based on deep reinforcement learning,Multimedia Tools and Applications,10.1007/s11042-020-08896-5,Springer,2022-01-01,"In healthcare, the human body is a controlled input-output system, which generates different observations with the variations of external interventions. The intervention acts as the input, and the output is the phenotype observation that reflects the latent health state of the body system. The objective of healthcare is to determine effective intervention strategies that can nurse an unhealthy human body to a healthy state. With the advances of Internet-of-Things (IoT) and body sensor networks, it becomes convenient to observe the multimedia data of the human body anywhere and anytime. To aid healthcare decision making, we put forward to construct the human body simulators based on deep neural networks (DNNs) for healthcare research. At first, we formulate the model of the human body system based on DNNs. During our analysis, we realize that DNN-based models could simulate practical situations, e.g. some health states are unreachable. Then, we combine deep reinforcement learning (DRL) with conceptual embedding techniques to explore effective healthcare strategies for simulated human bodies. We implement a virtual human body simulator, which can take interventions and represent its hidden states by high-dimensional images, and a DRL-based treatment module, which can diagnose latent health state through the image observations and choose interventions to nurse the simulated body to a target state. By combining the body simulator and treatment module, we create a dynamic closed-loop for healthcare information processing. Experimental simulations are performed to validate the feasibility of the offered approach.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-08896-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0840-8_35,Real-Time Speech Recognition Using Convolutional Neural Network,Advanced Machine Intelligence and Signal Processing,10.1007/978-981-19-0840-8_35,Springer,2022-01-01,"Due to the rapid advancements in algorithms, the systems based on speech recognition are now on the verge of huge commercial success. It is proved that speakers can be identified from their speech signals. In this paper, an algorithmically improved system for speech recognition in real time with convolutional neural network (CNN) based on English language has been discussed with obtained results. The system performs on mel frequency cepstral coefficient (MFCC) technique with cepstral-based features and for feature vector matching various techniques like CNN and dynamic time wrapping (DTW). Different speech processing techniques like pre-emphasis, framing, windowing, triangular filtering, DFT, and DCT have been used to process the uttered speech, and MFCC vectors are extracted from input speech sample, and based on obtained coefficients, feature vectors for each uttered speech are stored in database. Mapping is done by DTW for speech recognition. Total system is trained with CNN. A graphical user interface (GUI) is developed for efficient user friendly use and simplicity. The overall system is divided into training phase and testing phase. While training, system is trained with input samples and while testing, test sample is mapped with stored database obtained during training phase. In this paper, 70% database is used for training and 30% database is used for testing. This system is having accuracy almost 100% for 1758 speech samples containing letters and single word speech samples like right, A, E, etc. If database goes on increasing, accuracy may decrease with increase in number of speakers and number of speech samples uttered by each speaker.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0840-8_35,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-96308-8_8,Honey Bee Queen Presence Detection from Audio Field Recordings Using Summarized Spectrogram and Convolutional Neural Networks,Intelligent Systems Design and Applications,10.1007/978-3-030-96308-8_8,Springer,2022-01-01,"The present work proposes a simple supervised method based on a downsampled time-frequency representation of the input audio signal for detecting the presence of the queen in a beehive from noisy field recordings. Our proposed technique computes a “summarized-spectrogram” of the signal that is used as the input of a deep convolutional neural network. This approach has the advantage of reducing the dimension of the input layer and the computational cost while obtaining better classification results with the same deep neural architecture. Our comparative evaluation based on a cross-validation beehive-independent methodology shows a maximal accuracy of 96% using the proposed approach applied on the evaluation dataset. This corresponds to a significant improvement of the prediction accuracy in comparison to several state-of-the-art approaches reported by the literature. Baseline methods such as MFCC, constant-Q transform and classical STFT combined with a CNN fail to generalize the prediction of the queen presence in an unknown beehive and obtain a maximal accuracy of 55% in our experiments.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96308-8_8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-10461-9_23,EasyChat: A Chat Application for Deaf/Dumb People to Communicate with the General Community,Intelligent Computing,10.1007/978-3-031-10461-9_23,Springer,2022-01-01,"Sign Language is closely associated with the deaf and dumb community to communicate with each other. However, not everyone understands sign language or verbal languages, so these communities need proper ways to communicate online. Therefore, this paper presents EasyChat, a sign language chat application that can translate three main sign languages into Simple English text as well as Simple English text into sign language, which would benefit for deaf/dumb community to express their ideas with the general community by simply capturing their British Sign Language (BSL) or Makaton gestures/symbols or lip movements. These steps are handled by four components. The first component, Convert BSL into Simple English, and the second component, handles Lip Reading conversion. The Makaton gesture and symbol conversion component produces a simple English text-formatted output for identified Makaton hand signs. Finally, the Text/voice to Sign Converter works on converting entered English text back into the sign language-based images. By using these components, EasyChat can detect relevant gestures and lip movement inputs with superior accuracy and translate. This can lead to more effective and efficient online communication between the community of deaf/dumb individuals and the general public.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-10461-9_23,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-01942-5_5,Vocal Parameters Analysis for Amazigh Phonemes Recognition System,Digital Technologies and Applications,10.1007/978-3-031-01942-5_5,Springer,2022-01-01,"Several studies in the development of Automatic Speech Recognition (ASR) systems have been yet presented and well studied for many languages. However, the particularities of the Amazigh Language due to its specific acoustic features such as the emphasis on Amazigh consonants pose many problems for developing its automatic speech recognition system. In this paper, we deal with the spoken recognition problem in Amazigh Language. We consider here a particular case of the problem, focusing on the phonemes formants frequencies which are a principal variable in specifying the phonetic substance of discourse sounds. We propose an approach allowing the vocal parameters analysis of phonetic units, as well as a recognition algorithm by analyzing the formants.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-01942-5_5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-90633-7_30,Phonemes Recognition Using Formant Analysis in the Case of Consonant Vowel Transition Case “Amazigh Language”,Advanced Intelligent Systems for Sustainable Development (AI2SD’2020),10.1007/978-3-030-90633-7_30,Springer,2022-01-01,"Several studies in the development of Automatic Speech Recognition (ASR) systems have been yet presented and well studied for many languages. However, the particularities of the Amazigh Language due to its specific acoustic features such as the emphasis on Amazigh consonants pose many problems for developing its automatic speech recognition system. In this paper, we deal with the spoken recognition’s problem in Amazigh Language. We consider here a particular case of the problem, focusing on the vowel-consonant transitions, by treating the recognition of phoneme level. We propose a derivative algorithm allowing the phoneme separation based on the difference between the energies of both consonants and vowels.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-90633-7_30,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-89511-2_106,Speech Perception Model Based on Artificial Intelligence Technology,The 2021 International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy,10.1007/978-3-030-89511-2_106,Springer,2022-01-01,"This article analyzes various factors that affect English phonological perception, points out that artificial intelligence technology can help improve English learners’ English phonological perception, and puts forward some suggestions on specific teaching modes. The teaching of English phonological perception should follow the development of artificial intelligence, promote its own teaching reform and innovation, build an education system to realize personalized learning and lifelong development.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-89511-2_106,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9247-5_7,A Scalable 3D Array Architecture for Accelerating Convolutional Neural Networks,Cognitive Systems and Information Processing,10.1007/978-981-16-9247-5_7,Springer,2022-01-01,"Convolutional neural network (CNN) is widely used in computer vision and image recognition, and the structure of the CNN becomes more and more complex. The complexity of CNN brings challenges of performance and storage capacity for hardware implementation. To address these challenges, in this paper, we propose a novel 3D array architecture for accelerating CNN. This proposed architecture has several benefits: Firstly, the strategy of multilevel caches is employed to improve data reusage, and thus reducing the access frequency to external memory; Secondly, performance and throughout are balanced among 3D array nodes by using novel workload and weight partitioning schemes. Thirdly, computing and transmission are performed simultaneously, resulting in higher parallelism and lower hardware storage requirement; Finally, the efficient data mapping strategy is proposed for better scalability of the entire system. The experimental results show that our proposed 3D array architecture can effectively improve the overall computing performance of the system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9247-5_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0572-8_7,Design of Warehouse Chattel Supervision System Based on AI Video,Advanced Manufacturing and Automation XI,10.1007/978-981-19-0572-8_7,Springer,2022-01-01,"In this paper, the AI video supervision system of chattel is designed to monitor and manage chattel. The video is logged to AI processors by means of the image or video collecting hardware in this system. Once the system finding difference between the video and original setting, the data is output to alarms. The function of automatic, all-weather, intelligent and no dead angle is realized in the supervision of chattel system, and the system provides an effective and feasible way for the high efficiency and low cost of intelligent warehouse operation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0572-8_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-2597-8_37,Video Event Classification and Recognition Using AI and DNN,International Conference on Innovative Computing and Communications,10.1007/978-981-16-2597-8_37,Springer,2022-01-01,"The paper proposes an efficient deep learning and AI approach for video event recognition. The proposed approach consists of several steps as video acquisition, framing, object detection, joint velocity detection, heat map generation, pose data collection, finally, the classification using deep neural network. The proposed approach is divided into two parts; the first part is object detection, in which we use bounding boxes to detect human presence. The second part is posing detection, which includes identifying human body joints by using open pose. Then, we extract features of body velocity; normalize joint positions and joint velocity. After this step we apply Principle Component Analysis (PCA) to reduce the features and then classify by using Deep Neural Network (DNN) of 3 layers of 50 × 50 × 50. The proposed approach can render up to 6 fps; however, it can be improved with the GPU. The performance of the proposed approach is quite satisfactory with the accuracy of 94.67%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-2597-8_37,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-81716-9_8,A Self-adaptive Hybrid Model/data-Driven Approach to SHM Based on Model Order Reduction and Deep Learning,Structural Health Monitoring Based on Data Science Techniques,10.1007/978-3-030-81716-9_8,Springer,2022-01-01,"Aging of structures and infrastructures urges new approaches to ensure higher safety levels without service interruptions. Structural health monitoring (SHM) aims to cope with this need by processing the data continuously acquired by pervasive sensor networks, handled as vibration recordings. Damage diagnosis of a structure consists of detecting, localizing, and quantifying any relevant state of damage. Deep learning (DL) can provide an effective framework for data processing, regression, and classification tasks used for the aforementioned damage diagnosis purposes. Within this framework, we propose an approach that exploits a deep convolutional neural network (NN) architecture. The training of the NN is carried out by exploiting a dataset, numerically built through a physics-based model of the structure to be monitored. Parametric model order reduction (MOR) techniques are then exploited to reduce the computational burden related to the dataset construction. Within the proposed approach, whenever a damage state is detected, the physical model of the structure is adaptively updated, and the dataset is enriched to retrain the NN, allowing for the previously detected damage state as the new baseline.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-81716-9_8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06156-1_6,Continuous Self-adaptation of Control Policies in Automatic Cloud Management,Euro-Par 2021: Parallel Processing Workshops,10.1007/978-3-031-06156-1_6,Springer,2022-01-01,"Deep Reinforcement Learning has been recently a very active field of research. The policies generated with use of that class of training algorithms are flexible and thus have many practical applications. In this paper we present the results of our attempt to use the recent advancements in Reinforcement Learning to automate the management of resources in a compute cloud environment. We describe a new approach to self-adaptation of autonomous management, which uses a digital clone of the managed infrastructure to continuously update the control policy. We present the architecture of our system and discuss the results of evaluation which includes autonomous management of a sample application deployed to Amazon Web Services cloud. We also provide the details of training of the management policy using the Proximal Policy Optimization algorithm. Finally, we discuss the feasibility to extend the presented approach to further scenarios.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06156-1_6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11383-0,Content modification of soccer videos using a supervised deep learning framework,Multimedia Tools and Applications,10.1007/s11042-021-11383-0,Springer,2022-01-01,"In this paper, we initially propose a novel framework for replacing advertisement contents in soccer videos in an automatic way by using deep learning strategies. For this purpose, we begin by applying UNET (an image segmentation convolutional neural network technique) for content segmentation and detection. Subsequently, after reconstructing the segmented content in the video frames (considering the apparent loss in detection), we will replace the unwanted content by new one using a homography mapping procedure. Furthermore, the replacement key points will be tracked into the next frames considering the zoom-in and zoom-out controlling using multiplication of the key point coordinates by the homography matrix between each two consecutive frames. Since the movement of objects in video can disrupt the alignment between frames and correspondingly make the homography matrix calculation erroneous, we use Mask R-CNN algorithm to mask and remove the moving objects from the scene. Accordingly, the replacement will be consistent to the video motion of scene. Such framework is denominated as REP-Model which stands for a replacing model. In addition, we have examined the REP-Model over a large database regarding soccer match videos for removing and replacing the playground billboard contents and the results reveal the discriminative nature of our proposed framework. Furthermore, in order to key out the covered object beneath the new content, we use an unsupervised approach in an adversarial learning set-up by learning object masks with playing a game of cut-and-paste, using a discriminator model to find out whether the covered object has been revealed correctly.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11383-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-6624-7_32,Deep Learning-Based Violence Detection from Videos,Intelligent Data Engineering and Analytics,10.1007/978-981-16-6624-7_32,Springer,2022-01-01,"Violence has been one of the major concerns among human interactions. Violent activities turn out to be worse in public places like parks, halls, stadiums, and many more. The presence of efficient detection algorithms is the need of the hour as unusual events such as fights have been comparatively studied less. The existing system requires manual monitoring of videos from surveillance cameras. Recognition of violent interactions is important to develop automated video monitoring systems. Approaches based on deep Learning promise better results in recognition of images and human actions. Our proposed deep learning-based model uses CNN and LSTM based on DarkNet19 architecture as the pretrained model. The proposed model achieves 95 and 100% accuracy on the benchmark hockey and Peliculas datasets. Unlike most previous works which involve the use of single-frame models (models which do not include temporal features), our model is able to learn temporal features as well as spatial features. The two datasets, namely Hockey Fight and Movie datasets, contain motions of sudden camera recordings and are challenging datasets to work on. It is also observed that different variations in optical flow were taken into consideration for the work in hand. However, the existing system was unable to utilize an efficient pretrained model for the task of violence detection. The proposed approach puts forward a more efficient pretrained model architecture which is less studied, but is very effective than traditional methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-6624-7_32,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-1677-9_64,Studies on Steganography Images and Videos Using Deep Learning Techniques,Innovations in Electrical and Electronic Engineering,10.1007/978-981-19-1677-9_64,Springer,2022-01-01,"Securing information has been very anxious regarding the advancement of data science, technology as well as research fields. To resolve the security issues, information hiding algorithms have developed based on deep learning technique. Steganography refers to the process of hiding information within specific files whereas Steganalysis refers the process of recognition of hidden information within those files. In this chapter, we have reviewed several existing work done by various researchers on steganography videos and images using both machine learning and deep learning algorithms. In addition, we focused on Convolutional Neural Network framework, block diagrams for performing Steganalysis process especially hidden information that referred by several researchers. It has five sections namely image steganography, video steganography, automatic detection, integration of audio and video steganography, and finally state of art using deep learning algorithms have illustrated. This proposed review creates an entire idea on steganography videos and image data hiding based on both ML and DL technique.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-1677-9_64,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00464-021-08336-x,A contextual detector of surgical tools in laparoscopic videos using deep learning,Surgical Endoscopy,10.1007/s00464-021-08336-x,Springer,2022-01-01,"Background The complexity of laparoscopy requires special training and assessment. Analyzing the streaming videos during the surgery can potentially improve surgical education. The tedium and cost of such an analysis can be dramatically reduced using an automated tool detection system, among other things. We propose a new multilabel classifier, called LapTool-Net to detect the presence of surgical tools in each frame of a laparoscopic video. Methods The novelty of LapTool-Net is the exploitation of the correlations among the usage of different tools and, the tools and tasks—i.e., the context of the tools' usage. Towards this goal, the pattern in the co-occurrence of the tools is utilized for designing a decision policy for the multilabel classifier based on a Recurrent Convolutional Neural Network (RCNN), which is trained in an end-to-end manner. In the post-processing step, the predictions are corrected by modeling the long-term tasks’ order with an RNN. Results LapTool-Net was trained using publicly available datasets of laparoscopic cholecystectomy, viz., M2CAI16 and Cholec80. For M2CAI16, our exact match accuracies (when all the tools in one frame are predicted correctly) in online and offline modes were 80.95% and 81.84% with per-class F1-score of 88.29% and 90.53%. For Cholec80, the accuracies were 85.77% and 91.92% with F1-scores if 93.10% and 96.11% for online and offline, respectively. Conclusions The results show LapTool-Net outperformed state-of-the-art methods significantly, even while using fewer training samples and a shallower architecture. Our context-aware model does not require expert’s domain-specific knowledge, and the simple architecture can potentially improve all existing methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00464-021-08336-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0619-0_15,Violence Detection in Videos Using Deep Learning: A Survey,Advances in Information Communication Technology and Computing,10.1007/978-981-19-0619-0_15,Springer,2022-01-01,"There is a significant need of intelligent surveillance systems to monitor people and recognize their violent behavior at public environments (like banks, hospitals, market centers, and railway stations, and so on). Therefore, violent activity recognition becomes an emerging topic among the researchers in the field of computer vision. Effective and successful approaches for identification of violence in videos are extremely needed for safety concerns. Over the past few years, several techniques based on handcrafted and deep learning features have been introduced for detection of these activities. This paper commences with common framework for violence recognition followed by review on different deep learning techniques and methods based on convolution neural network (CNN) and long short-term memory networks (LSTM) used for violence detection. Besides that, popular datasets and challenges of this topic are also discussed in this review. The findings of the research have been also discussed which may help to find future work in this domain.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0619-0_15,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-7618-5_20,Deep Learning-Based Sentiment Analysis on COVID-19 News Videos,Proceedings of International Conference on Information Technology and Applications,10.1007/978-981-16-7618-5_20,Springer,2022-01-01,"Coronavirus disease (COVID-19) has adversely affected all walks of human life. The whole world is confronting this deadly virus, and no country in this world remains untouched during this pandemic. There are several online news videos related to COVID-19 that are shared on various online platforms such as YouTube, DailyMotion, and Vimeo. There were several arguments on the genuineness of the contents, people watch them, share them, and most importantly express their views and opinions as comments on those platforms. Analyzing these comments can unearth the patterns hidden in them to study people's responses to videos on COVID-19. This paper proposes a deep learning-based sentiment analysis approach to people's response toward online COVID-19 video news. This work implements different deep learning approaches such as LSTM, Bi-LSTM, CNN, and GRU to classify sentiment from the comments collected from YouTube.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7618-5_20,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-96802-1_1,IoHCT: Internet of Cultural Heritage Things Digital Twins for Conservation and Health Monitoring of Cultural in the Age of Digital Transformation,Digital Twins for Digital Transformation: Innovation in Industry,10.1007/978-3-030-96802-1_1,Springer,2022-01-01,"The Building Information Modeling (BIM) technique is gaining traction and has many applications, including asset management and new construction facilities. It has recently been used to preserve constructed heritage as part of the so-called Historical BIM (HBIM) field. A BIM model powered by Digital Twins (DT) is an ideal instrument for monitoring and inferring the behavior, deterioration of heritage structures, performance, collecting and classifying varied data that can co-exist in the model of an asset for artifact preservation. The value of the first original copy is directly proportional to the quality of the model multiplied by the intrinsic value of the original, if and only if the first original can be identified and validated. This paper emphasizes the necessity to explore the importance of heritage assets in the HBIM process and discuss a new framework integrating HBIM, DT, and blockchain technology to provide a more efficient and effective preventive conservation. On the other hand, digital copies are subject to further reproductions, and therefore, the value of an exact copy can never be considered equivalent to its original. So, a blockchain approach is suggested to credit, Identifying and authenticate the first original copy. Problems, challenges, and future trends have been proposed and presented.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96802-1_1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-95711-7_5,Comparison of Modelling ASR System with Different Features Extraction Methods Using Sequential Model,Artificial Intelligence and Speech Technology,10.1007/978-3-030-95711-7_5,Springer,2022-01-01,"Speech recognition refers to a device’s ability to respond to spoken instructions. Speech recognition facilitates hands-free use of various gadgets and appliances (a godsend for many incapacitated persons), as well as supplying input for automatic translation and ready-to-print dictation. Many industries, including healthcare, military telecommunications, and personal computing, use speech recognition programmes. In our paper, we are including the comparison between the different feature extraction methods (BFCC, GFCC, MFCC, MFCC Delta, MFCC Double Delta, LFCC and NGCC) using neural networks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95711-7_5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-7657-4_62,Comparative Study of Robust Feature Extraction Techniques for ASR for Limited Resource Hindi Language,Proceedings of Second International Conference on Sustainable Expert Systems,10.1007/978-981-16-7657-4_62,Springer,2022-01-01,"Hindi is the most spoken language in India. It is desirable to have a communication system in a local language which permits ordinary people to communicate with machines via speech interface to retrieve information or to perform their daily activities. It is observed that conventional automatic speech recognition systems which use the Mel frequency cepstral coefficient or perceptual linear prediction as features do not perform well in noisy environments. Here, we have studied different feature extraction techniques in noisy environments, and experiment results show that gammatone frequency cepstral coefficients perform well in noisy environments. We have used DNN-HMM with a trigram language model to recognize Hindi speech. Compared to other approaches, our proposed algorithm outperforms. In comparison to MFCC and PLP characteristics, the basilar-membrane frequency-band cepstral coefficient likewise produces good results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7657-4_62,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-08648-9_58,Augmentative and Alternative Interaction Service with AI Speakers to Access Home IoT Devices and Internet Services for People with Multiple Disabilities,Computers Helping People with Special Needs,10.1007/978-3-031-08648-9_58,Springer,2022-01-01,"With the growing demand for Internet-of-Things (IoT) and internet services for people with multiple disabilities, including communication and motor disabilities, we propose an Augmentative and Alternative Interaction (AAI) service for them. Interacting with IoT devices and internet services can be done by voice-controllable smart (or Artificial Intelligence (AI)) speakers. AI speakers are difficult to use for those people with multiple disabilities. The direct application of Augmentative and Alternative Communication (AAC) to the interaction is ineffective, since AAC mainly focuses on face-to-face communication and the sound produced by Text-to-Speech (TTS) for AAC symbols does not reflect the AI speakers’ speech recognition features. Based on a user survey for AI speaker commands, we developed AAI symbols and boards to interact with home IoT devices and internet services through AI speakers. In order to improve speech recognition performance of AI speakers, we developed the TTS production format for AAI service, including wake-up words, silence intervals and commands. The AAI service allows people with multiple disabilities to control home IoT devices by themselves and to enjoy their spare time on their own by interacting with internet services such as music streaming, Over-the-Top (OTT) media services, and news searching through AI speakers.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-08648-9_58,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0604-6_11,Speech Age Estimation Using a Ranking Convolutional Neural Network,Proceedings of International Conference on Computing and Communication Networks,10.1007/978-981-19-0604-6_11,Springer,2022-01-01,"The age of 18 has been chosen as the legal age to enter many sites, receive any service or to get some license. Since age has a huge effect on the human being voice, many researchers have worked on automatic age estimation (AAE) in speech analysis. Through this work, a new approach has been designed to estimate the age of the human being depending on his speech. This work has regarded common voice dataset in its experiments with 60 different languages and seven age limits. The features depended were the smoothness and pitch features for their strong capability in recognizing the human voice frequency properties that have a strong relationship with human age. The chi square feature selection was utilized in this work. A ranking convolutional neural network (CNN) was used to calculate the performance of the designed approach. The results gained through this work outperformed the results gained through the state of the art in the field of age recognition. The highest accuracy age estimation was 87.97%, gained through the common voice dataset, testing both genders and all age limits.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0604-6_11,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-97874-7_94,3D Human Body Action Identification of Sports Based on Artificial Intelligence,Cyber Security Intelligence and Analytics,10.1007/978-3-030-97874-7_94,Springer,2022-01-01,"With the continuous emplacement of modern life, people will be busy working and ignore physical exercise, which leads to a decline in the corners of physical health. The best way to solve this problem is to develop an easy-to-install, low-cost physical education system, which is very important for people to achieve better exercises in the project. The quality of this system is very important. A complete set of large-scale teaching systems include human motion identification, human movement 3D reconstruction and human activity analysis. Through a large number of literature reading and experimental investigations, the status quo of human body action identification research is summarized, introducing the connotation of artificial intelligence and human motion recognition; in order to in depth, the human body recognition of three-dimensional body action based on artificial intelligence Action recognition, this paper uses the artificial intelligent adaptation algorithm related data, and then the flowchart of the human body action identification algorithm identified by artificial intelligence sports teaching, and the flow of different modules in this process is accurately tested. The experimental results show that the accuracy of all module data operation in the process of the human body action identification algorithm for sports teaching is high, and it is suitable for the three-dimensional body action identification of sports teaching.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-97874-7_94,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-05237-8_36,Application of Artificial Intelligence Technology in the Field of Livestreaming E-Commerce,Application of Intelligent Systems in Multi-modal Information Analytics,10.1007/978-3-031-05237-8_36,Springer,2022-01-01,"Today, the network as a new way of social gradually changing people’s way of life, the development of mobile Internet platforms for social networking and technical support, live electrical business has gradually become the current hot way of online shopping, whether in the traditional shopping electric business platform, or in some hot short video platform, Livestreaming e-commerce has entered a period of wild and savage growth. On the other hand, with the rapid development of science and technology and economy, artificial intelligence technology is also changing people’s living and consumption habits, especially in the field of e-commerce. With its strong technical advantages, ARTIFICIAL intelligence technology provides strong technical support for the development of livestreaming e-commerce. Based on the background, the paper focuses on the application of artificial intelligence technology in the field of livestreaming e-commerce, starting with the concept of artificial intelligence technology and livestreaming e-commerce.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-05237-8_36,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-0739-4_75,Human Anomaly Detection in Surveillance Videos: A Review,Information and Communication Technology for Competitive Strategies (ICTCS 2020),10.1007/978-981-16-0739-4_75,Springer,2022-01-01,"This paper presents an exhaustive survey on different approaches for human anomaly detection which has most significance in the field of computer vision due to its many applications in real life. Human anomaly detection is a pressing problem in surveillance video system. We have also presented various feature extraction techniques, classification, clustering, deep learning, and transfer learning models, frameworks used in human anomaly detection area, various kinds of standard datasets which can be used for benchmarking the algorithms, challenges, and applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-0739-4_75,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-85540-6_70,Between 3D Models and 3D Printers. Human- and AI-Based Methods Used in Additive Manufacturing Suitability Evaluations,"Human Interaction, Emerging Technologies and Future Systems V",10.1007/978-3-030-85540-6_70,Springer,2022-01-01,"This paper presents the Additive Manufacturing (AM) evaluation methods and methodologies. A comparative analysis is conducted in order to categorize the methods according to different criteria. The comparison describes various approaches, along with their objectives and requirements. The emphasis is put on the aspects of automation and machine learning in the context of AM suitability evaluation. The aim of the article is to offer a high-level reference point for researchers who verify the potential of AM in the context of their studies or business activities. The comparison should facilitate the choice of an optimal, applicable method for identifying AM potential in a specific scenario. Additionally, the analysis offers an insight into the trends of the AM potential analysis methods, evaluating the role of AI and other aspects of Industry 4.0 in the field.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85540-6_70,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-981-19-2531-3_7,Virtual World Under AI: Augmented Reality and Deep Synthesis,AI Ethics and Governance,10.1007/978-981-19-2531-3_7,Springer,2022-01-01,"The relationship between people and digital information has been changing all the time. On the one hand, people can obtain an enormous amount of information from texts, images and videos in various ways. On the other hand, from keyboard, mouse, touch screen to voice, the interactive modes of information are getting closer to the five natural senses of the human being. The popularity of the virtual world will promote the evolution of the relationship once again. The presentation of information will move toward the integration of virtual and real information, and the interaction of information will step toward a totally new form driven by technologies such as image recognition and gesture recognition.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-2531-3_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-08473-7_2,Convolutional Graph Neural Networks for Hate Speech Detection in Data-Poor Settings,Natural Language Processing and Information Systems,10.1007/978-3-031-08473-7_2,Springer,2022-01-01,"Hate speech detection has received a lot of attention in recent years. However, there are still a number of challenges to monitor hateful content in social media, especially in scenarios with few data. In this paper we propose HaGNN, a convolutional graph neural network that is capable of performing an accurate text classification in a supervised way with a small amount of labeled data. Moreover, we propose Similarity Penalty, a novel loss function that considers the similarity among nodes in the graph to improve the final classification. Particularly, our goal is to overcome hate speech detection in data-poor settings. As a result we found that our model is more stable than other state-of-the-art deep learning models with few data in the considered datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-08473-7_2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11387-w,Reinforcement learning based energy efficient protocol for wireless multimedia sensor networks,Multimedia Tools and Applications,10.1007/s11042-021-11387-w,Springer,2022-01-01,"With the advancements in sensor networks, Wireless multimedia sensor networks (WMSNs) have emerged and shifted the objectives of sensor nodes to multimedia devices which can retrieve audio, images, and video. In WMSNs, the sensor nodes are tiny microphones and cameras which can transmit image, audio or video using the network. However, these nodes are battery constrained (i.e., may become dead after passing certain iterations). Therefore, improvement of the network lifetime is a challenging issue of WMSNs. In this paper, a reinforcement-based energy-aware protocol is designed and implemented. To successfully implement the reinforcement-based protocol, a State-Action-Reward-State-Action (SARSA) is used for learning a Markov decision process. Extensive experiments are considered to evaluate the significant improvement of the proposed protocol. Comparisons are also drawn between the competitive protocols and the proposed protocol. From comparative analysis, it is found that the proposed protocol conserves more energy as compared to the competitive protocols.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11387-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-981-19-0568-1_18,"The Future of Interaction: Augmented Reality, Holography and Artificial Intelligence in Early Childhood Science Education","STEM, Robotics, Mobile Apps in Early Childhood and Primary Education",10.1007/978-981-19-0568-1_18,Springer,2022-01-01,"There is little doubt that the development of technology has changed the landscape of science learning in both formal and informal settings. However, often existing research studies lack a strong conceptual underpinning in terms of pedagogic theory. Regardless of the fair body of studies relating to early childhood education and science education, early childhood science learning remains a relatively under-researched area. As representatives of advanced technologies which have been widely adopted in many fields, augmented reality (AR), holography and artificial intelligence (AI) have rarely been applied and studied in early childhood science education despite the enormous potential they offer. Drawing upon Vygotsky’s notions of the zone of proximal development (ZPD), tools and mediation, this chapter provides a new perspective by exploring the potential use of AR applications (apps), holography and AI-based tools in early childhood science education. The key argument is that these tools can potentially change the nature of the interaction between learners and learning materials, and they offer significant affordances in early childhood science education. The mission of the present chapter is to inform the design and development of educational technology based on psychological and pedagogical perspectives, and help parents and early childhood teachers understand the potential use of AR, holography and AI in science education.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0568-1_18,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8759-4_48,Universal Convolutional Neural Network for Recognition of Traffic Lights and Road Signs in Video Frames,SMART Automatics and Energy,10.1007/978-981-16-8759-4_48,Springer,2022-01-01,"The paper proposes a universal architecture of a convolutional neural network designed to recognize road signs and traffic lights on a video frame. The main aim was to simultaneously recognize objects of different classes, such as road signs and traffic lights of different types, by one neural network at the same processing step simultaneously. The neural network implementation was based on Keras and TensorFlow for classification and OpenCV to highlight the contours of possible road signs and traffic lights on video frames. By processing video recordings, datasets were prepared containing images of road signs and traffic lights that are used in Russia. On the prepared datasets, a study was carried out and the accuracy of object recognition by video frame was calculated: the recognition accuracy of road signs was 94.4% and of traffic lights was 95.2%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8759-4_48,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-99200-2_13,Distributed Deep Reinforcement Learning Based Mode Selection and Resource Allocation for VR Transmission in Edge Networks,Communications and Networking,10.1007/978-3-030-99200-2_13,Springer,2022-01-01,"Wireless virtual reality (VR) is expected to be one of the most pivotal applications in 5G and beyond, which provides an immersive experience and will greatly renovate the way people communicate. However, the challenges of VR service transmission to provide high quality of experience (QoE) and a huge data rate remain unsolved. In this paper, we formulate an optimization of the mode selection and resource allocation to maximize the QoE of VR users, aiming at the optimal transmission of VR service based on the cloud-edge-end architecture. Moreover, a distributed game theory based deep reinforcement learning (DGTB-DRL) algorithm is proposed to solve the problem, which can achieve a Nash equilibrium (NE) rapidly. The simulation results demonstrate that the proposed method can achieve better performance in terms of training efficiency, QoE utility values.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-99200-2_13,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-99108-1_7,About Perfection of Digital Twin Models,"Service Oriented, Holonic and Multi-agent Manufacturing Systems for Industry of the Future",10.1007/978-3-030-99108-1_7,Springer,2022-01-01,"Modelling and real-time control methods continue to face major challenges like the lack of models capable of accurately replicating the physical systems while integrating real-time manufacturing data. In this context, the digital twin has widely emerged to address this challenge to connect the physical and digital worlds. The interdependent combination of digital model and physical assets must be harmonized with one another to work efficiently. For this reason, the virtual model needs to be thoroughly evaluated and validated before implementing it in the general framework. In this paper, we discuss the different modelling methods for the virtual model and the various approaches to validate its use in literature.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-99108-1_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9268-0_30,Image Processing: Impact of Train and Test Sizes on Custom Image Recognition Algorithms,Marketing and Smart Technologies,10.1007/978-981-16-9268-0_30,Springer,2022-01-01,"This paper intends to demonstrate results on applying machine learning algorithms to process image recognition to identify professions. This kind of project points us to a relation between humans and machines, so in a way, we might say that the human brain and vision process are being passed to a machine in order to bring us many benefits in our daily life. In this paper, we decided to compare how different parameters influence the performance and accuracy of the following neural networks: EfficientNetB0, NASNetMobile, MobileNetV2, ResNet50, InceptionV3, and DenseNet121.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9268-0_30,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-07012-9_6,Using a Technique Based on Moment of Inertia About an Axis for the Recognition of Handwritten Digit in Devanagri Script,Emerging Technologies in Computer Engineering: Cognitive Computing and Intelligent IoT,10.1007/978-3-031-07012-9_6,Springer,2022-01-01,"A neural network for handwritten digit recognition is dependent on the dataset used to train it. Most of the techniques available in the literature recognize the handwritten characters of any script based upon the moment of inertia of the character uses a central point to it and suffers from certain drawbacks. In this paper, we develop methods that can be applied to any neural network, to make the dataset insensitive to brush size and insensitive to rotation applied to the digit based on the moment of inertia that the digit produces about an axis. All images in the dataset undergo pre-processing. First, the image is made independent of brush size. Then, the image is rotated to negate the rotation that is applied to the image according to the axis of the least moment of inertia. This pre-processing makes the dataset independent of brush size and rotation. The results show that when this preprocessing technique is applied to neural a network, it does makes it insensitive to rotation and brush size.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-07012-9_6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-3440-7_7,Digital Twin in Healthcare Through the Eyes of the Vitruvian Man,Innovation in Medicine and Healthcare,10.1007/978-981-19-3440-7_7,Springer,2022-01-01,"In recent years, worldwide, with the development of technology, a huge amount of data is collected in Electronic Health Records (EHRs). Although vast progress has been made with the use of artificial intelligence in various areas of health domain and for specific problems, it is a fact that to date there is no holistic approach to a patient’s state of health using these technologies. Digital Twin refers to a complete physical and functional description of an item, product, or system, which includes pretty much all the information that could be useful in all—current and next—life cycle phases. This paper presents a platform that, using state of the art technologies such as Microservice Architecture (MSA), containerization (Docker), orchestration (Kubernetes) and Machine Learning Operations (MLOps), whereas it is inspired by Leonardo DaVinci’s Vitruvian man, building the Digital Twin of Patient platform. To achieve that, the platform’s architecture is designed with multiple clusters of Docker containers and Kubernetes orchestration. Specific parts or organs of the human body, are represented by clusters called “digital_twin_components”—DTCs. The set of those DTCs structure the “patient_digital_twin” cluster in which appropriate pipelines define and monitor in real time the “best” possible construction of the patient’s digital twin.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-3440-7_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-0739-4_70,Voice-Based Gender Recognition Using Neural Network,Information and Communication Technology for Competitive Strategies (ICTCS 2020),10.1007/978-981-16-0739-4_70,Springer,2022-01-01,"The human speech contains paralinguistic information used in many speech recognition applications like automatic speech recognition, speaker recognition, and verification. Gender from voice is considered as one of the essential tasks to be detected for such applications. To build a model from a training set, a set of relevant speech features is extracted in order to distinguish gender (i.e., female or male) from a speech signal. This paper focuses on comparison of the proposed neural network (NN) model with the different features like MFCC and mel spectrogram extracted from the speech signal to recognize the gender. Experiments are carried on Mozilla voice dataset and evaluated performance of the network. Experiments show that the combination of MFCC and mel feature sets shows the better accuracy with 94.32%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-0739-4_70,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-97874-7_89,Neural Network Algorithm in the Design of Intelligent English Teaching System,Cyber Security Intelligence and Analytics,10.1007/978-3-030-97874-7_89,Springer,2022-01-01,"At present, the content of foreign language teaching in our country is mainly realized through classroom teaching. However, due to the constraints of traditional teaching concepts and teaching conditions, the current foreign language acquisition is mainly achieved through classroom teaching without a language environment, and it is difficult for students to understand English well. This article adopts the current popular B/S3 layer model design. Compared with the traditional C/S structure, it is easy to expand, the development cycle is short, and the transplantation is convenient, and there is no need to install the client. On the sample set of students’ English learning characteristics, the application experiment of the LCAM model is carried out in two steps. The first step is to train the neural network with the IPSOBP algorithm through the training set; the second step is to test and evaluate the effect of the diagnostic model through the test set. When using the IPSOBP algorithm for experiments, three different particle numbers were selected, respectively, 5, 10, and 20, in order to be able to compare and analyze the operating efficiency of the algorithm under different particle numbers. The proportion of time used in the introduction and review and consolidation links is relatively high, accounting for 36.36% and 30.98% of the total time of the corresponding links respectively. The results show that the neural network algorithm can improve the stability of the intelligent English teaching system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-97874-7_89,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-05643-7_36,Speech Disorders Classification by CNN in Phonetic E-Learning System,Artificial Intelligence in HCI,10.1007/978-3-031-05643-7_36,Springer,2022-01-01,"Speech disorders may affect the process of phonetic transcriptions. In the Automated Phonetic Transcription-the grading tool (APTgt), a linguistic E-learning system, to reduce the influence of disordered speech in the phonetic exams, we proposed a speech disorders classification module that aims to classify disordered speech and non-disordered speech. The Mel-frequency cepstral coefficients (MFCCs) are utilized to represent the features of the speech sound files. With the two different formats of MFCCs, we adopted two approaches to classifying the MFCCs: calculating the similarity between MFCC values by dynamic time warping (DTW) algorithm and classifying the distances by support vector machine (SVM); directly image classification by the convolutional neural network (CNN). We will focus on the second approach in this paper.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-05643-7_36,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-82199-9_38,"Creating a Robot Brain with Object Recognition Using Vocal Control, Text-to-Speech Support and a Simple Webcam",Intelligent Systems and Applications,10.1007/978-3-030-82199-9_38,Springer,2022-01-01,"This paper presents the creation of a robot brain from a computing device. The robot brain is software which can recognize an object based on the taken picture. The system is a neural network which was previously trained with images and the decision is made by comparing a similar known picture to the actual picture of the object which needs to be recognized. The images from the neural network have name associated. The name of most similar picture from the neural network is associated with the newly taken picture and that will be the name of the recognized object. The system can be voice controlled and reads the recognized word loud with text-to-speech software. This way the user can talk to the system to recognize an object and say it loud what it is. The software can be used by children, who are learning objects' name, by blind people who would like to know what specific object they have in their hand or by anyone who has a strange object in hand and does not know what it is.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-82199-9_38,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06767-9_36,Video-Based Pedestrian Re-identification with Non-local Attention Module,Advances in Artificial Intelligence and Security,10.1007/978-3-031-06767-9_36,Springer,2022-01-01,"This paper studies a video-based person re-identification (Re-ID) model with non-local attention module. Firstly, on the basis of the residual module embedded in the 3D convolutional neural network, an non-local attention module is added. This module can associate the long-distance information among video frames, and establish connections between pixels at a certain distance, and enrich the pedestrian feature representations from local and global aspects; Then, many experiments on two public video-based datasets Mars and DukeMTMC-VideoREID proves that our proposed method is competitive with some recent video-based Re-ID methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06767-9_36,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-98385-7_6,Three Uses of One Neural Network: Automatic Segmentation of Kidney Tumor and Cysts Based on 3D U-Net,Kidney and Kidney Tumor Segmentation,10.1007/978-3-030-98385-7_6,Springer,2022-01-01,"Medical image processing plays an increasingly important role in clinical diagnosis and treatment. Using the results of kidney CT image segmentation for three-dimensional reconstruction is an intuitive and accurate method for diagnosis. In this paper, we propose a three-step automatic segmentation method for kidney, tumors and cysts, including roughly segmenting the kidney and tumor from low-resolution CT, locating each kidney and fine segmenting the kidney, and finally extracting the tumor and cyst from the segmented kidney. The results show that the average dice of our method for kidney, tumor and cysts is about 0.93, 0.57, 0.73.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98385-7_6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-3-030-87132-1_10,Human Fall Detection in Depth-Videos Using Temporal Templates and Convolutional Neural Networks,Advances in Assistive Technologies,10.1007/978-3-030-87132-1_10,Springer,2022-01-01,"The task of Human Fall detection is taken up to develop efficient technology to assist humans in activities of their daily living. In homes, the aged people are more prone to fall with improper gait, due to weakened muscles, lack of balance, sensation and co-ordination. The toddlers frequently fall down as the above abilities are not yet well developed in them. The elderly people and the toddlers need constant watch in homes by a system to alert the care-takers immediately after a fall, that makes our homes smart and safe. This research addresses this problem. In this work, we use motion information obtained from video to compute new temporal templates, which are in turn used by a convolutional neural network to recognize the human actions. A new representation capturing the pose of the subject over a period of time is proposed for human action recognition. This temporal template representation computed at the beginning and at the end of the fall-event is explored in this work for fall detection. The ConvNet feature extracted from these temporal templates are used by an extreme learning machine (ELM) for action recognition. The efficacy of the proposed approach is demonstrated on SDU Fall detection, UP-Fall, UR Fall, and MIVIA action datasets.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-87132-1_10,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter,doi:10.1007/978-981-16-5644-6_14,The Effects of Industry 4.0 Components on the Tourism Sector,Logistics 4.0 and Future of Supply Chains,10.1007/978-981-16-5644-6_14,Springer,2022-01-01,"The industrial revolution that emerged in Europe in the 1800s has affected all sectors by spreading to different continents with the use of different technologies Technology over time. Developments in technology Technology have enabled the tourism sector to develop by affecting both production and consumption dimensions Dimension . In addition to the factors that improve the tourism supply, such as transportation and communication technologies Technology , the increase in commercial Commercial activities with the ease of production and supply provided by mass production, and the expansion of the tourism market Market with the increase of travel, technologies Technology that can be closely followed digitalized tourism market Market , allowing customized marketing Marketing , have revealed the demand for tourism. In this section, industrial revolutions and their effects on the tourism sector are evaluated under the titles of Industry 4.0 Industry 4.0 components such as big data Big Data analytics, augmented reality applications, internet İnternet of things and artificial intelligence Artificial Intelligence (AI) .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5644-6_14,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-98438-0_17,A Blueprint for an AI & AR-Based Eye Tracking System to Train Cardiology Professionals Better Interpret Electrocardiograms,Persuasive Technology,10.1007/978-3-030-98438-0_17,Springer,2022-01-01,"The electrocardiogram is one of the most used medical tests worldwide. Despite its prevalent use in the healthcare sector, there exists a limited understanding in how medical practitioners interpret it. This is mainly due to the scarcity of international guidelines that unify its interpretation across different health institutions. This leads to a lack of training and unpreparedness by medical students who are about to join the medical workforce. In this paper, we propose a blueprint for a proactive artificial intelligence and augmented reality-based eye tracking system to train cardiology professionals for a better electrocardiogram interpretation. The proposed blueprint is inspired from extensive interviews with cardiology medical practitioners as well as students who interpret electrocardiograms as part of their daily practice. The interviews contributed to identifying the major pain-points within the process of electrocardiogram interpretation. The interviews were also critical in conceptualizing the persuasive components of the training system for a guided correct electrocardiogram interpretation. Throughout the presented blueprint, we detail the three components that constitute the system. These are the augmented reality-based interactive training interface, the artificial intelligence-based processing sub-system, and finally the adaptive electrocardiogram dataset.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98438-0_17,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-6309-3_63,Abnormal Activity Detection Using Deep Learning,Intelligent Sustainable Systems,10.1007/978-981-16-6309-3_63,Springer,2022-01-01,"Identifying abnormal activity is a laborious job, and this has led to the advancement in the domain of deep learning for surveillance which assured performance gain. Abnormal detection plays a pivotal role in the field of research and application systems. Detecting real-world problems such as accidents, burglary, explosion, fighting, robbery and other critical events is crucial, so we developed a deep learning-based algorithm to reduce manual work and time. To provide an optimal solution for the same, we have designed a model that detects abnormal events. To achieve this objective, we have implemented two approaches. The first approach is the model that we have trained (auto-encoder), and the second approach is the pre-trained model (C3D feature extraction). In approach 1, we have introduced a spatiotemporal auto-encoder, which is based on a 3D convolution neural network. After training the model with auto-encoder, our model has reconstructed the frames with a minimum loss of 8% and a maximum accuracy of 71%. To improve accuracy and to reduce loss, we have exercised approach 2 (C3D extraction) that computes features from the fully connected layers of the C3D which resulted in increased accuracy. To detect the abnormal activity in banks, roads and many more crowded areas, this can be implemented through CCTV cameras to automate and simplify manual work.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-6309-3_63,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-91581-0_30,Research on the Applicability of Monocular 3d Object Detection Using CARLA Simulator,"Advances in Neural Computation, Machine Learning, and Cognitive Research V",10.1007/978-3-030-91581-0_30,Springer,2022-01-01,"Monocular 3d object detection methods are promising in the field of making autonomous robots without lidar, which can reduce costs of production significantly. However monocular 3d object detection methods tend to have low precision due to inaccurate inference of distances to objects. Nevertheless, there are several ways to measure the impact of detection precision on the downstream autonomous driving task. In this work, autonomous agents which use lidar, monocular camera, and ground truth for 3d object detection are compared in the CARLA simulator. Each agent has passed a set of routes with challenging traffic situations, totaling 122.5 km driven. Quality of movement was assessed using the collisions statistics, as a result, the agent using a monocular camera performed 4.5% better than the agent using lidar. This indicates the applicability of monocular 3d object detection algorithms in certain cases.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-91581-0_30,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-19-0745-6_63,Classification and Area Computation Modelling of Remote Sensing Images Using Histogram and Convolutional Neural Network,Topical Drifts in Intelligent Computing,10.1007/978-981-19-0745-6_63,Springer,2022-01-01,"Remote sensing is an important field in science and technology and consists of the images of the Earth taken by the means of artificial satellites or aircraft. Satellite images or high-resolution aerial images are flexible to work with and easy to monitor. Since the total area of the earth is so large, high-resolution remote sensing images produce vast amount of data, even image processing is time consuming. This project represents a combination of unsupervised and supervised process to classify high spatial resolution satellite images so that minimal human intervention is needed. For this purpose, histogram peak-based classification approach is used to classify remote sensing image into subcategories like urban land, vegetation land, water body, etc. To detect different objects, present in the image, convolutional neural network-based approach is used. The neural network model is trained using custom dataset. Then, object localization operation is performed to get the coordinates of the object present in the image. Then, histogram-based segmentation operation is performed to compute the area of different objects present in the image. After that 3D model is constructed using the coordinates obtained. Georeferencing technique is used to calculate the area of different classes observed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-19-0745-6_63,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-7985-8_16,POS Tagger Model for South Indian Language Using a Deep Learning Approach,ICCCE 2021,10.1007/978-981-16-7985-8_16,Springer,2022-01-01,"Computational Linguistics is necessary for understanding the language which brings human being with an insight into thinking and intelligence. It is an inspiring research concept in Natural Language Processing (NLP) domain. Parts of Speech (POS) labeling is a very crucial phase in NLP, since based on this, most of the other tasks like Syntax parsing, Semantic parsing, Sentiment analysis, Sentence level classification etc. are carried out. This paper presents a POS tagger model developed on Kannada texts which is one of the South Indian Languages by employing deep neural network methodology. The deep learning model adopted in this work has a combination of word embedding and Recurrent Neural Network (RNN) along with Long-Short memory (LSTM) techniques. The total size of the dataset used during this implementation is 10,000 annotated Kannada sentences (190,000 Kannada words) comprising of five different domains like Agriculture, Sports, Literature, Tourism and Science and Technology. Most of the sentences from the available dataset are compound and ranged up to 10–11 words. The dataset is taken from Technology Development for Indian Languages (TDIL) website. It has been divided into 8000 Kannada sentences as a training dataset and 2,000 Kannada sentences as a test dataset. The BIS (Bureau of Indian Standards) tagset is adopted for POS tags, in which we considered 27 prime POS tags. An average accuracy of POS tagging on an unseen dataset obtained from the trained POS tagger model is 81%. The results are extended with plotted graphs and shown in this paper.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7985-8_16,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-93780-5_10,Training Social Skills in Virtual Reality Machine Learning as a Process of Co-Creation,"Disruptive Technologies in Media, Arts and Design",10.1007/978-3-030-93780-5_10,Springer,2022-01-01,"The training of social skills in interactive Virtual Reality scenes has become an issue of rising interest in research and development. This is also the case of the project Virtual Skills Lab that deals with the question how the potential of such a training can be applied to the context of organizations. In a virtual office scene, the interaction between the player and the computer-generated agent is based on speech recognition and conversational Artificial Intelligence (AI). While studies and experiments have shown that humans are able to treat agents in Virtual Reality as if they were real persons, it is still impossible to establish truly spontaneous forms of interaction by combining speech recognition, conversational AI and Virtual Reality. Hence, existing applications for social skills training in Virtual Reality focus on the possibility to literally take over the other’s perspective or the ability to overcome social anxieties. Yet, social interaction in the workplace is always embedded in a given organizational culture and therefore characterized by behavioral patterns. In this sense, a Virtual Reality training tool for social interaction in the workplace can be enriched by design elements that enable machine learning in terms of pattern recognition. The paper outlines how the project team has dealt with the existing limitations of conversational AI in the process of working out the storyline and what design elements can be introduced in the Virtual Reality interaction in order to enable the machinic recognition of patterns of organizational culture.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-93780-5_10,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-4943-1_34,Convolutional Encoder–Decoder Architecture for Speech Enhancement,Proceedings of International Conference on Power Electronics and Renewable Energy Systems,10.1007/978-981-16-4943-1_34,Springer,2022-01-01,"Signal processing faces the quandary of not being able to separate non-stationary noise from speech signal. Traditional methodologies relied on spectral subtraction for the same; however, such techniques relied on approximation of spectral mask of the noise. This paper proposes an effective and novel convolutional encoder–decoder architecture to effectuate clean speech from the input audio through denoising the audio input. The architecture uses skip connections to increase information flow from encoder to decoder, which helped the authors bolster the performance of the network. The generated output is evaluated on objective and subjective metrics like signal-to-noise ratio (SDR), perceptual evaluation of speech quality (PESQ) and short time objective intelligibility (STOI). The proposed system outperforms the state-of-the-art systems with respect to SDR, PESQ and STOI. The architecture finds applications in various fields such as speech recognition, machine translation and telecommunication.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-4943-1_34,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-90633-7_49,End-to-End Acoustic Model Using 1D CNN and BLSTM Networks with Focal CTC Loss,Advanced Intelligent Systems for Sustainable Development (AI2SD’2020),10.1007/978-3-030-90633-7_49,Springer,2022-01-01,"With the advancement of modern technologies, the human-machine interaction conveyed towards a more natural means of communication. The frequent example used in the devices is speech, where an Automatic Speech Recognition (ASR) system is prominent to convert the uttered word to text. In this paper, we focus on the acoustic model which translate the relationship between the acoustic features and the phonemes to a probability distribution. To directly output the spoken phonemes from the input features, we introduce an End-to-End acoustic model. The proposed model is built with the combination of the one-dimensional Convolutional Neural Networks (1D CNNs) and Bidirectional Long Short-Term Memory (BLSTM) while using Focal Connectionist Temporal Classification (CTC) loss in the training phase. Experimental results show that the proposed acoustic model reaches up to 20.3% in terms of Phone Error Rate (PER).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-90633-7_49,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-95463-5_3,Investigation of the Directional Characteristics of the Emitted Airborne Sound by Friction Stir Welding for Online Process Monitoring,2nd International Conference on Advanced Joining Processes (AJP 2021),10.1007/978-3-030-95463-5_3,Springer,2022-01-01,"Currently, destructive or non-destructive testing methods are used to verify the weld seam quality subsequent to the manufacturing process. Therefore, pre-processes such as visible or mechanical testing require additional efforts, which can lead to expensive reworking or rejection of the components. The acoustic process characterization for Friction Stir Welding (FSW) applications permits a comparatively new approach of process monitoring to detect weld seam irregularities by the characterization of the emitted noise in the audible frequency range (airborne sound signal). In previous publications, the acoustic detection of weld seam irregularities was mostly based on structure-borne sound sensors. Although a correlation between weld defects and audio signals has been demonstrated, there are process-related deficits in the use of structure-borne sound sensors. These include a fixed installation position and limited applicability for large-scale components such as battery cases. In contrast airborne sound sensors (microphones) can be mounted directly in the area of the joining process and thus influences of component size, joining materials, and weld seam geometry can be reduced. However, the use of airborne sound sensors for FSW applications requires preparatory considerations on the sensor position towards the joining process (sidely, in front of or behind the processing tool). Therefore, in this study an approach will be presented to evaluate the directional characteristic of the airborne sound emitted by the FSW process. First, the positioning of the microphone for the various welding directions were investigated. This was done to determine a suitable microphone orientation during the process. Then, the general determination of audio signals from the FSW process will be considered and compared to the process force feedback. Further, it was demonstrated that acoustic analysis can be used for detection of weld seam irregularities such as flash formation on 5 mm AA 5754 H111 sheets. All experiments were performed with a robotized FSW setup that was modified by a self-developed acoustic measuring device.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95463-5_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-07005-1_14,Harnessing Sustainable Development in Image Recognition Through No-Code AI Applications: A Comparative Analysis,Recent Trends in Image Processing and Pattern Recognition,10.1007/978-3-031-07005-1_14,Springer,2022-01-01,"Artificial intelligence (AI) solutions and sustainable development have increasingly received public attention and research interest. In this study, the authors discuss the emerging trend of no-code AI solutions, with regards to their contribution to achieving the Sustainable Development Goals, specifically goal 8 (decent work and economic growth) and goal 10 (reduced inequalities). To demonstrate the opportunities that no-code AI may facilitate, the authors compare the performance of conventionally coded models with a no-code model created in Microsoft Lobe, based on secondary data from a dataset offered by Kermany et al. [ 1 ] of chest x-rays for pneumonia detection. A total of 5840 JPEG images is used for training and testing, 1575 for normal and 4265 for pneumonia, respectively. Results indicate that the output generated by the studied no-code solution can keep up with coded ones, and partly even outperform them. Possible applications for industries and society include usability cases beyond image recognition, the application in citizen science as well as the exploration of economic development opportunities of no-code AI. Finally, no-code AI could perhaps offer an alternative and emerge as an industry best practice for delivering efficient low-cost solutions in emerging markets, where demographic data is scattered across various homogenous groups.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-07005-1_14,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-6309-3_59,Detection of Alzheimer’s Disease Through Speech Features and Machine Learning Classifiers,Intelligent Sustainable Systems,10.1007/978-981-16-6309-3_59,Springer,2022-01-01,"The most common neurodegenerative disease identified in elderly people is Alzheimer’s disease (AD). In general, identification of Alzheimer’s is carried out based on medical images of the brain. The other non-invasive signal processing techniques for AD detection are reviewed. Possibly, these reviewed techniques can also provide an early diagnosis of AD, which might help delay the development of the disease. These models are based on the spontaneous speech (SS) features extracted, such as vocal, linguistic, acoustic, prosodic, and features. Various classifiers and machine learning algorithms that are feasible in AD detection are described. These are discussed and compared based on their performance accuracies of classification results. Few models developed on repository corpuses and others based on speech data collected from persons of different age groups, affected with AD, having mild cognitive impairment (MCI) and/or healthy control (HC) are presented. The models reviewed may help further research toward the development of a reliable machine assistive technology (MAT) for providing health care to elderly people and exploring cures or solutions to the diseases like AD, PD, vascular dementia, down syndrome, frontotemporal dementia, etc.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-6309-3_59,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-2594-7_20,Impact of Lightweight Machine Learning Models for Speech Emotion Recognition,International Conference on Innovative Computing and Communications,10.1007/978-981-16-2594-7_20,Springer,2022-01-01,"Speech Emotion Recognition (SER) is the operation or series of steps used for the identification of human emotions from verbal expressions. SER is a challenging task, and extensive reliance has been placed on models which use audio features in developing well-performing classifiers. Practitioners rely more on the power of the deep learning models, but even lighter and more interpretable ML models can achieve performance that was achieved by DL-based models with close values of accuracy, f-score, precision, and recall. Lighter machine learning-based models trained over a few handcrafted features, such as pitch, harmonics, speech energy, and pause, are able to achieve a performance comparable to the current deep learning-based state-of-the-art method for emotion recognition. To increase the accuracy of the models, we also used the features such as MFCC in the feature extraction process. In this paper, we implement the lightweight interpretable machine learning models namely, Random Forest, Gradient Boosting, Support Vector Machines, Naive Bayes, and Logistic Regression, for speech emotion detection. The ensembling of these models is done to find out the best combination for better accuracy. We developed an application for improving the customer-care services by classifying and prioritizing the feedback based on the emotions of the voice data.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-2594-7_20,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-4177-0_22,Early Detection of Parkinson’s Disease Through Speech Features and Machine Learning: A Review,ICT with Intelligent Applications,10.1007/978-981-16-4177-0_22,Springer,2022-01-01,"Parkinson’s Disease (PD) is a kind of neurodegenerative disorder. The disease causes communication impairment based on its progression. In general, identification of PD carried out based on medical images of brain. But it was recently identified that voice is acting as biomarkers for several neurological disorders. A review of speech features and machine learning algorithms is presented. This might be helpful for development of a non-invasive signal processing techniques for early detection of PD. Several models developed for disease detection is discussed, which are developed based on features like acoustic, phonation, articulation, dysphonia, etc. Machine learning algorithms like Logistic Regression (LG), Support Vector Machine (SVM), Boosting Regression Tree, bagging Regression, etc., and their performance accuracies in classification of Patient with PD (PWP) and Healthy Controls (HC) are reviewed. All these classification algorithms are trained and tested on several repository corpuses and customized datasets. The Spontaneous Speech (SS) is an efficient tool for the early detection of diseases like Parkinson’s, Alzheimer’s, Autism and several other dementia types in elderly people.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-4177-0_22,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8225-4_27,Improved Hate Speech Detection System Using Multi-layers Hybrid Machine Learning Model,Computer Vision and Robotics,10.1007/978-981-16-8225-4_27,Springer,2022-01-01,"Hate speech and cyber-harassment have been a major concern on the Internet for a long time. Furthermore, social media platforms, particularly Twitter and Facebook, have elevated it to a worldwide platform on which hate speeches can spread much faster. Manually detecting hate speech from social media is a time-consuming process. Hence, several studies are still being conducted in this field. We build a two-layer hybrid machine learning model using existing machine learning algorithms. This hybrid machine learning algorithm is capable of efficiently detecting hate speech from social media texts. The hybrid approach combines nine different machine learning algorithms to make one hybrid machine learning model. Additionally, we used the bag-of-words and TF-IDF techniques with the two-gram approach to extract the features. Significant experiments are carried out on the hate speech dataset. The accuracy gained by the hybrid machine learning model is much higher than that of available conventional machine learning models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8225-4_27,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-6616-2_3,Text to Speech Conversion of Handwritten Kannada Words Using Various Machine Learning Models,Evolution in Computational Intelligence,10.1007/978-981-16-6616-2_3,Springer,2022-01-01,"Recognition of handwritten characters and words is challenging due to the presence of complex character sets and the complexity of the words. The machine learning models with feature extraction methods will help us to solve the problem of recognizing handwritten words. The various preprocessing techniques applied to the word are Bilateral filters, resizing the images to find the Region of Interest (ROI) by contour detection and cropping the images. After resizing the image, it is further deskewed for better results. The recognition of handwritten Kannada words by extracting histogram of oriented gradients (HOG) features from the word image using various Machine Learning (ML) techniques are presented in this paper. Then the recognized word is converted to speech using the Google Text-to-Speech (gTTS) API. The dataset consists of 54,742 handwritten word images. Various machine learning models like Support Vector Machine (SVM), k-nearest neighbors (KNN), and random forest were applied to the dataset. Average accuracy of 88% is obtained using the SVM classifier with Radial Basis Function (RBF) kernel.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-6616-2_3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-03789-4_25,Classifying Biometric Data for Musical Interaction Within Virtual Reality,"Artificial Intelligence in Music, Sound, Art and Design",10.1007/978-3-031-03789-4_25,Springer,2022-01-01,"Since 2015, commercial gestural interfaces have widened accessibility for researchers and artists to use novel Electromyographic (EMG) biometric data. EMG data measures musclar amplitude and allows us to enhance Human-Computer Interaction (HCI) through providing natural gestural interaction with digital media. Virtual Reality (VR) is an immersive technology capable of simulating the real world and abstractions of it. However, current commercial VR technology is not equipped to process and use biometric information. Using biometrics within VR allows for better gestural detailing and use of complex custom gestures, such as those found within instrumental music performance, compared to using optical sensors for gesture recognition in current commercial VR equipment. However, EMG data is complex and machine learning must be used to employ it. This study uses a Myo armband to classify four custom gestures in Wekinator and observe their prediction accuracies and representations (including or omitting signal onset) to compose music within VR. Results show that specific regression and classification models, according to gesture representation type, are the most accurate when classifying four music gestures for advanced music HCI in VR. We apply and record our results, showing that EMG biometrics are promising for future interactive music composition systems in VR.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-03789-4_25,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-08939-8_31,KCUBE: A Knowledge Graph University Curriculum Framework for Student Advising and Career Planning,Blended Learning: Engaging Students in the New Normal Era,10.1007/978-3-031-08939-8_31,Springer,2022-01-01,"Knowledge representations and interactions are at the forefront of teaching, learning, and career planning activities in all endeavors of education and career development. University students are increasingly faced with a myriad of interdisciplinary topics that are seemingly unrelated when unstructured knowledge representations are presented, especially during advising and career orientation sessions. This is especially challenging in fast changing technical domains such as Computer Science and Engineering where university curricula are reviewed on an annual basis. This makes it increasingly difficult for instructors and administrators to present both the big picture as well as the detailed knowledge components of degree programs to students when choosing a career or establish a plan of study and assessment. This paper introduces the KCUBE project, a virtual reality knowledge graph framework for structuring and presenting both the overall view of the Computer Science curriculum taught in the Department of Computing at the Hong Kong Polytechnic University as well as the scheduling alternatives in managing course content and presentation views by instructors and students. We employ computational information storage and retrieval methods, machine learning, and interactive virtual reality to better understand, manipulate, and visualize abstract concepts and relationships in the development of teaching and learning activities in our department.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-08939-8_31,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-05939-1_7,Using Multi-modal Machine Learning for User Behavior Prediction in Simulated Smart Home for Extended Reality,"Virtual, Augmented and Mixed Reality: Design and Development",10.1007/978-3-031-05939-1_7,Springer,2022-01-01,"We propose a multi-modal approach to manipulating smart home devices in a smart home environment simulated in virtual reality. Our multi-modal approach seeks to determine the user’s intent in the form of the user’s target smart home device and the desired action for that device to perform. We do this by examining information from two main modalities: spoken utterance and spatial information (such as gestures, positions, hand interactions, etc.). Our approach makes use of spoken utterance, spatial information, and additional information such as the device’s state to predict the user’s intent. Since the information contained in the user’s utterance and the spatial information can be disjoint or complementary to one another, we process the two sources of information in parallel using multiple machine learning models to determine intent. The results of these models are ensembled to produce our final prediction results. Aside from the proposed approach, we also discuss our prototype and discuss our initial findings.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-05939-1_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-96753-6_12,A Machine Learning Approach for Detecting Traffic Incidents from Video Cameras,Smart Cities,10.1007/978-3-030-96753-6_12,Springer,2022-01-01,"In the area of vehicular traffic analysis, many cities have surveillance devices (cameras) installed in different junctions as well as along roads, to obtain information on how vehicles behave in a certain area. To help in the process of preventing accidents, this article proposes a computer vision pipeline for detecting different types of dangerous/risky driving behavior. The pipeline includes object detection, tracking, speed normalization, and the application of computational intelligence, among other relevant features for pattern detection and traffic behavior analysis. The developed models are applied on videos from traffic cameras from eight different sites in the metropolitan area of the city of Montevideo. Three different algorithms were developed for detecting dangerous incidents in traffic in real time. Accurate results are reported for the case studies addressed in the experimental validation, reaching precision values up to 0.82 and recall values up to 0.91.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-96753-6_12,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12559-021-09845-6,An Ensemble Method for Radicalization and Hate Speech Detection Online Empowered by Sentic Computing,Cognitive Computation,10.1007/s12559-021-09845-6,Springer,2022-01-01,"The dramatic growth of the Web has motivated researchers to extract knowledge from enormous repositories and to exploit the knowledge in myriad applications. In this study, we focus on natural language processing (NLP) and, more concretely, the emerging field of affective computing to explore the automation of understanding human emotions from texts. This paper continues previous efforts to utilize and adapt affective techniques into different areas to gain new insights. This paper proposes two novel feature extraction methods that use the previous sentic computing resources AffectiveSpace and SenticNet. These methods are efficient approaches for extracting affect-aware representations from text. In addition, this paper presents a machine learning framework using an ensemble of different features to improve the overall classification performance. Following the description of this approach, we also study the effects of known feature extraction methods such as TF-IDF and SIMilarity-based sentiment projectiON (SIMON). We perform a thorough evaluation of the proposed features across five different datasets that cover radicalization and hate speech detection tasks. To compare the different approaches fairly, we conducted a statistical test that ranks the studied methods. The obtained results indicate that combining affect-aware features with the studied textual representations effectively improves performance. We also propose a criterion considering both classification performance and computational complexity to select among the different methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12559-021-09845-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8542-2_29,Analysis of Online Toxicity Detection Using Machine Learning Approaches,International Conference on Artificial Intelligence and Sustainable Engineering,10.1007/978-981-16-8542-2_29,Springer,2022-01-01,"Social media and the Internet have become an integral part of how people spread and consume information. Over a period of time, social media evolved dramatically, and almost half of the population is using social media to express their views and opinions. Online hate speech is one of the drawbacks of social media nowadays, which needs to be controlled. In this paper, we will understand how hate speech originated and what are the consequences of it and trends of machine learning algorithms to solve an online hate speech problem. This study contributes by providing a systematic approach to help researchers to identify a new research direction and elucidating the shortcomings of the studies and model, as well as providing future directions to advance the field.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8542-2_29,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-10467-1_38,Reducing Exposure to Hateful Speech Online,Intelligent Computing,10.1007/978-3-031-10467-1_38,Springer,2022-01-01,"It has been observed that regular exposure to hateful content online can reduce levels of empathy in individuals, as well as affect the mental health of targeted groups. Research shows that a significant number of young people fall victim to hateful speech online. Unfortunately, such content is often poorly controlled by online platforms, leaving users to mitigate the problem by themselves. It’s possible that Machine Learning and browser extensions could be used to identify hateful content and assist users in reducing their exposure to hate speech online. A proof-of-concept extension was developed for the Google Chrome web browser, using both a local word blocker and a cloud-based model, to explore how effective browser extensions could be in identifying and managing exposure to hateful speech online. The extension was evaluated by 124 participants regarding the usability and functionality of the extension, to gauge the feasibility of this approach. Users responded positively on the usability of the extension, as well as giving feedback regarding where the proof-of-concept could be improved. The research demonstrates the potential for a browser extension aimed at average users to reduce individuals’ exposure to hateful speech online, using both word blocking and cloud-based Machine Learning techniques.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-10467-1_38,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11614-4,Age group classification and gender recognition from speech with temporal convolutional neural networks,Multimedia Tools and Applications,10.1007/s11042-021-11614-4,Springer,2022-01-01,"This paper analyses the performance of different types of Deep Neural Networks to jointly estimate age and identify gender from speech, to be applied in Interactive Voice Response systems available in call centres. Deep Neural Networks are used, because they have recently demonstrated discriminative and representation capabilities in a wide range of applications, including speech processing problems based on feature extraction and selection. Networks with different sizes are analysed to obtain information on how performance depends on the network architecture and the number of free parameters. The speech corpus used for the experiments is Mozilla’s Common Voice dataset, an open and crowdsourced speech corpus. The results are really good for gender classification, independently of the type of neural network, but improve with the network size. Regarding the classification by age groups, the combination of convolutional neural networks and temporal neural networks seems to be the best option among the analysed, and again, the larger the size of the network, the better the results. The results are promising for use in IVR systems, with the best systems achieving a gender identification error of less than 2% and a classification error by age group of less than 20%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11614-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-7330-6_59,Video-Based Elderly Fall Detection Using Convolutional Neural Networks,"Proceedings of Third International Conference on Intelligent Computing, Information and Control Systems",10.1007/978-981-16-7330-6_59,Springer,2022-01-01,"Fortuitous collapses are a significant concern for people, particularly who are frail owing to agedness, disability, or further impairments. According to World Health Organization data, significant falls kill over 646,000 people per year, with the majority of these deaths occurring in underdeveloped nations, particularly among persons aged 65 and over. The physiological changes of elderly individuals make them weaker, making them more susceptible to black out and collapsing. Consequences of those falls might be devastating, resulting in long-term hospitalization or, even worse, premature death. The fall’s criticality is determined by its impact. It might be a slight or catastrophic injury, but observers have an obligation to assist the sufferer. What happens, though, if no one is available to assist or if the casualties go unreported by society? This is where a fall recognition architecture comes in handy. The detected falls are designed to identify a fall quickly and send a warning to local aid centers or concerned people so that the individuals might be rescued as soon as possible.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-7330-6_59,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-97546-3_53,Video-Based Student Engagement Estimation via Time Convolution Neural Networks for Remote Learning,AI 2021: Advances in Artificial Intelligence,10.1007/978-3-030-97546-3_53,Springer,2022-01-01,"Given the recent outbreak of COVID-19 pandemic globally, most of the schools and universities have adapted many of the learning materials and lectures to be delivered online. As a result, the necessity to have some quantifiable measures of how the students are perceiving and interacting with this ‘new normal’ way of education became inevitable. In this work, we are focusing on the engagement metric which was shown in the literature to be a strong indicator of how students are dealing with the information and the knowledge being presented to them. In this regard, we have proposed a novel data-driven approach based on a special variant of convolutional neural networks that can predict the students’ engagement levels from a video feed of students’ faces. Our proposed framework has achieved a promising mean-squared error (MSE) score of only 0.07 when evaluated on a real dataset of students taking an online course. Moreover, the proposed framework has achieved superior results when compared with two baseline models that are commonly utilised in the literature for tackling this problem.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-97546-3_53,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-98388-8_36,Prototype Design of a Multi-modal AI-Based Web Application for Hateful Content Detection in Social Media Posts,"Sense, Feel, Design",10.1007/978-3-030-98388-8_36,Springer,2022-01-01,"Hate-Speech Detection and filtering of hateful content is an important aspect of any social media post. The ever increasing amount of content posted daily on social media has led to an excessive amount of digital hate being spread in the form of posts, images and comments. The proposed system is developed in order to act as a tool for determining if a particular social media post is hateful and is aimed to aid any benign social media user who has been affected by hate speech and wants to report it. The proposed system uses a multimodal artificial intelligence based approach by classifying different formats of posts, i.e., images and comments or captions separately. An ensemble convolutional neural network architecture is used for this classification, thus, proving to be a strong tool for finding evidence of any prevalent hate speech. This system is tested using the Likert scale for its user interface, accuracy and utility. Based on the result this paper proposes a prototype design of a web application which can be used for hateful content detection.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98388-8_36,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06495-5,"A dual-stream fused neural network for fall detection in multi-camera and 
              
                
              
              $$360^{\circ }$$
              
                
                  360
                  ∘
                
              
             videos",Neural Computing and Applications,10.1007/s00521-021-06495-5,Springer,2022-01-01,"Globally, human falls are the second leading cause of deaths induced due to unintentional injuries. These fatalities, in most cases, arise due to a lack of timely medication. Therefore, over the years, there has been an immense demand for systems that can quickly send fall-related information to the caretakers so that the medical relief team can reach on time. The traditional schemes for fall detection using wearable sensors such as accelerometers, gyroscopes, etc., are highly intrusive and generate high false positives in real-world conditions. Consequently, the current research directions in this domain have been toward harnessing the availability of low-cost vision sensors and the power of deep learning. To this end, in this work, we present a dual-stream fused neural network (DSFNN) for fall detection in multi-camera and $$360^{\circ }$$ 360 ∘ video streams. The DSFNN model learns to extract spatial-temporal information using two neural networks, trained independently on the RGB video sequences of fall and non-fall activities and their corresponding single dynamic images. Once trained, the model fuses the prediction scores of the two neural networks using a weighted fusion scheme to obtain the final decision. We assessed the performance of the proposed DSFNN on two multi-camera fall datasets, namely UP-Fall and URFD, and on a new in-house $$360^{\circ }$$ 360 ∘ video dataset of fall and non-fall activities. The evaluation results in terms of different performance metrics demonstrated the superiority of the proposed fall detection scheme. The framework achieved superior performance and outperformed the previous state-of-the-art fall detection methods. For further research and analysis in the fall detection domain, we will make the source code and the in-house fall dataset available to the research community on request.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06495-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-9480-6_34,Research of an Intelligent System for Face Recognition on Embedded Platforms with Limited Computing Power,Proceedings of 2nd International Conference on Smart Computing and Cyber Security,10.1007/978-981-16-9480-6_34,Springer,2022-01-01,"Today, computer vision technologies are used in many areas of human activity. There are many methods for extracting faces in the original image, the most promising of which is the use of algorithms based on neural networks. The aim of the study was to design and test an intelligent system for face recognition on embedded platforms with limited computing power. In the course of a preliminary analysis of machine learning methods for solving the problem, it was found that it is most expedient to use machine learning methods based on the analysis of facial micro-motions with the subsequent construction of a map of points. Based on the data obtained, a face identification system was developed using computer vision technology, which was based on a method for creating complex architectures using various features with additional algorithms. A distinctive feature of the developed intelligent system is the ability to analyze several frames that confirm micro-movements of the head or blinking. As a result of testing the resulting system using the gradient boosting algorithm for regression trees, a map of 68 face points was obtained, on the basis of which human faces were identified with objects from the database.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-9480-6_34,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-021-02547-4,A temporal attention based appearance model for video object segmentation,Applied Intelligence,10.1007/s10489-021-02547-4,Springer,2022-01-01,"More and more researchers have recently paid attention to video object segmentation because it is an important building block for numerous computer vision applications. Although many algorithms promote its development, there are still some open challenges. Efficient and robust pipelines are needed to address appearance changes and the distraction from similar background objects in the video object segmentation. This paper proposes a novel neural network that integrates a temporal attention based appearance model and a boundary-aware loss. The appearance model fuses the appearance information of the first frame, the previous frame, and the current frame in the feature space, which assists the proposed method to learn a discriminative and robust target representation and avoid the drift problem of traditional propagation schemes. Moreover, the boundary-aware loss is employed for network training. Equipped with the boundary-aware loss, the proposed method achieves more accurate segmentation results with clear boundaries. The proposed method is compared with several recent state-of-the-art algorithms on popular benchmark datasets. Comprehensive experiments show that the proposed method achieves favorable performance with a high frame rate.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02547-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-031-06767-9_46,Find the Unseen Actions: Abnormal Action Recognition,Advances in Artificial Intelligence and Security,10.1007/978-3-031-06767-9_46,Springer,2022-01-01,"Deep neural networks are gaining increasing traction in action recognition. Existing literature are based on a basic assumption that the test sample belongs to a certain category in the training set. However, this assumption is severely limited in real world where unseen (socalled “abnormal”) events may occur. Most existing methods usually fail to identify abnormal actions, which are usually classified as a normal category. To address these issues, we propose abnormal action recognition that effectively detect unpredictable abnormal actions. Specifically, we treat multiple categories of the training set as normal categories, and the test set contains normal and abnormal categories (not belonging to any of the categories in the training set). We study abnormal action recognition from three aspects: 1) The softmax distribution of the classifier; 2) Mahalanobis distance-based confidence score; 3) Classification feature. Finally, we analyze the experiment in detail and discuss the idea of improving the recognition of abnormal actions. Our codes and results can be verified at https://github.com/zhaijianyang/abnormal-action-recognition .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-031-06767-9_46,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-4369-9_12,Abnormal Behavior Detection of Students in the Examination Hall from Surveillance Videos,Advanced Computational Paradigms and Hybrid Intelligent Computing,10.1007/978-981-16-4369-9_12,Springer,2022-01-01,"Video analytic is a method of video processing, data collecting, and data analysis for obtaining the information of a specific domain. In the recent trend, besides analyzing each video for retrieval of the information, analyzing live surveillance video for recognizing behavior that occurs in its covering region has become more significant. This paper aims to presents a methodology that will analyze and recognize the activities of students from the videos recorded by the surveillance cameras during the exam. Automated video surveillance provides an optimal method to help monitor the students and recognize the abnormal/suspicious behavior instantly. The abnormal behavior of students may be copies of the answers from hidden sources with him such as written answers on hands, mobile-phones, summary papers, and books, or copies of answers from other students such as exchange papers between students and peeking at the others' answers papers. This work requires three main techniques: head movement detection, iris movement detection, and hand movement detection to identify the contact between the hands and face of the same student and that between different students. Automatically detect abnormal behavior will help reduce the error rate due to manual surveillance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-4369-9_12,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-5689-7_12,Applications of High Dimensional Neural Networks: A Survey,Advances in Data and Information Sciences,10.1007/978-981-16-5689-7_12,Springer,2022-01-01,"The evolution of artificial neural networks has always been inspired by enormous power of human brain. This survey can be an eye-opener for researchers as its diverse applications of HDNNs in present scenario shows an intelligent way to mimic human brain without creating a complex neuronal architecture having large number of layers. HDNN’s urgency is evident because in science many quantities are measured not by single values for each of them but a group of values defines 1 single unit as: signal has two values: amplitude and phase. The deficiency in present literature on the questions allied with HDNN application looks like sluggish down research focal point and growth in the area. Hence, there exists a call for state-of-the-art addressing high-dimensional problems in neural networks. The study equips readers with a lucid acquaintance of the existing and novel inclination in HDNN replicas. A lot of applications of HDNNs in various disciplines like: healthcare, climate, security, speech recognition, computer vision, music signal processing, production, stock, science, etc. are covered here to confirm advancement in HDNNs. Study divulges that HDNNs is prevalently known as: CVNN, QVNN, 3D VVNN, and OVNN. This paper also reveals that HDNNs have outperformed real valued neural networks in terms of resource utilization, training data set requirement, and accuracy of results. To see a comparative picture of significance and possible implementation of different HDNNs few charts are provided. A motivational message and suggestions for future researches in this area of High-Dimensionality will conclude this paper.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5689-7_12,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-93722-5_11,Mesh Convolutional Neural Networks for Wall Shear Stress Estimation in 3D Artery Models,"Statistical Atlases and Computational Models of the Heart. Multi-Disease, Multi-View, and Multi-Center Right Ventricular Segmentation in Cardiac MRI Challenge",10.1007/978-3-030-93722-5_11,Springer,2022-01-01,"Computational fluid dynamics (CFD) is a valuable tool for personalised, non-invasive evaluation of hemodynamics in arteries, but its complexity and time-consuming nature prohibit large-scale use in practice. Recently, the use of deep learning for rapid estimation of CFD parameters like wall shear stress (WSS) on surface meshes has been investigated. However, existing approaches typically depend on a hand-crafted re-parametrisation of the surface mesh to match convolutional neural network architectures. In this work, we propose to instead use mesh convolutional neural networks that directly operate on the same finite-element surface mesh as used in CFD. We train and evaluate our method on two datasets of synthetic coronary artery models with and without bifurcation, using a ground truth obtained from CFD simulation. We show that our flexible deep learning model can accurately predict 3D WSS vectors on this surface mesh. Our method processes new meshes in less than 5 [s], consistently achieves a normalised mean absolute error of $${\le }1.6$$ ≤ 1.6  [%], and peaks at 90.5 [%] median approximation accuracy over the held-out test set, comparing favourably to previously published work. This demonstrates the feasibility of CFD surrogate modelling using mesh convolutional neural networks for hemodynamic parameter estimation in artery models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-93722-5_11,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-3-030-98253-9_22,Segmentation and Risk Score Prediction of Head and Neck Cancers in PET/CT Volumes with 3D U-Net and Cox Proportional Hazard Neural Networks,Head and Neck Tumor Segmentation and Outcome Prediction,10.1007/978-3-030-98253-9_22,Springer,2022-01-01,"We utilized a 3D nnU-Net model with residual layers supplemented by squeeze and excitation (SE) normalization for tumor segmentation from PET/CT images provided by the Head and Neck Tumor segmentation challenge (HECKTOR). Our proposed loss function incorporates the Unified Focal and Mumford-Shah losses to take the advantage of distribution, region, and boundary-based loss functions. The results of leave-one-out-center-cross-validation performed on different centers showed a segmentation performance of 0.82 average Dice score (DSC) and 3.16 median Hausdorff Distance (HD), and our results on the test set achieved 0.77 DSC and 3.01 HD. Following lesion segmentation, we proposed training a case-control proportional hazard Cox model with an MLP neural net backbone to predict the hazard risk score for each discrete lesion. This hazard risk prediction model (CoxCC) was to be trained on a number of PET/CT radiomic features extracted from the segmented lesions, patient and lesion demographics, and encoder features provided from the penultimate layer of a multi-input 2D PET/CT convolutional neural network tasked with predicting time-to-event for each lesion. A 10-fold cross-validated CoxCC model resulted in a c-index validation score of 0.89, and a c-index score of 0.61 on the HECKTOR challenge test dataset.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-98253-9_22,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Chapter ConferencePaper,doi:10.1007/978-981-16-8484-5_7,Violence Recognition Using Convolutional Neural Networks,Computational Intelligence in Machine Learning,10.1007/978-981-16-8484-5_7,Springer,2022-01-01,"Livestream videos are becoming the norm in this digital era. With the advancement of streaming technology, real-time sharing has been effortless for many of the global population. There are many platforms that allow anyone to create or view livestream videos. This situation, however, creates a high possibility of showing inappropriate videos as well. Violent actions explicit languages and even crimes can be streamed, intentionally or otherwise. The impact on viewers may vary, though mostly negative. Thus, this research is motivated to help censor these unwanted scenes in real-time so that the negative impact can be reduced or avoided. By implementing the convolutional neural networks (CNNs) and training on three different off-the-shelf models in a restricted environment, it was found that the VGG19 model is the best for this research to be deployed in the censoring application compared to the MobileNetV2 and InceptionResNetV2 models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-8484-5_7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11468-w,Application of region-based video surveillance in smart cities using deep learning,Multimedia Tools and Applications,10.1007/s11042-021-11468-w,Springer,2021-12-27,"Smart video surveillance helps to build more robust smart city environment. The varied angle cameras act as smart sensors and collect visual data from smart city environment and transmit it for further visual analysis. The transmitted visual data is required to be in high quality for efficient analysis which is a challenging task while transmitting videos on low capacity bandwidth communication channels. In latest smart surveillance cameras, high quality of video transmission is maintained through various video encoding techniques such as high efficiency video coding. However, these video coding techniques still provide limited capabilities and the demand of high-quality based encoding for salient regions such as pedestrians, vehicles, cyclist/motorcyclist and road in video surveillance systems is still not met. This work is a contribution towards building an efficient salient region-based surveillance framework for smart cities. The proposed framework integrates a deep learning-based video surveillance technique that extracts salient regions from a video frame without information loss, and then encodes it in reduced size. We have applied this approach in diverse case studies environments of smart city to test the applicability of the framework. The successful result in terms of bitrate 56.92%, peak signal to noise ratio 5.35 bd and SR based segmentation accuracy of 92% and 96% for two different benchmark datasets is the outcome of proposed work. Consequently, the generation of less computational region-based video data makes it adaptable to improve surveillance solution in Smart Cities.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11468-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11432-021-3348-6,Learning practically feasible policies for online 3D bin packing,Science China Information Sciences,10.1007/s11432-021-3348-6,Springer,2021-12-27,"We tackle the online 3D bin packing problem (3D-BPP), a challenging yet practically useful variant of the classical bin packing problem. In this problem, the items are delivered to the agent without informing the full sequence information. The agent must directly pack these items into the target bin stably without changing their arrival order, and no further adjustment is permitted. Online 3D-BPP can be naturally formulated as a Markov decision process (MDP). We adopt deep reinforcement learning, in particular, the on-policy actor-critic framework, to solve this MDP with constrained action space. To learn a practically feasible packing policy, we propose three critical designs. First, we propose an online analysis of packing stability based on a novel stacking tree. It attains a high analysis accuracy while reducing the computational complexity from O ( N ^2) to O ( N log N ), making it especially suited for reinforcement learning training. Second, we propose a decoupled packing policy learning for different dimensions of placement which enables high-resolution spatial discretization and hence high packing precision. Third, we introduce a reward function that dictates the robot to place items in a far-to-near order and therefore simplifies the collision avoidance in movement planning of the robotic arm. Furthermore, we provide a comprehensive discussion on several key implemental issues. The extensive evaluation demonstrates that our learned policy outperforms the state-of-the-art methods significantly and is practically usable for real-world applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11432-021-3348-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40537-021-00550-7,Cyberbullying detection: advanced preprocessing techniques & deep learning architecture for Roman Urdu data,Journal of Big Data,10.1186/s40537-021-00550-7,Springer,2021-12-22,"Social media have become a very viable medium for communication, collaboration, exchange of information, knowledge, and ideas. However, due to anonymity preservation, the incidents of hate speech and cyberbullying have been diversified across the globe. This intimidating problem has recently sought the attention of researchers and scholars worldwide and studies have been undertaken to formulate solution strategies for automatic detection of cyberaggression and hate speech, varying from machine learning models with vast features to more complex deep neural network models and different SN platforms. However, the existing research is directed towards mature languages and highlights a huge gap in newly embraced resource poor languages. One such language that has been recently adopted worldwide and more specifically by south Asian countries for communication on social media is Roman Urdu i-e Urdu language written using Roman scripting. To address this research gap, we have performed extensive preprocessing on Roman Urdu microtext. This typically involves formation of Roman Urdu slang- phrase dictionary and mapping slangs after tokenization. We have also eliminated cyberbullying domain specific stop words for dimensionality reduction of corpus. The unstructured data were further processed to handle encoded text formats and metadata/non-linguistic features. Furthermore, we performed extensive experiments by implementing RNN-LSTM, RNN-BiLSTM and CNN models varying epochs executions, model layers and tuning hyperparameters to analyze and uncover cyberbullying textual patterns in Roman Urdu. The efficiency and performance of models were evaluated using different metrics to present the comparative analysis. Results highlight that RNN-LSTM and RNN-BiLSTM performed best and achieved validation accuracy of 85.5 and 85% whereas F1 score was 0.7 and 0.67 respectively over aggression class.",https://www.biomedcentral.com/openurl?doi=10.1186/s40537-021-00550-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40645-021-00459-y,Classification of imbalanced cloud image data using deep neural networks: performance improvement through a data science competition,Progress in Earth and Planetary Science,10.1186/s40645-021-00459-y,Springer,2021-12-15,"Image data classification using machine learning is an effective method for detecting atmospheric phenomena. However, extreme weather events with a small number of cases cause a decrease in classification prediction accuracy owing to the imbalance in data between the target class and the other classes. To build a highly accurate classification model, I held a data analysis competition to determine the best classification performance for two classes of cloud image data, specifically tropical cyclones including precursors and other classes. For the top models in the competition, minority data oversampling, majority data undersampling, ensemble learning, deep layer neural networks, and cost-effective loss functions were used to improve the classification performance of the imbalanced data. In particular, the best model of 209 submissions succeeded in improving the classification capability by 65.4% over similar conventional methods in a measure of the low false alarm ratio.",https://www.biomedcentral.com/openurl?doi=10.1186/s40645-021-00459-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s42003-021-02919-z,Deciphering tumour tissue organization by 3D electron microscopy and machine learning,Communications Biology,10.1038/s42003-021-02919-z,Nature,2021-12-13,"de Senneville et al. demonstrate an integrated workflow combining 3D imaging, manual and machine learning-based semi-automatic segmentation, mathematics and infographics to study the spatial organization of patient-derived hepatoblastoma xenograft tissues. Their approach potentially assists investigations of this childhood liver tumour and other types of tumour tissues. Despite recent progress in the characterization of tumour components, the tri-dimensional (3D) organization of this pathological tissue and the parameters determining its internal architecture remain elusive. Here, we analysed the spatial organization of patient-derived xenograft tissues generated from hepatoblastoma, the most frequent childhood liver tumour, by serial block-face scanning electron microscopy using an integrated workflow combining 3D imaging, manual and machine learning-based semi-automatic segmentations, mathematics and infographics. By digitally reconstituting an entire hepatoblastoma sample with a blood capillary, a bile canaliculus-like structure, hundreds of tumour cells and their main organelles (e.g. cytoplasm, nucleus, mitochondria), we report unique 3D ultrastructural data about the organization of tumour tissue. We found that the size of hepatoblastoma cells correlates with the size of their nucleus, cytoplasm and mitochondrial mass. We also found anatomical connections between the blood capillary and the planar alignment and size of tumour cells in their 3D milieu. Finally, a set of tumour cells polarized in the direction of a hot spot corresponding to a bile canaliculus-like structure. In conclusion, this pilot study allowed the identification of bioarchitectural parameters that shape the internal and spatial organization of tumours, thus paving the way for future investigations in the emerging onconanotomy field.",https://www.nature.com/articles/s42003-021-02919-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1208/s12249-021-02083-x,A Novel Computational Approach Coupled with Machine Learning to Predict the Extent of Agglomeration in Particulate Processes,AAPS PharmSciTech,10.1208/s12249-021-02083-x,Springer,2021-12-13,"Solid particle agglomeration is a prevalent phenomenon in various processes across the chemical, food, and pharmaceutical industries. In pharmaceutical manufacturing, agglomeration is both desired in unit operations like wet granulation and undesired in unit operations such as agitated filter drying of highly potent active pharmaceutical ingredients (API). Agglomeration needs to be controlled for optimal physical properties of the API powder. Even after decades of work in the field, there is still very limited understanding of how to quantify, predict, and control the extent of agglomeration, owing to the complex interaction between the solvent and the solid particles and stochasticity imparted by mixing. Furthermore, a large size of industrial scale particulate process systems makes it computationally intractable. To overcome these challenges, we present a novel theory and computational methodology to predict the agglomeration extent by coupling the experimental measurements of agglomeration risk zone or “sticky zone” with discrete element method. The proposed model shows good agreement with experiments. Further, a machine learning model was built to predict agglomeration extent as a function of input variables, such as material properties and processing conditions, in order to build a digital twin of the unit operation. While the focus of the present study is the agglomeration of particles during industrial drying processes, the proposed methodology can be readily applied to numerous other particulate processes where agglomeration is either desired or undesired.",http://link.springer.com/openurl/fulltext?id=doi:10.1208/s12249-021-02083-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-021-03219-6,Deep learning for anatomical interpretation of video bronchoscopy images,Scientific Reports,10.1038/s41598-021-03219-6,Nature,2021-12-09,"Anesthesiologists commonly use video bronchoscopy to facilitate intubation or confirm the location of the endotracheal tube; however, depth and orientation in the bronchial tree can often be confused because anesthesiologists cannot trace the airway from the oropharynx when it is performed using an endotracheal tube. Moreover, the decubitus position is often used in certain surgeries. Although it occurs rarely, the misinterpretation of tube location can cause accidental extubation or endobronchial intubation, which can lead to hyperinflation. Thus, video bronchoscopy with a decision supporting system using artificial intelligence would be useful in the anesthesiologic process. In this study, we aimed to develop an artificial intelligence model robust to rotation and covering using video bronchoscopy images. We collected video bronchoscopic images from an institutional database. Collected images were automatically labeled by an optical character recognition engine as the carina and left/right main bronchus. Except 180 images for the evaluation dataset, 80% were randomly allocated to the training dataset. The remaining images were assigned to the validation and test datasets in a 7:3 ratio. Random image rotation and circular cropping were applied. Ten kinds of pretrained models with < 25 million parameters were trained on the training and validation datasets. The model showing the best prediction accuracy for the test dataset was selected as the final model. Six human experts reviewed the evaluation dataset for the inference of anatomical locations to compare its performance with that of the final model. In the experiments, 8688 images were prepared and assigned to the evaluation (180), training (6806), validation (1191), and test (511) datasets. The EfficientNetB1 model showed the highest accuracy (0.86) and was selected as the final model. For the evaluation dataset, the final model showed better performance (accuracy, 0.84) than almost all human experts (0.38, 0.44, 0.51, 0.68, and 0.63), and only the most-experienced pulmonologist showed performance comparable (0.82) with that of the final model. The performance of human experts was generally proportional to their experiences. The performance difference between anesthesiologists and pulmonologists was marked in discrimination of the right main bronchus. Using bronchoscopic images, our model could distinguish anatomical locations among the carina and both main bronchi under random rotation and covering. The performance was comparable with that of the most-experienced human expert. This model can be a basis for designing a clinical decision support system with video bronchoscopy.",https://www.nature.com/articles/s41598-021-03219-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s42991-021-00180-9,"Advanced image recognition: a fully automated, high-accuracy photo-identification matching system for humpback whales",Mammalian Biology,10.1007/s42991-021-00180-9,Springer,2021-12-07,"We describe the development and application of a new convolutional neural network-based photo-identification algorithm for individual humpback whales ( Megaptera novaeangliae ). The method uses a Densely Connected Convolutional Network (DenseNet) to extract special keypoints of an image of the ventral surface of the fluke and then a separate DenseNet trained to look for features within these keypoints. The extracted features are then compared against those of the reference set of previously known humpback whales for similarity. This offers the potential to successfully automate recognition of individuals in large photographic datasets such as in ocean basin-wide marine mammal studies. The algorithm requires minimal image pre-processing and is capable of accurate, rapid matching of fair to high-quality humpback fluke photographs. In real world testing compared to manual image matching, the algorithm reduces image management time by at least 98% and reduces error rates of missing potential matches from approximately 6–9% to 1–3%. The success of this new system permits automated comparisons to be made for the first time across photo-identification datasets with tens to hundreds of thousands of individually identified encounters, with profound implications for long-term and large population studies of the species.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s42991-021-00180-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41467-021-27396-0,DeepRank: a deep learning framework for data mining 3D protein-protein interfaces,Nature Communications,10.1038/s41467-021-27396-0,Nature,2021-12-03,"The authors present DeepRank, a deep learning framework for the data mining of large sets of 3D protein-protein interfaces (PPI). They use DeepRank to address two challenges in structural biology: distinguishing biological versus crystallographic PPIs in crystal structures, and secondly the ranking of docking models. Three-dimensional (3D) structures of protein complexes provide fundamental information to decipher biological processes at the molecular scale. The vast amount of experimentally and computationally resolved protein-protein interfaces (PPIs) offers the possibility of training deep learning models to aid the predictions of their biological relevance. We present here DeepRank, a general, configurable deep learning framework for data mining PPIs using 3D convolutional neural networks (CNNs). DeepRank maps features of PPIs onto 3D grids and trains a user-specified CNN on these 3D grids. DeepRank allows for efficient training of 3D CNNs with data sets containing millions of PPIs and supports both classification and regression. We demonstrate the performance of DeepRank on two distinct challenges: The classification of biological versus crystallographic PPIs, and the ranking of docking models. For both problems DeepRank is competitive with, or outperforms, state-of-the-art methods, demonstrating the versatility of the framework for research in structural biology.",https://www.nature.com/articles/s41467-021-27396-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12517-021-09178-6,Retraction Note: Research on urban modern architectural art based on artificial intelligence and GIS image recognition system,Arabian Journal of Geosciences,10.1007/s12517-021-09178-6,Springer,2021-12-02,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12517-021-09178-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10994-021-06055-x,Deep learning and multivariate time series for cheat detection in video games,Machine Learning,10.1007/s10994-021-06055-x,Springer,2021-12-01,"Online video games drive a multi-billion dollar industry dedicated to maintaining a competitive and enjoyable experience for players. Traditional cheat detection systems struggle when facing new exploits or sophisticated fraudsters. More advanced solutions based on machine learning are more adaptive but rely heavily on in-game data, which means that each game has to develop its own cheat detection system. In this work, we propose a novel approach to cheat detection that doesn’t require in-game data. Firstly, we treat the multimodal interactions between the player and the platform as multivariate time series. We then use convolutional neural networks to classify these time series as corresponding to legitimate or fraudulent gameplay. Our models achieve an average accuracy of respectively 99.2% and 98.9% in triggerbot and aimbot (two widespread cheats), in an experiment to validate the system’s ability to detect cheating in players never seen before. Because this approach is based solely on player behavior, it can be applied to any game or input method, and even various tasks related to modeling human activity.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10994-021-06055-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10055-021-00527-0,Correction to: Predicting user visual attention in virtual reality with a deep learning model,Virtual Reality,10.1007/s10055-021-00527-0,Springer,2021-12-01,A correction to this paper has been published: https://doi.org/10.1007/s10055-021-00527-0,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-021-00527-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10055-021-00512-7,Predicting user visual attention in virtual reality with a deep learning model,Virtual Reality,10.1007/s10055-021-00512-7,Springer,2021-12-01,"Recent studies show that user’s visual attention during virtual reality museum navigation can be effectively estimated with deep learning models. However, these models rely on large-scale datasets that usually are of high structure complexity and context specific, which is challenging for nonspecialist researchers and designers. Therefore, we present the deep learning model, ALRF, to generalise on real-time user visual attention prediction in virtual reality context. The model combines two parallel deep learning streams to process the compact dataset of temporal–spatial salient features of user’s eye movements and virtual object coordinates. The prediction accuracy outperformed the state-of-the-art deep learning models by reaching record high 91.03%. Importantly, with quick parametric tuning, the model showed flexible applicability across different environments of the virtual reality museum and outdoor scenes. Implications for how the proposed model may be implemented as a generalising tool for adaptive virtual reality application design and evaluation are discussed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-021-00512-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00779-021-01614-4,RETRACTED ARTICLE: Vocational education reform based on improved convolutional neural network and speech recognition,Personal and Ubiquitous Computing,10.1007/s00779-021-01614-4,Springer,2021-12-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00779-021-01614-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-020-09674-2,"Fundamentals, present and future perspectives of speech enhancement",International Journal of Speech Technology,10.1007/s10772-020-09674-2,Springer,2021-12-01,"Speech enhancement has substantial interest in the utilization of speaker identification, video-conference, speech transmission through communication channels, speech-based biometric system, mobile phones, hearing aids, microphones, voice conversion etc. Pattern mining methods have a vital step in the growth of speech enhancement schemes. To design a successful speech enhancement system consideration to the background noise processing is needed. A substantial number of methods from traditional techniques and machine learning have been utilized to process and remove the additive noise from a speech signal. With the advancement of machine learning and deep learning, classification of speech has become more significant. Methods of speech enhancement consist of different stages, such as feature extraction of the input speech signal, feature selection, feature selection followed by classification. Deep learning techniques are also an emerging field in the classification domain, which is discussed in this review. The intention of this paper is to provide a state-of-the-art summary and present approaches for using the widely used machine learning and deep learning methods to detect the challenges along with future research directions of speech enhancement systems.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-020-09674-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-021-02478-y,An efficient multi-path 3D convolutional neural network for false-positive reduction of pulmonary nodule detection,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-021-02478-y,Springer,2021-12-01,"Purpose Considering that false-positive and true pulmonary nodules are highly similar in shapes and sizes between lung computed tomography scans, we develop and evaluate a false-positive nodules reduction method applied to the computer-aided diagnosis system. Methods To improve the pulmonary nodule diagnosis quality, a 3D convolutional neural networks (CNN) model is constructed to effectively extract spatial information of candidate nodule features through the hierarchical architecture. Furthermore, three paths corresponding to three receptive field sizes are adopted and concatenated in the network model, so that the feature information is fully extracted and fused to actively adapting to the changes in shapes, sizes, and contextual information between pulmonary nodules. In this way, the false-positive reduction is well implemented in pulmonary nodule detection. Results Multi-path 3D CNN is performed on LUNA16 dataset, which achieves an average competitive performance metric score of 0.881, and excellent sensitivity of 0.952 and 0.962 occurs to 4, 8 FP/Scans. Conclusion By constructing a multi-path 3D CNN to fully extract candidate target features, it accurately identifies pulmonary nodules with different sizes, shapes, and background information. In addition, the proposed general framework is also suitable for similar 3D medical image classification tasks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-021-02478-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-021-01530-3,Guest Editorial: Special Issue on Deep Learning for Video Analysis and Compression,International Journal of Computer Vision,10.1007/s11263-021-01530-3,Springer,2021-12-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-021-01530-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-021-09853-9,A deep learning approach for automatic speech recognition of The Holy Qur’ān recitations,International Journal of Speech Technology,10.1007/s10772-021-09853-9,Springer,2021-12-01,"Being the main spiritual source and reference for Muslims, The Holy Qur’ān can be recited in ten recitations (Qiraat). Each recitation (Qiraah) possesses certain features and characteristics that can be discriminated using Tajweed rules, which can best be defined as the elocution rules for reciting The Holy Qur’ān. This paper describes our efforts towards preparing, designing, developing, and evaluating a large-vocabulary speaker-independent and continuous speech recognizer for The Holy Qur’ān based on the narration of Hafs from A’asim by utilizing the state-of-the-art Automatic Speech Recognition (ASR) evolutionary approaches. Several Tajweed rules as depicted from the narration of Hafs from A’asim have been addressed and embedded in the development of the speech recognizer in our work. In addition, this paper presents the preparation process of The Holy Qur’ān speech corpus, which was used to train and test the speech recognizer. For training the acoustic model in our speech recognizer, four experimental setups were used within KALDI toolkit that are different in terms of dataset size and Tajweed rules. The best experimental setup is based on Time Delay Neural Networks (TDNN) with sub-sampling technique and obtained a Word Error Rate (WER) in the range of (0.27–6.31%) and a Sentence Error Rate (SER) in the range of (0.4–17.39%). Therefore, the experimental results are very promising and they indicate that the speech recognizer is able to recognize The Holy Qur’ān based on the narration of Hafs from A’asim.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-021-09853-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11554-021-01094-y,A real-time video smoke detection algorithm based on Kalman filter and CNN,Journal of Real-Time Image Processing,10.1007/s11554-021-01094-y,Springer,2021-12-01,"Smoke detection represents a critical task for avoiding large scale fire disaster in industrial environment and cities. Including intelligent video-based techniques in existing camera infrastructure enables faster response time if compared to traditional analog smoke detectors. In this work presents a hybrid approach to assess the rapid and precise identification of smoke in a video sequence. The algorithm combines a traditional feature detector based on Kalman filtering and motion detection, and a lightweight shallow convolutional neural network. This technique allows the automatic selection of specific regions of interest within the image by the generation of bounding boxes for gray colored moving objects. In the final step the convolutional neural network verifies the actual presence of smoke in the proposed regions of interest. The algorithm provides also an alarm generator that can trigger an alarm signal if the smoke is persistent in a time window of 3 s. The proposed technique has been compared to the state of the art methods available in literature by using several videos of public and non-public dataset showing an improvement in the metrics. Finally, we developed a portable solution for embedded systems and evaluated its performance for the Raspberry Pi 3 and the Nvidia Jetson Nano.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-021-01094-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-020-02798-y,CPS-based manufacturing workcell for the production of hybrid medical devices,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-020-02798-y,Springer,2021-12-01,"Cyber-physical system (CPS) can increase efficiency and lower costs in manufacturing operations. CPS and related technologies can help medical device manufacturers to meet the rising demands for quality and affordable healthcare products and services. In this paper, a novel architecture for CPS-based manufacturing workcell systems was proposed. Its contribution is to provide quality inspection and operator guidance in manual assembly systems, specifically for the production of hybrid medical devices. The proposed system involves the integration of enabling technologies including Machine Vision (MV), Internet-of-things (IoT) and Augmented Reality. The group illustrated and evaluated the implementation of two CPS prototypes based on the proposed architecture on the fabrication of a specific hybrid medical device, verifying their effectiveness as guidance tools for operators. The two prototypes, CPS-1 and CPS-2 differ in their MV subsystems by having non-deep learning and deep learning-based image processing methods respectively. Experimental results illustrate that, when exposed to various changing conditions, the Regional Convolutional Neural Network (R-CNN) of CPS-2, on average, has outperformed the non-deep learning image processing method of CPS-1 by 42.3% and 8.5%, in terms of accuracy and precision respectively. Hence, this presents evidence on the usefulness of R-CNN-based MV system to increase the flexibility and adaptability of the CPS in terms of object monitoring and reporting.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-020-02798-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12206-021-1125-8,Detection of surface roughness of mechanical drawings with deep learning,Journal of Mechanical Science and Technology,10.1007/s12206-021-1125-8,Springer,2021-12-01,"Engineering drawing inspection is important to CAD modeling of mechanical parts. Traditional inspection methods mainly rely on manual analysis by using the CAD software, which requires expert knowledge and massive time. In view of simplifying the analysis for non-experts and improving detection efficiency and accuracy, this study proposes a generic approach combining object detection and image recognition methods to identify surface roughness of mechanical drawings. For both the object detection and image recognition methods, deep learning models with different backbone networks are trained and tested independently. Experimental results show that a combination of Faster-RCNN with ResNet101 as backbone network, and SSD with ResNet50 as backbone network achieves the best performance under our evaluation metrics.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12206-021-1125-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06327-6,Deep learning with accelerated execution: toward a real-time video analysis system,Neural Computing and Applications,10.1007/s00521-021-06327-6,Springer,2021-12-01,"This paper demonstrates how deep learning (DL) pipeline can be accelerated by the use of field-programmable gate array (FPGA)-based accelerator. An affordable Cyclone V FPGA was tested against two benchmark programs which perform classification and object detection. Use of FPGA helps significant reduction of average processing time for classification and boosts real-time capabilities for object detection without extra overheads, thus offering high processing rate (in FPS). The repeated tests were conducted and showed the performance speedup obtained by utilizing out-off-the-box optimization from within the Open Visual Inference and Neural network Optimization (OpenVINO) tookit. These two benchmark programs are run as pure software and then accelerated by hardware and demonstrate the advantages of FPGA DL acceleration and the OpenVINO tool kit.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06327-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1134/S1064226921120202,Convolutional Neural Network Training Optimization for Low Point Density Image Recognition,Journal of Communications Technology and Electronics,10.1134/S1064226921120202,Springer,2021-12-01,"Abstract Methods for structure optimization of convolutional neural network used for low point density images recognition are proposed to accelerate the training and new images recognition, as well as to reduce the training and recognition procedures resource consumption. Optimized neural network showed a significant increase in speed without accuracy drop in the low point density images recognition, as well as a significantly reduced overfitting tendency.",http://link.springer.com/openurl/fulltext?id=doi:10.1134/S1064226921120202,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00500-021-06480-z,Research on intelligent language translation system based on deep learning algorithm,Soft Computing,10.1007/s00500-021-06480-z,Springer,2021-12-01,"In order to improve the effect of intelligent language translation, this paper analyzes the problems of the MSE cost function used by most of the current DNN-based speech enhancement algorithms and proposes a deep learning speech enhancement algorithm based on perception-related cost functions. Moreover, this paper embeds the suppression gain parameter estimation into the architecture of the traditional speech enhancement algorithm and converts the relationship between the noisy speech spectrum and the enhanced speech spectrum into a simple multiplication relationship based on suppression gain combined with deep learning algorithms to construct an intelligent language translation system. Moreover, this paper evaluates the translation effect of the system, analyzes the actual results, and uses simulation tests to verify the performance of the intelligent language translation model constructed in this paper. From the experimental results, it can be seen that the intelligent language translation system based on deep learning algorithms has good results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-021-06480-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10346-021-01761-y,Dense 3D displacement vector fields for point cloud-based landslide monitoring,Landslides,10.1007/s10346-021-01761-y,Springer,2021-12-01,"We propose a novel fully automated deformation analysis pipeline capable of estimating real 3D displacement vectors from point cloud data. Different from the traditional methods that establish displacements based on the proximity in the Euclidean space, our approach estimates dense 3D displacement vector fields by searching for corresponding points across the epochs in the space of 3D local feature descriptors. Due to this formulation, our method is also sensitive to motion and deformations that occur parallel to the underlying surface. By enabling efficient parallel processing, the proposed method can be applied to point clouds of arbitrary size. We compare our approach to the traditional methods on point cloud data of two landslides and show that while the traditional methods often underestimate the displacements, our method correctly estimates full 3D displacement vectors.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10346-021-01761-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s41095-021-0215-y,Real-time face view correction for front-facing cameras,Computational Visual Media,10.1007/s41095-021-0215-y,Springer,2021-12-01,"Face views are particularly important in person-to-person communication. Differenes between the camera location and the face orientation can result in undesirable facial appearances of the participants during video conferencing. This phenomenon is particularly noticeable when using devices where the front-facing camera is placed in unconventional locations such as below the display or within the keyboard. In this paper, we take a video stream from a single RGB camera as input, and generate a video stream that emulates the view from a virtual camera at a designated location. The most challenging issue in this problem is that the corrected view often needs out-of-plane head rotations. To address this challenge, we reconstruct the 3D face shape and re-render it into synthesized frames according to the virtual camera location. To output the corrected video stream with natural appearance in real time, we propose several novel techniques including accurate eyebrow reconstruction, high-quality blending between the corrected face image and background, and template-based 3D reconstruction of glasses. Our system works well for different lighting conditions and skin tones, and can handle users wearing glasses. Extensive experiments and user studies demonstrate that our method provides high-quality results.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41095-021-0215-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11770-021-0909-z,Three-dimensional gravity inversion based on 3D U-Net++,Applied Geophysics,10.1007/s11770-021-0909-z,Springer,2021-12-01,"The gravity inversion is to restore genetic density distribution of the underground target to be explored for explaining the internal structure and distribution of the Earth. In this paper, we propose a new 3D gravity inversion method based on 3D U-Net++. Compared with two-dimensional gravity inversion, three-dimensional (3D) gravity inversion can more precisely describe the density distribution of underground space. However, conventional 3D gravity inversion method input is two-dimensional, the input and output of the network proposed in our method are three-dimensional. In the training stage, we design a large number of diversified simulation model-data pairs by using the random walk method to improve the generalization ability of the network. In the test phase, we verify the network performance by using the model-data pairs generated by the simulation. To further illustrate the effectiveness of the algorithm, we apply the method to the inversion of the San Nicolas mining area, and the inversion results are basically consistent with the borehole measurement results. Moreover, the results of the 3D U-Net++ inversion and the 3D U-Net inversion are compared. The density models of the 3D U-Net++ inversion have higher resolution, more concentrated inversion results, and a clearer boundary of the density model.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11770-021-0909-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00500-021-06292-1,Predicting the abnormality of brain and compute the cognitive power of human using deep learning techniques using functional magnetic resonance images,Soft Computing,10.1007/s00500-021-06292-1,Springer,2021-12-01,"In modern times, digital medical images play a significant progression in clinical diagnosis to treat the populace earlier to hoard their lives. Magnetic resonance imaging (MRI) is one of the most advanced medical imaging modalities that facilitate scanning various parts of the human body like the head, chest, abdomen, and pelvis and identify the diseases. Numerous studies on the same discipline have proposed different algorithms, techniques, and methods for analyzing medical digital images, especially MRI. Most of them have mainly focused on identifying and classifying the images as either normal or abnormal. Computing brainpower is essential to understand and handle various brain diseases efficiently in critical situations. This paper knuckles down to design and implement a computer-aided framework, enhancing the identification of humans' cognitive power from their MRI Images. The proposed framework converts the 3D DICOM images into 2D medical images, pre-processing, enhancement, learning, and extracting various image information to classify it as normal or abnormal and provide the brain's cognitive power. This study widens the efficient use of machine learning methods, voxel residual network (VRN), with multimodality fusion architecture to learn and analyze the image to classify and predict cognitive power. The experimental results denote that the proposed framework demonstrates better performance than the existing approaches.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-021-06292-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-021-02476-0,Seeing under the cover with a 3D U-Net: point cloud-based weight estimation of covered patients,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-021-02476-0,Springer,2021-12-01,"Purpose Body weight is a crucial parameter for patient-specific treatments, particularly in the context of proper drug dosage. Contactless weight estimation from visual sensor data constitutes a promising approach to overcome challenges arising in emergency situations. Machine learning-based methods have recently been shown to perform accurate weight estimation from point cloud data. The proposed methods, however, are designed for controlled conditions in terms of visibility and position of the patient, which limits their practical applicability. In this work, we aim to decouple accurate weight estimation from such specific conditions by predicting the weight of covered patients from voxelized point cloud data. Methods We propose a novel deep learning framework, which comprises two 3D CNN modules solving the given task in two separate steps. First, we train a 3D U-Net to virtually uncover the patient, i.e. to predict the patient’s volumetric surface without a cover. Second, the patient’s weight is predicted from this 3D volume by means of a 3D CNN architecture, which we optimized for weight regression. Results We evaluate our approach on a lying pose dataset (SLP) under two different cover conditions. The proposed framework considerably improves on the baseline model by up to $${16}{\%}$$ 16 % and reduces the gap between the accuracy of weight estimates for covered and uncovered patients by up to $${52}{\%}$$ 52 % . Conclusion We present a novel pipeline to estimate the weight of patients, which are covered by a blanket. Our approach relaxes the specific conditions that were required for accurate weight estimates by previous contactless methods and thus constitutes an important step towards fully automatic weight estimation in clinical practice.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-021-02476-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12650-021-00768-w,"Visualization of plasma shape in the LHD-type helical fusion reactor, FFHR, by a deep learning technique",Journal of Visualization,10.1007/s12650-021-00768-w,Springer,2021-12-01,"Abstract A magnetic field is used to confine the plasma to achieve controlled fusion. Therefore, since the movement of the plasma follows magnetic field lines, a plurality of magnetic field lines is calculated from electromagnetic field simulation results in a fusion reactor. Because of the complicated distribution of magnetic field lines in three-dimensional (3D) space, existing analysis measures which are mostly based on two-dimensional poloidal plasma cross-sections are unsatisfactory for domain experts. To solve this problem, we propose a technique for reconstructing a regular scalar field from the magnetic field lines. First, on poloidal plasma cross-sections, intersection points of magnetic field lines are used to make annotations of learning the plasma shape. Then, a deep neural network is built to approximate the scalar field that represents the probability of the existence of magnetic field lines. Consequently, a 3D model of plasma shape has managed to be constructed by applying the marching cubes method. The effectiveness of the proposed method is demonstrated by comparing it with the conventional method and domain experts’ reviews. Graphic abstract ",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12650-021-00768-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s43465-021-00455-w,Artificial Intelligence to Automatically Assess Scan Quality in Hip Ultrasound,Indian Journal of Orthopaedics,10.1007/s43465-021-00455-w,Springer,2021-12-01,"Purpose Since it is fast, inexpensive and increasingly portable, ultrasound can be used for early detection of Developmental Dysplasia of the Hip (DDH) in infants at point-of-care. However, accurate interpretation\is highly dependent on scan quality. Poor-quality images lead to misdiagnosis, but inexperienced users may not even recognize the deficiencies in the images. Currently, users assess scan quality subjectively, based on image landmarks which are prone to human errors. Instead, we propose using Artificial Intelligence (AI) to automatically assess scan quality. Methods We trained separate Convolutional Neural Network (CNN) models to detect presence of each of four commonly used ultrasound landmarks in each hip image: straight horizontal iliac wing, labrum, os ischium and midportion of the femoral head. We used 100 3D ultrasound (3DUS) images for training and validated the technique on a set of 107 3DUS images also scored for landmarks by three non-expert readers and one expert radiologist. Results We got AI ≥ 85% accuracy for all four landmarks (ilium = 0.89, labrum = 0.94, os ischium = 0.85, femoral head = 0.98) as a binary classifier between adequate and inadequate scan quality. Our technique also showed excellent agreement with manual assessment in terms of Intraclass Correlation Coefficient (ICC) and Cohen’s kappa coefficient ( K ) for ilium (ICC = 0.81, K  = 0.56), os ischium (ICC = 0.89, K  = 0.63) and femoral head (ICC = 0.83, K  = 0.66), and moderate to good agreement for labrum (ICC = 0.65, K  = 0.33). Conclusion This new technique could ensure high scan quality and facilitate more widespread use of ultrasound in population screening of DDH. Graphical abstract ",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s43465-021-00455-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10278-021-00526-2,Evolutionary Deep Attention Convolutional Neural Networks for 2D and 3D Medical Image Segmentation,Journal of Digital Imaging,10.1007/s10278-021-00526-2,Springer,2021-12-01,"Developing a convolutional neural network (CNN) for medical image segmentation is a complex task, especially when dealing with the limited number of available labelled medical images and computational resources. This task can be even more difficult if the aim is to develop a deep network and using a complicated structure like attention blocks. Because of various types of noises, artefacts and diversity in medical images, using complicated network structures like attention mechanism to improve the accuracy of segmentation is inevitable. Therefore, it is necessary to develop techniques to address the above difficulties. Neuroevolution is the combination of evolutionary computation and neural networks to establish a network automatically. However, Neuroevolution is computationally expensive, specifically to create 3D networks. In this paper, an automatic, efficient, accurate, and robust technique is introduced to develop deep attention convolutional neural networks utilising Neuroevolution for both 2D and 3D medical image segmentation. The proposed evolutionary technique can find a very good combination of six attention modules to recover spatial information from downsampling section and transfer them to the upsampling section of a U-Net-based network—six different CT and MRI datasets are employed to evaluate the proposed model for both 2D and 3D image segmentation. The obtained results are compared to state-of-the-art manual and automatic models, while our proposed model outperformed all of them.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10278-021-00526-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10514-021-10020-x,Autonomous assembly planning of demonstrated skills with reinforcement learning in simulation,Autonomous Robots,10.1007/s10514-021-10020-x,Springer,2021-12-01,"Industrial robots used to assemble customized products in small batches require a lot of reprogramming. With this work we aim to reduce the programming complexity by autonomously finding the fastest assembly plans without any collisions with the environment. First, a digital twin of the robot uses a gym in simulation to learn which assembly skills (programmed by demonstration) are physically possible (i.e. no collisions with the environment). Only from this reduced solution space will the physical twin look for the fastest assembly plans. Experiments show that the system indeed converges to the fastest assembly plans. Moreover, pre-training in simulation drastically reduces the number of interactions before convergence compared to directly learning on the physical robot. This two-step procedure allows for the robot to autonomously find correct and fast assembly sequences, without any additional human input or mismanufactured products.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10514-021-10020-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s41095-021-0230-z,HDR-Net-Fusion: Real-time 3D dynamic scene reconstruction with a hierarchical deep reinforcement network,Computational Visual Media,10.1007/s41095-021-0230-z,Springer,2021-12-01,"Reconstructing dynamic scenes with commodity depth cameras has many applications in computer graphics, computer vision, and robotics. However, due to the presence of noise and erroneous observations from data capturing devices and the inherently ill-posed nature of non-rigid registration with insufficient information, traditional approaches often produce low-quality geometry with holes, bumps, and misalignments. We propose a novel 3D dynamic reconstruction system, named HDR-Net-Fusion, which learns to simultaneously reconstruct and refine the geometry on the fly with a sparse embedded deformation graph of surfels, using a hierarchical deep reinforcement (HDR) network. The latter comprises two parts: a global HDR-Net which rapidly detects local regions with large geometric errors, and a local HDR-Net serving as a local patch refinement operator to promptly complete and enhance such regions. Training the global HDR-Net is formulated as a novel reinforcement learning problem to implicitly learn the region selection strategy with the goal of improving the overall reconstruction quality. The applicability and efficiency of our approach are demonstrated using a large-scale dynamic reconstruction dataset. Our method can reconstruct geometry with higher quality than traditional methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41095-021-0230-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-021-03840-2,Applying image registration algorithm combined with CNN model to video image stitching,The Journal of Supercomputing,10.1007/s11227-021-03840-2,Springer,2021-12-01,"The purposes are to explore the video image stitching technique of Unmanned Aerial Vehicles (UAVs), expand the application of image registration algorithms and new sensing equipment in video image stitching, and improve the development of video remote sensing image processing. First, how to remotely stitch UAV video images is analyzed. Second, the Scale Invariant Feature Transform (SIFT) algorithm is improved by re-dividing the pixel region and incorporating the vector correlation coefficient. Then, the matching results of the improved SIFT algorithm are compared with those of the traditional SIFT algorithm and Speed Up Robust Feature algorithm. Second, the Random Sample Consensus algorithm is introduced to match the video images accurately, and the matching accuracy of the converted images is verified. Finally, the improved image registration algorithm, the homography matrix estimation model based on convolutional neural network, and the conformation equation of the video sensor are combined to complete the video image stitching of UAVs. Also, the stitching quality of the video image is analyzed. The results show that the improved SIFT algorithm is better than the traditional SIFT algorithm in terms of correct matching and matching time. The visually transformed finely-matched image has a higher matching accuracy rate, and the combination of the two registration algorithms eliminates mismatch points effectively. Compared with the traditional stitching method, the video image stitching method proposed in this study has a higher structural similarity index and edge difference spectrum index, which is feasible and effective. The combination of image registration algorithm with new sensors and deep learning has great application potential in video image stitching.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-021-03840-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00170-021-07084-5,Machine learning and simulation-based surrogate modeling for improved process chain operation,The International Journal of Advanced Manufacturing Technology,10.1007/s00170-021-07084-5,Springer,2021-12-01,"In this contribution, a concept is presented that combines different simulation paradigms during the engineering phase. These methods are transferred into the operation phase by the use of data-based surrogates. As an virtual production scenario, the process combination of thermoforming continuous fiber-reinforced thermoplastic sheets and injection overmolding of thermoplastic polymers is investigated. Since this process is very sensitive regarding the temperature, the volatile transfer time is considered in a dynamic process chain control. Based on numerical analyses of the injection molding process, a surrogate model is developed. It enables a fast prediction of the product quality based on the temperature history. The physical model is transferred to an agent-based process chain simulation identifying lead time, bottle necks and quality rates taking into account the whole process chain. In the second step of surrogate modeling, a feasible soft sensor model is derived for quality control over the process chain during the operation stage. For this specific uses case, the production rejection can be reduced by 12% compared to conventional static approaches.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00170-021-07084-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00259-021-05445-6,How molecular imaging will enable robotic precision surgery,European Journal of Nuclear Medicine and Molecular Imaging,10.1007/s00259-021-05445-6,Springer,2021-12-01,"Molecular imaging is one of the pillars of precision surgery. Its applications range from early diagnostics to therapy planning, execution, and the accurate assessment of outcomes. In particular, molecular imaging solutions are in high demand in minimally invasive surgical strategies, such as the substantially increasing field of robotic surgery. This review aims at connecting the molecular imaging and nuclear medicine community to the rapidly expanding armory of surgical medical devices. Such devices entail technologies ranging from artificial intelligence and computer-aided visualization technologies (software) to innovative molecular imaging modalities and surgical navigation (hardware). We discuss technologies based on their role at different steps of the surgical workflow, i.e., from surgical decision and planning, over to target localization and excision guidance, all the way to (back table) surgical verification. This provides a glimpse of how innovations from the technology fields can realize an exciting future for the molecular imaging and surgery communities.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00259-021-05445-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-020-09794-9,A model for spectroscopic food sample analysis using data sonification,International Journal of Speech Technology,10.1007/s10772-020-09794-9,Springer,2021-12-01,"In this paper, we propose a method to generate an audio output based on spectroscopy data in order to discriminate two classes of data, based on the features of our spectral dataset. To do this, we first perform spectral pre-processing, and then extract features, followed by machine learning, for dimensionality reduction. The features are then mapped to the parameters of a sound synthesiser, as part of the audio processing, so as to generate audio samples in order to compute statistical results and identify important descriptors for the classification of the dataset. To optimise the process, we compare Amplitude Modulation (AM) and Frequency Modulation (FM) synthesis, as applied to two real-life datasets to evaluate the performance of sonification as a method for discriminating data. FM synthesis provides a higher subjective classification accuracy as compared with to AM synthesis. We then further compare the dimensionality reduction method of Principal Component Analysis (PCA) and Linear Discriminant Analysis in order to optimise our sonification algorithm. The results of classification accuracy using FM synthesis as the sound synthesiser and PCA as the dimensionality reduction method yields a mean classification accuracies of 93.81% and 88.57% for the coffee dataset and the fruit puree dataset respectively, and indicate that this spectroscopic analysis model is able to provide relevant information on the spectral data, and most importantly, is able to discriminate accurately between the two spectra and thus provides a complementary tool to supplement current methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-020-09794-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40860-021-00131-8,Elephant–railway conflict minimisation using real-time video data and machine learning,Journal of Reliable Intelligent Environments,10.1007/s40860-021-00131-8,Springer,2021-12-01,Elephant–train collision has been a major issue for both the railway as well as the forest departments. In this study real-time video data is analysed for detecting elephant to alert the train driver in case of elephants crossing the railway track in which the train is approaching. The HAAR feature extraction and adaptive boosting-based machine learning algorithm are used for detecting elephants from real-time video data. The experimental result shows the average precision of the proposed technique in detecting elephants using real-time video data is more than 96%.,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40860-021-00131-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10919-021-00375-1,Superior Communication of Positive Emotions Through Nonverbal Vocalisations Compared to Speech Prosody,Journal of Nonverbal Behavior,10.1007/s10919-021-00375-1,Springer,2021-12-01,"The human voice communicates emotion through two different types of vocalizations: nonverbal vocalizations (brief non-linguistic sounds like laughs) and speech prosody (tone of voice). Research examining recognizability of emotions from the voice has mostly focused on either nonverbal vocalizations or speech prosody, and included few categories of positive emotions. In two preregistered experiments, we compare human listeners’ (total n  = 400) recognition performance for 22 positive emotions from nonverbal vocalizations ( n  = 880) to that from speech prosody ( n  = 880). The results show that listeners were more accurate in recognizing most positive emotions from nonverbal vocalizations compared to prosodic expressions. Furthermore, acoustic classification experiments with machine learning models demonstrated that positive emotions are expressed with more distinctive acoustic patterns for nonverbal vocalizations as compared to speech prosody. Overall, the results suggest that vocal expressions of positive emotions are communicated more successfully when expressed as nonverbal vocalizations compared to speech prosody.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10919-021-00375-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11554-020-01063-x,Coding mode decision algorithm for fast HEVC transrating using heuristics and machine learning,Journal of Real-Time Image Processing,10.1007/s11554-020-01063-x,Springer,2021-12-01,"This article describes a framework to speed up the HEVC encoding decisions for on-demand transrating of bitstreams. The methods proposed collect information from a high-quality reference bitstream which after processing is used to limit the number of modes evaluated in subsequent re-encodings at different bitrates. In this way, the time required to process re-encode-time computing-intensive decisions, such as partitioning and motion estimation is significantly reduced. The methods proposed are a combination of heuristics with a statistical basis and fast decision techniques trained using automatic learning methodologies. Experimental results using the HEVC reference encoder show that jointly the methods proposed reduce the transcoding computational complexity by up to 78.8%, with Bjontegaard bitrate deltas penalties smaller than 1.06%. A comparison with related works showed that the proposed method is able to outperform state-of-the-art solutions in terms of combined rate-distortion–complexity performance indicators.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-020-01063-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1134/S0361768821080272,Optimization of Neural Network Training for Image Recognition Based on Trigonometric Polynomial Approximation,Programming and Computer Software,10.1134/S0361768821080272,Springer,2021-12-01,"Abstract The paper discusses optimization issues of training Artificial Neural Networks (ANNs) using a nonlinear trigonometric polynomial function. The proposed method presents the mathematical model of an ANN as an information transmission system where effective techniques to restore signals are widely used. To optimize ANN training, we use energy characteristics assuming ANNs as data transmission systems. We propose a nonlinear layer in the form of a trigonometric polynomial that approximates the “syncular” function based on the generalized approximation theorem and the wave model. To confirm the theoretical results, the efficiency of the proposed approach is compared with standard ANN implementations with sigmoid and Rectified Linear Unit (ReLU) activation functions. The experimental evaluation shows the same accuracy of standard ANNs with a time reduction of the training phase of supervised learning for the proposed model.",http://link.springer.com/openurl/fulltext?id=doi:10.1134/S0361768821080272,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.3103/S0147688221050117,An Intelligent Video Surveillance System for Human Behavior,Scientific and Technical Information Processing,10.3103/S0147688221050117,Springer,2021-12-01,"Abstract This paper describes an intelligent video surveillance system for human behavior and provides a brief overview of systems with similar functional characteristics. We present a collection of methods and algorithms for automatic recognition of human images, their identification, and analysis of behavior based on motor activity using a cascade of neural networks. A technology for capturing a person’s image and tracking their movements between video cameras is proposed.",http://link.springer.com/openurl/fulltext?id=doi:10.3103/S0147688221050117,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11673-021-10139-7,Semi-Automated Care: Video-Algorithmic Patient Monitoring and Surveillance in Care Settings,Journal of Bioethical Inquiry,10.1007/s11673-021-10139-7,Springer,2021-12-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11673-021-10139-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s43465-021-00511-5,Narrative Review on the Role of Imaging in DDH,Indian Journal of Orthopaedics,10.1007/s43465-021-00511-5,Springer,2021-12-01,"Background Developmental dysplasia of hip (DDH) represents a spectrum from acetabular dysplasia to fixed dislocation, giving disability through premature osteoarthritis. Most DDH cases continue to present without any known risk factors such as breech presentation, female sex, and family history. Incidence and population-based outcomes of DDH are difficult to reliably establish due to many DDH definitions and classifications using different types of examinations. Purpose This review takes a historical perspective on the role of imaging in DDH. Methods Pelvic radiographs (X-Ray) were amongst the first medical images identifying DDH, but these have a limited role in infancy due to absent ossification. In the 1980s, ultrasound led to a large expansion in infant DDH screening. Unfortunately, even for well-trained users, DDH indices on ultrasound generally lack reproducibility, and have led to overdiagnosis of mild DDH. CT and MRI more thoroughly evaluate the 3D hip deformity in DDH, but are costly, less available and involve radiation dose and/or anaesthesia. Results Recently 3D ultrasound has been used to characterize the 3D deformity of DDH more fully, with improved inter-observer reliability, particularly amongst novice users. 3D ultrasound is also well suited to automated image analysis, but high-resolution 3D probes are costly and not widely available. Conclusion Combining the latest handheld portable ultrasound probes and artificial intelligence analysis could lead to an inexpensive tool permitting practical mass population screening for DDH. Overall, our understanding of DDH is heavily influenced by the imaging tools used to visualize it and changing quickly with modern technology.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s43465-021-00511-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11063-021-10581-z,BLSTM and CNN Stacking Architecture for Speech Emotion Recognition,Neural Processing Letters,10.1007/s11063-021-10581-z,Springer,2021-12-01,"Speech Emotion Recognition (SER) is a huge challenge for distinguishing and interpreting the sentiments carried in speech. Fortunately, deep learning is proved to have great ability to deal with acoustic features. For instance, Bidirectional Long Short Term Memory (BLSTM) has an advantage of solving time series acoustic features and Convolutional Neural Network (CNN) can discover the local structure among different features. This paper proposed the BLSTM and CNN Stacking Architecture (BCSA) to enhance the ability to recognition emotions. In order to match the input formats of BLSTM and CNN, slicing feature matrices is necessary. For utilizing the different roles of the BLSTM and CNN, the Stacking is employed to integrate the BLSTM and CNN. In detail, taking into account overfitting problem, the estimates of probabilistic quantities from BLSTM and CNN are combined as new data using K-fold cross validation. Finally, based on the Stacking models, the logistic regression is used to recognize emotions effectively by fitting the new data. The experiment results demonstrate that the performance of proposed architecture is better than that of single model. Furthermore, compared with the state-of-the-art model on SER in our knowledge, the proposed method BCSA may be more suitable for SER by integrating time series acoustic features and the local structure among different features.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11063-021-10581-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12530-021-09397-y,3D convolution neural network-based person identification using gait cycles,Evolving Systems,10.1007/s12530-021-09397-y,Springer,2021-12-01,"Human identification plays a prominent role in terms of security. In modern times security is becoming the key term for an individual or a country, especially for countries which are facing internal or external threats. Gait analysis is interpreted as the systematic study of the locomotive in humans. It can be used to extract the exact walking features of individuals. Walking features depends on biological as well as the physical feature of the object; hence, it is unique to every individual. In this work, gait features are used to identify an individual. The steps involve object detection, background subtraction, silhouettes extraction, skeletonization, and training 3D Convolution Neural Network (3D-CNN) on these gait features. The model is trained and evaluated on the dataset acquired by CASIA—B Gait, which consists of 15,000 videos of 124 subjects’ walking pattern captured from 11 different angles carrying objects such as bag and coat. The proposed method focuses more on the lower body part to extract features such as the angle between knee and thighs, hip angle, angle of contact, and many other features. The experimental results are compared with amongst accuracies of silhouettes as datasets for training and skeletonized image as training data. The results show that extracting the information from skeletonized data yields improved accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12530-021-09397-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06233-x,Non-iterative online sequential learning strategy for autoencoder and classifier,Neural Computing and Applications,10.1007/s00521-021-06233-x,Springer,2021-12-01,"Artificial neural network training algorithms aim to optimize the network parameters regarding the pre-defined cost function. Gradient-based artificial neural network training algorithms support iterative learning and have gained immense popularity for training different artificial neural networks end-to-end. However, training through gradient methods is time-consuming. Another family of training algorithms is based on the Moore–Penrose inverse, which is much faster than many other gradient methods. Nevertheless, most of those algorithms are non-iterative and thus do not support mini-batch learning in nature. This work extends two non-iterative Moore–Penrose inverse-based training algorithms to enable online sequential learning: a single-hidden-layer autoencoder training algorithm and a sub-network-based classifier training algorithm. We further present an approach that uses the proposed autoencoder for self-supervised dimension reduction and then uses the proposed classifier for supervised classification. The experimental results show that the proposed approach achieves satisfactory classification accuracy on many benchmark datasets with extremely low time consumption (up to 50 times faster than the support vector machine on CIFAR 10 dataset).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06233-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10844-021-00658-5,Music emotion recognition using recurrent neural networks and pretrained models,Journal of Intelligent Information Systems,10.1007/s10844-021-00658-5,Springer,2021-12-01,"The article presents conducted experiments using recurrent neural networks for emotion detection in musical segments. Trained regression models were used to predict the continuous values of emotions on the axes of Russell’s circumplex model. A process of audio feature extraction and creating sequential data for learning networks with long short-term memory (LSTM) units is presented. Models were implemented using the WekaDeeplearning4j package and a number of experiments were carried out with data with different sets of features and varying segmentation. The usefulness of dividing the data into sequences as well as the point of using recurrent networks to recognize emotions in music, the results of which have even exceeded the SVM algorithm for regression, were demonstrated. The author analyzed the effect of the network structure and the set of used features on the results of the regressors recognizing values on two axes of the emotion model: arousal and valence. Finally, the use of a pretrained model for processing audio features and training a recurrent network with new sequences of features is presented.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10844-021-00658-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12369-021-00842-1,Video Captioning Based on Both Egocentric and Exocentric Views of Robot Vision for Human-Robot Interaction,International Journal of Social Robotics,10.1007/s12369-021-00842-1,Springer,2021-11-30,"Robot vision provides the most important information to robots so that they can read the context and interact with human partners successfully. Moreover, to allow humans recognize the robot’s visual understanding during human-robot interaction (HRI), the best way is for the robot to provide an explanation of its understanding in natural language. In this paper, we propose a new approach by which to interpret robot vision from an egocentric standpoint and generate descriptions to explain egocentric videos particularly for HRI. Because robot vision equals to egocentric video on the robot’s side, it contains as much egocentric view information as exocentric view information. Thus, we propose a new dataset, referred to as the global, action, and interaction (GAI) dataset, which consists of egocentric video clips and GAI descriptions in natural language to represent both egocentric and exocentric information. The encoder-decoder based deep learning model is trained based on the GAI dataset and its performance on description generation assessments is evaluated. We also conduct experiments in actual environments to verify whether the GAI dataset and the trained deep learning model can improve a robot vision system",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12369-021-00842-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10994-021-06112-5,Bimodal variational autoencoder for audiovisual speech recognition,Machine Learning,10.1007/s10994-021-06112-5,Springer,2021-11-24,"Multimodal fusion is the idea of combining information in a joint representation of multiple modalities. The goal of multimodal fusion is to improve the accuracy of results from classification or regression tasks. This paper proposes a Bimodal Variational Autoencoder (BiVAE) model for audiovisual features fusion. Reliance on audiovisual signals in a speech recognition task increases the recognition accuracy, especially when an audio signal is corrupted. The BiVAE model is trained and validated on the CUAVE dataset. Three classifiers have evaluated the fused audiovisual features: Long-short Term Memory, Deep Neural Network, and Support Vector Machine. The experiment involves the evaluation of the fused features in the case of whether two modalities are available or there is only one modality available (i.e., cross-modality). The experimental results display the superiority of the proposed model (BiVAE) of audiovisual features fusion over the state-of-the-art models by an average accuracy difference $$\simeq $$ ≃  3.28% and 13.28% for clean and noisy, respectively. Additionally, BiVAE outperforms the state-of-the-art models in the case of cross-modality by an accuracy difference $$\simeq $$ ≃  2.79% when the only audio signal is available and 1.88% when the only video signal is available. Furthermore, SVM satisfies the best recognition accuracy compared with other classifiers.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10994-021-06112-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s13244-021-01117-z,Saliency-based 3D convolutional neural network for categorising common focal liver lesions on multisequence MRI,Insights into Imaging,10.1186/s13244-021-01117-z,Springer,2021-11-24,"Background The imaging features of focal liver lesions (FLLs) are diverse and complex. Diagnosing FLLs with imaging alone remains challenging. We developed and validated an interpretable deep learning model for the classification of seven categories of FLLs on multisequence MRI and compared the differential diagnosis between the proposed model and radiologists. Methods In all, 557 lesions examined by multisequence MRI were utilised in this retrospective study and divided into training–validation ( n  = 444) and test ( n  = 113) datasets. The area under the receiver operating characteristic curve (AUC) was calculated to evaluate the performance of the model. The accuracy and confusion matrix of the model and individual radiologists were compared. Saliency maps were generated to highlight the activation region based on the model perspective. Results The AUC of the two- and seven-way classifications of the model were 0.969 (95% CI 0.944–0.994) and from 0.919 (95% CI 0.857–0.980) to 0.999 (95% CI 0.996–1.000), respectively. The model accuracy (79.6%) of the seven-way classification was higher than that of the radiology residents (66.4%, p  = 0.035) and general radiologists (73.5%, p  = 0.346) but lower than that of the academic radiologists (85.4%, p  = 0.291). Confusion matrices showed the sources of diagnostic errors for the model and individual radiologists for each disease. Saliency maps detected the activation regions associated with each predicted class. Conclusion This interpretable deep learning model showed high diagnostic performance in the differentiation of FLLs on multisequence MRI. The analysis principle contributing to the predictions can be explained via saliency maps.",http://link.springer.com/openurl/fulltext?id=doi:10.1186/s13244-021-01117-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10586-021-03439-5,An explainable and efficient deep learning framework for video anomaly detection,Cluster Computing,10.1007/s10586-021-03439-5,Springer,2021-11-23,"Deep learning-based video anomaly detection methods have drawn significant attention in the past few years due to their superior performance. However, almost all the leading methods for video anomaly detection rely on large-scale training datasets with long training times. As a result, many real-world video analysis tasks are still not applicable for fast deployment. On the other hand, the leading methods cannot provide interpretability due to the uninterpretable feature representations hiding the decision-making process when anomaly detection models are considered as a black box. However, the interpretability for anomaly detection is crucial since the corresponding response to the anomalies in the video is determined by their severity and nature. To tackle these problems, this paper proposes an efficient deep learning framework for video anomaly detection and provides explanations. The proposed framework uses pre-trained deep models to extract high-level concept and context features for training denoising autoencoder (DAE), requiring little training time (i.e., within 10 s on UCSD Pedestrian datasets) while achieving comparable detection performance to the leading methods. Furthermore, this framework presents the first video anomaly detection use of combing autoencoder and SHapley Additive exPlanations (SHAP) for model interpretability. The framework can explain each anomaly detection result in surveillance videos. In the experiments, we evaluate the proposed framework's effectiveness and efficiency while also explaining anomalies behind the autoencoder’s prediction. On the USCD Pedestrian datasets, the DAE achieved 85.9% AUC with a training time of 5 s on the USCD Ped1 and 92.4% AUC with a training time of 2.9 s on the UCSD Ped2.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10586-021-03439-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s13195-021-00924-2,Improving 3D convolutional neural network comprehensibility via interactive visualization of relevance maps: evaluation in Alzheimer’s disease,Alzheimer's Research & Therapy,10.1186/s13195-021-00924-2,BioMed Central,2021-11-23,"Background Although convolutional neural networks (CNNs) achieve high diagnostic accuracy for detecting Alzheimer’s disease (AD) dementia based on magnetic resonance imaging (MRI) scans, they are not yet applied in clinical routine. One important reason for this is a lack of model comprehensibility. Recently developed visualization methods for deriving CNN relevance maps may help to fill this gap as they allow the visualization of key input image features that drive the decision of the model. We investigated whether models with higher accuracy also rely more on discriminative brain regions predefined by prior knowledge. Methods We trained a CNN for the detection of AD in N = 663 T1-weighted MRI scans of patients with dementia and amnestic mild cognitive impairment (MCI) and verified the accuracy of the models via cross-validation and in three independent samples including in total N = 1655 cases. We evaluated the association of relevance scores and hippocampus volume to validate the clinical utility of this approach. To improve model comprehensibility, we implemented an interactive visualization of 3D CNN relevance maps, thereby allowing intuitive model inspection. Results Across the three independent datasets, group separation showed high accuracy for AD dementia versus controls ( AUC ≥ 0.91) and moderate accuracy for amnestic MCI versus controls ( AUC ≈ 0.74). Relevance maps indicated that hippocampal atrophy was considered the most informative factor for AD detection, with additional contributions from atrophy in other cortical and subcortical regions. Relevance scores within the hippocampus were highly correlated with hippocampal volumes (Pearson’s r ≈ −0.86, p < 0.001). Conclusion The relevance maps highlighted atrophy in regions that we had hypothesized a priori. This strengthens the comprehensibility of the CNN models, which were trained in a purely data-driven manner based on the scans and diagnosis labels. The high hippocampus relevance scores as well as the high performance achieved in independent samples support the validity of the CNN models in the detection of AD-related MRI abnormalities. The presented data-driven and hypothesis-free CNN modeling approach might provide a useful tool to automatically derive discriminative features for complex diagnostic tasks where clear clinical criteria are still missing, for instance for the differential diagnosis between various types of dementia.",https://www.biomedcentral.com/openurl?doi=10.1186/s13195-021-00924-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s12938-021-00951-y,A mobile-assisted voice condition analysis system for Parkinson’s disease: assessment of usability conditions,BioMedical Engineering OnLine,10.1186/s12938-021-00951-y,BioMed Central,2021-11-21,"Background and objective Automatic voice condition analysis systems to detect Parkinson’s disease (PD) are generally based on speech data recorded under acoustically controlled conditions and professional supervision. The performance of these approaches in a free-living scenario is unknown. The aim of this research is to investigate the impact of uncontrolled conditions (realistic acoustic environment and lack of supervision) on the performance of automatic PD detection systems based on speech. Methods A mobile-assisted voice condition analysis system is proposed to aid in the detection of PD using speech. The system is based on a server–client architecture. In the server, feature extraction and machine learning algorithms are designed and implemented to discriminate subjects with PD from healthy ones. The Android app allows patients to submit phonations and physicians to check the complete record of every patient. Six different machine learning classifiers are applied to compare their performance on two different speech databases. One of them is an in-house database (UEX database), collected under professional supervision by using the same Android-based smartphone in the same room, whereas the other one is an age, sex and health-status balanced subset of mPower study for PD, which provides real-world data. By applying identical methodology, single-database experiments have been performed on each database, and also cross-database tests. Cross-validation has been applied to assess generalization performance and hypothesis tests have been used to report statistically significant differences. Results In the single-database experiments, a best accuracy rate of 0.92 (AUC = 0.98) has been obtained on UEX database, while a considerably lower best accuracy rate of 0.71 (AUC = 0.76) has been achieved using the mPower-based database. The cross-database tests provided very degraded accuracy metrics. Conclusion The results clearly show the potential of the proposed system as an aid for general practitioners to conduct triage or an additional tool for neurologists to perform diagnosis. However, due to the performance degradation observed using data from mPower study, semi-controlled conditions are encouraged, i.e., voices recorded at home by the patients themselves following a strict recording protocol and control of the information about patients by the medical doctor at charge.",https://www.biomedcentral.com/openurl?doi=10.1186/s12938-021-00951-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40842-021-00134-7,Type 2 diabetes reversal with digital twin technology-enabled precision nutrition and staging of reversal: a retrospective cohort study,Clinical Diabetes and Endocrinology,10.1186/s40842-021-00134-7,BioMed Central,2021-11-15,"Background Type 2 diabetes reversal has been viewed in the literature primarily as a dichotomous event (reversed or not reversed), even though this viewpoint may not be optimal for clinicians or patients. This cohort study’s objectives were to define stages of type 2 diabetes reversal and measure changes in reversal stages before and after 90 days of digital twin-enabled precision nutrition therapy. Methods This study defines seven stages of diabetes reversal. The study is a retrospective pre/post comparison of changes in reversal stage, hemoglobin A1c (HbA1c), weight, body mass index (BMI), and other metrics measured before and after precision nutrition therapy. Reversal stages were defined as Stage 0: HbA1c < 5.7% without medication for > 1 year, Stage 1: HbA1c < 5.7% without medication for < 1 year, Stage 2: HbA1c < 6.5% without medication, Stage 3: estimated HbA1c (eA1c) between 5.7 and 6.4% without medication, Stage 4: estimated HbA1c (eA1c) between 5.7 and 6.4% with metformin monotherapy, Stage 5: dual oral therapy, Stage 6: > = 3 medications. Results Reversal stage information was available for 463 patients at baseline and 90 days. At baseline, the proportions of patients in each reversal stage were Stages 1 and 2: 0%, Stage 3: 1%, Stage 4: 8%, Stage 5: 6%, and Stage 6: 85%. After 90 days, the proportions in each reversal stage were Stage 1: 2%, Stage 2: 9%, Stage 3: 32%, Stage 4: 39%, Stage 5: 7%, and Stage 6: 11%, indicating significant progress. Reversal stage progression rates varied by patient subgroup. Conclusions Type 2 diabetes patients reached differing reversal stages during 90 days of precision nutrition therapy. Use of reversal stages may benefit patients during therapy. Trial registration This was a retrospective study that was approved by the Medisys Clinisearch Ethical Review Board (without registration number) in 2019.",https://www.biomedcentral.com/openurl?doi=10.1186/s40842-021-00134-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12517-021-08909-z,Retraction Note to: Image recognition of coastal environment and aerobics sports based on remote sensing images based on deep learning,Arabian Journal of Geosciences,10.1007/s12517-021-08909-z,Springer,2021-11-12,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12517-021-08909-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-021-01313-3,Machining feature recognition based on deep neural networks to support tight integration with 3D CAD systems,Scientific Reports,10.1038/s41598-021-01313-3,Nature,2021-11-12,"Recently, studies applying deep learning technology to recognize the machining feature of three-dimensional (3D) computer-aided design (CAD) models are increasing. Since the direct utilization of boundary representation (B-rep) models as input data for neural networks in terms of data structure is difficult, B-rep models are generally converted into a voxel, mesh, or point cloud model and used as inputs for neural networks for the application of 3D models to deep learning. However, the model’s resolution decreases during the format conversion of 3D models, causing the loss of some features or difficulties in identifying areas of the converted model corresponding to a specific face of the B-rep model. To solve these problems, this study proposes a method enabling tight integration of a 3D CAD system with a deep neural network using feature descriptors as inputs to neural networks for recognizing machining features. Feature descriptor denotes an explicit representation of the main property items of a face. We constructed 2236 data to train and evaluate the deep neural network. Of these, 1430 were used for training the deep neural network, and 358 were used for validation. And 448 were used to evaluate the performance of the trained deep neural network. In addition, we conducted an experiment to recognize a total of 17 types (16 types of machining features and a non-feature) from the B-rep model, and the types for all 75 test cases were successfully recognized.",https://www.nature.com/articles/s41598-021-01313-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10055-021-00590-7,Intelligence augmentation: rethinking the future of work by leveraging human performance and abilities,Virtual Reality,10.1007/s10055-021-00590-7,Springer,2021-11-09,"Nowadays, digitalization has an immense impact on the landscape of jobs. This technological revolution creates new industries and professions, promises greater efficiency and improves the quality of working life. However, emerging technologies such as robotics and artificial intelligence (AI) are reducing human intervention, thus advancing automation and eliminating thousands of jobs and whole occupational images. To prepare employees for the changing demands of work, adequate and timely training of the workforce and real-time support of workers in new positions is necessary. Therefore, it is investigated whether user-oriented technologies, such as augmented reality (AR) and virtual reality (VR) can be applied “on-the-job” for such training and support—also known as intelligence augmentation (IA). To address this problem, this work synthesizes results of a systematic literature review as well as a practically oriented search on augmented reality and virtual reality use cases within the IA context. A total of 150 papers and use cases are analyzed to identify suitable areas of application in which it is possible to enhance employees' capabilities. The results of both, theoretical and practical work, show that VR is primarily used to train employees without prior knowledge, whereas AR is used to expand the scope of competence of individuals in their field of expertise while on the job. Based on these results, a framework is derived which provides practitioners with guidelines as to how AR or VR can support workers at their job so that they can keep up with anticipated skill demands. Furthermore, it shows for which application areas AR or VR can provide workers with sufficient training to learn new job tasks. By that, this research provides practical recommendations in order to accompany the imminent distortions caused by AI and similar technologies and to alleviate associated negative effects on the German labor market.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-021-00590-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-021-03558-2,Head movements for behavior recognition from real time video based on deep learning ConvNet transfer learning,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-021-03558-2,Springer,2021-11-06,"Human Behavior recognition is prominently emerging topic in the current computer vision era which is major part of work considered under pose estimation. Also, understanding human behavior in images and video is a crucial part which gives more attention in present generation. Behavior or activity recognition involves in tracking the different poses of human which will be useful for activity, gesture or gait identification. Every time, we use the term either activity or behavior to specify the movements observed in human body. Activity is the process of doing some sort of physical work whereas, behavior involves collection of different actions as well as recurring actions. More formally, action can be defined as movements made by humans to complete a work. Behavior, instead informs the overall response of gestures made by individual. Now a days, activity recognition is useful in enhancing the security surveillance, for understanding body signs, sports activity monitoring, students interactiveness checking, industrial environments for employee’s safety and in health care for patients monitoring. In this work, we consider the head movements as it provides important social and communication signals which are helpful for behavior recognition. We applied CNN with and without transfer learning model for predicting the head movements and compared the results of both models. For evaluation, we have also experimented model with CNN SVM. The System provided accuracy of $$\approx$$ ≈ 98.97% using the models developed.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-021-03558-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40708-021-00146-0,Classifying the tracing difficulty of 3D neuron image blocks based on deep learning,Brain Informatics,10.1186/s40708-021-00146-0,Springer,2021-11-05,"Quickly and accurately tracing neuronal morphologies in large-scale volumetric microscopy data is a very challenging task. Most automatic algorithms for tracing multi-neuron in a whole brain are designed under the Ultra-Tracer framework, which begins the tracing of a neuron from its soma and traces all signals via a block-by-block strategy. Some neuron image blocks are easy for tracing and their automatic reconstructions are very accurate, and some others are difficult and their automatic reconstructions are inaccurate or incomplete. The former are called low Tracing Difficulty Blocks (low-TDBs), while the latter are called high Tracing Difficulty Blocks (high-TDBs). We design a model named 3D-SSM to classify the tracing difficulty of 3D neuron image blocks, which is based on 3D Residual neural Network (3D-ResNet), Fully Connected Neural Network (FCNN) and Long Short-Term Memory network (LSTM). 3D-SSM contains three modules: Structure Feature Extraction (SFE), Sequence Information Extraction (SIE) and Model Fusion (MF). SFE utilizes a 3D-ResNet and a FCNN to extract two kinds of features in 3D image blocks and their corresponding automatic reconstruction blocks. SIE uses two LSTMs to learn sequence information hidden in 3D image blocks. MF adopts a concatenation operation and a FCNN to combine outputs from SIE. 3D-SSM can be used as a stop condition of an automatic tracing algorithm in the Ultra-Tracer framework. With its help, neuronal signals in low-TDBs can be traced by the automatic algorithm and in high-TDBs may be reconstructed by annotators. 12732 training samples and 5342 test samples are constructed on neuron images of a whole mouse brain. The 3D-SSM achieves classification accuracy rates 87.04% on the training set and 84.07% on the test set. Furthermore, the trained 3D-SSM is tested on samples from another whole mouse brain and its accuracy rate is 83.21%.",http://link.springer.com/openurl/fulltext?id=doi:10.1186/s40708-021-00146-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-021-09916-x,Handling high dimensional features by ensemble learning for emotion identification from speech signal,International Journal of Speech Technology,10.1007/s10772-021-09916-x,Springer,2021-11-02,"In the recent past , handling the curse of dimensionality observed in acoustic features of the speech signal in machine learning-based emotion detection has been considered a crucial objective. The contemporary emotion prediction methods are experiencing false alarming due to the high dimensionality of the features used in training phase of the machine learning models. The majority of the contemporary models have endeavored to handle the curse of high dimensionality of the training corpus. However, the contemporary models are focusing more on using fusion of multiple classifiers, which is barely improvising the decision accuracy, if the volume of the training corpus is high. The contribution of this manuscript endeavored to portray a novel ensemble model that using fusion of diversity measures to suggest the optimal features. Moreover, the proposed method attempts to reduce the impact of the high dimensionality in feature values by using a novel clustering process. The experimental study signifies the proposed method performance in term of emotion prediction from speech signals and compared to contemporary models of emotion detection using machine learning. The fourfold cross-validation of standard data corpus has used in performance analysis.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-021-09916-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40658-021-00423-1,Application of dual-stream 3D convolutional neural network based on ^18F-FDG PET/CT in distinguishing benign and invasive adenocarcinoma in ground-glass lung nodules,EJNMMI Physics,10.1186/s40658-021-00423-1,Springer,2021-11-02,"Purpose This work aims to train, validate, and test a dual-stream three-dimensional convolutional neural network (3D-CNN) based on fluorine 18 (^18F)-fluorodeoxyglucose (FDG) PET/CT to distinguish benign lesions and invasive adenocarcinoma (IAC) in ground-glass nodules (GGNs). Methods We retrospectively analyzed patients with suspicious GGNs who underwent ^18F-FDG PET/CT in our hospital from November 2011 to November 2020. The patients with benign lesions or IAC were selected for this study. According to the ratio of 7:3, the data were randomly divided into training data and testing data. Partial image feature extraction software was used to segment PET and CT images, and the training data after using the data augmentation were used for the training and validation (fivefold cross-validation) of the three CNNs (PET, CT, and PET/CT networks). Results A total of 23 benign nodules and 92 IAC nodules from 106 patients were included in this study. In the training set, the performance of PET network (accuracy, sensitivity, and specificity of 0.92 ± 0.02, 0.97 ± 0.03, and 0.76 ± 0.15) was better than the CT network (accuracy, sensitivity, and specificity of 0.84 ± 0.03, 0.90 ± 0.07, and 0.62 ± 0.16) (especially accuracy was significant, P -value was 0.001); in the testing set, the performance of both networks declined. However, the accuracy and sensitivity of PET network were still higher than that of CT network (0.76 vs. 0.67; 0.85 vs. 0.70). For dual-stream PET/CT network, its performance was almost the same as PET network in the training set ( P -value was 0.372–1.000), while in the testing set, although its performance decreased, the accuracy and sensitivity (0.85 and 0.96) were still higher than both CT and PET networks. Moreover, the accuracy of PET/CT network was higher than two nuclear medicine physicians [physician 1 (3-year experience): 0.70 and physician 2 (10-year experience): 0.73]. Conclusion The 3D-CNN based on ^18F-FDG PET/CT can be used to distinguish benign lesions and IAC in GGNs, and the performance is better when both CT and PET images are used together.",https://www.biomedcentral.com/openurl?doi=10.1186/s40658-021-00423-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s40708-021-00144-2,3D convolutional neural networks-based multiclass classification of Alzheimer’s and Parkinson’s diseases using PET and SPECT neuroimaging modalities,Brain Informatics,10.1186/s40708-021-00144-2,Springer,2021-11-02,"Background Alzheimer’s disease (AD) is a neurodegenerative brain pathology formed due to piling up of amyloid proteins, development of plaques and disappearance of neurons. Another common subtype of dementia like AD, Parkinson’s disease (PD) is determined by the disappearance of dopaminergic neurons in the region known as substantia nigra pars compacta located in the midbrain. Both AD and PD target aged population worldwide forming a major chunk of healthcare costs. Hence, there is a need for methods that help in the early diagnosis of these diseases. PD subjects especially those who have confirmed postmortem plaque are a strong candidate for a second AD diagnosis. Modalities such as positron emission tomography (PET) and single photon emission computed tomography (SPECT) can be combined with deep learning methods to diagnose these two diseases for the benefit of clinicians. Result In this work, we deployed a 3D Convolutional Neural Network (CNN) to extract features for multiclass classification of both AD and PD in the frequency and spatial domains using PET and SPECT neuroimaging modalities to differentiate between AD, PD and Normal Control (NC) classes. Discrete Cosine Transform has been deployed as a frequency domain learning method along with random weak Gaussian blurring and random zooming in/out augmentation methods in both frequency and spatial domains. To select the hyperparameters of the 3D-CNN model, we deployed both 5- and 10-fold cross-validation (CV) approaches. The best performing model was found to be AD/NC(SPECT)/PD classification with random weak Gaussian blurred augmentation in the spatial domain using fivefold CV approach while the worst performing model happens to be AD/NC(PET)/PD classification without augmentation in the frequency domain using tenfold CV approach. We also found that spatial domain methods tend to perform better than their frequency domain counterparts. Conclusion The proposed model provides a good performance in discriminating AD and PD subjects due to minimal correlation between these two dementia types on the clinicopathological continuum between AD and PD subjects from a neuroimaging perspective.",http://link.springer.com/openurl/fulltext?id=doi:10.1186/s40708-021-00144-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-10809-z,Design of an intelligent video surveillance system for crime prevention: applying deep learning technology,Multimedia Tools and Applications,10.1007/s11042-021-10809-z,Springer,2021-11-01,"As the security threat and crime rate have been increased all over the globe, the video surveillance system using closed-circuit television (CCTV) has become an essential tool for many security-related applications and is widely used in many areas as a monitoring system. However, most of the data collected by the video surveillance system is used as evidence of objective data after crime and disaster have occurred. And, often time, video surveillance systems tend to be used in a passive manner due to the high cost and human resources. The video surveillance system should actively respond to detect crime and accidents in advance through real-time monitoring and immediately transmit data in case of an accident. This study proposes developing an intelligent video surveillance system that can actively monitor in real-time without human input. In solving the problems of the existing video surveillance system, deep learning technology will be carried through the data processing model design to visualize data for crime detection after building an artificial intelligence server and video surveillance camera. In addition, this design proposes an intelligent surveillance system to quickly and effectively detect crimes by sending a video image and notification message to the web through real-time processing.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-10809-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-10931-y,Correction to: Design of an intelligent video surveillance system for crime prevention: applying deep learning technology,Multimedia Tools and Applications,10.1007/s11042-021-10931-y,Springer,2021-11-01,"In the original publication, copyright holder name was incorrect. The author regrets this mistake. The original article has been corrected.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-10931-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11713-2,Real-time 2D/ 3D image processing with deep learning,Multimedia Tools and Applications,10.1007/s11042-021-11713-2,Springer,2021-11-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11713-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-020-08818-5,An analysis of environmental big data through the establishment of emotional classification system model based on machine learning: focus on multimedia contents for portal applications,Multimedia Tools and Applications,10.1007/s11042-020-08818-5,Springer,2021-11-01,"With the advent of the Internet, there have been many changes. As existing means of off-line communication have transferred to the Internet, a wide range of multimedia services are emerging. And unlike in the past, these work as tremendous sources from which people’s opinions and attitudes can be obtained. Particularly, diverse opinions and reviews posted on the web in real time help corporations or the State establish the directions of policies. In this context, this study developed machine learning-based environment issue sentiment classifier in order to find out, from comments posted below news, how people recognize issues about climate change in terms of environment. Based on training data constructed by this study, a sentiment classification algorithm was constructed by applying SVM (support vector machine) and Naive Bayes, which are machine learning techniques, and a sentiment classifier was constructed by applying CNN (convolutional neural networks) and Bi-LSTM (bidirectional long-short term memory) techniques among deep learning techniques recently researched vigorously; and then their performance was compared.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-08818-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00464-020-08110-5,Deep learning for surgical phase recognition using endoscopic videos,Surgical Endoscopy,10.1007/s00464-020-08110-5,Springer,2021-11-01,"Background Operating room planning is a complex task as pre-operative estimations of procedure duration have a limited accuracy. This is due to large variations in the course of procedures. Therefore, information about the progress of procedures is essential to adapt the daily operating room schedule accordingly. This information should ideally be objective, automatically retrievable and in real-time. Recordings made during endoscopic surgeries are a potential source of progress information. A trained observer is able to recognize the ongoing surgical phase from watching these videos. The introduction of deep learning techniques brought up opportunities to automatically retrieve information from surgical videos. The aim of this study was to apply state-of-the art deep learning techniques on a new set of endoscopic videos to automatically recognize the progress of a procedure, and to assess the feasibility of the approach in terms of performance, scalability and practical considerations. Methods A dataset of 33 laparoscopic cholecystectomies (LC) and 35 total laparoscopic hysterectomies (TLH) was used. The surgical tools that were used and the ongoing surgical phases were annotated in the recordings. Neural networks were trained on a subset of annotated videos. The automatic recognition of surgical tools and phases was then assessed on another subset. The scalability of the networks was tested and practical considerations were kept up. Results The performance of the surgical tools and phase recognition reached an average precision and recall between 0.77 and 0.89. The scalability tests showed diverging results. Legal considerations had to be taken into account and a considerable amount of time was needed to annotate the datasets. Conclusion This study shows the potential of deep learning to automatically recognize information contained in surgical videos. This study also provides insights in the applicability of such a technique to support operating room planning.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00464-020-08110-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12083-021-01146-x,Deep reinforcement learning-based incentive mechanism design for short video sharing through D2D communication,Peer-to-Peer Networking and Applications,10.1007/s12083-021-01146-x,Springer,2021-11-01,"With the development of 5th generation (5G) wireless communication networks and the popularity of short video applications, there has been a rapid increase in short video traffic in cellular networks. Device-to-device (D2D) communication-based short video sharing is considered to be an effective way to offload traffic from cellular networks. Due to the selfish nature of mobile user equipment (MUEs), how to dynamically motivate MUEs to engage in short video sharing while ensuring the Quality of Service, which makes it critical to design an appropriate incentive mechanism. In this paper, we firstly analyze the rationale for dynamically setting rewards and penalties and then define the rewards and penalties setting dynamically for maximizing the utility of the mobile edge computing server (RPSDMU) problem. The problem is proved NP-hard. Furthermore, we formulate the dynamic incentive process as the Markov Decision Process problem. Considering the complexity and dynamics of the problem, we design a Dynamic Incentive Mechanism algorithm of D2D-based Short Video Sharing based on Asynchronous Advantage Actor-Critic (DIM-A3C) to solve the problem. Simulation results show that the proposed dynamic incentive mechanism can increase the utility of mobile edge computing server by an average of 22% and 16% compared with the existing proportional incentive mechanism (PIM) and scoring-based incentive mechanism (SIM). Meanwhile, DIM-A3C achieves a higher degree of satisfaction than PIM and SIM.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12083-021-01146-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11027-3,A pose estimation scheme based on distance scaling algorithm in real-time environment,Multimedia Tools and Applications,10.1007/s11042-021-11027-3,Springer,2021-11-01,The innovation of convolutional neural networks motivated the study on the many aspects of image processing and object detection. One topic of interest in the field of object detection is human detection and the human pose estimation scheme. Human pose estimation scheme enables computer vision with a higher accuracy of identifying human motion and movement. Pose estimation scheme has been applied in making 2D images into 3D representation. This paper aims to determine the distance between the camera and the human subject with real-time application which takes advantage of existing pose estimation schemes. The distance scaling applied in the paper showed high performance of about 0.27 m error from the actual distance.,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11027-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-021-01924-3,A spatial-frequency-temporal 3D convolutional neural network for motor imagery EEG signal classification,"Signal, Image and Video Processing",10.1007/s11760-021-01924-3,Springer,2021-11-01,"Motor imagery (MI) EEG signal classification is a critical issue for brain–computer interface (BCI) systems. In traditional MI EEG machine learning algorithms, feature extraction and classification often have different objective functions, thus resulting in information loss. To solve this problem, a novel spatial-frequency-temporal (SFT) 3D CNN model is proposed. Specifically, the energies of EEG signals located in multiple local SFT ranges are extracted to obtain a novel 3D MI EEG feature representation, and a novel 3D CNN model is designed to simultaneously learn the complex MI EEG features in the entire SFT domains and carry out classification. An extensive experimental study is implemented on two public EEG datasets to evaluate the effectiveness of our method. For BCI Competition III Dataset IVa, the average accuracy rate of five subjects obtained by the proposed method reaches 86.6% and yields 4.1% improvement over the state-of-the-art filter band common spatial pattern (FBCSP) method. For BCI Competition III dataset IIIa, by achieving an average accuracy rate of 91.85%, the proposed method outperforms the state-of-the-art dictionary pair learning (DPL) method by 4.44%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-021-01924-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06201-5,Deep learning-based bird eye view social distancing monitoring using surveillance video for curbing the COVID-19 spread,Neural Computing and Applications,10.1007/s00521-021-06201-5,Springer,2021-11-01,"The escalating transmission intensity of COVID-19 pandemic is straining the healthcare systems worldwide. Due to the unavailability of effective pharmaceutical treatment and vaccines, monitoring social distancing is the only viable tool to strive against asymptomatic transmission. Pertaining to the need of monitoring the social distancing at populated areas, a novel bird eye view computer vision-based framework implementing deep learning and utilizing surveillance video is proposed. This proposed method employs YOLO v3 object detection model and uses key point regressor to detect the key feature points. Additionally, as the massive crowd is detected, the bounding boxes on objects are received, and red boxes are also visible if social distancing is violated. When empirically tested over real-time data, proposed method is established to be efficacious than the existing approaches in terms of inference time and frame rate.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06201-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-021-01503-6,Deep Learning Geometry Compression Artifacts Removal for Video-Based Point Cloud Compression,International Journal of Computer Vision,10.1007/s11263-021-01503-6,Springer,2021-11-01,"Point cloud is an essential format for three-dimensional (3-D) object modelling and interaction in Augmented Reality and Virtual Reality applications. In the current state of the art video-based point cloud compression (V-PCC), a dynamic point cloud is projected onto geometry and attribute videos patch by patch, each represented by its texture, depth, and occupancy map for reconstruction. To deal with occlusion, each patch is projected onto near and far depth fields in the geometry video. Once there are artifacts on the compressed two-dimensional (2-D) geometry video, they would be propagated to the 3-D point-cloud frames. In addition, in the lossy compression, there always exists a tradeoff between the rate of bitstream and distortion. Although some geometry-related methods were proposed to attenuate these artifacts and improve the coding efficiency, the interactive correlation between projected near and far depth fields has been ignored. Moreover, the non-linear representation ability of Convolutional Neural Network has not been fully considered. Therefore, we propose a learning-based approach to remove the geometry artifacts and improve the compressing efficiency. We have the following contributions. We devise a two-step method working on the near and far depth fields decomposed from geometry. The first stage is learning-based Pseudo-Motion Compensation. The second stage exploits the potential of the strong correlations between near and far depth fields. Our proposed algorithm is embedded in the V-PCC reference software. To the best of our knowledge, this is the first learning-based solution of the geometry artifacts removal in V-PCC. The extensive experimental results show that the proposed approach achieves significant gains on geometry artifacts removal and quality improvement of 3-D point-cloud reconstruction compared to the state-of-the-art schemes.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-021-01503-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-020-09649-0,Ambient light noise filtering technique for multimedia high speed transmission system in MIMO-VLC,Multimedia Tools and Applications,10.1007/s11042-020-09649-0,Springer,2021-11-01,"In this paper, we analyze the degradation of system performance due to the variation of ambient illumination in MIMO-VLC environment for high-speed multimedia transmission and propose an adaptive filter technique to solve this problem. Conventional PD-based visible light communication systems are not suitable for multimedia high-speed transmission. It is vulnerable to optical interference in the presence of a large number of LEDs, and it is difficult to implement MIMO. For large-capacity high-speed transmission, it is necessary to increase the transmission capacity. MIMO-VLC using m * m LED matrix and image sensor enables high-speed transmission through data parallel transmission. However, the received power is greatly influenced by the surrounding light source environment and the receiving distance. Illumination changes caused by non-scattered lights in the indoor environment, which deteriorates the performance of the system. In order to solve this problem, adaptive filter coefficients according to the ambient illuminance are applied to improve on / off recognition rate of LED matrix. By using this technique to adjust the filter values for image processing according to the step-by-step illumination, the multi-LED pixel can be accurately recognized and the MISO-VLC QoS can be maintained.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-09649-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41592-021-01275-4,Deep learning improves macromolecule identification in 3D cellular cryo-electron tomograms,Nature Methods,10.1038/s41592-021-01275-4,Nature,2021-11-01,"Cryogenic electron tomography (cryo-ET) visualizes the 3D spatial distribution of macromolecules at nanometer resolution inside native cells. However, automated identification of macromolecules inside cellular tomograms is challenged by noise and reconstruction artifacts, as well as the presence of many molecular species in the crowded volumes. Here, we present DeepFinder, a computational procedure that uses artificial neural networks to simultaneously localize multiple classes of macromolecules. Once trained, the inference stage of DeepFinder is faster than template matching and performs better than other competitive deep learning methods at identifying macromolecules of various sizes in both synthetic and experimental datasets. On cellular cryo-ET data, DeepFinder localized membrane-bound and cytosolic ribosomes (roughly 3.2 MDa), ribulose 1,5-bisphosphate carboxylase–oxygenase (roughly 560 kDa soluble complex) and photosystem II (roughly 550 kDa membrane complex) with an accuracy comparable to expert-supervised ground truth annotations. DeepFinder is therefore a promising algorithm for the semiautomated analysis of a wide range of molecular targets in cellular tomograms. DeepFinder is a deep learning-based tool for identifying macromolecules in cellular cryo-electron tomograms. DeepFinder performs with an accuracy comparable to expert-supervised ground truth annotations on multiple experimental datasets.",https://www.nature.com/articles/s41592-021-01275-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-021-02504-z,Preliminary study of generalized semiautomatic segmentation for 3D voxel labeling of lesions based on deep learning,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-021-02504-z,Springer,2021-11-01,"Purpose The three-dimensional (3D) voxel labeling of lesions requires significant radiologists’ effort in the development of computer-aided detection software. To reduce the time required for the 3D voxel labeling, we aimed to develop a generalized semiautomatic segmentation method based on deep learning via a data augmentation-based domain generalization framework. In this study, we investigated whether a generalized semiautomatic segmentation model trained using two types of lesion can segment previously unseen types of lesion. Methods We targeted lung nodules in chest CT images, liver lesions in hepatobiliary-phase images of Gd-EOB-DTPA-enhanced MR imaging, and brain metastases in contrast-enhanced MR images. For each lesion, the 32 × 32 × 32 isotropic volume of interest (VOI) around the center of gravity of the lesion was extracted. The VOI was input into a 3D U-Net model to define the label of the lesion. For each type of target lesion, we compared five types of data augmentation and two types of input data. Results For all considered target lesions, the highest dice coefficients among the training patterns were obtained when using a combination of the existing data augmentation-based domain generalization framework and random monochrome inversion and when using the resized VOI as the input image. The dice coefficients were 0.639 ± 0.124 for the lung nodules, 0.660 ± 0.137 for the liver lesions, and 0.727 ± 0.115 for the brain metastases. Conclusions Our generalized semiautomatic segmentation model could label unseen three types of lesion with different contrasts from the surroundings. In addition, the resized VOI as the input image enables the adaptation to the various sizes of lesions even when the size distribution differed between the training set and the test set.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-021-02504-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-020-09413-4,Implementation of user playstyle coaching using video processing and statistical methods in league of legends,Multimedia Tools and Applications,10.1007/s11042-020-09413-4,Springer,2021-11-01,"Recently, the game market is growing rapidly with the growth of e-sports. In particular, the market for League of Legends games continues to grow. Massively Online Battle Arena (MOBA) games have grown to become a massive industry, projected to reach over 140 billion USD worth of market value. Many users learn the League of Legends game, but their skills do not improve. Current analysis of the player’s manual playback through visual media such as video is the most common method. Therefore, this paper extracts data from gameplay videos and analyzes intuitive gameplay “styles” in the popular MMO game League of Legends to provide coaching-specific information. we were able to classify whether the player is cooperative and aggressive, but if additional information which we did not extract nor process like map vision were taken into account, Big Data and Machine Learning could come into play.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-09413-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-020-10160-9,Learning similarity and dissimilarity in 3D faces with triplet network,Multimedia Tools and Applications,10.1007/s11042-020-10160-9,Springer,2021-11-01,"Face is the most preferred biometric trait used to recognize a person. The 2D face can be considered as a promising biometric trait; however, it may be affected by changes in the age, skin color, texture, or any other environmental factors like illumination variations, occlusion, and low image resolution. The 3D face is an emerging biometric trait, which is being used recently for human recognition. The discriminating power of the 3D face is highly motivating for many tasks such as security, surveillance, and many other technological application in day to day life. Although there are many techniques available for 3D face recognition, most of these techniques are based on volumetric or depth/range images. The conversion of 3D face data, which is originally in point cloud format to volumetric representation makes the data bulkier. Further, some of the geometric properties may be lost when 3D data is converted to a representation of lower dimensions such as depth/range images. The driving objective behind this research is to perform 3D face recognition by directly using faces represented in the form of 3D point cloud. We propose a novel approach for 3D face recognition by learning the similarity and dissimilarity in 3D faces, and for this purpose, introduce a triplet network. The network is an ensemble of our proposed Convolutional PointNet (CPN) network, used for feature extraction and triplet loss. The proposed network maps a 3D face data to Euclidean space where distance based scores represent the similarity among the 3D faces. We also introduce a new evaluation approach for computing the dissimilarity between highly similar 3D face biometric data. Experimentation has been carried out on two databases, namely IIT Indore 3D face database (our in-house database) and Bosphorus 3D face database. To handle the training issues due to the limited availability of samples for each subject in both the databases, we propose a technique for 3D data augmentation. We perform various experiments using the proposed network and show the performance in terms of verification rate and ROC curve. Our point cloud based triplet network shows encouraging performance as compared to other state-of-the-art techniques.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-10160-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00414-021-02675-z,A potential method for sex estimation of human skeletons using deep learning and three-dimensional surface scanning,International Journal of Legal Medicine,10.1007/s00414-021-02675-z,Springer,2021-11-01,"Deep learning based on radiological methods has attracted considerable attention in forensic anthropology because of its superior classification capacities over human experts. However, radiological instruments are limited in their nature of high cost and immobility. Here, we integrated a deep learning algorithm and three-dimensional (3D) surface scanning technique into a portable system for pelvic sex estimation. Briefly, the images of the ventral pubis (VP), dorsal pubis (DP), and greater sciatic notch (GSN) were cropped from virtual pelvic samples reconstructed from CT scans of 1000 individuals; 80% of them were used to train and internally evaluate convolutional neural networks (CNNs) that were then evaluated externally with the remaining samples. An additional 105 real pelvises were documented virtually with a handheld 3D surface scanner, and the corresponding snapshots of the VP, DP, and GSN were predicted by the trained CNN models. The CNN models achieved excellent performance in the external testing using CT-based images, with accuracies of 98.0%, 98.5%, and 94.0% for VP, DP, and GSN, respectively. When the CT-based models were applied to 3D scanning images, they obtained satisfactory accuracies above 95% on the VP and DP images compared to the GSN with 73.3%. In a single-blind trial, a multiple design that combined the three CNN models yielded a superior accuracy of 97.1% with 3D surface scanning images over two anthropologists. Our study demonstrates the great potential of deep learning and 3D surface scanning for rapid and accurate sex estimation of skeletal remains.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00414-021-02675-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-020-09303-9,Cost-effective broad learning-based ultrasound biomicroscopy with 3D reconstruction for ocular anterior segmentation,Multimedia Tools and Applications,10.1007/s11042-020-09303-9,Springer,2021-11-01,"Anterior Chamber Angle (ACA) assessment plays an important role for the diagnosis of glaucoma. Most of the existing techniques relied on Anterior Segment Optical Coherence Tomography (AS-OCT) or Swept Source Optical Coherence Tomography (SS-OCT). We proposed a system for 360° overview of iridocorneal angle of anterior chamber (ICAAC) via Ultrasound Biomicroscopy (UBM). UBM approach acquires the visualization of anterior segment components as well as diseased structures (glaucoma). Our system consists of a new pairing scheme of feature descriptors, i.e. (FREAK, BRISK), (SURF, BRISK) and Broad Learning System (BLS) for 3D reconstruction and segmentation of ICAAC. The 360° overview of 2D ICAAC gives global conception for ACA assessment. 3D images provide a detailed assessment with the amount of opposition’s and synechiae in angle-closure suspects, angle-closure and angle-closure glaucoma in bright light conditions. Extensive evaluations are performed on dataset consists of 650 ICAAC images in five directions of 65 subjects with 10 samples per subject (5 left eye and 5 right eye) from Shanghai Sixth People’s Hospital. Experiments showed that our approach achieves an overall accuracy of 98.72% with training and testing time 29.26( s ), 1.232( s ) respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-09303-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11431-020-1777-4,Mobile phone recognition method based on bilinear convolutional neural network,Science China Technological Sciences,10.1007/s11431-020-1777-4,Springer,2021-11-01,"Model recognition of second-hand mobile phones has been considered as an essential process to improve the efficiency of phone recycling. However, due to the diversity of mobile phone appearances, it is difficult to realize accurate recognition. To solve this problem, a mobile phone recognition method based on bilinear-convolutional neural network (B-CNN) is proposed in this paper. First, a feature extraction model, based on B-CNN, is designed to adaptively extract local features from the images of secondhand mobile phones. Second, a joint loss function, constructed by center distance and softmax, is developed to reduce the interclass feature distance during the training process. Third, a parameter downscaling method, derived from the kernel discriminant analysis algorithm, is introduced to eliminate redundant features in B-CNN. Finally, the experimental results demonstrate that the B-CNN method can achieve higher accuracy than some existing methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11431-020-1777-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-021-02422-0,Explaining a model predicting quality of surgical practice: a first presentation to and review by clinical experts,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-021-02422-0,Springer,2021-11-01,"Purpose Surgical Data Science (SDS) is an emerging research domain offering data-driven answers to challenges encountered by clinicians during training and practice. We previously developed a framework to assess quality of practice based on two aspects: exposure of the surgical scene ( ESS ) and the surgeon’s profile of practice ( SPP ). Here, we wished to investigate the clinical relevance of the parameters learned by this model by (1) interpreting these parameters and identifying associated representative video samples and (2) presenting this information to surgeons in the form of a video-enhanced questionnaire. To our knowledge, this is the first approach in the field of SDS for laparoscopy linking the choices made by a machine learning model predicting surgical quality to clinical expertise. Method Spatial features and quality of practice scores extracted from labeled and segmented frames in 30 laparoscopic videos were used to predict the ESS and the SPP . The relationships between the inputs and outputs of the model were then analyzed and translated into meaningful sentences (statements, e.g., “To optimize the ESS, it is very important to correctly handle the spleen”). Representative video clips illustrating these statements were semi-automatically identified. Eleven statements and video clips were used in a survey presented to six experienced digestive surgeons to gather their opinions on the algorithmic analyses. Results All but one of the surgeons agreed with the proposed questionnaire overall. On average, surgeons agreed with 7/11 statements. Conclusion This proof-of-concept study provides preliminary validation of our model which has a high potential for use to analyze and understand surgical practices.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-021-02422-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-021-02285-7,Detecting earthquakes: a novel deep learning-based approach for effective disaster response,Applied Intelligence,10.1007/s10489-021-02285-7,Springer,2021-11-01,"In the present study, we present an intelligent earthquake signal detector that provides added assistance to automate traditional disaster responses. To effectively respond in a crisis scenario, additional sensors and automation are always necessary. Deep learning has achieved success in various low signal-to-noise ratio tasks, which motivated us to propose a novel 3-dimensional (3D) CNN-RNN-based earthquake detector from a demonstration paradigm to real-time implementation. Data taken from the ST anford EA rthquake D ataset (STEAD) are used to train the network. After preprocessing the raw earthquake signals, features such as log-mel spectrograms are extracted. Once the model has learned spatial and temporal information from low-frequency earthquake waves, it can be employed in real time to distinguish small and large earthquakes from seismic noise with an accuracy, sensitivity, and specificity of 99.057%, 98.488%, and 99.621%, respectively. We also observe that the choice of filters in log-mel spectrogram impacts the results much more than the model complexity. Furthermore, we implement and test the model on data collected continuously over two months by a personal seismometer in the laboratory. The inference speed for a single prediction is 2.27 seconds, and the system delivers a stable detection of all 63 major earthquakes from November 2019 to December 2019 reported by the Japan Meteorological Agency.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02285-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-020-09870-x,Video smoke detection base on dense optical flow and convolutional neural network,Multimedia Tools and Applications,10.1007/s11042-020-09870-x,Springer,2021-11-01,"Fire is one of the disasters with the highest probability among natural disasters and social disasters. It poses a serious threat to human life and life safety. In order to reduce fire losses, a reliable fire warning method is particularly important. But due to huge variations of smoke in color, shapes, and texture and complex application environments, the existing methods still do not meet the application requirements well. To solve these problems, in this paper, we propose a two-stage real-time video smoke detection method base on dense optical flow and convolutional neural network. In the first stage, we propose a fast pre-positioning module to obtain suspicious smoke areas through the dynamic characteristics of smoke which can greatly reduce the subsequent computational complexity, and only extract the moving optical flow of suspicious smoke areas as the dynamic features of the smoke which reduce the subsequent processing time cost. Instead of simply using moving optical flow as the dynamic characteristics of smoke, we found that the optical flow of the blue channel (OFBC) can effectively reflect the motion characteristics of smoke, so we combine the OFBC of suspicious smoke areas with its three RGB color channels to form a quaternion matrix for subsequent classification. In the second stage, we choose ResNet as our pre-classifier, and a temporal enhanced adjustment algorithm was proposed as the pre-classified follow-up fine optimization module, which can fully utilize the characteristics of the smoke movement in the video to improve detection rate. The experimental results show that compared with the existing smoke detection methods, our proposed method achieves high detection rate and low false alarm rate.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-020-09870-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-021-02434-w,Object and anatomical feature recognition in surgical video images based on a convolutional neural network,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-021-02434-w,Springer,2021-11-01,"Purpose Artificial intelligence-enabled techniques can process large amounts of surgical data and may be utilized for clinical decision support to recognize or forecast adverse events in an actual intraoperative scenario. To develop an image-guided navigation technology that will help in surgical education, we explored the performance of a convolutional neural network (CNN)-based computer vision system in detecting intraoperative objects. Methods The surgical videos used for annotation were recorded during surgeries conducted in the Department of Surgery of Tokyo Women’s Medical University from 2019 to 2020. Abdominal endoscopic images were cut out from manually captured surgical videos. An open-source programming framework for CNN was used to design a model that could recognize and segment objects in real time through IBM Visual Insights. The model was used to detect the GI tract, blood, vessels, uterus, forceps, ports, gauze and clips in the surgical images. Results The accuracy, precision and recall of the model were 83%, 80% and 92%, respectively. The mean average precision (mAP), the calculated mean of the precision for each object, was 91%. Among surgical tools, the highest recall and precision of 96.3% and 97.9%, respectively, were achieved for forceps. Among the anatomical structures, the highest recall and precision of 92.9% and 91.3%, respectively, were achieved for the GI tract. Conclusion The proposed model could detect objects in operative images with high accuracy, highlighting the possibility of using AI-based object recognition techniques for intraoperative navigation. Real-time object recognition will play a major role in navigation surgery and surgical education.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-021-02434-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41467-021-26491-6,3D printed biomimetic cochleae and machine learning co-modelling provides clinical informatics for cochlear implant patients,Nature Communications,10.1038/s41467-021-26491-6,Nature,2021-10-29,"Current spread hampers the efficacy of neuromodulation, while existing animal, in vitro and in silico models have failed to give patient-centric insights. Here the authors employ 3D printing and machine learning to advance clinical predictions of current spread for cochlear implant patients. Cochlear implants restore hearing in patients with severe to profound deafness by delivering electrical stimuli inside the cochlea. Understanding stimulus current spread, and how it correlates to patient-dependent factors, is hampered by the poor accessibility of the inner ear and by the lack of clinically-relevant in vitro, in vivo or in silico models. Here, we present 3D printing-neural network co-modelling for interpreting electric field imaging profiles of cochlear implant patients. With tuneable electro-anatomy, the 3D printed cochleae can replicate clinical scenarios of electric field imaging profiles at the off-stimuli positions. The co-modelling framework demonstrated autonomous and robust predictions of patient profiles or cochlear geometry, unfolded the electro-anatomical factors causing current spread, assisted on-demand printing for implant testing, and inferred patients’ in vivo cochlear tissue resistivity (estimated mean = 6.6 kΩcm). We anticipate our framework will facilitate physical modelling and digital twin innovations for neuromodulation implants.",https://www.nature.com/articles/s41467-021-26491-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s40747-021-00557-w,Hybrid design for sports data visualization using AI and big data analytics,Complex & Intelligent Systems,10.1007/s40747-021-00557-w,Springer,2021-10-29,"In sports data analysis and visualization, understanding collective tactical behavior has become an integral part. Interactive and automatic data analysis is instrumental in making use of growing amounts of compound information. In professional team sports, gathering and analyzing sportsperson monitoring data are common practice, intending to evaluate fatigue and succeeding adaptation responses, analyze performance potential, and reduce injury and illness risk. Data visualization technology born in the era of big data analytics provides a good foundation for further developing fitness tools based on artificial intelligence (AI). Hence, this study proposed a video-based effective visualization framework (VEVF) based on artificial intelligence and big data analytics. This study uses the machine learning method to categorize the sports video by extracting both the videos' temporal and spatial features. Our system is based on convolutional neural networks united with temporal pooling layers. The experimental outcomes demonstrate that the recommended VEVF model enhances the accuracy ratio of 98.7%, recall ratio of 94.5%, F1-score ratio of 97.9%, the precision ratio of 96.7%, the error rate of 29.1%, the performance ratio of 95.2%, an efficiency ratio of 96.1% compared to other existing models.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s40747-021-00557-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s41133-021-00051-5,Deep Learning and Particle Swarm Optimisation-Based Techniques for Visually Impaired Humans' Text Recognition and Identification,Augmented Human Research,10.1007/s41133-021-00051-5,Springer,2021-10-29,"Blind people can benefit greatly from a system capable of localising and reading comprehension text embedded in natural scenes and providing useful information that boosts their self-esteem and autonomy in everyday situations. Regardless of the fact that existing optical character recognition programmes seem to be quick and effective, the majority of them are not able to correctly recognise text embedded in usual panorama images. The methodology described in this paper is to localise textual image regions and pre-process them using the naïve Bayesian algorithm. A weighted reading technique is used to generate the correct text data from the complicated image regions. Usually, images hold some disturbance as a result of the fact that filtration is proposed during the early pre-processing step. To restore the image's quality, the input image is processed employing gradient and contrast image methods. Following that, the contrast of the source images would be enhanced using an adaptive image map. The stroke width transform, Gabor’s transform, and weighted naïve Bayesian classifier methodologies have been used in complicated degraded images to segment, feature extraction, and detect textual and non-textual elements. Finally, to identify categorised textual data, the confluence of deep neural networks and particle swarm optimisation is being used. The text in the image is transformed into an acoustic output after identification. The dataset IIIT5K is used for the development portion, and the performance of the suggested come up is evaluated using parameters such as accuracy, recall, precision, and F1-score.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41133-021-00051-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11701-021-01316-2,Evaluating robotic-assisted surgery training videos with multi-task convolutional neural networks,Journal of Robotic Surgery,10.1007/s11701-021-01316-2,Springer,2021-10-28,"We seek to understand if an automated algorithm can replace human scoring of surgical trainees performing the urethrovesical anastomosis in radical prostatectomy with synthetic tissue. Specifically, we investigate neural networks for predicting the surgical proficiency score (GEARS score) from video clips. We evaluate videos of surgeons performing the urethral anastomosis using synthetic tissue. The algorithm tracks surgical instrument locations from video, saving the positions of key points on the instruments over time. These positional features are used to train a multi-task convolutional network to infer each sub-category of the GEARS score to determine the proficiency level of trainees. Experimental results demonstrate that the proposed method achieves good performance with scores matching manual inspection in 86.1% of all GEARS sub-categories. Furthermore, the model can detect the difference between proficiency (novice to expert) in 83.3% of videos. Evaluation of GEARS sub-categories with artificial neural networks is possible for novice and intermediate surgeons, but additional research is needed to understand if expert surgeons can be evaluated with a similar automated system.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11701-021-01316-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10846-021-01481-4,Multi-Sound-Source Localization Using Machine Learning for Small Autonomous Unmanned Vehicles with a Self-Rotating Bi-Microphone Array,Journal of Intelligent & Robotic Systems,10.1007/s10846-021-01481-4,Springer,2021-10-27,"While vision-based localization techniques have been widely studied for small autonomous unmanned vehicles (SAUVs), sound-source localization capabilities have not been fully enabled for SAUVs. This paper presents two novel approaches for SAUVs to perform three-dimensional (3D) multi-sound-sources localization (MSSL) using only the inter-channel time difference (ICTD) signal generated by a self-rotating bi-microphone array. The proposed two approaches are based on two machine learning techniques viz., Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Random Sample Consensus (RANSAC) algorithms, respectively, whose performances were tested and compared in both simulations and experiments. The results show that both approaches are capable of correctly identifying the number of sound sources along with their 3D orientations in a reverberant environment.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10846-021-01481-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06576-5,Deep multi-task learning for image/video distortions identification,Neural Computing and Applications,10.1007/s00521-021-06576-5,Springer,2021-10-26,"Identifying distortions in images and videos is important and useful in various visual applications, such as image quality enhancement and assessment techniques. Instead of applying them blindly, these techniques can be applied or adjusted depending on the type of distortion identified. In this paper, we propose a deep multi-task learning (MTL) model for identifying the types of distortion in both images and videos, considering both single and multiple distortions. The proposed MTL model is composed of one convolutional neural network (CNN) shared between all tasks and N parallel classifiers, where each classifier is dedicated to identify a type of distortion. The proposed architecture also allows to adjust the number of tasks according to the number of distortion types considered, making the solution scalable. The proposed method has been evaluated on natural scene images and laparoscopic videos databases, each presenting a rich set of distortions. The experimental results demonstrate that our model achieves the best performance among the state-of-the-art methods for both single and multiple distortions (Code is available at: https://github.com/zoubidaameur/Deep-Multi-Task-Learning-for-Image-Video-Distortions-Identification ).",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06576-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10479-021-04348-x,Deep learning approach to Automated data collection and processing of video surveillance in sports activity prediction,Annals of Operations Research,10.1007/s10479-021-04348-x,Springer,2021-10-25,"Human activity recognition is one of today's key fields of automated video surveillance. The technology of smart surveillance technology plays a crucial role. Despite efforts in recent years, it is still difficult to recognize human behaviors from live video. Human activity can vary from basic behaviors to complicated behaviors. Depth cameras currently released have an efficient 3D estimate of body connecting locations in the temporal depth map collection. This article proposed a method for recognizing human behavior and considered the challenge of achieving a descriptive marking of activities by labeling individual sub-activities. The behaviors take place over a long period and have many sequential sub-activities. A sports activity prediction of video surveillance framework is proposed in this article. The suggested operation descriptor considers the sequence classification challenge to be the behavior recognition problem. Deep Learning is used to detect human behaviors in the proposed method. The method is tested on two regular identification benchmark functions. Effects of the research revealed that the solution developed exceeds cutting-edge methodologies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10479-021-04348-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s13634-021-00813-8,Speech enhancement from fused features based on deep neural network and gated recurrent unit network,EURASIP Journal on Advances in Signal Processing,10.1186/s13634-021-00813-8,Springer,2021-10-24,"Speech is easily interfered by external environment in reality, which results in the loss of important features. Deep learning has become a popular speech enhancement method because of its superior potential in solving nonlinear mapping problems for complex features. However, the deficiency of traditional deep learning methods is the weak learning capability of important information from previous time steps and long-term event dependencies between the time-series data. To overcome this problem, we propose a novel speech enhancement method based on the fused features of deep neural networks (DNNs) and gated recurrent unit (GRU). The proposed method uses GRU to reduce the number of parameters of DNNs and acquire the context information of the speech, which improves the enhanced speech quality and intelligibility. Firstly, DNN with multiple hidden layers is used to learn the mapping relationship between the logarithmic power spectrum (LPS) features of noisy speech and clean speech. Secondly, the LPS feature of the deep neural network is fused with the noisy speech as the input of GRU network to compensate the missing context information. Finally, GRU network is performed to learn the mapping relationship between LPS features and log power spectrum features of clean speech spectrum. The proposed model is experimentally compared with traditional speech enhancement models, including DNN, CNN, LSTM and GRU. Experimental results demonstrate that the PESQ, SSNR and STOI of the proposed algorithm are improved by 30.72%, 39.84% and 5.53%, respectively, compared with the noise signal under the condition of matched noise. Under the condition of unmatched noise, the PESQ and STOI of the algorithm are improved by 23.8% and 37.36%, respectively. The advantage of the proposed method is that it uses the key information of features to suppress noise in both matched and unmatched noise cases and the proposed method outperforms other common methods in speech enhancement.",https://www.biomedcentral.com/openurl?doi=10.1186/s13634-021-00813-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-021-03542-w,Generating synthetic dysarthric speech to overcome dysarthria acoustic data scarcity,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-021-03542-w,Springer,2021-10-18,"Dysarthria is a disorder that affects an individual’s speech intelligibility due to the paralysis of muscles and organs involved in the articulation process. As the condition is often associated with physically debilitating disabilities, performing daily tasks can become challenging. Not only do such individuals face communication problems, but interactions with digital devices can also become a burden. For such individuals, speech-to-text and text-to-normal-speech technologies can make a significant difference as computers and smartphones may become an interaction medium, enabling them to communicate. However, automatic speech recognition (ASR) technologies designed to understand normal speakers are incapable of perceiving dysarthric speech, and other attempts to design dysarthric ASR systems have progressed slowly, mainly due to the scarcity of dysarthric speech. As these systems’ performances rely heavily on dysarthric speech samples for training, generating synthetic dysarthric speech can significantly boost their efficiencies. This paper reports on adapting normal speech generation systems to produce dysarthric speech utilizing transfer learning and considering both subjective and objective evaluations. The results reveal that the syntactically produced dysarthric speech improved our novel dysarthric ASR accuracy by up to 5.67% for severe dysarthria, which has traditionally been the most challenging type of dysarthric speech to recognize. Adopting this study’s findings, other researchers can produce an unlimited amount of synthetic dysarthric speech by capturing a limited amount of speech data from dysarthric individuals and utilizing synthetic data to tackle the data scarcity problem in their studies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-021-03542-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-021-09900-5,Construction of complex environment speech signal communication system based on 5G and AI driven feature extraction techniques,International Journal of Speech Technology,10.1007/s10772-021-09900-5,Springer,2021-10-18,"In daily life, the most direct and important way of human communication is voice. With the rise of the Internet and the development of communication technology, the proportion of non voice signals such as image and data in communication system is increasing. However, in most communication systems, voice transmission function is generally required, so it is still one of the necessary functions for many communication systems to transmit voice information effectively. With the rapid development of the Internet, especially the recent 5G technology commercial and future civil, human–computer interaction will become more and more intelligent in the future, which also poses a greater challenge to speech recognition as a human–computer interface. Noise interference is one of the biggest obstacles to the practical application of speech system. Although a large number of noisy data based on deep learning can solve part of the noise robustness problem, non-stationary noise interference is still a great challenge for speech recognition system in very low SNR complex scenes. In addition, in the multi information fusion communication system, there are many kinds of data to be transmitted, and there are high requirements for bandwidth and storage space. Hence, this paper studies the construction of voice signal communication system based on the artificial intelligence and 5G technology. The model is designed and implemented considering the complex scenarios. The performance is validated through the simulations compared with the state-of-the-art methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-021-09900-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11276-021-02810-z,Dancing with the sound in edge computing environments,Wireless Networks,10.1007/s11276-021-02810-z,Springer,2021-10-14,"Conventional motion predictions have achieved promising performance. However, the length of the predicted motion sequences of most literatures are short, and the rhythm of the generated pose sequence has rarely been explored. To pursue high quality, rhythmic, and long-term pose sequence prediction, this paper explores a novel dancing with the sound task, which is appealing and challenging in computer vision field. To tackle this problem, a novel model is proposed, which takes the sound as an indicator input and outputs the dancing pose sequence. Specifically, our model is based on the variational autoencoder (VAE) framework, which encodes the continuity and rhythm of the sound information into the hidden space to generate a coherent, diverse, rhythmic and long-term pose video. Extensive experiments validated the effectiveness of audio cues in the generation of dancing pose sequences. Concurrently, a novel dataset of audiovisual multimodal sequence generation has been released to promote the development of this field.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11276-021-02810-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12021-021-09544-5,Convolutional Neural Network Based Frameworks for Fast Automatic Segmentation of Thalamic Nuclei from Native and Synthesized Contrast Structural MRI,Neuroinformatics,10.1007/s12021-021-09544-5,Springer,2021-10-09,"Thalamic nuclei have been implicated in several neurological diseases. Thalamic nuclei parcellation from structural MRI is challenging due to poor intra-thalamic nuclear contrast while methods based on diffusion and functional MRI are affected by limited spatial resolution and image distortion. Existing multi-atlas based techniques are often computationally intensive and time-consuming. In this work, we propose a 3D convolutional neural network (CNN) based framework for thalamic nuclei parcellation using T1-weighted Magnetization Prepared Rapid Gradient Echo (MPRAGE) images. Transformation of images to an efficient representation has been proposed to improve the performance of subsequent classification tasks especially when working with limited labeled data. We investigate this by transforming the MPRAGE images to White-Matter-nulled MPRAGE (WMn-MPRAGE) contrast, previously shown to exhibit good intra-thalamic nuclear contrast, prior to the segmentation step. We trained two 3D segmentation frameworks using MPRAGE images (n = 35 subjects): (a) a native contrast segmentation (NCS) on MPRAGE images and (b) a synthesized contrast segmentation (SCS) where synthesized WMn-MPRAGE representation generated by a contrast synthesis CNN were used. Thalamic nuclei labels were generated using THOMAS, a multi-atlas segmentation technique proposed for WMn-MPRAGE images. The segmentation accuracy and clinical utility were evaluated on a healthy cohort (n = 12) and a cohort (n = 45) comprising of healthy subjects and patients with alcohol use disorder (AUD), respectively. Both the segmentation CNNs yielded comparable performances on most thalamic nuclei with Dice scores greater than 0.84 for larger nuclei and at least 0.7 for smaller nuclei. However, for some nuclei, the SCS CNN yielded significant improvements in Dice scores (medial geniculate nucleus, P = 0.003﻿, centromedian nucleus, P = 0.01) and percent volume difference (ventral anterior, P = 0.001, ventral posterior lateral, P = 0.01) over NCS. In the AUD cohort, the SCS CNN demonstrated a significant atrophy in ventral lateral posterior nucleus in AUD patients compared to healthy age-matched controls (P = 0.01), agreeing with previous studies on thalamic atrophy in alcoholism, whereas the NCS CNN showed spurious atrophy of the ventral posterior lateral nucleus. CNN-based segmentation of thalamic nuclei provides a fast and automated technique for thalamic nuclei prediction in MPRAGE images. The transformation of images to an efficient representation, such as WMn-MPRAGE, can provide further improvements in segmentation performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12021-021-09544-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02301-4,Three-stage generative network for single-view point cloud completion,The Visual Computer,10.1007/s00371-021-02301-4,Springer,2021-10-08,"3D shape completion from single-view scan is an important task for follow-up applications such as recognition and segmentation, but it is challenging due to the critical sparsity and structural incompleteness of single-view point clouds. In this paper, a three-stage generative network (TSGN) is proposed for single-view point cloud completion, which generates fine-grained dense point clouds step by step and effectively overcomes the ubiquitous problem—the imbalance between general and individual characteristics. In the first stage, an encoder–decoder network consumes a partial point cloud and generates a rough sparse point cloud inferring the complete geometric shape. Then, a bi-channel residual network is designed to refine the preliminary result with assistance of the original partial input. A local-based folding network is introduced in the last stage to extract local context information from the revised result and build a dense point cloud with finer-grained details. Experiments on ShapeNet dataset and KITTI dataset validate the effectiveness of TSGN. The results on ShapeNet demonstrate the competitive performance on both CD and EMD.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02301-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11558-9,Using Machine Learning to Quantify the Multimedia Risk Due to Fuzzing,Multimedia Tools and Applications,10.1007/s11042-021-11558-9,Springer,2021-10-06,"There have been considerable advancements in multimedia technologies over the past 5 years. It has been observed that state-of-the-art multimedia systems face three broad categories of challenges: (1) Dependency on continuous network connections, (2) Data-sharing applications & collaboration, and (3) Security issues. Among these, security vulnerability poses a major threat to modern multimedia systems. Therefore, it is imperative to carefully investigate the security issues that can endanger wireless and mobile communications. At present, multimedia security research mainly focuses on wireless traffic monitoring, wireless system attacks, and wireless and mobile security. In this paper, we have used the network attack-type, “ Reconnaissance ”, which contains two types of malicious activities: (1) OS scanning, and (2) Fuzzing. The goal of this paper is to quantify multimedia security risks due to Fuzzing by using various types of machine learning models. The highest accuracy i.e., 96.8%, is obtained using the XGBoost classifier, which is good compared to the existing models present in the literature. This is the first paper, to the best of our knowledge, that uses machine learning methods to differentiate between benign and malignant Fuzzing attacks.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11558-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00500-021-06291-2,Enhancement of single channel speech quality and intelligibility in multiple noise conditions using wiener filter and deep CNN,Soft Computing,10.1007/s00500-021-06291-2,Springer,2021-10-06,"Nowadays, deep neural network has become the prime approach for enhancing speech signals as it yields good results compared to the traditional methods. This paper describes the transformation in the enhanced speech signal by applying the deep convolutional neural network (Deep CNN), which can model nonlinear relationships and compare it with the Wiener filtering method, which is the best technique for speech enhancement among the traditional methods. Denoising is performed in the frequency domain and converted back to the time domain to analyze performance metrics such as speech quality and speech intelligibility. The speech quality is analyzed based on the signal to noise ratio (SNR) and perceptual evaluation of speech quality (PESQ). Speech intelligibility is analyzed by short-time objective intelligibility (STOI). Both the methods evaluated the denoised speech, and the analysis made on the results shows that the SNR of the conventional Wiener filtering method is much improved when compared with Deep CNN. However, the PESQ and STOI of Deep CNN-based enhanced speech outperform the Wiener filtering method. The performance metrics indicate that Deep CNN achieves better results than the conventional technique.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-021-06291-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00500-021-06258-3,Characteristics recognition and soft multimedia system for Japanese machine translation and edge-driven hardware implementations,Soft Computing,10.1007/s00500-021-06258-3,Springer,2021-10-06,"With the development of recent economic globalization, international exchanges and cooperation are increasingly frequent and in-depth. In this process, there is a serious obstacle, that is, language differences. Therefore, the development of high-quality and practical machine translation system is of great significance. Japan has a close relationship with China, so it is necessary to acquire and process Japanese information. Japanese translation is the basis of Japanese information processing, which plays an important role in cross language information retrieval, machine translation, information extraction and other practical applications. In recent years, machine translation has made remarkable progress, but there is still much room for improvement in the quality of translation. Multimedia assisted instruction is an important application of computer technology. Therefore, this paper realizes the feature recognition and hardware structure of Japanese machine translation; for the efficient implementation of the model, the edge-driven model is combined. The proposed model is implemented through the edge devices and the performance of the model is validated through the testing. The recognition accuracy is much higher than the traditional models and the robustness is higher.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-021-06258-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-021-03525-x,"Framework for biometric iris recognition in video, by deep learning and quality assessment of the iris-pupil region",Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-021-03525-x,Springer,2021-10-05,"In the current world scenario the influence of the COVID19 pandemic has reached universal proportions affecting almost all countries. In this sense, the need has arisen to wear gloves or to reduce direct contact with objects (such as sensors for capturing fingerprints or palm prints) as a sanitary measure to protect against the virus. In this new reality, it is necessary to have a biometric identification method that allows safe and rapid recognition of people at borders, or in quarantine controls, or in access to places of high biological risk, among others. In this scenario, iris biometric recognition has reached increasing relevance. This biometric modality avoids all the aforementioned inconveniences with proven high efficiency. However, there are still problems associated with the iris capturing and segmentation in real time that could affect the effectiveness of a System of this nature and that it is necessary to take into account. This work presents a framework for real time iris detection and segmentation in video as part of a biometric recognition system. Our proposal focuses on the stages of image capture, iris detection and segmentation in RGB video frames under controlled conditions (conditions of border and access controls, where people collaborate in the recognition process). The proposed framework is based on the direct detection of the iris-pupil region using the YOLO network, the evaluation of its quality and the semantic segmentation of iris by a Fully Convolutional Network. (FCN). The proposal of an evaluation step of the quality of the iris-pupil region reduce the passage to the system of images with problems of out of focus, blurring, occlusions, light changing and pose of the subject. For the evaluation of image quality, we propose a measure that combines parameters defined in ISO/IEC 19794-6 2005 and others derived from the systematization of the knowledge of the specialized literature. The experiments carried out in four different reference databases and an own video data set demonstrates the feasibility of its application under controlled conditions of border and access controls. The achieved results exceed or equal state-of-the-art methods under these working conditions.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-021-03525-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11119-021-09803-0,Object-level classification of vegetable crops in 3D LiDAR point cloud using deep learning convolutional neural networks,Precision Agriculture,10.1007/s11119-021-09803-0,Springer,2021-10-01,"Crop discrimination at the plant or patch level is vital for modern technology-enabled agriculture. Multispectral and hyperspectral remote sensing data have been widely used for crop classification. Even though spectral data are successful in classifying row-crops and orchards, they are limited in discriminating vegetable and cereal crops at plant or patch level. Terrestrial laser scanning is a potential remote sensing approach that offers distinct structural features useful for classification of crops at plant or patch level. The objective of this research is the improvement and application of an advanced deep learning framework for object-based classification of three vegetable crops: cabbage, tomato, and eggplant using high-resolution LiDAR point cloud. Point clouds from a terrestrial laser scanner (TLS) were acquired over experimental plots of the University of Agricultural Sciences, Bengaluru, India. As part of the methodology, a deep convolution neural network (CNN) model named CropPointNet is devised for the semantic segmentation of crops from a 3D perspective. The CropPointNet is an adaptation of the PointNet deep CNN model developed for the segmentation of indoor objects in a typical computer vision scenario. Apart from adapting to 3D point cloud segmentation of crops, the significant methodological improvements made in the CropPointNet are a random sampling scheme for training point cloud, and optimization of the network architecture to enable structural attribute-based segmentation of point clouds of unstructured objects such as TLS point clouds crops. The performance of the 3D crop classification has been validated and compared against two popular deep learning architectures: PointNet, and the Dynamic Graph-based Convolutional Neural Network (DGCNN). Results indicate consistent plant level object-based classification of crop point cloud with overall accuracies of 81% or better for all the three crops. The CropPointNet architecture proposed in this research can be generalized for segmentation and classification of other row crops and natural vegetation types.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11119-021-09803-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-021-02240-6,Convolutional neural networks with hybrid weights for 3D point cloud classification,Applied Intelligence,10.1007/s10489-021-02240-6,Springer,2021-10-01,"The classification of 3D point clouds is a regular task, but remains a highly challenging problem because 3D point clouds usually contain a large amount of information on irregular shapes. Several recent studies have shown the excellent performance of deep learning in 3D point cloud classification. Convolutional neural network (CNN)-based 3D point cloud classification methods are also increasingly used owing to their efficient and convenient feature extraction capability. However, most of these methods do not take much prior information and local structural information into consideration, often resulting in their inability to extract sufficient information to improve the classification accuracy. In this study, we present a novel convolution operation named HyConv, which includes two key components. First, inspired by 2D convolution, we design a feature transformation module to capture more local structural information. Second, to extract the prior information, a hybrid weight module is introduced to estimate two types of weights on the basis of the distribution information of the spatial and feature domains. Additionally, we propose an adaptive method to learn hybrid weights to obtain hybrid distribution information. Finally, based on the proposed convolutional operator HyConv, we build a deep neural network Hybrid-CNN and conduct experiments on two commonly used datasets. The results show that our hybrid network outperforms most existing methods on ModelNet40. Furthermore, state-of-the-art performance is achieved with ScanObjectNN, which is a great improvement compared with existing methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02240-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-021-02195-8,An efficient attention module for 3d convolutional neural networks in action recognition,Applied Intelligence,10.1007/s10489-021-02195-8,Springer,2021-10-01,"Due to illumination changes, varying postures, and occlusion, accurately recognizing actions in videos is still a challenging task. A three-dimensional convolutional neural network (3D CNN), which can simultaneously extract spatio-temporal features from sequences, is one of the mainstream models for action recognition. However, most of the existing 3D CNN models ignore the importance of individual frames and spatial regions when recognizing actions. To address this problem, we propose an efficient attention module (EAM) that contains two sub-modules, that is, a spatial efficient attention module (EAM-S) and a temporal efficient attention module (EAM-T). Specifically, without dimensionality reduction, EAM-S concentrates on mining category-based correlation by local cross-channel interaction and assigns high weights to important image regions, while EAM-T estimates the importance score of different frames by cross-frame interaction between each frame and its neighbors. The proposed EAM module is lightweight yet effective, and it can be easily embedded into 3D CNN-based action recognition models. Extensive experiments on the challenging HMDB-51 and UCF-101 datasets showed that our proposed module achieves state-of-the-art performance and can significantly improve the recognition accuracy of 3D CNN-based action recognition methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02195-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11554-020-00957-0,Real-time automated video highlight generation with dual-stream hierarchical growing self-organizing maps,Journal of Real-Time Image Processing,10.1007/s11554-020-00957-0,Springer,2021-10-01,"Video has rapidly become one of the most common sources of visual information transfer. The number of videos uploaded to YouTube in a single day is estimated to take over 82 years to watch. Automated tools and techniques for analyzing and understanding video content, thus, have become an essential requirement. This paper addresses the problem of video highlight generation for large video files. We propose a novel skimming-based unsupervised video highlight generation method utilizing statistical image processing and data clustering, which process frame-level static and dynamic features of input video in two streams. The dynamic feature stream is represented by computing a dense optical flow for each consecutive frame, providing instantaneous velocity information for every pixel, which is then characterized by a per-frame orientation histogram, weighted by the norm, with orientations quantized. To process multi-scene videos, we utilize the divisive hierarchical clustering capability of growing self-organizing map (GSOM) using a dual-step top-down hierarchical approach in which the first level consists of clustering of spatial and temporal features of the video and in the second level, each parent cluster is hierarchically subdivided into child clusters using GSOM. The video highlight generation process is conducted real time by evaluating segments of video snippets based on a pre-defined time interval. We demonstrate the accuracy, robustness and the quality of highlights generated using a qualitative analysis conducted using 1625 human experts on highlights generated from two datasets. Further, we conduct a runtime analysis to demonstrate the efficient processing capability of the proposed method, to be used in real-time settings.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-020-00957-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12204-021-2348-7,Intelligent Analysis of Abnormal Vehicle Behavior Based on a Digital Twin,Journal of Shanghai Jiaotong University (Science),10.1007/s12204-021-2348-7,Springer,2021-10-01,"Analyzing a vehicle’s abnormal behavior in surveillance videos is a challenging field, mainly due to the wide variety of anomaly cases and the complexity of surveillance videos. In this study, a novel intelligent vehicle behavior analysis framework based on a digital twin is proposed. First, detecting vehicles based on deep learning is implemented, and Kalman filtering and feature matching are used to track vehicles. Subsequently, the tracked vehicle is mapped to a digital-twin virtual scene developed in the Unity game engine, and each vehicle’s behavior is tested according to the customized detection conditions set up in the scene. The stored behavior data can be used to reconstruct the scene again in Unity for a secondary analysis. The experimental results using real videos from traffic cameras illustrate that the detection rate of the proposed framework is close to that of the state-of-the-art abnormal event detection systems. In addition, the implementation and analysis process show the usability, generalization, and effectiveness of the proposed framework.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12204-021-2348-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10845-021-01769-0,Editorial: intelligent manufacturing systems towards industry 4.0 era,Journal of Intelligent Manufacturing,10.1007/s10845-021-01769-0,Springer,2021-10-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10845-021-01769-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00259-021-05319-x,A novel deep-learning–based approach for automatic reorientation of 3D cardiac SPECT images,European Journal of Nuclear Medicine and Molecular Imaging,10.1007/s00259-021-05319-x,Springer,2021-10-01,"Purpose Reconstructed transaxial cardiac SPECT images need to be reoriented into standard short-axis slices for subsequent accurate processing and analysis. We proposed a novel deep-learning–based method for fully automatic reorientation of cardiac SPECT images and evaluated its performance on data from two clinical centers. Methods We used a convolutional neural network to predict the 6 rigid-body transformation parameters and a spatial transformation network was then implemented to apply these parameters on the input images for image reorientation. A novel compound loss function which balanced the parametric similarity and penalized discrepancy of the prediction and training dataset was utilized in the training stage. Data from a set of 322 patients underwent data augmentation to 6440 groups of images for the network training, and a dataset of 52 patients from the same center and 23 patients from another center were used for evaluation. Similarity of the 6 parameters was analyzed between the proposed and the manual methods. Polar maps were generated from the output images and the averaged count values of the 17 segments were computed from polar maps to evaluate the quantitative accuracy of the proposed method. Results All the testing patients achieved automatic reorientation successfully. Linear regression results showed the 6 predicted rigid parameters and the average count value of the 17 segments having good agreement with the reference manual method. No significant difference by paired t-test was noticed between the rigid parameters of our method and the manual method ( p  > 0.05). Average count values of the 17 segments show a smaller difference of the proposed and manual methods than those between the existing and manual methods. Conclusion The results strongly indicate the feasibility of our method in accurate automatic cardiac SPECT reorientation. This deep-learning–based reorientation method has great promise for clinical application and warrants further investigation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00259-021-05319-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00158-021-02953-9,Integrating deep learning into CAD/CAE system: generative design and evaluation of 3D conceptual wheel,Structural and Multidisciplinary Optimization,10.1007/s00158-021-02953-9,Springer,2021-10-01,"Engineering design research integrating artificial intelligence (AI) into computer-aided design (CAD) and computer-aided engineering (CAE) is actively being conducted. This study proposes a deep learning-based CAD/CAE framework in the conceptual design phase that automatically generates 3D CAD designs and evaluates their engineering performance. The proposed framework comprises seven stages: (1) 2D generative design, (2) dimensionality reduction, (3) design of experiment in latent space, (4) CAD automation, (5) CAE automation, (6) transfer learning, and (7) visualization and analysis. The proposed framework is demonstrated through a road wheel design case study and indicates that AI can be practically incorporated into an end-use product design project. Engineers and industrial designers can jointly review a large number of generated 3D CAD models by using this framework along with the engineering performance results estimated by AI and find conceptual design candidates for the subsequent detailed design stage.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00158-021-02953-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11554-020-00966-z,V3O2: hybrid deep learning model for hyperspectral image classification using vanilla-3D and octave-2D convolution,Journal of Real-Time Image Processing,10.1007/s11554-020-00966-z,Springer,2021-10-01,"Remote sensing image analysis is an emerging area of research and is used for various applications such as climate analysis, crop monitoring and change detection. Hyperspectral image (HSI) is one of the dominant remote sensing imaging modalities that captures information beyond the visible spectrum. The evolution of deep learning has made a significant impact on HSI analysis, mainly for its classification. The spatial–spectral feature-based classification model improves the classification accuracy of hyperspectral images (HSIs). However, these models are computationally expensive, and redundancy exists in the spatial dimension of features. This research work proposes a hybrid convolutional neural network (CNN) for HSI classification. The proposed model uses principal component analysis (PCA) as a preprocessing technique for optimal band extraction from HSIs. The hybrid CNN classification technique extracts the spectral and spatial features using three-dimensional CNN (3D CNN). These features are fed into a two-dimensional CNN (2D CNN) for further feature extraction and classification. The redundancy in spatial features of the hybrid CNN model is reduced by octave convolution (OctConv) instead of standard vanilla convolution. OctConv factorizes the spatial features into lower and higher spatial frequencies, and different convolutions are performed on them based on their frequencies. The hybrid model is compared against various state-of-the-art CNN-based techniques and found that the accuracy is boosted with a lesser computational cost.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-020-00966-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11227-021-03772-x,Applying TS-DBN model into sports behavior recognition with deep learning approach,The Journal of Supercomputing,10.1007/s11227-021-03772-x,Springer,2021-10-01,"The purposes are to automatically collect information about human sports behavior from massive video data and provide an explicit recognition and analysis of body movements. The analysis of multi-scale input data, the improvement of spatiotemporal Deep Belief Network (DBN), and the different pooling strategies are regarded as the focuses to improve the belief networks in deep learning (DL). Moreover, a human sports behavior recognition model is proposed based on particular spatio-temporal features. Also, video frame data are collected from the Royal Institute of Technology (KTH) and University of Central Florida (UCF) datasets for training. The TensorFlow platform is employed to simulate the built algorithm. Finally, the constructed algorithm model is compared with the DBN proposed by Yang et al. the Convolutional Neural Network (CNN) proposed by Ullah et al. and the DBN-Hidden Markov Model (HMM) algorithm proposed by Xu et al. to analyse its performance. The recognition effects of each algorithm in the two datasets are analyzed. Results demonstrate that CNN developed by Ullah et al. has the worst accuracy on the KTH and UCF datasets, followed by DBN developed by Yang et al. and DBN-HMM developed by Xu et al. The constructed algorithm model can provide the highest accuracy, reaching about 90%, and the recognition accuracy of human sports behaviors of each algorithm on the KTH dataset is lower than that on the UCF dataset. On the KTH dataset, the recognition accuracy for boxing is the highest and running the lowest. Analyzing the model’s accuracy in the four scenes (S1, S2, S3, and S4) on the KTH dataset suggests that the recognition accuracy for the indoor scene (S4) is higher than that of the outdoor scenes (S1, S2, and S3). On the UCF dataset, the recognition accuracy for lifting is the highest, reaching 99%, and that for walking is the lowest, reaching 51%. Therefore, the proposed human sports recognition model can provide a higher accuracy than other classic DL algorithms, providing an experimental basis for subsequent human sports recognition research.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11227-021-03772-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-021-01505-4,Just Recognizable Distortion for Machine Vision Oriented Image and Video Coding,International Journal of Computer Vision,10.1007/s11263-021-01505-4,Springer,2021-10-01,"Machine visual intelligence has exploded in recent years. Large-scale, high-quality image and video datasets significantly empower learning-based machine vision models, especially deep-learning models. However, images and videos are usually compressed before being analyzed in practical situations where transmission or storage is limited, leading to a noticeable performance loss of vision models. In this work, we broadly investigate the impact on the performance of machine vision from image and video coding. Based on the investigation, we propose Just Recognizable Distortion (JRD) to present the maximum distortion caused by data compression that will reduce the machine vision model performance to an unacceptable level. A large-scale JRD-annotated dataset containing over 340,000 images is built for various machine vision tasks, where the factors for different JRDs are studied. Furthermore, an ensemble-learning-based framework is established to predict the JRDs for diverse vision tasks under few- and non-reference conditions, which consists of multiple binary classifiers to improve the prediction accuracy. Experiments prove the effectiveness of the proposed JRD-guided image and video coding to significantly improve compression and machine vision performance. Applying predicted JRD is able to achieve remarkably better machine vision task accuracy and save a large number of bits.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-021-01505-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11554-021-01144-5,A real-time person tracking system based on SiamMask network for intelligent video surveillance,Journal of Real-Time Image Processing,10.1007/s11554-021-01144-5,Springer,2021-10-01,"Real-time video surveillance systems are widely deployed in various environments, including public areas, commercial buildings, and public infrastructures. Person detection is a key and crucial task in different video surveillance applications, such as person detection, segmentation, and tracking. Researchers presented different image processing and artificial intelligence-based approaches (including machine and deep learning) for person detection and tracking, but mainly comprised of frontal view camera perspective. A real-time person tracking and segmentation system is introduced in this work, using an overhead camera perspective. The system applied a deep learning-based algorithm, i.e., SiamMask, a simple, versatile, fast, and surpassing other real-time tracking algorithms. The algorithm also performs segmentation of the target person by combining a mask branch to the fully convolutional twin neural network for target or person tracking. First, the person video sequences are obtained from an overhead perspective, and then additional training is performed with the help of transfer learning. Finally, a comparison is performed with other tracking algorithms. The SiamMask algorithm delivers good results, with a tracking accuracy of 95%.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-021-01144-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11554-021-01116-9,Real-time crowd behavior recognition in surveillance videos based on deep learning methods,Journal of Real-Time Image Processing,10.1007/s11554-021-01116-9,Springer,2021-10-01,"Automatic video surveillance in public crowded places has been an active research area for security purposes. Traditional approaches try to solve the crowd behavior recognition task using a sequential two-stage pipeline as low-level feature extraction and classification. Lately, deep learning has shown promising results in comparison to traditional methods by extracting high-level representation and solving the problem in an end-to-end pipeline. In this paper, we investigate a deep architecture for crowd event recognition to detect seven behavior categories in PETS2009 event recognition dataset. More especially, we apply an integrated handcrafted and Conv-LSTM-AE method with optical flow images as input to extract a high-level representation of data and conduct classification. After achieving a latent representation of input optical flow image sequences in the bottleneck of autoencoder(AE), the architecture is split into two separate branches, one as AE decoder and the other as the classifier. The proposed architecture is jointly trained for representation and classification by defining two different losses. The experimental results in comparison to the state-of-the-art methods demonstrate that our algorithm can be promising for real-time event recognition and achieves a better performance in calculated metrics.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11554-021-01116-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11831-021-09609-3,A Digital-Twin and Machine-Learning Framework for Ventilation System Optimization for Capturing Infectious Disease Respiratory Emissions,Archives of Computational Methods in Engineering,10.1007/s11831-021-09609-3,Springer,2021-10-01,"The pandemic of 2019 has led to an enormous interest in all aspects of modeling and simulation of infectious diseases. One central issue is the redesign and deployment of ventilation systems to mitigate the transmission of infectious diseases, produced by respiratory emissions such as coughs. This work seeks to develop a combined Digital-Twin and Machine-Learning framework to optimize ventilation systems by building on rapidly computable respiratory emission models developed in Zohdi (Comput Mech 64:1025–1034, 2020). This framework ascertains the placement and flow rates of multiple ventilation units, in order to optimally sequester particles released from respiratory emissions such as coughs, sneezes, etc. Numerical examples are provided to illustrate the framework.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11831-021-09609-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11063-021-10535-5,Online Speech Enhancement by Retraining of LSTM Using SURE Loss and Policy Iteration,Neural Processing Letters,10.1007/s11063-021-10535-5,Springer,2021-10-01,"Speech enhancement is required for improving the quality and intelligibility in various applications such as recognition, hearing aids and other personal assistant devices. Due to the varying acoustic environments, online enhancement is a very significant aspect for its applicability in practical scenarios. This emphasizes the need to observe the environment and enhance the speech accordingly. Adaptive filters were used previously to provide online enhancement, but a neural network based online enhancement has not been proposed previously. In this paper, we employ a unique architecture based on Long- Short Term Memory (LSTM) networks to enhance single channel speech online. The LSTM network is trained online in a novel way by minimizing the Stein’s unbiased risk estimate. This method of retraining helps the network to learn denoising without using a clean sample or ground truth. To avoid training for each and every sample we have used policy iteration with reward function based on ITU-T P.563, the widely-used single ended perceptual measure. The performance of this LSTM retraining can be observed with the increased PESQ of the enhanced speech by 0.53 on average. The proposed method also improves intelligibility which can be seen from the improvement in the metric STOI by 0.22.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11063-021-10535-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.3103/S1068798X21100142,Deriving Design Knowledge from 3D Models,Russian Engineering Research,10.3103/S1068798X21100142,Springer,2021-10-01,"Abstract A mathematical description is proposed for the derivation of design information from 3D models. Methods of comparing the knowledge obtained are considered. On that basis, geometrically similar mechanical systems may be identified.",http://link.springer.com/openurl/fulltext?id=doi:10.3103/S1068798X21100142,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.3103/S1068335621100080,ANALYSIS OF BIOLOGICAL OBJECTS BY DIGITAL OPTICAL MICROSCOPY USING NEURAL NETWORKS,Bulletin of the Lebedev Physics Institute,10.3103/S1068335621100080,Springer,2021-10-01,"Abstract An interdisciplinary (physics, biology, medicine, computer science) problem of recognizing bone marrow cells for the diagnosis of acute leukemia and minimal residual disease is considered. The use of neural networks for determining cell classes in a microscopic image of a bone marrow preparation is considered. Images are formed in transmitted light. The structure of the neural network and image sample for experimental studies are presented. The structure of the neural network is refined based on the experimental results. The accuracy characteristics of the developed system are determined.",http://link.springer.com/openurl/fulltext?id=doi:10.3103/S1068335621100080,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00500-021-06149-7,Human action recognition using a hybrid deep learning heuristic,Soft Computing,10.1007/s00500-021-06149-7,Springer,2021-10-01,"Human action recognition in the surveillance video is currently one of the challenging research topics. Most of the works in this area are based on either building classifiers on sophisticated handcrafted features or designing deep learning-based convolutional neural networks (CNNs), which directly act on raw inputs and extract meaningful information from the video. To capture the motion information between adjacent frames, 3D CNN extracts features in temporal dimension along with spatial dimension. Even though this technique is very effective in human action recognition but limited to very few fixed frames, all human actions are not limited to a fixed number of frames; they may span several frames. If we increase the size of the input window in CNN, handling all trainable parameters in the network will be very complicated. Hence, it is advisable to encode high-level motion features from different sources to the CNN model. This paper proposed a novel framework to extract handcrafted high-level motion features and in-depth features by CNN in parallel to recognize human action. SIFT is used as handcrafted feature to encode high-level motion features from the maximum number of input video frames. The combination of deep and handcrafted features preserves more extended temporal information from entire video frames present in action video with minimal computational power. Finally, we pass the extracted SIFT into the dense layer and concatenate it with a fully connected layer of CNN for classification. We evaluate the proposed combined CNN framework against regular 3D CNN and traditional handcrafted features like optical flow with SVM, SIFT with SVM on UCF, and KTH human action dataset. We achieve better performance in terms of computational cost and processing time in the proposed CNN framework compared to the other three methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00500-021-06149-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11053-021-09893-7,"Machine Learning-Based 3D Modeling of Mineral Prospectivity Mapping in the Anqing Orefield, Eastern China",Natural Resources Research,10.1007/s11053-021-09893-7,Springer,2021-10-01,"Successful delineation of high potential targets for exploration in maturely-explored orefields is still a tough challenge. A reliable prediction model achieved by integration of various ore-related geological factors and exploration information in the 3D space is an effective approach to deal with this challenge. The Anqing orefield has been intensively exploited for decades, and thereby the possible potential left must be at depth. The accumulated abundant data of exploration and research provide us a possibility for carrying out machine learning-based 3D modeling. The 3D block models of the main geological bodies, resistivity and volumetric strain field in this orefield were used as multi-resource geological data to construct prediction models by using weight-of-evidence and machine learning methods. Through performance evaluation and comparison, the following conclusions were obtained: (1) it is more scientific and reasonable to use all the geological prospecting factors concurrently for mineral prospectivity mapping (MPM) rather than use one or a part of them; (2) random forest (RF) algorithm seems capable of MPM because of its high accuracy and reliability in prediction; and (3) rational selection of training and learning samples, especially, those from actual geological objects and exploration engineering, plays a more critical role in MPM than algorithm and methods themselves. Two different RF prediction models were obtained for MPM in the east and the surrounding part of this orefield based on the outcome of geophysical prospecting. The spaces with prediction probabilities higher than 0.508 in the east part and 0.501 in the surroundings take up only 3.71% volume of the whole orefield, but contain 95.92% of the mineralized blocks. The high potential targets are most likely parts of the above spaces with high prediction probabilities that have not been drilled yet up to now. Actual geological data, accurate models and precise samples are critical for ore targeting. The RF-based prediction model is more applicable for mapping mineral prospectivity than other algorithms in this study. The determination of sample set is more important than algorithm if there is not enough field data.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11053-021-09893-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-05892-0,Health monitoring and fault prediction using a lightweight deep convolutional neural network optimized by Levy flight optimization algorithm,Neural Computing and Applications,10.1007/s00521-021-05892-0,Springer,2021-10-01,"Agricultural machines (AMs) refer to equipment usually used in agriculture such as tractors, hand tools, and power tools. It reduces the labor work, increases farms produce, enhances goods quality, and reduces farming time and cost-saving. However, the faults in the fuel system, blades, engine of the AM will often result in degraded vehicle performance, compromising the vehicle’s efficiency and strength. To overcome these problems, fault detection algorithms are developed to identify the faults even before they occur with high classification accuracy. The deep convolutional neural network (DCNN) is a popular deep learning model that offers a high classification recognition rate, and it is widely adopted in similar fields for monitoring the health status of machines. Very few state-of-the-art works are available to identify the health state of agricultural machines using deep learning techniques and extracting the acoustic features from an audio recording. The acoustic signal-based agricultural machine health monitoring and fault prediction model using smartphones is a cost-effective option that is deployed in this proposed work. To optimize the network structure of the DCNN, this paper proposes a Levy flight optimization algorithm (LFOA). The DCNN-LFOA model is implemented on the smartphone’s on-board device (OBD) along with the health monitoring application. The LFOA algorithm minimizes the number of neurons in the DCNN hidden layer and the number of input features from the audio recordings and enhances the classification accuracy. The LFOA algorithm provides the optimal solution which is essential in developing a lightweight DCNN model to implement in the edge processor (smartphone). The experimental results prove that the proposed model gives improved accuracy for the six faults to be classified and serves as a new research model to identify the health condition of the vehicles.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-05892-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12264-021-00715-7,Simultaneous 3D Visualization of the Microvascular and Neural Network in Mouse Spinal Cord Using Synchrotron Radiation Micro-Computed Tomography,Neuroscience Bulletin,10.1007/s12264-021-00715-7,Springer,2021-10-01,"Effective methods for visualizing neurovascular morphology are essential for understanding the normal spinal cord and the morphological alterations associated with diseases. However, ideal techniques for simultaneously imaging neurovascular structure in a broad region of a specimen are still lacking. In this study, we combined Golgi staining with angiography and synchrotron radiation micro-computed tomography (SRμCT) to visualize the 3D neurovascular network in the mouse spinal cord. Using our method, the 3D neurons, nerve fibers, and vasculature in a broad region could be visualized in the same image at cellular resolution without destructive sectioning. Besides, we found that the 3D morphology of neurons, nerve fiber tracts, and vasculature visualized by SRμCT were highly consistent with that visualized using the histological method. Moreover, the 3D neurovascular structure could be quantitatively evaluated by the combined methodology. The method shown here will be useful in fundamental neuroscience studies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12264-021-00715-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-021-02447-5,Automatic mandible segmentation from CT image using 3D fully convolutional neural network based on DenseASPP and attention gates,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-021-02447-5,Springer,2021-10-01,"Purpose In cranio-maxillofacial surgery, it is of great clinical significance to segment mandible accurately and automatically from CT images. However, the connected region and blurred boundary in teeth and condyles make the process challenging. At present, the mandible is commonly segmented by experienced doctors using manually or semi-automatic methods, which is time-consuming and has poor segmentation consistency. In addition, existing automatic segmentation methods still have problems such as region misjudgment, low accuracy, and time-consuming. Methods For these issues, an automatic mandibular segmentation method using 3d fully convolutional neural network based on densely connected atrous spatial pyramid pooling (DenseASPP) and attention gates (AG) was proposed in this paper. Firstly, the DenseASPP module was added to the network for extracting dense features at multiple scales. Thereafter, the AG module was applied in each skip connection to diminish irrelevant background information and make the network focus on segmentation regions. Finally, a loss function combining dice coefficient and focal loss was used to solve the imbalance among sample categories. Results Test results showed that the proposed network obtained a relatively good segmentation result, with a Dice score of 97.588 ± 0.425%, Intersection over Union of 95.293 ± 0.812%, sensitivity of 96.252 ± 1.106%, average surface distance of 0.065 ± 0.020 mm and 95% Hausdorff distance of 0.491 ± 0.021 mm in segmentation accuracy. The comparison with other segmentation networks showed that our network not only had a relatively high segmentation accuracy but also effectively reduced the network's misjudgment. Meantime, the surface distance error also showed that our segmentation results were relatively close to the ground truth. Conclusion The proposed network has better segmentation performance and realizes accurate and automatic segmentation of the mandible. Furthermore, its segmentation time is 50.43 s for one CT scan, which greatly improves the doctor's work efficiency. It will have practical significance in cranio-maxillofacial surgery in the future.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-021-02447-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-021-01890-w,Temporal attention learning for action quality assessment in sports video,"Signal, Image and Video Processing",10.1007/s11760-021-01890-w,Springer,2021-10-01,"This paper proposes an end-to-end temporal attention learning method to improve the performance of action quality assessment in sports video. For temporal weighted training, an attention-learning module is built to simulate the attention mechanism and judgement preference of human perception on action quality assessment. The weights are learned based on the loss of the segmented prediction errors and used to balance the significance of segmented features. We evaluate the proposed method on diving and gym-vault action of the benchmark AQA-7 dataset. The experimental results show that the proposed attention-aware feature training method is more effective than temporal aggregation and existing temporal relationship learning methods. Furthermore, only using the distance loss between the predicated score and the ground-truth score, without considering the ranking loss of different videos for training, this paper has achieved the state-of-the-art performance on both of the spearman rank correlation and mean Euclidean distance of the predicted scores against the judge’s scores.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-021-01890-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-021-01510-7,Deep Maximum a Posterior Estimator for Video Denoising,International Journal of Computer Vision,10.1007/s11263-021-01510-7,Springer,2021-10-01,"Unlike the maturity of image denoising research, video denoising has remained a challenging problem. A fundamental issue at the core of the video denoising (VD) problem is how to efficiently remove noise by exploiting temporal redundancy in video frames in a principled manner. Based on the maximum a posterior (MAP) estimation framework and recent advances in deep learning, we present a novel deep MAP-based video denoising method named MAP-VDNet with adaptive temporal fusion and deep image prior. The proposed MAP-based VD algorithm allows computationally efficient untangling of motion estimation (frame alignment) and image restoration (denoising). To address the misalignment issue, we also present a robust multi-frame fusion strategy for predicting spatially varying fusion weights by a neural network. To facilitate end-to-end optimization, we unfold the proposed iterative MAP-based VD algorithm into a deep convolutional network named MAP-VDNet . Extensive experimental results on three popular video datasets have shown that the proposed MAP-VDNet significantly outperforms current state-of-the-art VD techniques such as ViDeNN and FastDVDnet. The code is available at https://see.xidian.edu.cn/faculty/wsdong/Projects/MAP-VDNet.htm .",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-021-01510-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11403-z,3D convolutional networks with multi-layer-pooling selection fusion for video classification,Multimedia Tools and Applications,10.1007/s11042-021-11403-z,Springer,2021-10-01,"C3D has been widely used for video representation and understanding. However, it is performed on spatio-temporal contexts in a global view, which often weakens its capacity of learning local representation. To alleviate this problem, a concise and novel multi-layer feature fusion network with the cooperation of local and global views is introduced. For the current network, the global view branch is used to learn the core video semantics, while the local view branch is used to capture the contextual local semantics. Unlike traditional C3D model, the global view branch can only provide the big view branch with the most activated video features from a broader 3D receptive field. Via adding such shallow-view contexts, the local view branch can learn more robust and discriminative spatio-temporal representations for video classification. Thus we propose 3D convolutional networks with multi-layer-pooling selection fusion for video classification, the integrated deep global feature is combined with the information originated from shallow layer of local feature extraction networks, through the space-time pyramid pooling, adaptive pooling and attention pooling three different pooling units, different time–space feature information is obtained, and finally cascaded and used for classification. Experiments on the UCF-101 and HMDB-51 datasets achieve correct classification rate 95.0% and 72.2% respectively. The results show that the proposed 3D convolutional networks with multi-layer-pooling selection fusion has better classification performance.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11403-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12652-020-02598-4,Multi-objective long-short term memory recurrent neural networks for speech enhancement,Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-020-02598-4,Springer,2021-10-01,"Speech-in-noise perception is an important research problem in many real-world multimedia applications. The noise-reduction methods contributed significantly; however rely on a priori information about the noise signals. Deep learning approaches are developed for enhancing the speech signals in nonstationary noisy backgrounds and their benefits are evaluated for the perceived speech quality and intelligibility. In this paper, a multi-objective speech enhancement based on the Long-Short Term Memory (LSTM) recurrent neural network (RNN) is proposed to simultaneously estimate the magnitude and phase spectra of clean speech. During training, the noisy phase spectrum is incorporated as a target and the unstructured phase spectrum is transformed to its derivative that has an identical structure to corresponding magnitude spectrum. Critical Band Importance Functions (CBIFs) are used in training process to further improve the network performance. The results verified that the proposed multi-objective LSTM (MO-LSTM) successfully outscored the standard magnitude-aware LSTM (MA-LSTM), magnitude-aware DNN (MA-DNN), phase-aware DNN (PA-DNN), magnitude-aware GNN (MA-GNN) and magnitude-aware CNN (MA-CNN). Moreover, the proposed speech enhancement considerably improved the speech quality, intelligibility, noise-reduction and automatic speech recognition in changing noisy backgrounds, which is confirmed by the ANalysis Of VAriance (ANOVA) statistical analysis.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12652-020-02598-4,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11263-021-01500-9,A Decomposable Winograd Method for N–D Convolution Acceleration in Video Analysis,International Journal of Computer Vision,10.1007/s11263-021-01500-9,Springer,2021-10-01,"Winograd’s minimal filtering algorithm has been widely used in 2-D Convolutional Neural Networks (CNNs) to reduce the number of multiplications for faster processing. However, it is only effective on convolutions with kernel size as $$3$$ 3 and stride as 1, because it suffers from significantly increased FLOPs and numerical accuracy problems for kernel size larger than $$3$$ 3 and fails on convolution with stride larger than 1. Worse, the extension to N–D convolution will intensify the numerical accuracy problem. These problems severely obstruct Winograd’s minimal filtering algorithm’s application to video analysis. In this paper, we propose a novel Decomposable Winograd Method (DWM) for the N–D convolution acceleration, which breaks through the limitation of original Winograd’s minimal filtering algorithm to more general convolutions. DWM decomposes kernels with large size or stride>1 to several small kernels with stride as 1 for further applying Winograd algorithm, so that DWM can reduce the number of multiplications while keeping the numerical accuracy. It enables the fast exploration of larger kernel size, larger stride value, and higher dimensions in CNNs for high performance and accuracy and even the potential for new CNNs. Comparing against the original Winograd algorithm, the proposed DWM is able to support all kinds of N–D convolutions with a speedup of $$1.44\times $$ 1.44 × – $$3.38\times $$ 3.38 × , without affecting the numerical accuracy.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11263-021-01500-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11036-020-01730-0,Video Question Answering: a Survey of Models and Datasets,Mobile Networks and Applications,10.1007/s11036-020-01730-0,Springer,2021-10-01,"Video question answering (VideoQA) automatically answers natural language question according to the content of videos. It promotes the development of online education, scenario analysis, video content retrieving, etc. VideoQA is a challenging task because it requires a model to understand semantic information of the video and the question to generate the answer. Firstly, we propose a general framework of VideoQA which consists of a video feature extraction module, a text feature extraction module, an integration module, and an answer generation module. The integration module is the core module, including core processing model, recurrent neural networks (RNNs) encoder and feature fusion. These three sub-modules cooperate to generate the contextual representation, and the answer generation module generates the answer based on it. Then, we summarize the methods in core processing model, and introduce the ideas and applications of the methods in detail, such as encoder-decoder, attention model, and memory network and other methods. Additionally, we introduce the widely used datasets and evaluation criteria, as well as the analysis of experimental results on benchmark datasets. Finally, we discuss challenges in the field of VideoQA and provide some possible directions for future work.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11036-020-01730-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11242-021-01617-y,Computationally Efficient Multiscale Neural Networks Applied to Fluid Flow in Complex 3D Porous Media,Transport in Porous Media,10.1007/s11242-021-01617-y,Springer,2021-10-01,"The permeability of complex porous materials is of interest to many engineering disciplines. This quantity can be obtained via direct flow simulation, which provides the most accurate results, but is very computationally expensive. In particular, the simulation convergence time scales poorly as the simulation domains become less porous or more heterogeneous. Semi-analytical models that rely on averaged structural properties (i.e., porosity and tortuosity) have been proposed, but these features only partly summarize the domain, resulting in limited applicability. On the other hand, data-driven machine learning approaches have shown great promise for building more general models by virtue of accounting for the spatial arrangement of the domains’ solid boundaries. However, prior approaches building on the convolutional neural network (ConvNet) literature concerning 2D image recognition problems do not scale well to the large 3D domains required to obtain a representative elementary volume (REV). As such, most prior work focused on homogeneous samples, where a small REV entails that the global nature of fluid flow could be mostly neglected, and accordingly, the memory bottleneck of addressing 3D domains with ConvNets was side-stepped. Therefore, important geometries such as fractures and vuggy domains could not be modeled properly. In this work, we address this limitation with a general multiscale deep learning model that is able to learn from porous media simulation data. By using a coupled set of neural networks that view the domain on different scales, we enable the evaluation of large ( $$>512^3$$ > 512 3 ) images in approximately one second on a single graphics processing unit. This model architecture opens up the possibility of modeling domain sizes that would not be feasible using traditional direct simulation tools on a desktop computer. We validate our method with a laminar fluid flow case using vuggy samples and fractures. As a result of viewing the entire domain at once, our model is able to perform accurate prediction on domains exhibiting a large degree of heterogeneity. We expect the methodology to be applicable to many other transport problems where complex geometries play a central role.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11242-021-01617-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00330-021-07853-6,Automated cartilage segmentation and quantification using 3D ultrashort echo time (UTE) cones MR imaging with deep convolutional neural networks,European Radiology,10.1007/s00330-021-07853-6,Springer,2021-10-01,"Objective To develop a fully automated full-thickness cartilage segmentation and mapping of T1, T1ρ, and T2*, as well as macromolecular fraction (MMF) by combining a series of quantitative 3D ultrashort echo time (UTE) cones MR imaging with a transfer learning–based U-Net convolutional neural networks (CNN) model. Methods Sixty-five participants (20 normal, 29 doubtful-minimal osteoarthritis (OA), and 16 moderate-severe OA) were scanned using 3D UTE cones T1 (Cones-T1), adiabatic T1ρ (Cones-AdiabT1ρ), T2* (Cones-T2*), and magnetization transfer (Cones-MT) sequences at 3 T. Manual segmentation was performed by two experienced radiologists, and automatic segmentation was completed using the proposed U-Net CNN model. The accuracy of cartilage segmentation was evaluated using the Dice score and volumetric overlap error (VOE). Pearson correlation coefficient and intraclass correlation coefficient (ICC) were calculated to evaluate the consistency of quantitative MR parameters extracted from automatic and manual segmentations. UTE biomarkers were compared among different subject groups using one-way ANOVA. Results The U-Net CNN model provided reliable cartilage segmentation with a mean Dice score of 0.82 and a mean VOE of 29.86%. The consistency of Cones-T1, Cones-AdiabT1ρ, Cones-T2*, and MMF calculated using automatic and manual segmentations ranged from 0.91 to 0.99 for Pearson correlation coefficients, and from 0.91 to 0.96 for ICCs, respectively. Significant increases in Cones-T1, Cones-AdiabT1ρ, and Cones-T2* ( p  < 0.05) and a decrease in MMF ( p  < 0.001) were observed in doubtful-minimal OA and/or moderate-severe OA over normal controls. Conclusion Quantitative 3D UTE cones MR imaging combined with the proposed U-Net CNN model allows a fully automated comprehensive assessment of articular cartilage. Key Points • 3D UTE cones imaging combined with U-Net CNN model was able to provide fully automated cartilage segmentation. • UTE parameters obtained from automatic segmentation were able to reliably provide a quantitative assessment of cartilage.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00330-021-07853-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13369-021-05884-1,Application of Deep Learning Method in Automatic Collection and Processing of Video Surveillance Data for Basketball Sports Prediction,Arabian Journal for Science and Engineering,10.1007/s13369-021-05884-1,Springer,2021-09-28,"Sport difficulty in competitions must be investigated with noninvasive approaches, which are indispensable. In this case, creating a noninvasive system for data acquisition that recognizes players' roles in official basketball matches can be helpful by computer vision, image processing and software teaching techniques. The risk factors in predicting basketball sports surveillance data include rebound using the simple game and video review statistics while playing basketball. In this paper, a deep learning-based novel video framework has been proposed to rebound and build a system to predict which will get it with the player's position data. Conventional regression technique is implemented to determine the individual's position and move toward the ball drop position. The simulation analysis is performed based on complexity; viability, performance and efficiency prove the proposed framework's reliability.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13369-021-05884-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-021-98697-z,Automatic segmentation tool for 3D digital rocks by deep learning,Scientific Reports,10.1038/s41598-021-98697-z,Nature,2021-09-27,"Obtaining an accurate segmentation of images obtained by computed microtomography (micro-CT) techniques is a non-trivial process due to the wide range of noise types and artifacts present in these images. Current methodologies are often time-consuming, sensitive to noise and artifacts, and require skilled people to give accurate results. Motivated by the rapid advancement of deep learning-based segmentation techniques in recent years, we have developed a tool that aims to fully automate the segmentation process in one step, without the need for any extra image processing steps such as noise filtering or artifact removal. To get a general model, we train our network using a dataset made of high-quality three-dimensional micro-CT images from different scanners, rock types, and resolutions. In addition, we use a domain-specific augmented training pipeline with various types of noise, synthetic artifacts, and image transformation/distortion. For validation, we use a synthetic dataset to measure accuracy and analyze noise/artifact sensitivity. The results show a robust and accurate segmentation performance for the most common types of noises present in real micro-CT images. We also compared the segmentation of our method and five expert users, using commercial and open software packages on real rock images. We found that most of the current tools fail to reduce the impact of local and global noises and artifacts. We quantified the variation on human-assisted segmentation results in terms of physical properties and observed a large variation. In comparison, the new method is more robust to local noises and artifacts, outperforming the human segmentation and giving consistent results. Finally, we compared the porosity of our model segmented images with experimental porosity measured in the laboratory for ten different untrained samples, finding very encouraging results.",https://www.nature.com/articles/s41598-021-98697-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s41204-021-00148-7,Application of intelligent speech analysis based on BiLSTM and CNN dual attention model in power dispatching,Nanotechnology for Environmental Engineering,10.1007/s41204-021-00148-7,Springer,2021-09-27,"As the most natural language and emotional carrier, speech is widely used in intelligent furniture, vehicle navigation and other speech recognition technologies. With the continuous improvement of China's comprehensive national strength, the power industry has also ushered in a new stage of vigorous development. As the basis of production and life, it is a general trend to absorb voice processing technology. In order to better meet the actual needs of power grid dispatching, this paper applies voice processing technology to the field of smart grid dispatching. By testing and evaluating the recognition rate of the existing speech recognition system, a speech emotion recognition technology based on BiLSTM and CNN network dual attention model is proposed, which is suitable for the human–machine interaction system in the field of intelligent scheduling. Firstly, mel spectrum sequence of speech signal is extracted as input of BiLSTM network, and then, time context feature of speech signal is extracted by BiLSTM network. On this basis, the CNN network is used to extract the high-level emotional features from the low-level features and complete the emotional classification of speech signals. Emotional recognition tests were conducted on three different emotional databases, eNTERAFACE05, RML and AFW6.0. The experimental results show that the average recognition rates of this technology on three databases are 55.82%, 88.23% and 43.70%, respectively. In addition, the traditional speech emotion recognition technology is compared with the speech emotion recognition technology based on BiLSTM or CNN, which verifies the effectiveness of the technology.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s41204-021-00148-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10479-021-04264-0,Detecting and preventing criminal activities in shopping malls using massive video surveillance based on deep learning models,Annals of Operations Research,10.1007/s10479-021-04264-0,Springer,2021-09-26,"Video surveillance devices are a valuable tool in various contexts to automate different danger conditions and enable security guards to make effective decisions to improve asset safety. This article suggests detecting and preventing criminal activities in shopping malls (DPCA-SM) framework that detects suspected activity in shopping malls in real-time. The video monitoring approach makes some suggestions that create a comprehensive application capable of effectively tracking people's pathways and detecting measures in a shop setting. The proposed system utilizes the publicly accessible CAVIAR data collection to validate the proposed approach for monitoring occlusions with a performance of nearly 92% to assess the accuracy of the principal inputs of the proposed initiative. The alerts provided by the proposed framework are also evaluated on naturalism, private datasets, demonstrating that in a shopping mall setting, the professional surveillance cameras strategy can efficiently detect unusual activity.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10479-021-04264-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s42162-021-00161-9,Greenhouse industry 4.0 – digital twin technology for commercial greenhouses,Energy Informatics,10.1186/s42162-021-00161-9,Springer,2021-09-24,"The project aims to create a Greenhouse Industry 4.0 Digital Twin software platform for combining the Industry 4.0 technologies (IoT, AI, Big Data, cloud computing, and Digital Twins) as integrated parts of the greenhouse production systems. The integration provides a new disruptive approach for vertical integration and optimization of the greenhouse production processes to improve energy efficiency, production throughput, and productivity without compromising product quality or sustainability. Applying the Industry 4.0 Digital Twin concept to the Danish horticulture greenhouse industry provides digital models for simulating and evaluating the physical greenhouse facility’s performance. A Digital Twin combines modeling, AI, and Big Data analytics with IoT and traditional sensor data from the production and cloud-based enterprise data to predict how the physical twin will perform under varying operational conditions. The Digital Twins support the co-optimization of the production schedule, energy consumption, and labor cost by considering influential factors, including production deadlines, quality grading, heating, artificial lighting, energy prices (gas and electricity), and weather forecasts. The ecosystem of digital twins extends the state-of-the-art by adopting a scalable distributed approach of “system of systems” that interconnects Digital Twins in a production facility. A collection of specialized Digital Twins are linked together to describe and simulate all aspects of the production chain, such as overall production capacity, energy consumption, delivery dates, and supply processes. The contribution of this project is to develop an ecosystem of digital twins that collectively capture the behavior of an industrial greenhouse facility. The ecosystem will enable the industrial greenhouse facilities to become increasingly active participants in the electricity grid.",http://link.springer.com/openurl/fulltext?id=doi:10.1186/s42162-021-00161-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s41747-021-00235-z,Performance of a 3D convolutional neural network in the detection of hypoperfusion at CT pulmonary angiography in patients with chronic pulmonary embolism: a feasibility study,European Radiology Experimental,10.1186/s41747-021-00235-z,Springer,2021-09-24,"Background Chronic pulmonary embolism (CPE) is a life-threatening disease easily misdiagnosed on computed tomography. We investigated a three-dimensional convolutional neural network (CNN) algorithm for detecting hypoperfusion in CPE from computed tomography pulmonary angiography (CTPA). Methods Preoperative CTPA of 25 patients with CPE and 25 without pulmonary embolism were selected. We applied a 48%–12%–40% training-validation-testing split (12 positive and 12 negative CTPA volumes for training, 3 positives and 3 negatives for validation, 10 positives and 10 negatives for testing). The median number of axial images per CTPA was 335 (min–max, 111–570). Expert manual segmentations were used as training and testing targets. The CNN output was compared to a method in which a Hounsfield unit (HU) threshold was used to detect hypoperfusion. Receiver operating characteristic area under the curve (AUC) and Matthew correlation coefficient (MCC) were calculated with their 95% confidence interval (CI). Results The predicted segmentations of CNN showed AUC 0.87 (95% CI 0.82–0.91), those of HU-threshold method 0.79 (95% CI 0.74–0.84). The optimal global threshold values were CNN output probability ≥ 0.37 and ≤ -850 HU. Using these values, MCC was 0.46 (95% CI 0.29–0.59) for CNN and 0.35 (95% CI 0.18–0.48) for HU-threshold method (average difference in MCC in the bootstrap samples 0.11 (95% CI 0.05–0.16). A high CNN prediction probability was a strong predictor of CPE. Conclusions We proposed a deep learning method for detecting hypoperfusion in CPE from CTPA. This model may help evaluating disease extent and supporting treatment planning.",http://link.springer.com/openurl/fulltext?id=doi:10.1186/s41747-021-00235-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10055-021-00538-x,Predicting motor behavior: an efficient EEG signal processing pipeline to detect brain states with potential therapeutic relevance for VR-based neurorehabilitation,Virtual Reality,10.1007/s10055-021-00538-x,Springer,2021-09-23,"Virtual reality (VR)-based motor therapy is an emerging approach in neurorehabilitation. The combination of VR with electroencephalography (EEG) presents further opportunities to improve therapeutic efficacy by personalizing the paradigm. Specifically, the idea is to synchronize the choice and timing of stimuli in the perceived virtual world with fluctuating brain states relevant to motor behavior. Here, we present an open source EEG single-trial based classification pipeline that is designed to identify ongoing brain states predictive of the planning and execution of movements. 9 healthy volunteers each performed 1080 trials of a repetitive reaching task with an implicit two-alternative forced choice, i.e., use of the right or left hand, in response to the appearance of a visual target. The performance of the EEG decoding pipeline was assessed with respect to classification accuracy of right vs. left arm use, based on the EEG signal at the time of the stimulus. Different features, feature extraction methods, and classifiers were compared at different time windows; the number and location of informative EEG channels and the number of calibration trials needed were also quantified, as well as any benefits from individual-level optimization of pipeline parameters. This resulted in a set of recommended parameters that achieved an average 83.3% correct prediction on never-before-seen testing data, and a state-of-the-art 77.1% in a real-time simulation. Neurophysiological plausibility of the resulting classifiers was assessed by time–frequency and event-related potential analyses, as well as by Independent Component Analysis topographies and cortical source localization. We expect that this pipeline will facilitate the identification of relevant brain states as prospective therapeutic targets in closed-loop EEG-VR motor neurorehabilitation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-021-00538-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1186/s12938-021-00932-1,A 2D–3D hybrid convolutional neural network for lung lobe auto-segmentation on standard slice thickness computed tomography of patients receiving radiotherapy,BioMedical Engineering OnLine,10.1186/s12938-021-00932-1,BioMed Central,2021-09-23,"Background Accurate segmentation of lung lobe on routine computed tomography (CT) images of locally advanced stage lung cancer patients undergoing radiotherapy can help radiation oncologists to implement lobar-level treatment planning, dose assessment and efficacy prediction. We aim to establish a novel 2D–3D hybrid convolutional neural network (CNN) to provide reliable lung lobe auto-segmentation results in the clinical setting. Methods We retrospectively collected and evaluated thorax CT scans of 105 locally advanced non-small-cell lung cancer (NSCLC) patients treated at our institution from June 2019 to August 2020. The CT images were acquired with 5 mm slice thickness. Two CNNs were used for lung lobe segmentation, a 3D CNN for extracting 3D contextual information and a 2D CNN for extracting texture information. Contouring quality was evaluated using six quantitative metrics and visual evaluation was performed to assess the clinical acceptability. Results For the 35 cases in the test group, Dice Similarity Coefficient (DSC) of all lung lobes contours exceeded 0.75, which met the pass criteria of the segmentation result. Our model achieved high performances with DSC as high as 0.9579, 0.9479, 0.9507, 0.9484, and 0.9003 for left upper lobe (LUL), left lower lobe (LLL), right upper lobe (RUL), right lower lobe (RLL), and right middle lobe (RML), respectively. The proposed model resulted in accuracy, sensitivity, and specificity of 99.57, 98.23, 99.65 for LUL; 99.6, 96.14, 99.76 for LLL; 99.67, 96.13, 99.81 for RUL; 99.72, 92.38, 99.83 for RML; 99.58, 96.03, 99.78 for RLL, respectively. Clinician's visual assessment showed that 164/175 lobe contours met the requirements for clinical use, only 11 contours need manual correction. Conclusions Our 2D–3D hybrid CNN model achieved accurate automatic segmentation of lung lobes on conventional slice-thickness CT of locally advanced lung cancer patients, and has good clinical practicability.",https://www.biomedcentral.com/openurl?doi=10.1186/s12938-021-00932-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00138-021-01241-2,Detection of inclusion by using 3D laser scanner in composite prepreg manufacturing technique using convolutional neural networks,Machine Vision and Applications,10.1007/s00138-021-01241-2,Springer,2021-09-21,"Among different manufacturing techniques available for composite aircraft structures, prepreg-based manual layup is widely used. During the fabrication process, the protective films of the prepregs or other materials used in the process could get inside as a foreign object between the layers. The present method of finding the inclusions during the prepreg layup is by visual inspection in the cleanroom. Carrying out visual inspection is challenging as the layup is usually carried out on large surfaces and reflective by nature. This paper proposes a 3D laser scanner-based approach for the detection of inclusion on flat and curved surfaces. Using the portable laser scanner, the surfaces of each layer are scanned and compared the resulting point clouds using with a reference layer data. Thicknesses between two surfaces are computed with Cloud to Cloud, Mesh to Cloud and Hausdorff distance to enhance the visibility of inclusions. It was found that this approach could enhance the visibility of inclusions over 50 micron and above. These enhanced features are used to train a multiview convolutional neural network to mark the inclusion regions, which can aid the inspector to identify the inclusion regions in a fast and efficient way.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00138-021-01241-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13204-021-02072-3,Deepfake detection using rationale-augmented convolutional neural network,Applied Nanoscience,10.1007/s13204-021-02072-3,Springer,2021-09-13,"Deepfake network is a prominent topic of research as an application to various systems about security measures. Although there have been many recent advancements in facial reconstruction, the greatest challenge to overcome has been the means of finding an efficient and quick way to compute facial similarities or matches. This work is created utilizing the rationale-augmented convolutional neural network (CNN) on MATLAB R2019a platform using the Kaggle DeepFake Video dataset with an accuracy of 95.77%. Hence, real-time deepfake facial reconstruction for security purposes is difficult to complete concerning limited hardware and efficiency. This research paper looks into rational augmented CNN state-of-the-art technology utilized for deepfake facial reconstruction via hardware such as webcams and security cameras in real time. Additionally, discuss a history of face reconstruction and provide an overview of how it is accomplished.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13204-021-02072-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11277-021-09072-0,Deep Reinforcement Learning-Based Smart Manufacturing Plants with a Novel Digital Twin Training Model,Wireless Personal Communications,10.1007/s11277-021-09072-0,Springer,2021-09-11,"Setting up new doors for smart production is to fill the gaps between virtual and physical systems. Using a digital twin to represent production cells, simulate system behavior, forecast process failures, and alter variables adaptively. This paper presents a novel digital twin training model to automate intelligent production systems using digital transformation techniques. First, the cell is customized to computer-aided applications, industrial Product Lifecycle Management solutions, and automation platforms. Second, a network of interfaces between the environments is developed to allow communications between the digital world and the factory to achieve near-synchronous controls. Thirdly, the skills of some members of the deep reinforcement learning (DRL) family are addressed in combination with Smart Manufacturing's manufacturing aspects. Thus the experimental results show the new variety of data science and the production sectors will form the DRL method to automated production control issues under straightforward optimization environments.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11277-021-09072-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13198-021-01327-0,An analysis of English classroom behavior by intelligent image recognition in IoT,International Journal of System Assurance Engineering and Management,10.1007/s13198-021-01327-0,Springer,2021-09-09,"In order to strengthen the management of English classroom discipline and improve the efficiency of students’ English classroom learning, students’ English classroom behavior based on intelligent image recognition is analyzed in IoT (Internet of things). The working scenes and practical significance of deep learning and IoT are analyzed and then the effects of four models on students' behavior analysis in English classroom are discussed. The results show that the classroom behavior analysis model proposed is feasible. The recognition system judges whether the students are listening seriously from three aspects, namely students' side face, head up and down, and their eyelid opening. The comparison of the four models of VGG16, ResNet18, ResNet50 and AlexNet shows that the accurate recognition rate of VGG16 for students' behavior in English classroom reaches 94.15%. Experiments show that the method provides a more objective evaluation of students’ classroom behavior. As a whole, students’ classroom behavior analysis based on IIRT (intelligent image recognition technology) in IOT is practicable for improving English classroom efficiency.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13198-021-01327-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10044-021-01020-9,Harnessing emotions for depression detection,Pattern Analysis and Applications,10.1007/s10044-021-01020-9,Springer,2021-09-09,"Human emotions using textual cues, speech patterns, and facial expressions can give insight into their mental state. Although there are several uni-modal datasets for emotion recognition, there are very few labeled datasets for multi-modal depression detection. Uni-modal emotion recognition datasets can be harnessed, using the technique of transfer learning, for multi-modal binary emotion detection through video, audio, and text. We propose emotion transfer for mood indication framework based on deep learning to address the task of binary classification of depression using a one-of-three scheme: If the prediction from the network for at least one modality is of the depressed class, we consider the final output as depressed. Such a scheme is beneficial since it will detect an abnormality in any of the modalities and will alert a user to seek help well in advance. Long short-term memory is used to combine the temporal aspects of the audio and the video modalities, and the context of the text. This is followed by fine-tuning the network on a binary dataset for depression detection that has been independently labeled by a standard questionnaire used by psychologists. Data augmentation techniques are used for the generalization of data and to resolve the class imbalance. Our experiments show that our method for binary depression classification (using an ensemble of three modalities) on the Distress Analysis Interview Corpus—Wizard of Oz dataset has higher accuracy in comparison with other benchmark methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10044-021-01020-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-021-06491-9,Deep learning-based video quality enhancement for the new versatile video coding,Neural Computing and Applications,10.1007/s00521-021-06491-9,Springer,2021-09-08,"Multimedia IoT (M-IoT) is an emerging type of Internet of things (IoT) relaying multimedia data (images, videos, audio and speech, etc.). The rapid growth of M-IoT devices enables the creation of a massive volume of multimedia data with different characteristics and requirements. With the development of artificial intelligence (AI), AI-based multimedia IoT systems have been recently designed and deployed for various video-based services for contemporary daily life, like video surveillance with high definition (HD) and ultra-high definition (UHD) and mobile multimedia streaming. These new services need higher video quality in order to meet the quality of experience (QoE) required by the users. Versatile video coding (VVC) is the new video coding standard that achieves significant coding efficiency over its predecessor high-efficiency video coding (HEVC). Moreover, VVC can achieve up to 30% BD rate savings compared to HEVC. Inspired by the rapid advancements in deep learning, we propose in this paper a wide-activated squeeze-and-excitation deep convolutional neural network (WSE-DCNN) technique-based video quality enhancement for VVC. Therefore, we replace the conventional in-loop filtering in VVC by the proposed WSE-DCNN model that eliminates the compression artifacts in order to improve visual quality and hence increase the end user QoE. The obtained results prove that the proposed in-loop filtering technique achieves $$-2.85$$ - 2.85 %, $$-8.89$$ - 8.89 %, and $$-10.05$$ - 10.05 % BD rate reduction for luma and both chroma components under random access configuration. Compared to the traditional CNN-based filtering approaches, the proposed WSE-DCNN-based in-loop filtering framework achieves efficient performance in terms of RD cost.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-021-06491-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s13204-021-02068-z,Deep learning-based garbage image recognition algorithm,Applied Nanoscience,10.1007/s13204-021-02068-z,Springer,2021-09-08,"To solve the problems of over-fitting, poor convergence, and reduced recall and accuracy of traditional image recognition algorithms, a junk image recognition algorithm based on deep learning is proposed. Dropout is introduced to overcome over fitting, adagrad adaptive method is used to debug the parameters of deep neural network, and ReLU is adopted to solve the gradient dispersion of neural network training, realize the centralized processing of garbage image data, and extract the edge features, color features and texture features of garbage image in the data set, respectively, shape features. The modified probability density function is used for image classification. According to the different characteristics of garbage image, the image to be recognized is divided into different categories to complete garbage image recognition. The experimental results show that the designed algorithm has good convergence, high recall and accuracy, and short recognition time, indicating that the algorithm is feasible and practical.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s13204-021-02068-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11255-7,Effects of similarity/distance metrics on k-means algorithm with respect to its applications in IoT and multimedia: a review,Multimedia Tools and Applications,10.1007/s11042-021-11255-7,Springer,2021-09-06,"Recently, Internet of Things (IoT) and multimedia are gaining popularity because of their usages in various applications. Numerous sensors and automated devices are generating huge volumes of data. Therefore, it is required to efficiently and effectively analyze this voluminous data. It can be achieved by using appropriate machine leaning techniques such as clustering. Among the clustering techniques, the k-means method/algorithm is one of the simplest, effective and commonly used methods. For making the cluster, it uses a measure of similarity/distance among the data observations. Nearby/similar data observations are placed within the same cluster whereas distant/dis-similar data observations are placed in other clusters. Hence, the similarity/distance metric plays a major role on the performance and accuracy of the k-means. Therefore, using an appropriate similarity/distance metric, the performance and accuracy of the k-means can be improved. K-means algorithm is majorly implemented using Euclidean distance metric. With the objective to explore the better and/or alternate similarity/distance metric(s) for k-means, a case study, based on empirical evaluation, of thirteen different similarity/distance metrics on six well-known datasets is performed and presented in this paper. By using the efficient and effective similarity / distance metrics, the performance and accuracy of the k-means algorithm can be improved which leads to formation of good clusters of various data observations or things or images etc. The results of the empirical study are analyzed and compared on the basis of widely used statistical clustering evaluation/validation measures. Based on the comparative results, these metrics are assigned with the ranks. Overall, the results demonstrate that Manhattan and Minkowski distance metrics gives better results for k-means algorithm.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11255-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11701-021-01305-5,The evolution of image guidance in robotic-assisted laparoscopic prostatectomy (RALP): a glimpse into the future,Journal of Robotic Surgery,10.1007/s11701-021-01305-5,Springer,2021-09-04,"Objectives To describe the innovative intraoperative technologies emerging to aid surgeons during minimally invasive robotic-assisted laparoscopic prostatectomy. Methods We searched multiple electronic databases reporting on intraoperative imaging and navigation technologies, robotic surgery in combination with 3D modeling and 3D printing used during laparoscopic or robotic-assisted laparoscopic prostatectomy. Additional searches were conducted for articles that considered the role of artificial intelligence and machine learning and their application to robotic surgery. We excluded studies using intraoperative navigation technologies during open radical prostatectomy and studies considering technology to visualize lymph nodes. Summary of findings Intraoperative imaging using either transrectal ultrasonography or augmented reality was associated with a potential decrease in positive surgical margins rates. Improvements in detecting capsular involvement may be seen with augmented reality. The benefit, feasibility and applications of other imaging modalities such as 3D-printed models and optical imaging are discussed. Conclusion The application of image-guided surgery and robotics has led to the development of promising new intraoperative imaging technologies such as augmented reality, fluorescence imaging, optical coherence tomography, confocal laser endomicroscopy and 3D printing. Currently challenges regarding tissue deformation and automatic tracking of prostate movements remain and there is a paucity in the literature supporting the use of these technologies. Urologic surgeons are encouraged to improve and test these advanced technologies in the clinical arena, preferably with comparative, randomized, trials.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11701-021-01305-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12517-021-08336-0,Subsurface drain spacing in the unsteady conditions by HYDRUS-3D and artificial neural networks,Arabian Journal of Geosciences,10.1007/s12517-021-08336-0,Springer,2021-09-02,"The methodological process for defining the drainage retention capacity of surface layers under conditions of unsteady-state groundwater flow was demonstrated. An artificial neural network analyst model was advanced based on the information from the well-tested model HYDRUS-2D/3D. Artificial neural network knowledge is reported as an intermittent to physical-based modeling of subsurface water distribution from trickle emitters. Three options are prospected to create input-output functional relations from information created using a numerical model (HYDRUS-2D). Artificial neural networks are a tool for modeling of non-linear systems in various engineering fields. These networks are effective tools for modeling non-linear systems. Each artificial neural network includes an input layer and an output layer between which there are one or some hidden layers. In each layer, there are one or several processing elements or neurons. The neurons of the input layer are independent variables of the understudy issue and the neurons of the output layer are its dependent variables. An artificial neural system, through exerting weight on inputs and by using an activation function, attempts to achieve a desirable output. In this research, in order to calculate the drain spacing in an unsteady state in a region situated in the northeast of Ahwaz, Iran, with different soil properties and drain spacing, the artificial neural networks have been used. The neurons in the input layer were specific yield, hydraulic conductivity, depth of the impermeable layer, and height of the water table in the middle of the interval between the drains in two-time steps. The neurons in the output layer were drain spacing. The network designed in this research included a hidden layer with four neurons. The distance of drains computed via this method had a good agreement with real values and had a high precision in comparison with other methods. This was done for three types of linear activation functions and hyperbolic and sigmoid tangents. The mean error was 0.1455, 0.092, and 0.0491, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12517-021-08336-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00264-021-05175-2,"Digital twins, artificial intelligence, and machine learning technology to identify a real personalized motion axis of the tibiotalar joint for robotics in total ankle arthroplasty",International Orthopaedics,10.1007/s00264-021-05175-2,Springer,2021-09-01,"Purpose Axial alignment of the talar implant in total ankle arthroplasty remains a major issue, since the real axis of motion of each patient is impossible to determine with usual techniques. Further knowledge regarding individual axis of motion of the ankle is therefore needed. Material and methods Therefore, digital twins, artificial intelligence, and machine learning technology were used to identify a real personalized motion axis of the tibiotalar joint. Three-dimensional (3D) models of distal extremities were generated using computed tomography data of normal patients. Digital twins were used to reproduce the mobility of the ankles, and the real ankle of the patients was matched to the digital twin with machine learning technology. Results The results showed that a personalized axis can be obtained for each patient. When the origin of the axis is the centre of mass of the talus, this axis can be represented in a geodesic system. The mean value of the axis is a line passing in first approximation through the centre of the sphere (with a variation of 3 mm from the centre of the mass of the talus) and through a point with the coordinates 91.6° west and 7.4° north (range 84° to 98° west; − 2° to 12° north). This study improves the understanding of the axis of the ankle, as well as its relationship to the possibility to use the geodesic system for robotic in ankle arthroplasty. Conclusion The consideration of a personalized axis of the ankle might be helpful for better understanding of ankle surgery and particularly total ankle arthroplasty.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00264-021-05175-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12369-020-00712-2,Performing the Kick During Walking for RoboCup 3D Soccer Simulation League Using Reinforcement Learning Algorithm,International Journal of Social Robotics,10.1007/s12369-020-00712-2,Springer,2021-09-01,"Nowadays, humanoid soccer serves as a benchmark for artificial intelligence and robotic problems. The factors such as the kicking speed and the number of kicks by robot soccer players are the most significant aims that the participating teams are pursued in the RoboCup 3D Soccer Simulation League. The proposed method presents a kicking strategy during walking for humanoid soccer robots. Achieving an accurate and powerful kicking while robots are moving requires a dynamic optimization of the speed and motion parameters of the robot. In this paper, a curved motion path has been designed based on the robot position relative to the ball and the goal. Ultimately, the robot will be able to kick at the goal by walking along this curve path. The speed and angle of the walking robot are set towards the ball with regard to the robots curved motion path. After the final step of the robot, the accurate and effective adjustment of these two parameters ensures that the robot is located in the ideal position to perform the perfect kick. Due to the noise and walking condition of the robot, it is essential that the speed and angle of motion to be measured more accurately. For this purpose, we use a reinforcement learning model to adjust the robots step size and so does achieve the optimal value of two abovementioned parameters. Using reinforcement learning, robot would learn to pursue an optimal policy to correctly kick towards designated points. Therefore, the proposed method is a model-free and based on dynamic programming. The experiments reveal that the proposed method has significantly improved the team overall performance and robots ability to kick. Our proposed method has been 9.32% successful on average and outperformed the UTAustinVilla agent in terms of goal-scoring time in a non-opponent simulator.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12369-020-00712-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11548-021-02432-y,Real-time deep learning semantic segmentation during intra-operative surgery for 3D augmented reality assistance,International Journal of Computer Assisted Radiology and Surgery,10.1007/s11548-021-02432-y,Springer,2021-09-01,"Purpose The current study aimed to propose a Deep Learning (DL) and Augmented Reality (AR) based solution for a in-vivo robot-assisted radical prostatectomy (RARP), to improve the precision of a published work from our group. We implemented a two-steps automatic system to align a 3D virtual ad-hoc model of a patient’s organ with its 2D endoscopic image, to assist surgeons during the procedure. Methods This approach was carried out using a Convolutional Neural Network (CNN) based structure for semantic segmentation and a subsequent elaboration of the obtained output, which produced the needed parameters for attaching the 3D model. We used a dataset obtained from 5 endoscopic videos ( A, B, C, D, E ), selected and tagged by our team’s specialists. We then evaluated the most performing couple of segmentation architecture and neural network and tested the overlay performances. Results U-Net stood out as the most effecting architectures for segmentation. ResNet and MobileNet obtained similar Intersection over Unit (IoU) results but MobileNet was able to elaborate almost twice operations per seconds. This segmentation technique outperformed the results from the former work, obtaining an average IoU for the catheter of 0.894 ( σ = 0.076) compared to 0.339 ( σ = 0.195). This modifications lead to an improvement also in the 3D overlay performances, in particular in the Euclidean Distance between the predicted and actual model’s anchor point, from 12.569 ( σ = 4.456) to 4.160 ( σ = 1.448) and in the Geodesic Distance between the predicted and actual model’s rotations, from 0.266 ( σ = 0.131) to 0.169 ( σ = 0.073). Conclusion This work is a further step through the adoption of DL and AR in the surgery domain. In future works, we will overcome the limits of this approach and finally improve every step of the surgical procedure.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11548-021-02432-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10055-020-00476-0,Implementation of escape room system based on augmented reality involving deep convolutional neural network,Virtual Reality,10.1007/s10055-020-00476-0,Springer,2021-09-01,"Escape room is a live-action adventure game, where the players search clues, solve puzzles and achieve the assigned tasks. This paper proposed a novel escape room system combining augmented reality and deep learning technology. The system adopts a client–server architecture and can be divided into the server module, the smart glasses module and the player–hardware interaction module. The player–hardware interaction module consists of subsystems each of which includes a Raspberry Pi 3. HoloLens is used as the smart glasses in the experiment of the paper. The server communicates with all the Raspberry Pis and HoloLens through TCP/IP protocol and manages all the devices to achieve the game flow by following the process timeline. The smart glasses module provides two display modes, i.e., the AR 3D models display and the 2D text clues display. In the first mode, the SDK Vuforia is used for detection and tracking of markers. In the second mode, the scene images captured by HoloLens camera are sent to the pre-trained image classifier based on deep convolutional neural network. Considering both the image category and the game status value, the server decides the text clue image to be displayed on HoloLens. The accuracy of the image classification model reaches 94.9%, which can be correctly classified for a certain rotation angle and partial occlusion. The integration of AR, deep learning, electronics and escape room games opens up exciting new directions for the development of escape room. Finally, a built mini-escape room is analyzed to prove that the proposed system can support more complicated narratives showing the potential of achieving immersion.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10055-020-00476-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-020-01844-8,A natural language-inspired multilabel video streaming source identification method based on deep neural networks,"Signal, Image and Video Processing",10.1007/s11760-020-01844-8,Springer,2021-09-01,Existing website fingerprinting techniques are not effective with video streaming traffic when the encrypted traffic contains multiple streams. This paper presents a deep learning-based source identification method for identifying multiple video sources within a single encrypted tunnel. The core contribution is a novel feature inspired by natural language processing (NLP) that allows existing NLP techniques to identify the source. The feature extraction method is described. A large dataset containing video streaming and web traffic is created to verify its effectiveness. Results are obtained by applying several NLP methods to show that the proposed method performs well on both binary and multilabel traffic classification problems. The work proves that the method can overcome the challenges given by mixed-traffic tunnels.,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-020-01844-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00264-021-05191-2,"Ankle and foot surgery: from arthrodesis to arthroplasty, three dimensional printing, sensors, artificial intelligence, machine learning technology, digital twins, and cell therapy",International Orthopaedics,10.1007/s00264-021-05191-2,Springer,2021-09-01,,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00264-021-05191-2,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10489-021-02293-7,Deep learning in multi-object detection and tracking: state of the art,Applied Intelligence,10.1007/s10489-021-02293-7,Springer,2021-09-01,"Object detection and tracking is one of the most important and challenging branches in computer vision, and have been widely applied in various fields, such as health-care monitoring, autonomous driving, anomaly detection, and so on. With the rapid development of deep learning (DL) networks and GPU’s computing power, the performance of object detectors and trackers has been greatly improved. To understand the main development status of object detection and tracking pipeline thoroughly, in this survey, we have critically analyzed the existing DL network-based methods of object detection and tracking and described various benchmark datasets. This includes the recent development in granulated DL models. Primarily, we have provided a comprehensive overview of a variety of both generic object detection and specific object detection models. We have enlisted various comparative results for obtaining the best detector, tracker, and their combination. Moreover, we have listed the traditional and new applications of object detection and tracking showing its developmental trends. Finally, challenging issues, including the relevance of granular computing, in the said domain are elaborated as a future scope of research, together with some concerns. An extensive bibliography is also provided.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10489-021-02293-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-020-01843-9,Low-complexity QTMT partition based on deep neural network for Versatile Video Coding,"Signal, Image and Video Processing",10.1007/s11760-020-01843-9,Springer,2021-09-01,"Versatile Video Coding (VVC), the newest standard for future video coding, is currently under development. This proposal aimed to improve the encoder performance over the latest standard namely High Efficiency Video Coding, carried with a high increase in coding complexity. The VVC partition structure is mainly based on the quadtree with nested multi-type tree (QTMT) block scheme. Such an improvement leads to a more flexible block partition and promotes a high encoding efficiency, but generates a huge coding complexity. In order to deal with this issue, a fast QTMT intra partition algorithm, based on a deep neural network named Early Terminated Hierarchical Convolution Neural Network, is applied to predict the $$64\times $$ 64 × 64 block QT partition structure. The proposed algorithm determines the QTMT partition structure based on the decision of whether to split or skip the corresponding CU, in order to get $$128\times $$ 128 × 128 Coding Tree Unit partition architecture. In this paper, the proposed intra partition work achieves a significant speedup in encoding gain that reaches 32.96% in best cases for Ultra High Definition video sequences compared to the reference VVC software VTM-3.0. For all video sequences, 24.49% time saving is reached on average. This improvement comes with an increase of 4.18% and a decrease of 0.18 dB in terms of BDBR and BDPSNR, respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-020-01843-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-021-09838-8,Language dialect based speech emotion recognition through deep learning techniques,International Journal of Speech Technology,10.1007/s10772-021-09838-8,Springer,2021-09-01,"The primordial way of communication is through vocal signals, which pave the way for support between individuals in a social structure. Computer applications provide a way to create Automatic Speech Recognition (ASR) with a combination of Speech Emotion Recognition (SER) to detect and identify emotions in the speech signals. The semantic relatedness of words with abstract concepts proves to be complicated than concrete ideas. An ensemble of different clustering techniques is utilized to automatically segregate sense distinctions in the various dialects of sentences spoken to tackle this issue. The interpretation of word sense of a word may change with time and group of people. The proposed model maps characters to word sense with weights provided by Senticnet with trial-and-error methods and tuning. The proposed model utilizes stop words to distinguish word senses with 72.78% accuracy for regional dialects.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-021-09838-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00521-020-05572-5,DENet: a deep architecture for audio surveillance applications,Neural Computing and Applications,10.1007/s00521-020-05572-5,Springer,2021-09-01,"In the last years, a big interest of both the scientific community and the market has been devoted to the design of audio surveillance systems, able to analyse the audio stream and to identify events of interest; this is particularly true in security applications, in which the audio analytics can be profitably used as an alternative to video analytics systems, but also combined with them. Within this context, in this paper we propose a novel recurrent convolutional neural network architecture, named DENet; it is based on a new layer that we call denoising-enhancement (DE) layer, which performs denoising and enhancement of the original signal by applying an attention map on the components of the band-filtered signal. Differently from state-of-the-art methodologies, DENet takes as input the lossless raw waveform and is able to automatically learn the evolution of the frequencies-of-interest over time, by combining the proposed layer with a bidirectional gated recurrent unit. Using the feedbacks coming from classifications related to consecutive frames (i.e. that belong to the same event), the proposed method is able to drastically reduce the misclassifications. We carried out experiments on the MIVIA Audio Events and MIVIA Road Events public datasets, confirming the effectiveness of our approach with respect to other state-of-the-art methodologies.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00521-020-05572-5,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12194-021-00620-8,Investigation of clinical target volume segmentation for whole breast irradiation using three-dimensional convolutional neural networks with gradient-weighted class activation mapping,Radiological Physics and Technology,10.1007/s12194-021-00620-8,Springer,2021-09-01,"This study aims to implement three-dimensional convolutional neural networks (3D-CNN) for clinical target volume (CTV) segmentation for whole breast irradiation and investigate the focus of 3D-CNNs during decision-making using gradient-weighted class activation mapping (Grad-CAM). A 3D-UNet CNN was adopted to conduct automatic segmentation of the CTV for breast cancer. The 3D-UNet was trained using three datasets of left-, right-, and both left- and right-sided breast cancer patients. Segmentation accuracy was evaluated using the Dice﻿ similarity coefficient (DSC). Grad-CAM was applied to trained CNNs. The DSCs for the datasets of the left-, right-, and both left- and right-sided breasts were on an average 0.88, 0.89, and 0.85, respectively. The Grad-CAM heatmaps showed that the 3D-UNet used for segmentation determined the CTV region from the target-side breast tissue and by referring to the opposite-side breast. Although the size of the dataset was limited, DSC ≥ 0.85 was achieved for the segmentation of breast CTV using the 3D-UNet. Grad-CAM indicates the applicable scope and limitations of using a CNN by indicating the focus of such networks during decision-making.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12194-021-00620-8,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12524-021-01358-x,An Automatic Method for Tree Species Point Cloud Segmentation Based on Deep Learning,Journal of the Indian Society of Remote Sensing,10.1007/s12524-021-01358-x,Springer,2021-09-01,"Tree species segmentation is an essential condition for research forestry and has a large impact on forest resource monitoring, sustainable forest management, and biodiversity research. Recently, the development of hardware and software has been rapidly increasing. Regarding hardware, the active remote sensing system LiDAR can be used to obtain many point clouds and can significantly improve the tree segmentation accuracy compared with traditional optical remote sensing hardware. With respect to software, deep learning theory is effectively utilized to process 3D point clouds, such as extracting the features of data. However, deep learning-based methods are underutilized in tree species point cloud segmentation. Therefore, it is extremely important to combine current technological advantages for this application. In this article, we construct a point cloud processing dataset that comprises substantial tree information and 5 tree species, including willow, fir, bamboo, palm, and rubber. The novel representation of point clouds via a superpoint graph is utilized to pre-process the point clouds in a large outdoor scene. We propose to apply state-of-the-art deep learning frameworks, including PointNet network and graph convolution networks, to process tree species point clouds in complex forest scenes. We also discuss the effectiveness of the method and the situations influenced by different parameters. The experimental results finally verify the effectiveness of the framework in tree species segmentation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12524-021-01358-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11210-6,Modified dense convolutional networks based emotion detection from speech using its paralinguistic features,Multimedia Tools and Applications,10.1007/s11042-021-11210-6,Springer,2021-09-01,"Emotion recognition through speech is one of the fundamental approaches for human interaction. Speech modulations stipulate different emotions and context. In this paper, we propose modified dense convolutional networks (modified DenseNet201) for emotion detection from speech using its paralinguistic features such as vocal tract features. The proposed network performs emotion classification from speech using spectrograms of its audio files. The proposed network outperforms other alternative models like residual networks, AlexNet, VGG16, SVM, XGBoost, boosted random forest etc. for emotion classification from speech. Moreover, the proposed network surpasses all other existing methods proposed in the literature and obtains state-of-the-art results in most of the cases. Further, the proposed network has been successfully validated on two different language datasets: ‘EmoDB’ and ‘SAVEE’ which qualifies it as a language-independent emotion detection system from speech.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11210-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10772-021-09851-x,A novel stochastic deep resilient network for effective speech recognition,International Journal of Speech Technology,10.1007/s10772-021-09851-x,Springer,2021-09-01,Speech recognition is a subjective occurrence. This work proposes a novel stochastic deep resilient network(SDRN) for speech recognition. It uses a deep neural network (DNN) for classification to predict the input speech signal. The hidden layers of DNN and its neurons are additionally optimized to reduce the computation time by using a neural-based opposition whale optimization algorithm (NOWOA). The novelty of the SDRN network is in using NOWOA to recognize large vocabulary isolated and continuous speech signals. The trained DNN features are then utilized for predicting isolated and continuous speech signals. The standard database is used for training and testing. The real-time data (recorded in ambient condition) for isolated words and continuous speech signals are additionally used for validation to increase the accuracy of the SDRN network. The proposed methodology unveils an accuracy of 99.6% and 98.1% for isolated words (standard and real-time) database and 98.7% for continuous speech signal (real-time). The obtained results exhibit the supremacy of SDRN over other techniques.,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10772-021-09851-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10484-021-09518-y,Computer Gaming and Physiological Changes in the Brain: An Insight from QEEG Complexity Analysis,Applied Psychophysiology and Biofeedback,10.1007/s10484-021-09518-y,Springer,2021-09-01,"To compare the pattern of brain waves in video game addicts and normal individuals, a case–control study was carried out on both. Thirty participants were recruited from 14 to 20 years old males from two gaming centers. Twenty healthy participants were gathered from different schools in Tehran using the available sampling method. The QEEG data collection was performed in three states: closed-eye and open-eye states, and during a working memory task. As expected, the power ratios did not show a significant difference between the two groups. Regarding our interest in the complexity of signals, we used the Higuchi algorithm as the feature extractor to provide the input materials for the multilayer perceptron classifier. The results showed that the model had at least a 95% precision rate in classifying the addicts and healthy controls in all three types of tasks. Moreover, significant differences in the Higuchi Fractal Dimension of a few EEG channels have been observed. This study confirms the importance of brain wave complexity in QEEG data analysis and assesses the correlation between EEG-complexity and gaming disorder. Moreover, feature extraction by Higuchi algorithm can render support vector machine classification of the brain waves of addicts and healthy controls more accurate.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10484-021-09518-y,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41598-021-97116-7,3D cephalometric landmark detection by multiple stage deep reinforcement learning,Scientific Reports,10.1038/s41598-021-97116-7,Nature,2021-09-01,"The lengthy time needed for manual landmarking has delayed the widespread adoption of three-dimensional (3D) cephalometry. We here propose an automatic 3D cephalometric annotation system based on multi-stage deep reinforcement learning (DRL) and volume-rendered imaging. This system considers geometrical characteristics of landmarks and simulates the sequential decision process underlying human professional landmarking patterns. It consists mainly of constructing an appropriate two-dimensional cutaway or 3D model view, then implementing single-stage DRL with gradient-based boundary estimation or multi-stage DRL to dictate the 3D coordinates of target landmarks. This system clearly shows sufficient detection accuracy and stability for direct clinical applications, with a low level of detection error and low inter-individual variation (1.96 ± 0.78 mm). Our system, moreover, requires no additional steps of segmentation and 3D mesh-object construction for landmark detection. We believe these system features will enable fast-track cephalometric analysis and planning and expect it to achieve greater accuracy as larger CT datasets become available for training and testing.",https://www.nature.com/articles/s41598-021-97116-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00170-021-07543-z,Decision-making in a fast fashion company in the Industry 4.0 era: a Digital Twin proposal to support operational planning,The International Journal of Advanced Manufacturing Technology,10.1007/s00170-021-07543-z,Springer,2021-09-01,"This work explores the improvement of operational decision-making in a Fast Fashion manufacturing company, considering the Industry 4.0 era. The segment requires agile and flexible decision-making techniques to guarantee the companies survival in a high variety environment of products and demand. The proposed approach was based on three stages. First, we suggested changes and improvements in the system to adapt it to the Industry 4.0 principles. Then, we proposed a Digital Twin (DT) focused on operational resource planning (physical and human). The DT was composed of a Discrete Event Simulation model, an Artificial Intelligence model, and a decision dashboard that provides a user-friendly interface for the decision-maker. Finally, the last stage corresponds to cyclical and constant DT-based decision-making. The DT-based decisions helped to decrease the number of operators in the line reducing their idleness and, at the same time, the total lead time became shorter. Therefore, we highlight that the concepts and solutions of Industry 4.0 might be consistent with small companies without major structural changes, contributing to the evolution of the manufacturing systems.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00170-021-07543-z,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11093-7,Exploiting local spatio-temporal characteristics for effective video understanding,Multimedia Tools and Applications,10.1007/s11042-021-11093-7,Springer,2021-09-01,"The explosive growth in online video streaming presents challenges for video understanding with high accuracy and low computation complexity. Recent methods have realized global video representation without considering the local spatial structures of the videos over time. In this paper, we propose a method called partial channel fusion (PCF), which exploits local spatio-temporal characteristics for video understanding. We also present an agnostic and effective module for PCF which can provide both high efficiency and high performance in a variety of networks. Rather than independently modeling the spatial structure and motion structure of videos, the PCF module enables information exchange among multiple frames by partially fusing channels over the temporal dimension. By inserting the PCF module into different layers of a 2D convolutional network (2D-convNets), the local and global spatio-temporal characteristics of videos can be captured. Experimental results on two challenging datasets demonstrate the superiority of PCF in improving the accuracy of a 2D-convNets, advancing the state-of-the-art without increasing computational complexity.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11093-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00542-020-05084-1,Predictions optimal routing algorithm based on artificial intelligence technique for 3D NoC systems,Microsystem Technologies,10.1007/s00542-020-05084-1,Springer,2021-09-01,"Recently, the demand for features such as shrinkable sizes, and the concurrent need to pack increasing numbers of transistors into a single chip, have led to the utilization of hundreds of CMPs as processer elements for significant data processing, such as cloud computing systems with high performance and minimum latency-power consumption. A 3D NoC is introduced as a promising solution for the next generations of CMPs. However, there are different design issues, as selecting an efficient routing algorithm is a process which still faces some challenges. In this paper, to handle the deficiencies of the selection of the optimal routing algorithms, artificial intelligence technology is used to predict the efficient routing algorithm with higher throughput and lower power consumption. Experimental results based on the 3D NOXIM simulator are presented, and illustrate that the performance of the proposed system can predict, with high accuracy, optimal routing algorithms by switching between existing 3D routing algorithms, depending on the traffic load rate for the NoC system. The NN-prediction approach is tested under PARSEC workloads to validate the effectiveness of 3D NoC throughput, energy consumption and hotspot distribution metrics.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00542-020-05084-1,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00464-021-08578-9,SAGES consensus recommendations on an annotation framework for surgical video,Surgical Endoscopy,10.1007/s00464-021-08578-9,Springer,2021-09-01,"Background The growing interest in analysis of surgical video through machine learning has led to increased research efforts; however, common methods of annotating video data are lacking. There is a need to establish recommendations on the annotation of surgical video data to enable assessment of algorithms and multi-institutional collaboration. Methods Four working groups were formed from a pool of participants that included clinicians, engineers, and data scientists. The working groups were focused on four themes: (1) temporal models, (2) actions and tasks, (3) tissue characteristics and general anatomy, and (4) software and data structure. A modified Delphi process was utilized to create a consensus survey based on suggested recommendations from each of the working groups. Results After three Delphi rounds, consensus was reached on recommendations for annotation within each of these domains. A hierarchy for annotation of temporal events in surgery was established. Conclusions While additional work remains to achieve accepted standards for video annotation in surgery, the consensus recommendations on a general framework for annotation presented here lay the foundation for standardization. This type of framework is critical to enabling diverse datasets, performance benchmarks, and collaboration.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00464-021-08578-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12205-021-1272-6,A Markov Decision Process Workflow for Automating Interior Design,KSCE Journal of Civil Engineering,10.1007/s12205-021-1272-6,Springer,2021-09-01,"We present a novel workflow based on artificial intelligence (AI) techniques to automate interior design processes. We discuss the essential steps for creating an intelligent agent that can automatically perceive the design environment (perception) and produce design ideas (action). In the first step, we use photographs or video images to model three-dimensional coordinates and exact positions of surface points on objects inside the interior space. We then convert the collected spatial data to a set or cloud of points. To fully model the interior space, we create either a triangulated surface or a mesh from the points and then transform it into a detailed building information model (BIM). Last, we apply texture data to either the 3D surface/mesh or the building information model. In the second step, we develop a sequential decision-making model based on Markov decision process for the intelligent agent to make design decisions in the BIM environment. We apply the proposed workflow to a case study with 512 possible design options, conduct experiments with 20 participants where design decisions are made based on AI insights, and perform statistical analysis on the experiment results. Our findings show the proposed workflow is capable of improving participants’ satisfaction by only searching through on average 5.1% of all possible design options. Also, across all performance measures, design decisions proposed by the AI system outperform designs made randomly.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12205-021-1272-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s00371-021-02245-9,Residual connection-based graph convolutional neural networks for gait recognition,The Visual Computer,10.1007/s00371-021-02245-9,Springer,2021-09-01,"The walking manner of a person, also known as gait, is a unique behavioral biometric trait. Existing methods for gait recognition predominantly utilize traditional machine learning. However, the performance of gait recognition can deteriorate under challenging conditions including environmental occlusion, bulky clothing, and different viewing angles. To provide an effective solution to gait recognition under these conditions, this paper proposes a novel deep learning architecture using Graph Convolutional Neural Network (GCNN) that incorporates residual connections for gait recognition from videos. The optimized feature map of the proposed GCNN architecture exhibits the invariant property to viewing angle and subject’s clothing. The residual connection is used to capture both spatial and temporal features of a gait sequence. The kinematic dependency extracted from shallower network layer is propagated to deeper layer using residual connection-based GCNN architecture. The proposed method is validated on CASIA-B gait dataset and outperforms all recent state-of-the-art methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02245-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s42044-020-00076-w,Designing a new recurrent convolutional neural network for face detection and recognition in a color image,Iran Journal of Computer Science,10.1007/s42044-020-00076-w,Springer,2021-09-01,"One of the hot and popular topics in the today’s modern world is face recognition. It is especially useful in security issues for identifying people and finding their background. In general, in the process of diagnosis, images or features are usually converted to vectors. These methods usually leads to distortion in the correlation information of the elements in the removal of an image matrix. In this article, while introducing a new recurrent convolutional neural network (RCNN), it is used to face detection and recognition in color images. The use of the radial basis function neural network (RBFN) as well as the application of feedback in it, has led to the creation of a very strong convolution neural network for face recognition. The recurrent convolutional neural network first receives the image database as a 3D matrix and, after training, selects the closest face with acceptable accuracy. A comparison between recurrent convolutional neural network and traditional convolutional neural network has been done in the experimental analysis. There is also a comparison in the recognition rate between our proposed method and a number of recent articles. The experimental results show the efficiency of the proposed recurrent convolutional neural network.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s42044-020-00076-w,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11265-021-01661-3,PureMIC: A New Audio Dataset for the Classification of Musical Instruments based on Convolutional Neural Networks,Journal of Signal Processing Systems,10.1007/s11265-021-01661-3,Springer,2021-09-01,"Automatic classification of musical instruments from audio relies heavily on datasets of acoustic recordings of the instruments to train models of those instruments. To do this, precise labels of the instrument’s events are mandatory. Also, it is very difficult to obtain such labels, especially in polyphonic performances. OpenMic-2018 is a polyphonic dataset created specifically with the aim to train instrument models. However, this dataset is based on weak and incomplete labels. The automatic classification of sound events, based on the VGGish bottleneck layer as proposed before by the AudioSet, implies the classification of only one second at a time, making it hard to find the label of that exact moment. To answer this question, this paper proposes PureMIC, a new strongly labeled dataset (SLD) that isolates 1000 single instrument clips manually labeled . Moreover, the proposed model classifies clips over time and also enhances the labeling robustness of a high number of unlabeled samples in OpenMIC-2018 due to its ability of classification over time. In the paper we disambiguate and report the automatic labeling of previously unlabeled samples. The proposed new labels achieve a mean average precision (mAP) of 0.701 for OpenMIC test data, outperforming its baseline ( 0.66 ). The code is released online so that the research community can replicate and follow the proposed implementation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11265-021-01661-3,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s12524-021-01387-6,The Method of Classifying Fog Level of Outdoor Video Images Based on Convolutional Neural Networks,Journal of the Indian Society of Remote Sensing,10.1007/s12524-021-01387-6,Springer,2021-09-01,"The outdoor video has become an important mean of monitoring on-site and target recognition recently, which requires higher and higher quality of the outdoor video images. Foggy weather can cause a significant drop in the quality of outdoor video images, directly affecting the accuracy of monitoring on-site and target recognition. In order to automatically select high-quality images with no fog or mist from the outdoor video, a method of fog level classification in outdoor video images based on convolutional neural networks (CNN) is proposed in this paper. A type of CNN with three convolutional layers, three pooling layers, and three fully connected layers is constructed, and the parameters such as the convolution kernel size, pooling kernel size and padding size are adjusted to obtain the optimal network type. The network can automatically learn the fog concentration characteristics in the image, and can effectively divide the image into three levels: fog-free, mist and dense fog. The method is compared with K-means clustering, support vector machine, and random forest classification. The results show that the constructed CNN has the highest accuracy for fog classification, which can reach more than 99%. Nevertheless, the accuracies of K-means clustering, support vector machine (SVM), and random forest classification are 85.60%, 81.48%, and 89.48%, respectively. With the CNN model constructed in this study for fog level classification, high-quality images with no fog or mist can be automatically selected for outdoor video image analysis and applications.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s12524-021-01387-6,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s10586-020-03207-x,FastTTPS: fast approach for video transcoding time prediction and scheduling for HTTP adaptive streaming videos,Cluster Computing,10.1007/s10586-020-03207-x,Springer,2021-09-01,"HTTP adaptive streaming of video content becomes an integrated part of the Internet and dominates other streaming protocols and solutions. The duration of creating video content for adaptive streaming ranges from seconds or up to several hours or days, due to the plethora of video transcoding parameters and video source types. Although, the computing resources of different transcoding platforms and services constantly increase, accurate and fast transcoding time prediction and scheduling is still crucial. We propose in this paper a novel method called fast video transcoding time prediction and scheduling (FastTTPS) of x264 encoded videos based on three phases: (i) transcoding data engineering, (ii) transcoding time prediction, and (iii) transcoding scheduling. The first phase is responsible for video sequence selection, segmentation and feature data collection required for predicting the transcoding time. The second phase develops an artificial neural network (ANN) model for segment transcoding time prediction based on transcoding parameters and derived video complexity features. The third phase compares a number of parallel schedulers to map the predicted transcoding segments on the underlying high-performance computing resources. Experimental results show that our predictive ANN model minimizes the transcoding mean absolute error (MAE) and mean square error (MSE) by up to 1.7 and 26.8, respectively. In terms of scheduling, our method reduces the transcoding time by up to 38% using a Max–Min algorithm compared to the actual transcoding time without prediction information.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s10586-020-03207-x,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11760-021-01854-0,Semantic frustum-based sparsely embedded convolutional detection,"Signal, Image and Video Processing",10.1007/s11760-021-01854-0,Springer,2021-09-01,"Frustum-based 3D detection methods suffer from the ignorance of a 2D detector for that the object will never be detected in point cloud if it is omitted by a 2D image proposal. In this work, we propose a novel method named semantic frustum-based sparsely embedded convolutional detection (SFB-SECOND) for 3D object detection, which is devoted to solving the limitation of frustum-based methods, i.e., heavily relying on the accurate 2D detector. Specifically, for the image and LIDAR describing the same scene, we initially use developed methods of semantic segmentation and object detection to generate the object mask, selecting all potential targets within two confidence-related regions. Through this object mask, we quickly locate the objects of interest in LIDAR and dig them up as semantic frustum. This selected frustum not only rules out more background and irrelevant objects in LIDAR but also maximizes the use of rich 3D information. Then, to accurate the orientation estimation, we introduce a refined form of region-aware loss regression to cooperate with the region-aware frustum. Besides, a new data augmentation strategy is proposed to further make haste the convergence speed and improve detection performance. In addition, the proposed SFB-SECOND achieves state-of-the-art performances on the 3D object detection benchmark KITTI with real-time speed, showing superiority over previous methods.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11760-021-01854-0,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11517-021-02397-9,Segmentation of dermoscopy images based on deformable 3D convolution and ResU-NeXt + +,Medical & Biological Engineering & Computing,10.1007/s11517-021-02397-9,Springer,2021-09-01,"Melanoma is one of the most dangerous skin cancers. The current melanoma segmentation is mainly based on FCNs (fully connected networks) and U-Net. Nevertheless, these two kinds of neural networks are prone to parameter redundancy, and the gradient of neural networks disappears that occurs when the neural network backpropagates as the neural network gets deeper, which will reduce the Jaccard index of the skin lesion image segmentation model. To solve the above problems and improve the survival rate of melanoma patients, an improved skin lesion segmentation model based on deformable 3D convolution and ResU-NeXt++ (D3DC- ResU-NeXt++) is proposed in this paper. The new modules in D3DC-ResU-NeXt++ can replace ordinary modules in the existing 2D convolutional neural networks (CNNs) that can be trained efficiently through standard backpropagation with high segmentation accuracy. In particular, we introduce a new data preprocessing method with dilation, crop operation, resizing, and hair removal (DCRH), which improves the Jaccard index of skin lesion image segmentation. Because rectified Adam (RAdam) does not easily fall into a local optimal solution and can converge quickly in segmentation model training, we also introduce RAdam as the training optimizer. The experiments show that our model has excellent performance on the segmentation of the ISIC2018 Task I dataset, and the Jaccard index achieves 86.84%. The proposed method improves the Jaccard index of segmentation of skin lesion images and can also assist dermatological doctors in determining and diagnosing the types of skin lesions and the boundary between lesions and normal skin, so as to improve the survival rate of skin cancer patients. Graphical abstract Overview of the proposed model. An improved skin lesion segmentation model based on deformable 3D convolution and ResU-NeXt++ (D3DC- ResU-NeXt++) is proposed in this paper. D3DC-ResU-NeXt++ has strong spatial geometry processing capabilities, it is used to segment the skin lesion sample image; DCRH and transfer learning are used to preprocess the data set and D3DC-ResU-NeXt++ respectively, which can highlight the difference between the lesion area and the normal skin, and enhance the segmentation efficiency and robustness of the neural network; RAdam is used to speed up the convergence speed of neural network and improve the efficiency of segmentation.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11517-021-02397-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1038/s41467-021-25503-9,Predicting post-operative right ventricular failure using video-based deep learning,Nature Communications,10.1038/s41467-021-25503-9,Nature,2021-08-31,"Despite progressive improvements over the decades, the rich temporally resolved data in an echocardiogram remain underutilized. Human assessments reduce the complex patterns of cardiac wall motion, to a small list of measurements of heart function. All modern echocardiography artificial intelligence (AI) systems are similarly limited by design – automating measurements of the same reductionist metrics rather than utilizing the embedded wealth of data. This underutilization is most evident where clinical decision making is guided by subjective assessments of disease acuity. Predicting the likelihood of developing post-operative right ventricular failure (RV failure) in the setting of mechanical circulatory support is one such example. Here we describe a video AI system trained to predict post-operative RV failure using the full spatiotemporal density of information in pre-operative echocardiography. We achieve an AUC of 0.729, and show that this ML system significantly outperforms a team of human experts at the same task on independent evaluation. The echocardiogram allows for a comprehensive assessment of the cardiac musculature and valves, but its rich temporally resolved data remain underutilized. Here, the authors develop a video AI system trained to predict post-operative right ventricular failure.",https://www.nature.com/articles/s41467-021-25503-9,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
Article,doi:10.1007/s11042-021-11271-7,Hyperparameter search based convolution neural network with Bi-LSTM model for intrusion detection system in multimedia big data environment,Multimedia Tools and Applications,10.1007/s11042-021-11271-7,Springer,2021-08-31,"In recent years, there is an exponential increase in the growth of the multimedia data, which is being generated from zettabyte to petabyte scale. At the same time, security issues in networks, Internets and organizations are also continues to increase. The process of finding intrusions in such a big data environment is not easier. Different types of intrusion-detection system (IDS) have been presented for diverse kinds of networking attacks, however, many models could be identified unknown attacks. Deep learning (DL) approaches lately employed to large-scale big data analysis for effectual outcome. In this view, this paper presents a new deep learning based hyperparameter search (HPS) convolutional neural network with Bi-directional long short term memory (CBL) model called HPS-CBL for intrusion detection in big data environment. The HPS-CBL model make use of CBL technique for the identification of intrusions in the network. Since the proper tuning of hyperparameters of the CBL network is highly important, the proposed model uses improved genetic algorithm (IGA) for hyperparameter tuning. The proposed HPS-CBL is validated using a UNSW-NB15 dataset and the results are validated under diverse evaluation parameters. The obtained experimental outcome clearly stated the superior nature of the HPS-CBL model over the compared methods by attaining a maximum precision of 99.24%, recall of 98.69%, F-score of 98.97% and accuracy of 98.18% respectively.",http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11271-7,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
