id,updated,published,title,summary,database,query_name,query_value
http://arxiv.org/abs/2207.11173v1,2022-07-22T16:18:04Z,2022-07-22T16:18:04Z,Verifying Fairness in Quantum Machine Learning,"Due to the beyond-classical capability of quantum computing, quantum machine
learning is applied independently or embedded in classical models for decision
making, especially in the field of finance. Fairness and other ethical issues
are often one of the main concerns in decision making. In this work, we define
a formal framework for the fairness verification and analysis of quantum
machine learning decision models, where we adopt one of the most popular
notions of fairness in the literature based on the intuition -- any two similar
individuals must be treated similarly and are thus unbiased. We show that
quantum noise can improve fairness and develop an algorithm to check whether a
(noisy) quantum machine learning model is fair. In particular, this algorithm
can find bias kernels of quantum data (encoding individuals) during checking.
These bias kernels generate infinitely many bias pairs for investigating the
unfairness of the model. Our algorithm is designed based on a highly efficient
data structure -- Tensor Networks -- and implemented on Google's TensorFlow
Quantum. The utility and effectiveness of our algorithm are confirmed by the
experimental results, including income prediction and credit scoring on
real-world data, for a class of random (noisy) quantum decision models with 27
qubits ($2^{27}$-dimensional state space) tripling ($2^{18}$ times more than)
that of the state-of-the-art algorithms for verifying quantum machine learning
models.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2207.10130v1,2022-07-20T18:18:40Z,2022-07-20T18:18:40Z,Latent Discriminant deterministic Uncertainty,"Predictive uncertainty estimation is essential for deploying Deep Neural
Networks in real-world autonomous systems. However, most successful approaches
are computationally intensive. In this work, we attempt to address these
challenges in the context of autonomous driving perception tasks. Recently
proposed Deterministic Uncertainty Methods (DUM) can only partially meet such
requirements as their scalability to complex computer vision tasks is not
obvious. In this work we advance a scalable and effective DUM for
high-resolution semantic segmentation, that relaxes the Lipschitz constraint
typically hindering practicality of such architectures. We learn a discriminant
latent space by leveraging a distinction maximization layer over an
arbitrarily-sized set of trainable prototypes. Our approach achieves
competitive results over Deep Ensembles, the state-of-the-art for uncertainty
prediction, on image classification, segmentation and monocular depth
estimation tasks. Our code is available at https://github.com/ENSTA-U2IS/LDU",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2207.10124v1,2022-07-20T18:06:46Z,2022-07-20T18:06:46Z,"HOLISMOKES -- X. Comparison between neural network and semi-automated
  traditional modeling of strong lenses","Modeling of strongly gravitationally lensed galaxies is often required in
order to use them as astrophysical or cosmological probes. With current and
upcoming wide-field imaging surveys, the number of detected lenses is
increasing significantly such that automated and fast modeling procedures for
ground-based data are urgently needed. This is especially pertinent to
short-lived lensed transients in order to plan follow-up observations.
Therefore, we present in a companion paper (submitted) a neural network
predicting the parameter values with corresponding uncertainties of a Singular
Isothermal Ellipsoid (SIE) mass profile with external shear. In this work, we
present a newly-developed pipeline glee_auto.py to model consistently any
galaxy-scale lensing system. In contrast to previous automated modeling
pipelines that require high-resolution images, glee_auto.py is optimized for
ground-based images such as those from the Hyper-Suprime-Cam (HSC) or the
upcoming Rubin Observatory Legacy Survey of Space and Time. We further present
glee_tools.py, a flexible automation code for individual modeling that has no
direct decisions and assumptions implemented. Both pipelines, in addition to
our modeling network, minimize the user input time drastically and thus are
important for future modeling efforts. We apply the network to 31 real
galaxy-scale lenses of HSC and compare the results to the traditional models.
In the direct comparison, we find a very good match for the Einstein radius
especially for systems with $\theta_E \gtrsim 2$"". The lens mass center and
ellipticity show reasonable agreement. The main discrepancies are on the
external shear as expected from our tests on mock systems. In general, our
study demonstrates that neural networks are a viable and ultra fast approach
for measuring the lens-galaxy masses from ground-based data in the upcoming era
with $\sim10^5$ lenses expected.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2207.07958v1,2022-07-16T14:30:15Z,2022-07-16T14:30:15Z,"FastML Science Benchmarks: Accelerating Real-Time Scientific Edge
  Machine Learning","Applications of machine learning (ML) are growing by the day for many unique
and challenging scientific applications. However, a crucial challenge facing
these applications is their need for ultra low-latency and on-detector ML
capabilities. Given the slowdown in Moore's law and Dennard scaling, coupled
with the rapid advances in scientific instrumentation that is resulting in
growing data rates, there is a need for ultra-fast ML at the extreme edge. Fast
ML at the edge is essential for reducing and filtering scientific data in
real-time to accelerate science experimentation and enable more profound
insights. To accelerate real-time scientific edge ML hardware and software
solutions, we need well-constrained benchmark tasks with enough specifications
to be generically applicable and accessible. These benchmarks can guide the
design of future edge ML hardware for scientific applications capable of
meeting the nanosecond and microsecond level latency requirements. To this end,
we present an initial set of scientific ML benchmarks, covering a variety of ML
and embedded system techniques.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2207.02890v1,2022-07-06T18:07:36Z,2022-07-06T18:07:36Z,Humans Social Relationship Classification during Accompaniment,"This paper presents the design of deep learning architectures which allow to
classify the social relationship existing between two people who are walking in
a side-by-side formation into four possible categories --colleagues, couple,
family or friendship. The models are developed using Neural Networks or
Recurrent Neural Networks to achieve the classification and are trained and
evaluated using a database of readings obtained from humans performing an
accompaniment process in an urban environment. The best achieved model
accomplishes a relatively good accuracy in the classification problem and its
results enhance partially the outcomes from a previous study [1]. Furthermore,
the model proposed shows its future potential to improve its efficiency and to
be implemented in a real robot.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2207.02036v1,2022-07-05T13:27:38Z,2022-07-05T13:27:38Z,"PRoA: A Probabilistic Robustness Assessment against Functional
  Perturbations","In safety-critical deep learning applications robustness measurement is a
vital pre-deployment phase. However, existing robustness verification methods
are not sufficiently practical for deploying machine learning systems in the
real world. On the one hand, these methods attempt to claim that no
perturbations can ``fool'' deep neural networks (DNNs), which may be too
stringent in practice. On the other hand, existing works rigorously consider
$L_p$ bounded additive perturbations on the pixel space, although
perturbations, such as colour shifting and geometric transformations, are more
practically and frequently occurring in the real world. Thus, from the
practical standpoint, we present a novel and general {\it probabilistic
robustness assessment method} (PRoA) based on the adaptive concentration, and
it can measure the robustness of deep learning models against functional
perturbations. PRoA can provide statistical guarantees on the probabilistic
robustness of a model, \textit{i.e.}, the probability of failure encountered by
the trained model after deployment. Our experiments demonstrate the
effectiveness and flexibility of PRoA in terms of evaluating the probabilistic
robustness against a broad range of functional perturbations, and PRoA can
scale well to various large-scale deep neural networks compared to existing
state-of-the-art baselines. For the purpose of reproducibility, we release our
tool on GitHub: \url{ https://github.com/TrustAI/PRoA}.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2207.00670v1,2022-07-01T22:05:07Z,2022-07-01T22:05:07Z,DRESS: Dynamic REal-time Sparse Subnets,"The limited and dynamically varied resources on edge devices motivate us to
deploy an optimized deep neural network that can adapt its sub-networks to fit
in different resource constraints. However, existing works often build
sub-networks through searching different network architectures in a
hand-crafted sampling space, which not only can result in a subpar performance
but also may cause on-device re-configuration overhead. In this paper, we
propose a novel training algorithm, Dynamic REal-time Sparse Subnets (DRESS).
DRESS samples multiple sub-networks from the same backbone network through
row-based unstructured sparsity, and jointly trains these sub-networks in
parallel with weighted loss. DRESS also exploits strategies including parameter
reusing and row-based fine-grained sampling for efficient storage consumption
and efficient on-device adaptation. Extensive experiments on public vision
datasets show that DRESS yields significantly higher accuracy than
state-of-the-art sub-networks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2207.00254v1,2022-07-01T07:56:48Z,2022-07-01T07:56:48Z,"A Survey on Active Simultaneous Localization and Mapping: State of the
  Art and New Frontiers","Active Simultaneous Localization and Mapping (SLAM) is the problem of
planning and controlling the motion of a robot to build the most accurate and
complete model of the surrounding environment. Since the first foundational
work in active perception appeared, more than three decades ago, this field has
received increasing attention across different scientific communities. This has
brought about many different approaches and formulations, and makes a review of
the current trends necessary and extremely valuable for both new and
experienced researchers. In this work, we survey the state-of-the-art in active
SLAM and take an in-depth look at the open challenges that still require
attention to meet the needs of modern applications. % in order to achieve
real-world deployment. After providing a historical perspective, we present a
unified problem formulation and review the classical solution scheme, which
decouples the problem into three stages that identify, select, and execute
potential navigation actions. We then analyze alternative approaches, including
belief-space planning and modern techniques based on deep reinforcement
learning, and review related work on multi-robot coordination. The manuscript
concludes with a discussion of new research directions, addressing reproducible
research, active spatial perception, and practical applications, among other
topics.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.13703v2,2022-07-11T00:13:42Z,2022-06-28T02:27:23Z,"Kwame for Science: An AI Teaching Assistant Based on Sentence-BERT for
  Science Education in West Africa","Africa has a high student-to-teacher ratio which limits students' access to
teachers. Consequently, students struggle to get answers to their questions. In
this work, we extended Kwame, our previous AI teaching assistant, adapted it
for science education, and deployed it as a web app. Kwame for Science answers
questions of students based on the Integrated Science subject of the West
African Senior Secondary Certificate Examination (WASSCE). Kwame for Science is
a Sentence-BERT-based question-answering web app that displays 3 paragraphs as
answers along with a confidence score in response to science questions.
Additionally, it displays the top 5 related past exam questions and their
answers in addition to the 3 paragraphs. Our preliminary evaluation of the
Kwame for Science with a 2.5-week real-world deployment showed a top 3 accuracy
of 87.5% (n=56) with 190 users across 11 countries. Kwame for Science will
enable the delivery of scalable, cost-effective, and quality remote education
to millions of people across Africa.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.12542v1,2022-06-25T03:02:25Z,2022-06-25T03:02:25Z,"Value-Consistent Representation Learning for Data-Efficient
  Reinforcement Learning","Deep reinforcement learning (RL) algorithms suffer severe performance
degradation when the interaction data is scarce, which limits their real-world
application. Recently, visual representation learning has been shown to be
effective and promising for boosting sample efficiency in RL. These methods
usually rely on contrastive learning and data augmentation to train a
transition model for state prediction, which is different from how the model is
used in RL--performing value-based planning. Accordingly, the learned model may
not be able to align well with the environment and generate consistent value
predictions, especially when the state transition is not deterministic. To
address this issue, we propose a novel method, called value-consistent
representation learning (VCR), to learn representations that are directly
related to decision-making. More specifically, VCR trains a model to predict
the future state (also referred to as the ''imagined state'') based on the
current one and a sequence of actions. Instead of aligning this imagined state
with a real state returned by the environment, VCR applies a $Q$-value head on
both states and obtains two distributions of action values. Then a distance is
computed and minimized to force the imagined state to produce a similar action
value prediction as that by the real state. We develop two implementations of
the above idea for the discrete and continuous action spaces respectively. We
conduct experiments on Atari 100K and DeepMind Control Suite benchmarks to
validate their effectiveness for improving sample efficiency. It has been
demonstrated that our methods achieve new state-of-the-art performance for
search-free RL algorithms.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.11623v1,2022-06-23T11:21:04Z,2022-06-23T11:21:04Z,"Waypoint Generation in Row-based Crops with Deep Learning and
  Contrastive Clustering","The development of precision agriculture has gradually introduced automation
in the agricultural process to support and rationalize all the activities
related to field management. In particular, service robotics plays a
predominant role in this evolution by deploying autonomous agents able to
navigate in fields while executing different tasks without the need for human
intervention, such as monitoring, spraying and harvesting. In this context,
global path planning is the first necessary step for every robotic mission and
ensures that the navigation is performed efficiently and with complete field
coverage. In this paper, we propose a learning-based approach to tackle
waypoint generation for planning a navigation path for row-based crops,
starting from a top-view map of the region-of-interest. We present a novel
methodology for waypoint clustering based on a contrastive loss, able to
project the points to a separable latent space. The proposed deep neural
network can simultaneously predict the waypoint position and cluster assignment
with two specialized heads in a single forward pass. The extensive
experimentation on simulated and real-world images demonstrates that the
proposed approach effectively solves the waypoint generation problem for both
straight and curved row-based crops, overcoming the limitations of previous
state-of-the-art methodologies.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.11457v1,2022-06-23T02:29:13Z,2022-06-23T02:29:13Z,"Exploring Physics of Ferroelectric Domain Walls in Real Time: Deep
  Learning Enabled Scanning Probe Microscopy","The functionality of ferroelastic domain walls in ferroelectric materials is
explored in real-time via the in-situ implementation of computer vision
algorithms in scanning probe microscopy (SPM) experiment. The robust deep
convolutional neural network (DCNN) is implemented based on a deep residual
learning framework (Res) and holistically-nested edge detection (Hed), and
ensembled to minimize the out-of-distribution drift effects. The DCNN is
implemented for real-time operations on SPM, converting the data stream into
the semantically segmented image of domain walls and the corresponding
uncertainty. We further demonstrate the pre-selected experimental workflows on
thus discovered domain walls, and report alternating high- and low-
polarization dynamic (out-of-plane) ferroelastic domain walls in a (PbTiO3) PTO
thin film and high polarization dynamic (out-of-plane) at short ferroelastic
walls (compared with long ferroelastic walls) in a lead zirconate titanate
(PZT) thin film. This work establishes the framework for real-time DCNN
analysis of data streams in scanning probe and other microscopies and
highlights the role of out-of-distribution effects and strategies to ameliorate
them in real time analytics.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.10122v1,2022-06-21T05:53:23Z,2022-06-21T05:53:23Z,"Safe and Psychologically Pleasant Traffic Signal Control with
  Reinforcement Learning using Action Masking","Reinforcement learning (RL) for traffic signal control (TSC) has shown better
performance in simulation for controlling the traffic flow of intersections
than conventional approaches. However, due to several challenges, no RL-based
TSC has been deployed in the field yet. One major challenge for real-world
deployment is to ensure that all safety requirements are met at all times
during operation. We present an approach to ensure safety in a real-world
intersection by using an action space that is safe by design. The action space
encompasses traffic phases, which represent the combination of non-conflicting
signal colors of the intersection. Additionally, an action masking mechanism
makes sure that only appropriate phase transitions are carried out. Another
challenge for real-world deployment is to ensure a control behavior that avoids
stress for road users. We demonstrate how to achieve this by incorporating
domain knowledge through extending the action masking mechanism. We test and
verify our approach in a realistic simulation scenario. By ensuring safety and
psychologically pleasant control behavior, our approach drives development
towards real-world deployment of RL for TSC.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.09743v1,2022-06-20T12:46:35Z,2022-06-20T12:46:35Z,"Guided Safe Shooting: model based reinforcement learning with safety
  constraints","In the last decade, reinforcement learning successfully solved complex
control tasks and decision-making problems, like the Go board game. Yet, there
are few success stories when it comes to deploying those algorithms to
real-world scenarios. One of the reasons is the lack of guarantees when dealing
with and avoiding unsafe states, a fundamental requirement in critical control
engineering systems. In this paper, we introduce Guided Safe Shooting (GuSS), a
model-based RL approach that can learn to control systems with minimal
violations of the safety constraints. The model is learned on the data
collected during the operation of the system in an iterated batch fashion, and
is then used to plan for the best action to perform at each time step. We
propose three different safe planners, one based on a simple random shooting
strategy and two based on MAP-Elites, a more advanced divergent-search
algorithm. Experiments show that these planners help the learning agent avoid
unsafe situations while maximally exploring the state space, a necessary aspect
when learning an accurate model of the system. Furthermore, compared to
model-free approaches, learning a model allows GuSS reducing the number of
interactions with the real-system while still reaching high rewards, a
fundamental requirement when handling engineering systems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.08536v1,2022-06-17T04:03:25Z,2022-06-17T04:03:25Z,Low-latency Mini-batch GNN Inference on CPU-FPGA Heterogeneous Platform,"Mini-batch inference of Graph Neural Networks (GNNs) is a key problem in many
real-world applications. Recently, a GNN design principle of model
depth-receptive field decoupling has been proposed to address the well-known
issue of neighborhood explosion. Decoupled GNN models achieve higher accuracy
than original models and demonstrate excellent scalability for mini-batch
inference.
  We map Decoupled GNNs onto CPU-FPGA heterogeneous platforms to achieve
low-latency mini-batch inference. On the FPGA platform, we design a novel GNN
hardware accelerator with an adaptive datapath denoted Adaptive Computation
Kernel (ACK) that can execute various computation kernels of GNNs with
low-latency: (1) for dense computation kernels expressed as matrix
multiplication, ACK works as a systolic array with fully localized connections,
(2) for sparse computation kernels, ACK follows the scatter-gather paradigm and
works as multiple parallel pipelines to support the irregular connectivity of
graphs. The proposed task scheduling hides the CPU-FPGA data communication
overhead to reduce the inference latency. We develop a fast design space
exploration algorithm to generate a single accelerator for multiple target GNN
models. We implement our accelerator on a state-of-the-art CPU-FPGA platform
and evaluate the performance using three representative models (GCN, GraphSAGE,
and GAT). Results show that our CPU-FPGA implementation achieves
$21.4-50.8\times$, $2.9-21.6\times$, $4.7\times$ latency reduction compared
with state-of-the-art implementations on CPU-only, CPU-GPU and CPU-FPGA
platforms.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.07635v1,2022-06-15T16:25:57Z,2022-06-15T16:25:57Z,AI Ethics Issues in Real World: Evidence from AI Incident Database,"With the powerful performance of Artificial Intelligence (AI) also comes
prevalent ethical issues. Though governments and corporations have curated
multiple AI ethics guidelines to curb unethical behavior of AI, the effect has
been limited, probably due to the vagueness of the guidelines. In this paper,
we take a closer look at how AI ethics issues take place in real world, in
order to have a more in-depth and nuanced understanding of different ethical
issues as well as their social impact. With a content analysis of AI Incident
Database, which is an effort to prevent repeated real world AI failures by
cataloging incidents, we identified 13 application areas which often see
unethical use of AI, with intelligent service robots, language/vision models
and autonomous driving taking the lead. Ethical issues appear in 8 different
forms, from inappropriate use and racial discrimination, to physical safety and
unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI
practitioners with a practical guideline when trying to deploy AI applications
ethically.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.07459v1,2022-06-15T11:30:41Z,2022-06-15T11:30:41Z,"READ: Aggregating Reconstruction Error into Out-of-distribution
  Detection","Detecting out-of-distribution (OOD) samples is crucial to the safe deployment
of a classifier in the real world. However, deep neural networks are known to
be overconfident for abnormal data. Existing works directly design score
function by mining the inconsistency from classifier for in-distribution (ID)
and OOD. In this paper, we further complement this inconsistency with
reconstruction error, based on the assumption that an autoencoder trained on ID
data can not reconstruct OOD as well as ID. We propose a novel method, READ
(Reconstruction Error Aggregated Detector), to unify inconsistencies from
classifier and autoencoder. Specifically, the reconstruction error of raw
pixels is transformed to latent space of classifier. We show that the
transformed reconstruction error bridges the semantic gap and inherits
detection performance from the original. Moreover, we propose an adjustment
strategy to alleviate the overconfidence problem of autoencoder according to a
fine-grained characterization of OOD data. Under two scenarios of pre-training
and retraining, we respectively present two variants of our method, namely
READ-MD (Mahalanobis Distance) only based on pre-trained classifier and READ-ED
(Euclidean Distance) which retrains the classifier. Our methods do not require
access to test time OOD data for fine-tuning hyperparameters. Finally, we
demonstrate the effectiveness of the proposed methods through extensive
comparisons with state-of-the-art OOD detection algorithms. On a CIFAR-10
pre-trained WideResNet, our method reduces the average FPR@95TPR by up to 9.8%
compared with previous state-of-the-art.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.06004v1,2022-06-13T09:48:38Z,2022-06-13T09:48:38Z,"A Novel Multi-Layer Modular Approach for Real-Time Gravitational-Wave
  Detection","Advanced LIGO and Advanced Virgo ground-based interferometers are poised to
probe an unprecedentedly large volume of space, enhancing the discovery power
of the observations to even new sources of gravitational wave emitters. In this
scenario, the development of highly optimized gravitational wave detection
algorithms is crucial. We propose a novel layered framework for real-time
detection of gravitational waves inspired by speech processing techniques and,
in the present implementation, based on a state-of-the-art machine learning
approach involving a hybridization of genetic programming and neural networks.
The key aspects of the newly proposed framework are: the well structured,
layered approach, and the low computational complexity. The paper describes the
basic concepts of the framework and the derivation of the first three layers.
Even if, in the present implementation, the layers are based on models derived
using a machine learning approach, the proposed layered structure has a
universal nature. To train and test the models, we used simulated binary black
hole gravitational wave waveforms in synthetic Gaussian noise representative of
Advanced LIGO sensitivity design. Compared to more complex approaches, such as
convolutional neural networks, our framework, even using the simple ground
model described in the paper, has similar performance but with a much lower
computational complexity and a higher degree of modularity. Furthermore, the
underlying exploitation of short-term features makes the results of the new
framework virtually independent against time-position of gravitational wave
signals, simplifying its future exploitation in real-time multi-layer pipelines
for gravitational-wave detection with second generation interferometers.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.05592v1,2022-06-11T19:18:59Z,2022-06-11T19:18:59Z,"Homunculus: Auto-Generating Efficient Data-Plane ML Pipelines for
  Datacenter Networks","Support for Machine Learning (ML) applications in networks has significantly
improved over the last decade. The availability of public datasets and
programmable switching fabrics (including low-level languages to program them)
present a full-stack to the programmer for deploying in-network ML. However,
the diversity of tools involved, coupled with complex optimization tasks of ML
model design and hyperparameter tuning while complying with the network
constraints (like throughput and latency), put the onus on the network operator
to be an expert in ML, network design, and programmable hardware. This
multi-faceted nature of in-network tools and expertise in ML and hardware is a
roadblock for ML to become mainstream in networks, today.
  We present Homunculus, a high-level framework that enables network operators
to specify their ML requirements in a declarative, rather than imperative way.
Homunculus takes as input, the training data and accompanying network
constraints, and automatically generates and installs a suitable model onto the
underlying switching hardware. It performs model design-space exploration,
training, and platform code-generation as compiler stages, leaving network
operators to focus on acquiring high-quality network data. Our evaluations on
real-world ML applications show that Homunculus's generated models achieve up
to 12% better F1 score compared to hand-tuned alternatives, while requiring
only 30 lines of single-script code on average. We further demonstrate the
performance of the generated models on emerging per-packet ML platforms to
showcase its timely and practical significance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.03957v1,2022-06-08T15:22:41Z,2022-06-08T15:22:41Z,"Construction of a spike-based memory using neural-like logic gates based
  on Spiking Neural Networks on SpiNNaker","Neuromorphic engineering concentrates the efforts of a large number of
researchers due to its great potential as a field of research, in a search for
the exploitation of the advantages of the biological nervous system and the
brain as a whole for the design of more efficient and real-time capable
applications. For the development of applications as close to biology as
possible, Spiking Neural Networks (SNNs) are used, considered
biologically-plausible and that form the third generation of Artificial Neural
Networks (ANNs). Since some SNN-based applications may need to store data in
order to use it later, something that is present both in digital circuits and,
in some form, in biology, a spiking memory is needed. This work presents a
spiking implementation of a memory, which is one of the most important
components in the computer architecture, and which could be essential in the
design of a fully spiking computer. In the process of designing this spiking
memory, different intermediate components were also implemented and tested. The
tests were carried out on the SpiNNaker neuromorphic platform and allow to
validate the approach used for the construction of the presented blocks. In
addition, this work studies in depth how to build spiking blocks using this
approach and includes a comparison between it and those used in other similar
works focused on the design of spiking components, which include both spiking
logic gates and spiking memory. All implemented blocks and developed tests are
available in a public repository.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.06780v1,2022-06-08T11:18:02Z,2022-06-08T11:18:02Z,"Memory-Oriented Design-Space Exploration of Edge-AI Hardware for XR
  Applications","Low-Power Edge-AI capabilities are essential for on-device extended reality
(XR) applications to support the vision of Metaverse. In this work, we
investigate two representative XR workloads: (i) Hand detection and (ii) Eye
segmentation, for hardware design space exploration. For both applications, we
train deep neural networks and analyze the impact of quantization and hardware
specific bottlenecks. Through simulations, we evaluate a CPU and two systolic
inference accelerator implementations. Next, we compare these hardware
solutions with advanced technology nodes. The impact of integrating
state-of-the-art emerging non-volatile memory technology (STT/SOT/VGSOT MRAM)
into the XR-AI inference pipeline is evaluated. We found that significant
energy benefits (>=80%) can be achieved for hand detection (IPS=40) and eye
segmentation (IPS=6) by introducing non-volatile memory in the memory hierarchy
for designs at 7nm node while meeting minimum IPS (inference per second).
Moreover, we can realize substantial reduction in area (>=30%) owing to the
small form factor of MRAM compared to traditional SRAM.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.03669v2,2022-06-09T03:51:44Z,2022-06-08T04:09:13Z,Toward Certified Robustness Against Real-World Distribution Shifts,"We consider the problem of certifying the robustness of deep neural networks
against real-world distribution shifts. To do so, we bridge the gap between
hand-crafted specifications and realistic deployment settings by proposing a
novel neural-symbolic verification framework, in which we train a generative
model to learn perturbations from data and define specifications with respect
to the output of the learned model. A unique challenge arising from this
setting is that existing verifiers cannot tightly approximate sigmoid
activations, which are fundamental to many state-of-the-art generative models.
To address this challenge, we propose a general meta-algorithm for handling
sigmoid activations which leverages classical notions of counter-example-guided
abstraction refinement. The key idea is to ""lazily"" refine the abstraction of
sigmoid functions to exclude spurious counter-examples found in the previous
abstraction, thus guaranteeing progress in the verification process while
keeping the state-space small. Experiments on the MNIST and CIFAR-10 datasets
show that our framework significantly outperforms existing methods on a range
of challenging distribution shifts.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.01364v1,2022-06-03T02:04:15Z,2022-06-03T02:04:15Z,"Robotic Planning under Uncertainty in Spatiotemporal Environments in
  Expeditionary Science","In the expeditionary sciences, spatiotemporally varying environments --
hydrothermal plumes, algal blooms, lava flows, or animal migrations -- are
ubiquitous. Mobile robots are uniquely well-suited to study these dynamic,
mesoscale natural environments. We formalize expeditionary science as a
sequential decision-making problem, modeled using the language of
partially-observable Markov decision processes (POMDPs). Solving the
expeditionary science POMDP under real-world constraints requires efficient
probabilistic modeling and decision-making in problems with complex dynamics
and observational models. Previous work in informative path planning, adaptive
sampling, and experimental design have shown compelling results, largely in
static environments, using data-driven models and information-based rewards.
However, these methodologies do not trivially extend to expeditionary science
in spatiotemporal environments: they generally do not make use of scientific
knowledge such as equations of state dynamics, they focus on information
gathering as opposed to scientific task execution, and they make use of
decision-making approaches that scale poorly to large, continuous problems with
long planning horizons and real-time operational constraints. In this work, we
discuss these and other challenges related to probabilistic modeling and
decision-making in expeditionary science, and present some of our preliminary
work that addresses these gaps. We ground our results in a real expeditionary
science deployment of an autonomous underwater vehicle (AUV) in the deep ocean
for hydrothermal vent discovery and characterization. Our concluding thoughts
highlight remaining work to be done, and the challenges that merit
consideration by the reinforcement learning and decision-making community.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.01272v1,2022-06-02T19:52:11Z,2022-06-02T19:52:11Z,"Data-Driven Linear Koopman Embedding for Model-Predictive Power System
  Control","This paper presents a linear Koopman embedding for model predictive emergency
voltage regulation in power systems, by way of a data-driven lifting of the
system dynamics into a higher dimensional linear space over which the MPC
(model predictive control) is exercised, thereby scaling as well as expediting
the MPC computation for its real-time implementation for practical systems. We
develop a {\em Koopman-inspired deep neural network} (KDNN) architecture for
the linear embedding of the voltage dynamics subjected to reactive controls.
The training of the KDNN for the purposes of linear embedding is done using the
simulated voltage trajectories under a variety of applied control inputs and
load conditions. The proposed framework learns the underlying system dynamics
from the input/output data in the form of a triple of transforms: A Neural
Network (NN)-based lifting to a higher dimension, a linear dynamics within that
higher dynamics, and an NN-based projection to original space. This approach
alleviates the burden of an ad-hoc selection of the basis functions for the
purposes of lifting to higher dimensional linear space. The MPC is computed
over the linear dynamics, making the control computation scalable and also
real-time.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2206.00129v1,2022-05-31T22:16:44Z,2022-05-31T22:16:44Z,Fairness Transferability Subject to Bounded Distribution Shift,"Given an algorithmic predictor that is ""fair"" on some source distribution,
will it still be fair on an unknown target distribution that differs from the
source within some bound? In this paper, we study the transferability of
statistical group fairness for machine learning predictors (i.e., classifiers
or regressors) subject to bounded distribution shift, a phenomenon frequently
caused by user adaptation to a deployed model or a dynamic environment. Herein,
we develop a bound characterizing such transferability, flagging potentially
inappropriate deployments of machine learning for socially consequential tasks.
We first develop a framework for bounding violations of statistical fairness
subject to distribution shift, formulating a generic upper bound for
transferred fairness violation as our primary result. We then develop bounds
for specific worked examples, adopting two commonly used fairness definitions
(i.e., demographic parity and equalized odds) for two classes of distribution
shift (i.e., covariate shift and label shift). Finally, we compare our
theoretical bounds to deterministic models of distribution shift as well as
real-world data.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.14136v1,2022-05-27T17:55:54Z,2022-05-27T17:55:54Z,PSL is Dead. Long Live PSL,"Property Specification Language (PSL) is a form of temporal logic that has
been mainly used in discrete domains (e.g. formal hardware verification). In
this paper, we show that by merging machine learning techniques with PSL
monitors, we can extend PSL to work on continuous domains. We apply this
technique in machine learning-based anomaly detection to analyze scenarios of
real-time streaming events from continuous variables in order to detect
abnormal behaviors of a system. By using machine learning with formal models,
we leverage the strengths of both machine learning methods and formal semantics
of time. On one hand, machine learning techniques can produce distributions on
continuous variables, where abnormalities can be captured as deviations from
the distributions. On the other hand, formal methods can characterize discrete
temporal behaviors and relations that cannot be easily learned by machine
learning techniques. Interestingly, the anomalies detected by machine learning
and the underlying time representation used are discrete events. We implemented
a temporal monitoring package (TEF) that operates in conjunction with normal
data science packages for anomaly detection machine learning systems, and we
show that TEF can be used to perform accurate interpretation of temporal
correlation between events.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.10044v1,2022-05-20T09:35:26Z,2022-05-20T09:35:26Z,Towards biologically plausible Dreaming and Planning,"Humans and animals can learn new skills after practicing for a few hours,
while current reinforcement learning algorithms require a large amount of data
to achieve good performances. Recent model-based approaches show promising
results by reducing the number of necessary interactions with the environment
to learn a desirable policy. However, these methods require biological
implausible ingredients, such as the detailed storage of older experiences, and
long periods of offline learning. The optimal way to learn and exploit
word-models is still an open question. Taking inspiration from biology, we
suggest that dreaming might be an efficient expedient to use an inner model. We
propose a two-module (agent and model) neural network in which ""dreaming""
(living new experiences in a model-based simulated environment) significantly
boosts learning. We also explore ""planning"", an online alternative to dreaming,
that shows comparable performances. Importantly, our model does not require the
detailed storage of experiences, and learns online the world-model. This is a
key ingredient for biological plausibility and implementability (e.g., in
neuromorphic hardware). Our network is composed of spiking neurons, further
increasing the energetic efficiency and the plausibility of the model. To our
knowledge, there are no previous works proposing biologically plausible
model-based reinforcement learning in recurrent spiking networks. Our work is a
step toward building efficient neuromorphic systems for autonomous robots,
capable of learning new skills in real-world environments. Even when the
environment is no longer accessible, the robot optimizes learning by
""reasoning"" in its own ""mind"". These approaches are of great relevance when the
acquisition from the environment is slow, expensive (robotics) or unsafe
(autonomous driving).",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.09934v2,2022-07-01T01:50:37Z,2022-05-20T02:50:15Z,Towards Explanation for Unsupervised Graph-Level Representation Learning,"Due to the superior performance of Graph Neural Networks (GNNs) in various
domains, there is an increasing interest in the GNN explanation problem
""\emph{which fraction of the input graph is the most crucial to decide the
model's decision?}"" Existing explanation methods focus on the supervised
settings, \eg, node classification and graph classification, while the
explanation for unsupervised graph-level representation learning is still
unexplored. The opaqueness of the graph representations may lead to unexpected
risks when deployed for high-stake decision-making scenarios. In this paper, we
advance the Information Bottleneck principle (IB) to tackle the proposed
explanation problem for unsupervised graph representations, which leads to a
novel principle, \textit{Unsupervised Subgraph Information Bottleneck} (USIB).
We also theoretically analyze the connection between graph representations and
explanatory subgraphs on the label space, which reveals that the expressiveness
and robustness of representations benefit the fidelity of explanatory
subgraphs. Experimental results on both synthetic and real-world datasets
demonstrate the superiority of our developed explainer and the validity of our
theoretical analysis.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.08404v1,2022-05-17T14:38:38Z,2022-05-17T14:38:38Z,"A Comprehensive Study on Artificial Intelligence Algorithms to Implement
  Safety Using Communication Technologies","The recent development of artificial intelligence (AI) has increased the
interest of researchers and practitioners towards applying its techniques into
multiple domains like automotive, health care and air space to achieve
automation. Combined to these applications, the attempt to use AI techniques
into carrying out safety issues is momentarily at a progressive state. As AI
problems are getting even more complex, large processing power is demanded for
safety-critical systems to fulfill real-time requirements. These challenges can
be solved through edge or cloud computing, which makes the communication an
integral part of the solution. This study aims at providing a comprehensive
picture of the state of the art AI based safety solutions that uses different
communication technologies in diverse application domains. To achieve this, a
systematic mapping study is conducted and 565 relevant papers are shortlisted
through a multistage selection process, which are then analyzed according to a
systematically defined classification framework. The results of the study are
based on these main objectives: to clarify current research gaps in the field,
to identify the possibility of increased usage of cellular communication in
multiple domains, to identify the mostly used AI algorithms and to summarize
the emerging future research trends on the topic. The results demonstrate that
automotive domain is the one applying AI and communication the most to
implement safety and the most used AI in this domain is neural networks,
clustering and computer vision; applying cellular communication to automotive
domain is highest; the use of non-cellular communication technologies is
dominant however a clear trend of a rapid increase in the use of cellular
communication is observed specially from 2020 with the roll-out of 5G
technology.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.08387v1,2022-05-17T14:09:29Z,2022-05-17T14:09:29Z,"cfd.xyz/rom.js: An open-source framework for generating and visualizing
  parametric CFD results on the web","We present in this technical note an open-source web framework for the
generation and visualization of parametric CFD results from surrogate models.
It consists of a JavaScript module (rom.js) and a React JS web app (cfd.xyz) to
explore fluid dynamics problems efficiently and easily for a wide range of
parameters. rom.js is a JavaScript port of a set of open-source packages
(Eigen, Splinter, VTK/C++ and ITHACA-FV) to solve the online stage of
reduced-order models (ROM) generated by the ITHACA-FV tool. It can be executed
outside a web browser within a backend JavaScript runtime environment, or in a
given web solution. This methodology can also be extended to methods using
machine learning. The rom.js module was used in cfd.xyz, an open-source web
service to deliver a collection of interactive CFD cases in a parametric space.
The framework provides a proof of technology for OpenFOAM tutorials, showing
the whole process from the generation of the surrogate model to the web
browser. It also includes a standalone web tool for visualizing users' ROMs by
directly dragging and dropping the output folder of the offline stage. Beyond
the current proof of technology, this enables a collaborative effort for the
implementation of OpenFOAM surrogate models in applications demanding real-time
solutions such as digital twins and other digital transformation technologies.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.07716v1,2022-05-16T14:30:11Z,2022-05-16T14:30:11Z,Generalizing to New Tasks via One-Shot Compositional Subgoals,"The ability to generalize to previously unseen tasks with little to no
supervision is a key challenge in modern machine learning research. It is also
a cornerstone of a future ""General AI"". Any artificially intelligent agent
deployed in a real world application, must adapt on the fly to unknown
environments. Researchers often rely on reinforcement and imitation learning to
provide online adaptation to new tasks, through trial and error learning.
However, this can be challenging for complex tasks which require many timesteps
or large numbers of subtasks to complete. These ""long horizon"" tasks suffer
from sample inefficiency and can require extremely long training times before
the agent can learn to perform the necessary longterm planning. In this work,
we introduce CASE which attempts to address these issues by training an
Imitation Learning agent using adaptive ""near future"" subgoals. These subgoals
are recalculated at each step using compositional arithmetic in a learned
latent representation space. In addition to improving learning efficiency for
standard long-term tasks, this approach also makes it possible to perform
one-shot generalization to previously unseen tasks, given only a single
reference trajectory for the task in a different environment. Our experiments
show that the proposed approach consistently outperforms the previous
state-of-the-art compositional Imitation Learning approach by 30%.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.07554v1,2022-05-16T10:01:06Z,2022-05-16T10:01:06Z,Towards on-sky adaptive optics control using reinforcement learning,"The direct imaging of potentially habitable Exoplanets is one prime science
case for the next generation of high contrast imaging instruments on
ground-based extremely large telescopes. To reach this demanding science goal,
the instruments are equipped with eXtreme Adaptive Optics (XAO) systems which
will control thousands of actuators at a framerate of kilohertz to several
kilohertz. Most of the habitable exoplanets are located at small angular
separations from their host stars, where the current XAO systems' control laws
leave strong residuals.Current AO control strategies like static matrix-based
wavefront reconstruction and integrator control suffer from temporal delay
error and are sensitive to mis-registration, i.e., to dynamic variations of the
control system geometry. We aim to produce control methods that cope with these
limitations, provide a significantly improved AO correction and, therefore,
reduce the residual flux in the coronagraphic point spread function.
  We extend previous work in Reinforcement Learning for AO. The improved
method, called PO4AO, learns a dynamics model and optimizes a control neural
network, called a policy. We introduce the method and study it through
numerical simulations of XAO with Pyramid wavefront sensing for the 8-m and
40-m telescope aperture cases. We further implemented PO4AO and carried out
experiments in a laboratory environment using MagAO-X at the Steward
laboratory. PO4AO provides the desired performance by improving the
coronagraphic contrast in numerical simulations by factors 3-5 within the
control region of DM and Pyramid WFS, in simulation and in the laboratory. The
presented method is also quick to train, i.e., on timescales of typically 5-10
seconds, and the inference time is sufficiently small (< ms) to be used in
real-time control for XAO with currently available hardware even for extremely
large telescopes.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.06305v1,2022-05-12T18:44:00Z,2022-05-12T18:44:00Z,"Real-time Virtual-Try-On from a Single Example Image through Deep
  Inverse Graphics and Learned Differentiable Renderers","Augmented reality applications have rapidly spread across online platforms,
allowing consumers to virtually try-on a variety of products, such as makeup,
hair dying, or shoes. However, parametrizing a renderer to synthesize realistic
images of a given product remains a challenging task that requires expert
knowledge. While recent work has introduced neural rendering methods for
virtual try-on from example images, current approaches are based on large
generative models that cannot be used in real-time on mobile devices. This
calls for a hybrid method that combines the advantages of computer graphics and
neural rendering approaches. In this paper we propose a novel framework based
on deep learning to build a real-time inverse graphics encoder that learns to
map a single example image into the parameter space of a given augmented
reality rendering engine. Our method leverages self-supervised learning and
does not require labeled training data which makes it extendable to many
virtual try-on applications. Furthermore, most augmented reality renderers are
not differentiable in practice due to algorithmic choices or implementation
constraints to reach real-time on portable devices. To relax the need for a
graphics-based differentiable renderer in inverse graphics problems, we
introduce a trainable imitator module. Our imitator is a generative network
that learns to accurately reproduce the behavior of a given non-differentiable
renderer. We propose a novel rendering sensitivity loss to train the imitator,
which ensures that the network learns an accurate and continuous representation
for each rendering parameter. Our framework enables novel applications where
consumers can virtually try-on a novel unknown product from an inspirational
reference image on social media. It can also be used by graphics artists to
automatically create realistic rendering from a reference product image.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.06212v2,2022-07-20T13:58:24Z,2022-05-12T16:52:48Z,"Contingency-constrained economic dispatch with safe reinforcement
  learning","Future power systems will rely heavily on micro grids with a high share of
decentralised renewable energy sources and energy storage systems. The high
complexity and uncertainty in this context might make conventional power
dispatch strategies infeasible. Reinforcement-learning based (RL) controllers
can address this challenge, however, cannot themselves provide safety
guarantees, preventing their deployment in practice. To overcome this
limitation, we propose a formally validated RL controller for economic
dispatch. We extend conventional constraints by a time-dependent constraint
encoding the islanding contingency. The contingency constraint is computed
using set-based backwards reachability analysis and actions of the RL agent are
verified through a safety layer. Unsafe actions are projected into the safe
action space while leveraging constrained zonotope set representations for
computational efficiency. The developed approach is demonstrated on a
residential use case using real-world measurements.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.13103v1,2022-04-27T17:59:25Z,2022-04-27T17:59:25Z,"FlowGNN: A Dataflow Architecture for Universal Graph Neural Network
  Inference via Multi-Queue Streaming","Graph neural networks (GNNs) have recently exploded in popularity thanks to
their broad applicability to graph-related problems such as quantum chemistry,
drug discovery, and high energy physics. However, meeting demand for novel GNN
models and fast inference simultaneously is challenging because of the gap
between developing efficient accelerators and the rapid creation of new GNN
models. Prior art focuses on the acceleration of specific classes of GNNs, such
as Graph Convolutional Network (GCN), but lacks the generality to support a
wide range of existing or new GNN models. Meanwhile, most work rely on graph
pre-processing to exploit data locality, making them unsuitable for real-time
applications. To address these limitations, in this work, we propose a generic
dataflow architecture for GNN acceleration, named FlowGNN, which can flexibly
support the majority of message-passing GNNs. The contributions are three-fold.
First, we propose a novel and scalable dataflow architecture, which flexibly
supports a wide range of GNN models with message-passing mechanism. The
architecture features a configurable dataflow optimized for simultaneous
computation of node embedding, edge embedding, and message passing, which is
generally applicable to all models. We also propose a rich library of
model-specific components. Second, we deliver ultra-fast real-time GNN
inference without any graph pre-processing, making it agnostic to dynamically
changing graph structures. Third, we verify our architecture on the Xilinx
Alveo U50 FPGA board and measure the on-board end-to-end performance. We
achieve a speed-up of up to 51-254x against CPU (6226R) and 1.3-477x against
GPU (A6000) (with batch sizes 1 through 1024); we also outperform the SOTA GNN
accelerator I-GCN by 1.03x and 1.25x across two datasets. Our implementation
code and on-board measurement are publicly available on GitHub.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.12380v1,2022-04-26T15:25:55Z,2022-04-26T15:25:55Z,"Multi-task Learning for Concurrent Prediction of Thermal Comfort,
  Sensation, and Preference","Indoor thermal comfort immensely impacts the health and performance of
occupants. Therefore, researchers and engineers have proposed numerous
computational models to estimate thermal comfort (TC). Given the impetus toward
energy efficiency, the current focus is on data-driven TC prediction solutions
that leverage state-of-the-art machine learning (ML) algorithms. However, an
indoor occupant's perception of indoor thermal comfort (TC) is subjective and
multi-dimensional. Different aspects of TC are represented by various standard
metrics/scales viz., thermal sensation (TSV), thermal comfort (TCV), and
thermal preference (TPV). The current ML-based TC prediction solutions adopt
the Single-task Learning approach, i.e., one prediction model per metric.
Consequently, solutions often focus on only one TC metric. Moreover, when
several metrics are considered, multiple TC models for a single indoor space
lead to conflicting predictions, making real-world deployment infeasible. This
work addresses these problems. With the vision toward energy conservation and
real-world application, naturally ventilated primary school classrooms are
considered. First, month-long field experiments are conducted in 5 schools and
14 classrooms, including 512 unique student participants. Further,
""DeepComfort,"" a Multi-task Learning inspired deep-learning model is proposed.
DeepComfort predicts multiple TC output metrics viz., TSV, TPV, and TCV,
simultaneously, through a single model. It demonstrates high F1-scores,
Accuracy (>90%), and generalization capability when validated on the ASHRAE-II
database and the dataset created in this study. DeepComfort is also shown to
outperform 6 popular metric-specific single-task machine learning algorithms.
To the best of our knowledge, this work is the first application of Multi-task
Learning to thermal comfort prediction in classrooms.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.12200v2,2022-04-27T20:47:50Z,2022-04-26T10:06:04Z,Hypergraph Contrastive Collaborative Filtering,"Collaborative Filtering (CF) has emerged as fundamental paradigms for
parameterizing users and items into latent representation space, with their
correlative patterns from interaction data. Among various CF techniques, the
development of GNN-based recommender systems, e.g., PinSage and LightGCN, has
offered the state-of-the-art performance. However, two key challenges have not
been well explored in existing solutions: i) The over-smoothing effect with
deeper graph-based CF architecture, may cause the indistinguishable user
representations and degradation of recommendation results. ii) The supervision
signals (i.e., user-item interactions) are usually scarce and skewed
distributed in reality, which limits the representation power of CF paradigms.
To tackle these challenges, we propose a new self-supervised recommendation
framework Hypergraph Contrastive Collaborative Filtering (HCCF) to jointly
capture local and global collaborative relations with a hypergraph-enhanced
cross-view contrastive learning architecture. In particular, the designed
hypergraph structure learning enhances the discrimination ability of GNN-based
CF paradigm, so as to comprehensively capture the complex high-order
dependencies among users. Additionally, our HCCF model effectively integrates
the hypergraph structure encoding with self-supervised learning to reinforce
the representation quality of recommender systems, based on the
hypergraph-enhanced self-discrimination. Extensive experiments on three
benchmark datasets demonstrate the superiority of our model over various
state-of-the-art recommendation methods, and the robustness against sparse user
interaction data. Our model implementation codes are available at
https://github.com/akaxlh/HCCF.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.11216v1,2022-04-24T08:35:55Z,2022-04-24T08:35:55Z,"RealNet: Combining Optimized Object Detection with Information Fusion
  Depth Estimation Co-Design Method on IoT","Depth Estimation and Object Detection Recognition play an important role in
autonomous driving technology under the guidance of deep learning artificial
intelligence. We propose a hybrid structure called RealNet: a co-design method
combining the model-streamlined recognition algorithm, the depth estimation
algorithm with information fusion, and deploying them on the Jetson-Nano for
unmanned vehicles with monocular vision sensors. We use ROS for experiment. The
method proposed in this paper is suitable for mobile platforms with high
real-time request. Innovation of our method is using information fusion to
compensate the problem of insufficient frame rate of output image, and improve
the robustness of target detection and depth estimation under monocular
vision.Object Detection is based on YOLO-v5. We have simplified the network
structure of its DarkNet53 and realized a prediction speed up to 0.01s. Depth
Estimation is based on the VNL Depth Estimation, which considers multiple
geometric constraints in 3D global space. It calculates the loss function by
calculating the deviation of the virtual normal vector VN and the label, which
can obtain deeper depth information. We use PnP fusion algorithm to solve the
problem of insufficient frame rate of depth map output. It solves the motion
estimation depth from three-dimensional target to two-dimensional point based
on corner feature matching, which is faster than VNL calculation. We
interpolate VNL output and PnP output to achieve information fusion.
Experiments show that this can effectively eliminate the jitter of depth
information and improve robustness. At the control end, this method combines
the results of target detection and depth estimation to calculate the target
position, and uses a pure tracking control algorithm to track it.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.10740v3,2022-07-21T12:55:19Z,2022-04-22T15:02:08Z,"Embracing AWKWARD! Real-time Adjustment of Reactive Plans Using Social
  Norms","This paper presents the AWKWARD architecture for the development of hybrid
agents in Multi-Agent Systems. AWKWARD agents can have their plans
re-configured in real time to align with social role requirements under
changing environmental and social circumstances. The proposed hybrid
architecture makes use of Behaviour Oriented Design (BOD) to develop agents
with reactive planning and of the well-established OperA framework to provide
organisational, social, and interaction definitions in order to validate and
adjust agents' behaviours. Together, OperA and BOD can achieve real-time
adjustment of agent plans for evolving social roles, while providing the
additional benefit of transparency into the interactions that drive this
behavioural change in individual agents. We present this architecture to
motivate the bridging between traditional symbolic- and behaviour-based AI
communities, where such combined solutions can help MAS researchers in their
pursuit of building stronger, more robust intelligent agent teams. We use
DOTA2, a game where success is heavily dependent on social interactions, as a
medium to demonstrate a sample implementation of our proposed hybrid
architecture.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.09748v1,2022-04-20T19:14:14Z,2022-04-20T19:14:14Z,"Inferring ice sheet damage models from limited observations using
  CRIKit: the Constitutive Relation Inference Toolkit","We examine the prospect of learning ice sheet damage models from
observational data. Our approach, implemented in CRIKit (the Constitutive
Relation Inference Toolkit), is to model the material time derivative of damage
as a frame-invariant neural network, and to optimize the parameters of the
model from simulations of the flow of an ice dome. Using the model of Albrecht
and Levermann as the ground truth to generate synthetic observations, we
measure the difference of optimized neural network models from that model to
try to understand how well this process generates models that can then transfer
to other ice sheet simulations.
  The use of so-called ""deep-learning"" models for constitutive equations,
equations of state, sub-grid-scale processes, and other pointwise relations
that appear in systems of PDEs has been successful in other disciplines, yet
our inference setting has some confounding factors. The first is the type of
observations that are available: we compare the quality of the inferred models
when the loss of the numerical simulations includes observation misfits
throughout the ice, which is unobtainable in real settings, to losses that
include only combinations of surface and borehole observations. The second
confounding factor is the evolution of damage in an ice sheet, which is
advection dominated. The non-local effect of perturbations in a damage models
results in loss functions that have both many local minima and many parameter
configurations for which the system is unsolvable.
  Our experience suggests that basic neural networks have several deficiencies
that affect the quality of the optimized models. We suggest several approaches
to incorporating additional inductive biases into neural networks which may
lead to better performance in future work.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.08587v2,2022-05-07T05:47:12Z,2022-04-18T23:46:01Z,"Spatial-Temporal Hypergraph Self-Supervised Learning for Crime
  Prediction","Crime has become a major concern in many cities, which calls for the rising
demand for timely predicting citywide crime occurrence. Accurate crime
prediction results are vital for the beforehand decision-making of government
to alleviate the increasing concern about the public safety. While many efforts
have been devoted to proposing various spatial-temporal forecasting techniques
to explore dependence across locations and time periods, most of them follow a
supervised learning manner, which limits their spatial-temporal representation
ability on sparse crime data. Inspired by the recent success in self-supervised
learning, this work proposes a Spatial-Temporal Hypergraph Self-Supervised
Learning framework (ST-HSL) to tackle the label scarcity issue in crime
prediction. Specifically, we propose the cross-region hypergraph structure
learning to encode region-wise crime dependency under the entire urban space.
Furthermore, we design the dual-stage self-supervised learning paradigm, to not
only jointly capture local- and global-level spatial-temporal crime patterns,
but also supplement the sparse crime representation by augmenting region
self-discrimination. We perform extensive experiments on two real-life crime
datasets. Evaluation results show that our ST-HSL significantly outperforms
state-of-the-art baselines. Further analysis provides insights into the
superiority of our ST-HSL method in the representation of spatial-temporal
crime patterns. The implementation code is available at
https://github.com/LZH-YS1998/STHSL.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.06979v1,2022-04-14T14:08:55Z,2022-04-14T14:08:55Z,"HyDe: The First Open-Source, Python-Based, GPU-Accelerated Hyperspectral
  Denoising Package","As with any physical instrument, hyperspectral cameras induce different kinds
of noise in the acquired data. Therefore, Hyperspectral denoising is a crucial
step for analyzing hyperspectral images (HSIs). Conventional computational
methods rarely use GPUs to improve efficiency and are not fully open-source.
Alternatively, deep learning-based methods are often open-source and use GPUs,
but their training and utilization for real-world applications remain
non-trivial for many researchers. Consequently, we propose HyDe: the first
open-source, GPU-accelerated Python-based, hyperspectral image denoising
toolbox, which aims to provide a large set of methods with an easy-to-use
environment. HyDe includes a variety of methods ranging from low-rank
wavelet-based methods to deep neural network (DNN) models. HyDe's interface
dramatically improves the interoperability of these methods and the performance
of the underlying functions. In fact, these methods maintain similar HSI
denoising performance to their original implementations while consuming nearly
ten times less energy. Furthermore, we present a method for training DNNs for
denoising HSIs which are not spatially related to the training dataset, i.e.,
training on ground-level HSIs for denoising HSIs with other perspectives
including airborne, drone-borne, and space-borne. To utilize the trained DNNs,
we show a sliding window method to effectively denoise HSIs which would
otherwise require more than 40 GB. The package can be found at:
\url{https://github.com/Helmholtz-AI-Energy/HyDe}.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.06700v1,2022-04-14T01:54:44Z,2022-04-14T01:54:44Z,"Gallery D.C.: Auto-created GUI Component Gallery for Design Search and
  Knowledge Discovery","GUI design is an integral part of software development. The process of
designing a mobile application typically starts with the ideation and
inspiration search from existing designs. However, existing
information-retrieval based, and database-query based methods cannot
efficiently gain inspirations in three requirements: design practicality,
design granularity and design knowledge discovery. In this paper we propose a
web application, called \tool that aims to facilitate the process of user
interface design through real world GUI component search. Gallery D.C. indexes
GUI component designs using reverse engineering and deep learning based
computer vision techniques on millions of real world applications. To perform
an advanced design search and knowledge discovery, our approach extracts
information about size, color, component type, and text information to help
designers explore multi-faceted design space and distill higher-order of design
knowledge. Gallery D.C. is well received via an informal evaluation with 7
professional designers. Web Link: http://mui-collection.herokuapp.com/. Demo
Video Link: https://youtu.be/zVmsz_wY5OQ.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.00001v3,2022-07-06T22:27:43Z,2022-04-14T00:35:52Z,"Brainish: Formalizing A Multimodal Language for Intelligence and
  Consciousness","Having a rich multimodal inner language is an important component of human
intelligence that enables several necessary core cognitive functions such as
multimodal prediction, translation, and generation. Building upon the Conscious
Turing Machine (CTM), a machine model for consciousness proposed by Blum and
Blum (2021), we describe the desiderata of a multimodal language called
Brainish, comprising words, images, audio, and sensations combined in
representations that the CTM's processors use to communicate with each other.
We define the syntax and semantics of Brainish before operationalizing this
language through the lens of multimodal artificial intelligence, a vibrant
research area studying the computational tools necessary for processing and
relating information from heterogeneous signals. Our general framework for
learning Brainish involves designing (1) unimodal encoders to segment and
represent unimodal data, (2) a coordinated representation space that relates
and composes unimodal features to derive holistic meaning across multimodal
inputs, and (3) decoders to map multimodal representations into predictions
(for fusion) or raw data (for translation or generation). Through discussing
how Brainish is crucial for communication and coordination in order to achieve
consciousness in the CTM, and by implementing a simple version of Brainish and
evaluating its capability of demonstrating intelligence on multimodal
prediction and retrieval tasks on several real-world image, text, and audio
datasets, we argue that such an inner language will be important for advances
in machine models of intelligence and consciousness.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.13989v1,2022-04-13T21:14:58Z,2022-04-13T21:14:58Z,"Dynamic Diagnosis of the Progress and Shortcomings of Student Learning
  using Machine Learning based on Cognitive, Social, and Emotional Features","Student diversity, like academic background, learning styles, career and life
goals, ethnicity, age, social and emotional characteristics, course load and
work schedule, offers unique opportunities in education, like learning new
skills, peer mentoring and example setting. But student diversity can be
challenging too as it adds variability in the way in which students learn and
progress over time. A single teaching approach is likely to be ineffective and
result in students not meeting their potential. Automated support could address
limitations of traditional teaching by continuously assessing student learning
and implementing needed interventions. This paper discusses a novel methodology
based on data analytics and Machine Learning to measure and causally diagnose
the progress and shortcomings of student learning, and then utilizes the
insight gained on individuals to optimize learning. Diagnosis pertains to
dynamic diagnostic formative assessment, which aims to uncover the causes of
learning shortcomings. The methodology groups learning difficulties into four
categories: recall from memory, concept adjustment, concept modification, and
problem decomposition into sub-goals (sub-problems) and concept combination.
Data models are predicting the occurrence of each of the four challenge types,
as well as a student's learning trajectory. The models can be used to
automatically create real-time, student-specific interventions (e.g., learning
cues) to address less understood concepts. We envision that the system will
enable new adaptive pedagogical approaches to unleash student learning
potential through customization of the course material to the background,
abilities, situation, and progress of each student; and leveraging
diversity-related learning experiences.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.05838v1,2022-04-12T14:27:33Z,2022-04-12T14:27:33Z,"Benchmarking Active Learning Strategies for Materials Optimization and
  Discovery","Autonomous physical science is revolutionizing materials science. In these
systems, machine learning controls experiment design, execution, and analysis
in a closed loop. Active learning, the machine learning field of optimal
experiment design, selects each subsequent experiment to maximize knowledge
toward the user goal. Autonomous system performance can be further improved
with implementation of scientific machine learning, also known as inductive
bias-engineered artificial intelligence, which folds prior knowledge of
physical laws (e.g., Gibbs phase rule) into the algorithm. As the number,
diversity, and uses for active learning strategies grow, there is an associated
growing necessity for real-world reference datasets to benchmark strategies. We
present a reference dataset and demonstrate its use to benchmark active
learning strategies in the form of various acquisition functions. Active
learning strategies are used to rapidly identify materials with optimal
physical properties within a ternary materials system. The data is from an
actual Fe-Co-Ni thin-film library and includes previously acquired experimental
data for materials compositions, X-ray diffraction patterns, and two functional
properties of magnetic coercivity and the Kerr rotation. Popular active
learning methods along with a recent scientific active learning method are
benchmarked for their materials optimization performance. We discuss the
relationship between algorithm performance, materials search space complexity,
and the incorporation of prior knowledge.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.05509v1,2022-04-12T03:45:48Z,2022-04-12T03:45:48Z,"Learning Design and Construction with Varying-Sized Materials via
  Prioritized Memory Resets","Can a robot autonomously learn to design and construct a bridge from
varying-sized blocks without a blueprint? It is a challenging task with long
horizon and sparse reward -- the robot has to figure out physically stable
design schemes and feasible actions to manipulate and transport blocks. Due to
diverse block sizes, the state space and action trajectories are vast to
explore. In this paper, we propose a hierarchical approach for this problem. It
consists of a reinforcement-learning designer to propose high-level building
instructions and a motion-planning-based action generator to manipulate blocks
at the low level. For high-level learning, we develop a novel technique,
prioritized memory resetting (PMR) to improve exploration. PMR adaptively
resets the state to those most critical configurations from a replay buffer so
that the robot can resume training on partial architectures instead of from
scratch. Furthermore, we augment PMR with auxiliary training objectives and
fine-tune the designer with the locomotion generator. Our experiments in
simulation and on a real deployed robotic system demonstrate that it is able to
effectively construct bridges with blocks of varying sizes at a high success
rate. Demos can be found at https://sites.google.com/view/bridge-pmr.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.05397v1,2022-04-11T20:40:13Z,2022-04-11T20:40:13Z,"Accelerated Design and Deployment of Low-Carbon Concrete for Data
  Centers","Concrete is the most widely used engineered material in the world with more
than 10 billion tons produced annually. Unfortunately, with that scale comes a
significant burden in terms of energy, water, and release of greenhouse gases
and other pollutants; indeed 8% of worldwide carbon emissions are attributed to
the production of cement, a key ingredient in concrete. As such, there is
interest in creating concrete formulas that minimize this environmental burden,
while satisfying engineering performance requirements including compressive
strength. Specifically for computing, concrete is a major ingredient in the
construction of data centers.
  In this work, we use conditional variational autoencoders (CVAEs), a type of
semi-supervised generative artificial intelligence (AI) model, to discover
concrete formulas with desired properties. Our model is trained just using a
small open dataset from the UCI Machine Learning Repository joined with
environmental impact data from standard lifecycle analysis. Computational
predictions demonstrate CVAEs can design concrete formulas with much lower
carbon requirements than existing formulations while meeting design
requirements. Next we report laboratory-based compressive strength experiments
for five AI-generated formulations, which demonstrate that the formulations
exceed design requirements. The resulting formulations were then used by Ozinga
Ready Mix -- a concrete supplier -- to generate field-ready concrete
formulations, based on local conditions and their expertise in concrete design.
Finally, we report on how these formulations were used in the construction of
buildings and structures in a Meta data center in DeKalb, IL, USA. Results from
field experiments as part of this real-world deployment corroborate the
efficacy of AI-generated low-carbon concrete mixes.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.05027v1,2022-04-11T11:55:06Z,2022-04-11T11:55:06Z,"Exploring the Pareto front of multi-objective COVID-19 mitigation
  policies using reinforcement learning","Infectious disease outbreaks can have a disruptive impact on public health
and societal processes. As decision making in the context of epidemic
mitigation is hard, reinforcement learning provides a methodology to
automatically learn prevention strategies in combination with complex epidemic
models. Current research focuses on optimizing policies w.r.t. a single
objective, such as the pathogen's attack rate. However, as the mitigation of
epidemics involves distinct, and possibly conflicting criteria (i.a.,
prevalence, mortality, morbidity, cost), a multi-objective approach is
warranted to learn balanced policies. To lift this decision-making process to
real-world epidemic models, we apply deep multi-objective reinforcement
learning and build upon a state-of-the-art algorithm, Pareto Conditioned
Networks (PCN), to learn a set of solutions that approximates the Pareto front
of the decision problem. We consider the first wave of the Belgian COVID-19
epidemic, which was mitigated by a lockdown, and study different deconfinement
strategies, aiming to minimize both COVID-19 cases (i.e., infections and
hospitalizations) and the societal burden that is induced by the applied
mitigation measures. We contribute a multi-objective Markov decision process
that encapsulates the stochastic compartment model that was used to inform
policy makers during the COVID-19 epidemic. As these social mitigation measures
are implemented in a continuous action space that modulates the contact matrix
of the age-structured epidemic model, we extend PCN to this setting. We
evaluate the solution returned by PCN, and observe that it correctly learns to
reduce the social burden whenever the hospitalization rates are sufficiently
low. In this work, we thus show that multi-objective reinforcement learning is
attainable in complex epidemiological models and provides essential insights to
balance complex mitigation policies.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.03998v1,2022-04-08T11:08:03Z,2022-04-08T11:08:03Z,"SnapMode: An Intelligent and Distributed Large-Scale Fashion Image
  Retrieval Platform Based On Big Data and Deep Generative Adversarial Network
  Technologies","Fashion is now among the largest industries worldwide, for it represents
human history and helps tell the worlds story. As a result of the Fourth
Industrial Revolution, the Internet has become an increasingly important source
of fashion information. However, with a growing number of web pages and social
data, it is nearly impossible for humans to manually catch up with the ongoing
evolution and the continuously variable content in this domain. The proper
management and exploitation of big data can pave the way for the substantial
growth of the global economy as well as citizen satisfaction. Therefore,
computer scientists have found it challenging to handle e-commerce fashion
websites by using big data and machine learning technologies. This paper first
proposes a scalable focused Web Crawler engine based on the distributed
computing platforms to extract and process fashion data on e-commerce websites.
The role of the proposed platform is then described in developing a
disentangled feature extraction method by employing deep convolutional
generative adversarial networks (DCGANs) for content-based image indexing and
retrieval. Finally, the state-of-the-art solutions are compared, and the
results of the proposed approach are analyzed on a standard dataset. For the
real-life implementation of the proposed solution, a Web-based application is
developed on Apache Storm, Kafka, Solr, and Milvus platforms to create a
fashion search engine called SnapMode.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.03931v1,2022-04-08T08:52:23Z,2022-04-08T08:52:23Z,"HINNPerf: Hierarchical Interaction Neural Network for Performance
  Prediction of Configurable Systems","Modern software systems are usually highly configurable, providing users with
customized functionality through various configuration options. Understanding
how system performance varies with different option combinations is important
to determine optimal configurations that meet specific requirements. Due to the
complex interactions among multiple options and the high cost of performance
measurement under a huge configuration space, it is challenging to study how
different configurations influence the system performance. To address these
challenges, we propose HINNPerf, a novel hierarchical interaction neural
network for performance prediction of configurable systems. HINNPerf employs
the embedding method and hierarchic network blocks to model the complicated
interplay between configuration options, which improves the prediction accuracy
of the method. Besides, we devise a hierarchical regularization strategy to
enhance the model robustness. Empirical results on 10 real-world configurable
systems show that our method statistically significantly outperforms
state-of-the-art approaches by achieving average 22.67% improvement in
prediction accuracy. In addition, combined with the Integrated Gradients
method, the designed hierarchical architecture provides some insights about the
interaction complexity and the significance of configuration options, which
might help users and developers better understand how the configurable system
works and efficiently identify significant options affecting the performance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.03296v2,2022-06-22T19:47:02Z,2022-04-07T08:53:18Z,"Deep Learning for Real Time Satellite Pose Estimation on Low Power Edge
  TPU","Pose estimation of an uncooperative space resident object is a key asset
towards autonomy in close proximity operations. In this context monocular
cameras are a valuable solution because of their low system requirements.
However, the associated image processing algorithms are either too
computationally expensive for real time on-board implementation, or not enough
accurate. In this paper we propose a pose estimation software exploiting neural
network architectures which can be scaled to different accuracy-latency
trade-offs. We designed our pipeline to be compatible with Edge Tensor
Processing Units to show how low power machine learning accelerators could
enable Artificial Intelligence exploitation in space. The neural networks were
tested both on the benchmark Spacecraft Pose Estimation Dataset, and on the
purposely developed Cosmo Photorealistic Dataset, which depicts a COSMO-SkyMed
satellite in a variety of random poses and steerable solar panels orientations.
The lightest version of our architecture achieves state-of-the-art accuracy on
both datasets but at a fraction of networks complexity, running at 7.7 frames
per second on a Coral Dev Board Mini consuming just 2.2W.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2204.01872v1,2022-04-04T22:22:09Z,2022-04-04T22:22:09Z,Internet of Things: System Reference Architecture,"This document describes an IoT system reference architecture. It is a general
system architecture in the sense that it embodies key system components,
functions, and flows that are commonly encountered in IoT systems. Much of the
existing literature on IoT system architecture covers either specific
commercial solutions or abstract architecture descriptions that are conceptual
but not readily reducible to practice. This paper describes a general,
vendor-neutral architecture of the IoT system derived by abstracting the common
requirements and features of a variety of system specifications and
implementations. It covers architectural and design principles for constructing
the core IoT system overlay, specifically functions and components involved in
acquiring data, real-time processing, storage and delivery to applications and
services, such as visualization and AI. The appendix outlines the principles of
IoT information modeling and data and metadata handling. As a reference
architecture, this document is meant to serve as a template and a starting
blueprint for constructing specific IoT system solutions. The intent is to
maintain this material as a living document and evolve it over time into a
broader technical community consensus by incorporating and acknowledging the
accumulated feedback and contributions. Direct comments and suggestions are
invited now. More structured mechanisms of managing crowdsourcing and community
collaboration are planned for implementation if warranted.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2205.01042v1,2022-04-01T07:05:13Z,2022-04-01T07:05:13Z,"Machine Learning and Artificial Intelligence in Circular Economy: A
  Bibliometric Analysis and Systematic Literature Review","With unorganized, unplanned and improper use of limited raw materials, an
abundant amount of waste is being produced, which is harmful to our environment
and ecosystem. While traditional linear production lines fail to address
far-reaching issues like waste production and a shorter product life cycle, a
prospective concept, namely circular economy (CE), has shown promising
prospects to be adopted at industrial and governmental levels. CE aims to
complete the product life cycle loop by bringing out the highest values from
raw materials in the design phase and later on by reusing, recycling, and
remanufacturing. Innovative technologies like artificial intelligence (AI) and
machine learning(ML) provide vital assistance in effectively adopting and
implementing CE in real-world practices. This study explores the adoption and
integration of applied AI techniques in CE. First, we conducted bibliometric
analysis on a collection of 104 SCOPUS indexed documents exploring the critical
research criteria in AI and CE. Forty papers were picked to conduct a
systematic literature review from these documents. The selected documents were
further divided into six categories: sustainable development, reverse
logistics, waste management, supply chain management, recycle & reuse, and
manufacturing development. Comprehensive research insights and trends have been
extracted and delineated. Finally, the research gap needing further attention
has been identified and the future research directions have also been
discussed.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.16154v1,2022-03-30T08:55:08Z,2022-03-30T08:55:08Z,"Learning to Socially Navigate in Pedestrian-rich Environments with
  Interaction Capacity","Existing navigation policies for autonomous robots tend to focus on collision
avoidance while ignoring human-robot interactions in social life. For instance,
robots can pass along the corridor safer and easier if pedestrians notice them.
Sounds have been considered as an efficient way to attract the attention of
pedestrians, which can alleviate the freezing robot problem. In this work, we
present a new deep reinforcement learning (DRL) based social navigation
approach for autonomous robots to move in pedestrian-rich environments with
interaction capacity. Most existing DRL based methods intend to train a general
policy that outputs both navigation actions, i.e., expected robot's linear and
angular velocities, and interaction actions, i.e., the beep action, in the
context of reinforcement learning. Different from these methods, we intend to
train the policy via both supervised learning and reinforcement learning. In
specific, we first train an interaction policy in the context of supervised
learning, which provides a better understanding of the social situation, then
we use this interaction policy to train the navigation policy via multiple
reinforcement learning algorithms. We evaluate our approach in various
simulation environments and compare it to other methods. The experimental
results show that our approach outperforms others in terms of the success rate.
We also deploy the trained policy on a real-world robot, which shows a nice
performance in crowded environments.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.14078v1,2022-03-26T13:42:57Z,2022-03-26T13:42:57Z,"Computationally efficient joint coordination of multiple electric
  vehicle charging points using reinforcement learning","A major challenge in todays power grid is to manage the increasing load from
electric vehicle (EV) charging. Demand response (DR) solutions aim to exploit
flexibility therein, i.e., the ability to shift EV charging in time and thus
avoid excessive peaks or achieve better balancing. Whereas the majority of
existing research works either focus on control strategies for a single EV
charger, or use a multi-step approach (e.g., a first high level aggregate
control decision step, followed by individual EV control decisions), we rather
propose a single-step solution that jointly coordinates multiple charging
points at once. In this paper, we further refine an initial proposal using
reinforcement learning (RL), specifically addressing computational challenges
that would limit its deployment in practice. More precisely, we design a new
Markov decision process (MDP) formulation of the EV charging coordination
process, exhibiting only linear space and time complexity (as opposed to the
earlier quadratic space complexity). We thus improve upon earlier
state-of-the-art, demonstrating 30% reduction of training time in our case
study using real-world EV charging session data. Yet, we do not sacrifice the
resulting performance in meeting the DR objectives: our new RL solutions still
improve the performance of charging demand coordination by 40-50% compared to a
business-as-usual policy (that charges EV fully upon arrival) and 20-30%
compared to a heuristic policy (that uniformly spreads individual EV charging
over time).",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.14907v1,2022-03-24T10:50:33Z,2022-03-24T10:50:33Z,"Q-PPG: Energy-Efficient PPG-based Heart Rate Monitoring on Wearable
  Devices","Hearth Rate (HR) monitoring is increasingly performed in wrist-worn devices
using low-cost photoplethysmography (PPG) sensors. However, Motion Artifacts
(MAs) caused by movements of the subject's arm affect the performance of
PPG-based HR tracking. This is typically addressed coupling the PPG signal with
acceleration measurements from an inertial sensor. Unfortunately, most standard
approaches of this kind rely on hand-tuned parameters, which impair their
generalization capabilities and their applicability to real data in the field.
In contrast, methods based on deep learning, despite their better
generalization, are considered to be too complex to deploy on wearable devices.
  In this work, we tackle these limitations, proposing a design space
exploration methodology to automatically generate a rich family of deep
Temporal Convolutional Networks (TCNs) for HR monitoring, all derived from a
single ""seed"" model. Our flow involves a cascade of two Neural Architecture
Search (NAS) tools and a hardware-friendly quantizer, whose combination yields
both highly accurate and extremely lightweight models. When tested on the
PPG-Dalia dataset, our most accurate model sets a new state-of-the-art in Mean
Absolute Error. Furthermore, we deploy our TCNs on an embedded platform
featuring a STM32WB55 microcontroller, demonstrating their suitability for
real-time execution. Our most accurate quantized network achieves 4.41 Beats
Per Minute (BPM) of Mean Absolute Error (MAE), with an energy consumption of
47.65 mJ and a memory footprint of 412 kB. At the same time, the smallest
network that obtains a MAE < 8 BPM, among those generated by our flow, has a
memory footprint of 1.9 kB and consumes just 1.79 mJ per inference.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.11355v1,2022-03-21T21:33:55Z,2022-03-21T21:33:55Z,"Origami in N dimensions: How feed-forward networks manufacture linear
  separability","Neural networks can implement arbitrary functions. But, mechanistically, what
are the tools at their disposal to construct the target? For classification
tasks, the network must transform the data classes into a linearly separable
representation in the final hidden layer. We show that a feed-forward
architecture has one primary tool at hand to achieve this separability:
progressive folding of the data manifold in unoccupied higher dimensions. The
operation of folding provides a useful intuition in low-dimensions that
generalizes to high ones. We argue that an alternative method based on shear,
requiring very deep architectures, plays only a small role in real-world
networks. The folding operation, however, is powerful as long as layers are
wider than the data dimensionality, allowing efficient solutions by providing
access to arbitrary regions in the distribution, such as data points of one
class forming islands within the other classes. We argue that a link exists
between the universal approximation property in ReLU networks and the
fold-and-cut theorem (Demaine et al., 1998) dealing with physical paper
folding. Based on the mechanistic insight, we predict that the progressive
generation of separability is necessarily accompanied by neurons showing mixed
selectivity and bimodal tuning curves. This is validated in a network trained
on the poker hand task, showing the emergence of bimodal tuning curves during
training. We hope that our intuitive picture of the data transformation in deep
networks can help to provide interpretability, and discuss possible
applications to the theory of convolutional networks, loss landscapes, and
generalization.
  TL;DR: Shows that the internal processing of deep networks can be thought of
as literal folding operations on the data distribution in the N-dimensional
activation space. A link to a well-known theorem in origami theory is provided.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.11331v1,2022-03-21T20:44:30Z,2022-03-21T20:44:30Z,On The Robustness of Offensive Language Classifiers,"Social media platforms are deploying machine learning based offensive
language classification systems to combat hateful, racist, and other forms of
offensive speech at scale. However, despite their real-world deployment, we do
not yet comprehensively understand the extent to which offensive language
classifiers are robust against adversarial attacks. Prior work in this space is
limited to studying robustness of offensive language classifiers against
primitive attacks such as misspellings and extraneous spaces. To address this
gap, we systematically analyze the robustness of state-of-the-art offensive
language classifiers against more crafty adversarial attacks that leverage
greedy- and attention-based word selection and context-aware embeddings for
word replacement. Our results on multiple datasets show that these crafty
adversarial attacks can degrade the accuracy of offensive language classifiers
by more than 50% while also being able to preserve the readability and meaning
of the modified text.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.10975v1,2022-03-21T13:35:55Z,2022-03-21T13:35:55Z,"GCF: Generalized Causal Forest for Heterogeneous Treatment Effect
  Estimation in Online Marketplace","Uplift modeling is a rapidly growing approach that utilizes machine learning
and causal inference methods to estimate the heterogeneous treatment effects.
It has been widely adopted and applied to online marketplaces to assist
large-scale decision-making in recent years. The existing popular methods, like
forest-based modeling, either work only for discrete treatments or make
partially linear or parametric assumptions that may suffer from model
misspecification. To alleviate these problems, we extend causal forest (CF)
with non-parametric dose-response functions (DRFs) that can be estimated
locally using a kernel-based doubly robust estimator. Moreover, we propose a
distance-based splitting criterion in the functional space of conditional DRFs
to capture the heterogeneity for the continuous treatments. We call the
proposed algorithm generalized causal forest (GCF) as it generalizes the use
case of CF to a much broader setup. We show the effectiveness of GCF by
comparing it to popular uplift modeling models on both synthetic and real-world
datasets. We implement GCF in Spark and successfully deploy it into DiDi's
real-time pricing system. Online A/B testing results further validate the
superiority of GCF.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.11211v1,2022-03-21T13:17:30Z,2022-03-21T13:17:30Z,"ReCCoVER: Detecting Causal Confusion for Explainable Reinforcement
  Learning","Despite notable results in various fields over the recent years, deep
reinforcement learning (DRL) algorithms lack transparency, affecting user trust
and hindering their deployment to high-risk tasks. Causal confusion refers to a
phenomenon where an agent learns spurious correlations between features which
might not hold across the entire state space, preventing safe deployment to
real tasks where such correlations might be broken. In this work, we examine
whether an agent relies on spurious correlations in critical states, and
propose an alternative subset of features on which it should base its decisions
instead, to make it less susceptible to causal confusion. Our goal is to
increase transparency of DRL agents by exposing the influence of learned
spurious correlations on its decisions, and offering advice to developers about
feature selection in different parts of state space, to avoid causal confusion.
We propose ReCCoVER, an algorithm which detects causal confusion in agent's
reasoning before deployment, by executing its policy in alternative
environments where certain correlations between features do not hold. We
demonstrate our approach in taxi and grid world environments, where ReCCoVER
detects states in which an agent relies on spurious correlations and offers a
set of features that should be considered instead.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.10823v1,2022-03-21T09:16:56Z,2022-03-21T09:16:56Z,Long Short-Term Memory for Spatial Encoding in Multi-Agent Path Planning,"Reinforcement learning-based path planning for multi-agent systems of varying
size constitutes a research topic with increasing significance as progress in
domains such as urban air mobility and autonomous aerial vehicles continues.
Reinforcement learning with continuous state and action spaces is used to train
a policy network that accommodates desirable path planning behaviors and can be
used for time-critical applications. A Long Short-Term Memory module is
proposed to encode an unspecified number of states for a varying, indefinite
number of agents. The described training strategies and policy architecture
lead to a guidance that scales to an infinite number of agents and unlimited
physical dimensions, although training takes place at a smaller scale. The
guidance is implemented on a low-cost, off-the-shelf onboard computer. The
feasibility of the proposed approach is validated by presenting flight test
results of up to four drones, autonomously navigating collision-free in a
real-world environment.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.10518v1,2022-03-20T10:21:52Z,2022-03-20T10:21:52Z,"Learning on the Job: Long-Term Behavioural Adaptation in Human-Robot
  Interactions","In this work, we propose a framework for allowing autonomous robots deployed
for extended periods of time in public spaces to adapt their own behaviour
online from user interactions. The robot behaviour planning is embedded in a
Reinforcement Learning (RL) framework, where the objective is maximising the
level of overall user engagement during the interactions. We use the
Upper-Confidence-Bound Value-Iteration (UCBVI) algorithm, which gives a helpful
way of managing the exploration-exploitation trade-off for real-time
interactions. An engagement model trained end-to-end generates the reward
function in real-time during policy execution. We test this approach in a
public museum in Lincoln (UK), where the robot is deployed as a tour guide for
the visitors. Results show that after a couple of months of exploration, the
robot policy learned to maintain the engagement of users for longer, with an
increase of 22.8% over the initial static policy in the number of items visited
during the tour and a 30% increase in the probability of completing the tour.
This work is a promising step toward behavioural adaptation in long-term
scenarios for robotics applications in social settings.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.09911v1,2022-03-18T12:39:35Z,2022-03-18T12:39:35Z,"Why we need biased AI -- How including cognitive and ethical machine
  biases can enhance AI systems","This paper stresses the importance of biases in the field of artificial
intelligence (AI) in two regards. First, in order to foster efficient
algorithmic decision-making in complex, unstable, and uncertain real-world
environments, we argue for the structurewise implementation of human cognitive
biases in learning algorithms. Secondly, we argue that in order to achieve
ethical machine behavior, filter mechanisms have to be applied for selecting
biased training stimuli that represent social or behavioral traits that are
ethically desirable. We use insights from cognitive science as well as ethics
and apply them to the AI field, combining theoretical considerations with seven
case studies depicting tangible bias implementation scenarios. Ultimately, this
paper is the first tentative step to explicitly pursue the idea of a
re-evaluation of the ethical significance of machine biases, as well as putting
the idea forth to implement cognitive biases into machines.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.09678v2,2022-05-03T12:00:26Z,2022-03-18T01:12:18Z,Self-Ensemble Adversarial Training for Improved Robustness,"Due to numerous breakthroughs in real-world applications brought by machine
intelligence, deep neural networks (DNNs) are widely employed in critical
applications. However, predictions of DNNs are easily manipulated with
imperceptible adversarial perturbations, which impedes the further deployment
of DNNs and may result in profound security and privacy implications. By
incorporating adversarial samples into the training data pool, adversarial
training is the strongest principled strategy against various adversarial
attacks among all sorts of defense methods. Recent works mainly focus on
developing new loss functions or regularizers, attempting to find the unique
optimal point in the weight space. But none of them taps the potentials of
classifiers obtained from standard adversarial training, especially states on
the searching trajectory of training. In this work, we are dedicated to the
weight states of models through the training process and devise a simple but
powerful \emph{Self-Ensemble Adversarial Training} (SEAT) method for yielding a
robust classifier by averaging weights of history models. This considerably
improves the robustness of the target model against several well known
adversarial attacks, even merely utilizing the naive cross-entropy loss to
supervise. We also discuss the relationship between the ensemble of predictions
from different adversarially trained models and the prediction of
weight-ensembled models, as well as provide theoretical and empirical evidence
that the proposed self-ensemble method provides a smoother loss landscape and
better robustness than both individual models and the ensemble of predictions
from different classifiers. We further analyze a subtle but fatal issue in the
general settings for the self-ensemble model, which causes the deterioration of
the weight-ensembled method in the late phases.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.07967v3,2022-03-23T13:46:13Z,2022-03-15T14:52:52Z,Intrinsic Neural Fields: Learning Functions on Manifolds,"Neural fields have gained significant attention in the computer vision
community due to their excellent performance in novel view synthesis, geometry
reconstruction, and generative modeling. Some of their advantages are a sound
theoretic foundation and an easy implementation in current deep learning
frameworks. While neural fields have been applied to signals on manifolds,
e.g., for texture reconstruction, their representation has been limited to
extrinsically embedding the shape into Euclidean space. The extrinsic embedding
ignores known intrinsic manifold properties and is inflexible wrt. transfer of
the learned function. To overcome these limitations, this work introduces
intrinsic neural fields, a novel and versatile representation for neural fields
on manifolds. Intrinsic neural fields combine the advantages of neural fields
with the spectral properties of the Laplace-Beltrami operator. We show
theoretically that intrinsic neural fields inherit many desirable properties of
the extrinsic neural field framework but exhibit additional intrinsic
qualities, like isometry invariance. In experiments, we show intrinsic neural
fields can reconstruct high-fidelity textures from images with state-of-the-art
quality and are robust to the discretization of the underlying manifold. We
demonstrate the versatility of intrinsic neural fields by tackling various
applications: texture transfer between deformed shapes & different shapes,
texture reconstruction from real-world images with view dependence, and
discretization-agnostic learning on meshes and point clouds.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.07474v1,2022-03-14T20:18:24Z,2022-03-14T20:18:24Z,"Distributed On-Sensor Compute System for AR/VR Devices: A
  Semi-Analytical Simulation Framework for Power Estimation","Augmented Reality/Virtual Reality (AR/VR) glasses are widely foreseen as the
next generation computing platform. AR/VR glasses are a complex ""system of
systems"" which must satisfy stringent form factor, computing-, power- and
thermal- requirements. In this paper, we will show that a novel distributed
on-sensor compute architecture, coupled with new semiconductor technologies
(such as dense 3D-IC interconnects and Spin-Transfer Torque Magneto Random
Access Memory, STT-MRAM) and, most importantly, a full hardware-software
co-optimization are the solutions to achieve attractive and socially acceptable
AR/VR glasses. To this end, we developed a semi-analytical simulation framework
to estimate the power consumption of novel AR/VR distributed on-sensor
computing architectures. The model allows the optimization of the main
technological features of the system modules, as well as the computer-vision
algorithm partition strategy across the distributed compute architecture. We
show that, in the case of the compute-intensive machine learning based Hand
Tracking algorithm, the distributed on-sensor compute architecture can reduce
the system power consumption compared to a centralized system, with the
additional benefits in terms of latency and privacy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.07390v1,2022-03-14T18:00:03Z,2022-03-14T18:00:03Z,"There's no difference: Convolutional Neural Networks for transient
  detection without template subtraction","We present a Convolutional Neural Network (CNN) model for the separation of
astrophysical transients from image artifacts, a task known as ""real-bogus""
classification, that does not rely on Difference Image Analysis (DIA) which is
a computationally expensive process involving image matching on small spatial
scales in large volumes of data. We explore the use of CNNs to (1) automate the
""real-bogus"" classification, (2) reduce the computational costs of transient
discovery. We compare the efficiency of two CNNs with similar architectures,
one that uses ""image triplets"" (templates, search, and the corresponding
difference image) and one that adopts a similar architecture but takes as input
the template and search only. Without substantially changing the model
architecture or retuning the hyperparameters to the new input, we observe only
a small decrease in model efficiency (97% to 92% accuracy). We further
investigate how the model that does not receive the difference image learns the
required information from the template and search by exploring the saliency
maps. Our work demonstrates that (1) CNNs are excellent models for ""real-bogus""
classification that rely exclusively on the imaging data and require no feature
engineering task; (2) high-accuracy models can be built without the need to
construct difference images. Since once trained, neural networks can generate
predictions at minimal computational costs, we argue that future
implementations of this methodology could dramatically reduce the computational
costs in the detection of genuine transients in synoptic surveys like Rubin
Observatory's Legacy Survey of Space and Time by bypassing the DIA step
entirely.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.06498v8,2022-06-02T16:26:22Z,2022-03-12T18:26:24Z,"The worst of both worlds: A comparative analysis of errors in learning
  from data in psychology and machine learning","Recent arguments that machine learning (ML) is facing a reproducibility and
replication crisis suggest that some published claims in ML research cannot be
taken at face value. These concerns inspire analogies to the replication crisis
affecting the social and medical sciences. They also inspire calls for the
integration of statistical approaches to causal inference and predictive
modeling. A deeper understanding of what reproducibility concerns in supervised
ML research have in common with the replication crisis in experimental science
puts the new concerns in perspective, and helps researchers avoid ""the worst of
both worlds,"" where ML researchers begin borrowing methodologies from
explanatory modeling without understanding their limitations and vice versa. We
contribute a comparative analysis of concerns about inductive learning that
arise in causal attribution as exemplified in psychology versus predictive
modeling as exemplified in ML. We identify themes that re-occur in reform
discussions, like overreliance on asymptotic theory and non-credible beliefs
about real-world data generating processes. We argue that in both fields,
claims from learning are implied to generalize outside the specific environment
studied (e.g., the input dataset or subject sample, modeling implementation,
etc.) but are often impossible to refute due to undisclosed sources of variance
in the learning pipeline. In particular, errors being acknowledged in ML expose
cracks in long-held beliefs that optimizing predictive accuracy using huge
datasets absolves one from having to consider a true data generating process or
formally represent uncertainty in performance claims. We conclude by discussing
risks that arise when sources of errors are misdiagnosed and the need to
acknowledge the role of human inductive biases in learning and reform.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.05334v1,2022-03-10T12:30:50Z,2022-03-10T12:30:50Z,"Iterative Corresponding Geometry: Fusing Region and Depth for Highly
  Efficient 3D Tracking of Textureless Objects","Tracking objects in 3D space and predicting their 6DoF pose is an essential
task in computer vision. State-of-the-art approaches often rely on object
texture to tackle this problem. However, while they achieve impressive results,
many objects do not contain sufficient texture, violating the main underlying
assumption. In the following, we thus propose ICG, a novel probabilistic
tracker that fuses region and depth information and only requires the object
geometry. Our method deploys correspondence lines and points to iteratively
refine the pose. We also implement robust occlusion handling to improve
performance in real-world settings. Experiments on the YCB-Video, OPT, and Choi
datasets demonstrate that, even for textured objects, our approach outperforms
the current state of the art with respect to accuracy and robustness. At the
same time, ICG shows fast convergence and outstanding efficiency, requiring
only 1.3 ms per frame on a single CPU core. Finally, we analyze the influence
of individual components and discuss our performance compared to deep
learning-based methods. The source code of our tracker is publicly available.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.06245v2,2022-03-17T02:33:13Z,2022-03-08T16:13:41Z,"Predatory Medicine: Exploring and Measuring the Vulnerability of Medical
  AI to Predatory Science","Medical Artificial Intelligence (MedAI) for diagnosis, treatment options, and
drug development represents the new age of healthcare. The security, integrity,
and credibility of MedAI tools are paramount issues because human lives are at
stake. MedAI solutions are often heavily dependent on scientific medical
research literature as a primary data source that draws the attacker's
attention as a potential target. We present a first study of how the output of
MedAI can be polluted with Predatory Publications Presence (PPP). We study two
MedAI systems: mediKanren (disease independent) and CancerMine
(Disease-specific), which use research literature as primary data input from
the research repository PubMed, PubMed derived database SemMedDB, and NIH
translational Knowledge Graphs (KGs). Our study has a three-pronged focus: (1)
identifying the PPP in PubMed; (2) verifying the PPP in SemMedDB and the KGs;
(3) demonstrating the existing vulnerability of PPP traversing to the MedAI
output. Our contribution lies in identifying the existing PPP in the MedAI
inputs and demonstrating how predatory science can jeopardize the credibility
of MedAI solutions, making their real-life deployment questionable.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.02662v2,2022-04-08T12:58:48Z,2022-03-05T05:33:55Z,"A Survey on Metaverse: Fundamentals, Security, and Privacy","Metaverse, as an evolving paradigm of the next-generation Internet, aims to
build a fully immersive, hyper spatiotemporal, and self-sustaining virtual
shared space for humans to play, work, and socialize. Driven by recent advances
in emerging technologies such as extended reality, artificial intelligence, and
blockchain, metaverse is stepping from the science fiction to an upcoming
reality. However, severe privacy invasions and security breaches (inherited
from underlying technologies or emerged in the new digital ecology) of
metaverse can impede its wide deployment. At the same time, a series of
fundamental challenges (e.g., scalability and interoperability) can arise in
metaverse security provisioning owing to the intrinsic characteristics of
metaverse, such as immersive realism, hyper spatiotemporality, sustainability,
and heterogeneity. In this paper, we present a comprehensive survey of the
fundamentals, security, and privacy of metaverse. Specifically, we first
investigate a novel distributed metaverse architecture and its key
characteristics with ternary-world interactions. Then, we discuss the security
and privacy threats, present the critical challenges of metaverse systems, and
review the state-of-the-art countermeasures. Finally, we draw open research
directions for building future metaverse systems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.01716v1,2022-03-03T13:53:27Z,2022-03-03T13:53:27Z,Detecting High-Quality GAN-Generated Face Images using Neural Networks,"In the past decades, the excessive use of the last-generation GAN (Generative
Adversarial Networks) models in computer vision has enabled the creation of
artificial face images that are visually indistinguishable from genuine ones.
These images are particularly used in adversarial settings to create fake
social media accounts and other fake online profiles. Such malicious activities
can negatively impact the trustworthiness of users identities. On the other
hand, the recent development of GAN models may create high-quality face images
without evidence of spatial artifacts. Therefore, reassembling uniform color
channel correlations is a challenging research problem. To face these
challenges, we need to develop efficient tools able to differentiate between
fake and authentic face images. In this chapter, we propose a new strategy to
differentiate GAN-generated images from authentic images by leveraging spectral
band discrepancies, focusing on artificial face image synthesis. In particular,
we enable the digital preservation of face images using the Cross-band
co-occurrence matrix and spatial co-occurrence matrix. Then, we implement these
techniques and feed them to a Convolutional Neural Networks (CNN) architecture
to identify the real from artificial faces. Additionally, we show that the
performance boost is particularly significant and achieves more than 92% in
different post-processing environments. Finally, we provide several research
observations demonstrating that this strategy improves a comparable detection
method based only on intra-band spatial co-occurrences.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.01584v1,2022-03-03T09:26:00Z,2022-03-03T09:26:00Z,"Fairness-aware Adversarial Perturbation Towards Bias Mitigation for
  Deployed Deep Models","Prioritizing fairness is of central importance in artificial intelligence
(AI) systems, especially for those societal applications, e.g., hiring systems
should recommend applicants equally from different demographic groups, and risk
assessment systems must eliminate racism in criminal justice. Existing efforts
towards the ethical development of AI systems have leveraged data science to
mitigate biases in the training set or introduced fairness principles into the
training process. For a deployed AI system, however, it may not allow for
retraining or tuning in practice. By contrast, we propose a more flexible
approach, i.e., fairness-aware adversarial perturbation (FAAP), which learns to
perturb input data to blind deployed models on fairness-related features, e.g.,
gender and ethnicity. The key advantage is that FAAP does not modify deployed
models in terms of parameters and structures. To achieve this, we design a
discriminator to distinguish fairness-related attributes based on latent
representations from deployed models. Meanwhile, a perturbation generator is
trained against the discriminator, such that no fairness-related features could
be extracted from perturbed inputs. Exhaustive experimental evaluation
demonstrates the effectiveness and superior performance of the proposed FAAP.
In addition, FAAP is validated on real-world commercial deployments
(inaccessible to model parameters), which shows the transferability of FAAP,
foreseeing the potential of black-box adaptation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.02285v1,2022-03-03T03:15:14Z,2022-03-03T03:15:14Z,"A photonic chip-based machine learning approach for the prediction of
  molecular properties","Machine learning methods have revolutionized the discovery process of new
molecules and materials. However, the intensive training process of neural
networks for molecules with ever increasing complexity has resulted in
exponential growth in computation cost, leading to long simulation time and
high energy consumption. Photonic chip technology offers an alternative
platform for implementing neural network with faster data processing and lower
energy usage compared to digital computers. Here, we demonstrate the capability
of photonic neural networks in predicting the quantum mechanical properties of
molecules. Additionally, we show that multiple properties can be learned
simultaneously in a photonic chip via a multi-task regression learning
algorithm, which we believe is the first of its kind, as most previous works
focus on implementing a network for the task of classification. Photonics
technology are also naturally capable of implementing complex-valued neural
networks at no additional hardware cost and we show that such neural networks
outperform conventional real-valued networks for molecular property prediction.
Our work opens the avenue for harnessing photonic technology for large-scale
machine learning applications in molecular sciences such as drug discovery and
materials design.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.00927v1,2022-03-02T08:14:06Z,2022-03-02T08:14:06Z,"TransDARC: Transformer-based Driver Activity Recognition with Latent
  Space Feature Calibration","Traditional video-based human activity recognition has experienced remarkable
progress linked to the rise of deep learning, but this effect was slower as it
comes to the downstream task of driver behavior understanding. Understanding
the situation inside the vehicle cabin is essential for Advanced Driving
Assistant System (ADAS) as it enables identifying distraction, predicting
driver's intent and leads to more convenient human-vehicle interaction. At the
same time, driver observation systems face substantial obstacles as they need
to capture different granularities of driver states, while the complexity of
such secondary activities grows with the rising automation and increased driver
freedom. Furthermore, a model is rarely deployed under conditions identical to
the ones in the training set, as sensor placements and types vary from vehicle
to vehicle, constituting a substantial obstacle for real-life deployment of
data-driven models. In this work, we present a novel vision-based framework for
recognizing secondary driver behaviours based on visual transformers and an
additional augmented feature distribution calibration module. This module
operates in the latent feature-space enriching and diversifying the training
set at feature-level in order to improve generalization to novel data
appearances, (e.g., sensor changes) and general feature quality. Our framework
consistently leads to better recognition rates, surpassing previous
state-of-the-art results of the public Drive&Act benchmark on all granularity
levels. Our code will be made publicly available at
https://github.com/KPeng9510/TransDARC.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2203.00112v2,2022-07-07T21:37:00Z,2022-02-28T22:00:02Z,GraphWorld: Fake Graphs Bring Real Insights for GNNs,"Despite advances in the field of Graph Neural Networks (GNNs), only a small
number (~5) of datasets are currently used to evaluate new models. This
continued reliance on a handful of datasets provides minimal insight into the
performance differences between models, and is especially challenging for
industrial practitioners who are likely to have datasets which look very
different from those used as academic benchmarks. In the course of our work on
GNN infrastructure and open-source software at Google, we have sought to
develop improved benchmarks that are robust, tunable, scalable,and
generalizable. In this work we introduce GraphWorld, a novel methodology and
system for benchmarking GNN models on an arbitrarily-large population of
synthetic graphs for any conceivable GNN task. GraphWorld allows a user to
efficiently generate a world with millions of statistically diverse datasets.
It is accessible, scalable, and easy to use. GraphWorld can be run on a single
machine without specialized hardware, or it can be easily scaled up to run on
arbitrary clusters or cloud frameworks. Using GraphWorld, a user has
fine-grained control over graph generator parameters, and can benchmark
arbitrary GNN models with built-in hyperparameter tuning. We present insights
from GraphWorld experiments regarding the performance characteristics of tens
of thousands of GNN models over millions of benchmark datasets. We further show
that GraphWorld efficiently explores regions of benchmark dataset space
uncovered by standard benchmarks, revealing comparisons between models that
have not been historically obtainable. Using GraphWorld, we also are able to
study in-detail the relationship between graph properties and task performance
metrics, which is nearly impossible with the classic collection of real-world
benchmarks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.12622v1,2022-02-25T11:19:05Z,2022-02-25T11:19:05Z,Towards neoRL networks; the emergence of purposive graphs,"The neoRL framework for purposive AI implements latent learning by emulated
cognitive maps, with general value functions (GVF) expressing operant desires
toward separate states. The agent's expectancy of reward, expressed as learned
projections in the considered space, allows the neoRL agent to extract
purposive behavior from the learned map according to the reward hypothesis. We
explore this allegory further, considering neoRL modules as nodes in a network
with desire as input and state-action Q-value as output; we see that action
sets with Euclidean significance imply an interpretation of state-action
vectors as Euclidean projections of desire. Autonomous desire from neoRL nodes
within the agent allows for deeper neoRL behavioral graphs. Experiments confirm
the effect of neoRL networks governed by autonomous desire, verifying the four
principles for purposive networks. A neoRL agent governed by purposive networks
can navigate Euclidean spaces in real-time while learning, exemplifying how
modern AI still can profit from inspiration from early psychology.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.10793v2,2022-05-15T21:53:10Z,2022-02-22T10:25:59Z,"PyTorch Geometric Signed Directed: A Survey and Software on Graph Neural
  Networks for Signed and Directed Graphs","Signed networks are ubiquitous in many real-world applications (e.g., social
networks encoding trust/distrust relationships, correlation networks arising
from time series data). While many signed networks are directed, there is a
lack of survey papers and software packages on graph neural networks (GNNs)
specially designed for directed networks. In this paper, we present PyTorch
Geometric Signed Directed, a survey and software on GNNs for signed and
directed networks. We review typical tasks, loss functions and evaluation
metrics in the analysis of signed and directed networks, discuss data used in
related experiments, and provide an overview of methods proposed. The deep
learning framework consists of easy-to-use GNN models, synthetic and real-world
data, as well as task-specific evaluation metrics and loss functions for signed
and directed networks. The software is presented in a modular fashion, so that
signed and directed networks can also be treated separately. As an extension
library for PyTorch Geometric, our proposed software is maintained with
open-source releases, detailed documentation, continuous integration, unit
tests and code coverage checks. Our code is publicly available at
\url{https://github.com/SherylHYX/pytorch_geometric_signed_directed}.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.10574v2,2022-06-14T09:17:44Z,2022-02-21T23:36:40Z,"A Multi-Agent Reinforcement Learning Framework for Off-Policy Evaluation
  in Two-sided Markets","The two-sided markets such as ride-sharing companies often involve a group of
subjects who are making sequential decisions across time and/or location. With
the rapid development of smart phones and internet of things, they have
substantially transformed the transportation landscape of human beings. In this
paper we consider large-scale fleet management in ride-sharing companies that
involve multiple units in different areas receiving sequences of products (or
treatments) over time. Major technical challenges, such as policy evaluation,
arise in those studies because (i) spatial and temporal proximities induce
interference between locations and times; and (ii) the large number of
locations results in the curse of dimensionality. To address both challenges
simultaneously, we introduce a multi-agent reinforcement learning (MARL)
framework for carrying policy evaluation in these studies. We propose novel
estimators for mean outcomes under different products that are consistent
despite the high-dimensionality of state-action space. The proposed estimator
works favorably in simulation experiments. We further illustrate our method
using a real dataset obtained from a two-sided marketplace company to evaluate
the effects of applying different subsidizing policies. A Python implementation
of our proposed method is available at
https://github.com/RunzheStat/CausalMARL.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.10335v1,2022-02-21T16:15:57Z,2022-02-21T16:15:57Z,Explainability in Machine Learning: a Pedagogical Perspective,"Given the importance of integrating of explainability into machine learning,
at present, there are a lack of pedagogical resources exploring this.
Specifically, we have found a need for resources in explaining how one can
teach the advantages of explainability in machine learning. Often pedagogical
approaches in the field of machine learning focus on getting students prepared
to apply various models in the real world setting, but much less attention is
given to teaching students the various techniques one could employ to explain a
model's decision-making process. Furthermore, explainability can benefit from a
narrative structure that aids one in understanding which techniques are
governed by which questions about the data.
  We provide a pedagogical perspective on how to structure the learning process
to better impart knowledge to students and researchers in machine learning,
when and how to implement various explainability techniques as well as how to
interpret the results. We discuss a system of teaching explainability in
machine learning, by exploring the advantages and disadvantages of various
opaque and transparent machine learning models, as well as when to utilize
specific explainability techniques and the various frameworks used to structure
the tools for explainability. Among discussing concrete assignments, we will
also discuss ways to structure potential assignments to best help students
learn to use explainability as a tool alongside any given machine learning
application.
  Data science professionals completing the course will have a birds-eye view
of a rapidly developing area and will be confident to deploy machine learning
more widely. A preliminary analysis on the effectiveness of a recently
delivered course following the structure presented here is included as evidence
supporting our pedagogical approach.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.10144v1,2022-02-21T11:39:24Z,2022-02-21T11:39:24Z,"Inferring Network Structure with Unobservable Nodes from Time Series
  Data","Network structures play important roles in social, technological and
biological systems. However, the observable nodes and connections in real cases
are often incomplete or unavailable due to measurement errors, private
protection issues, or other problems. Therefore, inferring the complete network
structure is useful for understanding human interactions and complex dynamics.
The existing studies have not fully solved the problem of inferring network
structure with partial information about connections or nodes. In this paper,
we tackle the problem by utilizing time-series data generated by network
dynamics. We regard the network inference problem based on dynamical time
series data as a problem of minimizing errors for predicting states of
observable nodes and proposed a novel data-driven deep learning model called
Gumbel-softmax Inference for Network (GIN) to solve the problem under
incomplete information. The GIN framework includes three modules: a dynamics
learner, a network generator, and an initial state generator to infer the
unobservable parts of the network. We implement experiments on artificial and
empirical social networks with discrete and continuous dynamics. The
experiments show that our method can infer the unknown parts of the structure
and the initial states of the observable nodes with up to 90\% accuracy. The
accuracy declines linearly with the increase of the fractions of unobservable
nodes. Our framework may have wide applications where the network structure is
hard to obtain and the time series data is rich.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.08450v1,2022-02-17T05:33:27Z,2022-02-17T05:33:27Z,"Design-Bench: Benchmarks for Data-Driven Offline Model-Based
  Optimization","Black-box model-based optimization (MBO) problems, where the goal is to find
a design input that maximizes an unknown objective function, are ubiquitous in
a wide range of domains, such as the design of proteins, DNA sequences,
aircraft, and robots. Solving model-based optimization problems typically
requires actively querying the unknown objective function on design proposals,
which means physically building the candidate molecule, aircraft, or robot,
testing it, and storing the result. This process can be expensive and time
consuming, and one might instead prefer to optimize for the best design using
only the data one already has. This setting -- called offline MBO -- poses
substantial and different algorithmic challenges than more commonly studied
online techniques. A number of recent works have demonstrated success with
offline MBO for high-dimensional optimization problems using high-capacity deep
neural networks. However, the lack of standardized benchmarks in this emerging
field is making progress difficult to track. To address this, we present
Design-Bench, a benchmark for offline MBO with a unified evaluation protocol
and reference implementations of recent methods. Our benchmark includes a suite
of diverse and realistic tasks derived from real-world optimization problems in
biology, materials science, and robotics that present distinct challenges for
offline MBO. Our benchmark and reference implementations are released at
github.com/rail-berkeley/design-bench and
github.com/rail-berkeley/design-baselines.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.08004v2,2022-06-15T12:11:37Z,2022-02-16T11:40:36Z,Deep Koopman Operator with Control for Nonlinear Systems,"Recently Koopman operator has become a promising data-driven tool to
facilitate real-time control for unknown nonlinear systems. It maps nonlinear
systems into equivalent linear systems in embedding space, ready for real-time
linear control methods. However, designing an appropriate Koopman embedding
function remains a challenging task. Furthermore, most Koopman-based algorithms
only consider nonlinear systems with linear control input, resulting in lousy
prediction and control performance when the system is fully nonlinear with the
control input. In this work, we propose an end-to-end deep learning framework
to learn the Koopman embedding function and Koopman Operator together to
alleviate such difficulties. We first parameterize the embedding function and
Koopman Operator with the neural network and train them end-to-end with the
K-steps loss function. Then, an auxiliary control network is augmented to
encode the nonlinear state-dependent control term to model the nonlinearity in
the control input. This encoded term is considered the new control variable
instead to ensure linearity of the modeled system in the embedding system.We
next deploy Linear Quadratic Regulator (LQR) on the linear embedding space to
derive the optimal control policy and decode the actual control input from the
control net. Experimental results demonstrate that our approach outperforms
other existing methods, reducing the prediction error by order of magnitude and
achieving superior control performance in several nonlinear dynamic systems
like damping pendulum, CartPole, and the seven DOF robotic manipulator.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.07785v1,2022-02-15T23:21:23Z,2022-02-15T23:21:23Z,Predictability and Surprise in Large Generative Models,"Large-scale pre-training has recently emerged as a technique for creating
capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG,
Gopher, and many others. In this paper, we highlight a counterintuitive
property of such models and discuss the policy implications of this property.
Namely, these generative models have an unusual combination of predictable loss
on a broad training distribution (as embodied in their ""scaling laws""), and
unpredictable specific capabilities, inputs, and outputs. We believe that the
high-level predictability and appearance of useful capabilities drives rapid
development of such models, while the unpredictable qualities make it difficult
to anticipate the consequences of model deployment. We go through examples of
how this combination can lead to socially harmful behavior with examples from
the literature and real world observations, and we also perform two novel
experiments to illustrate our point about harms from unpredictability.
Furthermore, we analyze how these conflicting properties combine to give model
developers various motivations for deploying these models, and challenges that
can hinder deployment. We conclude with a list of possible interventions the AI
community may take to increase the chance of these models having a beneficial
impact. We intend this paper to be useful to policymakers who want to
understand and regulate AI systems, technologists who care about the potential
policy impact of their work, and academics who want to analyze, critique, and
potentially develop large generative models.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.10336v1,2022-02-15T03:34:56Z,2022-02-15T03:34:56Z,Artificial Intelligence for the Metaverse: A Survey,"Along with the massive growth of the Internet from the 1990s until now,
various innovative technologies have been created to bring users breathtaking
experiences with more virtual interactions in cyberspace. Many virtual
environments with thousands of services and applications, from social networks
to virtual gaming worlds, have been developed with immersive experience and
digital transformation, but most are incoherent instead of being integrated
into a platform. In this context, metaverse, a term formed by combining meta
and universe, has been introduced as a shared virtual world that is fueled by
many emerging technologies, such as fifth-generation networks and beyond,
virtual reality, and artificial intelligence (AI). Among such technologies, AI
has shown the great importance of processing big data to enhance immersive
experience and enable human-like intelligence of virtual agents. In this
survey, we make a beneficial effort to explore the role of AI in the foundation
and development of the metaverse. We first deliver a preliminary of AI,
including machine learning algorithms and deep learning architectures, and its
role in the metaverse. We then convey a comprehensive investigation of AI-based
methods concerning six technical aspects that have potentials for the
metaverse: natural language processing, machine vision, blockchain, networking,
digital twin, and neural interface, and being potential for the metaverse.
Subsequently, several AI-aided applications, such as healthcare, manufacturing,
smart cities, and gaming, are studied to be deployed in the virtual worlds.
Finally, we conclude the key contribution of this survey and open some future
research directions in AI for the metaverse.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.06836v1,2022-02-14T16:19:40Z,2022-02-14T16:19:40Z,"A Machine Learning Framework for Event Identification via Modal Analysis
  of PMU Data","Power systems are prone to a variety of events (e.g. line trips and
generation loss) and real-time identification of such events is crucial in
terms of situational awareness, reliability, and security. Using measurements
from multiple synchrophasors, i.e., phasor measurement units (PMUs), we propose
to identify events by extracting features based on modal dynamics. We combine
such traditional physics-based feature extraction methods with machine learning
to distinguish different event types. Including all measurement channels at
each PMU allows exploiting diverse features but also requires learning
classification models over a high-dimensional space. To address this issue,
various feature selection methods are implemented to choose the best subset of
features. Using the obtained subset of features, we investigate the performance
of two well-known classification models, namely, logistic regression (LR) and
support vector machines (SVM) to identify generation loss and line trip events
in two datasets. The first dataset is obtained from simulated generation loss
and line trip events in the Texas 2000-bus synthetic grid. The second is a
proprietary dataset with labeled events obtained from a large utility in the
USA involving measurements from nearly 500 PMUs. Our results indicate that the
proposed framework is promising for identifying the two types of events.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.07475v1,2022-02-14T14:24:57Z,2022-02-14T14:24:57Z,"A Real-time System for Detecting Landslide Reports on Social Media using
  Artificial Intelligence","This paper presents an online system that leverages social media data in real
time to identify landslide-related information automatically using
state-of-the-art artificial intelligence techniques. The designed system can
(i) reduce the information overload by eliminating duplicate and irrelevant
content, (ii) identify landslide images, (iii) infer geolocation of the images,
and (iv) categorize the user type (organization or person) of the account
sharing the information. The system was deployed in February 2020 online at
https://landslide-aidr.qcri.org/landslide_system.php to monitor live Twitter
data stream and has been running continuously since then to provide
time-critical information to partners such as British Geological Survey and
European Mediterranean Seismological Centre. We trust this system can both
contribute to harvesting of global landslide data for further research and
support global landslide maps to facilitate emergency response and decision
making.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.06639v1,2022-02-14T11:47:26Z,2022-02-14T11:47:26Z,"On the Complexity of Object Detection on Real-world Public
  Transportation Images for Social Distancing Measurement","Social distancing in public spaces has become an essential aspect in helping
to reduce the impact of the COVID-19 pandemic. Exploiting recent advances in
machine learning, there have been many studies in the literature implementing
social distancing via object detection through the use of surveillance cameras
in public spaces. However, to date, there has been no study of social distance
measurement on public transport. The public transport setting has some unique
challenges, including some low-resolution images and camera locations that can
lead to the partial occlusion of passengers, which make it challenging to
perform accurate detection. Thus, in this paper, we investigate the challenges
of performing accurate social distance measurement on public transportation. We
benchmark several state-of-the-art object detection algorithms using real-world
footage taken from the London Underground and bus network. The work highlights
the complexity of performing social distancing measurement on images from
current public transportation onboard cameras. Further, exploiting domain
knowledge of expected passenger behaviour, we attempt to improve the quality of
the detections using various strategies and show improvement over using vanilla
object detection alone.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.06558v3,2022-06-22T14:25:21Z,2022-02-14T08:57:01Z,"Saute RL: Almost Surely Safe Reinforcement Learning Using State
  Augmentation","Satisfying safety constraints almost surely (or with probability one) can be
critical for the deployment of Reinforcement Learning (RL) in real-life
applications. For example, plane landing and take-off should ideally occur with
probability one. We address the problem by introducing Safety Augmented (Saute)
Markov Decision Processes (MDPs), where the safety constraints are eliminated
by augmenting them into the state-space and reshaping the objective. We show
that Saute MDP satisfies the Bellman equation and moves us closer to solving
Safe RL with constraints satisfied almost surely. We argue that Saute MDP
allows viewing the Safe RL problem from a different perspective enabling new
features. For instance, our approach has a plug-and-play nature, i.e., any RL
algorithm can be ""Sauteed"". Additionally, state augmentation allows for policy
generalization across safety constraints. We finally show that Saute RL
algorithms can outperform their state-of-the-art counterparts when constraint
satisfaction is of high importance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.05953v1,2022-02-12T02:13:55Z,2022-02-12T02:13:55Z,Open-set Adversarial Defense with Clean-Adversarial Mutual Learning,"Open-set recognition and adversarial defense study two key aspects of deep
learning that are vital for real-world deployment. The objective of open-set
recognition is to identify samples from open-set classes during testing, while
adversarial defense aims to robustify the network against images perturbed by
imperceptible adversarial noise. This paper demonstrates that open-set
recognition systems are vulnerable to adversarial samples. Furthermore, this
paper shows that adversarial defense mechanisms trained on known classes are
unable to generalize well to open-set samples. Motivated by these observations,
we emphasize the necessity of an Open-Set Adversarial Defense (OSAD) mechanism.
This paper proposes an Open-Set Defense Network with Clean-Adversarial Mutual
Learning (OSDN-CAML) as a solution to the OSAD problem. The proposed network
designs an encoder with dual-attentive feature-denoising layers coupled with a
classifier to learn a noise-free latent feature representation, which
adaptively removes adversarial noise guided by channel and spatial-wise
attentive filters. Several techniques are exploited to learn a noise-free and
informative latent feature space with the aim of improving the performance of
adversarial defense and open-set recognition. First, we incorporate a decoder
to ensure that clean images can be well reconstructed from the obtained latent
features. Then, self-supervision is used to ensure that the latent features are
informative enough to carry out an auxiliary task. Finally, to exploit more
complementary knowledge from clean image classification to facilitate feature
denoising and search for a more generalized local minimum for open-set
recognition, we further propose clean-adversarial mutual learning, where a peer
network (classifying clean images) is further introduced to mutually learn with
the classifier (classifying adversarial images).",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.05741v1,2022-02-11T16:27:14Z,2022-02-11T16:27:14Z,"Neural-Network Decoders for Quantum Error Correction using Surface
  Codes:A Space Exploration of the Hardware Cost-Performance Trade-Offs","Quantum Error Correction (QEC) is required in quantum computers to mitigate
the effect of errors on physical qubits. When adopting a QEC scheme based on
surface codes, error decoding is the most computationally expensive task in the
classical electronic back-end. Decoders employing neural networks (NN) are
well-suited for this task but their hardware implementation has not been
presented yet. This work presents a space exploration of fully-connected
feed-forward NN decoders for small distance surface codes. The goal is to
optimize the neural network for high decoding performance, while keeping a
minimalistic hardware implementation. This is needed to meet the tight delay
constraints of real-time surface code decoding. We demonstrate that hardware
based NN-decoders can achieve high decoding performance comparable to other
state-of-the-art decoding algorithms whilst being well below the tight delay
requirements $(\approx 440\ \mathrm{ns})$ of current solid-state qubit
technologies for both ASIC designs $(<30\ \mathrm{ns})$ and FPGA
implementations $(<90\ \mathrm{ns})$. These results designates NN-decoders as
fitting candidates for an integrated hardware implementation in future
large-scale quantum computers.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.05334v2,2022-04-25T13:25:02Z,2022-02-10T21:26:54Z,"Learning the Pedestrian-Vehicle Interaction for Pedestrian Trajectory
  Prediction","In this paper, we study the interaction between pedestrians and vehicles and
propose a novel neural network structure called the Pedestrian-Vehicle
Interaction (PVI) extractor for learning the pedestrian-vehicle interaction. We
implement the proposed PVI extractor on both sequential approaches (long
short-term memory (LSTM) models) and non-sequential approaches (convolutional
models). We use the Waymo Open Dataset that contains real-world urban traffic
scenes with both pedestrian and vehicle annotations. For the LSTM-based models,
our proposed model is compared with Social-LSTM and Social-GAN, and using our
proposed PVI extractor reduces the average displacement error (ADE) and the
final displacement error (FDE) by 7.46% and 5.24%, respectively. For the
convolutional-based models, our proposed model is compared with Social-STGCNN
and Social-IWSTCNN, and using our proposed PVI extractor reduces the ADE and
FDE by 2.10% and 1.27%, respectively. The results show that the
pedestrian-vehicle interaction influences pedestrian behavior, and the models
using the proposed PVI extractor can capture the interaction between
pedestrians and vehicles, and thereby outperform the compared methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.03087v2,2022-02-08T14:28:18Z,2022-02-07T11:55:23Z,Unsupervised Long-Term Person Re-Identification with Clothes Change,"We investigate unsupervised person re-identification (Re-ID) with clothes
change, a new challenging problem with more practical usability and scalability
to real-world deployment. Most existing re-id methods artificially assume the
clothes of every single person to be stationary across space and time. This
condition is mostly valid for short-term re-id scenarios since an average
person would often change the clothes even within a single day. To alleviate
this assumption, several recent works have introduced the clothes change facet
to re-id, with a focus on supervised learning person identity discriminative
representation with invariance to clothes changes. Taking a step further
towards this long-term re-id direction, we further eliminate the requirement of
person identity labels, as they are significantly more expensive and more
tedious to annotate in comparison to short-term person re-id datasets. Compared
to conventional unsupervised short-term re-id, this new problem is drastically
more challenging as different people may have similar clothes whilst the same
person can wear multiple suites of clothes over different locations and times
with very distinct appearance. To overcome such obstacles, we introduce a novel
Curriculum Person Clustering (CPC) method that can adaptively regulate the
unsupervised clustering criterion according to the clustering confidence.
Experiments on three long-term person re-id datasets show that our CPC
outperforms SOTA unsupervised re-id methods and even closely matches the
supervised re-id models.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.01197v4,2022-05-09T20:12:32Z,2022-02-02T18:43:01Z,VOS: Learning What You Don't Know by Virtual Outlier Synthesis,"Out-of-distribution (OOD) detection has received much attention lately due to
its importance in the safe deployment of neural networks. One of the key
challenges is that models lack supervision signals from unknown data, and as a
result, can produce overconfident predictions on OOD data. Previous approaches
rely on real outlier datasets for model regularization, which can be costly and
sometimes infeasible to obtain in practice. In this paper, we present VOS, a
novel framework for OOD detection by adaptively synthesizing virtual outliers
that can meaningfully regularize the model's decision boundary during training.
Specifically, VOS samples virtual outliers from the low-likelihood region of
the class-conditional distribution estimated in the feature space. Alongside,
we introduce a novel unknown-aware training objective, which contrastively
shapes the uncertainty space between the ID data and synthesized outlier data.
VOS achieves competitive performance on both object detection and image
classification models, reducing the FPR95 by up to 9.36% compared to the
previous best method on object detectors. Code is available at
https://github.com/deeplearning-wisc/vos.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2202.00617v1,2022-02-01T18:05:31Z,2022-02-01T18:05:31Z,"A General, Evolution-Inspired Reward Function for Social Robotics","The field of social robotics will likely need to depart from a paradigm of
designed behaviours and imitation learning and adopt modern reinforcement
learning (RL) methods to enable robots to interact fluidly and efficaciously
with humans. In this paper, we present the Social Reward Function as a
mechanism to provide (1) a real-time, dense reward function necessary for the
deployment of RL agents in social robotics, and (2) a standardised objective
metric for comparing the efficacy of different social robots. The Social Reward
Function is designed to closely mimic those genetically endowed social
perception capabilities of humans in an effort to provide a simple, stable and
culture-agnostic reward function. Presently, datasets used in social robotics
are either small or significantly out-of-domain with respect to social
robotics. The use of the Social Reward Function will allow larger in-domain
datasets to be collected close to the behaviour policy of social robots, which
will allow both further improvements to reward functions and to the behaviour
policies of social robots. We believe this will be the key enabler to
developing efficacious social robots in the future.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.12705v1,2022-01-30T02:10:01Z,2022-01-30T02:10:01Z,"A Robust Framework for Deep Learning Approaches to Facial Emotion
  Recognition and Evaluation","Facial emotion recognition is a vast and complex problem space within the
domain of computer vision and thus requires a universally accepted baseline
method with which to evaluate proposed models. While test datasets have served
this purpose in the academic sphere real world application and testing of such
models lacks any real comparison. Therefore we propose a framework in which
models developed for FER can be compared and contrasted against one another in
a constant standardized fashion. A lightweight convolutional neural network is
trained on the AffectNet dataset a large variable dataset for facial emotion
recognition and a web application is developed and deployed with our proposed
framework as a proof of concept. The CNN is embedded into our application and
is capable of instant real time facial emotion recognition. When tested on the
AffectNet test set this model achieves high accuracy for emotion classification
of eight different emotions. Using our framework the validity of this model and
others can be properly tested by evaluating a model efficacy not only based on
its accuracy on a sample test dataset, but also on in the wild experiments.
Additionally, our application is built with the ability to save and store any
image captured or uploaded to it for emotion recognition, allowing for the
curation of more quality and diverse facial emotion recognition datasets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.09151v2,2022-04-12T02:29:08Z,2022-01-23T00:44:56Z,"An External Stability Audit Framework to Test the Validity of
  Personality Prediction in AI Hiring","Automated hiring systems are among the fastest-developing of all high-stakes
AI systems. Among these are algorithmic personality tests that use insights
from psychometric testing, and promise to surface personality traits indicative
of future success based on job seekers' resumes or social media profiles. We
interrogate the validity of such systems using stability of the outputs they
produce, noting that reliability is a necessary, but not a sufficient,
condition for validity. Our approach is to (a) develop a methodology for an
external audit of stability of predictions made by algorithmic personality
tests, and (b) instantiate this methodology in an audit of two systems,
Humantic AI and Crystal. Crucially, rather than challenging or affirming the
assumptions made in psychometric testing -- that personality is a meaningful
and measurable construct, and that personality traits are indicative of future
success on the job -- we frame our methodology around testing the underlying
assumptions made by the vendors of the algorithmic personality tests
themselves.
  Our main contribution is the development of a socio-technical framework for
auditing the stability of algorithmic systems. This contribution is
supplemented with an open-source software library that implements the technical
components of the audit, and can be used to conduct similar stability audits of
algorithmic systems. We instantiate our framework with the audit of two
real-world personality prediction systems, namely Humantic AI and Crystal. The
application of our audit framework demonstrates that both these systems show
substantial instability with respect to key facets of measurement, and hence
cannot be considered valid testing instruments.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.08475v1,2022-01-20T22:30:59Z,2022-01-20T22:30:59Z,GenGNN: A Generic FPGA Framework for Graph Neural Network Acceleration,"Graph neural networks (GNNs) have recently exploded in popularity thanks to
their broad applicability to ubiquitous graph-related problems such as quantum
chemistry, drug discovery, and high energy physics. However, meeting demand for
novel GNN models and fast inference simultaneously is challenging because of
the gap between the difficulty in developing efficient FPGA accelerators and
the rapid pace of creation of new GNN models. Prior art focuses on the
acceleration of specific classes of GNNs but lacks the generality to work
across existing models or to extend to new and emerging GNN models. In this
work, we propose a generic GNN acceleration framework using High-Level
Synthesis (HLS), named GenGNN, with two-fold goals. First, we aim to deliver
ultra-fast GNN inference without any graph pre-processing for real-time
requirements. Second, we aim to support a diverse set of GNN models with the
extensibility to flexibly adapt to new models. The framework features an
optimized message-passing structure applicable to all models, combined with a
rich library of model-specific components. We verify our implementation
on-board on the Xilinx Alveo U50 FPGA and observe a speed-up of up to 25x
against CPU (6226R) baseline and 13x against GPU (A6000) baseline. Our HLS code
will be open-source on GitHub upon acceptance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.06599v1,2022-01-17T19:25:33Z,2022-01-17T19:25:33Z,"Who supervises the supervisor? Model monitoring in production using deep
  feature embeddings with applications to workpiece inspection","The automation of condition monitoring and workpiece inspection plays an
essential role in maintaining high quality as well as high throughput of the
manufacturing process. To this end, the recent rise of developments in machine
learning has lead to vast improvements in the area of autonomous process
supervision. However, the more complex and powerful these models become, the
less transparent and explainable they generally are as well. One of the main
challenges is the monitoring of live deployments of these machine learning
systems and raising alerts when encountering events that might impact model
performance. In particular, supervised classifiers are typically build under
the assumption of stationarity in the underlying data distribution. For
example, a visual inspection system trained on a set of material surface
defects generally does not adapt or even recognize gradual changes in the data
distribution - an issue known as ""data drift"" - such as the emergence of new
types of surface defects. This, in turn, may lead to detrimental
mispredictions, e.g. samples from new defect classes being classified as
non-defective. To this end, it is desirable to provide real-time tracking of a
classifier's performance to inform about the putative onset of additional error
classes and the necessity for manual intervention with respect to classifier
re-training. Here, we propose an unsupervised framework that acts on top of a
supervised classification system, thereby harnessing its internal deep feature
representations as a proxy to track changes in the data distribution during
deployment and, hence, to anticipate classifier performance degradation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.05858v1,2022-01-15T14:15:46Z,2022-01-15T14:15:46Z,"Smart Parking Space Detection under Hazy conditions using Convolutional
  Neural Networks: A Novel Approach","Limited urban parking space combined with urbanization has necessitated the
development of smart parking systems that can communicate the availability of
parking slots to the end users. Towards this, various deep learning based
solutions using convolutional neural networks have been proposed for parking
space occupation detection. Though these approaches are robust to partial
obstructions and lighting conditions, their performance is found to degrade in
the presence of haze conditions. Looking in this direction, this paper
investigates the use of dehazing networks that improves the performance of
parking space occupancy classifier under hazy conditions. Additionally,
training procedures are proposed for dehazing networks to maximize the
performance of the system on both hazy and non-hazy conditions. The proposed
system is deployable as part of existing smart parking systems where limited
number of cameras are used to monitor hundreds of parking spaces. To validate
our approach, we have developed a custom hazy parking system dataset from
real-world task-driven test set of RESIDE-\b{eta} dataset. The proposed
approach is tested against existing state-of-the-art parking space detectors on
CNRPark-EXT and hazy parking system datasets. Experimental results indicate
that there is a significant accuracy improvement of the proposed approach on
the hazy parking system dataset.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.06912v1,2022-01-14T17:41:26Z,2022-01-14T17:41:26Z,Digital Twin: From Concept to Practice,"Recent technological developments and advances in Artificial Intelligence
(AI) have enabled sophisticated capabilities to be a part of Digital Twin (DT),
virtually making it possible to introduce automation into all aspects of work
processes. Given these possibilities that DT can offer, practitioners are
facing increasingly difficult decisions regarding what capabilities to select
while deploying a DT in practice. The lack of research in this field has not
helped either. It has resulted in the rebranding and reuse of emerging
technological capabilities like prediction, simulation, AI, and Machine
Learning (ML) as necessary constituents of DT. Inappropriate selection of
capabilities in a DT can result in missed opportunities, strategic
misalignments, inflated expectations, and risk of it being rejected as just
hype by the practitioners. To alleviate this challenge, this paper proposes the
digitalization framework, designed and developed by following a Design Science
Research (DSR) methodology over a period of 18 months. The framework can help
practitioners select an appropriate level of sophistication in a DT by weighing
the pros and cons for each level, deciding evaluation criteria for the digital
twin system, and assessing the implications of the selected DT on the
organizational processes and strategies, and value creation. Three real-life
case studies illustrate the application and usefulness of the framework.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.05115v1,2022-01-13T18:20:32Z,2022-01-13T18:20:32Z,Functional Anomaly Detection: a Benchmark Study,"The increasing automation in many areas of the Industry expressly demands to
design efficient machine-learning solutions for the detection of abnormal
events. With the ubiquitous deployment of sensors monitoring nearly
continuously the health of complex infrastructures, anomaly detection can now
rely on measurements sampled at a very high frequency, providing a very rich
representation of the phenomenon under surveillance. In order to exploit fully
the information thus collected, the observations cannot be treated as
multivariate data anymore and a functional analysis approach is required. It is
the purpose of this paper to investigate the performance of recent techniques
for anomaly detection in the functional setup on real datasets. After an
overview of the state-of-the-art and a visual-descriptive study, a variety of
anomaly detection methods are compared. While taxonomies of abnormalities (e.g.
shape, location) in the functional setup are documented in the literature,
assigning a specific type to the identified anomalies appears to be a
challenging task. Thus, strengths and weaknesses of the existing approaches are
benchmarked in view of these highlighted types in a simulation study. Anomaly
detection methods are next evaluated on two datasets, related to the monitoring
of helicopters in flight and to the spectrometry of construction materials
namely. The benchmark analysis is concluded by recommendation guidance for
practitioners.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.05158v1,2022-01-13T16:35:45Z,2022-01-13T16:35:45Z,Decompositional Quantum Graph Neural Network,"Quantum machine learning is a fast emerging field that aims to tackle machine
learning using quantum algorithms and quantum computing. Due to the lack of
physical qubits and an effective means to map real-world data from Euclidean
space to Hilbert space, most of these methods focus on quantum analogies or
process simulations rather than devising concrete architectures based on
qubits. In this paper, we propose a novel hybrid quantum-classical algorithm
for graph-structured data, which we refer to as the Decompositional Quantum
Graph Neural Network (DQGNN). DQGNN implements the GNN theoretical framework
using the tensor product and unity matrices representation, which greatly
reduces the number of model parameters required. When controlled by a classical
computer, DQGNN can accommodate arbitrarily sized graphs by processing
substructures from the input graph using a modestly-sized quantum device. The
architecture is based on a novel mapping from real-world data to Hilbert space.
This mapping maintains the distance relations present in the data and reduces
information loss. Experimental results show that the proposed method
outperforms competitive state-of-the-art models with only 1.68\% parameters
compared to those models.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.04014v3,2022-04-04T08:32:40Z,2022-01-11T15:53:53Z,Captcha Attack: Turning Captchas Against Humanity,"Nowadays, people generate and share massive content on online platforms
(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook
users posted around 150 thousand photos every minute. Content moderators
constantly monitor these online platforms to prevent the spreading of
inappropriate content (e.g., hate speech, nudity images). Based on deep
learning (DL) advances, Automatic Content Moderators (ACM) help human
moderators handle high data volume. Despite their advantages, attackers can
exploit weaknesses of DL components (e.g., preprocessing, model) to affect
their performance. Therefore, an attacker can leverage such techniques to
spread inappropriate content by evading ACM.
  In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that
allows users to spread inappropriate text online by evading ACM controls. CAPA,
by generating custom textual CAPTCHAs, exploits ACM's careless design
implementations and internal procedures vulnerabilities. We test our attack on
real-world ACM, and the results confirm the ferocity of our simple yet
effective attack, reaching up to a 100% evasion success in most cases. At the
same time, we demonstrate the difficulties in designing CAPA mitigations,
opening new challenges in CAPTCHAs research area.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.03413v2,2022-04-26T15:15:13Z,2022-01-10T15:52:17Z,Systems Challenges for Trustworthy Embodied Systems,"A new generation of increasingly autonomous and self-learning embodied
systems is about to be developed. When deploying embodied systems into a
real-life context we face various engineering challenges, as it is crucial to
coordinate the behavior of embodied systems in a beneficial manner, ensure
their compatibility with our human-centered social values, and design
verifiably safe and reliable human-machine interaction. We are arguing that
traditional systems engineering is coming to a climacteric from embedded to
embodied systems, and with assuring the trustworthiness of dynamic federations
of situationally aware, intent-driven, explorative, ever-evolving, largely
non-predictable, and increasingly autonomous embodied systems in uncertain,
complex, and unpredictable real-world contexts. We are therefore identifying a
number of urgent systems challenges for trustworthy embodied systems, including
robust and human-centric AI, cognitive architectures, uncertainty
quantification, trustworthy self-integration, and continual analysis and
assurance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.03550v1,2022-01-09T17:43:03Z,2022-01-09T17:43:03Z,"Machine learning enabling high-throughput and remote operations at
  large-scale user facilities","Imaging, scattering, and spectroscopy are fundamental in understanding and
discovering new functional materials. Contemporary innovations in automation
and experimental techniques have led to these measurements being performed much
faster and with higher resolution, thus producing vast amounts of data for
analysis. These innovations are particularly pronounced at user facilities and
synchrotron light sources. Machine learning (ML) methods are regularly
developed to process and interpret large datasets in real-time with
measurements. However, there remain conceptual barriers to entry for the
facility general user community, whom often lack expertise in ML, and technical
barriers for deploying ML models. Herein, we demonstrate a variety of
archetypal ML models for on-the-fly analysis at multiple beamlines at the
National Synchrotron Light Source II (NSLS-II). We describe these examples
instructively, with a focus on integrating the models into existing
experimental workflows, such that the reader can easily include their own ML
techniques into experiments at NSLS-II or facilities with a common
infrastructure. The framework presented here shows how with little effort,
diverse ML models operate in conjunction with feedback loops via integration
into the existing Bluesky Suite for experimental orchestration and data
management.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.02503v1,2022-01-07T15:42:50Z,2022-01-07T15:42:50Z,"A Review of Deep Learning Techniques for Markerless Human Motion on
  Synthetic Datasets","Markerless motion capture has become an active field of research in computer
vision in recent years. Its extensive applications are known in a great variety
of fields, including computer animation, human motion analysis, biomedical
research, virtual reality, and sports science. Estimating human posture has
recently gained increasing attention in the computer vision community, but due
to the depth of uncertainty and the lack of the synthetic datasets, it is a
challenging task. Various approaches have recently been proposed to solve this
problem, many of which are based on deep learning. They are primarily focused
on improving the performance of existing benchmarks with significant advances,
especially 2D images. Based on powerful deep learning techniques and recently
collected real-world datasets, we explored a model that can predict the
skeleton of an animation based solely on 2D images. Frames generated from
different real-world datasets with synthesized poses using different body
shapes from simple to complex. The implementation process uses DeepLabCut on
its own dataset to perform many necessary steps, then use the input frames to
train the model. The output is an animated skeleton for human movement. The
composite dataset and other results are the ""ground truth"" of the deep model.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.02279v1,2022-01-06T23:50:09Z,2022-01-06T23:50:09Z,De-rendering 3D Objects in the Wild,"With increasing focus on augmented and virtual reality applications (XR)
comes the demand for algorithms that can lift objects from images and videos
into representations that are suitable for a wide variety of related 3D tasks.
Large-scale deployment of XR devices and applications means that we cannot
solely rely on supervised learning, as collecting and annotating data for the
unlimited variety of objects in the real world is infeasible. We present a
weakly supervised method that is able to decompose a single image of an object
into shape (depth and normals), material (albedo, reflectivity and shininess)
and global lighting parameters. For training, the method only relies on a rough
initial shape estimate of the training objects to bootstrap the learning
process. This shape supervision can come for example from a pretrained depth
network or - more generically - from a traditional structure-from-motion
pipeline. In our experiments, we show that the method can successfully
de-render 2D images into a decomposed 3D representation and generalizes to
unseen object categories. Since in-the-wild evaluation is difficult due to the
lack of ground truth data, we also introduce a photo-realistic synthetic test
set that allows for quantitative evaluation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.01163v2,2022-02-24T01:11:04Z,2022-01-03T17:00:17Z,"Analyzing Micro-Founded General Equilibrium Models with Many Agents
  using Deep Reinforcement Learning","Real economies can be modeled as a sequential imperfect-information game with
many heterogeneous agents, such as consumers, firms, and governments. Dynamic
general equilibrium (DGE) models are often used for macroeconomic analysis in
this setting. However, finding general equilibria is challenging using existing
theoretical or computational methods, especially when using microfoundations to
model individual agents. Here, we show how to use deep multi-agent
reinforcement learning (MARL) to find $\epsilon$-meta-equilibria over agent
types in microfounded DGE models. Whereas standard MARL fails to learn
non-trivial solutions, our structured learning curricula enable stable
convergence to meaningful solutions. Conceptually, our approach is more
flexible and does not need unrealistic assumptions, e.g., continuous market
clearing, that are commonly used for analytical tractability. Furthermore, our
end-to-end GPU implementation enables fast real-time convergence with a large
number of RL economic agents. We showcase our approach in open and closed
real-business-cycle (RBC) models with 100 worker-consumers, 10 firms, and a
social planner who taxes and redistributes. We validate the learned solutions
are $\epsilon$-meta-equilibria through best-response analyses, show that they
align with economic intuitions, and show our approach can learn a spectrum of
qualitatively distinct $\epsilon$-meta-equilibria in open RBC models. As such,
we show that hardware-accelerated MARL is a promising framework for modeling
the complexity of economies based on microfoundations.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2201.02734v1,2022-01-02T01:43:24Z,2022-01-02T01:43:24Z,Building Human-like Communicative Intelligence: A Grounded Perspective,"Modern Artificial Intelligence (AI) systems excel at diverse tasks, from
image classification to strategy games, even outperforming humans in many of
these domains. After making astounding progress in language learning in the
recent decade, AI systems, however, seem to approach the ceiling that does not
reflect important aspects of human communicative capacities. Unlike human
learners, communicative AI systems often fail to systematically generalize to
new data, suffer from sample inefficiency, fail to capture common-sense
semantic knowledge, and do not translate to real-world communicative
situations. Cognitive Science offers several insights on how AI could move
forward from this point. This paper aims to: (1) suggest that the dominant
cognitively-inspired AI directions, based on nativist and symbolic paradigms,
lack necessary substantiation and concreteness to guide progress in modern AI,
and (2) articulate an alternative, ""grounded"", perspective on AI advancement,
inspired by Embodied, Embedded, Extended, and Enactive Cognition (4E) research.
I review results on 4E research lines in Cognitive Science to distinguish the
main aspects of naturalistic learning conditions that play causal roles for
human language development. I then use this analysis to propose a list of
concrete, implementable components for building ""grounded"" linguistic
intelligence. These components include embodying machines in a
perception-action cycle, equipping agents with active exploration mechanisms so
they can build their own curriculum, allowing agents to gradually develop motor
abilities to promote piecemeal language development, and endowing the agents
with adaptive feedback from their physical and social environment. I hope that
these ideas can direct AI research towards building machines that develop
human-like language abilities through their experiences with the world.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.14070v1,2021-12-28T09:50:39Z,2021-12-28T09:50:39Z,Intelligent Document Processing -- Methods and Tools in the real world,"The originality of this publication is to look at the subject of IDP
(Intelligent Document Processing) from the perspective of an end-user and
industrialist and not that of a Computer Science researcher. This domain is one
part of the challenge of information digitalisation that constitutes the
Industrial Revolution of the twenty first century (Industry 4.0) and this paper
looks specifically at the difficult areas of classifying, extracting
information and subsequent integration into business processes with respect to
forms and invoices. Since the focus is on practical implementation a brief
review is carried out of the market in commercial tools for OCR, document
classification and data extraction in so far as this is publicly available
together with pricing (if known). Brief definitions of the main terms
encountered in Computer Science publications and commercial prospectuses are
provided in order to de-mystify the language for the layman. A small number of
practical tests are carried out on a few real documents in order to illustrate
the capabilities of tools that are commonly available at a reasonable price.
The unsolved (so far) issue of tables contained in invoices is raised. The case
of a typical large industrial company is evoked where the requirement is to
extract 100 per cent of the information with 100 per cent reliability in order
to integrate into the back-end Enterprise Resource Planning system. Finally a
brief description is given of the state-of-the-art research by the huge
corporations who are pushing the boundaries of deep learning techniques further
and further with massive computing and financial power - progress that will
undoubtedly trickle down into the real world at some later date. The paper
finishes by asking the question whether the objectives and timing of the
commercial world and the progress of Computer Science are fully aligned.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.13389v1,2021-12-26T14:38:17Z,2021-12-26T14:38:17Z,"Attributed Graph Neural Networks for Recommendation Systems on
  Large-Scale and Sparse Graph","Link prediction in structured-data is an important problem for many
applications, especially for recommendation systems. Existing methods focus on
how to learn the node representation based on graph-based structure.
High-dimensional sparse edge features are not fully exploited. Because
balancing precision and computation efficiency is significant for
recommendation systems in real world, multiple-level feature representation in
large-scale sparse graph still lacks effective and efficient solution. In this
paper, we propose a practical solution about graph neural networks called
Attributed Graph Convolutional Networks(AGCN) to incorporate edge attributes
when apply graph neural networks in large-scale sparse networks. We formulate
the link prediction problem as a subgraph classification problem. We firstly
propose an efficient two-level projection to decompose topological structures
to node-edge pairs and project them into the same interaction feature space.
Then we apply multi-layer GCN to combine the projected node-edge pairs to
capture the topological structures. Finally, the pooling representation of two
units is treated as the input of classifier to predict the probability. We
conduct offline experiments on two industrial datasets and one public dataset
and demonstrate that AGCN outperforms other excellent baselines. Moreover, we
also deploy AGCN method to important scenarios on Xianyu and AliExpress. In
online systems, AGCN achieves over 5% improvement on online metrics.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.13058v1,2021-12-24T13:27:05Z,2021-12-24T13:27:05Z,Tri-Transformer Hawkes Process: Three Heads are better than one,"Abstract. Most of the real world data we encounter are asynchronous event
sequence, so the last decades have been characterized by the implementation of
various point process into the field of social networks,electronic medical
records and financial transactions. At the beginning, Hawkes process and its
variants which can simulate simultaneously the self-triggering and mutual
triggering patterns between different events in complex sequences in a clear
and quantitative way are more popular.Later on, with the advances of neural
network, neural Hawkes process has been proposed one after another, and
gradually become a research hotspot. The proposal of the transformer Hawkes
process (THP) has gained a huge performance improvement, so a new upsurge of
the neural Hawkes process based on transformer is set off. However, THP does
not make full use of the information of occurrence time and type of event in
the asynchronous event sequence. It simply adds the encoding of event type
conversion and the location encoding of time conversion to the source encoding.
At the same time, the learner built from a single transformer will result in an
inescapable learning bias. In order to mitigate these problems, we propose a
tri-transformer Hawkes process (Tri-THP) model, in which the event and time
information are added to the dot-product attention as auxiliary information to
form a new multihead attention. The effectiveness of the Tri-THP is proved by a
series of well-designed experiments on both real world and synthetic data.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.12841v1,2021-12-23T20:52:33Z,2021-12-23T20:52:33Z,ABC of the Future,"Approximate Bayesian computation (ABC) has advanced in two decades from a
seminal idea to a practically applicable inference tool for simulator-based
statistical models, which are becoming increasingly popular in many research
domains. The computational feasibility of ABC for practical applications has
been recently boosted by adopting techniques from machine learning to build
surrogate models for the approximate likelihood or posterior and by the
introduction of a general-purpose software platform with several advanced
features, including automated parallelization. Here we demonstrate the
strengths of the advances in ABC by going beyond the typical benchmark examples
and considering real applications in astronomy, infectious disease
epidemiology, personalised cancer therapy and financial prediction. We
anticipate that the emerging success of ABC in producing actual added value and
quantitative insights in the real world will continue to inspire a plethora of
further applications across different fields of science, social science and
technology.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.11925v2,2022-01-28T10:26:03Z,2021-12-22T14:45:37Z,SOLIS -- The MLOps journey from data acquisition to actionable insights,"Machine Learning operations is unarguably a very important and also one of
the hottest topics in Artificial Intelligence lately. Being able to define very
clear hypotheses for actual real-life problems that can be addressed by machine
learning models, collecting and curating large amounts of data for model
training and validation followed by model architecture search and actual
optimization and finally presenting the results fits very well the scenario of
Data Science experiments. This approach however does not supply the needed
procedures and pipelines for the actual deployment of machine learning
capabilities in real production grade systems. Automating live configuration
mechanisms, on the fly adapting to live or offline data capture and
consumption, serving multiple models in parallel either on edge or cloud
architectures, addressing specific limitations of GPU memory or compute power,
post-processing inference or prediction results and serving those either as
APIs or with IoT based communication stacks in the same end-to-end pipeline are
the real challenges that we try to address in this particular paper. In this
paper we present a unified deployment pipeline and freedom-to-operate approach
that supports all above requirements while using basic cross-platform tensor
framework and script language engines.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.11561v2,2022-04-27T07:25:54Z,2021-12-21T22:51:37Z,"Explainable artificial intelligence for autonomous driving: An overview
  and guide for future research directions","Autonomous driving has achieved a significant milestone in research and
development over the last decade. There is increasing interest in the field as
the deployment of self-operating vehicles promises safer and more ecologically
friendly transportation systems. With the rise of computationally powerful
artificial intelligence (AI) techniques, autonomous vehicles can sense their
environment with high precision, make safe real-time decisions, and operate
reliably without human intervention. However, intelligent decision-making in
autonomous cars is not generally understandable by humans in the current state
of the art, and such deficiency hinders this technology from being socially
acceptable. Hence, aside from making safe real-time decisions, the AI systems
of autonomous vehicles also need to explain how their decisions are constructed
in order to be regulatory compliant across many jurisdictions. Our study sheds
a comprehensive light on the development of explainable artificial intelligence
(XAI) approaches for autonomous vehicles. In particular, we make the following
contributions. First, we provide a thorough overview of the state-of-the-art
studies on XAI for autonomous driving. We then propose an XAI framework that
considers all the societal and legal requirements for explainability of
autonomous driving systems. Finally, as future research directions, we provide
a guide to XAI approaches that can improve operational safety and transparency
to support public approval of autonomous driving technology by regulators,
manufacturers, and all engaged stakeholders.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.10513v1,2021-12-20T13:13:05Z,2021-12-20T13:13:05Z,"Learning Robust Policy against Disturbance in Transition Dynamics via
  State-Conservative Policy Optimization","Deep reinforcement learning algorithms can perform poorly in real-world tasks
due to the discrepancy between source and target environments. This discrepancy
is commonly viewed as the disturbance in transition dynamics. Many existing
algorithms learn robust policies by modeling the disturbance and applying it to
source environments during training, which usually requires prior knowledge
about the disturbance and control of simulators. However, these algorithms can
fail in scenarios where the disturbance from target environments is unknown or
is intractable to model in simulators. To tackle this problem, we propose a
novel model-free actor-critic algorithm -- namely, state-conservative policy
optimization (SCPO) -- to learn robust policies without modeling the
disturbance in advance. Specifically, SCPO reduces the disturbance in
transition dynamics to that in state space and then approximates it by a simple
gradient-based regularizer. The appealing features of SCPO include that it is
simple to implement and does not require additional knowledge about the
disturbance or specially designed simulators. Experiments in several robot
control tasks demonstrate that SCPO learns robust policies against the
disturbance in transition dynamics.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.12061v1,2021-12-20T05:10:48Z,2021-12-20T05:10:48Z,"Adaptive model reduction and state estimation of agro-hydrological
  systems","Closed-loop irrigation can deliver a promising solution for precision
irrigation. The accurate soil moisture (state) estimation is critical in
implementing the closed-loop irrigation of agrohydrological systems. In
general, the agricultural fields are high dimensional systems. Due to the high
dimensionality for a large field, it is very challenging to solve an
optimizationbased advanced state estimator like moving horizon estimation
(MHE). This work addresses the aforementioned challenge and proposes a
systematic approach for state estimation of large agricultural fields. We use a
non-linear state-space model based on discretization of the cylindrical
coordinate version of Richards equation to describe the agro-hydrological
systems equipped with a central pivot irrigation system. We propose a
structure-preserving adaptive model reduction method using trajectory-based
unsupervised machine learning techniques. Furthermore, the adaptive MHE
algorithm is developed based on an adaptive reduced model. The proposed
algorithms are applied to a small simulated field to compare the performance of
adaptive MHE over original MHE. Finally, the proposed approach is applied to a
large-scale real agricultural field to test the effectiveness and superiority
to address the current challenges. Extensive simulations are carried out to
show the efficiency of the proposed approach.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.09159v2,2022-05-06T12:48:41Z,2021-12-16T19:11:29Z,"Implementation of a Binary Neural Network on a Passive Array of Magnetic
  Tunnel Junctions","The increasing scale of neural networks and their growing application space
have produced demand for more energy- and memory-efficient
artificial-intelligence-specific hardware. Avenues to mitigate the main issue,
the von Neumann bottleneck, include in-memory and near-memory architectures, as
well as algorithmic approaches. Here we leverage the low-power and the
inherently binary operation of magnetic tunnel junctions (MTJs) to demonstrate
neural network hardware inference based on passive arrays of MTJs. In general,
transferring a trained network model to hardware for inference is confronted by
degradation in performance due to device-to-device variations, write errors,
parasitic resistance, and nonidealities in the substrate. To quantify the
effect of these hardware realities, we benchmark 300 unique weight matrix
solutions of a 2-layer perceptron to classify the Wine dataset for both
classification accuracy and write fidelity. Despite device imperfections, we
achieve software-equivalent accuracy of up to 95.3 % with proper tuning of
network parameters in 15 x 15 MTJ arrays having a range of device sizes. The
success of this tuning process shows that new metrics are needed to
characterize the performance and quality of networks reproduced in mixed signal
hardware.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.08211v1,2021-12-15T15:36:57Z,2021-12-15T15:36:57Z,"TrialGraph: Machine Intelligence Enabled Insight from Graph Modelling of
  Clinical Trials","A major impediment to successful drug development is the complexity, cost,
and scale of clinical trials. The detailed internal structure of clinical trial
data can make conventional optimization difficult to achieve. Recent advances
in machine learning, specifically graph-structured data analysis, have the
potential to enable significant progress in improving the clinical trial
design. TrialGraph seeks to apply these methodologies to produce a
proof-of-concept framework for developing models which can aid drug development
and benefit patients. In this work, we first introduce a curated clinical trial
data set compiled from the CT.gov, AACT and TrialTrove databases (n=1191
trials; representing one million patients) and describe the conversion of this
data to graph-structured formats. We then detail the mathematical basis and
implementation of a selection of graph machine learning algorithms, which
typically use standard machine classifiers on graph data embedded in a
low-dimensional feature space. We trained these models to predict side effect
information for a clinical trial given information on the disease, existing
medical conditions, and treatment. The MetaPath2Vec algorithm performed
exceptionally well, with standard Logistic Regression, Decision Tree, Random
Forest, Support Vector, and Neural Network classifiers exhibiting typical
ROC-AUC scores of 0.85, 0.68, 0.86, 0.80, and 0.77, respectively. Remarkably,
the best performing classifiers could only produce typical ROC-AUC scores of
0.70 when trained on equivalent array-structured data. Our work demonstrates
that graph modelling can significantly improve prediction accuracy on
appropriate datasets. Successive versions of the project that refine modelling
assumptions and incorporate more data types can produce excellent predictors
with real-world applications in drug development.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.08097v1,2021-12-15T13:14:08Z,2021-12-15T13:14:08Z,"Fusing Low-Latency Data Feeds with Death Data to Accurately Nowcast
  COVID-19 Related Deaths","The emergence of the novel coronavirus (COVID-19) has generated a need to
quickly and accurately assemble up-to-date information related to its spread.
While it is possible to use deaths to provide a reliable information feed, the
latency of data derived from deaths is significant. Confirmed cases derived
from positive test results potentially provide a lower latency data feed.
However, the sampling of those tested varies with time and the reason for
testing is often not recorded. Hospital admissions typically occur around 1-2
weeks after infection and can be considered out of date in relation to the time
of initial infection. The extent to which these issues are problematic is
likely to vary over time and between countries.
  We use a machine learning algorithm for natural language processing, trained
in multiple languages, to identify symptomatic individuals derived from social
media and, in particular Twitter, in real-time. We then use an extended SEIRD
epidemiological model to fuse combinations of low-latency feeds, including the
symptomatic counts from Twitter, with death data to estimate parameters of the
model and nowcast the number of people in each compartment. The model is
implemented in the probabilistic programming language Stan and uses a bespoke
numerical integrator. We present results showing that using specific
low-latency data feeds along with death data provides more consistent and
accurate forecasts of COVID-19 related deaths than using death data alone.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.07508v3,2022-06-17T09:00:26Z,2021-12-14T16:12:30Z,"Anti-Money Laundering Alert Optimization Using Machine Learning with
  Graphs","Money laundering is a global problem that concerns legitimizing proceeds from
serious felonies (1.7-4 trillion euros annually) such as drug dealing, human
trafficking, or corruption. The anti-money laundering systems deployed by
financial institutions typically comprise rules aligned with regulatory
frameworks. Human investigators review the alerts and report suspicious cases.
Such systems suffer from high false-positive rates, undermining their
effectiveness and resulting in high operational costs. We propose a machine
learning triage model, which complements the rule-based system and learns to
predict the risk of an alert accurately. Our model uses both entity-centric
engineered features and attributes characterizing inter-entity relations in the
form of graph-based features. We leverage time windows to construct the dynamic
graph, optimizing for time and space efficiency. We validate our model on a
real-world banking dataset and show how the triage model can reduce the number
of false positives by 80% while detecting over 90% of true positives. In this
way, our model can significantly improve anti-money laundering operations.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.07159v2,2022-02-09T22:21:20Z,2021-12-14T04:47:12Z,Birds Eye View Social Distancing Analysis System,"Social distancing can reduce the infection rates in respiratory pandemics
such as COVID-19. Traffic intersections are particularly suitable for
monitoring and evaluation of social distancing behavior in metropolises. We
propose and evaluate a privacy-preserving social distancing analysis system
(B-SDA), which uses bird's-eye view video recordings of pedestrians who cross
traffic intersections. We devise algorithms for video pre-processing, object
detection and tracking which are rooted in the known computer-vision and deep
learning techniques, but modified to address the problem of detecting very
small objects/pedestrians captured by a highly elevated camera. We propose a
method for incorporating pedestrian grouping for detection of social distancing
violations. B-SDA is used to compare pedestrian behavior based on pre-pandemic
and pandemic videos in a major metropolitan area. The accomplished pedestrian
detection performance is $63.0\%$ $AP_{50}$ and the tracking performance is
$47.6\%$ MOTA. The social distancing violation rate of $15.6\%$ during the
pandemic is notably lower than $31.4\%$ pre-pandemic baseline, indicating that
pedestrians followed CDC-prescribed social distancing recommendations. The
proposed system is suitable for deployment in real-world applications.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.05393v1,2021-12-10T08:56:55Z,2021-12-10T08:56:55Z,A Self-supervised Mixed-curvature Graph Neural Network,"Graph representation learning received increasing attentions in recent years.
Most of existing methods ignore the complexity of the graph structures and
restrict graphs in a single constant-curvature representation space, which is
only suitable to particular kinds of graph structure indeed. Additionally,
these methods follow the supervised or semi-supervised learning paradigm, and
thereby notably limit their deployment on the unlabeled graphs in real
applications. To address these aforementioned limitations, we take the first
attempt to study the self-supervised graph representation learning in the
mixed-curvature spaces. In this paper, we present a novel Self-supervised
Mixed-curvature Graph Neural Network (SelfMGNN). Instead of working on one
single constant-curvature space, we construct a mixed-curvature space via the
Cartesian product of multiple Riemannian component spaces and design
hierarchical attention mechanisms for learning and fusing the representations
across these component spaces. To enable the self-supervisd learning, we
propose a novel dual contrastive approach. The mixed-curvature Riemannian space
actually provides multiple Riemannian views for the contrastive learning. We
introduce a Riemannian projector to reveal these views, and utilize a
well-designed Riemannian discriminator for the single-view and cross-view
contrastive learning within and across the Riemannian views. Finally, extensive
experiments show that SelfMGNN captures the complicated graph structures in
reality and outperforms state-of-the-art baselines.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.05792v1,2021-12-08T15:55:57Z,2021-12-08T15:55:57Z,"Digital Twin of Electrical Tomography for Quantitative Multiphase Flow
  Imaging","We report a digital twin (DT) framework of electrical tomography (ET) to
address the challenge of real-time quantitative multiphase flow imaging based
on non-invasive and non-radioactive technologies. Multiphase flow is ubiquitous
in nature, industry, and research. Accurate flow imaging is the key to
understanding this complex phenomenon. Existing non-radioactive multiphase flow
imaging methods based on electrical tomography are limited to providing
qualitative images. The proposed DT framework, building upon a synergistic
integration of 3D field coupling simulation, model-based deep learning, and
edge computing, allows ET to dynamically learn the flow features in the virtual
space and implement the model in the physical system, thus providing
unprecedented resolution and accuracy. The DT framework is demonstrated on
gas-liquid two-phase flow and electrical capacitance tomography (ECT). It can
be readily extended to various tomography modalities, scenarios, and scales in
biomedical, energy, and aerospace applications as an effective alternative to
radioactive solutions for precise flow visualization and characterization.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.04263v1,2021-12-08T12:50:33Z,2021-12-08T12:50:33Z,"Artificial Intelligence Powered Mobile Networks: From Cognition to
  Decision","Mobile networks (MN) are anticipated to provide unprecedented opportunities
to enable a new world of connected experiences and radically shift the way
people interact with everything. MN are becoming more and more complex, driven
by ever-increasingly complicated configuration issues and blossoming new
service requirements. This complexity poses significant challenges in
deployment, management, operation, optimization, and maintenance, since they
require a complete understanding and cognition of MN. Artificial intelligence
(AI), which deals with the simulation of intelligent behavior in computers, has
demonstrated enormous success in many application domains, suggesting its
potential in cognizing the state of MN and making intelligent decisions. In
this paper, we first propose an AI-powered mobile network architecture and
discuss challenges in terms of cognition complexity, decisions with
high-dimensional action space, and self-adaption to system dynamics. Then,
potential solutions that are associated with AI are discussed. Finally, we
propose a deep learning approach that directly maps the state of MN to
perceived QoS, integrating cognition with the decision. Our proposed approach
helps operators in making more intelligent decisions to guarantee QoS.
Meanwhile, the effectiveness and advantages of our proposed approach are
demonstrated on a real-world dataset, involving $31261$ users over $77$
stations within $5$ days.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.03732v1,2021-12-07T14:41:28Z,2021-12-07T14:41:28Z,A coarse space acceleration of deep-DDM,"The use of deep learning methods for solving PDEs is a field in full
expansion. In particular, Physical Informed Neural Networks, that implement a
sampling of the physical domain and use a loss function that penalizes the
violation of the partial differential equation, have shown their great
potential. Yet, to address large scale problems encountered in real
applications and compete with existing numerical methods for PDEs, it is
important to design parallel algorithms with good scalability properties. In
the vein of traditional domain decomposition methods (DDM), we consider the
recently proposed deep-ddm approach. We present an extension of this method
that relies on the use of a coarse space correction, similarly to what is done
in traditional DDM solvers. Our investigations shows that the coarse correction
is able to alleviate the deterioration of the convergence of the solver when
the number of subdomains is increased thanks to an instantaneous information
exchange between subdomains at each iteration. Experimental results demonstrate
that our approach induces a remarkable acceleration of the original deep-ddm
method, at a reduced additional computational cost.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.02637v1,2021-12-05T17:53:06Z,2021-12-05T17:53:06Z,"Modeling Live Video Streaming: Real-Time Classification, QoE Inference,
  and Field Evaluation","Social media, professional sports, and video games are driving rapid growth
in live video streaming, on platforms such as Twitch and YouTube Live. Live
streaming experience is very susceptible to short-time-scale network congestion
since client playback buffers are often no more than a few seconds.
Unfortunately, identifying such streams and measuring their QoE for network
management is challenging, since content providers largely use the same
delivery infrastructure for live and video-on-demand (VoD) streaming, and
packet inspection techniques (including SNI/DNS query monitoring) cannot always
distinguish between the two.
  In this paper, we design, build, and deploy ReCLive: a machine learning
method for live video detection and QoE measurement based on network-level
behavioral characteristics. Our contributions are four-fold: (1) We analyze
about 23,000 video streams from Twitch and YouTube, and identify key features
in their traffic profile that differentiate live and on-demand streaming. We
release our traffic traces as open data to the public; (2) We develop an
LSTM-based binary classifier model that distinguishes live from on-demand
streams in real-time with over 95% accuracy across providers; (3) We develop a
method that estimates QoE metrics of live streaming flows in terms of
resolution and buffer stall events with overall accuracies of 93% and 90%,
respectively; and (4) Finally, we prototype our solution, train it in the lab,
and deploy it in a live ISP network serving more than 7,000 subscribers. Our
method provides ISPs with fine-grained visibility into live video streams,
enabling them to measure and improve user experience.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.02424v2,2022-01-28T17:22:50Z,2021-12-04T20:27:31Z,Variational Wasserstein gradient flow,"Wasserstein gradient flow has emerged as a promising approach to solve
optimization problems over the space of probability distributions. A recent
trend is to use the well-known JKO scheme in combination with input convex
neural networks to numerically implement the proximal step. The most
challenging step, in this setup, is to evaluate functions involving density
explicitly, such as entropy, in terms of samples. This paper builds on the
recent works with a slight but crucial difference: we propose to utilize a
variational formulation of the objective function formulated as maximization
over a parametric class of functions. Theoretically, the proposed variational
formulation allows the construction of gradient flows directly for empirical
distributions with a well-defined and meaningful objective function.
Computationally, this approach replaces the computationally expensive step in
existing methods, to handle objective functions involving density, with inner
loop updates that only require a small batch of samples and scale well with the
dimension. The performance and scalability of the proposed method are
illustrated with the aid of several numerical experiments involving
high-dimensional synthetic and real datasets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.01650v1,2021-12-03T00:11:36Z,2021-12-03T00:11:36Z,"The impact of varying electrical stimulation parameters on neuromuscular
  response","High density neurostimulation systems are coming to market to help spinal
cord injury patients by stimulating and recording neuromuscular function.
However, the parameter space that these systems have to explore is exceedingly
large, and would need an artificial intelligence (AI) system to optimize. We
need a platform that will allow us to determine the optimal parameter space for
these systems. Our project aims to build a platform for mapping and controlling
neuromuscular activity, as a high-throughput testbed for implementing and
testing closed-loop neuromuscular activity. This abstract presents the first
phase (the mapping phase) of building that testbed by combining multi-electrode
stimulation/recording with visual motion-tracking. A 3D-printed rectangular
raceway was used with 4 pairs of differential recording electrodes, and two
stimulation electrodes embedded in the raceway bed. Non-anesthetized earthworms
were placed on the raceway with their head section on the stimulating
electrodes. Bipolar sinusoidal stimulation pulses of a range of voltages (2 to
6Vp-p), pulse durations (2 ms to 6.7 ms), and a burst rate of 1 pulse per
second were applied, and action potentials and physical motion were recorded
and analyzed. Action potentials were found to correlate with
expansion/contraction displacements of worm segments, and voltage increases
were shown to increase action potential propagation amplitude. Using the
multiple electrode recording allowed us to capture the wave propagation of
action potential pulse over the length of the worm. Feasibility of a platform
to simultaneously monitor action potentials and motion of earthworms with
real-time mapping was demonstrated.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2112.01031v3,2022-05-06T00:24:41Z,2021-12-02T07:49:24Z,"Improving sensitivity of the ARIANNA detector by rejecting thermal noise
  with deep learning","The ARIANNA experiment is an Askaryan detector designed to record radio
signals induced by neutrino interactions in the Antarctic ice. Because of the
low neutrino flux at high energies ($E > 10^{16} $), the physics output is
limited by statistics. Hence, an increase in sensitivity significantly improves
the interpretation of data and offers the ability to probe new parameter
spaces. The amplitudes of the trigger threshold are limited by the rate of
triggering on unavoidable thermal noise fluctuations. We present a real-time
thermal noise rejection algorithm that enables the trigger thresholds to be
lowered, which increases the sensitivity to neutrinos by up to a factor of two
(depending on energy) compared to the current ARIANNA capabilities. A deep
learning discriminator, based on a Convolutional Neural Network (CNN), is
implemented to identify and remove thermal events in real time. We describe a
CNN trained on MC data that runs on the current ARIANNA microcomputer and
retains 95 percent of the neutrino signal at a thermal noise rejection factor
of $10^5$, compared to a template matching procedure which reaches only $10^2$
for the same signal efficiency. Then the results are verified in a lab
measurement by feeding in generated neutrino-like signal pulses and thermal
noise directly into the ARIANNA data acquisition system. Lastly, the same CNN
is used to classify cosmic-rays events to make sure they are not rejected. The
network classified 102 out of 104 cosmic-ray events as signal.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.15646v3,2022-04-13T02:11:46Z,2021-11-30T18:28:19Z,The Exponentially Tilted Gaussian Prior for Variational Autoencoders,"An important property for deep neural networks is the ability to perform
robust out-of-distribution detection on previously unseen data. This property
is essential for safety purposes when deploying models for real world
applications. Recent studies show that probabilistic generative models can
perform poorly on this task, which is surprising given that they seek to
estimate the likelihood of training data. To alleviate this issue, we propose
the exponentially tilted Gaussian prior distribution for the Variational
Autoencoder (VAE) which pulls points onto the surface of a hyper-sphere in
latent space. This achieves state-of-the art results on the area under the
curve-receiver operator characteristics metric using just the log-likelihood
that the VAE naturally assigns. Because this prior is a simple modification of
the traditional VAE prior, it is faster and easier to implement than
competitive methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.14826v2,2022-04-07T04:55:16Z,2021-11-29T18:59:55Z,"Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via
  Generalized Straight-Through Estimation","The nonuniform quantization strategy for compressing neural networks usually
achieves better performance than its counterpart, i.e., uniform strategy, due
to its superior representational capacity. However, many nonuniform
quantization methods overlook the complicated projection process in
implementing the nonuniformly quantized weights/activations, which incurs
non-negligible time and space overhead in hardware deployment. In this study,
we propose Nonuniform-to-Uniform Quantization (N2UQ), a method that can
maintain the strong representation ability of nonuniform methods while being
hardware-friendly and efficient as the uniform quantization for model
inference. We achieve this through learning the flexible in-equidistant input
thresholds to better fit the underlying distribution while quantizing these
real-valued inputs into equidistant output levels. To train the quantized
network with learnable input thresholds, we introduce a generalized
straight-through estimator (G-STE) for intractable backward derivative
calculation w.r.t. threshold parameters. Additionally, we consider entropy
preserving regularization to further reduce information loss in weight
quantization. Even under this adverse constraint of imposing uniformly
quantized weights and activations, our N2UQ outperforms state-of-the-art
nonuniform quantization methods by 0.5~1.7 on ImageNet, demonstrating the
contribution of N2UQ design. Code and models are available at:
https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.14837v2,2022-03-16T01:20:39Z,2021-11-29T12:21:47Z,"p2pGNN: A Decentralized Graph Neural Network for Node Classification in
  Peer-to-Peer Networks","In this work, we aim to classify nodes of unstructured peer-to-peer networks
with communication uncertainty, such as users of decentralized social networks.
Graph Neural Networks (GNNs) are known to improve the accuracy of simple
classifiers in centralized settings by leveraging naturally occurring network
links, but graph convolutional layers are challenging to implement in
decentralized settings when node neighbors are not constantly available. We
address this problem by employing decoupled GNNs, where base classifier
predictions and errors are diffused through graphs after training. For these,
we deploy pre-trained and gossip-trained base classifiers and implement
peer-to-peer graph diffusion under communication uncertainty. In particular, we
develop an asynchronous decentralized formulation of diffusion that converges
to centralized predictions in distribution and linearly with respect to
communication rates. We experiment on three real-world graphs with node
features and labels and simulate peer-to-peer networks with uniformly random
communication frequencies; given a portion of known labels, our decentralized
graph diffusion achieves comparable accuracy to centralized GNNs with minimal
communication overhead (less than 3% of what gossip training already adds).",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.14176v1,2021-11-28T15:28:31Z,2021-11-28T15:28:31Z,UAV-based Crowd Surveillance in Post COVID-19 Era,"To cope with the current pandemic situation and reinstate pseudo-normal daily
life, several measures have been deployed and maintained, such as mask wearing,
social distancing, hands sanitizing, etc. Since outdoor cultural events,
concerts, and picnics, are gradually allowed, a close monitoring of the crowd
activity is needed to avoid undesired contact and disease transmission. In this
context, intelligent unmanned aerial vehicles (UAVs) can be occasionally
deployed to ensure the surveillance of these activities, that health
restriction measures are applied, and to trigger alerts when the latter are not
respected. Consequently, we propose in this paper a complete UAV framework for
intelligent monitoring of post COVID-19 outdoor activities. Specifically, we
propose a three steps approach. In the first step, captured images by a UAV are
analyzed using machine learning to detect and locate individuals. The second
step consists of a novel coordinates mapping approach to evaluate distances
among individuals, then cluster them, while the third step provides an
energy-efficient and/or reliable UAV trajectory to inspect clusters for
restrictions violation such as mask wearing. Obtained results provide the
following insights: 1) Efficient detection of individuals depends on the angle
from which the image was captured, 2) coordinates mapping is very sensitive to
the estimation error in individuals' bounding boxes, and 3) UAV trajectory
design algorithm 2-Opt is recommended for practical real-time deployments due
to its low-complexity and near-optimal performance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.13330v2,2021-12-11T19:25:47Z,2021-11-26T06:35:15Z,"ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural
  Networks","Over the past few years, deep neural networks (DNNs) have achieved tremendous
success and have been continuously applied in many application domains.
However, during the practical deployment in the industrial tasks, DNNs are
found to be erroneous-prone due to various reasons such as overfitting, lacking
robustness to real-world corruptions during practical usage. To address these
challenges, many recent attempts have been made to repair DNNs for version
updates under practical operational contexts by updating weights (i.e., network
parameters) through retraining, fine-tuning, or direct weight fixing at a
neural level. In this work, as the first attempt, we initiate to repair DNNs by
jointly optimizing the architecture and weights at a higher (i.e., block)
level.
  We first perform empirical studies to investigate the limitation of whole
network-level and layer-level repairing, which motivates us to explore a novel
repairing direction for DNN repair at the block level. To this end, we first
propose adversarial-aware spectrum analysis for vulnerable block localization
that considers the neurons' status and weights' gradients in blocks during the
forward and backward processes, which enables more accurate candidate block
localization for repairing even under a few examples. Then, we further propose
the architecture-oriented search-based repairing that relaxes the targeted
block to a continuous repairing search space at higher deep feature levels. By
jointly optimizing the architecture and weights in that space, we can identify
a much better block architecture. We implement our proposed repairing
techniques as a tool, named ArchRepair, and conduct extensive experiments to
validate the proposed method. The results show that our method can not only
repair but also enhance accuracy & robustness, outperforming the
state-of-the-art DNN repair techniques.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.12184v1,2021-11-23T22:45:04Z,2021-11-23T22:45:04Z,Style-Guided Web Application Exploration,"A wide range of analysis and testing techniques targeting modern web apps
rely on the automated exploration of their state space by firing events that
mimic user interactions. However, finding out which elements are actionable in
web apps is not a trivial task. To improve the efficacy of exploring the event
space of web apps, we propose a browser-independent, instrumentation-free
approach based on structural and visual stylistic cues. Our approach,
implemented in a tool called StyleX, employs machine learning models, trained
on 700,000 web elements from 1,000 real-world websites, to predict actionable
elements on a webpage a priori. In addition, our approach uses stylistic cues
for ranking these actionable elements while exploring the app. Our actionable
predictor models achieve 90.14\% precision and 87.76\% recall when considering
the click event listener, and on average, 75.42\% precision and 77.76\% recall
when considering the five most-frequent event types. Our evaluations show that
StyleX can improve the JavaScript code coverage achieved by a general-purpose
crawler by up to 23\%.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.12142v1,2021-11-23T20:28:07Z,2021-11-23T20:28:07Z,"Phenomenological classification of the Zwicky Transient Facility
  astronomical event alerts","The Zwicky Transient Facility (ZTF), a state-of-the-art optical robotic sky
survey, registers on the order of a million transient events - such as
supernova explosions, changes in brightness of variable sources, or moving
object detections - every clear night, and generates associated real-time
alerts. We present Alert-Classifying Artificial Intelligence (ACAI), an
open-source deep-learning framework for the phenomenological classification of
ZTF alerts. ACAI uses a set of five binary classifiers to characterize objects
which, in combination with the auxiliary/contextual event information available
from alert brokers, provides a powerful tool for alert stream filtering
tailored to different science cases, including early identification of
supernova-like and anomalous transient events. We report on the performance of
ACAI during the first months of deployment in a production setting.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.11872v1,2021-11-23T13:40:22Z,2021-11-23T13:40:22Z,"Real-time intelligent big data processing: technology, platform, and
  applications","Human beings keep exploring the physical space using information means. Only
recently, with the rapid development of information technologies and the
increasing accumulation of data, human beings can learn more about the unknown
world with data-driven methods. Given data timeliness, there is a growing
awareness of the importance of real-time data. There are two categories of
technologies accounting for data processing: batching big data and streaming
processing, which have not been integrated well. Thus, we propose an innovative
incremental processing technology named after Stream Cube to process both big
data and stream data. Also, we implement a real-time intelligent data
processing system, which is based on real-time acquisition, real-time
processing, real-time analysis, and real-time decision-making. The real-time
intelligent data processing technology system is equipped with a batching big
data platform, data analysis tools, and machine learning models. Based on our
applications and analysis, the real-time intelligent data processing system is
a crucial solution to the problems of the national society and economy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.09999v1,2021-11-19T01:35:10Z,2021-11-19T01:35:10Z,"TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep
  Neural Network Systems","Deep neural networks are vulnerable to attacks from adversarial inputs and,
more recently, Trojans to misguide or hijack the decision of the model. We
expose the existence of an intriguing class of bounded adversarial examples --
Universal NaTuralistic adversarial paTches -- we call TnTs, by exploring the
superset of the bounded adversarial example space and the natural input space
within generative adversarial networks. Now, an adversary can arm themselves
with a patch that is naturalistic, less malicious-looking, physically
realizable, highly effective -- achieving high attack success rates, and
universal. A TnT is universal because any input image captured with a TnT in
the scene will: i) misguide a network (untargeted attack); or ii) force the
network to make a malicious decision (targeted attack). Interestingly, now, an
adversarial patch attacker has the potential to exert a greater level of
control -- the ability to choose a location independent, natural-looking patch
as a trigger in contrast to being constrained to noisy perturbations -- an
ability is thus far shown to be only possible with Trojan attack methods
needing to interfere with the model building processes to embed a backdoor at
the risk discovery; but, still realize a patch deployable in the physical
world. Through extensive experiments on the large-scale visual classification
task, ImageNet with evaluations across its entire validation set of 50,000
images, we demonstrate the realistic threat from TnTs and the robustness of the
attack. We show a generalization of the attack to create patches achieving
higher attack success rates than existing state-of-the-art methods. Our results
show the generalizability of the attack to different visual classification
tasks (CIFAR-10, GTSRB, PubFig) and multiple state-of-the-art deep neural
networks such as WideResnet50, Inception-V3 and VGG-16.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.09805v2,2022-07-17T18:38:18Z,2021-11-18T17:11:43Z,DICE: Leveraging Sparsification for Out-of-Distribution Detection,"Detecting out-of-distribution (OOD) inputs is a central challenge for safely
deploying machine learning models in the real world. Previous methods commonly
rely on an OOD score derived from the overparameterized weight space, while
largely overlooking the role of sparsification. In this paper, we reveal
important insights that reliance on unimportant weights and units can directly
attribute to the brittleness of OOD detection. To mitigate the issue, we
propose a sparsification-based OOD detection framework termed DICE. Our key
idea is to rank weights based on a measure of contribution, and selectively use
the most salient weights to derive the output for OOD detection. We provide
both empirical and theoretical insights, characterizing and explaining the
mechanism by which DICE improves OOD detection. By pruning away noisy signals,
DICE provably reduces the output variance for OOD data, resulting in a sharper
output distribution and stronger separability from ID data. We demonstrate the
effectiveness of sparsification-based OOD detection on several benchmarks and
establish competitive performance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.08892v2,2021-11-26T16:13:57Z,2021-11-17T03:57:11Z,"SAPNet: Segmentation-Aware Progressive Network for Perceptual
  Contrastive Deraining","Deep learning algorithms have recently achieved promising deraining
performances on both the natural and synthetic rainy datasets. As an essential
low-level pre-processing stage, a deraining network should clear the rain
streaks and preserve the fine semantic details. However, most existing methods
only consider low-level image restoration. That limits their performances at
high-level tasks requiring precise semantic information. To address this issue,
in this paper, we present a segmentation-aware progressive network (SAPNet)
based upon contrastive learning for single image deraining. We start our method
with a lightweight derain network formed with progressive dilated units (PDU).
The PDU can significantly expand the receptive field and characterize
multi-scale rain streaks without the heavy computation on multi-scale images. A
fundamental aspect of this work is an unsupervised background segmentation
(UBS) network initialized with ImageNet and Gaussian weights. The UBS can
faithfully preserve an image's semantic information and improve the
generalization ability to unseen photos. Furthermore, we introduce a perceptual
contrastive loss (PCL) and a learned perceptual image similarity loss (LPISL)
to regulate model learning. By exploiting the rainy image and groundtruth as
the negative and the positive sample in the VGG-16 latent space, we bridge the
fine semantic details between the derained image and the groundtruth in a fully
constrained manner. Comprehensive experiments on synthetic and real-world rainy
images show our model surpasses top-performing methods and aids object
detection and semantic segmentation with considerable efficacy. A Pytorch
Implementation is available at
https://github.com/ShenZheng2000/SAPNet-for-image-deraining.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.08885v1,2021-11-17T03:29:59Z,2021-11-17T03:29:59Z,Jump Interval-Learning for Individualized Decision Making,"An individualized decision rule (IDR) is a decision function that assigns
each individual a given treatment based on his/her observed characteristics.
Most of the existing works in the literature consider settings with binary or
finitely many treatment options. In this paper, we focus on the continuous
treatment setting and propose a jump interval-learning to develop an
individualized interval-valued decision rule (I2DR) that maximizes the expected
outcome. Unlike IDRs that recommend a single treatment, the proposed I2DR
yields an interval of treatment options for each individual, making it more
flexible to implement in practice. To derive an optimal I2DR, our jump
interval-learning method estimates the conditional mean of the outcome given
the treatment and the covariates via jump penalized regression, and derives the
corresponding optimal I2DR based on the estimated outcome regression function.
The regressor is allowed to be either linear for clear interpretation or deep
neural network to model complex treatment-covariates interactions. To implement
jump interval-learning, we develop a searching algorithm based on dynamic
programming that efficiently computes the outcome regression function.
Statistical properties of the resulting I2DR are established when the outcome
regression function is either a piecewise or continuous function over the
treatment space. We further develop a procedure to infer the mean outcome under
the (estimated) optimal policy. Extensive simulations and a real data
application to a warfarin study are conducted to demonstrate the empirical
validity of the proposed I2DR.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.07171v1,2021-11-13T18:48:28Z,2021-11-13T18:48:28Z,"Deep Reinforcement Learning with Shallow Controllers: An Experimental
  Application to PID Tuning","Deep reinforcement learning (RL) is an optimization-driven framework for
producing control strategies for general dynamical systems without explicit
reliance on process models. Good results have been reported in simulation. Here
we demonstrate the challenges in implementing a state of the art deep RL
algorithm on a real physical system. Aspects include the interplay between
software and existing hardware; experiment design and sample efficiency;
training subject to input constraints; and interpretability of the algorithm
and control law. At the core of our approach is the use of a PID controller as
the trainable RL policy. In addition to its simplicity, this approach has
several appealing features: No additional hardware needs to be added to the
control system, since a PID controller can easily be implemented through a
standard programmable logic controller; the control law can easily be
initialized in a ""safe'' region of the parameter space; and the final product
-- a well-tuned PID controller -- has a form that practitioners can reason
about and deploy with confidence.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.05063v2,2022-04-09T08:13:34Z,2021-11-09T11:47:43Z,"Tightening the Approximation Error of Adversarial Risk with Auto Loss
  Function Search","Despite achieving great success, Deep Neural Networks (DNNs) are vulnerable
to adversarial examples. How to accurately evaluate the adversarial robustness
of DNNs is critical for their deployment in real-world applications. An ideal
indicator of robustness is adversarial risk. Unfortunately, since it involves
maximizing the 0-1 loss, calculating the true risk is technically intractable.
The most common solution for this is to compute an approximate risk by
replacing the 0-1 loss with a surrogate one. Some functions have been used,
such as Cross-Entropy (CE) loss and Difference of Logits Ratio (DLR) loss.
However, these functions are all manually designed and may not be well suited
for adversarial robustness evaluation. In this paper, we leverage AutoML to
tighten the error (gap) between the true and approximate risks. Our main
contributions are as follows. First, AutoLoss-AR, the first method to search
for surrogate losses for adversarial risk, with an elaborate search space, is
proposed. The experimental results on 10 adversarially trained models
demonstrate the effectiveness of the proposed method: the risks evaluated using
the best-discovered losses are 0.2% to 1.6% better than those evaluated using
the handcrafted baselines. Second, 5 surrogate losses with clean and readable
formulas are distilled out and tested on 7 unseen adversarially trained models.
These losses outperform the baselines by 0.8% to 2.4%, indicating that they can
be used individually as some kind of new knowledge. Besides, the possible
reasons for the better performance of these losses are explored.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.02149v1,2021-11-03T11:37:11Z,2021-11-03T11:37:11Z,"Deployment Optimization for Shared e-Mobility Systems with Multi-agent
  Deep Neural Search","Shared e-mobility services have been widely tested and piloted in cities
across the globe, and already woven into the fabric of modern urban planning.
This paper studies a practical yet important problem in those systems: how to
deploy and manage their infrastructure across space and time, so that the
services are ubiquitous to the users while sustainable in profitability.
However, in real-world systems evaluating the performance of different
deployment strategies and then finding the optimal plan is prohibitively
expensive, as it is often infeasible to conduct many iterations of
trial-and-error. We tackle this by designing a high-fidelity simulation
environment, which abstracts the key operation details of the shared e-mobility
systems at fine-granularity, and is calibrated using data collected from the
real-world. This allows us to try out arbitrary deployment plans to learn the
optimal given specific context, before actually implementing any in the
real-world systems. In particular, we propose a novel multi-agent neural search
approach, in which we design a hierarchical controller to produce tentative
deployment plans. The generated deployment plans are then tested using a
multi-simulation paradigm, i.e., evaluated in parallel, where the results are
used to train the controller with deep reinforcement learning. With this closed
loop, the controller can be steered to have higher probability of generating
better deployment plans in future iterations. The proposed approach has been
evaluated extensively in our simulation environment, and experimental results
show that it outperforms baselines e.g., human knowledge, and state-of-the-art
heuristic-based optimization approaches in both service coverage and net
revenue.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.01365v2,2022-06-28T09:01:51Z,2021-11-02T04:32:18Z,"Koopman Q-learning: Offline Reinforcement Learning via Symmetries of
  Dynamics","Offline reinforcement learning leverages large datasets to train policies
without interactions with the environment. The learned policies may then be
deployed in real-world settings where interactions are costly or dangerous.
Current algorithms over-fit to the training dataset and as a consequence
perform poorly when deployed to out-of-distribution generalizations of the
environment. We aim to address these limitations by learning a Koopman latent
representation which allows us to infer symmetries of the system's underlying
dynamic. The latter is then utilized to extend the otherwise static offline
dataset during training; this constitutes a novel data augmentation framework
which reflects the system's dynamic and is thus to be interpreted as an
exploration of the environments phase space. To obtain the symmetries we employ
Koopman theory in which nonlinear dynamics are represented in terms of a linear
operator acting on the space of measurement functions of the system and thus
symmetries of the dynamics may be inferred directly. We provide novel
theoretical results on the existence and nature of symmetries relevant for
control systems such as reinforcement learning settings. Moreover, we
empirically evaluate our method on several benchmark offline reinforcement
learning tasks and datasets including D4RL, Metaworld and Robosuite and find
that by using our framework we consistently improve the state-of-the-art of
model-free Q-learning methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.15245v1,2021-10-28T16:04:01Z,2021-10-28T16:04:01Z,"From Machine Learning to Robotics: Challenges and Opportunities for
  Embodied Intelligence","Machine learning has long since become a keystone technology, accelerating
science and applications in a broad range of domains. Consequently, the notion
of applying learning methods to a particular problem set has become an
established and valuable modus operandi to advance a particular field. In this
article we argue that such an approach does not straightforwardly extended to
robotics -- or to embodied intelligence more generally: systems which engage in
a purposeful exchange of energy and information with a physical environment. In
particular, the purview of embodied intelligent agents extends significantly
beyond the typical considerations of main-stream machine learning approaches,
which typically (i) do not consider operation under conditions significantly
different from those encountered during training; (ii) do not consider the
often substantial, long-lasting and potentially safety-critical nature of
interactions during learning and deployment; (iii) do not require ready
adaptation to novel tasks while at the same time (iv) effectively and
efficiently curating and extending their models of the world through targeted
and deliberate actions. In reality, therefore, these limitations result in
learning-based systems which suffer from many of the same operational
shortcomings as more traditional, engineering-based approaches when deployed on
a robot outside a well defined, and often narrow operating envelope. Contrary
to viewing embodied intelligence as another application domain for machine
learning, here we argue that it is in fact a key driver for the advancement of
machine learning technology. In this article our goal is to highlight
challenges and opportunities that are specific to embodied intelligence and to
propose research directions which may significantly advance the
state-of-the-art in robot learning.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.13720v2,2022-01-06T20:23:24Z,2021-10-26T14:13:57Z,"Deep DIC: Deep Learning-Based Digital Image Correlation for End-to-End
  Displacement and Strain Measurement","Digital image correlation (DIC) has become an industry standard to retrieve
accurate displacement and strain measurement in tensile testing and other
material characterization. Though traditional DIC offers a high precision
estimation of deformation for general tensile testing cases, the prediction
becomes unstable at large deformation or when the speckle patterns start to
tear. In addition, traditional DIC requires a long computation time and often
produces a low spatial resolution output affected by filtering and speckle
pattern quality. To address these challenges, we propose a new deep
learning-based DIC approach--Deep DIC, in which two convolutional neural
networks, DisplacementNet and StrainNet, are designed to work together for
end-to-end prediction of displacements and strains. DisplacementNet predicts
the displacement field and adaptively tracks a region of interest. StrainNet
predicts the strain field directly from the image input without relying on the
displacement prediction, which significantly improves the strain prediction
accuracy. A new dataset generation method is developed to synthesize a
realistic and comprehensive dataset, including the generation of speckle
patterns and the deformation of the speckle image with synthetic displacement
fields. Though trained on synthetic datasets only, Deep DIC gives highly
consistent and comparable predictions of displacement and strain with those
obtained from commercial DIC software for real experiments, while it
outperforms commercial software with very robust strain prediction even at
large and localized deformation and varied pattern qualities. In addition, Deep
DIC is capable of real-time prediction of deformation with a calculation time
down to milliseconds.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.00345v5,2022-05-06T09:57:01Z,2021-10-26T00:21:15Z,Multi-Agent Advisor Q-Learning,"In the last decade, there have been significant advances in multi-agent
reinforcement learning (MARL) but there are still numerous challenges, such as
high sample complexity and slow convergence to stable policies, that need to be
overcome before wide-spread deployment is possible. However, many real-world
environments already, in practice, deploy sub-optimal or heuristic approaches
for generating policies. An interesting question that arises is how to best use
such approaches as advisors to help improve reinforcement learning in
multi-agent domains. In this paper, we provide a principled framework for
incorporating action recommendations from online sub-optimal advisors in
multi-agent settings. We describe the problem of ADvising Multiple Intelligent
Reinforcement Agents (ADMIRAL) in nonrestrictive general-sum stochastic game
environments and present two novel Q-learning based algorithms: ADMIRAL -
Decision Making (ADMIRAL-DM) and ADMIRAL - Advisor Evaluation (ADMIRAL-AE),
which allow us to improve learning by appropriately incorporating advice from
an advisor (ADMIRAL-DM), and evaluate the effectiveness of an advisor
(ADMIRAL-AE). We analyze the algorithms theoretically and provide fixed-point
guarantees regarding their learning in general-sum stochastic games.
Furthermore, extensive experiments illustrate that these algorithms: can be
used in a variety of environments, have performances that compare favourably to
other related baselines, can scale to large state-action spaces, and are robust
to poor advice from advisors.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.13041v1,2021-10-25T15:25:25Z,2021-10-25T15:25:25Z,Applications and Techniques for Fast Machine Learning in Science,"In this community review report, we discuss applications and techniques for
fast machine learning (ML) in science -- the concept of integrating power ML
methods into the real-time experimental data processing loop to accelerate
scientific discovery. The material for the report builds on two workshops held
by the Fast ML for Science community and covers three main areas: applications
for fast ML across a number of scientific domains; techniques for training and
implementing performant and resource-efficient ML algorithms; and computing
architectures, platforms, and technologies for deploying these algorithms. We
also present overlapping challenges across the multiple scientific domains
where common solutions can be found. This community report is intended to give
plenty of examples and inspiration for scientific discovery through integrated
and accelerated ML solutions. This is followed by a high-level overview and
organization of technical advances, including an abundance of pointers to
source material, which can enable these breakthroughs.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.13650v1,2021-10-25T15:09:10Z,2021-10-25T15:09:10Z,GANash -- A GAN approach to steganography,"Data security is of the utmost concern of a communication system. Since the
early days, many developments have been made to improve the performance of the
system. PSNR of the received signal, secure transmission channel, quality of
encoding used, etc. are some of the key attributes of a good system. To ensure
security, the most commonly used technique is cryptography in which the message
is altered with respect to a key and using the same, the encoded message is
decoded at the receiver side. A complementary technique that is popularly used
to insure security is steganography. The advancements in Artificial
Intelligence(AI) have paved way for performing steganography in an intelligent,
tamper-proof manner. The recent discovery by researchers in the field of Deep
Learning(DL), an unsupervised learning network known as the Generative
Adversarial Networks(GAN) has improved the performance of this technique
exponentially. It has been demonstrated that deep neural networks are highly
sensitive to tiny perturbations of input data, giving rise to adversarial
examples. Though this property is usually considered a weakness of learned
models, it could be beneficial if used appropriately. The work that has been
accomplished by MIT for this purpose, a deep-neural model by the name of
SteganoGAN, has shown obligation for using this technique for steganography. In
this work, we have proposed a novel approach to improve the performance of the
existing system using latent space compression on the encoded data. This
theoretically would improve the performance exponentially. Thus, the algorithms
used to improve the system's performance and the results obtained have been
enunciated in this work. The results indicate the level of dominance this
system could achieve to be able to diminish the difficulties in solving
real-time problems in terms of security, deployment and database management.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.12749v1,2021-10-25T09:24:00Z,2021-10-25T09:24:00Z,"A GPU based single-pulse search pipeline (GSP) with database and its
  application to the commensal radio astronomy FAST survey (CRAFTS)","We developed a GPU based single-pulse search pipeline (GSP) with
candidate-archiving database. Largely based upon the infrastructure of Open
source pulsar search and analysis toolkit (PRESTO), GSP implements GPU
acceleration of the de-dispersion and integrates a candidate-archiving
database. We applied GSP to the data streams from the commensal radio astronomy
FAST survey (CRAFTS), which resulted in a quasi-real-time processing. The
integrated candidate database facilitates synergistic usage of multiple
machine-learning tools and thus improves efficient identification of radio
pulsars such as rotating radio transients (RRATs) and Fast Radio Bursts (FRBs).
We first tested GSP on pilot CRAFTS observations with the FAST Ultra-Wide Band
(UWB) receiver. GSP detected all pulsars known from the the Parkes multibeam
pulsar survey in the respective sky area covered by the FAST-UWB. GSP also
discovered 13 new pulsars. We measured the computational efficiency of GSP to
be ~120 times faster than the original PRESTO and ~60 times faster than a
MPI-parallelized version of PRESTO.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.10921v1,2021-10-21T06:26:31Z,2021-10-21T06:26:31Z,CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization,"Deep convolutional neural networks are shown to be overkill with high
parametric and computational redundancy in many application scenarios, and an
increasing number of works have explored model pruning to obtain lightweight
and efficient networks. However, most existing pruning approaches are driven by
empirical heuristics and rarely consider the joint impact of channels, leading
to unguaranteed and suboptimal performance. In this paper, we propose a novel
channel pruning method via class-aware trace ratio optimization (CATRO) to
reduce the computational burden and accelerate the model inference. Utilizing
class information from a few samples, CATRO measures the joint impact of
multiple channels by feature space discriminations and consolidates the
layer-wise impact of preserved channels. By formulating channel pruning as a
submodular set function maximization problem, CATRO solves it efficiently via a
two-stage greedy iterative optimization procedure. More importantly, we present
theoretical justifications on convergence and performance of CATRO.
Experimental results demonstrate that CATRO achieves higher accuracy with
similar computation cost or lower computation cost with similar accuracy than
other state-of-the-art channel pruning algorithms. In addition, because of its
class-aware property, CATRO is suitable to prune efficient networks adaptively
for various classification subtasks, enhancing handy deployment and usage of
deep networks in real-world applications.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.10655v2,2022-02-26T16:55:44Z,2021-10-20T16:49:26Z,"Socialbots on Fire: Modeling Adversarial Behaviors of Socialbots via
  Multi-Agent Hierarchical Reinforcement Learning","Socialbots are software-driven user accounts on social platforms, acting
autonomously (mimicking human behavior), with the aims to influence the
opinions of other users or spread targeted misinformation for particular goals.
As socialbots undermine the ecosystem of social platforms, they are often
considered harmful. As such, there have been several computational efforts to
auto-detect the socialbots. However, to our best knowledge, the adversarial
nature of these socialbots has not yet been studied. This begs a question ""can
adversaries, controlling socialbots, exploit AI techniques to their advantage?""
To this question, we successfully demonstrate that indeed it is possible for
adversaries to exploit computational learning mechanism such as reinforcement
learning (RL) to maximize the influence of socialbots while avoiding being
detected. We first formulate the adversarial socialbot learning as a
cooperative game between two functional hierarchical RL agents. While one agent
curates a sequence of activities that can avoid the detection, the other agent
aims to maximize network influence by selectively connecting with right users.
Our proposed policy networks train with a vast amount of synthetic graphs and
generalize better than baselines on unseen real-life graphs both in terms of
maximizing network influence (up to +18%) and sustainable stealthiness (up to
+40% undetectability) under a strong bot detector (with 90% detection
accuracy). During inference, the complexity of our approach scales linearly,
independent of a network's structure and the virality of news. This makes our
approach a practical adversarial attack when deployed in a real-life setting.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.09236v1,2021-10-18T12:32:35Z,2021-10-18T12:32:35Z,"Model-Based Reinforcement Learning Framework of Online Network Resource
  Allocation","Online Network Resource Allocation (ONRA) for service provisioning is a
fundamental problem in communication networks. As a sequential decision-making
under uncertainty problem, it is promising to approach ONRA via Reinforcement
Learning (RL). But, RL solutions suffer from the sample complexity issue; i.e.,
a large number of interactions with the environment needed to find an efficient
policy. This is a barrier to utilize RL for ONRA as on one hand, it is not
practical to train the RL agent offline due to lack of information about future
requests, and on the other hand, online training in the real network leads to
significant performance loss because of the sub-optimal policy during the
prolonged learning time. This performance degradation is even higher in
non-stationary ONRA where the agent should continually adapt the policy with
the changes in service requests. To deal with this issue, we develop a general
resource allocation framework, named RADAR, using model-based RL for a class of
ONRA problems with the known immediate reward of each action. RADAR improves
sample efficiency via exploring the state space in the background and
exploiting the policy in the decision-time using synthetic samples by the model
of the environment, which is trained by real interactions. Applying RADAR on
the multi-domain service federation problem, to maximize profit via selecting
proper domains for service requests deployment, shows its continual learning
capability and up to 44% performance improvement w.r.t. the standard model-free
RL solution.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.08760v1,2021-10-17T08:41:21Z,2021-10-17T08:41:21Z,"Adapting Membership Inference Attacks to GNN for Graph Classification:
  Approaches and Implications","Graph Neural Networks (GNNs) are widely adopted to analyse non-Euclidean
data, such as chemical networks, brain networks, and social networks, modelling
complex relationships and interdependency between objects. Recently, Membership
Inference Attack (MIA) against GNNs raises severe privacy concerns, where
training data can be leaked from trained GNN models. However, prior studies
focus on inferring the membership of only the components in a graph, e.g., an
individual node or edge. How to infer the membership of an entire graph record
is yet to be explored.
  In this paper, we take the first step in MIA against GNNs for graph-level
classification. Our objective is to infer whether a graph sample has been used
for training a GNN model. We present and implement two types of attacks, i.e.,
training-based attacks and threshold-based attacks from different adversarial
capabilities. We perform comprehensive experiments to evaluate our attacks in
seven real-world datasets using five representative GNN models. Both our
attacks are shown effective and can achieve high performance, i.e., reaching
over 0.7 attack F1 scores in most cases. Furthermore, we analyse the
implications behind the MIA against GNNs. Our findings confirm that GNNs can be
even more vulnerable to MIA than the models with non-graph structures. And
unlike the node-level classifier, MIAs on graph-level classification tasks are
more co-related with the overfitting level of GNNs rather than the statistic
property of their training graphs.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.08732v1,2021-10-17T06:12:02Z,2021-10-17T06:12:02Z,A Deep Learning-based Approach for Real-time Facemask Detection,"The COVID-19 pandemic is causing a global health crisis. Public spaces need
to be safeguarded from the adverse effects of this pandemic. Wearing a facemask
becomes one of the effective protection solutions adopted by many governments.
Manual real-time monitoring of facemask wearing for a large group of people is
becoming a difficult task. The goal of this paper is to use deep learning (DL),
which has shown excellent results in many real-life applications, to ensure
efficient real-time facemask detection. The proposed approach is based on two
steps. An off-line step aiming to create a DL model that is able to detect and
locate facemasks and whether they are appropriately worn. An online step that
deploys the DL model at edge computing in order to detect masks in real-time.
In this study, we propose to use MobileNetV2 to detect facemask in real-time.
Several experiments are conducted and show good performances of the proposed
approach (99% for training and testing accuracy). In addition, several
comparisons with many state-of-the-art models namely ResNet50, DenseNet, and
VGG16 show good performance of the MobileNetV2 in terms of training time and
accuracy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.08307v1,2021-10-15T18:29:46Z,2021-10-15T18:29:46Z,GrowSpace: Learning How to Shape Plants,"Plants are dynamic systems that are integral to our existence and survival.
Plants face environment changes and adapt over time to their surrounding
conditions. We argue that plant responses to an environmental stimulus are a
good example of a real-world problem that can be approached within a
reinforcement learning (RL)framework. With the objective of controlling a plant
by moving the light source, we propose GrowSpace, as a new RL benchmark. The
back-end of the simulator is implemented using the Space Colonisation
Algorithm, a plant growing model based on competition for space. Compared to
video game RL environments, this simulator addresses a real-world problem and
serves as a test bed to visualize plant growth and movement in a faster way
than physical experiments. GrowSpace is composed of a suite of challenges that
tackle several problems such as control, multi-stage learning,fairness and
multi-objective learning. We provide agent baselines alongside case studies to
demonstrate the difficulty of the proposed benchmark.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2111.09410v4,2022-05-31T13:45:32Z,2021-10-14T14:06:57Z,"EdgeML: Towards Network-Accelerated Federated Learning over Wireless
  Edge","Federated learning (FL) is a distributed machine learning technology for
next-generation AI systems that allows a number of workers, i.e., edge devices,
collaboratively learn a shared global model while keeping their data locally to
prevent privacy leakage. Enabling FL over wireless multi-hop networks can
democratize AI and make it accessible in a cost-effective manner. However, the
noisy bandwidth-limited multi-hop wireless connections can lead to delayed and
nomadic model updates, which significantly slows down the FL convergence speed.
To address such challenges, this paper aims to accelerate FL convergence over
wireless edge by optimizing the multi-hop federated networking performance. In
particular, the FL convergence optimization problem is formulated as a Markov
decision process (MDP). To solve such MDP, multi-agent reinforcement learning
(MA-RL) algorithms along with domain-specific action space refining schemes are
developed, which online learn the delay-minimum forwarding paths to minimize
the model exchange latency between the edge devices (i.e., workers) and the
remote server. To validate the proposed solutions, FedEdge is developed and
implemented, which is the first experimental framework in the literature for FL
over multi-hop wireless edge computing networks. FedEdge allows us to fast
prototype, deploy, and evaluate novel FL algorithms along with RL-based system
optimization methods in real wireless devices. Moreover, a physical
experimental testbed is implemented by customizing the widely adopted Linux
wireless routers and ML computing nodes.Finally, our experimentation results on
the testbed show that the proposed network-accelerated FL system can
practically and significantly improve FL convergence speed, compared to the FL
system empowered by the production-grade commercially available wireless
networking protocol, BATMAN-Adv.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.06674v1,2021-10-13T12:18:09Z,2021-10-13T12:18:09Z,Truthful AI: Developing and governing AI that does not lie,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is
harmful. While lying has traditionally been a human affair, AI systems that
make sophisticated verbal statements are becoming increasingly prevalent. This
raises the question of how we should limit the harm caused by AI ""lies"" (i.e.
falsehoods that are actively selected for). Human truthfulness is governed by
social norms and by laws (against defamation, perjury, and fraud). Differences
between AI and humans present an opportunity to have more precise standards of
truthfulness for AI, and to have these standards rise over time. This could
provide significant benefits to public epistemics and the economy, and mitigate
risks of worst-case AI futures.
  Establishing norms or laws of AI truthfulness will require significant work
to: (1) identify clear truthfulness standards; (2) create institutions that can
judge adherence to those standards; and (3) develop AI systems that are
robustly truthful.
  Our initial proposals for these areas include: (1) a standard of avoiding
""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2)
institutions to evaluate AI systems before and after real-world deployment; and
(3) explicitly training AI systems to be truthful via curated datasets and
human interaction.
  A concerning possibility is that evaluation mechanisms for eventual
truthfulness standards could be captured by political interests, leading to
harmful censorship and propaganda. Avoiding this might take careful attention.
And since the scale of AI speech acts might grow dramatically over the coming
decades, early truthfulness standards might be particularly important because
of the precedents they set.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.06196v1,2021-10-12T17:49:46Z,2021-10-12T17:49:46Z,GraPE: fast and scalable Graph Processing and Embedding,"Graph Representation Learning methods have enabled a wide range of learning
problems to be addressed for data that can be represented in graph form.
Nevertheless, several real world problems in economy, biology, medicine and
other fields raised relevant scaling problems with existing methods and their
software implementation, due to the size of real world graphs characterized by
millions of nodes and billions of edges. We present GraPE, a software resource
for graph processing and random walk based embedding, that can scale with large
and high-degree graphs and significantly speed up-computation. GraPE comprises
specialized data structures, algorithms, and a fast parallel implementation
that displays everal orders of magnitude improvement in empirical space and
time complexity compared to state of the art software resources, with a
corresponding boost in the performance of machine learning methods for edge and
node label prediction and for the unsupervised analysis of graphs.GraPE is
designed to run on laptop and desktop computers, as well as on high performance
computing clusters",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.05762v2,2021-11-15T18:31:11Z,2021-10-12T06:31:54Z,"Detecting Damage Building Using Real-time Crowdsourced Images and
  Transfer Learning","After significant earthquakes, we can see images posted on social media
platforms by individuals and media agencies owing to the mass usage of
smartphones these days. These images can be utilized to provide information
about the shaking damage in the earthquake region both to the public and
research community, and potentially to guide rescue work. This paper presents
an automated way to extract the damaged building images after earthquakes from
social media platforms such as Twitter and thus identify the particular user
posts containing such images. Using transfer learning and ~6500 manually
labelled images, we trained a deep learning model to recognize images with
damaged buildings in the scene. The trained model achieved good performance
when tested on newly acquired images of earthquakes at different locations and
ran in near real-time on Twitter feed after the 2020 M7.0 earthquake in Turkey.
Furthermore, to better understand how the model makes decisions, we also
implemented the Grad-CAM method to visualize the important locations on the
images that facilitate the decision.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.05606v2,2022-02-25T07:14:05Z,2021-10-11T20:57:01Z,"Nearest Subspace Search in The Signed Cumulative Distribution Transform
  Space for 1D Signal Classification","This paper presents a new method to classify 1D signals using the signed
cumulative distribution transform (SCDT). The proposed method exploits certain
linearization properties of the SCDT to render the problem easier to solve in
the SCDT space. The method uses the nearest subspace search technique in the
SCDT domain to provide a non-iterative, effective, and simple to implement
classification algorithm. Experiments show that the proposed technique
outperforms the state-of-the-art neural networks using a very low number of
training samples and is also robust to out-of-distribution examples on
simulated data. We also demonstrate the efficacy of the proposed technique in
real-world applications by applying it to an ECG classification problem. The
python code implementing the proposed classifier can be found in PyTransKit
(https://github.com/rohdelab/PyTransKit).",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.05098v1,2021-10-11T09:10:19Z,2021-10-11T09:10:19Z,SurroundNet: Towards Effective Low-Light Image Enhancement,"Although Convolution Neural Networks (CNNs) has made substantial progress in
the low-light image enhancement task, one critical problem of CNNs is the
paradox of model complexity and performance. This paper presents a novel
SurroundNet which only involves less than 150$K$ parameters (about 80-98
percent size reduction compared to SOTAs) and achieves very competitive
performance. The proposed network comprises several Adaptive Retinex Blocks
(ARBlock), which can be viewed as a novel extension of Single Scale Retinex in
feature space. The core of our ARBlock is an efficient illumination estimation
function called Adaptive Surround Function (ASF). It can be regarded as a
general form of surround functions and be implemented by convolution layers. In
addition, we also introduce a Low-Exposure Denoiser (LED) to smooth the
low-light image before the enhancement. We evaluate the proposed method on the
real-world low-light dataset. Experimental results demonstrate that the
superiority of our submitted SurroundNet in both performance and network
parameters against State-of-the-Art low-light image enhancement methods. Code
is available at https: github.com/ouc-ocean-group/SurroundNet.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.04953v1,2021-10-11T01:23:07Z,2021-10-11T01:23:07Z,"Compact CNN Models for On-device Ocular-based User Recognition in Mobile
  Devices","A number of studies have demonstrated the efficacy of deep learning
convolutional neural network (CNN) models for ocular-based user recognition in
mobile devices. However, these high-performing networks have enormous space and
computational complexity due to the millions of parameters and computations
involved. These requirements make the deployment of deep learning models to
resource-constrained mobile devices challenging. To this end, only a handful of
studies based on knowledge distillation and patch-based models have been
proposed to obtain compact size CNN models for ocular recognition in the mobile
environment. In order to further advance the state-of-the-art, this study for
the first time evaluates five neural network pruning methods and compares them
with the knowledge distillation method for on-device CNN inference and mobile
user verification using ocular images. Subject-independent analysis on VISOB
and UPFR-Periocular datasets suggest the efficacy of layerwise magnitude-based
pruning at a compression rate of 8 for mobile ocular-based authentication using
ResNet50 as the base model. Further, comparison with the knowledge distillation
suggests the efficacy of knowledge distillation over pruning methods in terms
of verification accuracy and the real-time inference measured as deep feature
extraction time on five mobile devices, namely, iPhone 6, iPhone X, iPhone XR,
iPad Air 2 and iPad 7th Generation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.04249v1,2021-10-08T16:58:57Z,2021-10-08T16:58:57Z,How Can AI Recognize Pain and Express Empathy,"Sensory and emotional experiences such as pain and empathy are relevant to
mental and physical health. The current drive for automated pain recognition is
motivated by a growing number of healthcare requirements and demands for social
interaction make it increasingly essential. Despite being a trending area, they
have not been explored in great detail. Over the past decades, behavioral
science and neuroscience have uncovered mechanisms that explain the
manifestations of pain. Recently, also artificial intelligence research has
allowed empathic machine learning methods to be approachable. Generally, the
purpose of this paper is to review the current developments for computational
pain recognition and artificial empathy implementation. Our discussion covers
the following topics: How can AI recognize pain from unimodality and
multimodality? Is it necessary for AI to be empathic? How can we create an AI
agent with proactive and reactive empathy? This article explores the challenges
and opportunities of real-world multimodal pain recognition from a
psychological, neuroscientific, and artificial intelligence perspective.
Finally, we identify possible future implementations of artificial empathy and
analyze how humans might benefit from an AI agent equipped with empathy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.04003v2,2021-12-06T15:10:15Z,2021-10-08T09:59:12Z,Learning to Centralize Dual-Arm Assembly,"Robotic manipulators are widely used in modern manufacturing processes.
However, their deployment in unstructured environments remains an open problem.
To deal with the variety, complexity, and uncertainty of real-world
manipulation tasks, it is essential to develop a flexible framework with
reduced assumptions on the environment characteristics. In recent years,
reinforcement learning (RL) has shown great results for single-arm robotic
manipulation. However, research focusing on dual-arm manipulation is still
rare. From a classical control perspective, solving such tasks often involves
complex modeling of interactions between two manipulators and the objects
encountered in the tasks, as well as the two robots coupling at a control
level. Instead, in this work, we explore the applicability of model-free RL to
dual-arm assembly. As we aim to contribute towards an approach that is not
limited to dual-arm assembly, but dual-arm manipulation in general, we keep
modeling efforts at a minimum. Hence, to avoid modeling the interaction between
the two robots and the used assembly tools, we present a modular approach with
two decentralized single-arm controllers which are coupled using a single
centralized learned policy. We reduce modeling effort to a minimum by using
sparse rewards only. Our architecture enables successful assembly and simple
transfer from simulation to the real world. We demonstrate the effectiveness of
the framework on dual-arm peg-in-hole and analyze sample efficiency and success
rates for different action spaces. Moreover, we compare results on different
clearances and showcase disturbance recovery and robustness, when dealing with
position uncertainties. Finally we zero-shot transfer policies trained in
simulation to the real world and evaluate their performance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.03979v2,2021-12-23T14:39:28Z,2021-10-08T08:58:36Z,"MilliTRACE-IR: Contact Tracing and Temperature Screening via mm-Wave and
  Infrared Sensing","Social distancing and temperature screening have been widely employed to
counteract the COVID-19 pandemic, sparking great interest from academia,
industry and public administrations worldwide. While most solutions have dealt
with these aspects separately, their combination would greatly benefit the
continuous monitoring of public spaces and help trigger effective
countermeasures. This work presents milliTRACE-IR, a joint mmWave radar and
infrared imaging sensing system performing unobtrusive and privacy preserving
human body temperature screening and contact tracing in indoor spaces.
milliTRACE-IR combines, via a robust sensor fusion approach, mmWave radars and
infrared thermal cameras. It achieves fully automated measurement of distancing
and body temperature, by jointly tracking the subjects's faces in the thermal
camera image plane and the human motion in the radar reference system.
Moreover, milliTRACE-IR performs contact tracing: a person with high body
temperature is reliably detected by the thermal camera sensor and subsequently
traced across a large indoor area in a non-invasive way by the radars. When
entering a new room, a subject is re-identified among several other individuals
by computing gait-related features from the radar reflections through a deep
neural network and using a weighted extreme learning machine as the final
re-identification tool. Experimental results, obtained from a real
implementation of milliTRACE-IR, demonstrate decimeter-level accuracy in
distance/trajectory estimation, inter-personal distance estimation (effective
for subjects getting as close as 0.2 m), and accurate temperature monitoring
(max. errors of 0.5{\deg}C). Furthermore, milliTRACE-IR provides contact
tracing through highly accurate (95%) person re-identification, in less than 20
seconds.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.05242v1,2021-10-08T06:35:20Z,2021-10-08T06:35:20Z,"Accelerating Multi-Objective Neural Architecture Search by Random-Weight
  Evaluation","For the goal of automated design of high-performance deep convolutional
neural networks (CNNs), Neural Architecture Search (NAS) methodology is
becoming increasingly important for both academia and industries.Due to the
costly stochastic gradient descent (SGD) training of CNNs for performance
evaluation, most existing NAS methods are computationally expensive for
real-world deployments. To address this issue, we first introduce a new
performance estimation metric, named Random-Weight Evaluation (RWE) to quantify
the quality of CNNs in a cost-efficient manner. Instead of fully training the
entire CNN, the RWE only trains its last layer and leaves the remainders with
randomly initialized weights, which results in a single network evaluation in
seconds.Second, a complexity metric is adopted for multi-objective NAS to
balance the model size and performance. Overall, our proposed method obtains a
set of efficient models with state-of-the-art performance in two real-world
search spaces. Then the results obtained on the CIFAR-10 dataset are
transferred to the ImageNet dataset to validate the practicality of the
proposed algorithm. Moreover, ablation studies on NAS-Bench-301 datasets reveal
the effectiveness of the proposed RWE in estimating the performance compared
with existing methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.02718v1,2021-10-06T13:05:45Z,2021-10-06T13:05:45Z,Generalizing Neural Networks by Reflecting Deviating Data in Production,"Trained with a sufficiently large training and testing dataset, Deep Neural
Networks (DNNs) are expected to generalize. However, inputs may deviate from
the training dataset distribution in real deployments. This is a fundamental
issue with using a finite dataset. Even worse, real inputs may change over time
from the expected distribution. Taken together, these issues may lead deployed
DNNs to mis-predict in production.
  In this work, we present a runtime approach that mitigates DNN
mis-predictions caused by the unexpected runtime inputs to the DNN. In contrast
to previous work that considers the structure and parameters of the DNN itself,
our approach treats the DNN as a blackbox and focuses on the inputs to the DNN.
Our approach has two steps. First, it recognizes and distinguishes ""unseen""
semantically-preserving inputs. For this we use a distribution analyzer based
on the distance metric learned by a Siamese network. Second, our approach
transforms those unexpected inputs into inputs from the training set that are
identified as having similar semantics. We call this process input reflection
and formulate it as a search problem over the embedding space on the training
set. This embedding space is learned by a Quadruplet network as an auxiliary
model for the subject model to improve the generalization.
  We implemented a tool called InputReflector based on the above two-step
approach and evaluated it with experiments on three DNN models trained on
CIFAR-10, MNIST, and FMINST image datasets. The results show that
InputReflector can effectively distinguish inputs that retain semantics of the
distribution (e.g., blurred, brightened, contrasted, and zoomed images) and
out-of-distribution inputs from normal inputs.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.05187v1,2021-10-06T06:19:30Z,2021-10-06T06:19:30Z,Clustering Plotted Data by Image Segmentation,"Clustering algorithms are one of the main analytical methods to detect
patterns in unlabeled data. Existing clustering methods typically treat samples
in a dataset as points in a metric space and compute distances to group
together similar points. In this paper, we present a wholly different way of
clustering points in 2-dimensional space, inspired by how humans cluster data:
by training neural networks to perform instance segmentation on plotted data.
Our approach, Visual Clustering, has several advantages over traditional
clustering algorithms: it is much faster than most existing clustering
algorithms (making it suitable for very large datasets), it agrees strongly
with human intuition for clusters, and it is by default hyperparameter free
(although additional steps with hyperparameters can be introduced for more
control of the algorithm). We describe the method and compare it to ten other
clustering methods on synthetic data to illustrate its advantages and
disadvantages. We then demonstrate how our approach can be extended to higher
dimensional data and illustrate its performance on real-world data. The
implementation of Visual Clustering is publicly available and can be applied to
any dataset in a few lines of code.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.04080v1,2021-10-03T10:52:19Z,2021-10-03T10:52:19Z,Landslide Detection in Real-Time Social Media Image Streams,"Lack of global data inventories obstructs scientific modeling of and response
to landslide hazards which are oftentimes deadly and costly. To remedy this
limitation, new approaches suggest solutions based on citizen science that
requires active participation. However, as a non-traditional data source,
social media has been increasingly used in many disaster response and
management studies in recent years. Inspired by this trend, we propose to
capitalize on social media data to mine landslide-related information
automatically with the help of artificial intelligence (AI) techniques.
Specifically, we develop a state-of-the-art computer vision model to detect
landslides in social media image streams in real time. To that end, we create a
large landslide image dataset labeled by experts and conduct extensive model
training experiments. The experimental results indicate that the proposed model
can be deployed in an online fashion to support global landslide susceptibility
maps and emergency response.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.00840v1,2021-10-02T16:52:28Z,2021-10-02T16:52:28Z,"Induction, Popper, and machine learning","Francis Bacon popularized the idea that science is based on a process of
induction by which repeated observations are, in some unspecified way,
generalized to theories based on the assumption that the future resembles the
past. This idea was criticized by Hume and others as untenable leading to the
famous problem of induction. It wasn't until the work of Karl Popper that this
problem was solved, by demonstrating that induction is not the basis for
science and that the development of scientific knowledge is instead based on
the same principles as biological evolution. Today, machine learning is also
taught as being rooted in induction from big data. Solomonoff induction
implemented in an idealized Bayesian agent (Hutter's AIXI) is widely discussed
and touted as a framework for understanding AI algorithms, even though
real-world attempts to implement something like AIXI immediately encounter
fatal problems. In this paper, we contrast frameworks based on induction with
Donald T. Campbell's universal Darwinism. We show that most AI algorithms in
use today can be understood as using an evolutionary trial and error process
searching over a solution space. In this work we argue that a universal
Darwinian framework provides a better foundation for understanding AI systems.
Moreover, at a more meta level the process of development of all AI algorithms
can be understood under the framework of universal Darwinism.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.00468v1,2021-10-01T15:03:03Z,2021-10-01T15:03:03Z,"New Evolutionary Computation Models and their Applications to Machine
  Learning","Automatic Programming is one of the most important areas of computer science
research today. Hardware speed and capability have increased exponentially, but
the software is years behind. The demand for software has also increased
significantly, but it is still written in old fashion: by using humans.
  There are multiple problems when the work is done by humans: cost, time,
quality. It is costly to pay humans, it is hard to keep them satisfied for a
long time, it takes a lot of time to teach and train them and the quality of
their output is in most cases low (in software, mostly due to bugs).
  The real advances in human civilization appeared during the industrial
revolutions. Before the first revolution, most people worked in agriculture.
Today, very few percent of people work in this field.
  A similar revolution must appear in the computer programming field.
Otherwise, we will have so many people working in this field as we had in the
past working in agriculture.
  How do people know how to write computer programs? Very simple: by learning.
Can we do the same for software? Can we put the software to learn how to write
software?
  It seems that is possible (to some degree) and the term is called Machine
Learning. It was first coined in 1959 by the first person who made a computer
perform a serious learning task, namely, Arthur Samuel.
  However, things are not so easy as in humans (well, truth to be said - for
some humans it is impossible to learn how to write software). So far we do not
have software that can learn perfectly to write software. We have some
particular cases where some programs do better than humans, but the examples
are sporadic at best. Learning from experience is difficult for computer
programs. Instead of trying to simulate how humans teach humans how to write
computer programs, we can simulate nature.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.00330v2,2022-01-26T11:45:33Z,2021-10-01T11:47:56Z,"Discovering Boundary Values of Feature-based Machine Learning
  Classifiers through Exploratory Datamorphic Testing","Testing has been widely recognised as difficult for AI applications. This
paper proposes a set of testing strategies for testing machine learning
applications in the framework of the datamorphism testing methodology. In these
strategies, testing aims at exploring the data space of a classification or
clustering application to discover the boundaries between classes that the
machine learning application defines. This enables the tester to understand
precisely the behaviour and function of the software under test. In the paper,
three variants of exploratory strategies are presented with the algorithms
implemented in the automated datamorphic testing tool Morphy. The correctness
of these algorithms are formally proved. Their capability and cost of
discovering borders between classes are evaluated via a set of controlled
experiments with manually designed subjects and a set of case studies with real
machine learning models.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2110.00218v2,2021-10-09T21:44:45Z,2021-10-01T05:19:32Z,"On the Importance of Gradients for Detecting Distributional Shifts in
  the Wild","Detecting out-of-distribution (OOD) data has become a critical component in
ensuring the safe deployment of machine learning models in the real world.
Existing OOD detection approaches primarily rely on the output or feature space
for deriving OOD scores, while largely overlooking information from the
gradient space. In this paper, we present GradNorm, a simple and effective
approach for detecting OOD inputs by utilizing information extracted from the
gradient space. GradNorm directly employs the vector norm of gradients,
backpropagated from the KL divergence between the softmax output and a uniform
probability distribution. Our key idea is that the magnitude of gradients is
higher for in-distribution (ID) data than that for OOD data, making it
informative for OOD detection. GradNorm demonstrates superior performance,
reducing the average FPR95 by up to 16.33% compared to the previous best
method.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.13570v2,2022-03-03T13:22:21Z,2021-09-28T09:00:55Z,"Adaptive Informative Path Planning Using Deep Reinforcement Learning for
  UAV-based Active Sensing","Aerial robots are increasingly being utilized for environmental monitoring
and exploration. However, a key challenge is efficiently planning paths to
maximize the information value of acquired data as an initially unknown
environment is explored. To address this, we propose a new approach for
informative path planning based on deep reinforcement learning (RL). Combining
recent advances in RL and robotic applications, our method combines tree search
with an offline-learned neural network predicting informative sensing actions.
We introduce several components making our approach applicable for robotic
tasks with high-dimensional state and large action spaces. By deploying the
trained network during a mission, our method enables sample-efficient online
replanning on platforms with limited computational resources. Simulations show
that our approach performs on par with existing methods while reducing runtime
by 8-10x. We validate its performance using real-world surface temperature
data.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.13521v2,2021-11-23T05:34:54Z,2021-09-28T06:49:40Z,"A multi-stage semi-supervised improved deep embedded clustering method
  for bearing fault diagnosis under the situation of insufficient labeled
  samples","Although data-driven fault diagnosis methods have been widely applied,
massive labeled data are required for model training. However, a difficulty of
implementing this in real industries hinders the application of these methods.
Hence, an effective diagnostic approach that can work well in such situation is
urgently needed.In this study, a multi-stage semi-supervised improved deep
embedded clustering (MS-SSIDEC) method, which combines semi-supervised learning
with improved deep embedded clustering (IDEC), is proposed to jointly explore
scarce labeled data and massive unlabeled data. In the first stage, a
skip-connection-based convolutional auto-encoder (SCCAE) that can automatically
map the unlabeled data into a low-dimensional feature space is proposed and
pre-trained to be a fault feature extractor. In the second stage, a
semi-supervised improved deep embedded clustering (SSIDEC) network is proposed
for clustering. It is first initialized with available labeled data and then
used to simultaneously optimize the clustering label assignment and make the
feature space to be more clustering-friendly. To tackle the phenomenon of
overfitting, virtual adversarial training (VAT) is introduced as a
regularization term in this stage. In the third stage, pseudo labels are
obtained by the high-quality results of SSIDEC. The labeled dataset can be
augmented by these pseudo-labeled data and then leveraged to train a bearing
fault diagnosis model. Two public datasets of vibration data from rolling
bearings are used to evaluate the performance of the proposed method.
Experimental results indicate that the proposed method achieves a promising
performance in both semi-supervised and unsupervised fault diagnosis tasks.
This method provides a new approach for fault diagnosis under the situation of
limited labeled samples by effectively exploring unsupervised data.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.13476v1,2021-09-28T04:21:07Z,2021-09-28T04:21:07Z,Fake News Detection using Semi-Supervised Graph Convolutional Network,"Social media becomes the central way for people to obtain and utilise news,
due to its rapidness and inexpensive value of data distribution. Though, such
features of social media platforms also present it a root cause of fake news
distribution, causing adverse consequences on both people and culture. Hence,
detecting fake news has become a significant research interest for bringing
feasible real time solutions to the problem. Most current techniques of fake
news disclosure are supervised, that need large cost in terms of time and
effort to make a certainly interpreted dataset. The proposed framework
concentrates on the text-based detection of fake news items while considering
that only limited number of labels are available. Graphs are functioned
extensively under several purposes of real-world problems on the strength of
their property to structure things easily. Deep neural networks are used to
generate great results within tasks that utilizes graph classification. The
Graph Convolution Network works as a deep learning paradigm which works on
graphs. Our proposed framework deals with limited amount of labelled data; we
go for a semi-supervised learning method. We come up with a semi-supervised
fake news detection technique based on GCN (Graph Convolutional Networks). The
recommended architecture comprises of three basic components: collecting word
embeddings from the news articles in datasets utilising GloVe, building
similarity graph using Word Movers Distance (WMD) and finally applying Graph
Convolution Network (GCN) for binary classification of news articles in
semi-supervised paradigm. The implemented technique is validated on three
different datasets by varying the volume of labelled data achieving 95.27 %
highest accuracy on Real or Fake dataset. Comparison with other contemporary
techniques also reinforced the supremacy of the proposed framework.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.10719v2,2021-09-27T10:56:50Z,2021-09-22T13:22:40Z,Autonomous Blimp Control using Deep Reinforcement Learning,"Aerial robot solutions are becoming ubiquitous for an increasing number of
tasks. Among the various types of aerial robots, blimps are very well suited to
perform long-duration tasks while being energy efficient, relatively silent and
safe. To address the blimp navigation and control task, in our recent work, we
have developed a software-in-the-loop simulation and a PID-based controller for
large blimps in the presence of wind disturbance. However, blimps have a
deformable structure and their dynamics are inherently non-linear and
time-delayed, often resulting in large trajectory tracking errors. Moreover,
the buoyancy of a blimp is constantly changing due to changes in the ambient
temperature and pressure. In the present paper, we explore a deep reinforcement
learning (DRL) approach to address these issues. We train only in simulation,
while keeping conditions as close as possible to the real-world scenario. We
derive a compact state representation to reduce the training time and a
discrete action space to enforce control smoothness. Our initial results in
simulation show a significant potential of DRL in solving the blimp control
task and robustness against moderate wind and parameter uncertainty. Extensive
experiments are presented to study the robustness of our approach. We also
openly provide the source code of our approach.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.09343v1,2021-09-20T07:54:09Z,2021-09-20T07:54:09Z,"Latexify Math: Mathematical Formula Markup Revision to Assist
  Collaborative Editing in Math Q&A Sites","Collaborative editing questions and answers plays an important role in
quality control of Mathematics Stack Exchange which is a math Q&A Site. Our
study of post edits in Mathematics Stack Exchange shows that there is a large
number of math-related edits about latexifying formulas, revising LaTeX and
converting the blurred math formula screenshots to LaTeX sequence. Despite its
importance, manually editing one math-related post especially those with
complex mathematical formulas is time-consuming and error-prone even for
experienced users. To assist post owners and editors to do this editing, we
have developed an edit-assistance tool, MathLatexEdit for formula
latexification, LaTeX revision and screenshot transcription. We formulate this
formula editing task as a translation problem, in which an original post is
translated to a revised post. MathLatexEdit implements a deep learning based
approach including two encoder-decoder models for textual and visual LaTeX edit
recommendation with math-specific inference. The two models are trained on
large-scale historical original-edited post pairs and synthesized
screenshot-formula pairs. Our evaluation of MathLatexEdit not only demonstrates
the accuracy of our model, but also the usefulness of MathLatexEdit in editing
real-world posts which are accepted in Mathematics Stack Exchange.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.08642v1,2021-09-17T16:52:03Z,2021-09-17T16:52:03Z,Efficient State Representation Learning for Dynamic Robotic Scenarios,"While the rapid progress of deep learning fuels end-to-end reinforcement
learning (RL), direct application, especially in high-dimensional space like
robotic scenarios still suffers from high sample efficiency. Therefore State
Representation Learning (SRL) is proposed to specifically learn to encode
task-relevant features from complex sensory data into low-dimensional states.
However, the pervasive implementation of SRL is usually conducted by a
decoupling strategy in which the observation-state mapping is learned
separately, which is prone to over-fit. To handle such problem, we present a
new algorithm called Policy Optimization via Abstract Representation which
integrates SRL into the original RL scale. Firstly, We engage RL loss to assist
in updating SRL model so that the states can evolve to meet the demand of
reinforcement learning and maintain a good physical interpretation. Secondly,
we introduce a dynamic parameter adjustment mechanism so that both models can
efficiently adapt to each other. Thirdly, we introduce a new prior called
domain resemblance to leverage expert demonstration to train the SRL model.
Finally, we provide a real-time access by state graph to monitor the course of
learning. Results show that our algorithm outperforms the PPO baselines and
decoupling strategies in terms of sample efficiency and final rewards. Thus our
model can efficiently deal with tasks in high dimensions and facilitate
training real-life robots directly from scratch.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.08267v2,2021-12-22T13:33:39Z,2021-09-17T01:02:27Z,"CompilerGym: Robust, Performant Compiler Optimization Environments for
  AI Research","Interest in applying Artificial Intelligence (AI) techniques to compiler
optimizations is increasing rapidly, but compiler research has a high entry
barrier. Unlike in other domains, compiler and AI researchers do not have
access to the datasets and frameworks that enable fast iteration and
development of ideas, and getting started requires a significant engineering
investment. What is needed is an easy, reusable experimental infrastructure for
real world compiler optimization tasks that can serve as a common benchmark for
comparing techniques, and as a platform to accelerate progress in the field.
  We introduce CompilerGym, a set of environments for real world compiler
optimization tasks, and a toolkit for exposing new optimization tasks to
compiler researchers. CompilerGym enables anyone to experiment on production
compiler optimization problems through an easy-to-use package, regardless of
their experience with compilers. We build upon the popular OpenAI Gym interface
enabling researchers to interact with compilers using Python and a familiar
API.
  We describe the CompilerGym architecture and implementation, characterize the
optimization spaces and computational efficiencies of three included compiler
environments, and provide extensive empirical evaluations. Compared to prior
works, CompilerGym offers larger datasets and optimization spaces, is 27x more
computationally efficient, is fault-tolerant, and capable of detecting
reproducibility bugs in the underlying compilers.
  In making it easy for anyone to experiment with compilers - irrespective of
their background - we aim to accelerate progress in the AI and compiler
research domains.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.06126v4,2022-07-21T15:48:43Z,2021-09-13T17:05:43Z,"Neural Network Guided Evolutionary Fuzzing for Finding Traffic
  Violations of Autonomous Vehicles","Self-driving cars and trucks, autonomous vehicles (AVs), should not be
accepted by regulatory bodies and the public until they have much higher
confidence in their safety and reliability -- which can most practically and
convincingly be achieved by testing. But existing testing methods are
inadequate for checking the end-to-end behaviors of AV controllers against
complex, real-world corner cases involving interactions with multiple
independent agents such as pedestrians and human-driven vehicles. While
test-driving AVs on streets and highways fails to capture many rare events,
existing simulation-based testing methods mainly focus on simple scenarios and
do not scale well for complex driving situations that require sophisticated
awareness of the surroundings. To address these limitations, we propose a new
fuzz testing technique, called AutoFuzz, which can leverage widely-used AV
simulators' API grammars to generate semantically and temporally valid complex
driving scenarios (sequences of scenes). To efficiently search for traffic
violations-inducing scenarios in a large search space, we propose a constrained
neural network (NN) evolutionary search method to optimize AutoFuzz. Evaluation
of our prototype on one state-of-the-art learning-based controller, two
rule-based controllers, and one industrial-grade controller in five scenarios
shows that AutoFuzz efficiently finds hundreds of traffic violations in
high-fidelity simulation environments. For each scenario, AutoFuzz can find on
average 10-39% more unique traffic violations than the best-performing baseline
method. Further, fine-tuning the learning-based controller with the traffic
violations found by AutoFuzz successfully reduced the traffic violations found
in the new version of the AV controller software.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.05821v1,2021-09-13T09:46:50Z,2021-09-13T09:46:50Z,Cyber-Security in the Emerging World of Smart Everything,"The fourth industrial revolution (4IR) is a revolution many authors believe
have come to stay. It is a revolution that has been fast blurring the line
between physical, digital and biological technologies. These disruptive
technologies largely rely on high-speed internet connectivity, Cloud
technologies, Augmented Reality, Additive Manufacturing, Data science and
Artificial Intelligence. Most developed economies have embraced the it while
the developing economies are struggling to adopt 4IR because they lack the
requisite skills, knowledge and technology. Thus, this study investigates
Nigeria as one of the developing economies to understand her readiness for 4IR
and the level of preparedness to mitigate the sophisticated cyber-attacks that
comes with it. The investigation adopted quantitative research approach and
developed an online questionnaire that was shared amongst the population of
interest that includes academic, industry experts and relevant stakeholders.
The questionnaire returned 116 valid responses which were analysed with
descriptive statistical tools in SPSS. Results suggest that 60 of the
respondents opined that Nigerian government at are not showing enough evidence
to demonstrate her preparedness to leverage these promised potentials by
developing 4IR relevant laws, strong institutional frameworks and policies.
They lack significant development capacity to mitigate risks associated with
digital ecosystem and cyber ecosystem that are ushered in by the 4IR. In the
universities, 52 of the courses offered at the undergraduate and 42 at the
post-graduate levels are relevant in the development of skills required in the
revolution. The study recommends that the government at all levels make
adequate efforts in developing the countrys intangible assets. In all, this
paper posits that successful implementation of these could equip Nigeria to
embrace the 4IR in all its aspects.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.13680v2,2021-09-07T01:18:16Z,2021-08-31T08:37:58Z,Learning Practically Feasible Policies for Online 3D Bin Packing,"We tackle the Online 3D Bin Packing Problem, a challenging yet practically
useful variant of the classical Bin Packing Problem. In this problem, the items
are delivered to the agent without informing the full sequence information.
Agent must directly pack these items into the target bin stably without
changing their arrival order, and no further adjustment is permitted. Online
3D-BPP can be naturally formulated as Markov Decision Process (MDP). We adopt
deep reinforcement learning, in particular, the on-policy actor-critic
framework, to solve this MDP with constrained action space. To learn a
practically feasible packing policy, we propose three critical designs. First,
we propose an online analysis of packing stability based on a novel stacking
tree. It attains a high analysis accuracy while reducing the computational
complexity from $O(N^2)$ to $O(N \log N)$, making it especially suited for RL
training. Second, we propose a decoupled packing policy learning for different
dimensions of placement which enables high-resolution spatial discretization
and hence high packing precision. Third, we introduce a reward function that
dictates the robot to place items in a far-to-near order and therefore
simplifies the collision avoidance in movement planning of the robotic arm.
Furthermore, we provide a comprehensive discussion on several key implemental
issues. The extensive evaluation demonstrates that our learned policy
outperforms the state-of-the-art methods significantly and is practically
usable for real-world applications.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.13268v3,2022-05-21T10:59:15Z,2021-08-30T14:29:57Z,Sensor-Based Navigation Using Hierarchical Reinforcement Learning,"Robotic systems are nowadays capable of solving complex navigation tasks.
However, their capabilities are limited to the knowledge of the designer and
consequently lack generalizability to initially unconsidered situations. This
makes deep reinforcement learning (DRL) especially interesting, as these
algorithms promise a self-learning system only relying on feedback from the
environment. In this paper, we consider the problem of lidar-based robot
navigation in continuous action space using DRL without providing any
goal-oriented or global information. By relying solely on local sensor data to
solve navigation tasks, we design an agent that assigns its own waypoints based
on intrinsic motivation. Our agent is able to learn goal-directed navigation
behavior even when facing only sparse feedback, i.e., delayed rewards when
reaching the target. To address this challenge and the complexity of the
continuous action space, we deploy a hierarchical agent structure in which the
exploration is distributed across multiple layers. Within the hierarchical
structure, our agent self-assigns internal goals and learns to extract
reasonable waypoints to reach the desired target position only based on local
sensor data. In our experiments, we demonstrate the navigation capabilities of
our agent in two environments and show that the hierarchical structure
seriously improves the performance in terms of success rate and success
weighted by path length in comparison to a flat structure. Furthermore, we
provide a real-robot experiment to illustrate that the trained agent can be
easily transferred to a real-world scenario.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.12704v1,2021-08-28T20:39:54Z,2021-08-28T20:39:54Z,"Compact representations of convolutional neural networks via weight
  pruning and quantization","The state-of-the-art performance for several real-world problems is currently
reached by convolutional neural networks (CNN). Such learning models exploit
recent results in the field of deep learning, typically leading to highly
performing, yet very large neural networks with (at least) millions of
parameters. As a result, the deployment of such models is not possible when
only small amounts of RAM are available, or in general within resource-limited
platforms, and strategies to compress CNNs became thus of paramount importance.
In this paper we propose a novel lossless storage format for CNNs based on
source coding and leveraging both weight pruning and quantization. We
theoretically derive the space upper bounds for the proposed structures,
showing their relationship with both sparsity and quantization levels of the
weight matrices. Both compression rates and excution times have been tested
against reference methods for matrix compression, and an empirical evaluation
of state-of-the-art quantization schemes based on weight sharing is also
discussed, to assess their impact on the performance when applied to both
convolutional and fully connected layers. On four benchmarks for classification
and regression problems and comparing to the baseline pre-trained uncompressed
network, we achieved a reduction of space occupancy up to 0.6% on fully
connected layers and 5.44% on the whole network, while performing at least as
competitive as the baseline.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.12430v1,2021-08-27T18:00:00Z,2021-08-27T18:00:00Z,"Hardware-accelerated Inference for Real-Time Gravitational-Wave
  Astronomy","The field of transient astronomy has seen a revolution with the first
gravitational-wave detections and the arrival of multi-messenger observations
they enabled. Transformed by the first detection of binary black hole and
binary neutron star mergers, computational demands in gravitational-wave
astronomy are expected to grow by at least a factor of two over the next five
years as the global network of kilometer-scale interferometers are brought to
design sensitivity. With the increase in detector sensitivity, real-time
delivery of gravitational-wave alerts will become increasingly important as an
enabler of multi-messenger followup. In this work, we report a novel
implementation and deployment of deep learning inference for real-time
gravitational-wave data denoising and astrophysical source identification. This
is accomplished using a generic Inference-as-a-Service model that is capable of
adapting to the future needs of gravitational-wave data analysis. Our
implementation allows seamless incorporation of hardware accelerators and also
enables the use of commercial or private (dedicated) as-a-service computing.
Based on our results, we propose a paradigm shift in low-latency and offline
computing in gravitational-wave astronomy. Such a shift can address key
challenges in peak-usage, scalability and reliability, and provide a data
analysis platform particularly optimized for deep learning applications. The
achieved sub-millisecond scale latency will also be relevant for any machine
learning-based real-time control systems that may be invoked in the operation
of near-future and next generation ground-based laser interferometers, as well
as the front-end collection, distribution and processing of data from such
instruments.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.11579v1,2021-08-26T05:00:27Z,2021-08-26T05:00:27Z,Modeling Item Response Theory with Stochastic Variational Inference,"Item Response Theory (IRT) is a ubiquitous model for understanding human
behaviors and attitudes based on their responses to questions. Large modern
datasets offer opportunities to capture more nuances in human behavior,
potentially improving psychometric modeling leading to improved scientific
understanding and public policy. However, while larger datasets allow for more
flexible approaches, many contemporary algorithms for fitting IRT models may
also have massive computational demands that forbid real-world application. To
address this bottleneck, we introduce a variational Bayesian inference
algorithm for IRT, and show that it is fast and scalable without sacrificing
accuracy. Applying this method to five large-scale item response datasets from
cognitive science and education yields higher log likelihoods and higher
accuracy in imputing missing data than alternative inference algorithms. Using
this new inference approach we then generalize IRT with expressive Bayesian
models of responses, leveraging recent advances in deep learning to capture
nonlinear item characteristic curves (ICC) with neural networks. Using an
eigth-grade mathematics test from TIMSS, we show our nonlinear IRT models can
capture interesting asymmetric ICCs. The algorithm implementation is
open-source, and easily usable.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.09862v1,2021-08-22T22:44:28Z,2021-08-22T22:44:28Z,"Explainable Machine Learning using Real, Synthetic and Augmented Fire
  Tests to Predict Fire Resistance and Spalling of RC Columns","This paper presents the development of systematic machine learning (ML)
approach to enable explainable and rapid assessment of fire resistance and
fire-induced spalling of reinforced concrete (RC) columns. The developed
approach comprises of an ensemble of three novel ML algorithms namely; random
forest (RF), extreme gradient boosted trees (ExGBT), and deep learning (DL).
These algorithms are trained to account for a wide collection of geometric
characteristics and material properties, as well as loading conditions to
examine fire performance of normal and high strength RC columns by analyzing a
comprehensive database of fire tests comprising of over 494 observations. The
developed ensemble is also capable of presenting quantifiable insights to ML
predictions; thus, breaking free from the notion of 'blackbox' ML and
establishing a solid step towards transparent and explainable ML. Most
importantly, this work tackles the scarcity of available fire tests by
proposing new techniques to leverage the use of real, synthetic and augmented
fire test observations. The developed ML ensemble has been calibrated and
validated for standard and design fire exposures and for one, two, three and
four-sided fire exposures thus; covering a wide range of practical scenarios
present during fire incidents. When fully deployed, the developed ensemble can
analyze over 5,000 RC columns in under 60 seconds thus, providing an attractive
solution for researchers and practitioners. The presented approach can also be
easily extended for evaluating fire resistance and spalling of other structural
members and under varying fire scenarios and loading conditions and hence paves
the way to modernize the state of this research area and practice.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.08631v2,2021-08-22T04:44:25Z,2021-08-19T11:51:36Z,"Determinant-free fermionic wave function using feed-forward neural
  networks","We propose a general framework for finding the ground state of many-body
fermionic systems by using feed-forward neural networks. The anticommutation
relation for fermions is usually implemented to a variational wave function by
the Slater determinant (or Pfaffian), which is a computational bottleneck
because of the numerical cost of $O(N^3)$ for $N$ particles. We bypass this
bottleneck by explicitly calculating the sign changes associated with particle
exchanges in real space and using fully connected neural networks for
optimizing the rest parts of the wave function. This reduces the computational
cost to $O(N^2)$ or less. We show that the accuracy of the approximation can be
improved by optimizing the ""variance"" of the energy simultaneously with the
energy itself. We also find that a reweighting method in Monte Carlo sampling
can stabilize the calculation. These improvements can be applied to other
approaches based on variational Monte Carlo methods. Moreover, we show that the
accuracy can be further improved by using the symmetry of the system, the
representative states, and an additional neural network implementing a
generalized Gutzwiller-Jastrow factor. We demonstrate the efficiency of the
method by applying it to a two-dimensional Hubbard model.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.13475v1,2021-08-18T13:39:50Z,2021-08-18T13:39:50Z,"An Analysis Of Entire Space Multi-Task Models For Post-Click Conversion
  Prediction","Industrial recommender systems are frequently tasked with approximating
probabilities for multiple, often closely related, user actions. For example,
predicting if a user will click on an advertisement and if they will then
purchase the advertised product. The conceptual similarity between these tasks
has promoted the use of multi-task learning: a class of algorithms that aim to
bring positive inductive transfer from related tasks. Here, we empirically
evaluate multi-task learning approaches with neural networks for an online
advertising task. Specifically, we consider approximating the probability of
post-click conversion events (installs) (CVR) for mobile app advertising on a
large-scale advertising platform, using the related click events (CTR) as an
auxiliary task. We use an ablation approach to systematically study recent
approaches that incorporate both multitask learning and ""entire space modeling""
which train the CVR on all logged examples rather than learning a conditional
likelihood of conversion given clicked. Based on these results we show that
several different approaches result in similar levels of positive transfer from
the data-abundant CTR task to the CVR task and offer some insight into how the
multi-task design choices address the two primary problems affecting the CVR
task: data sparsity and data bias. Our findings add to the growing body of
evidence suggesting that standard multi-task learning is a sensible approach to
modelling related events in real-world large-scale applications and suggest the
specific multitask approach can be guided by ease of implementation in an
existing system.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2109.00889v1,2021-08-17T00:35:43Z,2021-08-17T00:35:43Z,"Dealing with Distribution Mismatch in Semi-supervised Deep Learning for
  Covid-19 Detection Using Chest X-ray Images: A Novel Approach Using Feature
  Densities","In the context of the global coronavirus pandemic, different deep learning
solutions for infected subject detection using chest X-ray images have been
proposed. However, deep learning models usually need large labelled datasets to
be effective. Semi-supervised deep learning is an attractive alternative, where
unlabelled data is leveraged to improve the overall model's accuracy. However,
in real-world usage settings, an unlabelled dataset might present a different
distribution than the labelled dataset (i.e. the labelled dataset was sampled
from a target clinic and the unlabelled dataset from a source clinic). This
results in a distribution mismatch between the unlabelled and labelled
datasets. In this work, we assess the impact of the distribution mismatch
between the labelled and the unlabelled datasets, for a semi-supervised model
trained with chest X-ray images, for COVID-19 detection. Under strong
distribution mismatch conditions, we found an accuracy hit of almost 30\%,
suggesting that the unlabelled dataset distribution has a strong influence in
the behaviour of the model. Therefore, we propose a straightforward approach to
diminish the impact of such distribution mismatch. Our proposed method uses a
density approximation of the feature space. It is built upon the target dataset
to filter out the observations in the source unlabelled dataset that might harm
the accuracy of the semi-supervised model. It assumes that a small labelled
source dataset is available together with a larger source unlabelled dataset.
Our proposed method does not require any model training, it is simple and
computationally cheap. We compare our proposed method against two popular state
of the art out-of-distribution data detectors, which are also cheap and simple
to implement. In our tests, our method yielded accuracy gains of up to 32\%,
when compared to the previous state of the art methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.03713v1,2021-08-08T19:12:04Z,2021-08-08T19:12:04Z,"On the Difficulty of Generalizing Reinforcement Learning Framework for
  Combinatorial Optimization","Combinatorial optimization problems (COPs) on the graph with real-life
applications are canonical challenges in Computer Science. The difficulty of
finding quality labels for problem instances holds back leveraging supervised
learning across combinatorial problems. Reinforcement learning (RL) algorithms
have recently been adopted to solve this challenge automatically. The
underlying principle of this approach is to deploy a graph neural network (GNN)
for encoding both the local information of the nodes and the graph-structured
data in order to capture the current state of the environment. Then, it is
followed by the actor to learn the problem-specific heuristics on its own and
make an informed decision at each state for finally reaching a good solution.
Recent studies on this subject mainly focus on a family of combinatorial
problems on the graph, such as the travel salesman problem, where the proposed
model aims to find an ordering of vertices that optimizes a given objective
function. We use the security-aware phone clone allocation in the cloud as a
classical quadratic assignment problem (QAP) to investigate whether or not deep
RL-based model is generally applicable to solve other classes of such hard
problems. Extensive empirical evaluation shows that existing RL-based model may
not generalize to QAP.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.05713v2,2022-06-02T15:22:32Z,2021-08-08T11:29:16Z,Towards real-world navigation with deep differentiable planners,"We train embodied neural networks to plan and navigate unseen complex 3D
environments, emphasising real-world deployment. Rather than requiring prior
knowledge of the agent or environment, the planner learns to model the state
transitions and rewards. To avoid the potentially hazardous trial-and-error of
reinforcement learning, we focus on differentiable planners such as Value
Iteration Networks (VIN), which are trained offline from safe expert
demonstrations. Although they work well in small simulations, we address two
major limitations that hinder their deployment. First, we observed that current
differentiable planners struggle to plan long-term in environments with a high
branching complexity. While they should ideally learn to assign low rewards to
obstacles to avoid collisions, we posit that the constraints imposed on the
network are not strong enough to guarantee the network to learn sufficiently
large penalties for every possible collision. We thus impose a structural
constraint on the value iteration, which explicitly learns to model any
impossible actions. Secondly, we extend the model to work with a limited
perspective camera under translation and rotation, which is crucial for real
robot deployment. Many VIN-like planners assume a 360 degrees or overhead view
without rotation. In contrast, our method uses a memory-efficient lattice map
to aggregate CNN embeddings of partial observations, and models the rotational
dynamics explicitly using a 3D state-space grid (translation and rotation). Our
proposals significantly improve semantic navigation and exploration on several
2D and 3D environments, succeeding in settings that are otherwise challenging
for this class of methods. As far as we know, we are the first to successfully
perform differentiable planning on the difficult Active Vision Dataset,
consisting of real images captured from a robot.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.03173v1,2021-08-04T09:03:53Z,2021-08-04T09:03:53Z,"Incremental learning of LSTM framework for sensor fusion in attitude
  estimation","This paper presents a novel method for attitude estimation of an object in 3D
space by incremental learning of the Long-Short Term Memory (LSTM) network.
Gyroscope, accelerometer, and magnetometer are few widely used sensors in
attitude estimation applications. Traditionally, multi-sensor fusion methods
such as the Extended Kalman Filter and Complementary Filter are employed to
fuse the measurements from these sensors. However, these methods exhibit
limitations in accounting for the uncertainty, unpredictability, and dynamic
nature of the motion in real-world situations. In this paper, the inertial
sensors data are fed to the LSTM network which are then updated incrementally
to incorporate the dynamic changes in motion occurring in the run time. The
robustness and efficiency of the proposed framework is demonstrated on the
dataset collected from a commercially available inertial measurement unit. The
proposed framework offers a significant improvement in the results compared to
the traditional method, even in the case of a highly dynamic environment. The
LSTM framework-based attitude estimation approach can be deployed on a standard
AI-supported processing module for real-time applications.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2108.01846v2,2022-03-11T04:38:21Z,2021-08-04T04:59:05Z,"Learning Barrier Certificates: Towards Safe Reinforcement Learning with
  Zero Training-time Violations","Training-time safety violations have been a major concern when we deploy
reinforcement learning algorithms in the real world. This paper explores the
possibility of safe RL algorithms with zero training-time safety violations in
the challenging setting where we are only given a safe but trivial-reward
initial policy without any prior knowledge of the dynamics model and additional
offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe
RL (CRABS), which iteratively learns barrier certificates, dynamics models, and
policies. The barrier certificates, learned via adversarial training, ensure
the policy's safety assuming calibrated learned dynamics model. We also add a
regularization term to encourage larger certified regions to enable better
exploration. Empirical simulations show that zero safety violations are already
challenging for a suite of simple environments with only 2-4 dimensional state
space, especially if high-reward policies have to visit regions near the safety
boundary. Prior methods require hundreds of violations to achieve decent
rewards on these tasks, whereas our proposed algorithms incur zero violations.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.14796v1,2021-07-30T17:54:44Z,2021-07-30T17:54:44Z,Data-driven modeling of time-domain induced polarization,"We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.13480v1,2021-07-28T16:42:09Z,2021-07-28T16:42:09Z,Survival stacking: casting survival analysis as a classification problem,"While there are many well-developed data science methods for classification
and regression, there are relatively few methods for working with
right-censored data. Here, we present ""survival stacking"": a method for casting
survival analysis problems as classification problems, thereby allowing the use
of general classification methods and software in a survival setting. Inspired
by the Cox partial likelihood, survival stacking collects features and outcomes
of survival data in a large data frame with a binary outcome. We show that
survival stacking with logistic regression is approximately equivalent to the
Cox proportional hazards model. We further recommend methods for evaluating
model performance in the survival stacked setting, and we illustrate survival
stacking on real and simulated data. By reframing survival problems as
classification problems, we make it possible for data scientists to use
well-known learning algorithms (including random forests, gradient boosting
machines and neural networks) in a survival setting, and lower the barrier for
flexible survival modeling.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.13473v3,2022-06-15T22:52:09Z,2021-07-28T16:29:58Z,"The Portiloop: a deep learning-based open science tool for closed-loop
  brain stimulation","Closed-loop brain stimulation refers to capturing neurophysiological measures
such as electroencephalography (EEG), quickly identifying neural events of
interest, and producing auditory, magnetic or electrical stimulation so as to
interact with brain processes precisely. It is a promising new method for
fundamental neuroscience and perhaps for clinical applications such as
restoring degraded memory function; however, existing tools are expensive,
cumbersome, and offer limited experimental flexibility. In this article, we
propose the Portiloop, a deep learning-based, portable and low-cost closed-loop
stimulation system able to target specific brain oscillations. We first
document open-hardware implementations that can be constructed from
commercially available components. We also provide a fast, lightweight neural
network model and an exploration algorithm that automatically optimizes the
model hyperparameters to the desired brain oscillation. Finally, we validate
the technology on a challenging test case of real-time sleep spindle detection,
with results comparable to off-line expert performance on the Massive Online
Data Annotation spindle dataset (MODA; group consensus). Software and plans are
available to the community as an open science initiative to encourage further
development and advance closed-loop neuroscience research.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.11750v2,2021-07-30T08:42:18Z,2021-07-25T07:52:53Z,"Improving Variational Autoencoder based Out-of-Distribution Detection
  for Embedded Real-time Applications","Uncertainties in machine learning are a significant roadblock for its
application in safety-critical cyber-physical systems (CPS). One source of
uncertainty arises from distribution shifts in the input data between training
and test scenarios. Detecting such distribution shifts in real-time is an
emerging approach to address the challenge. The high dimensional input space in
CPS applications involving imaging adds extra difficulty to the task.
Generative learning models are widely adopted for the task, namely
out-of-distribution (OoD) detection. To improve the state-of-the-art, we
studied existing proposals from both machine learning and CPS fields. In the
latter, safety monitoring in real-time for autonomous driving agents has been a
focus. Exploiting the spatiotemporal correlation of motion in videos, we can
robustly detect hazardous motion around autonomous driving agents. Inspired by
the latest advances in the Variational Autoencoder (VAE) theory and practice,
we tapped into the prior knowledge in data to further boost OoD detection's
robustness. Comparison studies over nuScenes and Synthia data sets show our
methods significantly improve detection capabilities of OoD factors unique to
driving scenarios, 42% better than state-of-the-art approaches. Our model also
generalized near-perfectly, 97% better than the state-of-the-art across the
real-world and simulation driving data sets experimented. Finally, we
customized one proposed method into a twin-encoder model that can be deployed
to resource limited embedded devices for real-time OoD detection. Its execution
time was reduced over four times in low-precision 8-bit integer inference,
while detection capability is comparable to its corresponding floating-point
model.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.11397v1,2021-07-23T18:00:15Z,2021-07-23T18:00:15Z,"Artificial Neural Networks for Galaxy Clustering. Learning from the
  two-point correlation function of BOSS galaxies","The increasingly large amount of cosmological data coming from ground-based
and space-borne telescopes requires highly efficient and fast enough data
analysis techniques to maximise the scientific exploitation. In this work, we
explore the capabilities of supervised machine learning algorithms to learn the
properties of the large-scale structure of the Universe, aiming at constraining
the matter density parameter, Omega m. We implement a new Artificial Neural
Network for a regression data analysis, and train it on a large set of galaxy
two-point correlation functions in standard cosmologies with different values
of Omega m. The training set is constructed from log-normal mock catalogues
which reproduce the clustering of the Baryon Oscillation Spectroscopic Survey
(BOSS) galaxies. The presented statistical method requires no specific
analytical model to construct the likelihood function, and runs with negligible
computational cost, after training. We test this new Artificial Neural Network
on real BOSS data, finding Omega m=0.309p/m0.008, which is remarkably
consistent with standard analysis results.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.11003v1,2021-07-23T02:41:51Z,2021-07-23T02:41:51Z,"Model Selection for Offline Reinforcement Learning: Practical
  Considerations for Healthcare Settings","Reinforcement learning (RL) can be used to learn treatment policies and aid
decision making in healthcare. However, given the need for generalization over
complex state/action spaces, the incorporation of function approximators (e.g.,
deep neural networks) requires model selection to reduce overfitting and
improve policy performance at deployment. Yet a standard validation pipeline
for model selection requires running a learned policy in the actual
environment, which is often infeasible in a healthcare setting. In this work,
we investigate a model selection pipeline for offline RL that relies on
off-policy evaluation (OPE) as a proxy for validation performance. We present
an in-depth analysis of popular OPE methods, highlighting the additional
hyperparameters and computational requirements (fitting/inference of auxiliary
models) when used to rank a set of candidate policies. We compare the utility
of different OPE methods as part of the model selection pipeline in the context
of learning to treat patients with sepsis. Among all the OPE methods we
considered, fitted Q evaluation (FQE) consistently leads to the best validation
ranking, but at a high computational cost. To balance this trade-off between
accuracy of ranking and computational efficiency, we propose a simple two-stage
approach to accelerate model selection by avoiding potentially unnecessary
computation. Our work serves as a practical guide for offline RL model
selection and can help RL practitioners select policies using real-world
datasets. To facilitate reproducibility and future extensions, the code
accompanying this paper is available online at
https://github.com/MLD3/OfflineRL_ModelSelection.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.09822v2,2021-07-22T05:48:12Z,2021-07-21T00:43:32Z,"Bayesian Controller Fusion: Leveraging Control Priors in Deep
  Reinforcement Learning for Robotics","We present Bayesian Controller Fusion (BCF): a hybrid control strategy that
combines the strengths of traditional hand-crafted controllers and model-free
deep reinforcement learning (RL). BCF thrives in the robotics domain, where
reliable but suboptimal control priors exist for many tasks, but RL from
scratch remains unsafe and data-inefficient. By fusing uncertainty-aware
distributional outputs from each system, BCF arbitrates control between them,
exploiting their respective strengths. We study BCF on two real-world robotics
tasks involving navigation in a vast and long-horizon environment, and a
complex reaching task that involves manipulability maximisation. For both these
domains, there exist simple handcrafted controllers that can solve the task at
hand in a risk-averse manner but do not necessarily exhibit the optimal
solution given limitations in analytical modelling, controller miscalibration
and task variation. As exploration is naturally guided by the prior in the
early stages of training, BCF accelerates learning, while substantially
improving beyond the performance of the control prior, as the policy gains more
experience. More importantly, given the risk-aversity of the control prior, BCF
ensures safe exploration and deployment, where the control prior naturally
dominates the action distribution in states unknown to the policy. We
additionally show BCF's applicability to the zero-shot sim-to-real setting and
its ability to deal with out-of-distribution states in the real-world. BCF is a
promising approach for combining the complementary strengths of deep RL and
traditional robotic control, surpassing what either can achieve independently.
The code and supplementary video material are made publicly available at
https://krishanrana.github.io/bcf.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.08706v1,2021-07-19T09:30:52Z,2021-07-19T09:30:52Z,"Uncertainty-aware Cardinality Estimation by Neural Network Gaussian
  Process","Deep Learning (DL) has achieved great success in many real applications.
Despite its success, there are some main problems when deploying advanced DL
models in database systems, such as hyper-parameters tuning, the risk of
overfitting, and lack of prediction uncertainty. In this paper, we study
cardinality estimation for SQL queries with a focus on uncertainty, which we
believe is important in database systems when dealing with a large number of
user queries on various applications. With uncertainty ensured, instead of
trusting an estimator learned as it is, a query optimizer can explore other
options when the estimator learned has a large variance, and it also becomes
possible to update the estimator to improve its prediction in areas with high
uncertainty. The approach we explore is different from the direction of
deploying sophisticated DL models in database systems to build cardinality
estimators. We employ Bayesian deep learning (BDL), which serves as a bridge
between Bayesian inference and deep learning.The prediction distribution by BDL
provides principled uncertainty calibration for the prediction. In addition,
when the network width of a BDL model goes to infinity, the model performs
equivalent to Gaussian Process (GP). This special class of BDL, known as Neural
Network Gaussian Process (NNGP), inherits the advantages of Bayesian approach
while keeping universal approximation of neural network, and can utilize a much
larger model space to model distribution-free data as a nonparametric model. We
show that our uncertainty-aware NNGP estimator achieves high accuracy, can be
built very fast, and is robust to query workload shift, in our extensive
performance studies by comparing with the existing approaches.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.08114v2,2021-07-26T17:00:47Z,2021-07-16T20:49:30Z,"Decentralized Multi-Agent Reinforcement Learning for Task Offloading
  Under Uncertainty","Multi-Agent Reinforcement Learning (MARL) is a challenging subarea of
Reinforcement Learning due to the non-stationarity of the environments and the
large dimensionality of the combined action space. Deep MARL algorithms have
been applied to solve different task offloading problems. However, in
real-world applications, information required by the agents (i.e. rewards and
states) are subject to noise and alterations. The stability and the robustness
of deep MARL to practical challenges is still an open research problem. In this
work, we apply state-of-the art MARL algorithms to solve task offloading with
reward uncertainty. We show that perturbations in the reward signal can induce
decrease in the performance compared to learning with perfect rewards. We
expect this paper to stimulate more research in studying and addressing the
practical challenges of deploying deep MARL solutions in wireless
communications systems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.07502v2,2021-11-10T07:31:56Z,2021-07-15T17:54:36Z,MultiBench: Multiscale Benchmarks for Multimodal Representation Learning,"Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.06882v1,2021-07-14T17:55:28Z,2021-07-14T17:55:28Z,"Conservative Objective Models for Effective Offline Model-Based
  Optimization","Computational design problems arise in a number of settings, from synthetic
biology to computer architectures. In this paper, we aim to solve data-driven
model-based optimization (MBO) problems, where the goal is to find a design
input that maximizes an unknown objective function provided access to only a
static dataset of prior experiments. Such data-driven optimization procedures
are the only practical methods in many real-world domains where active data
collection is expensive (e.g., when optimizing over proteins) or dangerous
(e.g., when optimizing over aircraft designs). Typical methods for MBO that
optimize the design against a learned model suffer from distributional shift:
it is easy to find a design that ""fools"" the model into predicting a high
value. To overcome this, we propose conservative objective models (COMs), a
method that learns a model of the objective function that lower bounds the
actual value of the ground-truth objective on out-of-distribution inputs, and
uses it for optimization. Structurally, COMs resemble adversarial training
methods used to overcome adversarial examples. COMs are simple to implement and
outperform a number of existing methods on a wide range of MBO problems,
including optimizing protein sequences, robot morphologies, neural network
weights, and superconducting materials.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.04457v2,2021-11-16T07:44:54Z,2021-07-09T14:23:01Z,"Aligning an optical interferometer with beam divergence control and
  continuous action space","Reinforcement learning is finding its way to real-world problem application,
transferring from simulated environments to physical setups. In this work, we
implement vision-based alignment of an optical Mach-Zehnder interferometer with
a confocal telescope in one arm, which controls the diameter and divergence of
the corresponding beam. We use a continuous action space; exponential scaling
enables us to handle actions within a range of over two orders of magnitude.
Our agent trains only in a simulated environment with domain randomizations. In
an experimental evaluation, the agent significantly outperforms an existing
solution and a human expert.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.01830v1,2021-07-05T07:37:24Z,2021-07-05T07:37:24Z,ARM-Net: Adaptive Relation Modeling Network for Structured Data,"Relational databases are the de facto standard for storing and querying
structured data, and extracting insights from structured data requires advanced
analytics. Deep neural networks (DNNs) have achieved super-human prediction
performance in particular data types, e.g., images. However, existing DNNs may
not produce meaningful results when applied to structured data. The reason is
that there are correlations and dependencies across combinations of attribute
values in a table, and these do not follow simple additive patterns that can be
easily mimicked by a DNN. The number of possible such cross features is
combinatorial, making them computationally prohibitive to model. Furthermore,
the deployment of learning models in real-world applications has also
highlighted the need for interpretability, especially for high-stakes
applications, which remains another issue of concern to DNNs.
  In this paper, we present ARM-Net, an adaptive relation modeling network
tailored for structured data, and a lightweight framework ARMOR based on
ARM-Net for relational data analytics. The key idea is to model feature
interactions with cross features selectively and dynamically, by first
transforming the input features into exponential space, and then determining
the interaction order and interaction weights adaptively for each cross
feature. We propose a novel sparse attention mechanism to dynamically generate
the interaction weights given the input tuple, so that we can explicitly model
cross features of arbitrary orders with noisy features filtered selectively.
Then during model inference, ARM-Net can specify the cross features being used
for each prediction for higher accuracy and better interpretability. Our
extensive experiments on real-world datasets demonstrate that ARM-Net
consistently outperforms existing models and provides more interpretable
predictions for data-driven decision making.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.01427v1,2021-07-03T13:03:55Z,2021-07-03T13:03:55Z,Multi-Objective Congestion Control,"Decades of research on Internet congestion control (CC) has produced a
plethora of algorithms that optimize for different performance objectives.
Applications face the challenge of choosing the most suitable algorithm based
on their needs, and it takes tremendous efforts and expertise to customize CC
algorithms when new demands emerge. In this paper, we explore a basic question:
can we design a single CC algorithm to satisfy different objectives? We propose
MOCC, the first multi-objective congestion control algorithm that attempts to
address this challenge. The core of MOCC is a novel multi-objective
reinforcement learning framework for CC that can automatically learn the
correlations between different application requirements and the corresponding
optimal control policies. Under this framework, MOCC further applies transfer
learning to transfer the knowledge from past experience to new applications,
quickly adapting itself to a new objective even if it is unforeseen. We provide
both user-space and kernel-space implementation of MOCC. Real-world experiments
and extensive simulations show that MOCC well supports multi-objective,
competing or outperforming the best existing CC algorithms on individual
objectives, and quickly adapting to new applications (e.g., 14.2x faster than
prior work) without compromising old ones.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.14739v1,2021-06-28T14:11:48Z,2021-06-28T14:11:48Z,"Real-Time Human Pose Estimation on a Smart Walker using Convolutional
  Neural Networks","Rehabilitation is important to improve quality of life for mobility-impaired
patients. Smart walkers are a commonly used solution that should embed
automatic and objective tools for data-driven human-in-the-loop control and
monitoring. However, present solutions focus on extracting few specific metrics
from dedicated sensors with no unified full-body approach. We investigate a
general, real-time, full-body pose estimation framework based on two RGB+D
camera streams with non-overlapping views mounted on a smart walker equipment
used in rehabilitation. Human keypoint estimation is performed using a
two-stage neural network framework. The 2D-Stage implements a detection module
that locates body keypoints in the 2D image frames. The 3D-Stage implements a
regression module that lifts and relates the detected keypoints in both cameras
to the 3D space relative to the walker. Model predictions are low-pass filtered
to improve temporal consistency. A custom acquisition method was used to obtain
a dataset, with 14 healthy subjects, used for training and evaluating the
proposed framework offline, which was then deployed on the real walker
equipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage
and 44.05mm for the 3D-Stage were reported, with an inference time of 26.6ms
when deployed on the constrained hardware of the walker. We present a novel
approach to patient monitoring and data-driven human-in-the-loop control in the
context of smart walkers. It is able to extract a complete and compact body
representation in real-time and from inexpensive sensors, serving as a common
base for downstream metrics extraction solutions, and Human-Robot interaction
applications. Despite promising results, more data should be collected on users
with impairments, to assess its performance as a rehabilitation tool in
real-world scenarios.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.01019v1,2021-06-25T20:24:41Z,2021-06-25T20:24:41Z,"Toward 6G: From New Hardware Design to Wireless Semantic and
  Goal-Oriented Communication Paradigms","Several speculative visions are conjecturing on what 6G services will be able
to offer at the horizon of 2030. Nevertheless, the 6G design process is at its
preliminary stages. The reality today is that hardware, technologies and new
materials required to effectively meet the unprecedented performance targets
required for future 6G services and network operation, have not been designed,
tested or even do not exist yet. Today, a solid vision on the cost-benefit
trade-offs of machine learning and artificial intelligence support for 6G
network and services operation optimization is missing. This includes the
possible support from hardware efficiency, operation effectiveness and, the
immeasurable cost due to data acquisition-transfer-processing. The contribution
of this paper is three-fold. This is the first paper deriving crucial 6G key
performance indicators on hardware and technology design. Second, we present a
new hardware technologies design methodology conceived to enable the effective
software-hardware components integration required to meet the challenging
performance envisioned for future 6G networks. Third, we suggest a paradigm
shift towards goal-oriented and semantic communications, in which a totally new
opportunity of joint design of hardware, artificial intelligence and effective
communication is offered. The proposed vision is consolidated by our recent
results on hardware, technology and machine learning performance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.13842v2,2021-06-29T01:27:49Z,2021-06-25T18:43:23Z,"EARLIN: Early Out-of-Distribution Detection for Resource-efficient
  Collaborative Inference","Collaborative inference enables resource-constrained edge devices to make
inferences by uploading inputs (e.g., images) to a server (i.e., cloud) where
the heavy deep learning models run. While this setup works cost-effectively for
successful inferences, it severely underperforms when the model faces input
samples on which the model was not trained (known as Out-of-Distribution (OOD)
samples). If the edge devices could, at least, detect that an input sample is
an OOD, that could potentially save communication and computation resources by
not uploading those inputs to the server for inference workload. In this paper,
we propose a novel lightweight OOD detection approach that mines important
features from the shallow layers of a pretrained CNN model and detects an input
sample as ID (In-Distribution) or OOD based on a distance function defined on
the reduced feature space. Our technique (a) works on pretrained models without
any retraining of those models, and (b) does not expose itself to any OOD
dataset (all detection parameters are obtained from the ID training dataset).
To this end, we develop EARLIN (EARLy OOD detection for Collaborative
INference) that takes a pretrained model and partitions the model at the OOD
detection layer and deploys the considerably small OOD part on an edge device
and the rest on the cloud. By experimenting using real datasets and a prototype
implementation, we show that our technique achieves better results than other
approaches in terms of overall accuracy and cost when tested against popular
OOD datasets on top of popular deep learning models pretrained on benchmark
datasets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.13219v1,2021-06-24T17:52:43Z,2021-06-24T17:52:43Z,Towards Understanding and Mitigating Social Biases in Language Models,"As machine learning methods are deployed in real-world settings such as
healthcare, legal systems, and social science, it is crucial to recognize how
they shape social biases and stereotypes in these sensitive decision-making
processes. Among such real-world deployments are large-scale pretrained
language models (LMs) that can be potentially dangerous in manifesting
undesirable representational biases - harmful biases resulting from
stereotyping that propagate negative generalizations involving gender, race,
religion, and other social constructs. As a step towards improving the fairness
of LMs, we carefully define several sources of representational biases before
proposing new benchmarks and metrics to measure them. With these tools, we
propose steps towards mitigating social biases during text generation. Our
empirical results and human evaluation demonstrate effectiveness in mitigating
bias while retaining crucial contextual information for high-fidelity text
generation, thereby pushing forward the performance-fairness Pareto frontier.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.12605v1,2021-06-23T18:08:07Z,2021-06-23T18:08:07Z,Deep Fake Detection: Survey of Facial Manipulation Detection Solutions,"Deep Learning as a field has been successfully used to solve a plethora of
complex problems, the likes of which we could not have imagined a few decades
back. But as many benefits as it brings, there are still ways in which it can
be used to bring harm to our society. Deep fakes have been proven to be one
such problem, and now more than ever, when any individual can create a fake
image or video simply using an application on the smartphone, there need to be
some countermeasures, with which we can detect if the image or video is a fake
or real and dispose of the problem threatening the trustworthiness of online
information. Although the Deep fakes created by neural networks, may seem to be
as real as a real image or video, it still leaves behind spatial and temporal
traces or signatures after moderation, these signatures while being invisible
to a human eye can be detected with the help of a neural network trained to
specialize in Deep fake detection. In this paper, we analyze several such
states of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception
Net) and compare them against each other, to find an optimal solution for
various scenarios like real-time deep fake detection to be deployed in online
social media platforms where the classification should be made as fast as
possible or for a small news agency where the classification need not be in
real-time but requires utmost accuracy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.12372v2,2021-06-25T08:09:48Z,2021-06-23T13:09:58Z,Real-time Neural Radiance Caching for Path Tracing,"We present a real-time neural radiance caching method for path-traced global
illumination. Our system is designed to handle fully dynamic scenes, and makes
no assumptions about the lighting, geometry, and materials. The data-driven
nature of our approach sidesteps many difficulties of caching algorithms, such
as locating, interpolating, and updating cache points. Since pretraining neural
networks to handle novel, dynamic scenes is a formidable generalization
challenge, we do away with pretraining and instead achieve generalization via
adaptation, i.e. we opt for training the radiance cache while rendering. We
employ self-training to provide low-noise training targets and simulate
infinite-bounce transport by merely iterating few-bounce training updates. The
updates and cache queries incur a mild overhead -- about 2.6ms on full HD
resolution -- thanks to a streaming implementation of the neural network that
fully exploits modern hardware. We demonstrate significant noise reduction at
the cost of little induced bias, and report state-of-the-art, real-time
performance on a number of challenging scenarios.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.11686v1,2021-06-22T11:40:00Z,2021-06-22T11:40:00Z,Simulation-Driven COVID-19 Epidemiological Modeling with Social Media,"Modern Bayesian approaches and workflows emphasize in how simulation is
important in the context of model developing. Simulation can help researchers
understand how the model behaves in a controlled setting and can be used to
stress the model in different ways before it is exposed to any real data. This
improved understanding could be beneficial in epidemiological models, specially
when dealing with COVID-19. Unfortunately, few researchers perform any
simulations. We present a simulation algorithm that implements a simple
agent-based model for disease transmission that works with a standard
compartment epidemiological model for COVID-19. Our algorithm can be applied in
different parameterizations to reflect several plausible epidemic scenarios.
Additionally, we also model how social media information in the form of daily
symptom mentions can be incorporate into COVID-19 epidemiological models. We
test our social media COVID-19 model with two experiments. The first using
simulated data from our agent-based simulation algorithm and the second with
real data using a machine learning tweet classifier to identify tweets that
mention symptoms from noise. Our results shows how a COVID-19 model can be (1)
used to incorporate social media data and (2) assessed and evaluated with
simulated and real data.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.10352v1,2021-06-18T20:54:18Z,2021-06-18T20:54:18Z,"Semi-supervised Optimal Transport with Self-paced Ensemble for
  Cross-hospital Sepsis Early Detection","The utilization of computer technology to solve problems in medical scenarios
has attracted considerable attention in recent years, which still has great
potential and space for exploration. Among them, machine learning has been
widely used in the prediction, diagnosis and even treatment of Sepsis. However,
state-of-the-art methods require large amounts of labeled medical data for
supervised learning. In real-world applications, the lack of labeled data will
cause enormous obstacles if one hospital wants to deploy a new Sepsis detection
system. Different from the supervised learning setting, we need to use known
information (e.g., from another hospital with rich labeled data) to help build
a model with acceptable performance, i.e., transfer learning. In this paper, we
propose a semi-supervised optimal transport with self-paced ensemble framework
for Sepsis early detection, called SPSSOT, to transfer knowledge from the other
that has rich labeled data. In SPSSOT, we first extract the same clinical
indicators from the source domain (e.g., hospital with rich labeled data) and
the target domain (e.g., hospital with little labeled data), then we combine
the semi-supervised domain adaptation based on optimal transport theory with
self-paced under-sampling to avoid a negative transfer possibly caused by
covariate shift and class imbalance. On the whole, SPSSOT is an end-to-end
transfer learning method for Sepsis early detection which can automatically
select suitable samples from two domains respectively according to the number
of iterations and align feature space of two domains. Extensive experiments on
two open clinical datasets demonstrate that comparing with other methods, our
proposed SPSSOT, can significantly improve the AUC values with only 1% labeled
data in the target domain in two transfer learning scenarios, MIMIC
$rightarrow$ Challenge and Challenge $rightarrow$ MIMIC.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.10131v1,2021-06-18T13:47:56Z,2021-06-18T13:47:56Z,Enhancing user creativity: Semantic measures for idea generation,"Human creativity generates novel ideas to solve real-world problems. This
thereby grants us the power to transform the surrounding world and extend our
human attributes beyond what is currently possible. Creative ideas are not just
new and unexpected, but are also successful in providing solutions that are
useful, efficient and valuable. Thus, creativity optimizes the use of available
resources and increases wealth. The origin of human creativity, however, is
poorly understood, and semantic measures that could predict the success of
generated ideas are currently unknown. Here, we analyze a dataset of design
problem-solving conversations in real-world settings by using 49 semantic
measures based on WordNet 3.1 and demonstrate that a divergence of semantic
similarity, an increased information content, and a decreased polysemy predict
the success of generated ideas. The first feedback from clients also enhances
information content and leads to a divergence of successful ideas in creative
problem solving. These results advance cognitive science by identifying
real-world processes in human problem solving that are relevant to the success
of produced solutions and provide tools for real-time monitoring of problem
solving, student training and skill acquisition. A selected subset of
information content (IC S\'anchez-Batet) and semantic similarity
(Lin/S\'anchez-Batet) measures, which are both statistically powerful and
computationally fast, could support the development of technologies for
computer-assisted enhancements of human creativity or for the implementation of
creativity in machines endowed with general artificial intelligence.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.09357v1,2021-06-17T10:20:45Z,2021-06-17T10:20:45Z,"Cat-like Jumping and Landing of Legged Robots in Low-gravity Using Deep
  Reinforcement Learning","In this article, we show that learned policies can be applied to solve legged
locomotion control tasks with extensive flight phases, such as those
encountered in space exploration. Using an off-the-shelf deep reinforcement
learning algorithm, we trained a neural network to control a jumping quadruped
robot while solely using its limbs for attitude control. We present tasks of
increasing complexity leading to a combination of three-dimensional
(re-)orientation and landing locomotion behaviors of a quadruped robot
traversing simulated low-gravity celestial bodies. We show that our approach
easily generalizes across these tasks and successfully trains policies for each
case. Using sim-to-real transfer, we deploy trained policies in the real world
on the SpaceBok robot placed on an experimental testbed designed for
two-dimensional micro-gravity experiments. The experimental results demonstrate
that repetitive, controlled jumping and landing with natural agility is
possible.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.08437v1,2021-06-15T21:09:19Z,2021-06-15T21:09:19Z,Deep reinforcement learning on a multi-asset environment for trading,"Financial trading has been widely analyzed for decades with market
participants and academics always looking for advanced methods to improve
trading performance. Deep reinforcement learning (DRL), a recently
reinvigorated method with significant success in multiple domains, still has to
show its benefit in the financial markets. We use a deep Q-network (DQN) to
design long-short trading strategies for futures contracts. The state space
consists of volatility-normalized daily returns, with buying or selling being
the reinforcement learning action and the total reward defined as the
cumulative profits from our actions. Our trading strategy is trained and tested
both on real and simulated price series and we compare the results with an
index benchmark. We analyze how training based on a combination of artificial
data and actual price series can be successfully deployed in real markets. The
trained reinforcement learning agent is applied to trading the E-mini S&P 500
continuous futures contract. Our results in this study are preliminary and need
further improvement.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.07178v5,2022-04-20T01:16:29Z,2021-06-14T06:04:57Z,A Comprehensive Survey on Graph Anomaly Detection with Deep Learning,"Anomalies represent rare observations (e.g., data records or events) that
deviate significantly from others. Over several decades, research on anomaly
mining has received increasing interests due to the implications of these
occurrences in a wide range of disciplines. Anomaly detection, which aims to
identify rare observations, is among the most vital tasks in the world, and has
shown its power in preventing detrimental events, such as financial fraud,
network intrusion, and social spam. The detection task is typically solved by
identifying outlying data points in the feature space and inherently overlooks
the relational information in real-world data. Graphs have been prevalently
used to represent the structural information, which raises the graph anomaly
detection problem - identifying anomalous graph objects (i.e., nodes, edges and
sub-graphs) in a single graph, or anomalous graphs in a database/set of graphs.
However, conventional anomaly detection techniques cannot tackle this problem
well because of the complexity of graph data. For the advent of deep learning,
graph anomaly detection with deep learning has received a growing attention
recently. In this survey, we aim to provide a systematic and comprehensive
review of the contemporary deep learning techniques for graph anomaly
detection. We compile open-sourced implementations, public datasets, and
commonly-used evaluation metrics to provide affluent resources for future
studies. More importantly, we highlight twelve extensive future research
directions according to our survey results covering unsolved and emerging
research problems and real-world applications. With this survey, our goal is to
create a ""one-stop-shop"" that provides a unified understanding of the problem
categories and existing approaches, publicly available hands-on resources, and
high-impact open challenges for graph anomaly detection using deep learning.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.06150v1,2021-06-11T03:30:25Z,2021-06-11T03:30:25Z,Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs,"Graph neural networks (GNNs) are powerful tools for learning from graph data
and are widely used in various applications such as social network
recommendation, fraud detection, and graph search. The graphs in these
applications are typically large, usually containing hundreds of millions of
nodes. Training GNN models on such large graphs efficiently remains a big
challenge. Despite a number of sampling-based methods have been proposed to
enable mini-batch training on large graphs, these methods have not been proved
to work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU
training. The state-of-the-art sampling-based methods are usually not optimized
for these real-world hardware setups, in which data movement between CPUs and
GPUs is a bottleneck. To address this issue, we propose Global Neighborhood
Sampling that aims at training GNNs on giant graphs specifically for
mixed-CPU-GPU training. The algorithm samples a global cache of nodes
periodically for all mini-batches and stores them in GPUs. This global cache
allows in-GPU importance sampling of mini-batches, which drastically reduces
the number of nodes in a mini-batch, especially in the input layer, to reduce
data copy between CPU and GPU and mini-batch computation without compromising
the training convergence rate or model accuracy. We provide a highly efficient
implementation of this method and show that our implementation outperforms an
efficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant
graphs. It outperforms an efficient implementation of LADIES with small layers
by a factor of 2X-14X while achieving much higher accuracy than LADIES.We also
theoretically analyze the proposed algorithm and show that with cached node
data of a proper size, it enjoys a comparable convergence rate as the
underlying node-wise sampling method.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.04941v1,2021-06-09T09:33:33Z,2021-06-09T09:33:33Z,Symmetric Spaces for Graph Embeddings: A Finsler-Riemannian Approach,"Learning faithful graph representations as sets of vertex embeddings has
become a fundamental intermediary step in a wide range of machine learning
applications. We propose the systematic use of symmetric spaces in
representation learning, a class encompassing many of the previously used
embedding targets. This enables us to introduce a new method, the use of
Finsler metrics integrated in a Riemannian optimization scheme, that better
adapts to dissimilar structures in the graph. We develop a tool to analyze the
embeddings and infer structural properties of the data sets. For
implementation, we choose Siegel spaces, a versatile family of symmetric
spaces. Our approach outperforms competitive baselines for graph reconstruction
tasks on various synthetic and real-world datasets. We further demonstrate its
applicability on two downstream tasks, recommender systems and node
classification.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.04569v3,2022-05-31T16:41:58Z,2021-06-08T17:58:10Z,Simulated Adversarial Testing of Face Recognition Models,"Most machine learning models are validated and tested on fixed datasets. This
can give an incomplete picture of the capabilities and weaknesses of the model.
Such weaknesses can be revealed at test time in the real world. The risks
involved in such failures can be loss of profits, loss of time or even loss of
life in certain critical applications. In order to alleviate this issue,
simulators can be controlled in a fine-grained manner using interpretable
parameters to explore the semantic image manifold. In this work, we propose a
framework for learning how to test machine learning algorithms using simulators
in an adversarial manner in order to find weaknesses in the model before
deploying it in critical scenarios. We apply this method in a face recognition
setup. We show that certain weaknesses of models trained on real data can be
discovered using simulated samples. Using our proposed method, we can find
adversarial synthetic faces that fool contemporary face recognition models.
This demonstrates the fact that these models have weaknesses that are not
measured by commonly used validation datasets. We hypothesize that this type of
adversarial examples are not isolated, but usually lie in connected spaces in
the latent space of the simulator. We present a method to find these
adversarial regions as opposed to the typical adversarial points found in the
adversarial example literature.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.04008v2,2021-06-09T16:58:52Z,2021-06-07T23:31:47Z,Widening Access to Applied Machine Learning with TinyML,"Broadening access to both computational and educational resources is critical
to diffusing machine-learning (ML) innovation. However, today, most ML
resources and experts are siloed in a few countries and organizations. In this
paper, we describe our pedagogical approach to increasing access to applied ML
through a massive open online course (MOOC) on Tiny Machine Learning (TinyML).
We suggest that TinyML, ML on resource-constrained embedded devices, is an
attractive means to widen access because TinyML both leverages low-cost and
globally accessible hardware, and encourages the development of complete,
self-contained applications, from data collection to deployment. To this end, a
collaboration between academia (Harvard University) and industry (Google)
produced a four-part MOOC that provides application-oriented instruction on how
to develop solutions using TinyML. The series is openly available on the edX
MOOC platform, has no prerequisites beyond basic programming, and is designed
for learners from a global variety of backgrounds. It introduces pupils to
real-world applications, ML algorithms, data-set engineering, and the ethical
considerations of these technologies via hands-on programming and deployment of
TinyML applications in both the cloud and their own microcontrollers. To
facilitate continued learning, community building, and collaboration beyond the
courses, we launched a standalone website, a forum, a chat, and an optional
course-project competition. We also released the course materials publicly,
hoping they will inspire the next generation of ML practitioners and educators
and further broaden access to cutting-edge ML technologies.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.02964v1,2021-06-05T21:15:34Z,2021-06-05T21:15:34Z,"A Review of Machine Learning Classification Using Quantum Annealing for
  Real-world Applications","Optimizing the training of a machine learning pipeline helps in reducing
training costs and improving model performance. One such optimizing strategy is
quantum annealing, which is an emerging computing paradigm that has shown
potential in optimizing the training of a machine learning model. The
implementation of a physical quantum annealer has been realized by D-Wave
systems and is available to the research community for experiments. Recent
experimental results on a variety of machine learning applications using
quantum annealing have shown interesting results where the performance of
classical machine learning techniques is limited by limited training data and
high dimensional features. This article explores the application of D-Wave's
quantum annealer for optimizing machine learning pipelines for real-world
classification problems. We review the application domains on which a physical
quantum annealer has been used to train machine learning classifiers. We
discuss and analyze the experiments performed on the D-Wave quantum annealer
for applications such as image recognition, remote sensing imagery,
computational biology, and particle physics. We discuss the possible advantages
and the problems for which quantum annealing is likely to be advantageous over
classical computation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.15301v1,2021-06-05T19:28:16Z,2021-06-05T19:28:16Z,"VolterraNet: A higher order convolutional network with group
  equivariance for homogeneous manifolds","Convolutional neural networks have been highly successful in image-based
learning tasks due to their translation equivariance property. Recent work has
generalized the traditional convolutional layer of a convolutional neural
network to non-Euclidean spaces and shown group equivariance of the generalized
convolution operation. In this paper, we present a novel higher order Volterra
convolutional neural network (VolterraNet) for data defined as samples of
functions on Riemannian homogeneous spaces. Analagous to the result for
traditional convolutions, we prove that the Volterra functional convolutions
are equivariant to the action of the isometry group admitted by the Riemannian
homogeneous spaces, and under some restrictions, any non-linear equivariant
function can be expressed as our homogeneous space Volterra convolution,
generalizing the non-linear shift equivariant characterization of Volterra
expansions in Euclidean space. We also prove that second order functional
convolution operations can be represented as cascaded convolutions which leads
to an efficient implementation. Beyond this, we also propose a dilated
VolterraNet model. These advances lead to large parameter reductions relative
to baseline non-Euclidean CNNs. To demonstrate the efficacy of the VolterraNet
performance, we present several real data experiments involving classification
tasks on spherical-MNIST, atomic energy, Shrec17 data sets, and group testing
on diffusion MRI data. Performance comparisons to the state-of-the-art are also
presented.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2106.01503v1,2021-06-02T23:17:29Z,2021-06-02T23:17:29Z,Towards an Explanation Space to Align Humans and Explainable-AI Teamwork,"Providing meaningful and actionable explanations to end-users is a
fundamental prerequisite for implementing explainable intelligent systems in
the real world. Explainability is a situated interaction between a user and the
AI system rather than being static design principles. The content of
explanations is context-dependent and must be defined by evidence about the
user and its context. This paper seeks to operationalize this concept by
proposing a formative architecture that defines the explanation space from a
user-inspired perspective. The architecture comprises five intertwined
components to outline explanation requirements for a task: (1) the end-users
mental models, (2) the end-users cognitive process, (3) the user interface, (4)
the human-explainer agent, and the (5) agent process. We first define each
component of the architecture. Then we present the Abstracted Explanation
Space, a modeling tool that aggregates the architecture's components to support
designers in systematically aligning explanations with the end-users work
practices, needs, and goals. It guides the specifications of what needs to be
explained (content - end-users mental model), why this explanation is necessary
(context - end-users cognitive process), to delimit how to explain it (format -
human-explainer agent and user interface), and when should the explanations be
given. We then exemplify the tool's use in an ongoing case study in the
aircraft maintenance domain. Finally, we discuss possible contributions of the
tool, known limitations/areas for improvement, and future work to be done.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2107.01002v2,2021-09-27T13:42:44Z,2021-05-31T12:09:46Z,"WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise
  Labels","We introduce WiCluster, a new machine learning (ML) approach for passive
indoor positioning using radio frequency (RF) channel state information (CSI).
WiCluster can predict both a zone-level position and a precise 2D or 3D
position, without using any precise position labels during training. Prior
CSI-based indoor positioning work has relied on non-parametric approaches using
digital signal-processing (DSP) and, more recently, parametric approaches
(e.g., fully supervised ML methods). However these do not handle the complexity
of real-world environments well and do not meet requirements for large-scale
commercial deployments: the accuracy of DSP-based method deteriorates
significantly in non-line-of-sight conditions, while supervised ML methods need
large amounts of hard-to-acquire centimeter accuracy position labels. In
contrast, WiCluster is precise, requires weaker label-information that can be
easily collected, and works well in non-line-of-sight conditions. Our first
contribution is a novel dimensionality reduction method for charting. It
combines a triplet-loss with a multi-scale clustering-loss to map the
high-dimensional CSI representation to a 2D/3D latent space. Our second
contribution is two weakly supervised losses that map this latent space into a
Cartesian map, resulting in meter-accuracy position results. These losses only
require simple to acquire priors: a sketch of the floorplan, approximate
access-point locations and a few CSI packets that are labelled with the
corresponding zone in the floorplan. Thirdly, we report results and a
robustness study for 2D positioning in two single-floor office buildings and 3D
positioning in a two-story home.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.14280v1,2021-05-29T12:04:27Z,2021-05-29T12:04:27Z,Hashing-Accelerated Graph Neural Networks for Link Prediction,"Networks are ubiquitous in the real world. Link prediction, as one of the key
problems for network-structured data, aims to predict whether there exists a
link between two nodes. The traditional approaches are based on the explicit
similarity computation between the compact node representation by embedding
each node into a low-dimensional space. In order to efficiently handle the
intensive similarity computation in link prediction, the hashing technique has
been successfully used to produce the node representation in the Hamming space.
However, the hashing-based link prediction algorithms face accuracy loss from
the randomized hashing techniques or inefficiency from the learning to hash
techniques in the embedding process. Currently, the Graph Neural Network (GNN)
framework has been widely applied to the graph-related tasks in an end-to-end
manner, but it commonly requires substantial computational resources and memory
costs due to massive parameter learning, which makes the GNN-based algorithms
impractical without the help of a powerful workhorse. In this paper, we propose
a simple and effective model called #GNN, which balances the trade-off between
accuracy and efficiency. #GNN is able to efficiently acquire node
representation in the Hamming space for link prediction by exploiting the
randomized hashing technique to implement message passing and capture
high-order proximity in the GNN framework. Furthermore, we characterize the
discriminative power of #GNN in probability. The extensive experimental results
demonstrate that the proposed #GNN algorithm achieves accuracy comparable to
the learning-based algorithms and outperforms the randomized algorithm, while
running significantly faster than the learning-based algorithms. Also, the
proposed algorithm shows excellent scalability on a large-scale network with
the limited resources.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.13598v4,2021-11-30T19:29:55Z,2021-05-28T05:54:59Z,End-to-End Deep Fault Tolerant Control,"PUBLISHED ON IEEE/ASME TRANSACTIONS ON MECHATRONICS, DOI:
10.1109/TMECH.2021.3100150. Ideally, accurate sensor measurements are needed to
achieve a good performance in the closed-loop control of mechatronic systems.
As a consequence, sensor faults will prevent the system from working correctly,
unless a fault-tolerant control (FTC) architecture is adopted. As model-based
FTC algorithms for nonlinear systems are often challenging to design, this
paper focuses on a new method for FTC in the presence of sensor faults, based
on deep learning. The considered approach replaces the phases of fault
detection and isolation and controller design with a single recurrent neural
network, which has the value of past sensor measurements in a given time window
as input, and the current values of the control variables as output. This
end-to-end deep FTC method is applied to a mechatronic system composed of a
spherical inverted pendulum, whose configuration is changed via reaction
wheels, in turn actuated by electric motors. The simulation and experimental
results show that the proposed method can handle abrupt faults occurring in
link position/velocity sensors. The provided supplementary material includes a
video of real-world experiments and the software source code.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.13429v1,2021-05-27T20:05:41Z,2021-05-27T20:05:41Z,"Flow based features and validation metric for machine learning
  reconstruction of PIV data","Reconstruction of flow field from real sparse data by a physics-oriented
approach is a current challenge for fluid scientists in the AI community. The
problem includes feature recognition and implementation of AI algorithms that
link data to a physical feature space in order to produce reconstructed data.
The present article applies machine learning approach to study contribution of
different flow-based features with practical fluid mechanics applications for
reconstruction of the missing data of turbomachinery PIV measurements. Support
vector regression (SVR) and multi-layer perceptron (MLP) are selected as two
robust regressors capable of modelling non-linear fluid flow phenomena. The
proposed flow-based features are optimally scaled and filtered to extract the
best configuration. In addition to conventional data-based validation of the
regressors, a metric is proposed that reflects mass conservation law as an
important requirement for a physical flow reproduction. For a velocity field
including 25% of clustered missing data, the reconstruction accuracy achieved
by SVR in terms of R2-score is as high as 0.993 for the in-plane velocity
vectors in comparison with that obtained by MLP which is up to 0.981. In terms
of mass conservation metric, the SVR model by R2-score up to 0.96 is
considerably more accurate than the MLP estimator. For extremely sparse data
with a gappiness of 75%, vector and contour plots from SVR and MLP were
consistent with those of the original field.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.13191v1,2021-05-26T14:04:04Z,2021-05-26T14:04:04Z,"Deep Learning Techniques for Compressive Sensing-Based Reconstruction
  and Inference -- A Ubiquitous Systems Perspective","Compressive sensing (CS) is a mathematically elegant tool for reducing the
sampling rate, potentially bringing context-awareness to a wider range of
devices. Nevertheless, practical issues with the sampling and reconstruction
algorithms prevent further proliferation of CS in real world domains,
especially among heterogeneous ubiquitous devices. Deep learning (DL) naturally
complements CS for adapting the sampling matrix, reconstructing the signal, and
learning form the compressed samples. While the CS-DL integration has received
substantial research interest recently, it has not yet been thoroughly
surveyed, nor has the light been shed on practical issues towards bringing the
CS-DL to real world implementations in the ubicomp domain. In this paper we
identify main possible ways in which CS and DL can interplay, extract key ideas
for making CS-DL efficient, identify major trends in CS-DL research space, and
derive guidelines for future evolution of CS-DL within the ubicomp domain.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.11914v2,2021-08-24T13:58:11Z,2021-05-25T13:17:35Z,Theory and Design of Super-resolution Haptic Skins,"Haptic feedback is important to make robots more dexterous and effective in
unstructured environments. High-resolution haptic sensors are still not widely
available, and their application is often bound by the resolution-robustness
dilemma. A route towards high-resolution and robust skin embeds a few sensor
units (taxels) into a flexible surface material and uses signal processing to
achieve sensing with super-resolution accuracy. We propose a theory for
geometric super-resolution to guide the development of haptic sensors of this
kind and link it to machine learning techniques for signal processing. This
theory is based on sensor isolines and allows us to predict force sensitivity
and accuracy in contact position and force magnitude as a spatial quantity. We
evaluate the influence of different factors, such as elastic properties of the
material, structure design, and transduction methods, using finite element
simulations and by implementing real sensors. We empirically determine sensor
isolines and validate the theory in two custom-built sensors with barometric
units for 1D and 2D measurement surfaces. Using machine learning methods for
the inference of contact information, our sensors obtain an unparalleled
average super-resolution factor of over 100 and 1200, respectively. Our theory
can guide future haptic sensor designs and inform various design choices.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.12123v1,2021-05-25T09:45:59Z,2021-05-25T09:45:59Z,Photonic extreme learning machine by free-space optical propagation,"Photonic brain-inspired platforms are emerging as novel analog computing
devices, enabling fast and energy-efficient operations for machine learning.
These artificial neural networks generally require tailored optical elements,
such as integrated photonic circuits, engineered diffractive layers,
nanophotonic materials, or time-delay schemes, which are challenging to train
or stabilize. Here we present a neuromorphic photonic scheme - photonic extreme
learning machines - that can be implemented simply by using an optical encoder
and coherent wave propagation in free space. We realize the concept through
spatial light modulation of a laser beam, with the far field that acts as
feature mapping space. We experimentally demonstrated learning from data on
various classification and regression tasks, achieving accuracies comparable to
digital extreme learning machines. Our findings point out an optical machine
learning device that is easy-to-train, energetically efficient, scalable and
fabrication-constraint free. The scheme can be generalized to a plethora of
photonic systems, opening the route to real-time neuromorphic processing of
optical data.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.11328v1,2021-05-24T15:05:58Z,2021-05-24T15:05:58Z,Room Clearance with Feudal Hierarchical Reinforcement Learning,"Reinforcement learning (RL) is a general framework that allows systems to
learn autonomously through trial-and-error interaction with their environment.
In recent years combining RL with expressive, high-capacity neural network
models has led to impressive performance in a diverse range of domains.
However, dealing with the large state and action spaces often required for
problems in the real world still remains a significant challenge. In this paper
we introduce a new simulation environment, ""Gambit"", designed as a tool to
build scenarios that can drive RL research in a direction useful for military
analysis. Using this environment we focus on an abstracted and simplified room
clearance scenario, where a team of blue agents have to make their way through
a building and ensure that all rooms are cleared of (and remain clear) of enemy
red agents. We implement a multi-agent version of feudal hierarchical RL that
introduces a command hierarchy where a commander at the higher level sends
orders to multiple agents at the lower level who simply have to learn to follow
these orders. We find that breaking the task down in this way allows us to
solve a number of non-trivial floorplans that require the coordination of
multiple agents much more efficiently than the standard baseline RL algorithms
we compare with. We then go on to explore how qualitatively different behaviour
can emerge depending on what we prioritise in the agent's reward function (e.g.
clearing the building quickly vs. prioritising rescuing civilians).",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.10907v1,2021-05-23T10:34:48Z,2021-05-23T10:34:48Z,"An Efficient Application of Neuroevolution for Competitive Multiagent
  Learning","Multiagent systems provide an ideal environment for the evaluation and
analysis of real-world problems using reinforcement learning algorithms. Most
traditional approaches to multiagent learning are affected by long training
periods as well as high computational complexity. NEAT (NeuroEvolution of
Augmenting Topologies) is a popular evolutionary strategy used to obtain the
best performing neural network architecture often used to tackle optimization
problems in the field of artificial intelligence. This paper utilizes the NEAT
algorithm to achieve competitive multiagent learning on a modified pong game
environment in an efficient manner. The competing agents abide by different
rules while having similar observation space parameters. The proposed algorithm
utilizes this property of the environment to define a singular
neuroevolutionary procedure that obtains the optimal policy for all the agents.
The compiled results indicate that the proposed implementation achieves ideal
behaviour in a very short training period when compared to existing multiagent
reinforcement learning models.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.08841v1,2021-05-18T21:43:51Z,2021-05-18T21:43:51Z,A Deep Learning Method for AGILE-GRID GRB Detection,"The follow-up of external science alerts received from Gamma-Ray Bursts (GRB)
and Gravitational Waves (GW) detectors is one of the AGILE Team's current major
activities. The AGILE team developed an automated real-time analysis pipeline
to analyse AGILE Gamma-Ray Imaging Detector (GRID) data to detect possible
counterparts in the energy range 0.1-10 GeV. This work presents a new approach
for detecting GRBs using a Convolutional Neural Network (CNN) to classify the
AGILE-GRID intensity maps improving the GRBs detection capability over the
Li&Ma method, currently used by the AGILE team. The CNN is trained with large
simulated datasets of intensity maps. The AGILE complex observing pattern due
to the so-called 'spinning mode' is studied to prepare datasets to test and
evaluate the CNN. A GRB emission model is defined from the Second Fermi-LAT GRB
catalogue and convoluted with the AGILE observing pattern. Different p-value
distributions are calculated evaluating with the CNN millions of
background-only maps simulated varying the background level. The CNN is then
used on real data to analyse the AGILE-GRID data archive, searching for GRB
detections using the trigger time and position taken from the Swift-BAT,
Fermi-GBM, and Fermi-LAT GRB catalogues. From these catalogues, the CNN detects
21 GRBs with a significance $\geq 3 \sigma$, while the Li&Ma method detects
only two GRBs. The results shown in this work demonstrate that the CNN is more
effective in detecting GRBs than the Li&Ma method in this context and can be
implemented into the AGILE-GRID real-time analysis pipeline.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.07615v2,2021-08-16T13:01:14Z,2021-05-17T05:30:41Z,Differentially Private Federated Knowledge Graphs Embedding,"Knowledge graph embedding plays an important role in knowledge
representation, reasoning, and data mining applications. However, for multiple
cross-domain knowledge graphs, state-of-the-art embedding models cannot make
full use of the data from different knowledge domains while preserving the
privacy of exchanged data. In addition, the centralized embedding model may not
scale to the extensive real-world knowledge graphs. Therefore, we propose a
novel decentralized scalable learning framework, \emph{Federated Knowledge
Graphs Embedding} (FKGE), where embeddings from different knowledge graphs can
be learnt in an asynchronous and peer-to-peer manner while being
privacy-preserving. FKGE exploits adversarial generation between pairs of
knowledge graphs to translate identical entities and relations of different
domains into near embedding spaces. In order to protect the privacy of the
training data, FKGE further implements a privacy-preserving neural network
structure to guarantee no raw data leakage. We conduct extensive experiments to
evaluate FKGE on 11 knowledge graphs, demonstrating a significant and
consistent improvement in model quality with at most 17.85\% and 7.90\%
increases in performance on triple classification and link prediction tasks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.06631v4,2021-09-15T14:46:28Z,2021-05-14T03:49:59Z,Ordering-Based Causal Discovery with Reinforcement Learning,"It is a long-standing question to discover causal relations among a set of
variables in many empirical sciences. Recently, Reinforcement Learning (RL) has
achieved promising results in causal discovery from observational data.
However, searching the space of directed graphs and enforcing acyclicity by
implicit penalties tend to be inefficient and restrict the existing RL-based
method to small scale problems. In this work, we propose a novel RL-based
approach for causal discovery, by incorporating RL into the ordering-based
paradigm. Specifically, we formulate the ordering search problem as a
multi-step Markov decision process, implement the ordering generating process
with an encoder-decoder architecture, and finally use RL to optimize the
proposed model based on the reward mechanisms designed for~each ordering. A
generated ordering would then be processed using variable selection to obtain
the final causal graph. We analyze the consistency and computational complexity
of the proposed method, and empirically show that a pretrained model can be
exploited to accelerate training. Experimental results on both synthetic and
real data sets shows that the proposed method achieves a much improved
performance over existing RL-based method.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.06457v2,2022-01-08T03:48:30Z,2021-05-13T17:56:04Z,Conversational AI Systems for Social Good: Opportunities and Challenges,"Conversational artificial intelligence (ConvAI) systems have attracted much
academic and commercial attention recently, making significant progress on both
fronts. However, little existing work discusses how these systems can be
developed and deployed for social good in real-world applications, with
comprehensive case studies and analyses of pros and cons. In this paper, we
briefly review the progress the community has made towards better ConvAI
systems and reflect on how existing technologies can help advance social good
initiatives from various angles that are unique for ConvAI, or not yet become
common knowledge in the community. We further discuss about the challenges
ahead for ConvAI systems to better help us achieve these goals and highlight
the risks involved in their development and deployment in the real world.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.06442v1,2021-05-13T17:33:28Z,2021-05-13T17:33:28Z,"An Empirical Comparison of Bias Reduction Methods on Real-World Problems
  in High-Stakes Policy Settings","Applications of machine learning (ML) to high-stakes policy settings -- such
as education, criminal justice, healthcare, and social service delivery -- have
grown rapidly in recent years, sparking important conversations about how to
ensure fair outcomes from these systems. The machine learning research
community has responded to this challenge with a wide array of proposed
fairness-enhancing strategies for ML models, but despite the large number of
methods that have been developed, little empirical work exists evaluating these
methods in real-world settings. Here, we seek to fill this research gap by
investigating the performance of several methods that operate at different
points in the ML pipeline across four real-world public policy and social good
problems. Across these problems, we find a wide degree of variability and
inconsistency in the ability of many of these methods to improve model
fairness, but post-processing by choosing group-specific score thresholds
consistently removes disparities, with important implications for both the ML
research community and practitioners deploying machine learning to inform
consequential policy decisions.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.01879v1,2021-05-05T05:58:29Z,2021-05-05T05:58:29Z,"MOS: Towards Scaling Out-of-distribution Detection for Large Semantic
  Space","Detecting out-of-distribution (OOD) inputs is a central challenge for safely
deploying machine learning models in the real world. Existing solutions are
mainly driven by small datasets, with low resolution and very few class labels
(e.g., CIFAR). As a result, OOD detection for large-scale image classification
tasks remains largely unexplored. In this paper, we bridge this critical gap by
proposing a group-based OOD detection framework, along with a novel OOD scoring
function termed MOS. Our key idea is to decompose the large semantic space into
smaller groups with similar concepts, which allows simplifying the decision
boundaries between in- vs. out-of-distribution data for effective OOD
detection. Our method scales substantially better for high-dimensional class
space than previous approaches. We evaluate models trained on ImageNet against
four carefully curated OOD datasets, spanning diverse semantics. MOS
establishes state-of-the-art performance, reducing the average FPR95 by 14.33%
while achieving 6x speedup in inference compared to the previous best method.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.01774v2,2021-06-18T15:21:10Z,2021-05-04T21:40:04Z,"Envisioning Communities: A Participatory Approach Towards AI for Social
  Good","Research in artificial intelligence (AI) for social good presupposes some
definition of social good, but potential definitions have been seldom suggested
and never agreed upon. The normative question of what AI for social good
research should be ""for"" is not thoughtfully elaborated, or is frequently
addressed with a utilitarian outlook that prioritizes the needs of the majority
over those who have been historically marginalized, brushing aside realities of
injustice and inequity. We argue that AI for social good ought to be assessed
by the communities that the AI system will impact, using as a guide the
capabilities approach, a framework to measure the ability of different policies
to improve human welfare equity. Furthermore, we lay out how AI research has
the potential to catalyze social progress by expanding and equalizing
capabilities. We show how the capabilities approach aligns with a participatory
approach for the design and implementation of AI for social good research in a
framework we introduce called PACT, in which community members affected should
be brought in as partners and their input prioritized throughout the project.
We conclude by providing an incomplete set of guiding questions for carrying
out such participatory AI research in a way that elicits and respects a
community's own definition of social good.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.01636v1,2021-05-04T17:27:59Z,2021-05-04T17:27:59Z,Learning 3D Granular Flow Simulations,"Recently, the application of machine learning models has gained momentum in
natural sciences and engineering, which is a natural fit due to the abundance
of data in these fields. However, the modeling of physical processes from
simulation data without first principle solutions remains difficult. Here, we
present a Graph Neural Networks approach towards accurate modeling of complex
3D granular flow simulation processes created by the discrete element method
LIGGGHTS and concentrate on simulations of physical systems found in real world
applications like rotating drums and hoppers. We discuss how to implement Graph
Neural Networks that deal with 3D objects, boundary conditions, particle -
particle, and particle - boundary interactions such that an accurate modeling
of relevant physical quantities is made possible. Finally, we compare the
machine learning based trajectories to LIGGGHTS trajectories in terms of
particle flows and mixing entropies.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2105.01571v1,2021-05-03T14:13:42Z,2021-05-03T14:13:42Z,"Effective Sparsification of Neural Networks with Global Sparsity
  Constraint","Weight pruning is an effective technique to reduce the model size and
inference time for deep neural networks in real-world deployments. However,
since magnitudes and relative importance of weights are very different for
different layers of a neural network, existing methods rely on either manual
tuning or handcrafted heuristic rules to find appropriate pruning rates
individually for each layer. This approach generally leads to suboptimal
performance. In this paper, by directly working on the probability space, we
propose an effective network sparsification method called {\it probabilistic
masking} (ProbMask), which solves a natural sparsification formulation under
global sparsity constraint. The key idea is to use probability as a global
criterion for all layers to measure the weight importance. An appealing feature
of ProbMask is that the amounts of weight redundancy can be learned
automatically via our constraint and thus we avoid the problem of tuning
pruning rates individually for different layers in a network. Extensive
experimental results on CIFAR-10/100 and ImageNet demonstrate that our method
is highly effective, and can outperform previous state-of-the-art methods by a
significant margin, especially in the high pruning rate situation. Notably, the
gap of Top-1 accuracy between our ProbMask and existing methods can be up to
10\%. As a by-product, we show ProbMask is also highly effective in identifying
supermasks, which are subnetworks with high performance in a randomly weighted
dense neural network.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.14870v1,2021-04-30T09:53:28Z,2021-04-30T09:53:28Z,"Action in Mind: A Neural Network Approach to Action Recognition and
  Segmentation","Recognizing and categorizing human actions is an important task with
applications in various fields such as human-robot interaction, video analysis,
surveillance, video retrieval, health care system and entertainment industry.
This thesis presents a novel computational approach for human action
recognition through different implementations of multi-layer architectures
based on artificial neural networks. Each system level development is designed
to solve different aspects of the action recognition problem including online
real-time processing, action segmentation and the involvement of objects. The
analysis of the experimental results are illustrated and described in six
articles. The proposed action recognition architecture of this thesis is
composed of several processing layers including a preprocessing layer, an
ordered vector representation layer and three layers of neural networks. It
utilizes self-organizing neural networks such as Kohonen feature maps and
growing grids as the main neural network layers. Thus the architecture presents
a biological plausible approach with certain features such as topographic
organization of the neurons, lateral interactions, semi-supervised learning and
the ability to represent high dimensional input space in lower dimensional
maps. For each level of development the system is trained with the input data
consisting of consecutive 3D body postures and tested with generalized input
data that the system has never met before. The experimental results of
different system level developments show that the system performs well with
quite high accuracy for recognizing human actions.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.14210v2,2021-12-27T15:39:37Z,2021-04-29T08:59:36Z,"FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph
  Representation Learning","Graph representation learning has become a ubiquitous component in many
scenarios, ranging from social network analysis to energy forecasting in smart
grids. In several applications, ensuring the fairness of the node (or graph)
representations with respect to some protected attributes is crucial for their
correct deployment. Yet, fairness in graph deep learning remains
under-explored, with few solutions available. In particular, the tendency of
similar nodes to cluster on several real-world graphs (i.e., homophily) can
dramatically worsen the fairness of these procedures. In this paper, we propose
a novel biased edge dropout algorithm (FairDrop) to counter-act homophily and
improve fairness in graph representation learning. FairDrop can be plugged in
easily on many existing algorithms, is efficient, adaptable, and can be
combined with other fairness-inducing solutions. After describing the general
algorithm, we demonstrate its application on two benchmark tasks, specifically,
as a random walk model for producing node embeddings, and to a graph
convolutional network for link prediction. We prove that the proposed algorithm
can successfully improve the fairness of all models up to a small or negligible
drop in accuracy, and compares favourably with existing state-of-the-art
solutions. In an ablation study, we demonstrate that our algorithm can flexibly
interpolate between biasing towards fairness and an unbiased edge dropout.
Furthermore, to better evaluate the gains, we propose a new dyadic group
definition to measure the bias of a link prediction task when paired with
group-based fairness metrics. In particular, we extend the metric used to
measure the bias in the node embeddings to take into account the graph
structure.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.13478v2,2021-05-02T16:16:03Z,2021-04-27T21:09:51Z,"Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges","The last decade has witnessed an experimental revolution in data science and
machine learning, epitomised by deep learning methods. Indeed, many
high-dimensional learning tasks previously thought to be beyond reach -- such
as computer vision, playing Go, or protein folding -- are in fact feasible with
appropriate computational scale. Remarkably, the essence of deep learning is
built from two simple algorithmic principles: first, the notion of
representation or feature learning, whereby adapted, often hierarchical,
features capture the appropriate notion of regularity for each task, and
second, learning by local gradient-descent type methods, typically implemented
as backpropagation.
  While learning generic functions in high dimensions is a cursed estimation
problem, most tasks of interest are not generic, and come with essential
pre-defined regularities arising from the underlying low-dimensionality and
structure of the physical world. This text is concerned with exposing these
regularities through unified geometric principles that can be applied
throughout a wide spectrum of applications.
  Such a 'geometric unification' endeavour, in the spirit of Felix Klein's
Erlangen Program, serves a dual purpose: on one hand, it provides a common
mathematical framework to study the most successful neural network
architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand,
it gives a constructive procedure to incorporate prior physical knowledge into
neural architectures and provide principled way to build future architectures
yet to be invented.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.13386v1,2021-04-27T18:00:02Z,2021-04-27T18:00:02Z,"Deep physical neural networks enabled by a backpropagation algorithm for
  arbitrary physical systems","Deep neural networks have become a pervasive tool in science and engineering.
However, modern deep neural networks' growing energy requirements now
increasingly limit their scaling and broader use. We propose a radical
alternative for implementing deep neural network models: Physical Neural
Networks. We introduce a hybrid physical-digital algorithm called Physics-Aware
Training to efficiently train sequences of controllable physical systems to act
as deep neural networks. This method automatically trains the functionality of
any sequence of real physical systems, directly, using backpropagation, the
same technique used for modern deep neural networks. To illustrate their
generality, we demonstrate physical neural networks with three diverse physical
systems-optical, mechanical, and electrical. Physical neural networks may
facilitate unconventional machine learning hardware that is orders of magnitude
faster and more energy efficient than conventional electronic processors.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.12125v1,2021-04-25T10:33:35Z,2021-04-25T10:33:35Z,"Development of a Soft Actor Critic Deep Reinforcement Learning Approach
  for Harnessing Energy Flexibility in a Large Office Building","This research is concerned with the novel application and investigation of
`Soft Actor Critic' (SAC) based Deep Reinforcement Learning (DRL) to control
the cooling setpoint (and hence cooling loads) of a large commercial building
to harness energy flexibility. The research is motivated by the challenge
associated with the development and application of conventional model-based
control approaches at scale to the wider building stock. SAC is a model-free
DRL technique that is able to handle continuous action spaces and which has
seen limited application to real-life or high-fidelity simulation
implementations in the context of automated and intelligent control of building
energy systems. Such control techniques are seen as one possible solution to
supporting the operation of a smart, sustainable and future electrical grid.
This research tests the suitability of the SAC DRL technique through training
and deployment of the agent on an EnergyPlus based environment of the office
building. The SAC DRL was found to learn an optimal control policy that was
able to minimise energy costs by 9.7% compared to the default rule-based
control (RBC) scheme and was able to improve or maintain thermal comfort limits
over a test period of one week. The algorithm was shown to be robust to the
different hyperparameters and this optimal control policy was learnt through
the use of a minimal state space consisting of readily available variables. The
robustness of the algorithm was tested through investigation of the speed of
learning and ability to deploy to different seasons and climates. It was found
that the SAC DRL requires minimal training sample points and outperforms the
RBC after three months of operation and also without disruption to thermal
comfort during this period. The agent is transferable to other climates and
seasons although further retraining or hyperparameter tuning is recommended.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.11918v1,2021-04-24T10:04:14Z,2021-04-24T10:04:14Z,"Constraint-Guided Reinforcement Learning: Augmenting the
  Agent-Environment-Interaction","Reinforcement Learning (RL) agents have great successes in solving tasks with
large observation and action spaces from limited feedback. Still, training the
agents is data-intensive and there are no guarantees that the learned behavior
is safe and does not violate rules of the environment, which has limitations
for the practical deployment in real-world scenarios. This paper discusses the
engineering of reliable agents via the integration of deep RL with
constraint-based augmentation models to guide the RL agent towards safe
behavior. Within the constraints set, the RL agent is free to adapt and
explore, such that its effectiveness to solve the given problem is not
hindered. However, once the RL agent leaves the space defined by the
constraints, the outside models can provide guidance to still work reliably. We
discuss integration points for constraint guidance within the RL process and
perform experiments on two case studies: a strictly constrained card game and a
grid world environment with additional combinatorial subgoals. Our results show
that constraint-guidance does both provide reliability improvements and safer
behavior, as well as accelerated training.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.11360v1,2021-04-23T00:46:35Z,2021-04-23T00:46:35Z,"Normalized multivariate time series causality analysis and causal graph
  reconstruction","Causality analysis is an important problem lying at the heart of science, and
is of particular importance in data science and machine learning. An endeavor
during the past 16 years viewing causality as real physical notion so as to
formulate it from first principles, however, seems to go unnoticed. This study
introduces to the community this line of work, with a long-due generalization
of the information flow-based bivariate time series causal inference to
multivariate series, based on the recent advance in theoretical development.
The resulting formula is transparent, and can be implemented as a
computationally very efficient algorithm for application. It can be normalized,
and tested for statistical significance. Different from the previous work along
this line where only information flows are estimated, here an algorithm is also
implemented to quantify the influence of a unit to itself. While this forms a
challenge in some causal inferences, here it comes naturally, and hence the
identification of self-loops in a causal graph is fulfilled automatically as
the causalities along edges are inferred.
  To demonstrate the power of the approach, presented here are two applications
in extreme situations. The first is a network of multivariate processes buried
in heavy noises (with the noise-to-signal ratio exceeding 100), and the second
a network with nearly synchronized chaotic oscillators. In both graphs,
confounding processes exist. While it seems to be a huge challenge to
reconstruct from given series these causal graphs, an easy application of the
algorithm immediately reveals the desideratum. Particularly, the confounding
processes have been accurately differentiated. Considering the surge of
interest in the community, this study is very timely.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.11146v1,2021-04-22T15:59:56Z,2021-04-22T15:59:56Z,"An Efficient One-Class SVM for Anomaly Detection in the Internet of
  Things","Insecure Internet of things (IoT) devices pose significant threats to
critical infrastructure and the Internet at large; detecting anomalous behavior
from these devices remains of critical importance, but fast, efficient,
accurate anomaly detection (also called ""novelty detection"") for these classes
of devices remains elusive. One-Class Support Vector Machines (OCSVM) are one
of the state-of-the-art approaches for novelty detection (or anomaly detection)
in machine learning, due to their flexibility in fitting complex nonlinear
boundaries between {normal} and {novel} data. IoT devices in smart homes and
cities and connected building infrastructure present a compelling use case for
novelty detection with OCSVM due to the variety of devices, traffic patterns,
and types of anomalies that can manifest in such environments. Much previous
research has thus applied OCSVM to novelty detection for IoT. Unfortunately,
conventional OCSVMs introduce significant memory requirements and are
computationally expensive at prediction time as the size of the train set
grows, requiring space and time that scales with the number of training points.
These memory and computational constraints can be prohibitive in practical,
real-world deployments, where large training sets are typically needed to
develop accurate models when fitting complex decision boundaries. In this work,
we extend so-called Nystr\""om and (Gaussian) Sketching approaches to OCSVM, by
combining these methods with clustering and Gaussian mixture models to achieve
significant speedups in prediction time and space in various IoT settings,
without sacrificing detection accuracy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.10891v1,2021-04-22T06:43:02Z,2021-04-22T06:43:02Z,"Computer Vision-based Social Distancing Surveillance Solution with
  Optional Automated Camera Calibration for Large Scale Deployment","Social distancing has been suggested as one of the most effective measures to
break the chain of viral transmission in the current COVID-19 pandemic. We
herein describe a computer vision-based AI-assisted solution to aid compliance
with social distancing norms. The solution consists of modules to detect and
track people and to identify distance violations. It provides the flexibility
to choose between a tool-based mode or an automated mode of camera calibration,
making the latter suitable for large-scale deployments. In this paper, we
discuss different metrics to assess the risk associated with social distancing
violations and how we can differentiate between transient or persistent
violations. Our proposed solution performs satisfactorily under different test
scenarios, processes video feed at real-time speed as well as addresses data
privacy regulations by blurring faces of detected people, making it ideal for
deployments.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.10398v1,2021-04-21T08:09:57Z,2021-04-21T08:09:57Z,Learning future terrorist targets through temporal meta-graphs,"In the last 20 years, terrorism has led to hundreds of thousands of deaths
and massive economic, political, and humanitarian crises in several regions of
the world. Using real-world data on attacks occurred in Afghanistan and Iraq
from 2001 to 2018, we propose the use of temporal meta-graphs and deep learning
to forecast future terrorist targets. Focusing on three event dimensions, i.e.,
employed weapons, deployed tactics and chosen targets, meta-graphs map the
connections among temporally close attacks, capturing their operational
similarities and dependencies. From these temporal meta-graphs, we derive
2-day-based time series that measure the centrality of each feature within each
dimension over time. Formulating the problem in the context of the strategic
behavior of terrorist actors, these multivariate temporal sequences are then
utilized to learn what target types are at the highest risk of being chosen.
The paper makes two contributions. First, it demonstrates that engineering the
feature space via temporal meta-graphs produces richer knowledge than shallow
time-series that only rely on frequency of feature occurrences. Second, the
performed experiments reveal that bi-directional LSTM networks achieve superior
forecasting performance compared to other algorithms, calling for future
research aiming at fully discovering the potential of artificial intelligence
to counter terrorist violence.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.10159v1,2021-04-20T17:58:22Z,2021-04-20T17:58:22Z,MBRL-Lib: A Modular Library for Model-based Reinforcement Learning,"Model-based reinforcement learning is a compelling framework for
data-efficient learning of agents that interact with the world. This family of
algorithms has many subcomponents that need to be carefully selected and tuned.
As a result the entry-bar for researchers to approach the field and to deploy
it in real-world tasks can be daunting. In this paper, we present MBRL-Lib -- a
machine learning library for model-based reinforcement learning in continuous
state-action spaces based on PyTorch. MBRL-Lib is designed as a platform for
both researchers, to easily develop, debug and compare new algorithms, and
non-expert user, to lower the entry-bar of deploying state-of-the-art
algorithms. MBRL-Lib is open-source at
https://github.com/facebookresearch/mbrl-lib.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.09684v2,2022-03-15T15:04:18Z,2021-04-19T23:28:32Z,Suppressing simulation bias using multi-modal data,"Many problems in science and engineering require making predictions based on
few observations. To build a robust predictive model, these sparse data may
need to be augmented with simulated data, especially when the design space is
multi-dimensional. Simulations, however, often suffer from an inherent bias.
Estimation of this bias may be poorly constrained not only because of data
sparsity, but also because traditional predictive models fit only one type of
observed outputs, such as scalars or images, instead of all available output
data modalities, which might have been acquired and simulated at great cost. To
break this limitation and open up the path for multi-modal calibration, we
propose to combine a novel, transfer learning technique for suppressing the
bias with recent developments in deep learning, which allow building predictive
models with multi-modal outputs. First, we train an initial neural network
model on simulated data to learn important correlations between different
output modalities and between simulation inputs and outputs. Then, the model is
partially retrained, or transfer learned, to fit the experiments; a method that
has never been implemented in this type of architecture. Using fewer than 10
inertial confinement fusion experiments for training, transfer learning
systematically improves the simulation predictions while a simple output
calibration, which we design as a baseline, makes the predictions worse. We
also offer extensive cross-validation with real and carefully designed
synthetic data. The method described in this paper can be applied to a wide
range of problems that require transferring knowledge from simulations to the
domain of experiments.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.09650v2,2022-06-08T07:26:22Z,2021-04-19T21:32:44Z,"Mapping the Internet: Modelling Entity Interactions in Complex
  Heterogeneous Networks","Even though machine learning algorithms already play a significant role in
data science, many current methods pose unrealistic assumptions on input data.
The application of such methods is difficult due to incompatible data formats,
or heterogeneous, hierarchical or entirely missing data fragments in the
dataset. As a solution, we propose a versatile, unified framework called
`HMill' for sample representation, model definition and training. We review in
depth a multi-instance paradigm for machine learning that the framework builds
on and extends. To theoretically justify the design of key components of HMill,
we show an extension of the universal approximation theorem to the set of all
functions realized by models implemented in the framework. The text also
contains a detailed discussion on technicalities and performance improvements
in our implementation, which is published for download under the MIT License.
The main asset of the framework is its flexibility, which makes modelling of
diverse real-world data sources with the same tool possible. Additionally to
the standard setting in which a set of attributes is observed for each object
individually, we explain how message-passing inference in graphs that represent
whole systems of objects can be implemented in the framework. To support our
claims, we solve three different problems from the cybersecurity domain using
the framework. The first use case concerns IoT device identification from raw
network observations. In the second problem, we study how malicious binary
files can be classified using a snapshot of the operating system represented as
a directed graph. The last provided example is a task of domain blacklist
extension through modelling interactions between entities in the network. In
all three problems, the solution based on the proposed framework achieves
performance comparable to specialized approaches.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.09164v1,2021-04-19T09:41:32Z,2021-04-19T09:41:32Z,"HEAR: Human Action Recognition via Neural Networks on Homomorphically
  Encrypted Data","Remote monitoring to support ""aging in place"" is an active area of research.
Advanced computer vision technology based on deep learning can provide near
real-time home monitoring to detect falling and symptoms related to seizure,
and stroke. Affordable webcams, together with cloud computing services (to run
machine learning algorithms), can potentially bring significant social and
health benefits. However, it has not been deployed in practice because of
privacy and security concerns. People may feel uncomfortable sending their
videos of daily activities (with potentially sensitive private information) to
a computing service provider (e.g., on a commercial cloud). In this paper, we
propose a novel strategy to resolve this dilemma by applying fully homomorphic
encryption (FHE) to an alternative representation of human actions (i.e.,
skeleton joints), which guarantees information confidentiality while retaining
high-performance action detection at a low cost. We design an FHE-friendly
neural network for action recognition and present a secure neural network
evaluation strategy to achieve near real-time action detection. Our framework
for private inference achieves an 87.99% recognition accuracy (86.21%
sensitivity and 99.14% specificity in detecting falls) with a latency of 3.1
seconds on real-world datasets. Our evaluation shows that our elaborated and
fine-tuned method reduces the inference latency by 23.81%~74.67% over a
straightforward implementation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.10067v1,2021-04-17T09:17:53Z,2021-04-17T09:17:53Z,Spherical Multi-Modal Place Recognition for Heterogeneous Sensor Systems,"In this paper, we propose a robust end-to-end multi-modal pipeline for place
recognition where the sensor systems can differ from the map building to the
query. Our approach operates directly on images and LiDAR scans without
requiring any local feature extraction modules. By projecting the sensor data
onto the unit sphere, we learn a multi-modal descriptor of partially
overlapping scenes using a spherical convolutional neural network. The employed
spherical projection model enables the support of arbitrary LiDAR and camera
systems readily without losing information. Loop closure candidates are found
using a nearest-neighbor lookup in the embedding space. We tackle the problem
of correctly identifying the closest place by correlating the candidates' power
spectra, obtaining a confidence value per prospect. Our estimate for the
correct place corresponds then to the candidate with the highest confidence. We
evaluate our proposal w.r.t. state-of-the-art approaches in place recognition
using real-world data acquired using different sensors. Our approach can
achieve a recall that is up to 10% and 5% higher than for a LiDAR- and
vision-based system, respectively, when the sensor setup differs between model
training and deployment. Additionally, our place selection can correctly
identify up to 95% matches from the candidate set.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.07282v1,2021-04-15T07:40:27Z,2021-04-15T07:40:27Z,"Rule-Based Reinforcement Learning for Efficient Robot Navigation with
  Space Reduction","For real-world deployments, it is critical to allow robots to navigate in
complex environments autonomously. Traditional methods usually maintain an
internal map of the environment, and then design several simple rules, in
conjunction with a localization and planning approach, to navigate through the
internal map. These approaches often involve a variety of assumptions and prior
knowledge. In contrast, recent reinforcement learning (RL) methods can provide
a model-free, self-learning mechanism as the robot interacts with an initially
unknown environment, but are expensive to deploy in real-world scenarios due to
inefficient exploration. In this paper, we focus on efficient navigation with
the RL technique and combine the advantages of these two kinds of methods into
a rule-based RL (RuRL) algorithm for reducing the sample complexity and cost of
time. First, we use the rule of wall-following to generate a closed-loop
trajectory. Second, we employ a reduction rule to shrink the trajectory, which
in turn effectively reduces the redundant exploration space. Besides, we give
the detailed theoretical guarantee that the optimal navigation path is still in
the reduced space. Third, in the reduced space, we utilize the Pledge rule to
guide the exploration strategy for accelerating the RL process at the early
stage. Experiments conducted on real robot navigation problems in hex-grid
environments demonstrate that RuRL can achieve improved navigation performance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.06890v2,2021-11-17T11:57:35Z,2021-04-14T14:31:51Z,An Introduction of mini-AlphaStar,"StarCraft II (SC2) is a real-time strategy game in which players produce and
control multiple units to fight against opponent's units. Due to its
difficulties, such as huge state space, various action space, a long time
horizon, and imperfect information, SC2 has been a research hotspot in
reinforcement learning. Recently, an agent called AlphaStar (AS) has been
proposed, which shows good performance, obtaining a high win rate of 99.8%
against human players. We implemented a mini-scaled version of it called
mini-AlphaStar (mAS) based on AS's paper and pseudocode. The difference between
AS and mAS is that we substituted the hyper-parameters of AS with smaller ones
for mini-scale training. Codes of mAS are all open-sourced
(https://github.com/liuruoze/mini-AlphaStar) for future research.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.05819v1,2021-04-12T21:07:53Z,2021-04-12T21:07:53Z,Learning from Executions for Semantic Parsing,"Semantic parsing aims at translating natural language (NL) utterances onto
machine-interpretable programs, which can be executed against a real-world
environment. The expensive annotation of utterance-program pairs has long been
acknowledged as a major bottleneck for the deployment of contemporary neural
models to real-life applications. In this work, we focus on the task of
semi-supervised learning where a limited amount of annotated data is available
together with many unlabeled NL utterances. Based on the observation that
programs which correspond to NL utterances must be always executable, we
propose to encourage a parser to generate executable programs for unlabeled
utterances. Due to the large search space of executable programs, conventional
methods that use approximations based on beam-search such as self-training and
top-k marginal likelihood training, do not perform as well. Instead, we view
the problem of learning from executions from the perspective of posterior
regularization and propose a set of new training objectives. Experimental
results on Overnight and GeoQuery show that our new objectives outperform
conventional methods, bridging the gap between semi-supervised and supervised
learning.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.04148v2,2021-04-12T03:06:05Z,2021-04-09T01:54:58Z,"Individual Explanations in Machine Learning Models: A Case Study on
  Poverty Estimation","Machine learning methods are being increasingly applied in sensitive societal
contexts, where decisions impact human lives. Hence it has become necessary to
build capabilities for providing easily-interpretable explanations of models'
predictions. Recently in academic literature, a vast number of explanations
methods have been proposed. Unfortunately, to our knowledge, little has been
documented about the challenges machine learning practitioners most often face
when applying them in real-world scenarios. For example, a typical procedure
such as feature engineering can make some methodologies no longer applicable.
The present case study has two main objectives. First, to expose these
challenges and how they affect the use of relevant and novel explanations
methods. And second, to present a set of strategies that mitigate such
challenges, as faced when implementing explanation methods in a relevant
application domain -- poverty estimation and its use for prioritizing access to
social policies.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.03961v3,2022-02-02T18:12:40Z,2021-04-08T17:59:07Z,Generalized Approach to Matched Filtering using Neural Networks,"Gravitational wave science is a pioneering field with rapidly evolving data
analysis methodology currently assimilating and inventing deep learning
techniques. The bulk of the sophisticated flagship searches of the field rely
on the time-tested matched filtering principle within their core. In this
paper, we make a key observation on the relationship between the emerging deep
learning and the traditional techniques: matched filtering is formally
equivalent to a particular neural network. This means that a neural network can
be constructed analytically to exactly implement matched filtering, and can be
further trained on data or boosted with additional complexity for improved
performance. Moreover, we show that the proposed neural network architecture
can outperform matched filtering, both with or without knowledge of a prior on
the parameter distribution. When a prior is given, the proposed neural network
can approach the statistically optimal performance. We also propose and
investigate two different neural network architectures MNet-Shallow and
MNet-Deep, both of which implement matched filtering at initialization and can
be trained on data. MNet-Shallow has simpler structure, while MNet-Deep is more
flexible and can deal with a wider range of distributions. Our theoretical
findings are corroborated by experiments using real LIGO data and synthetic
injections, where our proposed methods significantly outperform matched
filtering at false positive rates above $5\times 10^{-3}\%$. The fundamental
equivalence between matched filtering and neural networks allows us to define a
""complexity standard candle"" to characterize the relative complexity of the
different approaches to gravitational wave signal searches in a common
framework. Finally, our results suggest new perspectives on the role of deep
learning in gravitational wave detection.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.03780v1,2021-04-08T14:05:15Z,2021-04-08T14:05:15Z,"Enabling Cross-Domain Communication: How to Bridge the Gap between AI
  and HW Engineers","A key issue in system design is the lack of communication between hardware,
software and domain expert. Recent research work shows progress in automatic
HW/SW co-design flows of neural accelerators that seems to make this kind of
communication obsolete. Most real-world systems, however, are a composition of
multiple processing units, communication networks and memories. A HW/SW
co-design process of (reconfigurable) neural accelerators, therefore, is an
important sub-problem towards a common co-design methodology. The ultimate
challenge is to define the constraints for the design space exploration on
system level - a task which requires deep knowledge and understanding of
hardware architectures, mapping of workloads onto hardware and the application
domain, e.g. artificial intelligence.
  For most projects, these skills are distributed among several people or even
different teams which is one of the major reasons why there is no established
end-to-end development methodology for digital systems. This position paper
discusses possibilities how to establish such a methodology for systems that
include (reconfigurable) dedicated accelerators and outlines the central role
that languages and tools play in the process.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.03693v1,2021-04-08T11:29:11Z,2021-04-08T11:29:11Z,Learning specialized activation functions with the Piecewise Linear Unit,"The choice of activation functions is crucial for modern deep neural
networks. Popular hand-designed activation functions like Rectified Linear
Unit(ReLU) and its variants show promising performance in various tasks and
models. Swish, the automatically discovered activation function, has been
proposed and outperforms ReLU on many challenging datasets. However, it has two
main drawbacks. First, the tree-based search space is highly discrete and
restricted, which is difficult for searching. Second, the sample-based
searching method is inefficient, making it infeasible to find specialized
activation functions for each dataset or neural architecture. To tackle these
drawbacks, we propose a new activation function called Piecewise Linear
Unit(PWLU), which incorporates a carefully designed formulation and learning
method. It can learn specialized activation functions and achieves SOTA
performance on large-scale datasets like ImageNet and COCO. For example, on
ImageNet classification dataset, PWLU improves 0.9%/0.53%/1.0%/1.7%/1.0% top-1
accuracy over Swish for
ResNet-18/ResNet-50/MobileNet-V2/MobileNet-V3/EfficientNet-B0. PWLU is also
easy to implement and efficient at inference, which can be widely applied in
real-world applications.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.02541v1,2021-04-06T14:31:23Z,2021-04-06T14:31:23Z,"Instantaneous Stereo Depth Estimation of Real-World Stimuli with a
  Neuromorphic Stereo-Vision Setup","The stereo-matching problem, i.e., matching corresponding features in two
different views to reconstruct depth, is efficiently solved in biology. Yet, it
remains the computational bottleneck for classical machine vision approaches.
By exploiting the properties of event cameras, recently proposed Spiking Neural
Network (SNN) architectures for stereo vision have the potential of simplifying
the stereo-matching problem. Several solutions that combine event cameras with
spike-based neuromorphic processors already exist. However, they are either
simulated on digital hardware or tested on simplified stimuli. In this work, we
use the Dynamic Vision Sensor 3D Human Pose Dataset (DHP19) to validate a
brain-inspired event-based stereo-matching architecture implemented on a
mixed-signal neuromorphic processor with real-world data. Our experiments show
that this SNN architecture, composed of coincidence detectors and disparity
sensitive neurons, is able to provide a coarse estimate of the input disparity
instantaneously, thereby detecting the presence of a stimulus moving in depth
in real-time.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.02459v2,2021-04-07T07:09:30Z,2021-04-06T12:35:23Z,Contrastive Explanations for Explaining Model Adaptations,"Many decision making systems deployed in the real world are not static - a
phenomenon known as model adaptation takes place over time. The need for
transparency and interpretability of AI-based decision models is widely
accepted and thus have been worked on extensively. Usually, explanation methods
assume a static system that has to be explained. Explaining non-static systems
is still an open research question, which poses the challenge how to explain
model adaptations. In this contribution, we propose and (empirically) evaluate
a framework for explaining model adaptations by contrastive explanations. We
also propose a method for automatically finding regions in data space that are
affected by a given model adaptation and thus should be explained.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.02306v1,2021-04-06T06:04:57Z,2021-04-06T06:04:57Z,Binary Neural Network for Speaker Verification,"Although deep neural networks are successful for many tasks in the speech
domain, the high computational and memory costs of deep neural networks make it
difficult to directly deploy highperformance Neural Network systems on
low-resource embedded devices. There are several mechanisms to reduce the size
of the neural networks i.e. parameter pruning, parameter quantization, etc.
This paper focuses on how to apply binary neural networks to the task of
speaker verification. The proposed binarization of training parameters can
largely maintain the performance while significantly reducing storage space
requirements and computational costs. Experiment results show that, after
binarizing the Convolutional Neural Network, the ResNet34-based network
achieves an EER of around 5% on the Voxceleb1 testing dataset and even
outperforms the traditional real number network on the text-dependent dataset:
Xiaole while having a 32x memory saving.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.02214v1,2021-04-06T01:04:28Z,2021-04-06T01:04:28Z,"Intelligent Building Control Systems for Thermal Comfort and
  Energy-Efficiency: A Systematic Review of Artificial Intelligence-Assisted
  Techniques","Building operations represent a significant percentage of the total primary
energy consumed in most countries due to the proliferation of Heating,
Ventilation and Air-Conditioning (HVAC) installations in response to the
growing demand for improved thermal comfort. Reducing the associated energy
consumption while maintaining comfortable conditions in buildings are
conflicting objectives and represent a typical optimization problem that
requires intelligent system design. Over the last decade, different
methodologies based on the Artificial Intelligence (AI) techniques have been
deployed to find the sweet spot between energy use in HVAC systems and suitable
indoor comfort levels to the occupants. This paper performs a comprehensive and
an in-depth systematic review of AI-based techniques used for building control
systems by assessing the outputs of these techniques, and their implementations
in the reviewed works, as well as investigating their abilities to improve the
energy-efficiency, while maintaining thermal comfort conditions. This enables a
holistic view of (1) the complexities of delivering thermal comfort to users
inside buildings in an energy-efficient way, and (2) the associated
bibliographic material to assist researchers and experts in the field in
tackling such a challenge. Among the 20 AI tools developed for both energy
consumption and comfort control, functions such as identification and
recognition patterns, optimization, predictive control. Based on the findings
of this work, the application of AI technology in building control is a
promising area of research and still an ongoing, i.e., the performance of
AI-based control is not yet completely satisfactory. This is mainly due in part
to the fact that these algorithms usually need a large amount of high-quality
real-world data, which is lacking in the building or, more precisely, the
energy sector.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.01757v1,2021-04-05T03:49:45Z,2021-04-05T03:49:45Z,Predicting Mergers and Acquisitions using Graph-based Deep Learning,"The graph data structure is a staple in mathematics, yet graph-based machine
learning is a relatively green field within the domain of data science. Recent
advances in graph-based ML and open source implementations of relevant
algorithms are allowing researchers to apply methods created in academia to
real-world datasets. The goal of this project was to utilize a popular graph
machine learning framework, GraphSAGE, to predict mergers and acquisitions
(M&A) of enterprise companies. The results were promising, as the model
predicted with 81.79% accuracy on a validation dataset. Given the abundance of
data sources and algorithmic decision making within financial data science,
graph-based machine learning offers a performant, yet non-traditional approach
to generating alpha.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.01747v7,2022-04-07T01:09:18Z,2021-04-05T02:59:45Z,Fast Design Space Exploration of Nonlinear Systems: Part I,"System design tools are often only available as input-output blackboxes: for
a given design as input they compute an output representing system behavior.
Blackboxes are intended to be run in the forward direction. This paper presents
a new method of solving the inverse design problem namely, given requirements
or constraints on output, find an input that also optimizes an objective
function. This problem is challenging for several reasons. First, blackboxes
are not designed to be run in reverse. Second, inputs and outputs can be
discrete and continuous. Third, finding designs concurrently satisfying a set
of requirements is hard because designs satisfying individual requirements may
conflict with each other. Fourth, blackbox evaluations can be expensive.
Finally, blackboxes can sometimes fail to produce an output. This paper
presents CNMA, a new method of solving the inverse problem that overcomes these
challenges. CNMA tries to sample only the part of the design space relevant to
solving the problem, leveraging the power of neural networks, Mixed Integer
Linear Programs, and a new learning-from-failure feedback loop. The paper also
presents a parallel version of CNMA that improves the efficiency and quality of
solutions over the sequential version, and tries to steer it away from local
optima. CNMA's performance is evaluated against conventional optimization
methods for seven nonlinear design problems of 8 (two problems), 10, 15, 36 and
60 real-valued dimensions and one with 186 binary dimensions. Conventional
methods evaluated are off-the-shelf implementations of Bayesian Optimization
with Gaussian Processes, Nelder Mead and Random Search. The first two do not
solve problems that are high-dimensional, have discrete and continuous
variables or whose blackboxes can fail to return values. CNMA solves all
problems, and surpasses the performance of conventional methods by up to 87%.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.01716v2,2021-04-11T07:30:29Z,2021-04-05T00:02:36Z,"Quaternion Factorization Machines: A Lightweight Solution to Intricate
  Feature Interaction Modelling","As a well-established approach, factorization machine (FM) is capable of
automatically learning high-order interactions among features to make
predictions without the need for manual feature engineering. With the prominent
development of deep neural networks (DNNs), there is a recent and ongoing trend
of enhancing the expressiveness of FM-based models with DNNs. However, though
better results are obtained with DNN-based FM variants, such performance gain
is paid off by an enormous amount (usually millions) of excessive model
parameters on top of the plain FM. Consequently, the heavy parameterization
impedes the real-life practicality of those deep models, especially efficient
deployment on resource-constrained IoT and edge devices. In this paper, we move
beyond the traditional real space where most deep FM-based models are defined,
and seek solutions from quaternion representations within the hypercomplex
space. Specifically, we propose the quaternion factorization machine (QFM) and
quaternion neural factorization machine (QNFM), which are two novel lightweight
and memory-efficient quaternion-valued models for sparse predictive analytics.
By introducing a brand new take on FM-based models with the notion of
quaternion algebra, our models not only enable expressive inter-component
feature interactions, but also significantly reduce the parameter size due to
lower degrees of freedom in the hypercomplex Hamilton product compared with
real-valued matrix multiplication. Extensive experimental results on three
large-scale datasets demonstrate that QFM achieves 4.36% performance
improvement over the plain FM without introducing any extra parameters, while
QNFM outperforms all baselines with up to two magnitudes' parameter size
reduction in comparison to state-of-the-art peer methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.00941v1,2021-04-02T08:41:51Z,2021-04-02T08:41:51Z,Multi-Class Data Description for Out-of-distribution Detection,"The capability of reliably detecting out-of-distribution samples is one of
the key factors in deploying a good classifier, as the test distribution always
does not match with the training distribution in most real-world applications.
In this work, we present a deep multi-class data description, termed as
Deep-MCDD, which is effective to detect out-of-distribution (OOD) samples as
well as classify in-distribution (ID) samples. Unlike the softmax classifier
that only focuses on the linear decision boundary partitioning its latent space
into multiple regions, our Deep-MCDD aims to find a spherical decision boundary
for each class which determines whether a test sample belongs to the class or
not. By integrating the concept of Gaussian discriminant analysis into deep
neural networks, we propose a deep learning objective to learn
class-conditional distributions that are explicitly modeled as separable
Gaussian distributions. Thereby, we can define the confidence score by the
distance of a test sample from each class-conditional distribution, and utilize
it for identifying OOD samples. Our empirical evaluation on multi-class tabular
and image datasets demonstrates that Deep-MCDD achieves the best performances
in distinguishing OOD samples while showing the classification accuracy as high
as the other competitors.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2104.00165v2,2022-03-08T00:16:31Z,2021-03-31T23:58:34Z,"Encoding Event-Based Data With a Hybrid SNN Guided Variational
  Auto-encoder in Neuromorphic Hardware","Neuromorphic hardware equipped with learning capabilities can adapt to new,
real-time data. While models of Spiking Neural Networks (SNNs) can now be
trained using gradient descent to reach an accuracy comparable to equivalent
conventional neural networks, such learning often relies on external labels.
However, real-world data is unlabeled which can make supervised methods
inapplicable. To solve this problem, we propose a Hybrid Guided Variational
Autoencoder (VAE) which encodes event based data sensed by a Dynamic Vision
Sensor (DVS) into a latent space representation using an SNN. These
representations can be used as an embedding to measure data similarity and
predict labels in real-world data. We show that the Hybrid Guided-VAE achieves
87% classification accuracy on the DVSGesture dataset and it can encode the
sparse, noisy inputs into an interpretable latent space representation,
visualized through T-SNE plots. We also implement the encoder component of the
model on neuromorphic hardware and discuss the potential for our algorithm to
enable real-time learning from real-world event data.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.16584v1,2021-03-30T18:01:06Z,2021-03-30T18:01:06Z,"Parameterized Hypercomplex Graph Neural Networks for Graph
  Classification","Despite recent advances in representation learning in hypercomplex (HC)
space, this subject is still vastly unexplored in the context of graphs.
Motivated by the complex and quaternion algebras, which have been found in
several contexts to enable effective representation learning that inherently
incorporates a weight-sharing mechanism, we develop graph neural networks that
leverage the properties of hypercomplex feature transformation. In particular,
in our proposed class of models, the multiplication rule specifying the algebra
itself is inferred from the data during training. Given a fixed model
architecture, we present empirical evidence that our proposed model
incorporates a regularization effect, alleviating the risk of overfitting. We
also show that for fixed model capacity, our proposed method outperforms its
corresponding real-formulated GNN, providing additional confirmation for the
enhanced expressivity of HC embeddings. Finally, we test our proposed
hypercomplex GNN on several open graph benchmark datasets and show that our
models reach state-of-the-art performance while consuming a much lower memory
footprint with 70& fewer parameters. Our implementations are available at
https://github.com/bayer-science-for-a-better-life/phc-gnn.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.16323v2,2021-04-08T09:28:14Z,2021-03-30T13:15:48Z,"Thermal Neural Networks: Lumped-Parameter Thermal Modeling With
  State-Space Machine Learning","With electric power systems becoming more compact and increasingly powerful,
the relevance of thermal stress especially during overload operation is
expected to increase ceaselessly. Whenever critical temperatures cannot be
measured economically on a sensor base, a thermal model lends itself to
estimate those unknown quantities. Thermal models for electric power systems
are usually required to be both, real-time capable and of high estimation
accuracy. Moreover, ease of implementation and time to production play an
increasingly important role. In this work, the thermal neural network (TNN) is
introduced, which unifies both, consolidated knowledge in the form of
heat-transfer-based lumped-parameter models, and data-driven nonlinear function
approximation with supervised machine learning. A quasi-linear
parameter-varying system is identified solely from empirical data, where
relationships between scheduling variables and system matrices are inferred
statistically and automatically. At the same time, a TNN has physically
interpretable states through its state-space representation, is end-to-end
trainable -- similar to deep learning models -- with automatic differentiation,
and requires no material, geometry, nor expert knowledge for its design.
Experiments on an electric motor data set show that a TNN achieves higher
temperature estimation accuracies than previous white-/grey- or black-box
models with a mean squared error of $3.18~\text{K}^2$ and a worst-case error of
$5.84~\text{K}$ at 64 model parameters.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.16010v1,2021-03-30T00:49:40Z,2021-03-30T00:49:40Z,"Theory-Guided Machine Learning for Process Simulation of Advanced
  Composites","Science-based simulation tools such as Finite Element (FE) models are
routinely used in scientific and engineering applications. While their success
is strongly dependent on our understanding of underlying governing physical
laws, they suffer inherent limitations including trade-off between
fidelity/accuracy and speed. The recent rise of Machine Learning (ML) proposes
a theory-agnostic paradigm. In complex multi-physics problems, however,
creating large enough datasets for successful training of ML models has proven
to be challenging. One promising strategy to bridge the divide between these
approaches and take advantage of their respective strengths is Theory-Guided
Machine Learning (TGML) which aims to integrate physical laws into ML
algorithms. In this paper, three case studies on thermal management during
processing of advanced composites are presented and studied using FE, ML and
TGML. A structured approach to incrementally adding increasingly complex
physics to training of TGML model is presented. The benefits of TGML over ML
models are seen in more accurate predictions, particularly outside the training
region, and ability to train with small datasets. One benefit of TGML over FE
is significant speed improvement to potentially develop real-time feedback
systems. A recent successful implementation of a TGML model to assess
producibility of aerospace composite parts is presented.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.15348v2,2021-06-21T16:24:36Z,2021-03-29T05:55:08Z,"LayoutParser: A Unified Toolkit for Deep Learning Based Document Image
  Analysis","Recent advances in document image analysis (DIA) have been primarily driven
by the application of neural networks. Ideally, research outcomes could be
easily deployed in production and extended for further investigation. However,
various factors like loosely organized codebases and sophisticated model
configurations complicate the easy reuse of important innovations by a wide
audience. Though there have been on-going efforts to improve reusability and
simplify deep learning (DL) model development in disciplines like natural
language processing and computer vision, none of them are optimized for
challenges in the domain of DIA. This represents a major gap in the existing
toolkit, as DIA is central to academic research across a wide range of
disciplines in the social sciences and humanities. This paper introduces
layoutparser, an open-source library for streamlining the usage of DL in DIA
research and applications. The core layoutparser library comes with a set of
simple and intuitive interfaces for applying and customizing DL models for
layout detection, character recognition, and many other document processing
tasks. To promote extensibility, layoutparser also incorporates a community
platform for sharing both pre-trained models and full document digitization
pipelines. We demonstrate that layoutparser is helpful for both lightweight and
large-scale digitization pipelines in real-word use cases. The library is
publicly available at https://layout-parser.github.io/.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.14321v4,2022-03-30T09:41:36Z,2021-03-26T08:24:28Z,"Online Learning Koopman operator for closed-loop electrical
  neurostimulation in epilepsy","Electrical neuromodulation as a palliative treatment has been increasingly
used in epilepsy. However, most current neuromodulations implement
pre-determined actuation strategies and lack self-adaptive patterns for
adjusting stimulation strategies. In this work, rooted in optimal control
theory, we propose a novel framework for real-time closed-loop electrical
neuromodulation in epilepsy, which combines i) a deep Koopman operator based
dynamical model to predict the temporal evolution of epileptic EEG with an
approximated finite-dimensional linear dynamics and ii) a model predictive
control (MPC) modular to design optimal seizure suppression strategies. It is
termed Koopman-MPC framework. The Koopman operator based linear dynamical model
is embedded in the latent state space of the autoencoder neural network, in
which we can approximate and update the Koopman operator online. The linear
dynamical property of the Koopman operator ensures the convexity of the
optimization problem for subsequent MPC control. The predictive capability of
the deep Koopman operator model is tested with both synthetic and real
epileptic EEG data. The results demonstrate that the deep Koopman operator
based model can map nonlinear neural dynamics into finite-dimensional linear
dynamics with higher performance in predicting the seizure dynamics, compared
with a 10-order autoregressive model (AR) model and a recurrent neural network
(RNN). Moreover, compared with the RNN-MPC framework, the Koopman-MPC framework
can better suppress seizure dynamics with less time consumption (only 0.035s),
enabling real-time updates of epilepsy control strategies. Our Koopman-MPC
framework opens a new window for model-based closed-loop neuromodulation and
sheds light on nonlinear neurodynamics and feedback control policies.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.14251v1,2021-03-26T04:16:20Z,2021-03-26T04:16:20Z,"Embedding Power Flow into Machine Learning for Parameter and State
  Estimation","Modern state and parameter estimations in power systems consist of two
stages: the outer problem of minimizing the mismatch between network
observation and prediction over the network parameters, and the inner problem
of predicting the system state for given values of the parameters. The standard
solution of the combined problem is iterative: (a) set the parameters, e.g. to
priors on the power line characteristics, (b) map input observation to
prediction of the output, (c) compute the mismatch between predicted and
observed output, (d) make a gradient descent step in the space of parameters to
minimize the mismatch, and loop back to (a). We show how modern Machine
Learning (ML), and specifically training guided by automatic differentiation,
allows to resolve the iterative loop more efficiently. Moreover, we extend the
scheme to the case of incomplete observations, where Phasor Measurement Units
(reporting real and reactive powers, voltage and phase) are available only at
the generators (PV buses), while loads (PQ buses) report (via SCADA controls)
only active and reactive powers. Considering it from the implementation
perspective, our methodology of resolving the parameter and state estimation
problem can be viewed as embedding of the Power Flow (PF) solver into the
training loop of the Machine Learning framework (PyTorch, in this study). We
argue that this embedding can help to resolve high-level optimization problems
in power system operations and planning.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.13452v1,2021-03-24T19:11:58Z,2021-03-24T19:11:58Z,"A Portable, Self-Contained Neuroprosthetic Hand with Deep Learning-Based
  Finger Control","Objective: Deep learning-based neural decoders have emerged as the prominent
approach to enable dexterous and intuitive control of neuroprosthetic hands.
Yet few studies have materialized the use of deep learning in clinical settings
due to its high computational requirements. Methods: Recent advancements of
edge computing devices bring the potential to alleviate this problem. Here we
present the implementation of a neuroprosthetic hand with embedded deep
learning-based control. The neural decoder is designed based on the recurrent
neural network (RNN) architecture and deployed on the NVIDIA Jetson Nano - a
compacted yet powerful edge computing platform for deep learning inference.
This enables the implementation of the neuroprosthetic hand as a portable and
self-contained unit with real-time control of individual finger movements.
Results: The proposed system is evaluated on a transradial amputee using
peripheral nerve signals (ENG) with implanted intrafascicular microelectrodes.
The experiment results demonstrate the system's capabilities of providing
robust, high-accuracy (95-99%) and low-latency (50-120 msec) control of
individual finger movements in various laboratory and real-world environments.
Conclusion: Modern edge computing platforms enable the effective use of deep
learning-based neural decoders for neuroprosthesis control as an autonomous
system. Significance: This work helps pioneer the deployment of deep neural
networks in clinical applications underlying a new class of wearable biomedical
devices with embedded artificial intelligence.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.12344v2,2021-03-30T03:47:32Z,2021-03-23T06:39:29Z,"Joint Distribution across Representation Space for Out-of-Distribution
  Detection","Deep neural networks (DNNs) have become a key part of many modern software
applications. After training and validating, the DNN is deployed as an
irrevocable component and applied in real-world scenarios. Although most DNNs
are built meticulously with huge volumes of training data, data in the real
world still remain unknown to the DNN model, which leads to the crucial
requirement of runtime out-of-distribution (OOD) detection. However, many
existing approaches 1) need OOD data for classifier training or parameter
tuning, or 2) simply combine the scores of each hidden layer as an ensemble of
features for OOD detection. In this paper, we present a novel outlook on
in-distribution data in a generative manner, which takes their latent features
generated from each hidden layer as a joint distribution across representation
spaces. Since only the in-distribution latent features are comprehensively
understood in representation space, the internal difference between
in-distribution and OOD data can be naturally revealed without the intervention
of any OOD data. Specifically, We construct a generative model, called Latent
Sequential Gaussian Mixture (LSGM), to depict how the in-distribution latent
features are generated in terms of the trace of DNN inference across
representation spaces. We first construct the Gaussian Mixture Model (GMM)
based on in-distribution latent features for each hidden layer, and then
connect GMMs via the transition probabilities of the inference traces.
Experimental evaluations on popular benchmark OOD datasets and models validate
the superiority of the proposed method over the state-of-the-art methods in OOD
detection.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.10975v1,2021-03-19T18:21:38Z,2021-03-19T18:21:38Z,Accelerating GMRES with Deep Learning in Real-Time,"GMRES is a powerful numerical solver used to find solutions to extremely
large systems of linear equations. These systems of equations appear in many
applications in science and engineering. Here we demonstrate a real-time
machine learning algorithm that can be used to accelerate the time-to-solution
for GMRES. Our framework is novel in that is integrates the deep learning
algorithm in an in situ fashion: the AI-accelerator gradually learns how to
optimizes the time to solution without requiring user input (such as a
pre-trained data set). We describe how our algorithm collects data and
optimizes GMRES. We demonstrate our algorithm by implementing an accelerated
(MLGMRES) solver in Python. We then use MLGMRES to accelerate a solver for the
Poisson equation -- a class of linear problems that appears in may
applications.
  Informed by the properties of formal solutions to the Poisson equation, we
test the performance of different neural networks. Our key takeaway is that
networks which are capable of learning non-local relationships perform well,
without needing to be scaled with the input problem size, making them good
candidates for the extremely large problems encountered in high-performance
computing. For the inputs studied, our method provides a roughly 2$\times$
acceleration.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.10562v1,2021-03-18T23:15:35Z,2021-03-18T23:15:35Z,Dynamic Grasping with Reachability and Motion Awareness,"Grasping in dynamic environments presents a unique set of challenges. A
stable and reachable grasp can become unreachable and unstable as the target
object moves, motion planning needs to be adaptive and in real time, the delay
in computation makes prediction necessary. In this paper, we present a dynamic
grasping framework that is reachability-aware and motion-aware. Specifically,
we model the reachability space of the robot using a signed distance field
which enables us to quickly screen unreachable grasps. Also, we train a neural
network to predict the grasp quality conditioned on the current motion of the
target. Using these as ranking functions, we quickly filter a large grasp
database to a few grasps in real time. In addition, we present a seeding
approach for arm motion generation that utilizes solution from previous time
step. This quickly generates a new arm trajectory that is close to the previous
plan and prevents fluctuation. We implement a recurrent neural network (RNN)
for modelling and predicting the object motion. Our extensive experiments
demonstrate the importance of each of these components and we validate our
pipeline on a real robot.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.08602v2,2022-02-07T19:27:39Z,2021-03-15T18:00:01Z,"Graph Optimization Perspective for Low-Depth Trotter-Suzuki
  Decomposition","Hamiltonian simulation represents an important module in a large class of
quantum algorithms and simulations such as quantum machine learning, quantum
linear algebra methods, and modeling for physics, material science and
chemistry. One of the most prominent methods for realizing the time-evolution
unitary is via the Trotter-Suzuki decomposition. However, there is a large
class of possible decompositions for the infinitesimal time-evolution operator
as the order in which the Hamiltonian terms are implemented is arbitrary. We
introduce a novel perspective for generating a low-depth Trotter-Suzuki
decomposition assuming the standard Clifford+RZ gate set by adapting ideas from
quantum error correction. We map a given Trotter-Suzuki decomposition to a
constrained path on a graph which we deem the Pauli Frame Graph (PFG). Each
node of the PFG represents the set of possible Hamiltonian terms currently
available to be applied, Clifford operations represent a move from one node to
another, and so the graph distance represents the gate cost of implementing the
decomposition. The problem of finding the optimal decomposition is then
equivalent to solving a problem similar to the traveling salesman. Though this
is an NP-hard problem, we demonstrate the simplest heuristic, greedy search,
and compare the resulting two-qubit gate count and circuit depth to more
standard methods for a large class of scientifically relevant Hamiltonians,
both fermionic and bosonic, found in chemical, vibrational and condensed matter
problems. Moreover, these models all have a natural scaling behavior. We find
that in nearly every case we study, the resulting depth and two-qubit gate
counts are less than those provided by standard methods. We also find the
method is efficient in producing these circuits and amenable to
parallelization, making the method scalable for problems of real interest.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.07541v1,2021-03-12T21:35:26Z,2021-03-12T21:35:26Z,"Machine Learning aided k-t SENSE for fast reconstruction of highly
  accelerated PCMR data","Purpose: We implemented the Machine Learning (ML) aided k-t SENSE
reconstruction to enable high resolution quantitative real-time phase contrast
MR (PCMR). Methods: A residual U-net and our U-net M were used to generate the
high resolution x-f space estimate for k-t SENSE regularisation prior. The
networks were judged on their ability to generalise to real undersampled data.
The in-vivo validation was done on 20 real-time 18x prospectively undersmapled
GASperturbed PCMR data. The ML aided k-t SENSE reconstruction results were
compared against the free-breathing Cartesian retrospectively gated sequence
and the compressed sensing (CS) reconstruction of the same data. Results: In
general, the ML aided k-t SENSE generated flow curves that were visually
sharper than those produced using CS. In two exceptional cases, U-net M
predictions exhibited blurring which propagated to the extracted velocity
curves. However, there were no statistical differences in the measured peak
velocities and stroke volumes between the tested methods. The ML aided k-t
SENSE was estimated to be ~3.6x faster in processing than CS. Conclusion: The
ML aided k-t SENSE reconstruction enables artefact suppression on a par with CS
with no significant differences in quantitative measures. The timing results
suggest the on-line implementation could deliver a substantial increase in
clinical throughput.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.07138v2,2021-04-13T04:21:44Z,2021-03-12T08:23:21Z,UIEC^2-Net: CNN-based Underwater Image Enhancement Using Two Color Space,"Underwater image enhancement has attracted much attention due to the rise of
marine resource development in recent years. Benefit from the powerful
representation capabilities of Convolution Neural Networks(CNNs), multiple
underwater image enhancement algorithms based on CNNs have been proposed in the
last few years. However, almost all of these algorithms employ RGB color space
setting, which is insensitive to image properties such as luminance and
saturation. To address this problem, we proposed Underwater Image Enhancement
Convolution Neural Network using 2 Color Space (UICE^2-Net) that efficiently
and effectively integrate both RGB Color Space and HSV Color Space in one
single CNN. To our best knowledge, this method is the first to use HSV color
space for underwater image enhancement based on deep learning. UIEC^2-Net is an
end-to-end trainable network, consisting of three blocks as follow: a RGB
pixel-level block implements fundamental operations such as denoising and
removing color cast, a HSV global-adjust block for globally adjusting
underwater image luminance, color and saturation by adopting a novel neural
curve layer, and an attention map block for combining the advantages of RGB and
HSV block output images by distributing weight to each pixel. Experimental
results on synthetic and real-world underwater images show the good performance
of our proposed method in both subjective comparisons and objective metrics.
The code are available at https://github.com/BIGWangYuDong/UWEnhancement.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.07085v1,2021-03-12T04:56:45Z,2021-03-12T04:56:45Z,Wireframe-Based UI Design Search Through Image Autoencoder,"UI design is an integral part of software development. For many developers
who do not have much UI design experience, exposing them to a large database of
real-application UI designs can help them quickly build up a realistic
understanding of the design space for a software feature and get design
inspirations from existing applications. However, existing keyword-based,
image-similarity-based, and component-matching-based methods cannot reliably
find relevant high-fidelity UI designs in a large database alike to the UI
wireframe that the developers sketch, in face of the great variations in UI
designs. In this article, we propose a deep-learning-based UI design search
engine to fill in the gap. The key innovation of our search engine is to train
a wireframe image autoencoder using a large database of real-application UI
designs, without the need for labeling relevant UI designs. We implement our
approach for Android UI design search, and conduct extensive experiments with
artificially created relevant UI designs and human evaluation of UI design
search results. Our experiments confirm the superior performance of our search
engine over existing image-similarity or component-matching-based methods and
demonstrate the usefulness of our search engine in real-world UI design tasks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.06450v3,2022-06-26T21:01:23Z,2021-03-11T04:37:29Z,Full Page Handwriting Recognition via Image to Sequence Extraction,"We present a Neural Network based Handwritten Text Recognition (HTR) model
architecture that can be trained to recognize full pages of handwritten or
printed text without image segmentation. Being based on Image to Sequence
architecture, it can extract text present in an image and then sequence it
correctly without imposing any constraints regarding orientation, layout and
size of text and non-text. Further, it can also be trained to generate
auxiliary markup related to formatting, layout and content. We use character
level vocabulary, thereby enabling language and terminology of any subject. The
model achieves a new state-of-art in paragraph level recognition on the IAM
dataset. When evaluated on scans of real world handwritten free form test
answers - beset with curved and slanted lines, drawings, tables, math,
chemistry and other symbols - it performs better than all commercially
available HTR cloud APIs. It is deployed in production as part of a commercial
web application.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.06327v1,2021-03-10T20:14:41Z,2021-03-10T20:14:41Z,"Closed Loop Predictive Control of Adaptive Optics Systems with
  Convolutional Neural Networks","Predictive wavefront control is an important and rapidly developing field of
adaptive optics (AO). Through the prediction of future wavefront effects, the
inherent AO system servo-lag caused by the measurement, computation, and
application of the wavefront correction can be significantly mitigated. This
lag can impact the final delivered science image, including reduced strehl and
contrast, and inhibits our ability to reliably use faint guidestars. We
summarize here a novel method for training deep neural networks for predictive
control based on an adversarial prior. Unlike previous methods in the
literature, which have shown results based on previously generated data or for
open-loop systems, we demonstrate our network's performance simulated in closed
loop. Our models are able to both reduce effects induced by servo-lag and push
the faint end of reliable control with natural guidestars, improving K-band
Strehl performance compared to classical methods by over 55% for 16th magnitude
guide stars on an 8-meter telescope. We further show that LSTM based approaches
may be better suited in high-contrast scenarios where servo-lag error is most
pronounced, while traditional feed forward models are better suited for high
noise scenarios. Finally, we discuss future strategies for implementing our
system in real-time and on astronomical telescope systems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.06326v2,2021-07-05T03:46:03Z,2021-03-10T20:13:21Z,"S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement
  Learning","Offline reinforcement learning proposes to learn policies from large
collected datasets without interacting with the physical environment. These
algorithms have made it possible to learn useful skills from data that can then
be deployed in the environment in real-world settings where interactions may be
costly or dangerous, such as autonomous driving or factories. However, current
algorithms overfit to the dataset they are trained on and exhibit poor
out-of-distribution generalization to the environment when deployed. In this
paper, we study the effectiveness of performing data augmentations on the state
space, and study 7 different augmentation schemes and how they behave with
existing offline RL algorithms. We then combine the best data performing
augmentation scheme with a state-of-the-art Q-learning technique, and improve
the function approximation of the Q-networks by smoothening out the learned
state-action space. We experimentally show that using this Surprisingly Simple
Self-Supervision technique in RL (S4RL), we significantly improve over the
current state-of-the-art algorithms on offline robot learning environments such
as MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4].",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.06109v3,2022-03-08T09:25:31Z,2021-03-10T15:03:48Z,Session-based Social and Dependency-aware Software Recommendation,"With the increase of complexity of modern software, social collaborative
coding and reuse of open source software packages become more and more popular,
which thus greatly enhances the development efficiency and software quality.
However, the explosive growth of open source software packages exposes
developers to the challenge of information overload. While this can be
addressed by conventional recommender systems, they usually do not consider
particular constraints of social coding such as social influence among
developers and dependency relations among software packages. In this paper, we
aim to model the dynamic interests of developers with both social influence and
dependency constraints, and propose the Session-based Social and
Dependency-aware software Recommendation (SSDRec) model. This model integrates
recurrent neural network (RNN) and graph attention network (GAT) into a unified
framework. An RNN is employed to model the short-term dynamic interests of
developers in each session and two GATs are utilized to capture social
influence from friends and dependency constraints from dependent software
packages, respectively. Extensive experiments are conducted on real-world
datasets and the results demonstrate that our model significantly outperforms
the competitive baselines.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.09054v1,2021-03-07T14:59:12Z,2021-03-07T14:59:12Z,Sentiment Analysis for Troll Detection on Weibo,"The impact of social media on the modern world is difficult to overstate.
Virtually all companies and public figures have social media accounts on
popular platforms such as Twitter and Facebook. In China, the micro-blogging
service provider, Sina Weibo, is the most popular such service. To influence
public opinion, Weibo trolls -- the so called Water Army -- can be hired to
post deceptive comments. In this paper, we focus on troll detection via
sentiment analysis and other user activity data on the Sina Weibo platform. We
implement techniques for Chinese sentence segmentation, word embedding, and
sentiment score calculation. In recent years, troll detection and sentiment
analysis have been studied, but we are not aware of previous research that
considers troll detection based on sentiment analysis. We employ the resulting
techniques to develop and test a sentiment analysis approach for troll
detection, based on a variety of machine learning strategies. Experimental
results are generated and analyzed. A Chrome extension is presented that
implements our proposed technique, which enables real-time troll detection when
a user browses Sina Weibo.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.04263v2,2021-03-11T01:08:38Z,2021-03-07T04:40:15Z,Deepfake Videos in the Wild: Analysis and Detection,"AI-manipulated videos, commonly known as deepfakes, are an emerging problem.
Recently, researchers in academia and industry have contributed several
(self-created) benchmark deepfake datasets, and deepfake detection algorithms.
However, little effort has gone towards understanding deepfake videos in the
wild, leading to a limited understanding of the real-world applicability of
research contributions in this space. Even if detection schemes are shown to
perform well on existing datasets, it is unclear how well the methods
generalize to real-world deepfakes. To bridge this gap in knowledge, we make
the following contributions: First, we collect and present the largest dataset
of deepfake videos in the wild, containing 1,869 videos from YouTube and
Bilibili, and extract over 4.8M frames of content. Second, we present a
comprehensive analysis of the growth patterns, popularity, creators,
manipulation strategies, and production methods of deepfake content in the
real-world. Third, we systematically evaluate existing defenses using our new
dataset, and observe that they are not ready for deployment in the real-world.
Fourth, we explore the potential for transfer learning schemes and
competition-winning techniques to improve defenses.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.03745v1,2021-03-05T15:17:13Z,2021-03-05T15:17:13Z,"Can You Fix My Neural Network? Real-Time Adaptive Waveform Synthesis for
  Resilient Wireless Signal Classification","Thanks to its capability of classifying complex phenomena without explicit
modeling, deep learning (DL) has been demonstrated to be a key enabler of
Wireless Signal Classification (WSC). Although DL can achieve a very high
accuracy under certain conditions, recent research has unveiled that the
wireless channel can disrupt the features learned by the DL model during
training, thus drastically reducing the classification performance in
real-world live settings. Since retraining classifiers is cumbersome after
deployment, existing work has leveraged the usage of carefully-tailored Finite
Impulse Response (FIR) filters that, when applied at the transmitter's side,
can restore the features that are lost because of the the channel actions,
i.e., waveform synthesis. However, these approaches compute FIRs using offline
optimization strategies, which limits their efficacy in highly-dynamic channel
settings. In this paper, we improve the state of the art by proposing Chares, a
Deep Reinforcement Learning (DRL)-based framework for channel-resilient
adaptive waveform synthesis. Chares adapts to new and unseen channel conditions
by optimally computing through DRL the FIRs in real-time. Chares is a DRL agent
whose architecture is-based upon the Twin Delayed Deep Deterministic Policy
Gradients (TD3), which requires minimal feedback from the receiver and explores
a continuous action space. Chares has been extensively evaluated on two
well-known datasets. We have also evaluated the real-time latency of Chares
with an implementation on field-programmable gate array (FPGA). Results show
that Chares increases the accuracy up to 4.1x when no waveform synthesis is
performed, by 1.9x with respect to existing work, and can compute new actions
within 41us.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2103.00740v3,2021-03-03T03:26:02Z,2021-03-01T04:13:21Z,"Towards Enhancing Database Education: Natural Language Generation Meets
  Query Execution Plans","The database systems course is offered as part of an undergraduate computer
science degree program in many major universities. A key learning goal of
learners taking such a course is to understand how SQL queries are processed in
a RDBMS in practice. Since a query execution plan (QEP) describes the execution
steps of a query, learners can acquire the understanding by perusing the QEPs
generated by a RDBMS. Unfortunately, in practice, it is often daunting for a
learner to comprehend these QEPs containing vendor-specific implementation
details, hindering her learning process. In this paper, we present a novel,
end-to-end, generic system called lantern that generates a natural language
description of a qep to facilitate understanding of the query execution steps.
It takes as input an SQL query and its QEP, and generates a natural language
description of the execution strategy deployed by the underlying RDBMS.
Specifically, it deploys a declarative framework called pool that enables
subject matter experts to efficiently create and maintain natural language
descriptions of physical operators used in QEPs. A rule-based framework called
RULE-LANTERN is proposed that exploits pool to generate natural language
descriptions of QEPs. Despite the high accuracy of RULE-LANTERN, our engagement
with learners reveal that, consistent with existing psychology theories,
perusing such rule-based descriptions lead to boredom due to repetitive
statements across different QEPs. To address this issue, we present a novel
deep learning-based language generation framework called NEURAL-LANTERN that
infuses language variability in the generated description by exploiting a set
of paraphrasing tools and word embedding. Our experimental study with real
learners shows the effectiveness of lantern in facilitating comprehension of
QEPs.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.12567v1,2021-02-24T21:34:40Z,2021-02-24T21:34:40Z,"Sketching Curvature for Efficient Out-of-Distribution Detection for Deep
  Neural Networks","In order to safely deploy Deep Neural Networks (DNNs) within the perception
pipelines of real-time decision making systems, there is a need for safeguards
that can detect out-of-training-distribution (OoD) inputs both efficiently and
accurately. Building on recent work leveraging the local curvature of DNNs to
reason about epistemic uncertainty, we propose Sketching Curvature of OoD
Detection (SCOD), an architecture-agnostic framework for equipping any trained
DNN with a task-relevant epistemic uncertainty estimate. Offline, given a
trained model and its training data, SCOD employs tools from matrix sketching
to tractably compute a low-rank approximation of the Fisher information matrix,
which characterizes which directions in the weight space are most influential
on the predictions over the training data. Online, we estimate uncertainty by
measuring how much perturbations orthogonal to these directions can alter
predictions at a new test input. We apply SCOD to pre-trained networks of
varying architectures on several tasks, ranging from regression to
classification. We demonstrate that SCOD achieves comparable or better OoD
detection performance with lower computational burden relative to existing
baselines.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.10477v1,2021-02-20T23:45:24Z,2021-02-20T23:45:24Z,"Neural Sampling Machine with Stochastic Synapse allows Brain-like
  Learning and Inference","Many real-world mission-critical applications require continual online
learning from noisy data and real-time decision making with a defined
confidence level. Probabilistic models and stochastic neural networks can
explicitly handle uncertainty in data and allow adaptive learning-on-the-fly,
but their implementation in a low-power substrate remains a challenge. Here, we
introduce a novel hardware fabric that implements a new class of stochastic NN
called Neural-Sampling-Machine that exploits stochasticity in synaptic
connections for approximate Bayesian inference. Harnessing the inherent
non-linearities and stochasticity occurring at the atomic level in emerging
materials and devices allows us to capture the synaptic stochasticity occurring
at the molecular level in biological synapses. We experimentally demonstrate
in-silico hybrid stochastic synapse by pairing a ferroelectric field-effect
transistor -based analog weight cell with a two-terminal stochastic selector
element. Such a stochastic synapse can be integrated within the
well-established crossbar array architecture for compute-in-memory. We
experimentally show that the inherent stochastic switching of the selector
element between the insulator and metallic state introduces a multiplicative
stochastic noise within the synapses of NSM that samples the conductance states
of the FeFET, both during learning and inference. We perform network-level
simulations to highlight the salient automatic weight normalization feature
introduced by the stochastic synapses of the NSM that paves the way for
continual online learning without any offline Batch Normalization. We also
showcase the Bayesian inferencing capability introduced by the stochastic
synapse during inference mode, thus accounting for uncertainty in data. We
report 98.25%accuracy on standard image classification task as well as
estimation of data uncertainty in rotated samples.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.10398v3,2021-02-28T00:47:42Z,2021-02-20T17:35:23Z,All-Chalcogenide Programmable All-Optical Deep Neural Networks,"Deeplearning algorithms are revolutionising many aspects of modern life.
Typically, they are implemented in CMOS-based hardware with severely limited
memory access times and inefficient data-routing. All-optical neural networks
without any electro-optic conversions could alleviate these shortcomings.
However, an all-optical nonlinear activation function, which is a vital
building block for optical neural networks, needs to be developed efficiently
on-chip. Here, we introduce and demonstrate both optical synapse weighting and
all-optical nonlinear thresholding using two different effects in a
chalcogenide material photonic platform. We show how the structural phase
transitions in a wide-bandgap phase-change material enables storing the neural
network weights via non-volatile photonic memory, whilst resonant bond
destabilisation is used as a nonlinear activation threshold without changing
the material. These two different transitions within chalcogenides enable
programmable neural networks with near-zero static power consumption once
trained, in addition to picosecond delays performing inference tasks not
limited by wire charging that limit electrical circuits; for instance, we show
that nanosecond-order weight programming and near-instantaneous weight updates
enable accurate inference tasks within 20 picoseconds in a 3-layer all-optical
neural network. Optical neural networks that bypass electro-optic conversion
altogether hold promise for network-edge machine learning applications where
decision-making in real-time are critical, such as for autonomous vehicles or
navigation systems such as signal pre-processing of LIDAR systems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.11026v1,2021-02-19T02:30:14Z,2021-02-19T02:30:14Z,High-order Differentiable Autoencoder for Nonlinear Model Reduction,"This paper provides a new avenue for exploiting deep neural networks to
improve physics-based simulation. Specifically, we integrate the classic
Lagrangian mechanics with a deep autoencoder to accelerate elastic simulation
of deformable solids. Due to the inertia effect, the dynamic equilibrium cannot
be established without evaluating the second-order derivatives of the deep
autoencoder network. This is beyond the capability of off-the-shelf automatic
differentiation packages and algorithms, which mainly focus on the gradient
evaluation. Solving the nonlinear force equilibrium is even more challenging if
the standard Newton's method is to be used. This is because we need to compute
a third-order derivative of the network to obtain the variational Hessian. We
attack those difficulties by exploiting complex-step finite difference, coupled
with reverse automatic differentiation. This strategy allows us to enjoy the
convenience and accuracy of complex-step finite difference and in the meantime,
to deploy complex-value perturbations as collectively as possible to save
excessive network passes. With a GPU-based implementation, we are able to wield
deep autoencoders (e.g., $10+$ layers) with a relatively high-dimension latent
space in real-time. Along this pipeline, we also design a sampling network and
a weighting network to enable \emph{weight-varying} Cubature integration in
order to incorporate nonlinearity in the model reduction. We believe this work
will inspire and benefit future research efforts in nonlinearly reduced
physical simulation problems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.09548v2,2021-08-28T19:59:03Z,2021-02-18T18:50:31Z,"Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug
  Discovery and Development","Therapeutics machine learning is an emerging field with incredible
opportunities for innovatiaon and impact. However, advancement in this field
requires formulation of meaningful learning tasks and careful curation of
datasets. Here, we introduce Therapeutics Data Commons (TDC), the first
unifying platform to systematically access and evaluate machine learning across
the entire range of therapeutics. To date, TDC includes 66 AI-ready datasets
spread across 22 learning tasks and spanning the discovery and development of
safe and effective medicines. TDC also provides an ecosystem of tools and
community resources, including 33 data functions and types of meaningful data
splits, 23 strategies for systematic model evaluation, 17 molecule generation
oracles, and 29 public leaderboards. All resources are integrated and
accessible via an open Python library. We carry out extensive experiments on
selected datasets, demonstrating that even the strongest algorithms fall short
of solving key therapeutics challenges, including real dataset distributional
shifts, multi-scale modeling of heterogeneous data, and robust generalization
to novel data points. We envision that TDC can facilitate algorithmic and
scientific advances and considerably accelerate machine-learning model
development, validation and transition into biomedical and clinical
implementation. TDC is an open-science initiative available at
https://tdcommons.ai.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.06336v1,2021-02-12T03:07:06Z,2021-02-12T03:07:06Z,"Dancing along Battery: Enabling Transformer with Run-time
  Reconfigurability on Mobile Devices","A pruning-based AutoML framework for run-time reconfigurability, namely RT3,
is proposed in this work. This enables Transformer-based large Natural Language
Processing (NLP) models to be efficiently executed on resource-constrained
mobile devices and reconfigured (i.e., switching models for dynamic hardware
conditions) at run-time. Such reconfigurability is the key to save energy for
battery-powered mobile devices, which widely use dynamic voltage and frequency
scaling (DVFS) technique for hardware reconfiguration to prolong battery life.
In this work, we creatively explore a hybrid block-structured pruning (BP) and
pattern pruning (PP) for Transformer-based models and first attempt to combine
hardware and software reconfiguration to maximally save energy for
battery-powered mobile devices. Specifically, RT3 integrates two-level
optimizations: First, it utilizes an efficient BP as the first-step compression
for resource-constrained mobile devices; then, RT3 heuristically generates a
shrunken search space based on the first level optimization and searches
multiple pattern sets with diverse sparsity for PP via reinforcement learning
to support lightweight software reconfiguration, which corresponds to available
frequency levels of DVFS (i.e., hardware reconfiguration). At run-time, RT3 can
switch the lightweight pattern sets within 45ms to guarantee the required
real-time constraint at different frequency levels. Results further show that
RT3 can prolong battery life over 4x improvement with less than 1% accuracy
loss for Transformer and 1.5% score decrease for DistilBERT.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.05334v2,2021-09-02T07:50:28Z,2021-02-10T09:16:09Z,"Enhancing Real-World Adversarial Patches through 3D Modeling of Complex
  Target Scenes","Adversarial examples have proven to be a concerning threat to deep learning
models, particularly in the image domain. However, while many studies have
examined adversarial examples in the real world, most of them relied on 2D
photos of the attack scene. As a result, the attacks proposed may have limited
effectiveness when implemented in realistic environments with 3D objects or
varied conditions. There are few studies on adversarial learning that use 3D
objects, and in many cases, other researchers are unable to replicate the
real-world evaluation process. In this study, we present a framework that uses
3D modeling to craft adversarial patches for an existing real-world scene. Our
approach uses a 3D digital approximation of the scene as a simulation of the
real world. With the ability to add and manipulate any element in the digital
scene, our framework enables the attacker to improve the adversarial patch's
impact in real-world settings. We use the framework to create a patch for an
everyday scene and evaluate its performance using a novel evaluation process
that ensures that our results are reproducible in both the digital space and
the real world. Our evaluation results show that the framework can generate
adversarial patches that are robust to different settings in the real world.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.04871v1,2021-02-09T15:14:27Z,2021-02-09T15:14:27Z,The Factory Must Grow: Automation in Factorio,"Efficient optimization of resources is paramount to success in many problems
faced today. In the field of operational research the efficient scheduling of
employees; packing of vans; routing of vehicles; logistics of airlines and
transport of materials can be the difference between emission reduction or
excess, profits or losses and feasibility or unworkable solutions. The video
game Factorio, by Wube Software, has a myriad of problems which are analogous
to such real-world problems, and is a useful simulator for developing solutions
for these problems. In this paper we define the logistic transport belt problem
and define mathematical integer programming model of it. We developed an
interface to allow optimizers in any programming language to interact with
Factorio, and we provide an initial benchmark of logistic transport belt
problems. We present results for Simulated Annealing, quick Genetic Programming
and Evolutionary Reinforcement Learning, three different meta-heuristic
techniques to optimize this novel problem.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.03616v1,2021-02-06T17:11:09Z,2021-02-06T17:11:09Z,"A Data Augmented Bayesian Network for Node Failure Prediction in Optical
  Networks","Failures in optical network backbone can cause significant interruption in
internet data traffic. Hence, it is very important to reduce such network
outages. Prediction of such failures would be a step forward to avoid such
disruption of internet services for users as well as operators. Several
research proposals are available in the literature which are applications of
data science and machine learning techniques. Most of the techniques rely on
significant amount of real time data collection. Network devices are assumed to
be equipped to collect data and these are then analysed by different algorithms
to predict failures. Every network element which is already deployed in the
field may not have these data gathering or analysis techniques designed into
them initially. However, such mechanisms become necessary later when they are
already deployed in the field. This paper proposes a Bayesian network based
failure prediction of network nodes, e.g., routers etc., using very basic
information from the log files of the devices and applying power law based data
augmentation to complement for scarce real time information. Numerical results
show that network node failure prediction can be performed with high accuracy
using the proposed mechanism.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2102.10997v1,2021-02-03T10:52:02Z,2021-02-03T10:52:02Z,"Trust Computational Heuristic for Social Internet of Things: A Machine
  Learning-based Approach","The Internet of Things (IoT) is an evolving network of billions of
interconnected physical objects, such as numerous sensors, smartphones,
wearables, and embedded devices. These physical objects, generally referred to
as the smart objects, when deployed in the real-world aggregates useful
information from their surrounding environment. As-of-late, this notion of IoT
has been extended to incorporate the social networking facets which have led to
the promising paradigm of the `Social Internet of Things' (SIoT). In SIoT, the
devices operate as an autonomous agent and provide an exchange of information
and service discovery in an intelligent manner by establishing social
relationships among them with respect to their owners. Trust plays an important
role in establishing trustworthy relationships among the physical objects and
reduces probable risks in the decision-making process. In this paper, a trust
computational model is proposed to extract individual trust features in a SIoT
environment. Furthermore, a machine learning-based heuristic is used to
aggregate all the trust features in order to ascertain an aggregate trust
score. Simulation results illustrate that the proposed trust-based model
isolates the trustworthy and untrustworthy nodes within the network in an
efficient manner.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.10955v2,2021-11-14T17:09:25Z,2021-01-26T17:23:46Z,"RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated
  Content","Blind or no-reference video quality assessment of user-generated content
(UGC) has become a trending, challenging, heretofore unsolved problem. Accurate
and efficient video quality predictors suitable for this content are thus in
great demand to achieve more intelligent analysis and processing of UGC videos.
Previous studies have shown that natural scene statistics and deep learning
features are both sufficient to capture spatial distortions, which contribute
to a significant aspect of UGC video quality issues. However, these models are
either incapable or inefficient for predicting the quality of complex and
diverse UGC videos in practical applications. Here we introduce an effective
and efficient video quality model for UGC content, which we dub the Rapid and
Accurate Video Quality Evaluator (RAPIQUE), which we show performs comparably
to state-of-the-art (SOTA) models but with orders-of-magnitude faster runtime.
RAPIQUE combines and leverages the advantages of both quality-aware scene
statistics features and semantics-aware deep convolutional features, allowing
us to design the first general and efficient spatial and temporal (space-time)
bandpass statistics model for video quality modeling. Our experimental results
on recent large-scale UGC video quality databases show that RAPIQUE delivers
top performances on all the datasets at a considerably lower computational
expense. We hope this work promotes and inspires further efforts towards
practical modeling of video quality problems for potential real-time and
low-latency applications. To promote public usage, an implementation of RAPIQUE
has been made freely available online: \url{https://github.com/vztu/RAPIQUE}.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.09577v1,2021-01-23T20:23:31Z,2021-01-23T20:23:31Z,"ReliefE: Feature Ranking in High-dimensional Spaces via Manifold
  Embeddings","Feature ranking has been widely adopted in machine learning applications such
as high-throughput biology and social sciences. The approaches of the popular
Relief family of algorithms assign importances to features by iteratively
accounting for nearest relevant and irrelevant instances. Despite their high
utility, these algorithms can be computationally expensive and not-well suited
for high-dimensional sparse input spaces. In contrast, recent embedding-based
methods learn compact, low-dimensional representations, potentially
facilitating down-stream learning capabilities of conventional learners. This
paper explores how the Relief branch of algorithms can be adapted to benefit
from (Riemannian) manifold-based embeddings of instance and target spaces,
where a given embedding's dimensionality is intrinsic to the dimensionality of
the considered data set. The developed ReliefE algorithm is faster and can
result in better feature rankings, as shown by our evaluation on 20 real-life
data sets for multi-class and multi-label classification tasks. The utility of
ReliefE for high-dimensional data sets is ensured by its implementation that
utilizes sparse matrix algebraic operations. Finally, the relation of ReliefE
to other ranking algorithms is studied via the Fuzzy Jaccard Index.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.09336v1,2021-01-22T21:13:46Z,2021-01-22T21:13:46Z,A Comprehensive Survey on Hardware-Aware Neural Architecture Search,"Neural Architecture Search (NAS) methods have been growing in popularity.
These techniques have been fundamental to automate and speed up the time
consuming and error-prone process of synthesizing novel Deep Learning (DL)
architectures. NAS has been extensively studied in the past few years. Arguably
their most significant impact has been in image classification and object
detection tasks where the state of the art results have been obtained. Despite
the significant success achieved to date, applying NAS to real-world problems
still poses significant challenges and is not widely practical. In general, the
synthesized Convolution Neural Network (CNN) architectures are too complex to
be deployed in resource-limited platforms, such as IoT, mobile, and embedded
systems. One solution growing in popularity is to use multi-objective
optimization algorithms in the NAS search strategy by taking into account
execution latency, energy consumption, memory footprint, etc. This kind of NAS,
called hardware-aware NAS (HW-NAS), makes searching the most efficient
architecture more complicated and opens several questions.
  In this survey, we provide a detailed review of existing HW-NAS research and
categorize them according to four key dimensions: the search space, the search
strategy, the acceleration technique, and the hardware cost estimation
strategies. We further discuss the challenges and limitations of existing
approaches and potential future directions. This is the first survey paper
focusing on hardware-aware NAS. We hope it serves as a valuable reference for
the various techniques and algorithms discussed and paves the road for future
research towards hardware-aware NAS.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.06582v1,2021-01-17T03:45:25Z,2021-01-17T03:45:25Z,"Tailored Learning-Based Scheduling for Kubernetes-Oriented Edge-Cloud
  System","Kubernetes (k8s) has the potential to merge the distributed edge and the
cloud but lacks a scheduling framework specifically for edge-cloud systems.
Besides, the hierarchical distribution of heterogeneous resources and the
complex dependencies among requests and resources make the modeling and
scheduling of k8s-oriented edge-cloud systems particularly sophisticated. In
this paper, we introduce KaiS, a learning-based scheduling framework for such
edge-cloud systems to improve the long-term throughput rate of request
processing. First, we design a coordinated multi-agent actor-critic algorithm
to cater to decentralized request dispatch and dynamic dispatch spaces within
the edge cluster. Second, for diverse system scales and structures, we use
graph neural networks to embed system state information, and combine the
embedding results with multiple policy networks to reduce the orchestration
dimensionality by stepwise scheduling. Finally, we adopt a two-time-scale
scheduling mechanism to harmonize request dispatch and service orchestration,
and present the implementation design of deploying the above algorithms
compatible with native k8s components. Experiments using real workload traces
show that KaiS can successfully learn appropriate scheduling policies,
irrespective of request arrival patterns and system scales. Moreover, KaiS can
enhance the average system throughput rate by 14.3% while reducing scheduling
cost by 34.7% compared to baselines.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.06448v4,2022-02-27T04:35:40Z,2021-01-16T14:20:32Z,"Self-Supervised Multi-Channel Hypergraph Convolutional Network for
  Social Recommendation","Social relations are often used to improve recommendation quality when
user-item interaction data is sparse in recommender systems. Most existing
social recommendation models exploit pairwise relations to mine potential user
preferences. However, real-life interactions among users are very complicated
and user relations can be high-order. Hypergraph provides a natural way to
model complex high-order relations, while its potentials for improving social
recommendation are under-explored. In this paper, we fill this gap and propose
a multi-channel hypergraph convolutional network to enhance social
recommendation by leveraging high-order user relations. Technically, each
channel in the network encodes a hypergraph that depicts a common high-order
user relation pattern via hypergraph convolution. By aggregating the embeddings
learned through multiple channels, we obtain comprehensive user representations
to generate recommendation results. However, the aggregation operation might
also obscure the inherent characteristics of different types of high-order
connectivity information. To compensate for the aggregating loss, we
innovatively integrate self-supervised learning into the training of the
hypergraph convolutional network to regain the connectivity information with
hierarchical mutual information maximization. The experimental results on
multiple real-world datasets show that the proposed model outperforms the SOTA
methods, and the ablation study verifies the effectiveness of the multi-channel
setting and the self-supervised task. The implementation of our model is
available via https://github.com/Coder-Yu/RecQ.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.04086v1,2021-01-11T18:29:50Z,2021-01-11T18:29:50Z,"System Design for a Data-driven and Explainable Customer Sentiment
  Monitor","The most important goal of customer services is to keep the customer
satisfied. However, service resources are always limited and must be
prioritized. Therefore, it is important to identify customers who potentially
become unsatisfied and might lead to escalations. Today this prioritization of
customers is often done manually. Data science on IoT data (esp. log data) for
machine health monitoring, as well as analytics on enterprise data for customer
relationship management (CRM) have mainly been researched and applied
independently. In this paper, we present a framework for a data-driven decision
support system which combines IoT and enterprise data to model customer
sentiment. Such decision support systems can help to prioritize customers and
service resources to effectively troubleshoot problems or even avoid them. The
framework is applied in a real-world case study with a major medical device
manufacturer. This includes a fully automated and interpretable machine
learning pipeline designed to meet the requirements defined with domain experts
and end users. The overall framework is currently deployed, learns and
evaluates predictive models from terabytes of IoT and enterprise data to
actively monitor the customer sentiment for a fleet of thousands of high-end
medical devices. Furthermore, we provide an anonymized industrial benchmark
dataset for the research community.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.02780v1,2021-01-07T22:01:30Z,2021-01-07T22:01:30Z,"SHARKS: Smart Hacking Approaches for RisK Scanning in Internet-of-Things
  and Cyber-Physical Systems based on Machine Learning","Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are
increasingly being deployed across multiple functionalities, ranging from
healthcare devices and wearables to critical infrastructures, e.g., nuclear
power plants, autonomous vehicles, smart cities, and smart homes. These devices
are inherently not secure across their comprehensive software, hardware, and
network stacks, thus presenting a large attack surface that can be exploited by
hackers. In this article, we present an innovative technique for detecting
unknown system vulnerabilities, managing these vulnerabilities, and improving
incident response when such vulnerabilities are exploited. The novelty of this
approach lies in extracting intelligence from known real-world CPS/IoT attacks,
representing them in the form of regular expressions, and employing machine
learning (ML) techniques on this ensemble of regular expressions to generate
new attack vectors and security vulnerabilities. Our results show that 10 new
attack vectors and 122 new vulnerability exploits can be successfully generated
that have the potential to exploit a CPS or an IoT ecosystem. The ML
methodology achieves an accuracy of 97.4% and enables us to predict these
attacks efficiently with an 87.2% reduction in the search space. We demonstrate
the application of our method to the hacking of the in-vehicle network of a
connected car. To defend against the known attacks and possible novel exploits,
we discuss a defense-in-depth mechanism for various classes of attacks and the
classification of data targeted by such attacks. This defense mechanism
optimizes the cost of security measures based on the sensitivity of the
protected resource, thus incentivizing its adoption in real-world CPS/IoT by
cybersecurity practitioners.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.02000v1,2021-01-06T13:15:21Z,2021-01-06T13:15:21Z,Weakly-Supervised Multi-Face 3D Reconstruction,"3D face reconstruction plays a very important role in many real-world
multimedia applications, including digital entertainment, social media,
affection analysis, and person identification. The de-facto pipeline for
estimating the parametric face model from an image requires to firstly detect
the facial regions with landmarks, and then crop each face to feed the deep
learning-based regressor. Comparing to the conventional methods performing
forward inference for each detected instance independently, we suggest an
effective end-to-end framework for multi-face 3D reconstruction, which is able
to predict the model parameters of multiple instances simultaneously using
single network inference. Our proposed approach not only greatly reduces the
computational redundancy in feature extraction but also makes the deployment
procedure much easier using the single network model. More importantly, we
employ the same global camera model for the reconstructed faces in each image,
which makes it possible to recover the relative head positions and orientations
in the 3D scene. We have conducted extensive experiments to evaluate our
proposed approach on the sparse and dense face alignment tasks. The
experimental results indicate that our proposed approach is very promising on
face alignment tasks without fully-supervision and pre-processing like
detection and crop. Our implementation is publicly available at
\url{https://github.com/kalyo-zjl/WM3DR}.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.15823v2,2021-03-29T23:48:56Z,2020-12-31T18:48:58Z,Binary Graph Neural Networks,"Graph Neural Networks (GNNs) have emerged as a powerful and flexible
framework for representation learning on irregular data. As they generalize the
operations of classical CNNs on grids to arbitrary topologies, GNNs also bring
much of the implementation challenges of their Euclidean counterparts. Model
size, memory footprint, and energy consumption are common concerns for many
real-world applications. Network binarization allocates a single bit to
parameters and activations, thus dramatically reducing the memory requirements
(up to 32x compared to single-precision floating-point numbers) and maximizing
the benefits of fast SIMD instructions on modern hardware for measurable
speedups. However, in spite of the large body of work on binarization for
classical CNNs, this area remains largely unexplored in geometric deep
learning. In this paper, we present and evaluate different strategies for the
binarization of graph neural networks. We show that through careful design of
the models, and control of the training process, binary graph neural networks
can be trained at only a moderate cost in accuracy on challenging benchmarks.
In particular, we present the first dynamic graph neural network in Hamming
space, able to leverage efficient k-NN search on binary vectors to speed-up the
construction of the dynamic graph. We further verify that the binary models
offer significant savings on embedded devices. Our code is publicly available
on Github.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.15005v2,2021-05-30T01:31:39Z,2020-12-30T02:03:25Z,"Infer-AVAE: An Attribute Inference Model Based on Adversarial
  Variational Autoencoder","User attributes, such as gender and education, face severe incompleteness in
social networks. In order to make this kind of valuable data usable for
downstream tasks like user profiling and personalized recommendation, attribute
inference aims to infer users' missing attribute labels based on observed data.
Recently, variational autoencoder (VAE), an end-to-end deep generative model,
has shown promising performance by handling the problem in a semi-supervised
way. However, VAEs can easily suffer from over-fitting and over-smoothing when
applied to attribute inference. To be specific, VAE implemented with
multi-layer perceptron (MLP) can only reconstruct input data but fail in
inferring missing parts. While using the trending graph neural networks (GNNs)
as encoder has the problem that GNNs aggregate redundant information from
neighborhood and generate indistinguishable user representations, which is
known as over-smoothing. In this paper, we propose an attribute
\textbf{Infer}ence model based on \textbf{A}dversarial \textbf{VAE}
(Infer-AVAE) to cope with these issues. Specifically, to overcome
over-smoothing, Infer-AVAE unifies MLP and GNNs in encoder to learn positive
and negative latent representations respectively. Meanwhile, an adversarial
network is trained to distinguish the two representations and GNNs are trained
to aggregate less noise for more robust representations through adversarial
training. Finally, to relieve over-fitting, mutual information constraint is
introduced as a regularizer for decoder, so that it can make better use of
auxiliary information in representations and generate outputs not limited by
observations. We evaluate our model on 4 real-world social network datasets,
experimental results demonstrate that our model averagely outperforms baselines
by 7.0$\%$ in accuracy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.13968v1,2020-12-27T16:03:32Z,2020-12-27T16:03:32Z,"Detecting Medical Misinformation on Social Media Using Multimodal Deep
  Learning","In 2019, outbreaks of vaccine-preventable diseases reached the highest number
in the US since 1992. Medical misinformation, such as antivaccine content
propagating through social media, is associated with increases in vaccine delay
and refusal. Our overall goal is to develop an automatic detector for
antivaccine messages to counteract the negative impact that antivaccine
messages have on the public health. Very few extant detection systems have
considered multimodality of social media posts (images, texts, and hashtags),
and instead focus on textual components, despite the rapid growth of
photo-sharing applications (e.g., Instagram). As a result, existing systems are
not sufficient for detecting antivaccine messages with heavy visual components
(e.g., images) posted on these newer platforms. To solve this problem, we
propose a deep learning network that leverages both visual and textual
information. A new semantic- and task-level attention mechanism was created to
help our model to focus on the essential contents of a post that signal
antivaccine messages. The proposed model, which consists of three branches, can
generate comprehensive fused features for predictions. Moreover, an ensemble
method is proposed to further improve the final prediction accuracy. To
evaluate the proposed model's performance, a real-world social media dataset
that consists of more than 30,000 samples was collected from Instagram between
January 2016 and October 2019. Our 30 experiment results demonstrate that the
final network achieves above 97% testing accuracy and outperforms other
relevant models, demonstrating that it can detect a large amount of antivaccine
messages posted daily. The implementation code is available at
https://github.com/wzhings/antivaccine_detection.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.15728v1,2020-12-27T06:13:29Z,2020-12-27T06:13:29Z,"Multi-Channel Sequential Behavior Networks for User Modeling in Online
  Advertising","Multiple content providers rely on native advertisement for revenue by
placing ads within the organic content of their pages. We refer to this setting
as ``queryless'' to differentiate from search advertisement where a user
submits a search query and gets back related ads. Understanding user intent is
critical because relevant ads improve user experience and increase the
likelihood of delivering clicks that have value to our advertisers.
  This paper presents Multi-Channel Sequential Behavior Network (MC-SBN), a
deep learning approach for embedding users and ads in a semantic space in which
relevance can be evaluated. Our proposed user encoder architecture summarizes
user activities from multiple input channels--such as previous search queries,
visited pages, or clicked ads--into a user vector. It uses multiple RNNs to
encode sequences of event sessions from the different channels and then applies
an attention mechanism to create the user representation. A key property of our
approach is that user vectors can be maintained and updated incrementally,
which makes it feasible to be deployed for large-scale serving. We conduct
extensive experiments on real-world datasets. The results demonstrate that
MC-SBN can improve the ranking of relevant ads and boost the performance of
both click prediction and conversion prediction in the queryless native
advertising setting.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.12305v2,2021-07-22T16:53:43Z,2020-12-22T19:27:11Z,"Confronting Abusive Language Online: A Survey from the Ethical and Human
  Rights Perspective","The pervasiveness of abusive content on the internet can lead to severe
psychological and physical harm. Significant effort in Natural Language
Processing (NLP) research has been devoted to addressing this problem through
abusive content detection and related sub-areas, such as the detection of hate
speech, toxicity, cyberbullying, etc. Although current technologies achieve
high classification performance in research studies, it has been observed that
the real-life application of this technology can cause unintended harms, such
as the silencing of under-represented groups. We review a large body of NLP
research on automatic abuse detection with a new focus on ethical challenges,
organized around eight established ethical principles: privacy, accountability,
safety and security, transparency and explainability, fairness and
non-discrimination, human control of technology, professional responsibility,
and promotion of human values. In many cases, these principles relate not only
to situational ethical codes, which may be context-dependent, but are in fact
connected to universal human rights, such as the right to privacy, freedom from
discrimination, and freedom of expression. We highlight the need to examine the
broad social impacts of this technology, and to bring ethical and human rights
considerations to every stage of the application life-cycle, from task
formulation and dataset design, to model training and evaluation, to
application deployment. Guided by these principles, we identify several
opportunities for rights-respecting, socio-technical solutions to detect and
confront online abuse, including `nudging', `quarantining', value sensitive
design, counter-narratives, style transfer, and AI-driven public education
applications.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.12142v1,2020-12-22T16:25:12Z,2020-12-22T16:25:12Z,High-Speed Robot Navigation using Predicted Occupancy Maps,"Safe and high-speed navigation is a key enabling capability for real world
deployment of robotic systems. A significant limitation of existing approaches
is the computational bottleneck associated with explicit mapping and the
limited field of view (FOV) of existing sensor technologies. In this paper, we
study algorithmic approaches that allow the robot to predict spaces extending
beyond the sensor horizon for robust planning at high speeds. We accomplish
this using a generative neural network trained from real-world data without
requiring human annotated labels. Further, we extend our existing control
algorithms to support leveraging the predicted spaces to improve collision-free
planning and navigation at high speeds. Our experiments are conducted on a
physical robot based on the MIT race car using an RGBD sensor where were able
to demonstrate improved performance at 4 m/s compared to a controller not
operating on predicted regions of the map.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.14325v1,2020-12-22T09:54:04Z,2020-12-22T09:54:04Z,Digital me ontology and ethics,"This paper addresses ontology and ethics of an AI agent called digital me. We
define digital me as autonomous, decision-making, and learning agent,
representing an individual and having practically immortal own life. It is
assumed that digital me is equipped with the big-five personality model,
ensuring that it provides a model of some aspects of a strong AI:
consciousness, free will, and intentionality. As computer-based personality
judgments are more accurate than those made by humans, digital me can judge the
personality of the individual represented by the digital me, other individuals'
personalities, and other digital me-s. We describe seven ontological qualities
of digital me: a) double-layer status of Digital Being versus digital me, b)
digital me versus real me, c) mind-digital me and body-digital me, d) digital
me versus doppelganger (shadow digital me), e) non-human time concept, f)
social quality, g) practical immortality. We argue that with the advancement of
AI's sciences and technologies, there exist two digital me thresholds. The
first threshold defines digital me having some (rudimentarily) form of
consciousness, free will, and intentionality. The second threshold assumes that
digital me is equipped with moral learning capabilities, implying that, in
principle, digital me could develop their own ethics which significantly
differs from human's understanding of ethics. Finally we discuss the
implications of digital me metaethics, normative and applied ethics, the
implementation of the Golden Rule in digital me-s, and we suggest two sets of
normative principles for digital me: consequentialist and duty based digital me
principles.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.11696v2,2021-06-19T00:16:56Z,2020-12-21T21:48:18Z,"Image Captioning as an Assistive Technology: Lessons Learned from VizWiz
  2020 Challenge","Image captioning has recently demonstrated impressive progress largely owing
to the introduction of neural network algorithms trained on curated dataset
like MS-COCO. Often work in this field is motivated by the promise of
deployment of captioning systems in practical applications. However, the
scarcity of data and contexts in many competition datasets renders the utility
of systems trained on these datasets limited as an assistive technology in
real-world settings, such as helping visually impaired people navigate and
accomplish everyday tasks. This gap motivated the introduction of the novel
VizWiz dataset, which consists of images taken by the visually impaired and
captions that have useful, task-oriented information. In an attempt to help the
machine learning computer vision field realize its promise of producing
technologies that have positive social impact, the curators of the VizWiz
dataset host several competitions, including one for image captioning. This
work details the theory and engineering from our winning submission to the 2020
captioning competition. Our work provides a step towards improved assistive
image captioning systems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2101.03889v2,2021-02-16T13:44:56Z,2020-12-21T10:25:27Z,A Comprehensive Survey of 6G Wireless Communications,"While fifth-generation (5G) communications are being rolled out worldwide,
sixth-generation (6G) communications have attracted much attention from both
the industry and the academia. Compared with 5G, 6G will have a wider frequency
band, higher transmission rate, spectrum efficiency, greater connection
capacity, shorter delay, broader coverage, and more robust anti-interference
capability to satisfy various network requirements. This survey presents an
insightful understanding of 6G wireless communications by introducing
requirements, features, critical technologies, challenges, and applications.
First, we give an overview of 6G from perspectives of technologies, security
and privacy, and applications. Subsequently, we introduce various 6G
technologies and their existing challenges in detail, e.g., artificial
intelligence (AI), intelligent surfaces, THz, space-air-ground-sea integrated
network, cell-free massive MIMO, etc. Because of these technologies, 6G is
expected to outperform existing wireless communication systems regarding the
transmission rate, latency, global coverage, etc. Next, we discuss security and
privacy techniques that can be applied to protect data in 6G. Since edge
devices are expected to gain popularity soon, the vast amount of generated data
and frequent data exchange make the leakage of data easily. Finally, we predict
real-world applications built on the technologies and features of 6G; for
example, smart healthcare, smart city, and smart manufacturing will be
implemented by taking advantage of AI.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.10943v2,2021-10-31T11:28:09Z,2020-12-20T14:52:57Z,"Trace-class Gaussian priors for Bayesian learning of neural networks
  with MCMC","This paper introduces a new neural network based prior for real valued
functions on $\mathbb R^d$ which, by construction, is more easily and cheaply
scaled up in the domain dimension $d$ compared to the usual Karhunen-Lo\`eve
function space prior. The new prior is a Gaussian neural network prior, where
each weight and bias has an independent Gaussian prior, but with the key
difference that the variances decrease in the width of the network in such a
way that the resulting function is almost surely well defined in the limit of
an infinite width network. We show that in a Bayesian treatment of inferring
unknown functions, the induced posterior over functions is amenable to Monte
Carlo sampling using Hilbert space Markov chain Monte Carlo (MCMC) methods.
This type of MCMC is popular, e.g. in the Bayesian Inverse Problems literature,
because it is stable under mesh refinement, i.e. the acceptance probability
does not shrink to $0$ as more parameters of the function's prior are
introduced, even ad infinitum. In numerical examples we demonstrate these
stated competitive advantages over other function space priors. We also
implement examples in Bayesian Reinforcement Learning to automate tasks from
data and demonstrate, for the first time, stability of MCMC to mesh refinement
for these type of problems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.10610v3,2021-02-16T17:31:15Z,2020-12-19T07:00:09Z,"SpaceML: Distributed Open-source Research with Citizen Scientists for
  the Advancement of Space Technology for NASA","Traditionally, academic labs conduct open-ended research with the primary
focus on discoveries with long-term value, rather than direct products that can
be deployed in the real world. On the other hand, research in the industry is
driven by its expected commercial return on investment, and hence focuses on a
real world product with short-term timelines. In both cases, opportunity is
selective, often available to researchers with advanced educational
backgrounds. Research often happens behind closed doors and may be kept
confidential until either its publication or product release, exacerbating the
problem of AI reproducibility and slowing down future research by others in the
field. As many research organizations tend to exclusively focus on specific
areas, opportunities for interdisciplinary research reduce. Undertaking
long-term bold research in unexplored fields with non-commercial yet great
public value is hard due to factors including the high upfront risk, budgetary
constraints, and a lack of availability of data and experts in niche fields.
Only a few companies or well-funded research labs can afford to do such
long-term research. With research organizations focused on an exploding array
of fields and resources spread thin, opportunities for the maturation of
interdisciplinary research reduce. Apart from these exigencies, there is also a
need to engage citizen scientists through open-source contributors to play an
active part in the research dialogue. We present a short case study of SpaceML,
an extension of the Frontier Development Lab, an AI accelerator for NASA.
SpaceML distributes open-source research and invites volunteer citizen
scientists to partake in development and deployment of high social value
products at the intersection of space and AI.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.10389v1,2020-12-18T17:53:39Z,2020-12-18T17:53:39Z,"Reinforcement Learning for Unified Allocation and Patrolling in
  Signaling Games with Uncertainty","Green Security Games (GSGs) have been successfully used in the protection of
valuable resources such as fisheries, forests and wildlife. While real-world
deployment involves both resource allocation and subsequent coordinated
patrolling with communication and real-time, uncertain information, previous
game models do not fully address both of these stages simultaneously.
Furthermore, adopting existing solution strategies is difficult since they do
not scale well for larger, more complex variants of the game models.
  We therefore first propose a novel GSG model that combines defender
allocation, patrolling, real-time drone notification to human patrollers, and
drones sending warning signals to attackers. The model further incorporates
uncertainty for real-time decision-making within a team of drones and human
patrollers. Second, we present CombSGPO, a novel and scalable algorithm based
on reinforcement learning, to compute a defender strategy for this game model.
CombSGPO performs policy search over a multi-dimensional, discrete action space
to compute an allocation strategy that is best suited to a best-response
patrolling strategy for the defender, learnt by training a multi-agent Deep
Q-Network. We show via experiments that CombSGPO converges to better strategies
and is more scalable than comparable approaches. Third, we provide a detailed
analysis of the coordination and signaling behavior learnt by CombSGPO, showing
group formation between defender resources and patrolling formations based on
signaling and notifications between resources. Importantly, we find that
strategic signaling emerges in the final learnt strategy. Finally, we perform
experiments to evaluate these strategies under different levels of uncertainty.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.08174v2,2021-03-29T17:15:00Z,2020-12-15T09:49:22Z,"Towards open and expandable cognitive AI architectures for large-scale
  multi-agent human-robot collaborative learning","Learning from Demonstration (LfD) constitutes one of the most robust
methodologies for constructing efficient cognitive robotic systems. Despite the
large body of research works already reported, current key technological
challenges include those of multi-agent learning and long-term autonomy.
Towards this direction, a novel cognitive architecture for multi-agent LfD
robotic learning is introduced, targeting to enable the reliable deployment of
open, scalable and expandable robotic systems in large-scale and complex
environments. In particular, the designed architecture capitalizes on the
recent advances in the Artificial Intelligence (AI) field, by establishing a
Federated Learning (FL)-based framework for incarnating a multi-human
multi-robot collaborative learning environment. The fundamental
conceptualization relies on employing multiple AI-empowered cognitive processes
(implementing various robotic tasks) that operate at the edge nodes of a
network of robotic platforms, while global AI models (underpinning the
aforementioned robotic tasks) are collectively created and shared among the
network, by elegantly combining information from a large number of human-robot
interaction instances. Regarding pivotal novelties, the designed cognitive
architecture a) introduces a new FL-based formalism that extends the
conventional LfD learning paradigm to support large-scale multi-agent
operational settings, b) elaborates previous FL-based self-learning robotic
schemes so as to incorporate the human in the learning loop and c) consolidates
the fundamental principles of FL with additional sophisticated AI-enabled
learning methodologies for modelling the multi-level inter-dependencies among
the robotic tasks. The applicability of the proposed framework is explained
using an example of a real-world industrial case study for agile
production-based Critical Raw Materials (CRM) recovery.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.08015v2,2021-08-26T16:29:07Z,2020-12-15T00:09:37Z,Active Learning for Deep Gaussian Process Surrogates,"Deep Gaussian processes (DGPs) are increasingly popular as predictive models
in machine learning (ML) for their non-stationary flexibility and ability to
cope with abrupt regime changes in training data. Here we explore DGPs as
surrogates for computer simulation experiments whose response surfaces exhibit
similar characteristics. In particular, we transport a DGP's automatic warping
of the input space and full uncertainty quantification (UQ), via a novel
elliptical slice sampling (ESS) Bayesian posterior inferential scheme, through
to active learning (AL) strategies that distribute runs non-uniformly in the
input space -- something an ordinary (stationary) GP could not do. Building up
the design sequentially in this way allows smaller training sets, limiting both
expensive evaluation of the simulator code and mitigating cubic costs of DGP
inference. When training data sizes are kept small through careful acquisition,
and with parsimonious layout of latent layers, the framework can be both
effective and computationally tractable. Our methods are illustrated on
simulation data and two real computer experiments of varying input
dimensionality. We provide an open source implementation in the ""deepgp""
package on CRAN.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.07938v1,2020-12-14T20:55:48Z,2020-12-14T20:55:48Z,NVIDIA SimNet^{TM}: an AI-accelerated multi-physics simulation framework,"We present SimNet, an AI-driven multi-physics simulation framework, to
accelerate simulations across a wide range of disciplines in science and
engineering. Compared to traditional numerical solvers, SimNet addresses a wide
range of use cases - coupled forward simulations without any training data,
inverse and data assimilation problems. SimNet offers fast turnaround time by
enabling parameterized system representation that solves for multiple
configurations simultaneously, as opposed to the traditional solvers that solve
for one configuration at a time. SimNet is integrated with parameterized
constructive solid geometry as well as STL modules to generate point clouds.
Furthermore, it is customizable with APIs that enable user extensions to
geometry, physics and network architecture. It has advanced network
architectures that are optimized for high-performance GPU computing, and offers
scalable performance for multi-GPU and multi-Node implementation with
accelerated linear algebra as well as FP32, FP64 and TF32 computations. In this
paper we review the neural network solver methodology, the SimNet architecture,
and the various features that are needed for effective solution of the PDEs. We
present real-world use cases that range from challenging forward multi-physics
simulations with turbulence and complex 3D geometries, to industrial design
optimization and inverse problems that are not addressed efficiently by the
traditional solvers. Extensive comparisons of SimNet results with open source
and commercial solvers show good correlation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.07145v1,2020-12-13T20:40:08Z,2020-12-13T20:40:08Z,Learning to Schedule Halide Pipelines for the GPU,"We present a new algorithm to automatically generate high-performance GPU
implementations of complex imaging and machine learning pipelines, directly
from high-level Halide algorithm code. It is fully automatic, requiring no
schedule templates or hand-optimized kernels, and it targets a diverse range of
computations which is significantly broader than existing autoschedulers. We
address the scalability challenge of extending previous approaches to schedule
large real world programs, while enabling a broad set of program rewrites that
take into account the nested parallelism and memory hierarchy introduced by GPU
architectures. We achieve this using a hierarchical sampling strategy that
groups programs into buckets based on their structural similarity, then samples
representatives to be evaluated, allowing us to explore a large space by only
considering a subset of the space, and a pre-pass that 'freezes' decisions for
the lowest cost sections of a program, allowing more time to be spent on the
important stages. We then apply an efficient cost model combining machine
learning, program analysis, and GPU architecture knowledge. Our method scales
combinatorially better with respect to the deeper nested parallelism required
by GPUs compared to previous work. We evaluate its performance on a diverse
suite of real-world imaging and machine learning pipelines. We demonstrate
results that are on average 1.66X faster than existing automatic solutions (up
to 5X), and competitive with what the best human experts were able to achieve
in an active effort to beat our automatic results.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.08364v1,2020-12-13T17:05:06Z,2020-12-13T17:05:06Z,GAP-net for Snapshot Compressive Imaging,"Snapshot compressive imaging (SCI) systems aim to capture high-dimensional
($\ge3$D) images in a single shot using 2D detectors. SCI devices include two
main parts: a hardware encoder and a software decoder. The hardware encoder
typically consists of an (optical) imaging system designed to capture
{compressed measurements}. The software decoder on the other hand refers to a
reconstruction algorithm that retrieves the desired high-dimensional signal
from those measurements. In this paper, using deep unfolding ideas, we propose
an SCI recovery algorithm, namely GAP-net, which unfolds the generalized
alternating projection (GAP) algorithm. At each stage, GAP-net passes its
current estimate of the desired signal through a trained convolutional neural
network (CNN). The CNN operates as a denoiser that projects the estimate back
to the desired signal space. For the GAP-net that employs trained
auto-encoder-based denoisers, we prove a probabilistic global convergence
result. Finally, we investigate the performance of GAP-net in solving video SCI
and spectral SCI problems. In both cases, GAP-net demonstrates competitive
performance on both synthetic and real data. In addition to having high
accuracy and high speed, we show that GAP-net is flexible with respect to
signal modulation implying that a trained GAP-net decoder can be applied in
different systems. Our code is at https://github.com/mengziyi64/ADMM-net.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.05996v1,2020-12-10T21:48:26Z,2020-12-10T21:48:26Z,"How to enhance quantum generative adversarial learning of noisy
  information","Quantum Machine Learning is where nowadays machine learning meets quantum
information science. In order to implement this new paradigm for novel quantum
technologies, we still need a much deeper understanding of its underlying
mechanisms, before proposing new algorithms to feasibly address real problems.
In this context, quantum generative adversarial learning is a promising
strategy to use quantum devices for quantum estimation or generative machine
learning tasks. However, the convergence behaviours of its training process,
which is crucial for its practical implementation on quantum processors, have
not been investigated in detail yet. Indeed here we show how different training
problems may occur during the optimization process, such as the emergence of
limit cycles. The latter may remarkably extend the convergence time in the
scenario of mixed quantum states playing a crucial role in the already
available noisy intermediate scale quantum devices. Then, we propose new
strategies to achieve a faster convergence in any operating regime. Our results
pave the way for new experimental demonstrations of such hybrid
classical-quantum protocols allowing to evaluate the potential advantages over
their classical counterparts.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.05214v2,2020-12-10T12:26:59Z,2020-12-09T18:23:21Z,E3D: Event-Based 3D Shape Reconstruction,"3D shape reconstruction is a primary component of augmented/virtual reality.
Despite being highly advanced, existing solutions based on RGB, RGB-D and Lidar
sensors are power and data intensive, which introduces challenges for
deployment in edge devices. We approach 3D reconstruction with an event camera,
a sensor with significantly lower power, latency and data expense while
enabling high dynamic range. While previous event-based 3D reconstruction
methods are primarily based on stereo vision, we cast the problem as multi-view
shape from silhouette using a monocular event camera. The output from a moving
event camera is a sparse point set of space-time gradients, largely sketching
scene/object edges and contours. We first introduce an event-to-silhouette
(E2S) neural network module to transform a stack of event frames to the
corresponding silhouettes, with additional neural branches for camera pose
regression. Second, we introduce E3D, which employs a 3D differentiable
renderer (PyTorch3D) to enforce cross-view 3D mesh consistency and fine-tune
the E2S and pose network. Lastly, we introduce a 3D-to-events simulation
pipeline and apply it to publicly available object datasets and generate
synthetic event/silhouette training pairs for supervised learning.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.04746v2,2021-04-02T22:28:33Z,2020-12-08T21:20:54Z,"Robust Neural Routing Through Space Partitions for Camera Relocalization
  in Dynamic Indoor Environments","Localizing the camera in a known indoor environment is a key building block
for scene mapping, robot navigation, AR, etc. Recent advances estimate the
camera pose via optimization over the 2D/3D-3D correspondences established
between the coordinates in 2D/3D camera space and 3D world space. Such a
mapping is estimated with either a convolution neural network or a decision
tree using only the static input image sequence, which makes these approaches
vulnerable to dynamic indoor environments that are quite common yet challenging
in the real world. To address the aforementioned issues, in this paper, we
propose a novel outlier-aware neural tree which bridges the two worlds, deep
learning and decision tree approaches. It builds on three important blocks: (a)
a hierarchical space partition over the indoor scene to construct the decision
tree; (b) a neural routing function, implemented as a deep classification
network, employed for better 3D scene understanding; and (c) an outlier
rejection module used to filter out dynamic points during the hierarchical
routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark
developed for camera relocalization in dynamic indoor environments. It achieves
robust neural routing through space partitions and outperforms the
state-of-the-art approaches by around 30% on camera pose accuracy, while
running comparably fast for evaluation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.01356v1,2020-12-02T17:56:44Z,2020-12-02T17:56:44Z,"Coinbot: Intelligent Robotic Coin Bag Manipulation Using Deep
  Reinforcement Learning And Machine Teaching","Given the laborious difficulty of moving heavy bags of physical currency in
the cash center of the bank, there is a large demand for training and deploying
safe autonomous systems capable of conducting such tasks in a collaborative
workspace. However, the deformable properties of the bag along with the large
quantity of rigid-body coins contained within it, significantly increases the
challenges of bag detection, grasping and manipulation by a robotic gripper and
arm. In this paper, we apply deep reinforcement learning and machine learning
techniques to the task of controlling a collaborative robot to automate the
unloading of coin bags from a trolley. To accomplish the task-specific process
of gripping flexible materials like coin bags where the center of the mass
changes during manipulation, a special gripper was implemented in simulation
and designed in physical hardware. Leveraging a depth camera and object
detection using deep learning, a bag detection and pose estimation has been
done for choosing the optimal point of grasping. An intelligent approach based
on deep reinforcement learning has been introduced to propose the best
configuration of the robot end-effector to maximize successful grasping. A
boosted motion planning is utilized to increase the speed of motion planning
during robot operation. Real-world trials with the proposed pipeline have
demonstrated success rates over 96\% in a real-world setting.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.14654v2,2020-12-16T19:56:16Z,2020-11-30T09:47:20Z,Feature Space Singularity for Out-of-Distribution Detection,"Out-of-Distribution (OoD) detection is important for building safe artificial
intelligence systems. However, current OoD detection methods still cannot meet
the performance requirements for practical deployment. In this paper, we
propose a simple yet effective algorithm based on a novel observation: in a
trained neural network, OoD samples with bounded norms well concentrate in the
feature space. We call the center of OoD features the Feature Space Singularity
(FSS), and denote the distance of a sample feature to FSS as FSSD. Then, OoD
samples can be identified by taking a threshold on the FSSD. Our analysis of
the phenomenon reveals why our algorithm works. We demonstrate that our
algorithm achieves state-of-the-art performance on various OoD detection
benchmarks. Besides, FSSD also enjoys robustness to slight corruption in test
data and can be further enhanced by ensembling. These make FSSD a promising
algorithm to be employed in real world. We release our code at
\url{https://github.com/megvii-research/FSSD_OoD_Detection}.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.14925v1,2020-11-26T02:37:39Z,2020-11-26T02:37:39Z,"Autonomous Graph Mining Algorithm Search with Best Speed/Accuracy
  Trade-off","Graph data is ubiquitous in academia and industry, from social networks to
bioinformatics. The pervasiveness of graphs today has raised the demand for
algorithms that can answer various questions: Which products would a user like
to purchase given her order list? Which users are buying fake followers to
increase their public reputation? Myriads of new graph mining algorithms are
proposed every year to answer such questions - each with a distinct problem
formulation, computational time, and memory footprint. This lack of unity makes
it difficult for a practitioner to compare different algorithms and pick the
most suitable one for a specific application. These challenges - even more
severe for non-experts - create a gap in which state-of-the-art techniques
developed in academic settings fail to be optimally deployed in real-world
applications. To bridge this gap, we propose AUTOGM, an automated system for
graph mining algorithm development. We first define a unified framework
UNIFIEDGM that integrates various message-passing based graph algorithms,
ranging from conventional algorithms like PageRank to graph neural networks.
Then UNIFIEDGM defines a search space in which five parameters are required to
determine a graph algorithm. Under this search space, AUTOGM explicitly
optimizes for the optimal parameter set of UNIFIEDGM using Bayesian
Optimization. AUTOGM defines a novel budget-aware objective function for the
optimization to incorporate a practical issue - finding the best speed-accuracy
trade-off under a computation budget - into the graph algorithm generation
problem. Experiments on real-world benchmark datasets demonstrate that AUTOGM
generates novel graph mining algorithms with the best speed/accuracy trade-off
compared to existing models with heuristic parameters.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.12945v2,2022-04-10T23:01:14Z,2020-11-25T18:50:32Z,"No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained
  Classification Problems","In real-world classification tasks, each class often comprises multiple
finer-grained ""subclasses."" As the subclass labels are frequently unavailable,
models trained using only the coarser-grained class labels often exhibit highly
variable performance across different subclasses. This phenomenon, known as
hidden stratification, has important consequences for models deployed in
safety-critical applications such as medicine. We propose GEORGE, a method to
both measure and mitigate hidden stratification even when subclass labels are
unknown. We first observe that unlabeled subclasses are often separable in the
feature space of deep neural networks, and exploit this fact to estimate
subclass labels for the training data via clustering techniques. We then use
these approximate subclass labels as a form of noisy supervision in a
distributionally robust optimization objective. We theoretically characterize
the performance of GEORGE in terms of the worst-case generalization error
across any subclass. We empirically validate GEORGE on a mix of real-world and
benchmark image classification datasets, and show that our approach boosts
worst-case subclass accuracy by up to 22 percentage points compared to standard
training techniques, without requiring any prior information about the
subclasses.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2012.02298v2,2021-06-15T06:28:13Z,2020-11-25T17:23:52Z,"Exploration in Online Advertising Systems with Deep Uncertainty-Aware
  Learning","Modern online advertising systems inevitably rely on personalization methods,
such as click-through rate (CTR) prediction. Recent progress in CTR prediction
enjoys the rich representation capabilities of deep learning and achieves great
success in large-scale industrial applications. However, these methods can
suffer from lack of exploration. Another line of prior work addresses the
exploration-exploitation trade-off problem with contextual bandit methods,
which are recently less studied in the industry due to the difficulty in
extending their flexibility with deep models. In this paper, we propose a novel
Deep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on
Gaussian processes, which can provide predictive uncertainty estimations while
maintaining the flexibility of deep neural networks. DUAL can be easily
implemented on existing models and deployed in real-time systems with minimal
extra computational overhead. By linking the predictive uncertainty estimation
ability of DUAL to well-known bandit algorithms, we further present DUAL-based
Ad-ranking strategies to boost up long-term utilities such as the social
welfare in advertising systems. Experimental results on several public datasets
demonstrate the effectiveness of our methods. Remarkably, an online A/B test
deployed in the Alibaba display advertising platform shows an 8.2% social
welfare improvement and an 8.0% revenue lift.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.11081v1,2020-11-22T18:30:23Z,2020-11-22T18:30:23Z,"Deep learning model trained on mobile phone-acquired frozen section
  images effectively detects basal cell carcinoma","Background: Margin assessment of basal cell carcinoma using the frozen
section is a common task of pathology intraoperative consultation. Although
frequently straight-forward, the determination of the presence or absence of
basal cell carcinoma on the tissue sections can sometimes be challenging. We
explore if a deep learning model trained on mobile phone-acquired frozen
section images can have adequate performance for future deployment. Materials
and Methods: One thousand two hundred and forty-one (1241) images of frozen
sections performed for basal cell carcinoma margin status were acquired using
mobile phones. The photos were taken at 100x magnification (10x objective). The
images were downscaled from a 4032 x 3024 pixel resolution to 576 x 432 pixel
resolution. Semantic segmentation algorithm Deeplab V3 with Xception backbone
was used for model training. Results: The model uses an image as input and
produces a 2-dimensional black and white output of prediction of the same
dimension; the areas determined to be basal cell carcinoma were displayed with
white color, in a black background. Any output with the number of white pixels
exceeding 0.5% of the total number of pixels is deemed positive for basal cell
carcinoma. On the test set, the model achieves area under curve of 0.99 for
receiver operator curve and 0.97 for precision-recall curve at the pixel level.
The accuracy of classification at the slide level is 96%. Conclusions: The deep
learning model trained with mobile phone images shows satisfactory performance
characteristics, and thus demonstrates the potential for deploying as a mobile
phone app to assist in frozen section interpretation in real time.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.09780v1,2020-11-19T11:37:40Z,2020-11-19T11:37:40Z,Kernel Phase and Coronagraphy with Automatic Differentiation,"The accumulation of aberrations along the optical path in a telescope
produces distortions and speckles in the resulting images, limiting the
performance of cameras at high angular resolution. It is important to achieve
the highest possible sensitivity to faint sources such as planets, using both
hardware and data analysis software. While analytic methods are efficient, real
systems are better-modelled numerically, but such models with many parameters
can be hard to understand, optimize and apply. Automatic differentiation
software developed for machine learning now makes calculating derivatives with
respect to aberrations straightforward for arbitrary optical systems. We apply
this powerful new tool to enhance high-angular-resolution astronomical imaging.
Self-calibrating observables such as the 'closure phase' or 'bispectrum' have
been widely used in optical and radio astronomy to mitigate optical aberrations
and achieve high-fidelity imagery. Kernel phases are a generalization of
closure phases in the limit of small phase errors. Using automatic
differentiation, we reproduce existing kernel phase theory within this
framework and demonstrate an extension to the Lyot coronagraph, finding
self-calibrating combinations of speckles which are resistant to phase noise,
but only in the very high-wavefront-quality regime. As an illustrative example,
we reanalyze Palomar adaptive optics observations of the binary alpha Ophiuchi,
finding consistency between the new pipeline and the existing standard. We
present a new Python package 'morphine' that incorporates these ideas, with an
interface similar to the popular package poppy, for optical simulation with
automatic differentiation. These methods may be useful for designing improved
astronomical optical systems by gradient descent.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.09747v2,2021-09-29T15:26:04Z,2020-11-19T09:53:27Z,"Energy Aware Deep Reinforcement Learning Scheduling for Sensors
  Correlated in Time and Space","Millions of battery-powered sensors deployed for monitoring purposes in a
multitude of scenarios, e.g., agriculture, smart cities, industry, etc.,
require energy-efficient solutions to prolong their lifetime. When these
sensors observe a phenomenon distributed in space and evolving in time, it is
expected that collected observations will be correlated in time and space. In
this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling
mechanism capable of taking advantage of correlated information. We design our
solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The
proposed mechanism is capable of determining the frequency with which sensors
should transmit their updates, to ensure accurate collection of observations,
while simultaneously considering the energy available. To evaluate our
scheduling mechanism, we use multiple datasets containing environmental
observations obtained in multiple real deployments. The real observations
enable us to model the environment with which the mechanism interacts as
realistically as possible. We show that our solution can significantly extend
the sensors' lifetime. We compare our mechanism to an idealized, all-knowing
scheduler to demonstrate that its performance is near-optimal. Additionally, we
highlight the unique feature of our design, energy-awareness, by displaying the
impact of sensors' energy levels on the frequency of updates.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.09643v2,2021-03-08T10:48:14Z,2020-11-19T04:18:01Z,Node Similarity Preserving Graph Convolutional Networks,"Graph Neural Networks (GNNs) have achieved tremendous success in various
real-world applications due to their strong ability in graph representation
learning. GNNs explore the graph structure and node features by aggregating and
transforming information within node neighborhoods. However, through
theoretical and empirical analysis, we reveal that the aggregation process of
GNNs tends to destroy node similarity in the original feature space. There are
many scenarios where node similarity plays a crucial role. Thus, it has
motivated the proposed framework SimP-GCN that can effectively and efficiently
preserve node similarity while exploiting graph structure. Specifically, to
balance information from graph structure and node features, we propose a
feature similarity preserving aggregation which adaptively integrates graph
structure and node features. Furthermore, we employ self-supervised learning to
explicitly capture the complex feature similarity and dissimilarity relations
between nodes. We validate the effectiveness of SimP-GCN on seven benchmark
datasets including three assortative and four disassorative graphs. The results
demonstrate that SimP-GCN outperforms representative baselines. Further probe
shows various advantages of the proposed framework. The implementation of
SimP-GCN is available at \url{https://github.com/ChandlerBang/SimP-GCN}.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.09290v3,2022-07-22T17:14:56Z,2020-11-18T14:02:04Z,Practical Privacy Attacks on Vertical Federated Learning,"Federated learning (FL) is a privacy-preserving learning paradigm that allows
multiple parities to jointly train a powerful machine learning model without
sharing their private data. According to the form of collaboration, FL can be
further divided into horizontal federated learning (HFL) and vertical federated
learning (VFL). In HFL, participants share the same feature space and
collaborate on data samples, while in VFL, participants share the same sample
IDs and collaborate on features. VFL has a broader scope of applications and is
arguably more suitable for joint model training between large enterprises.
  In this paper, we focus on VFL and investigate potential privacy leakage in
real-world VFL frameworks. We design and implement two practical privacy
attacks: reverse multiplication attack for the logistic regression VFL
protocol; and reverse sum attack for the XGBoost VFL protocol. We empirically
show that the two attacks are (1) effective - the adversary can successfully
steal the private training data, even when the intermediate outputs are
encrypted to protect data privacy; (2) evasive - the attacks do not deviate
from the protocol specification nor deteriorate the accuracy of the target
model; and (3) easy - the adversary needs little prior knowledge about the data
distribution of the target participant. We also show the leaked information is
as effective as the raw training data in training an alternative classifier. We
further discuss potential countermeasures and their challenges, which we hope
can lead to several promising research directions.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.09902v1,2020-11-17T04:11:31Z,2020-11-17T04:11:31Z,"Low-latency Federated Learning and Blockchain for Edge Association in
  Digital Twin empowered 6G Networks","Emerging technologies such as digital twins and 6th Generation mobile
networks (6G) have accelerated the realization of edge intelligence in
Industrial Internet of Things (IIoT). The integration of digital twin and 6G
bridges the physical system with digital space and enables robust instant
wireless connectivity. With increasing concerns on data privacy, federated
learning has been regarded as a promising solution for deploying distributed
data processing and learning in wireless networks. However, unreliable
communication channels, limited resources, and lack of trust among users,
hinder the effective application of federated learning in IIoT. In this paper,
we introduce the Digital Twin Wireless Networks (DTWN) by incorporating digital
twins into wireless networks, to migrate real-time data processing and
computation to the edge plane. Then, we propose a blockchain empowered
federated learning framework running in the DTWN for collaborative computing,
which improves the reliability and security of the system, and enhances data
privacy. Moreover, to balance the learning accuracy and time cost of the
proposed scheme, we formulate an optimization problem for edge association by
jointly considering digital twin association, training data batch size, and
bandwidth allocation. We exploit multi-agent reinforcement learning to find an
optimal solution to the problem. Numerical results on real-world dataset show
that the proposed scheme yields improved efficiency and reduced cost compared
to benchmark learning method.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.06501v2,2020-12-18T15:00:07Z,2020-11-12T17:11:50Z,VARCLUST: clustering variables using dimensionality reduction,"VARCLUST algorithm is proposed for clustering variables under the assumption
that variables in a given cluster are linear combinations of a small number of
hidden latent variables, corrupted by the random noise. The entire clustering
task is viewed as the problem of selection of the statistical model, which is
defined by the number of clusters, the partition of variables into these
clusters and the 'cluster dimensions', i.e. the vector of dimensions of linear
subspaces spanning each of the clusters. The optimal model is selected using
the approximate Bayesian criterion based on the Laplace approximations and
using a non-informative uniform prior on the number of clusters. To solve the
problem of the search over a huge space of possible models we propose an
extension of the ClustOfVar algorithm which was dedicated to subspaces of
dimension only 1, and which is similar in structure to the $K$-centroid
algorithm. We provide a complete methodology with theoretical guarantees,
extensive numerical experimentations, complete data analyses and
implementation. Our algorithm assigns variables to appropriate clusterse based
on the consistent Bayesian Information Criterion (BIC), and estimates the
dimensionality of each cluster by the PEnalized SEmi-integrated Likelihood
Criterion (PESEL), whose consistency we prove. Additionally, we prove that each
iteration of our algorithm leads to an increase of the Laplace approximation to
the model posterior probability and provide the criterion for the estimation of
the number of clusters. Numerical comparisons with other algorithms show that
VARCLUST may outperform some popular machine learning tools for sparse subspace
clustering. We also report the results of real data analysis including TCGA
breast cancer data and meteorological data. The proposed method is implemented
in the publicly available R package varclust.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.05373v1,2020-11-10T20:06:19Z,2020-11-10T20:06:19Z,"Emergent Reciprocity and Team Formation from Randomized Uncertain Social
  Preferences","Multi-agent reinforcement learning (MARL) has shown recent success in
increasingly complex fixed-team zero-sum environments. However, the real world
is not zero-sum nor does it have fixed teams; humans face numerous social
dilemmas and must learn when to cooperate and when to compete. To successfully
deploy agents into the human world, it may be important that they be able to
understand and help in our conflicts. Unfortunately, selfish MARL agents
typically fail when faced with social dilemmas. In this work, we show evidence
of emergent direct reciprocity, indirect reciprocity and reputation, and team
formation when training agents with randomized uncertain social preferences
(RUSP), a novel environment augmentation that expands the distribution of
environments agents play in. RUSP is generic and scalable; it can be applied to
any multi-agent environment without changing the original underlying game
dynamics or objectives. In particular, we show that with RUSP these behaviors
can emerge and lead to higher social welfare equilibria in both classic
abstract social dilemmas like Iterated Prisoner's Dilemma as well in more
complex intertemporal environments.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.05128v1,2020-11-10T14:51:25Z,2020-11-10T14:51:25Z,"Laplacian Eigenmaps with variational circuits: a quantum embedding of
  graph data","With the development of quantum algorithms, high-cost computations are being
scrutinized in the hope of a quantum advantage. While graphs offer a convenient
framework for multiple real-world problems, their analytics still comes with
high computation and space. By mapping the graph data into a low dimensional
space, in which graph structural information is preserved, the eigenvectors of
the Laplacian matrix constitute a powerful node embedding, called Laplacian
Eigenmaps. Computing these embeddings is on its own an expensive task knowing
that using specific sparse methods, the eigendecomposition of a Laplacian
matrix has a cost of O($rn^2$), $r$ being the ratio of nonzero elements.
  We propose a method to compute a Laplacian Eigenmap using a quantum
variational circuit. The idea of our algorithm is to reach the eigenstates of
the laplacian matrix, which can be considered as a hamiltonian operator, by
adapting the variational quantum eigensolver algorithm. By estimating the $d$
first eigenvectors of the Laplacian at the same time, our algorithm directly
generates a $d$ dimension quantum embedding of the graph. We demonstrate that
it is possible to use the embedding for graph machine learning tasks by
implementing a quantum classifier on the top of it. The overall circuit
consists in a full quantum node classification algorithm. Tests on 32 nodes
graph with a quantum simulator shows that we can achieve similar performances
as the classical laplacian eigenmap algorithm. Although mathematical properties
of this approximate approach are not fully understood, this algorithm opens
perspectives for graph pre-processing using noisy quantum computers.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.04813v2,2021-05-24T01:05:25Z,2020-11-09T22:49:19Z,"Bimanual Regrasping for Suture Needles using Reinforcement Learning for
  Rapid Motion Planning","Regrasping a suture needle is an important yet time-consuming process in
suturing. To bring efficiency into regrasping, prior work either designs a
task-specific mechanism or guides the gripper toward some specific pick-up
point for proper grasping of a needle. Yet, these methods are usually not
deployable when the working space is changed. Therefore, in this work, we
present rapid trajectory generation for bimanual needle regrasping via
reinforcement learning (RL). Demonstrations from a sampling-based motion
planning algorithm is incorporated to speed up the learning. In addition, we
propose the ego-centric state and action spaces for this bimanual planning
problem, where the reference frames are on the end-effectors instead of some
fixed frame. Thus, the learned policy can be directly applied to any feasible
robot configuration. Our experiments in simulation show that the success rate
of a single pass is 97%, and the planning time is 0.0212s on average, which
outperforms other widely used motion planning algorithms. For the real-world
experiments, the success rate is 73.3% if the needle pose is reconstructed from
an RGB image, with a planning time of 0.0846s and a run time of 5.1454s. If the
needle pose is known beforehand, the success rate becomes 90.5%, with a
planning time of 0.0807s and a run time of 2.8801s.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.16356v1,2020-10-30T16:25:26Z,2020-10-30T16:25:26Z,Cooperation dynamics of generalized reciprocity on complex networks,"Recent studies suggest that the emergence of cooperative behavior can be
explained by generalized reciprocity, a behavioral mechanism based on the
principle of ""help anyone if helped by someone"". In complex systems, the
cooperative dynamics is largely determined by the network structure which
dictates the interactions among neighboring individuals. Despite an abundance
of studies, the role of the network structure in in promoting cooperation
through generalized reciprocity remains an under-explored phenomenon. In this
doctoral thesis, we utilize basic tools from the dynamical systems theory, and
develop a unifying framework for investigating the cooperation dynamics of
generalized reciprocity on complex networks. We use this framework to present a
theoretical overview on the role of generalized reciprocity in promoting
cooperation in three distinct interaction structures: i) social dilemmas, ii)
multidimensional networks, and iii) fluctuating environments. The results
suggest that cooperation through generalized reciprocity always emerges as the
unique attractor in which the overall level of cooperation is maximized, while
simultaneously exploitation of the participating individuals is prevented. The
effect of the network structure is captured by a local centrality measure which
uniquely quantifies the propensity of the network structure to cooperation, by
dictating the degree of cooperation displayed both at microscopic and
macroscopic level. As a consequence, the implementation of our results may go
beyond explaining the evolution of cooperation. In particular, they can be
directly applied in domains that deal with the development of artificial
systems able to adequately mimic reality, such as reinforcement learning.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.15689v1,2020-10-29T15:32:00Z,2020-10-29T15:32:00Z,"Learning Deep Interleaved Networks with Asymmetric Co-Attention for
  Image Restoration","Recently, convolutional neural network (CNN) has demonstrated significant
success for image restoration (IR) tasks (e.g., image super-resolution, image
deblurring, rain streak removal, and dehazing). However, existing CNN based
models are commonly implemented as a single-path stream to enrich feature
representations from low-quality (LQ) input space for final predictions, which
fail to fully incorporate preceding low-level contexts into later high-level
features within networks, thereby producing inferior results. In this paper, we
present a deep interleaved network (DIN) that learns how information at
different states should be combined for high-quality (HQ) images
reconstruction. The proposed DIN follows a multi-path and multi-branch pattern
allowing multiple interconnected branches to interleave and fuse at different
states. In this way, the shallow information can guide deep representative
features prediction to enhance the feature expression ability. Furthermore, we
propose asymmetric co-attention (AsyCA) which is attached at each interleaved
node to model the feature dependencies. Such AsyCA can not only adaptively
emphasize the informative features from different states, but also improves the
discriminative ability of networks. Our presented DIN can be trained end-to-end
and applied to various IR tasks. Comprehensive evaluations on public benchmarks
and real-world datasets demonstrate that the proposed DIN perform favorably
against the state-of-the-art methods quantitatively and qualitatively.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.14866v1,2020-10-28T10:25:05Z,2020-10-28T10:25:05Z,"Deterministic and probabilistic deep learning models for inverse design
  of broadband acoustic cloak","Concealing an object from incoming waves (light and/or sound) remained
science fiction for a long time due to the absence of wave-shielding materials
in nature. Yet, the invention of artificial materials and new physical
principles for optical and sound wave manipulation translated this abstract
concept into reality by making an object acoustically invisible. Here, we
present the notion of a machine learning-driven acoustic cloak and demonstrate
an example of such a cloak with a multilayered core-shell configuration.
Importantly, we develop deterministic and probabilistic deep learning models
based on autoencoder-like neural network structure to retrieve the structural
and material properties of the cloaking shell surrounding the object that
suppresses scattering of sound in a broad spectral range, as if it was not
there. The probabilistic model enhances the generalization ability of design
procedure and uncovers the sensitivity of the cloak parameters on the spectral
response for practical implementation. This proposal opens up new avenues to
expedite the design of intelligent cloaking devices for tailored spectral
response and offers a feasible solution for inverse scattering problems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.14605v3,2021-06-07T10:06:48Z,2020-10-27T20:56:49Z,"Traffic Refinery: Cost-Aware Data Representation for Machine Learning on
  Network Traffic","Network management often relies on machine learning to make predictions about
performance and security from network traffic. Often, the representation of the
traffic is as important as the choice of the model. The features that the model
relies on, and the representation of those features, ultimately determine model
accuracy, as well as where and whether the model can be deployed in practice.
Thus, the design and evaluation of these models ultimately requires
understanding not only model accuracy but also the systems costs associated
with deploying the model in an operational network. Towards this goal, this
paper develops a new framework and system that enables a joint evaluation of
both the conventional notions of machine learning performance (e.g., model
accuracy) and the systems-level costs of different representations of network
traffic. We highlight these two dimensions for two practical network management
tasks, video streaming quality inference and malware detection, to demonstrate
the importance of exploring different representations to find the appropriate
operating point. We demonstrate the benefit of exploring a range of
representations of network traffic and present Traffic Refinery, a
proof-of-concept implementation that both monitors network traffic at 10 Gbps
and transforms traffic in real time to produce a variety of feature
representations for machine learning. Traffic Refinery both highlights this
design space and makes it possible to explore different representations for
learning, balancing systems costs related to feature extraction and model
training against model accuracy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.14000v2,2020-12-08T18:04:58Z,2020-10-27T02:19:40Z,"Graph-based Reinforcement Learning for Active Learning in Real Time: An
  Application in Modeling River Networks","Effective training of advanced ML models requires large amounts of labeled
data, which is often scarce in scientific problems given the substantial human
labor and material cost to collect labeled data. This poses a challenge on
determining when and where we should deploy measuring instruments (e.g.,
in-situ sensors) to collect labeled data efficiently. This problem differs from
traditional pool-based active learning settings in that the labeling decisions
have to be made immediately after we observe the input data that come in a time
series. In this paper, we develop a real-time active learning method that uses
the spatial and temporal contextual information to select representative query
samples in a reinforcement learning framework. To reduce the need for large
training data, we further propose to transfer the policy learned from
simulation data which is generated by existing physics-based models. We
demonstrate the effectiveness of the proposed method by predicting streamflow
and water temperature in the Delaware River Basin given a limited budget for
collecting labeled data. We further study the spatial and temporal distribution
of selected samples to verify the ability of this method in selecting
informative samples over space and time.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.12996v3,2021-04-20T18:22:35Z,2020-10-24T21:42:04Z,"Assessing the Potential of Deep Learning for Emulating Cloud
  Superparameterization in Climate Models with Real-Geography Boundary
  Conditions","We explore the potential of feed-forward deep neural networks (DNNs) for
emulating cloud superparameterization in realistic geography, using offline
fits to data from the Super Parameterized Community Atmospheric Model. To
identify the network architecture of greatest skill, we formally optimize
hyperparameters using ~250 trials. Our DNN explains over 70 percent of the
temporal variance at the 15-minute sampling scale throughout the mid-to-upper
troposphere. Autocorrelation timescale analysis compared against DNN skill
suggests the less good fit in the tropical, marine boundary layer is driven by
neural network difficulty emulating fast, stochastic signals in convection.
However, spectral analysis in the temporal domain indicates skillful emulation
of signals on diurnal to synoptic scales. A close look at the diurnal cycle
reveals correct emulation of land-sea contrasts and vertical structure in the
heating and moistening fields, but some distortion of precipitation.
Sensitivity tests targeting precipitation skill reveal complementary effects of
adding positive constraints vs. hyperparameter tuning, motivating the use of
both in the future. A first attempt to force an offline land model with DNN
emulated atmospheric fields produces reassuring results further supporting
neural network emulation viability in real-geography settings. Overall, the fit
skill is competitive with recent attempts by sophisticated Residual and
Convolutional Neural Network architectures trained on added information,
including memory of past states. Our results confirm the parameterizability of
superparameterized convection with continents through machine learning and we
highlight advantages of casting this problem locally in space and time for
accurate emulation and hopefully quick implementation of hybrid climate models.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.12751v2,2021-11-30T20:08:49Z,2020-10-24T03:09:37Z,"Model Extraction Attacks on Graph Neural Networks: Taxonomy and
  Realization","Machine learning models are shown to face a severe threat from Model
Extraction Attacks, where a well-trained private model owned by a service
provider can be stolen by an attacker pretending as a client. Unfortunately,
prior works focus on the models trained over the Euclidean space, e.g., images
and texts, while how to extract a GNN model that contains a graph structure and
node features is yet to be explored. In this paper, for the first time, we
comprehensively investigate and develop model extraction attacks against GNN
models. We first systematically formalise the threat modelling in the context
of GNN model extraction and classify the adversarial threats into seven
categories by considering different background knowledge of the attacker, e.g.,
attributes and/or neighbour connections of the nodes obtained by the attacker.
Then we present detailed methods which utilise the accessible knowledge in each
threat to implement the attacks. By evaluating over three real-world datasets,
our attacks are shown to extract duplicated models effectively, i.e., 84% - 89%
of the inputs in the target domain have the same output predictions as the
victim model.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.11884v1,2020-10-22T17:20:38Z,2020-10-22T17:20:38Z,"AEGIS: A real-time multimodal augmented reality computer vision based
  system to assist facial expression recognition for individuals with autism
  spectrum disorder","The ability to interpret social cues comes naturally for most people, but for
those living with Autism Spectrum Disorder (ASD), some experience a deficiency
in this area. This paper presents the development of a multimodal augmented
reality (AR) system which combines the use of computer vision and deep
convolutional neural networks (CNN) in order to assist individuals with the
detection and interpretation of facial expressions in social settings. The
proposed system, which we call AEGIS (Augmented-reality Expression Guided
Interpretation System), is an assistive technology deployable on a variety of
user devices including tablets, smartphones, video conference systems, or
smartglasses, showcasing its extreme flexibility and wide range of use cases,
to allow integration into daily life with ease. Given a streaming video camera
source, each real-world frame is passed into AEGIS, processed for facial
bounding boxes, and then fed into our novel deep convolutional time windowed
neural network (TimeConvNet). We leverage both spatial and temporal information
in order to provide an accurate expression prediction, which is then converted
into its corresponding visualization and drawn on top of the original video
frame. The system runs in real-time, requires minimal set up and is simple to
use. With the use of AEGIS, we can assist individuals living with ASD to learn
to better identify expressions and thus improve their social experiences.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.11411v2,2020-11-21T08:45:37Z,2020-10-22T03:27:19Z,"Value Cards: An Educational Toolkit for Teaching Social Impacts of
  Machine Learning through Deliberation","Recently, there have been increasing calls for computer science curricula to
complement existing technical training with topics related to Fairness,
Accountability, Transparency, and Ethics. In this paper, we present Value Card,
an educational toolkit to inform students and practitioners of the social
impacts of different machine learning models via deliberation. This paper
presents an early use of our approach in a college-level computer science
course. Through an in-class activity, we report empirical data for the initial
effectiveness of our approach. Our results suggest that the use of the Value
Cards toolkit can improve students' understanding of both the technical
definitions and trade-offs of performance metrics and apply them in real-world
contexts, help them recognize the significance of considering diverse social
values in the development of deployment of algorithmic systems, and enable them
to communicate, negotiate and synthesize the perspectives of diverse
stakeholders. Our study also demonstrates a number of caveats we need to
consider when using the different variants of the Value Cards toolkit. Finally,
we discuss the challenges as well as future applications of our approach.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.10969v2,2021-01-06T10:07:56Z,2020-10-21T13:00:05Z,"Incorporating Interpretable Output Constraints in Bayesian Neural
  Networks","Domains where supervised models are deployed often come with task-specific
constraints, such as prior expert knowledge on the ground-truth function, or
desiderata like safety and fairness. We introduce a novel probabilistic
framework for reasoning with such constraints and formulate a prior that
enables us to effectively incorporate them into Bayesian neural networks
(BNNs), including a variant that can be amortized over tasks. The resulting
Output-Constrained BNN (OC-BNN) is fully consistent with the Bayesian framework
for uncertainty quantification and is amenable to black-box inference. Unlike
typical BNN inference in uninterpretable parameter space, OC-BNNs widen the
range of functional knowledge that can be incorporated, especially for model
users without expertise in machine learning. We demonstrate the efficacy of
OC-BNNs on real-world datasets, spanning multiple domains such as healthcare,
criminal justice, and credit scoring.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.10346v2,2021-02-27T18:46:18Z,2020-10-20T15:12:30Z,"Deep Importance Sampling based on Regression for Model Inversion and
  Emulation","Understanding systems by forward and inverse modeling is a recurrent topic of
research in many domains of science and engineering. In this context, Monte
Carlo methods have been widely used as powerful tools for numerical inference
and optimization. They require the choice of a suitable proposal density that
is crucial for their performance. For this reason, several adaptive importance
sampling (AIS) schemes have been proposed in the literature. We here present an
AIS framework called Regression-based Adaptive Deep Importance Sampling
(RADIS). In RADIS, the key idea is the adaptive construction via regression of
a non-parametric proposal density (i.e., an emulator), which mimics the
posterior distribution and hence minimizes the mismatch between proposal and
target densities. RADIS is based on a deep architecture of two (or more) nested
IS schemes, in order to draw samples from the constructed emulator. The
algorithm is highly efficient since employs the posterior approximation as
proposal density, which can be improved adding more support points. As a
consequence, RADIS asymptotically converges to an exact sampler under mild
conditions. Additionally, the emulator produced by RADIS can be in turn used as
a cheap surrogate model for further studies. We introduce two specific RADIS
implementations that use Gaussian Processes (GPs) and Nearest Neighbors (NN)
for constructing the emulator. Several numerical experiments and comparisons
show the benefits of the proposed schemes. A real-world application in remote
sensing model inversion and emulation confirms the validity of the approach.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.09814v1,2020-10-19T19:47:38Z,2020-10-19T19:47:38Z,"Social Hierarchy-based Distributed Economic Model Predictive Control of
  Floating Offshore Wind Farms","This paper implements a recently developed social hierarchy-based distributed
economic model predictive control (DEMPC) algorithm in floating offshore wind
farms for the purpose of power maximization. The controller achieves this
objective using the concept of yaw and induction-based turbine repositioning
(YITuR), which minimizes the overlap areas between adjacent floating wind
turbine rotors in real-time to minimize the wake effect. Floating wind farm
dynamics and performance are predicted numerically using FOWFSim-Dyn. To ensure
fast decision-making by the DEMPC algorithm, feed-forward neural networks are
used to estimate floating wind turbine dynamics during the process of dynamic
optimization. For simulated wind farms with layouts ranging from 1-by-2 to
1-by-5, an increase of 20% in energy production is predicted when using YITuR
instead of greedy operation. Increased variability in wind speed and direction
is also studied and is shown to diminish controller performance due to rising
errors in neural network predictions.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.09635v1,2020-10-19T16:20:45Z,2020-10-19T16:20:45Z,"Deep Reinforcement Learning with Population-Coded Spiking Neural Network
  for Continuous Control","The energy-efficient control of mobile robots is crucial as the complexity of
their real-world applications increasingly involves high-dimensional
observation and action spaces, which cannot be offset by limited on-board
resources. An emerging non-Von Neumann model of intelligence, where spiking
neural networks (SNNs) are run on neuromorphic processors, is regarded as an
energy-efficient and robust alternative to the state-of-the-art real-time
robotic controllers for low dimensional control tasks. The challenge now for
this new computing paradigm is to scale so that it can keep up with real-world
tasks. To do so, SNNs need to overcome the inherent limitations of their
training, namely the limited ability of their spiking neurons to represent
information and the lack of effective learning algorithms. Here, we propose a
population-coded spiking actor network (PopSAN) trained in conjunction with a
deep critic network using deep reinforcement learning (DRL). The population
coding scheme dramatically increased the representation capacity of the network
and the hybrid learning combined the training advantages of deep networks with
the energy-efficient inference of spiking networks. To show the general
applicability of our approach, we integrated it with a spectrum of both
on-policy and off-policy DRL algorithms. We deployed the trained PopSAN on
Intel's Loihi neuromorphic chip and benchmarked our method against the
mainstream DRL algorithms for continuous control. To allow for a fair
comparison among all methods, we validated them on OpenAI gym tasks. Our
Loihi-run PopSAN consumed 140 times less energy per inference when compared
against the deep actor network on Jetson TX2, and had the same level of
performance. Our results support the efficiency of neuromorphic controllers and
suggest our hybrid RL as an alternative to deep learning, when both
energy-efficiency and robustness are important.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.10246v4,2021-03-16T12:54:40Z,2020-10-17T13:34:48Z,"MLCask: Efficient Management of Component Evolution in Collaborative
  Data Analytics Pipelines","With the ever-increasing adoption of machine learning for data analytics,
maintaining a machine learning pipeline is becoming more complex as both the
datasets and trained models evolve with time. In a collaborative environment,
the changes and updates due to pipeline evolution often cause cumbersome
coordination and maintenance work, raising the costs and making it hard to use.
Existing solutions, unfortunately, do not address the version evolution
problem, especially in a collaborative environment where non-linear version
control semantics are necessary to isolate operations made by different user
roles. The lack of version control semantics also incurs unnecessary storage
consumption and lowers efficiency due to data duplication and repeated data
pre-processing, which are avoidable. In this paper, we identify two main
challenges that arise during the deployment of machine learning pipelines, and
address them with the design of versioning for an end-to-end analytics system
MLCask. The system supports multiple user roles with the ability to perform
Git-like branching and merging operations in the context of the machine
learning pipelines. We define and accelerate the metric-driven merge operation
by pruning the pipeline search tree using reusable history records and pipeline
compatibility information. Further, we design and implement the prioritized
pipeline search, which gives preference to the pipelines that probably yield
better performance. The effectiveness of MLCask is evaluated through an
extensive study over several real-world deployment cases. The performance
evaluation shows that the proposed merge operation is up to 7.8x faster and
saves up to 11.9x storage space than the baseline method that does not utilize
history records.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.08600v2,2020-11-16T06:26:16Z,2020-10-16T19:40:08Z,"Robot Navigation in Constrained Pedestrian Environments using
  Reinforcement Learning","Navigating fluently around pedestrians is a necessary capability for mobile
robots deployed in human environments, such as buildings and homes. While
research on social navigation has focused mainly on the scalability with the
number of pedestrians in open spaces, typical indoor environments present the
additional challenge of constrained spaces such as corridors and doorways that
limit maneuverability and influence patterns of pedestrian interaction. We
present an approach based on reinforcement learning (RL) to learn policies
capable of dynamic adaptation to the presence of moving pedestrians while
navigating between desired locations in constrained environments. The policy
network receives guidance from a motion planner that provides waypoints to
follow a globally planned trajectory, whereas RL handles the local
interactions. We explore a compositional principle for multi-layout training
and find that policies trained in a small set of geometrically simple layouts
successfully generalize to more complex unseen layouts that exhibit composition
of the structural elements available during training. Going beyond walls-world
like domains, we show transfer of the learned policy to unseen 3D
reconstructions of two real environments. These results support the
applicability of the compositional principle to navigation in real-world
buildings and indicate promising usage of multi-agent simulation within
reconstructed environments for tasks that involve interaction.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.08098v2,2021-03-07T17:28:22Z,2020-10-16T02:04:45Z,"Agile Robot Navigation through Hallucinated Learning and Sober
  Deployment","Learning from Hallucination (LfH) is a recent machine learning paradigm for
autonomous navigation, which uses training data collected in completely safe
environments and adds numerous imaginary obstacles to make the environment
densely constrained, to learn navigation planners that produce feasible
navigation even in highly constrained (more dangerous) spaces. However, LfH
requires hallucinating the robot perception during deployment to match with the
hallucinated training data, which creates a need for sometimes-infeasible prior
knowledge and tends to generate very conservative planning. In this work, we
propose a new LfH paradigm that does not require runtime hallucination -- a
feature we call ""sober deployment"" -- and can therefore adapt to more realistic
navigation scenarios. This novel Hallucinated Learning and Sober Deployment
(HLSD) paradigm is tested in a benchmark testbed of 300 simulated navigation
environments with a wide range of difficulty levels, and in the real-world. In
most cases, HLSD outperforms both the original LfH method and a classical
navigation planner.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.07634v3,2021-01-24T10:00:18Z,2020-10-15T10:09:09Z,"Towards Reflectivity profile inversion through Artificial Neural
  Networks","The goal of Specular Neutron and X-ray Reflectometry is to infer materials
Scattering Length Density (SLD) profiles from experimental reflectivity curves.
This paper focuses on investigating an original approach to the ill-posed
non-invertible problem which involves the use of Artificial Neural Networks
(ANN). In particular, the numerical experiments described here deal with large
data sets of simulated reflectivity curves and SLD profiles, and aim to assess
the applicability of Data Science and Machine Learning technology to the
analysis of data generated at neutron scattering large scale facilities. It is
demonstrated that, under certain circumstances, properly trained Deep Neural
Networks are capable of correctly recovering plausible SLD profiles when
presented with never-seen-before simulated reflectivity curves. When the
necessary conditions are met, a proper implementation of the described approach
would offer two main advantages over traditional fitting methods when dealing
with real experiments, namely, 1. sample physical models are described under a
new paradigm: detailed layer-by-layer descriptions (SLDs, thicknesses,
roughnesses) are replaced by parameter free curves $\rho(z)$, allowing a-priori
assumptions to be fed in terms of the sample family to which a given sample
belongs (e.g. ""thin film"", ""lamellar structure"", etc.) 2. the time-to-solution
is shrunk by orders of magnitude, enabling faster batch analyses for large
datasets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.06425v1,2020-10-13T14:38:40Z,2020-10-13T14:38:40Z,"Temporal Collaborative Filtering with Graph Convolutional Neural
  Networks","Temporal collaborative filtering (TCF) methods aim at modelling non-static
aspects behind recommender systems, such as the dynamics in users' preferences
and social trends around items. State-of-the-art TCF methods employ recurrent
neural networks (RNNs) to model such aspects. These methods deploy
matrix-factorization-based (MF-based) approaches to learn the user and item
representations. Recently, graph-neural-network-based (GNN-based) approaches
have shown improved performance in providing accurate recommendations over
traditional MF-based approaches in non-temporal CF settings. Motivated by this,
we propose a novel TCF method that leverages GNNs to learn user and item
representations, and RNNs to model their temporal dynamics. A challenge with
this method lies in the increased data sparsity, which negatively impacts
obtaining meaningful quality representations with GNNs. To overcome this
challenge, we train a GNN model at each time step using a set of observed
interactions accumulated time-wise. Comprehensive experiments on real-world
data show the improved performance obtained by our method over several
state-of-the-art temporal and non-temporal CF models.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.05878v2,2022-01-18T19:43:10Z,2020-10-12T17:27:43Z,PECOS: Prediction for Enormous and Correlated Output Spaces,"Many large-scale applications amount to finding relevant results from an
enormous output space of potential candidates. For example, finding the best
matching product from a large catalog or suggesting related search phrases on a
search engine. The size of the output space for these problems can range from
millions to billions, and can even be infinite in some applications. Moreover,
training data is often limited for the long-tail items in the output space.
Fortunately, items in the output space are often correlated thereby presenting
an opportunity to alleviate the data sparsity issue. In this paper, we propose
the Prediction for Enormous and Correlated Output Spaces (PECOS) framework, a
versatile and modular machine learning framework for solving prediction
problems for very large output spaces, and apply it to the eXtreme Multilabel
Ranking (XMR) problem: given an input instance, find and rank the most relevant
items from an enormous but fixed and finite output space. We propose a three
phase framework for PECOS: (i) in the first phase, PECOS organizes the output
space using a semantic indexing scheme, (ii) in the second phase, PECOS uses
the indexing to narrow down the output space by orders of magnitude using a
machine learned matching scheme, and (iii) in the third phase, PECOS ranks the
matched items using a final ranking scheme. The versatility and modularity of
PECOS allows for easy plug-and-play of various choices for the indexing,
matching, and ranking phases. We also develop very fast inference procedures
which allow us to perform XMR predictions in real time; for example, inference
takes less than 1 millisecond per input on the dataset with 2.8 million labels.
The PECOS software is available at https://libpecos.org.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2011.02838v1,2020-10-11T15:04:34Z,2020-10-11T15:04:34Z,"Real-time parameter inference in reduced-order flame models with
  heteroscedastic Bayesian neural network ensembles","The estimation of model parameters with uncertainties from observed data is a
ubiquitous inverse problem in science and engineering. In this paper, we
suggest an inexpensive and easy to implement parameter estimation technique
that uses a heteroscedastic Bayesian Neural Network trained using anchored
ensembling. The heteroscedastic aleatoric error of the network models the
irreducible uncertainty due to parameter degeneracies in our inverse problem,
while the epistemic uncertainty of the Bayesian model captures uncertainties
which may arise from an input observation's out-of-distribution nature. We use
this tool to perform real-time parameter inference in a 6 parameter G-equation
model of a ducted, premixed flame from observations of acoustically excited
flames. We train our networks on a library of 2.1 million simulated flame
videos. Results on the test dataset of simulated flames show that the network
recovers flame model parameters, with the correlation coefficient between
predicted and true parameters ranging from 0.97 to 0.99, and well-calibrated
uncertainty estimates. The trained neural networks are then used to infer model
parameters from real videos of a premixed Bunsen flame captured using a
high-speed camera in our lab. Re-simulation using inferred parameters shows
excellent agreement between the real and simulated flames. Compared to Ensemble
Kalman Filter-based tools that have been proposed for this problem in the
combustion literature, our neural network ensemble achieves better
data-efficiency and our sub-millisecond inference times represent a savings on
computational costs by several orders of magnitude. This allows us to calibrate
our reduced-order flame model in real-time and predict the thermoacoustic
instability behaviour of the flame more accurately.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.04687v2,2021-01-18T19:52:07Z,2020-10-09T17:16:29Z,"A Series of Unfortunate Counterfactual Events: the Role of Time in
  Counterfactual Explanations","Counterfactual explanations are a prominent example of post-hoc
interpretability methods in the explainable Artificial Intelligence research
domain. They provide individuals with alternative scenarios and a set of
recommendations to achieve a sought-after machine learning model outcome.
Recently, the literature has identified desiderata of counterfactual
explanations, such as feasibility, actionability and sparsity that should
support their applicability in real-world contexts. However, we show that the
literature has neglected the problem of the time dependency of counterfactual
explanations. We argue that, due to their time dependency and because of the
provision of recommendations, even feasible, actionable and sparse
counterfactual explanations may not be appropriate in real-world applications.
This is due to the possible emergence of what we call ""unfortunate
counterfactual events."" These events may occur due to the retraining of machine
learning models whose outcomes have to be explained via counterfactual
explanation. Series of unfortunate counterfactual events frustrate the efforts
of those individuals who successfully implemented the recommendations of
counterfactual explanations. This negatively affects people's trust in the
ability of institutions to provide machine learning-supported decisions
consistently. We introduce an approach to address the problem of the emergence
of unfortunate counterfactual events that makes use of histories of
counterfactual explanations. In the final part of the paper we propose an
ethical analysis of two distinct strategies to cope with the challenge of
unfortunate counterfactual events. We show that they respond to an ethically
responsible imperative to preserve the trustworthiness of credit lending
organizations, the decision models they employ, and the social-economic
function of credit lending.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.03665v2,2020-10-22T16:37:39Z,2020-10-07T21:35:16Z,A Bandit-Based Algorithm for Fairness-Aware Hyperparameter Optimization,"Considerable research effort has been guided towards algorithmic fairness but
there is still no major breakthrough. In practice, an exhaustive search over
all possible techniques and hyperparameters is needed to find optimal
fairness-accuracy trade-offs. Hence, coupled with the lack of tools for ML
practitioners, real-world adoption of bias reduction methods is still scarce.
To address this, we present Fairband, a bandit-based fairness-aware
hyperparameter optimization (HO) algorithm. Fairband is conceptually simple,
resource-efficient, easy to implement, and agnostic to both the objective
metrics, model types and the hyperparameter space being explored. Moreover, by
introducing fairness notions into HO, we enable seamless and efficient
integration of fairness objectives into real-world ML pipelines. We compare
Fairband with popular HO methods on four real-world decision-making datasets.
We show that Fairband can efficiently navigate the fairness-accuracy trade-off
through hyperparameter optimization. Furthermore, without extra training cost,
it consistently finds configurations attaining substantially improved fairness
at a comparatively small decrease in predictive accuracy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.04012v2,2021-06-30T15:32:57Z,2020-10-07T14:22:51Z,Invertible Manifold Learning for Dimension Reduction,"Dimension reduction (DR) aims to learn low-dimensional representations of
high-dimensional data with the preservation of essential information. In the
context of manifold learning, we define that the representation after
information-lossless DR preserves the topological and geometric properties of
data manifolds formally, and propose a novel two-stage DR method, called
invertible manifold learning (inv-ML) to bridge the gap between theoretical
information-lossless and practical DR. The first stage includes a homeomorphic
sparse coordinate transformation to learn low-dimensional representations
without destroying topology and a local isometry constraint to preserve local
geometry. In the second stage, a linear compression is implemented for the
trade-off between the target dimension and the incurred information loss in
excessive DR scenarios. Experiments are conducted on seven datasets with a
neural network implementation of inv-ML, called i-ML-Enc. Empirically, i-ML-Enc
achieves invertible DR in comparison with typical existing methods as well as
reveals the characteristics of the learned manifolds. Through latent space
interpolation on real-world datasets, we find that the reliability of tangent
space approximated by the local neighborhood is the key to the success of
manifold-based DR algorithms.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.01397v1,2020-10-03T17:35:44Z,2020-10-03T17:35:44Z,Automated Performance Tuning for Highly-Configurable Software Systems,"Performance is an important non-functional aspect of the software
requirement. Modern software systems are highly-configurable and
misconfigurations may easily cause performance issues. A software system that
suffers performance issues may exhibit low program throughput and long response
time. However, the sheer size of the configuration space makes it challenging
for administrators to manually select and adjust the configuration options to
achieve better performance. In this paper, we propose ConfRL, an approach to
tune software performance automatically. The key idea of ConfRL is to use
reinforcement learning to explore the configuration space by a trial-and-error
approach and to use the feedback received from the environment to tune
configuration option values to achieve better performance. To reduce the cost
of reinforcement learning, ConfRL employs sampling, clustering, and dynamic
state reduction techniques to keep states in a large configuration space
manageable. Our evaluation of four real-world highly-configurable server
programs shows that ConfRL can efficiently and effectively guide software
systems to achieve higher long-term performance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.00432v1,2020-10-01T14:27:28Z,2020-10-01T14:27:28Z,"The RFML Ecosystem: A Look at the Unique Challenges of Applying Deep
  Learning to Radio Frequency Applications","While deep machine learning technologies are now pervasive in
state-of-the-art image recognition and natural language processing
applications, only in recent years have these technologies started to
sufficiently mature in applications related to wireless communications. In
particular, recent research has shown deep machine learning to be an enabling
technology for cognitive radio applications as well as a useful tool for
supplementing expertly defined algorithms for spectrum sensing applications
such as signal detection, estimation, and classification (termed here as Radio
Frequency Machine Learning, or RFML). A major driver for the usage of deep
machine learning in the context of wireless communications is that little, to
no, a priori knowledge of the intended spectral environment is required, given
that there is an abundance of representative data to facilitate training and
evaluation. However, in addition to this fundamental need for sufficient data,
there are other key considerations, such as trust, security, and
hardware/software issues, that must be taken into account before deploying deep
machine learning systems in real-world wireless communication applications.
This paper provides an overview and survey of prior work related to these major
research considerations. In particular, we present their unique considerations
in the RFML application space, which are not generally present in the image,
audio, and/or text application spaces.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2010.07029v2,2021-02-27T19:06:48Z,2020-09-29T21:32:28Z,"Basic principles and concept design of a real-time clinical decision
  support system for managing medical emergencies on missions to Mars","Space agencies and private companies prepare the beginning of human space
exploration for the 2030s with missions to put the first human on the Mars
surface. The absence of gravity and radiation, along with distance, isolation
and hostile environments, are expected to increase medical events where
previously unseen manifestations may arise. The current healthcare strategy
based on telemedicine and the possibility to stabilize and transport the
injured crewmember to a terrestrial definitive medical facility is not
applicable in exploration class missions. Therefore, the need for deploying the
full autonomous capability to solve medical emergencies may guide the design of
future onboard healthcare systems. We present ten basic principles and concept
design of a software suite to bring onboard decision support to help the crew
dealing with medical emergencies taking into consideration physiological
disturbances in space and spaceflight restrictions. 1) give real-time support
for emergency medical decision making, 2) give patient-specific advice for
executive problem-solving, 3) take into account available information from life
support and monitoring of crewmembers, 4) be fully autonomous from remote
facilities, 5) continuously adapt predictions to physiological disturbance and
changing conditions, 6) optimize emergency medical decision making in terms of
mission fundamental priorities, 7) take into account medical supplies and
equipment on board, 8) apply health standards for the level of care V, 9)
implement ethics responsibilities for spaceflights, and 10) apply ethical
standards for artificial intelligence. Based on these principles, we propose an
autonomous clinical decision support system (CDSS) to provide real-time advice
for emergency medical interventions on board of space exploration missions.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.12095v1,2020-09-25T09:11:52Z,2020-09-25T09:11:52Z,Fully reconfigurable coherent optical vector-matrix multiplication,"Optics is a promising platform in which to help realise the next generation
of fast, parallel and energy-efficient computation. We demonstrate a
reconfigurable free-space optical multiplier that is capable of over 3000
computations in parallel, using spatial light modulators with a pixel
resolution of only 340x340. This enables vector-matrix multiplication and
parallel vector-vector multiplication with vector size of up to 56. Our design
is the first to simultaneously support optical implementation of
reconfigurable, large-size and real-valued linear algebraic operations. Such an
optical multiplier can serve as a building block of special-purpose optical
processors such as optical neural networks and optical Ising machines.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.11380v2,2021-03-24T18:29:36Z,2020-09-23T21:19:55Z,"Combining Weighted Total Variation and Deep Image Prior for natural and
  medical image restoration via ADMM","In the last decades, unsupervised deep learning based methods have caught
researchers attention, since in many real applications, such as medical
imaging, collecting a great amount of training examples is not always feasible.
Moreover, the construction of a good training set is time consuming and hard
because the selected data have to be enough representative for the task. In
this paper, we focus on the Deep Image Prior (DIP) framework and we propose to
combine it with a space-variant Total Variation regularizer with an automatic
estimation of the local regularization parameters. Differently from other
existing approaches, we solve the arising minimization problem via the flexible
Alternating Direction Method of Multipliers (ADMM). Furthermore, we provide a
specific implementation also for the standard isotropic Total Variation. The
promising performances of the proposed approach, in terms of PSNR and SSIM
values, are addressed through several experiments on simulated as well as real
natural and medical corrupted images.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.09471v1,2020-09-20T16:36:25Z,2020-09-20T16:36:25Z,"SYNC: A Copula based Framework for Generating Synthetic Data from
  Aggregated Sources","A synthetic dataset is a data object that is generated programmatically, and
it may be valuable to creating a single dataset from multiple sources when
direct collection is difficult or costly. Although it is a fundamental step for
many data science tasks, an efficient and standard framework is absent. In this
paper, we study a specific synthetic data generation task called downscaling, a
procedure to infer high-resolution, harder-to-collect information (e.g.,
individual level records) from many low-resolution, easy-to-collect sources,
and propose a multi-stage framework called SYNC (Synthetic Data Generation via
Gaussian Copula). For given low-resolution datasets, the central idea of SYNC
is to fit Gaussian copula models to each of the low-resolution datasets in
order to correctly capture dependencies and marginal distributions, and then
sample from the fitted models to obtain the desired high-resolution subsets.
Predictive models are then used to merge sampled subsets into one, and finally,
sampled datasets are scaled according to low-resolution marginal constraints.
We make four key contributions in this work: 1) propose a novel framework for
generating individual level data from aggregated data sources by combining
state-of-the-art machine learning and statistical techniques, 2) perform
simulation studies to validate SYNC's performance as a synthetic data
generation algorithm, 3) demonstrate its value as a feature engineering tool,
as well as an alternative to data collection in situations where gathering is
difficult through two real-world datasets, 4) release an easy-to-use framework
implementation for reproducibility and scalability at the production level that
easily incorporates new data.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.09231v1,2020-09-19T13:47:33Z,2020-09-19T13:47:33Z,Adversarial Exposure Attack on Diabetic Retinopathy Imagery,"Diabetic retinopathy (DR) is a leading cause of vision loss in the world and
numerous cutting-edge works have built powerful deep neural networks (DNNs) to
automatically classify the DR cases via the retinal fundus images (RFIs).
However, RFIs are usually affected by the widely existing camera exposure while
the robustness of DNNs to the exposure is rarely explored. In this paper, we
study this problem from the viewpoint of adversarial attack and identify a
totally new task, i.e., adversarial exposure attack generating adversarial
images by tuning image exposure to mislead the DNNs with significantly high
transferability. To this end, we first implement a straightforward method,
i.e., multiplicative-perturbation-based exposure attack, and reveal the big
challenges of this new task. Then, to make the adversarial image naturalness,
we propose the adversarial bracketed exposure fusion that regards the exposure
attack as an element-wise bracketed exposure fusion problem in the
Laplacian-pyramid space. Moreover, to realize high transferability, we further
propose the convolutional bracketed exposure fusion where the element-wise
multiplicative operation is extended to the convolution. We validate our method
on the real public DR dataset with the advanced DNNs, e.g., ResNet50,
MobileNet, and EfficientNet, showing our method can achieve high image quality
and success rate of the transfer attack. Our method reveals the potential
threats to the DNN-based DR automated diagnosis and can definitely benefit the
development of exposure-robust automated DR diagnosis method in the future.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.05835v3,2021-04-03T15:08:50Z,2020-09-12T17:37:36Z,"How Much Can We Really Trust You? Towards Simple, Interpretable Trust
  Quantification Metrics for Deep Neural Networks","A critical step to building trustworthy deep neural networks is trust
quantification, where we ask the question: How much can we trust a deep neural
network? In this study, we take a step towards simple, interpretable metrics
for trust quantification by introducing a suite of metrics for assessing the
overall trustworthiness of deep neural networks based on their behaviour when
answering a set of questions. We conduct a thought experiment and explore two
key questions about trust in relation to confidence: 1) How much trust do we
have in actors who give wrong answers with great confidence? and 2) How much
trust do we have in actors who give right answers hesitantly? Based on insights
gained, we introduce the concept of question-answer trust to quantify
trustworthiness of an individual answer based on confident behaviour under
correct and incorrect answer scenarios, and the concept of trust density to
characterize the distribution of overall trust for an individual answer
scenario. We further introduce the concept of trust spectrum for representing
overall trust with respect to the spectrum of possible answer scenarios across
correctly and incorrectly answered questions. Finally, we introduce
NetTrustScore, a scalar metric summarizing overall trustworthiness. The suite
of metrics aligns with past social psychology studies that study the
relationship between trust and confidence. Leveraging these metrics, we
quantify the trustworthiness of several well-known deep neural network
architectures for image recognition to get a deeper understanding of where
trust breaks down. The proposed metrics are by no means perfect, but the hope
is to push the conversation towards better metrics to help guide practitioners
and regulators in producing, deploying, and certifying deep learning solutions
that can be trusted to operate in real-world, mission-critical scenarios.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.05780v1,2020-09-12T12:38:47Z,2020-09-12T12:38:47Z,"EdgeLoc: An Edge-IoT Framework for Robust Indoor Localization Using
  Capsule Networks","With the unprecedented demand for location-based services in indoor
scenarios, wireless indoor localization has become essential for mobile users.
While GPS is not available at indoor spaces, WiFi RSS fingerprinting has become
popular with its ubiquitous accessibility. However, it is challenging to
achieve robust and efficient indoor localization with two major challenges.
First, the localization accuracy can be degraded by the random signal
fluctuations, which would influence conventional localization algorithms that
simply learn handcrafted features from raw fingerprint data. Second, mobile
users are sensitive to the localization delay, but conventional indoor
localization algorithms are computation-intensive and time-consuming. In this
paper, we propose EdgeLoc, an edge-IoT framework for efficient and robust
indoor localization using capsule networks. We develop a deep learning model
with the CapsNet to efficiently extract hierarchical information from WiFi
fingerprint data, thereby significantly improving the localization accuracy.
Moreover, we implement an edge-computing prototype system to achieve a nearly
real-time localization process, by enabling mobile users with the deep-learning
model that has been well-trained by the edge server. We conduct a real-world
field experimental study with over 33,600 data points and an extensive
synthetic experiment with the open dataset, and the experimental results
validate the effectiveness of EdgeLoc. The best trade-off of the EdgeLoc system
achieves 98.5% localization accuracy within an average positioning time of only
2.31 ms in the field experiment.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.01654v1,2020-09-03T13:40:13Z,2020-09-03T13:40:13Z,Indoor Localization Techniques Within a Home Monitoring Platform,"This paper details a number of indoor localization techniques developed for
real-time monitoring of older adults. These were developed within the framework
of the i-Light research project that was funded by the European Union. The
project targeted the development and initial evaluation of a configurable and
cost-effective cyber-physical system for monitoring the safety of older adults
who are living in their own homes. Localization hardware consists of a number
of custom-developed devices that replace existing luminaires. In addition to
lighting capabilities, they measure the strength of a Bluetooth Low Energy
signal emitted by a wearable device on the user. Readings are recorded in real
time and sent to a software server for analysis. We present a comparative
evaluation of the accuracy achieved by several server-side algorithms,
including Kalman filtering, a look-back heuristic as well as a neural
network-based approach. It is known that approaches based on measuring signal
strength are sensitive to the placement of walls, construction materials used,
the presence of doors as well as existing furniture. As such, we evaluate the
proposed approaches in two separate locations having distinct building
characteristics. We show that the proposed techniques improve the accuracy of
localization. As the final step, we evaluate our results against comparable
existing approaches.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.00871v1,2020-09-02T07:45:03Z,2020-09-02T07:45:03Z,"HL-Pow: A Learning-Based Power Modeling Framework for High-Level
  Synthesis","High-level synthesis (HLS) enables designers to customize hardware designs
efficiently. However, it is still challenging to foresee the correlation
between power consumption and HLS-based applications at an early design stage.
To overcome this problem, we introduce HL-Pow, a power modeling framework for
FPGA HLS based on state-of-the-art machine learning techniques. HL-Pow
incorporates an automated feature construction flow to efficiently identify and
extract features that exert a major influence on power consumption, simply
based upon HLS results, and a modeling flow that can build an accurate and
generic power model applicable to a variety of designs with HLS. By using
HL-Pow, the power evaluation process for FPGA designs can be significantly
expedited because the power inference of HL-Pow is established on HLS instead
of the time-consuming register-transfer level (RTL) implementation flow.
Experimental results demonstrate that HL-Pow can achieve accurate power
modeling that is only 4.67% (24.02 mW) away from onboard power measurement. To
further facilitate power-oriented optimizations, we describe a novel design
space exploration (DSE) algorithm built on top of HL-Pow to trade off between
latency and power consumption. This algorithm can reach a close approximation
of the real Pareto frontier while only requiring running HLS flow for 20% of
design points in the entire design space.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.00814v1,2020-09-02T04:35:33Z,2020-09-02T04:35:33Z,Open-set Adversarial Defense,"Open-set recognition and adversarial defense study two key aspects of deep
learning that are vital for real-world deployment. The objective of open-set
recognition is to identify samples from open-set classes during testing, while
adversarial defense aims to defend the network against images with
imperceptible adversarial perturbations. In this paper, we show that open-set
recognition systems are vulnerable to adversarial attacks. Furthermore, we show
that adversarial defense mechanisms trained on known classes do not generalize
well to open-set samples. Motivated by this observation, we emphasize the need
of an Open-Set Adversarial Defense (OSAD) mechanism. This paper proposes an
Open-Set Defense Network (OSDN) as a solution to the OSAD problem. The proposed
network uses an encoder with feature-denoising layers coupled with a classifier
to learn a noise-free latent feature representation. Two techniques are
employed to obtain an informative latent feature space with the objective of
improving open-set performance. First, a decoder is used to ensure that clean
images can be reconstructed from the obtained latent features. Then,
self-supervision is used to ensure that the latent features are informative
enough to carry out an auxiliary task. We introduce a testing protocol to
evaluate OSAD performance and show the effectiveness of the proposed method in
multiple object classification datasets. The implementation code of the
proposed method is available at: https://github.com/rshaojimmy/ECCV2020-OSAD.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.13369v1,2020-08-31T05:12:57Z,2020-08-31T05:12:57Z,"Introducing Representations of Facial Affect in Automated Multimodal
  Deception Detection","Automated deception detection systems can enhance health, justice, and
security in society by helping humans detect deceivers in high-stakes
situations across medical and legal domains, among others. This paper presents
a novel analysis of the discriminative power of dimensional representations of
facial affect for automated deception detection, along with interpretable
features from visual, vocal, and verbal modalities. We used a video dataset of
people communicating truthfully or deceptively in real-world, high-stakes
courtroom situations. We leveraged recent advances in automated emotion
recognition in-the-wild by implementing a state-of-the-art deep neural network
trained on the Aff-Wild database to extract continuous representations of
facial valence and facial arousal from speakers. We experimented with unimodal
Support Vector Machines (SVM) and SVM-based multimodal fusion methods to
identify effective features, modalities, and modeling approaches for detecting
deception. Unimodal models trained on facial affect achieved an AUC of 80%, and
facial affect contributed towards the highest-performing multimodal approach
(adaptive boosting) that achieved an AUC of 91% when tested on speakers who
were not part of training sets. This approach achieved a higher AUC than
existing automated machine learning approaches that used interpretable visual,
vocal, and verbal features to detect deception in this dataset, but did not use
facial affect. Across all videos, deceptive and truthful speakers exhibited
significant differences in facial valence and facial arousal, contributing
computational support to existing psychological theories on affect and
deception. The demonstrated importance of facial affect in our models informs
and motivates the future development of automated, affect-aware machine
learning approaches for modeling and detecting deception and other social
behaviors in-the-wild.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.13223v2,2020-10-25T17:47:42Z,2020-08-30T17:29:43Z,"Deep Reinforcement Learning for Contact-Rich Skills Using Compliant
  Movement Primitives","In recent years, industrial robots have been installed in various industries
to handle advanced manufacturing and high precision tasks. However, further
integration of industrial robots is hampered by their limited flexibility,
adaptability and decision making skills compared to human operators. Assembly
tasks are especially challenging for robots since they are contact-rich and
sensitive to even small uncertainties. While reinforcement learning (RL) offers
a promising framework to learn contact-rich control policies from scratch, its
applicability to high-dimensional continuous state-action spaces remains rather
limited due to high brittleness and sample complexity. To address those issues,
we propose different pruning methods that facilitate convergence and
generalization. In particular, we divide the task into free and contact-rich
sub-tasks, perform the control in Cartesian rather than joint space, and
parameterize the control policy. Those pruning methods are naturally
implemented within the framework of dynamic movement primitives (DMP). To
handle contact-rich tasks, we extend the DMP framework by introducing a
coupling term that acts like the human wrist and provides active compliance
under contact with the environment. We demonstrate that the proposed method can
learn insertion skills that are invariant to space, size, shape, and closely
related scenarios, while handling large uncertainties. Finally we demonstrate
that the learned policy can be easily transferred from simulations to real
world and achieve similar performance on UR5e robot.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.13585v1,2020-08-26T14:03:25Z,2020-08-26T14:03:25Z,At Your Service: Coffee Beans Recommendation From a Robot Assistant,"With advances in the field of machine learning, precisely algorithms for
recommendation systems, robot assistants are envisioned to become more present
in the hospitality industry. Additionally, the COVID-19 pandemic has also
highlighted the need to have more service robots in our everyday lives, to
minimise the risk of human to-human transmission. One such example would be
coffee shops, which have become intrinsic to our everyday lives. However,
serving an excellent cup of coffee is not a trivial feat as a coffee blend
typically comprises rich aromas, indulgent and unique flavours and a lingering
aftertaste. Our work addresses this by proposing a computational model which
recommends optimal coffee beans resulting from the user's preferences.
Specifically, given a set of coffee bean properties (objective features), we
apply different supervised learning techniques to predict coffee qualities
(subjective features). We then consider an unsupervised learning method to
analyse the relationship between coffee beans in the subjective feature space.
Evaluated on a real coffee beans dataset based on digitised reviews, our
results illustrate that the proposed computational model gives up to 92.7
percent recommendation accuracy for coffee beans prediction. From this, we
propose how this computational model can be deployed on a service robot to
reliably predict customers' coffee bean preferences, starting from the user
inputting their coffee preferences to the robot recommending the coffee beans
that best meet the user's likings.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2009.07632v1,2020-08-26T08:58:29Z,2020-08-26T08:58:29Z,"Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia
  Research Agenda","Participation on social media platforms has many benefits but also poses
substantial threats. Users often face an unintended loss of privacy, are
bombarded with mis-/disinformation, or are trapped in filter bubbles due to
over-personalized content. These threats are further exacerbated by the rise of
hidden AI-driven algorithms working behind the scenes to shape users' thoughts,
attitudes, and behavior. We investigate how multimedia researchers can help
tackle these problems to level the playing field for social media users. We
perform a comprehensive survey of algorithmic threats on social media and use
it as a lens to set a challenging but important research agenda for effective
and real-time user nudging. We further implement a conceptual prototype and
evaluate it with experts to supplement our research agenda. This paper calls
for solutions that combat the algorithmic threats on social media by utilizing
machine learning and multimedia content analysis techniques but in a
transparent manner and for the benefit of the users.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.08563v2,2020-08-30T11:05:13Z,2020-08-19T17:41:37Z,"Physically-Constrained Transfer Learning through Shared Abundance Space
  for Hyperspectral Image Classification","Hyperspectral image (HSI) classification is one of the most active research
topics and has achieved promising results boosted by the recent development of
deep learning. However, most state-of-the-art approaches tend to perform poorly
when the training and testing images are on different domains, e.g., source
domain and target domain, respectively, due to the spectral variability caused
by different acquisition conditions. Transfer learning-based methods address
this problem by pre-training in the source domain and fine-tuning on the target
domain. Nonetheless, a considerable amount of data on the target domain has to
be labeled and non-negligible computational resources are required to retrain
the whole network. In this paper, we propose a new transfer learning scheme to
bridge the gap between the source and target domains by projecting the HSI data
from the source and target domains into a shared abundance space based on their
own physical characteristics. In this way, the domain discrepancy would be
largely reduced such that the model trained on the source domain could be
applied on the target domain without extra efforts for data labeling or network
retraining. The proposed method is referred to as physically-constrained
transfer learning through shared abundance space (PCTL-SAS). Extensive
experimental results demonstrate the superiority of the proposed method as
compared to the state-of-the-art. The success of this endeavor would largely
facilitate the deployment of HSI classification for real-world sensing
scenarios.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.05381v1,2020-08-12T15:29:11Z,2020-08-12T15:29:11Z,"Improving the Performance of Fine-Grain Image Classifiers via Generative
  Data Augmentation","Recent advances in machine learning (ML) and computer vision tools have
enabled applications in a wide variety of arenas such as financial analytics,
medical diagnostics, and even within the Department of Defense. However, their
widespread implementation in real-world use cases poses several challenges: (1)
many applications are highly specialized, and hence operate in a \emph{sparse
data} domain; (2) ML tools are sensitive to their training sets and typically
require cumbersome, labor-intensive data collection and data labelling
processes; and (3) ML tools can be extremely ""black box,"" offering users little
to no insight into the decision-making process or how new data might affect
prediction performance. To address these challenges, we have designed and
developed Data Augmentation from Proficient Pre-Training of Robust Generative
Adversarial Networks (DAPPER GAN), an ML analytics support tool that
automatically generates novel views of training images in order to improve
downstream classifier performance. DAPPER GAN leverages high-fidelity
embeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to
create novel imagery for previously unseen classes. We experimentally evaluate
this technique on the Stanford Cars dataset, demonstrating improved vehicle
make and model classification accuracy and reduced requirements for real data
using our GAN based data augmentation framework. The method's validity was
supported through an analysis of classifier performance on both augmented and
non-augmented datasets, achieving comparable or better accuracy with up to 30\%
less real data across visually similar classes. To support this method, we
developed a novel augmentation method that can manipulate semantically
meaningful dimensions (e.g., orientation) of the target object in the embedding
space.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.02321v2,2021-02-25T03:04:37Z,2020-08-05T19:00:36Z,"Can I Pour into It? Robot Imagining Open Containability Affordance of
  Previously Unseen Objects via Physical Simulations","Open containers, i.e., containers without covers, are an important and
ubiquitous class of objects in human life. In this letter, we propose a novel
method for robots to ""imagine"" the open containability affordance of a
previously unseen object via physical simulations. We implement our imagination
method on a UR5 manipulator. The robot autonomously scans the object with an
RGB-D camera. The scanned 3D model is used for open containability imagination
which quantifies the open containability affordance by physically simulating
dropping particles onto the object and counting how many particles are retained
in it. This quantification is used for open-container vs. non-open-container
binary classification (hereafter referred to as open container classification).
If the object is classified as an open container, the robot further imagines
pouring into the object, again using physical simulations, to obtain the
pouring position and orientation for real robot autonomous pouring. We evaluate
our method on open container classification and autonomous pouring of granular
material on a dataset containing 130 previously unseen objects with 57 object
categories. Although our proposed method uses only 11 objects for simulation
calibration (training), its open container classification aligns well with
human judgements. In addition, our method endows the robot with the capability
to autonomously pour into the 55 containers in the dataset with a very high
success rate. We also compare to a deep learning method. Results show that our
method achieves the same performance as the deep learning method on open
container classification and outperforms it on autonomous pouring. Moreover,
our method is fully explainable.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.01567v3,2021-02-20T01:06:06Z,2020-08-01T02:32:43Z,"Multi-Slice Fusion for Sparse-View and Limited-Angle 4D CT
  Reconstruction","Inverse problems spanning four or more dimensions such as space, time and
other independent parameters have become increasingly important.
State-of-the-art 4D reconstruction methods use model based iterative
reconstruction (MBIR), but depend critically on the quality of the prior
modeling. Recently, plug-and-play (PnP) methods have been shown to be an
effective way to incorporate advanced prior models using state-of-the-art
denoising algorithms. However, state-of-the-art denoisers such as BM4D and deep
convolutional neural networks (CNNs) are primarily available for 2D or 3D
images and extending them to higher dimensions is difficult due to algorithmic
complexity and the increased difficulty of effective training.
  In this paper, we present multi-slice fusion, a novel algorithm for 4D
reconstruction, based on the fusion of multiple low-dimensional denoisers. Our
approach uses multi-agent consensus equilibrium (MACE), an extension of
plug-and-play, as a framework for integrating the multiple lower-dimensional
models. We apply our method to 4D cone-beam X-ray CT reconstruction for non
destructive evaluation (NDE) of samples that are dynamically moving during
acquisition. We implement multi-slice fusion on distributed, heterogeneous
clusters in order to reconstruct large 4D volumes in reasonable time and
demonstrate the inherent parallelizable nature of the algorithm. We present
simulated and real experimental results on sparse-view and limited-angle CT
data to demonstrate that multi-slice fusion can substantially improve the
quality of reconstructions relative to traditional methods, while also being
practical to implement and train.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.14573v2,2021-06-02T03:00:12Z,2020-07-29T03:33:18Z,FIVES: Feature Interaction Via Edge Search for Large-Scale Tabular Data,"High-order interactive features capture the correlation between different
columns and thus are promising to enhance various learning tasks on ubiquitous
tabular data. To automate the generation of interactive features, existing
works either explicitly traverse the feature space or implicitly express the
interactions via intermediate activations of some designed models. These two
kinds of methods show that there is essentially a trade-off between feature
interpretability and search efficiency. To possess both of their merits, we
propose a novel method named Feature Interaction Via Edge Search (FIVES), which
formulates the task of interactive feature generation as searching for edges on
the defined feature graph. Specifically, we first present our theoretical
evidence that motivates us to search for useful interactive features with
increasing order. Then we instantiate this search strategy by optimizing both a
dedicated graph neural network (GNN) and the adjacency tensor associated with
the defined feature graph. In this way, the proposed FIVES method simplifies
the time-consuming traversal as a typical training course of GNN and enables
explicit feature generation according to the learned adjacency tensor.
Experimental results on both benchmark and real-world datasets show the
advantages of FIVES over several state-of-the-art methods. Moreover, the
interactive features identified by FIVES are deployed on the recommender system
of Taobao, a worldwide leading e-commerce platform. Results of an online A/B
testing further verify the effectiveness of the proposed method FIVES, and we
further provide FIVES as AI utilities for the customers of Alibaba Cloud.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.14432v1,2020-07-28T18:47:21Z,2020-07-28T18:47:21Z,"A Convolutional Neural Network for gaze preference detection: A
  potential tool for diagnostics of autism spectrum disorder in children","Early diagnosis of autism spectrum disorder (ASD) is known to improve the
quality of life of affected individuals. However, diagnosis is often delayed
even in wealthier countries including the US, largely due to the fact that gold
standard diagnostic tools such as the Autism Diagnostic Observation Schedule
(ADOS) and the Autism Diagnostic Interview-Revised (ADI-R) are time consuming
and require expertise to administer. This trend is even more pronounced lower
resources settings due to a lack of trained experts. As a result, alternative,
less technical methods that leverage the unique ways in which children with ASD
react to visual stimulation in a controlled environment have been developed to
help facilitate early diagnosis. Previous studies have shown that, when exposed
to a video that presents both social and abstract scenes side by side, a child
with ASD will focus their attention towards the abstract images on the screen
to a greater extent than a child without ASD. Such differential responses make
it possible to implement an algorithm for the rapid diagnosis of ASD based on
eye tracking against different visual stimuli. Here we propose a convolutional
neural network (CNN) algorithm for gaze prediction using images extracted from
a one-minute stimulus video. Our model achieved a high accuracy rate and
robustness for prediction of gaze direction with independent persons and
employing a different camera than the one used during testing. In addition to
this, the proposed algorithm achieves a fast response time, providing a near
real-time evaluation of ASD. Thereby, by applying the proposed method, we could
significantly reduce the diagnosis time and facilitate the diagnosis of ASD in
low resource regions.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.14390v5,2022-03-05T20:30:32Z,2020-07-28T17:59:07Z,Flower: A Friendly Federated Learning Research Framework,"Federated Learning (FL) has emerged as a promising technique for edge devices
to collaboratively learn a shared prediction model, while keeping their
training data on the device, thereby decoupling the ability to do machine
learning from the need to store the data in the cloud. However, FL is difficult
to implement realistically, both in terms of scale and systems heterogeneity.
Although there are a number of research frameworks available to simulate FL
algorithms, they do not support the study of scalable FL workloads on
heterogeneous edge devices.
  In this paper, we present Flower -- a comprehensive FL framework that
distinguishes itself from existing platforms by offering new facilities to
execute large-scale FL experiments and consider richly heterogeneous FL device
scenarios. Our experiments show Flower can perform FL experiments up to 15M in
client size using only a pair of high-end GPUs. Researchers can then seamlessly
migrate experiments to real devices to examine other parts of the design space.
We believe Flower provides the community with a critical new tool for FL study
and development.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.13127v1,2020-07-26T13:27:05Z,2020-07-26T13:27:05Z,What Government by Algorithm Might Look Like,"Algocracy is the rule by algorithms. This paper summarises technologies
useful to create algocratic social machines and presents idealistic examples of
their application. In particular, it describes smart contracts and their
implementations, challenges of behaviour mining and prediction, as well as
game-theoretic and AI approaches to mechanism design. The presented idealistic
examples of new algocratic solutions are picked from the reality of a modern
state. The examples are science funding, trade by organisations, regulation of
rental agreements, ranking of significance and sortition. Artificial General
Intelligence is not in the scope of this feasibility study.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.13004v1,2020-07-25T20:07:28Z,2020-07-25T20:07:28Z,Learning Attribute-Structure Co-Evolutions in Dynamic Graphs,"Most graph neural network models learn embeddings of nodes in static
attributed graphs for predictive analysis. Recent attempts have been made to
learn temporal proximity of the nodes. We find that real dynamic attributed
graphs exhibit complex co-evolution of node attributes and graph structure.
Learning node embeddings for forecasting change of node attributes and birth
and death of links over time remains an open problem. In this work, we present
a novel framework called CoEvoGNN for modeling dynamic attributed graph
sequence. It preserves the impact of earlier graphs on the current graph by
embedding generation through the sequence. It has a temporal self-attention
mechanism to model long-range dependencies in the evolution. Moreover, CoEvoGNN
optimizes model parameters jointly on two dynamic tasks, attribute inference
and link prediction over time. So the model can capture the co-evolutionary
patterns of attribute change and link formation. This framework can adapt to
any graph neural algorithms so we implemented and investigated three methods
based on it: CoEvoGCN, CoEvoGAT, and CoEvoSAGE. Experiments demonstrate the
framework (and its methods) outperform strong baselines on predicting an entire
unseen graph snapshot of personal attributes and interpersonal links in dynamic
social graphs and financial graphs.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.12640v1,2020-07-24T16:50:41Z,2020-07-24T16:50:41Z,"Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning
  on Graphs","We consider an autonomous exploration problem in which a range-sensing mobile
robot is tasked with accurately mapping the landmarks in an a priori unknown
environment efficiently in real-time; it must choose sensing actions that both
curb localization uncertainty and achieve information gain. For this problem,
belief space planning methods that forward-simulate robot sensing and
estimation may often fail in real-time implementation, scaling poorly with
increasing size of the state, belief and action spaces. We propose a novel
approach that uses graph neural networks (GNNs) in conjunction with deep
reinforcement learning (DRL), enabling decision-making over graphs containing
exploration information to predict a robot's optimal sensing action in belief
space. The policy, which is trained in different random environments without
human intervention, offers a real-time, scalable decision-making process whose
high-performance exploratory sensing actions yield accurate maps and high rates
of information gain.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.12327v2,2021-06-26T00:31:19Z,2020-07-24T03:09:26Z,"Stochastic Dynamic Information Flow Tracking Game using Supervised
  Learning for Detecting Advanced Persistent Threats","Advanced persistent threats (APTs) are organized prolonged cyberattacks by
sophisticated attackers. Although APT activities are stealthy, they interact
with the system components and these interactions lead to information flows.
Dynamic Information Flow Tracking (DIFT) has been proposed as one of the
effective ways to detect APTs using the information flows. However, wide range
security analysis using DIFT results in a significant increase in performance
overhead and high rates of false-positives and false-negatives generated by
DIFT. In this paper, we model the strategic interaction between APT and DIFT as
a non-cooperative stochastic game. The game unfolds on a state space
constructed from an information flow graph (IFG) that is extracted from the
system log. The objective of the APT in the game is to choose transitions in
the IFG to find an optimal path in the IFG from an entry point of the attack to
an attack target. On the other hand, the objective of DIFT is to dynamically
select nodes in the IFG to perform security analysis for detecting APT. Our
game model has imperfect information as the players do not have information
about the actions of the opponent. We consider two scenarios of the game (i)
when the false-positive and false-negative rates are known to both players and
(ii) when the false-positive and false-negative rates are unknown to both
players. Case (i) translates to a game model with complete information and we
propose a value iteration-based algorithm and prove the convergence. Case (ii)
translates to a game with unknown transition probabilities. In this case, we
propose Hierarchical Supervised Learning (HSL) algorithm that integrates a
neural network, to predict the value vector of the game, with a policy
iteration algorithm to compute an approximate equilibrium. We implemented our
algorithms on real attack datasets and validated the performance of our
approach.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.10243v1,2020-07-20T16:32:27Z,2020-07-20T16:32:27Z,Inter-Homines: Distance-Based Risk Estimation for Human Safety,"In this document, we report our proposal for modeling the risk of possible
contagiousity in a given area monitored by RGB cameras where people freely move
and interact. Our system, called Inter-Homines, evaluates in real-time the
contagion risk in a monitored area by analyzing video streams: it is able to
locate people in 3D space, calculate interpersonal distances and predict risk
levels by building dynamic maps of the monitored area. Inter-Homines works both
indoor and outdoor, in public and private crowded areas. The software is
applicable to already installed cameras or low-cost cameras on industrial PCs,
equipped with an additional embedded edge-AI system for temporary measurements.
From the AI-side, we exploit a robust pipeline for real-time people detection
and localization in the ground plane by homographic transformation based on
state-of-the-art computer vision algorithms; it is a combination of a people
detector and a pose estimator. From the risk modeling side, we propose a
parametric model for a spatio-temporal dynamic risk estimation, that, validated
by epidemiologists, could be useful for safety monitoring the acceptance of
social distancing prevention measures by predicting the risk level of the
scene.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.10227v2,2020-08-29T19:38:40Z,2020-07-20T16:17:27Z,"Nengo and low-power AI hardware for robust, embedded neurorobotics","In this paper we demonstrate how the Nengo neural modeling and simulation
libraries enable users to quickly develop robotic perception and action neural
networks for simulation on neuromorphic hardware using familiar tools, such as
Keras and Python. We identify four primary challenges in building robust,
embedded neurorobotic systems: 1) developing infrastructure for interfacing
with the environment and sensors; 2) processing task specific sensory signals;
3) generating robust, explainable control signals; and 4) compiling neural
networks to run on target hardware. Nengo helps to address these challenges by:
1) providing the NengoInterfaces library, which defines a simple but powerful
API for users to interact with simulations and hardware; 2) providing the
NengoDL library, which lets users use the Keras and TensorFlow API to develop
Nengo models; 3) implementing the Neural Engineering Framework, which provides
white-box methods for implementing known functions and circuits; and 4)
providing multiple backend libraries, such as NengoLoihi, that enable users to
compile the same model to different hardware. We present two examples using
Nengo to develop neural networks that run on CPUs, GPUs, and Intel's
neuromorphic chip, Loihi, to demonstrate this workflow. The first example is an
end-to-end spiking neural network that controls a rover simulated in Mujoco.
The network integrates a deep convolutional network that processes visual input
from mounted cameras to track a target, and a control system implementing
steering and drive functions to guide the rover to the target. The second
example augments a force-based operational space controller with neural
adaptive control to improve performance during a reaching task using a
real-world Kinova Jaco2 robotic arm. Code and details are provided with the
intent of enabling other researchers to build their own neurorobotic systems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.09055v1,2020-07-17T15:30:38Z,2020-07-17T15:30:38Z,Hyperparameter Selection for Offline Reinforcement Learning,"Offline reinforcement learning (RL purely from logged data) is an important
avenue for deploying RL techniques in real-world scenarios. However, existing
hyperparameter selection methods for offline RL break the offline assumption by
evaluating policies corresponding to each hyperparameter setting in the
environment. This online execution is often infeasible and hence undermines the
main aim of offline RL. Therefore, in this work, we focus on \textit{offline
hyperparameter selection}, i.e. methods for choosing the best policy from a set
of many policies trained using different hyperparameters, given only logged
data. Through large-scale empirical evaluation we show that: 1) offline RL
algorithms are not robust to hyperparameter choices, 2) factors such as the
offline RL algorithm and method for estimating Q values can have a big impact
on hyperparameter selection, and 3) when we control those factors carefully, we
can reliably rank policies across hyperparameter choices, and therefore choose
policies which are close to the best policy in the set. Overall, our results
present an optimistic view that offline hyperparameter selection is within
reach, even in challenging tasks with pixel observations, high dimensional
action spaces, and long horizon.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.10784v2,2021-07-10T17:50:27Z,2020-07-16T21:14:45Z,Fast Neural Models for Symbolic Regression at Scale,"Deep learning owes much of its success to the astonishing expressiveness of
neural networks. However, this comes at the cost of complex, black-boxed models
that extrapolate poorly beyond the domain of the training dataset, conflicting
with goals of finding analytic expressions to describe science, engineering and
real world data. Under the hypothesis that the hierarchical modularity of such
laws can be captured by training a neural network, we introduce OccamNet, a
neural network model that finds interpretable, compact, and sparse solutions
for fitting data, \`{a} la Occam's razor. Our model defines a probability
distribution over a non-differentiable function space. We introduce a two-step
optimization method that samples functions and updates the weights with
backpropagation based on cross-entropy matching in an evolutionary strategy: we
train by biasing the probability mass toward better fitting solutions. OccamNet
is able to fit a variety of symbolic laws including simple analytic functions,
recursive programs, implicit functions, simple image classification, and can
outperform noticeably state-of-the-art symbolic regression methods on real
world regression datasets. Our method requires minimal memory footprint, does
not require AI accelerators for efficient training, fits complicated functions
in minutes of training on a single CPU, and demonstrates significant
performance gains when scaled on a GPU. Our implementation, demonstrations and
instructions for reproducing the experiments are available at
https://github.com/druidowm/OccamNet_Public.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.09072v1,2020-07-15T09:40:13Z,2020-07-15T09:40:13Z,"Joint Multi-User DNN Partitioning and Computational Resource Allocation
  for Collaborative Edge Intelligence","Mobile Edge Computing (MEC) has emerged as a promising supporting
architecture providing a variety of resources to the network edge, thus acting
as an enabler for edge intelligence services empowering massive mobile and
Internet of Things (IoT) devices with AI capability. With the assistance of
edge servers, user equipments (UEs) are able to run deep neural network (DNN)
based AI applications, which are generally resource-hungry and
compute-intensive, such that an individual UE can hardly afford by itself in
real time. However the resources in each individual edge server are typically
limited. Therefore, any resource optimization involving edge servers is by
nature a resource-constrained optimization problem and needs to be tackled in
such realistic context. Motivated by this observation, we investigate the
optimization problem of DNN partitioning (an emerging DNN offloading scheme) in
a realistic multi-user resource-constrained condition that rarely considered in
previous works. Despite the extremely large solution space, we reveal several
properties of this specific optimization problem of joint multi-UE DNN
partitioning and computational resource allocation. We propose an algorithm
called Iterative Alternating Optimization (IAO) that can achieve the optimal
solution in polynomial time. In addition, we present rigorous theoretic
analysis of our algorithm in terms of time complexity and performance under
realistic estimation error. Moreover, we build a prototype that implements our
framework and conduct extensive experiments using realistic DNN models, whose
results demonstrate its effectiveness and efficiency.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.05889v5,2021-04-12T17:23:01Z,2020-07-12T02:21:19Z,"Deep Learning Techniques to make Gravitational Wave Detections from Weak
  Time-Series Data","Gravitational waves are ripples in the space time fabric when high energy
events such as black hole mergers or neutron star collisions take place. The
first Gravitational Wave (GW) detection (GW150914) was made by the Laser
Interferometer Gravitational-wave Observatory (LIGO) and Virgo Collaboration on
September 14, 2015. Furthermore, the proof of the existence of GWs had
countless implications from Stellar Evolution to General Relativity.
Gravitational waves detection requires multiple filters and the filtered data
has to be studied intensively to come to conclusions on whether the data is a
just a glitch or an actual gravitational wave detection. However, with the use
of Deep Learning the process is simplified heavily, as it reduces the level of
filtering greatly, and the output is more definitive, even though the model
produces a probabilistic result. Our technique, Deep Learning, utilizes a
different implementation of a one-dimensional convolutional neural network
(CNN). The model is trained by a composite of real LIGO noise, and injections
of GW waveform templates. The CNN effectively uses classification to
differentiate weak GW time series from non-gaussian noise from glitches in the
LIGO data stream. In addition, we are the first study to utilize fine-tuning as
a means to train the model with a second pass of data, while maintaining all
the learned features from the initial training iteration. This enables our
model to have a sensitivity of 100%, higher than all prior studies in this
field, when making real-time detections of GWs at an extremely low
Signal-to-noise ratios (SNR), while still being less computationally expensive.
This sensitivity, in part, is also achieved through the use of deep signal
manifolds from both the Hanford and Livingston detectors, which enable the
neural network to be responsive to false positives.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.04862v2,2020-07-12T15:32:16Z,2020-07-09T15:04:26Z,Attention or memory? Neurointerpretable agents in space and time,"In neuroscience, attention has been shown to bidirectionally interact with
reinforcement learning (RL) processes. This interaction is thought to support
dimensionality reduction of task representations, restricting computations to
relevant features. However, it remains unclear whether these properties can
translate into real algorithmic advantages for artificial agents, especially in
dynamic environments. We design a model incorporating a self-attention
mechanism that implements task-state representations in semantic feature-space,
and test it on a battery of Atari games. To evaluate the agent's selective
properties, we add a large volume of task-irrelevant features to observations.
In line with neuroscience predictions, self-attention leads to increased
robustness to noise compared to benchmark models. Strikingly, this
self-attention mechanism is general enough, such that it can be naturally
extended to implement a transient working-memory, able to solve a partially
observable maze task. Lastly, we highlight the predictive quality of attended
stimuli. Because we use semantic observations, we can uncover not only which
features the agent elects to base decisions on, but also how it chooses to
compile more complex, relational features from simpler ones. These results
formally illustrate the benefits of attention in deep RL and provide evidence
for the interpretability of self-attention mechanisms.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.03639v3,2021-01-11T11:02:34Z,2020-07-07T17:19:56Z,Human Trajectory Forecasting in Crowds: A Deep Learning Perspective,"Since the past few decades, human trajectory forecasting has been a field of
active research owing to its numerous real-world applications: evacuation
situation analysis, deployment of intelligent transport systems, traffic
operations, to name a few. Early works handcrafted this representation based on
domain knowledge. However, social interactions in crowded environments are not
only diverse but often subtle. Recently, deep learning methods have
outperformed their handcrafted counterparts, as they learned about human-human
interactions in a more generic data-driven fashion. In this work, we present an
in-depth analysis of existing deep learning-based methods for modelling social
interactions. We propose two knowledge-based data-driven methods to effectively
capture these social interactions. To objectively compare the performance of
these interaction-based forecasting models, we develop a large scale
interaction-centric benchmark TrajNet++, a significant yet missing component in
the field of human trajectory forecasting. We propose novel performance metrics
that evaluate the ability of a model to output socially acceptable
trajectories. Experiments on TrajNet++ validate the need for our proposed
metrics, and our method outperforms competitive baselines on both real-world
and synthetic datasets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.03578v2,2020-07-08T22:53:16Z,2020-07-07T15:55:50Z,"A Vision-based Social Distancing and Critical Density Detection System
  for COVID-19","Social distancing has been proven as an effective measure against the spread
of the infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are
not used to tracking the required 6-feet (2-meters) distance between themselves
and their surroundings. An active surveillance system capable of detecting
distances between individuals and warning them can slow down the spread of the
deadly disease. Furthermore, measuring social density in a region of interest
(ROI) and modulating inflow can decrease social distancing violation occurrence
chance.
  On the other hand, recording data and labeling individuals who do not follow
the measures will breach individuals' rights in free-societies. Here we propose
an Artificial Intelligence (AI) based real-time social distancing detection and
warning system considering four important ethical factors: (1) the system
should never record/cache data, (2) the warnings should not target the
individuals, (3) no human supervisor should be in the detection/warning loop,
and (4) the code should be open-source and accessible to the public. Against
this backdrop, we propose using a monocular camera and deep learning-based
real-time object detectors to measure social distancing. If a violation is
detected, a non-intrusive audio-visual warning signal is emitted without
targeting the individual who breached the social distancing measure. Also, if
the social density is over a critical value, the system sends a control signal
to modulate inflow into the ROI. We tested the proposed method across
real-world datasets to measure its generality and performance. The proposed
method is ready for deployment, and our code is open-sourced.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2007.01076v1,2020-07-02T13:08:39Z,2020-07-02T13:08:39Z,"Mining and Tailings Dam Detection In Satellite Imagery Using Deep
  Learning","This work explores the combination of free cloud computing, free open-source
software, and deep learning methods to analyse a real, large-scale problem: the
automatic country-wide identification and classification of surface mines and
mining tailings dams in Brazil. Locations of officially registered mines and
dams were obtained from the Brazilian government open data resource.
Multispectral Sentinel-2 satellite imagery, obtained and processed at the
Google Earth Engine platform, was used to train and test deep neural networks
using the TensorFlow 2 API and Google Colab platform. Fully Convolutional
Neural Networks were used in an innovative way, to search for unregistered ore
mines and tailing dams in large areas of the Brazilian territory. The efficacy
of the approach is demonstrated by the discovery of 263 mines that do not have
an official mining concession. This exploratory work highlights the potential
of a set of new technologies, freely available, for the construction of low
cost data science tools that have high social impact. At the same time, it
discusses and seeks to suggest practical solutions for the complex and serious
problem of illegal mining and the proliferation of tailings dams, which pose
high risks to the population and the environment, especially in developing
countries. Code is made publicly available at:
https://github.com/remis/mining-discovery-with-deep-learning.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2008.03226v1,2020-06-28T20:59:03Z,2020-06-28T20:59:03Z,"The Photoswitch Dataset: A Molecular Machine Learning Benchmark for the
  Advancement of Synthetic Chemistry","The space of synthesizable molecules is greater than $10^{60}$, meaning only
a vanishingly small fraction of these molecules have ever been realized in the
lab. In order to prioritize which regions of this space to explore next,
synthetic chemists need access to accurate molecular property predictions.
While great advances in molecular machine learning have been made, there is a
dearth of benchmarks featuring properties that are useful for the synthetic
chemist. Focussing directly on the needs of the synthetic chemist, we introduce
the Photoswitch Dataset, a new benchmark for molecular machine learning where
improvements in model performance can be immediately observed in the throughput
of promising molecules synthesized in the lab. Photoswitches are a versatile
class of molecule for medical and renewable energy applications where a
molecule's efficacy is governed by its electronic transition wavelengths. We
demonstrate superior performance in predicting these wavelengths compared to
both time-dependent density functional theory (TD-DFT), the incumbent first
principles quantum mechanical approach, as well as a panel of human experts.
Our baseline models are currently being deployed in the lab as part of the
decision process for candidate synthesis. It is our hope that this benchmark
can drive real discoveries in photoswitch chemistry and that future benchmarks
can be introduced to pivot learning algorithm development to benefit more
expansive areas of synthetic chemistry.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.15350v1,2020-06-27T12:13:22Z,2020-06-27T12:13:22Z,"MiniNet: An extremely lightweight convolutional neural network for
  real-time unsupervised monocular depth estimation","Predicting depth from a single image is an attractive research topic since it
provides one more dimension of information to enable machines to better
perceive the world. Recently, deep learning has emerged as an effective
approach to monocular depth estimation. As obtaining labeled data is costly,
there is a recent trend to move from supervised learning to unsupervised
learning to obtain monocular depth. However, most unsupervised learning methods
capable of achieving high depth prediction accuracy will require a deep network
architecture which will be too heavy and complex to run on embedded devices
with limited storage and memory spaces. To address this issue, we propose a new
powerful network with a recurrent module to achieve the capability of a deep
network while at the same time maintaining an extremely lightweight size for
real-time high performance unsupervised monocular depth prediction from video
sequences. Besides, a novel efficient upsample block is proposed to fuse the
features from the associated encoder layer and recover the spatial size of
features with the small number of model parameters. We validate the
effectiveness of our approach via extensive experiments on the KITTI dataset.
Our new model can run at a speed of about 110 frames per second (fps) on a
single GPU, 37 fps on a single CPU, and 2 fps on a Raspberry Pi 3. Moreover, it
achieves higher depth accuracy with nearly 33 times fewer model parameters than
state-of-the-art models. To the best of our knowledge, this work is the first
extremely lightweight neural network trained on monocular video sequences for
real-time unsupervised monocular depth estimation, which opens up the
possibility of implementing deep learning-based real-time unsupervised
monocular depth prediction on low-cost embedded devices.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.13379v1,2020-06-23T23:15:54Z,2020-06-23T23:15:54Z,Deep Generative Model-based Quality Control for Cardiac MRI Segmentation,"In recent years, convolutional neural networks have demonstrated promising
performance in a variety of medical image segmentation tasks. However, when a
trained segmentation model is deployed into the real clinical world, the model
may not perform optimally. A major challenge is the potential poor-quality
segmentations generated due to degraded image quality or domain shift issues.
There is a timely need to develop an automated quality control method that can
detect poor segmentations and feedback to clinicians. Here we propose a novel
deep generative model-based framework for quality control of cardiac MRI
segmentation. It first learns a manifold of good-quality image-segmentation
pairs using a generative model. The quality of a given test segmentation is
then assessed by evaluating the difference from its projection onto the
good-quality manifold. In particular, the projection is refined through
iterative search in the latent space. The proposed method achieves high
prediction accuracy on two publicly available cardiac MRI datasets. Moreover,
it shows better generalisation ability than traditional regression-based
methods. Our approach provides a real-time and model-agnostic quality control
for cardiac MRI segmentation, which has the potential to be integrated into
clinical image analysis workflows.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.13018v3,2021-01-20T22:37:14Z,2020-06-21T07:04:16Z,The classification for High-dimension low-sample size data,"Huge amount of applications in various fields, such as gene expression
analysis or computer vision, undergo data sets with high-dimensional
low-sample-size (HDLSS), which has putted forward great challenges for standard
statistical and modern machine learning methods. In this paper, we propose a
novel classification criterion on HDLSS, tolerance similarity, which emphasizes
the maximization of within-class variance on the premise of class separability.
According to this criterion, a novel linear binary classifier is designed,
denoted by No-separated Data Maximum Dispersion classifier (NPDMD). The
objective of NPDMD is to find a projecting direction w in which all of training
samples scatter in as large an interval as possible. NPDMD has several
characteristics compared to the state-of-the-art classification methods. First,
it works well on HDLSS. Second, it combines the sample statistical information
and local structural information (supporting vectors) into the objective
function to find the solution of projecting direction in the whole feature
spaces. Third, it solves the inverse of high dimensional matrix in low
dimensional space. Fourth, it is relatively simple to be implemented based on
Quadratic Programming. Fifth, it is robust to the model specification for
various real applications. The theoretical properties of NPDMD are deduced. We
conduct a series of evaluations on one simulated and six real-world benchmark
data sets, including face classification and mRNA classification. NPDMD
outperforms those widely used approaches in most cases, or at least obtains
comparable results.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.09191v2,2020-10-25T23:11:44Z,2020-06-16T14:34:40Z,"Sample-Efficient Optimization in the Latent Space of Deep Generative
  Models via Weighted Retraining","Many important problems in science and engineering, such as drug design,
involve optimizing an expensive black-box objective function over a complex,
high-dimensional, and structured input space. Although machine learning
techniques have shown promise in solving such problems, existing approaches
substantially lack sample efficiency. We introduce an improved method for
efficient black-box optimization, which performs the optimization in the
low-dimensional, continuous latent manifold learned by a deep generative model.
In contrast to previous approaches, we actively steer the generative model to
maintain a latent manifold that is highly useful for efficiently optimizing the
objective. We achieve this by periodically retraining the generative model on
the data points queried along the optimization trajectory, as well as weighting
those data points according to their objective function value. This weighted
retraining can be easily implemented on top of existing methods, and is
empirically shown to significantly improve their efficiency and performance on
synthetic and real-world optimization problems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.08852v1,2020-06-16T01:04:26Z,2020-06-16T01:04:26Z,Counterexample-Guided Learning of Monotonic Neural Networks,"The widespread adoption of deep learning is often attributed to its automatic
feature construction with minimal inductive bias. However, in many real-world
tasks, the learned function is intended to satisfy domain-specific constraints.
We focus on monotonicity constraints, which are common and require that the
function's output increases with increasing values of specific input features.
We develop a counterexample-guided technique to provably enforce monotonicity
constraints at prediction time. Additionally, we propose a technique to use
monotonicity as an inductive bias for deep learning. It works by iteratively
incorporating monotonicity counterexamples in the learning process. Contrary to
prior work in monotonic learning, we target general ReLU neural networks and do
not further restrict the hypothesis space. We have implemented these techniques
in a tool called COMET. Experiments on real-world datasets demonstrate that our
approach achieves state-of-the-art results compared to existing monotonic
learners, and can improve the model quality compared to those that were trained
without taking monotonicity constraints into account.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.07333v1,2020-06-12T17:17:01Z,2020-06-12T17:17:01Z,Targeting Learning: Robust Statistics for Reproducible Research,"Targeted Learning is a subfield of statistics that unifies advances in causal
inference, machine learning and statistical theory to help answer
scientifically impactful questions with statistical confidence. Targeted
Learning is driven by complex problems in data science and has been implemented
in a diversity of real-world scenarios: observational studies with missing
treatments and outcomes, personalized interventions, longitudinal settings with
time-varying treatment regimes, survival analysis, adaptive randomized trials,
mediation analysis, and networks of connected subjects. In contrast to the
(mis)application of restrictive modeling strategies that dominate the current
practice of statistics, Targeted Learning establishes a principled standard for
statistical estimation and inference (i.e., confidence intervals and p-values).
This multiply robust approach is accompanied by a guiding roadmap and a
burgeoning software ecosystem, both of which provide guidance on the
construction of estimators optimized to best answer the motivating question.
The roadmap of Targeted Learning emphasizes tailoring statistical procedures so
as to minimize their assumptions, carefully grounding them only in the
scientific knowledge available. The end result is a framework that honestly
reflects the uncertainty in both the background knowledge and the available
data in order to draw reliable conclusions from statistical analyses -
ultimately enhancing the reproducibility and rigor of scientific findings.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.06865v1,2020-06-11T22:58:36Z,2020-06-11T22:58:36Z,Exploring Algorithmic Fairness in Robust Graph Covering Problems,"Fueled by algorithmic advances, AI algorithms are increasingly being deployed
in settings subject to unanticipated challenges with complex social effects.
Motivated by real-world deployment of AI driven, social-network based suicide
prevention and landslide risk management interventions, this paper focuses on
robust graph covering problems subject to group fairness constraints. We show
that, in the absence of fairness constraints, state-of-the-art algorithms for
the robust graph covering problem result in biased node coverage: they tend to
discriminate individuals (nodes) based on membership in traditionally
marginalized groups. To mitigate this issue, we propose a novel formulation of
the robust graph covering problem with group fairness constraints and a
tractable approximation scheme applicable to real-world instances. We provide a
formal analysis of the price of group fairness (PoF) for this problem, where we
show that uncertainty can lead to greater PoF. We demonstrate the effectiveness
of our approach on several real-world social networks. Our method yields
competitive node coverage while significantly improving group fairness relative
to state-of-the-art methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.04924v1,2020-06-08T20:42:39Z,2020-06-08T20:42:39Z,A Self-supervised Approach for Adversarial Robustness,"Adversarial examples can cause catastrophic mistakes in Deep Neural Network
(DNNs) based vision systems e.g., for classification, segmentation and object
detection. The vulnerability of DNNs against such attacks can prove a major
roadblock towards their real-world deployment. Transferability of adversarial
examples demand generalizable defenses that can provide cross-task protection.
Adversarial training that enhances robustness by modifying target model's
parameters lacks such generalizability. On the other hand, different input
processing based defenses fall short in the face of continuously evolving
attacks. In this paper, we take the first step to combine the benefits of both
approaches and propose a self-supervised adversarial training mechanism in the
input space. By design, our defense is a generalizable approach and provides
significant robustness against the \textbf{unseen} adversarial attacks (\eg by
reducing the success rate of translation-invariant \textbf{ensemble} attack
from 82.6\% to 31.9\% in comparison to previous state-of-the-art). It can be
deployed as a plug-and-play solution to protect a variety of vision systems, as
we demonstrate for the case of classification, segmentation and detection. Code
is available at: {\small\url{https://github.com/Muzammal-Naseer/NRP}}.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2006.04403v1,2020-06-08T08:09:20Z,2020-06-08T08:09:20Z,Global Robustness Verification Networks,"The wide deployment of deep neural networks, though achieving great success
in many domains, has severe safety and reliability concerns. Existing
adversarial attack generation and automatic verification techniques cannot
formally verify whether a network is globally robust, i.e., the absence or not
of adversarial examples in the input space. To address this problem, we develop
a global robustness verification framework with three components: 1) a novel
rule-based ``back-propagation'' finding which input region is responsible for
the class assignment by logic reasoning; 2) a new network architecture Sliding
Door Network (SDN) enabling feasible rule-based ``back-propagation''; 3) a
region-based global robustness verification (RGRV) approach. Moreover, we
demonstrate the effectiveness of our approach on both synthetic and real
datasets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.14302v1,2020-05-28T21:25:21Z,2020-05-28T21:25:21Z,Monocular Depth Estimators: Vulnerabilities and Attacks,"Recent advancements of neural networks lead to reliable monocular depth
estimation. Monocular depth estimated techniques have the upper hand over
traditional depth estimation techniques as it only needs one image during
inference. Depth estimation is one of the essential tasks in robotics, and
monocular depth estimation has a wide variety of safety-critical applications
like in self-driving cars and surgical devices. Thus, the robustness of such
techniques is very crucial. It has been shown in recent works that these deep
neural networks are highly vulnerable to adversarial samples for tasks like
classification, detection and segmentation. These adversarial samples can
completely ruin the output of the system, making their credibility in real-time
deployment questionable. In this paper, we investigate the robustness of the
most state-of-the-art monocular depth estimation networks against adversarial
attacks. Our experiments show that tiny perturbations on an image that are
invisible to the naked eye (perturbation attack) and corruption less than about
1% of an image (patch attack) can affect the depth estimation drastically. We
introduce a novel deep feature annihilation loss that corrupts the hidden
feature space representation forcing the decoder of the network to output poor
depth maps. The white-box and black-box test compliments the effectiveness of
the proposed attack. We also perform adversarial example transferability tests,
mainly cross-data transferability.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.13506v4,2020-09-30T09:58:49Z,2020-05-27T17:32:51Z,"Non-Intrusive Reduced-Order Modeling Using Uncertainty-Aware Deep Neural
  Networks and Proper Orthogonal Decomposition: Application to Flood Modeling","Deep Learning research is advancing at a fantastic rate, and there is much to
gain from transferring this knowledge to older fields like Computational Fluid
Dynamics in practical engineering contexts. This work compares state-of-the-art
methods that address uncertainty quantification in Deep Neural Networks,
pushing forward the reduced-order modeling approach of Proper Orthogonal
Decomposition-Neural Networks (POD-NN) with Deep Ensembles and Variational
Inference-based Bayesian Neural Networks on two-dimensional problems in space.
These are first tested on benchmark problems, and then applied to a real-life
application: flooding predictions in the Mille \^Iles river in the Montreal,
Quebec, Canada metropolitan area. Our setup involves a set of input parameters,
with a potentially noisy distribution, and accumulates the simulation data
resulting from these parameters. The goal is to build a non-intrusive surrogate
model that is able to know when it does not know, which is still an open
research area in Neural Networks (and in AI in general). With the help of this
model, probabilistic flooding maps are generated, aware of the model
uncertainty. These insights on the unknown are also utilized for an uncertainty
propagation task, allowing for flooded area predictions that are broader and
safer than those made with a regular uncertainty-uninformed surrogate model.
Our study of the time-dependent and highly nonlinear case of a dam break is
also presented. Both the ensembles and the Bayesian approach lead to reliable
results for multiple smooth physical solutions, providing the correct warning
when going out-of-distribution. However, the former, referred to as POD-EnsNN,
proved much easier to implement and showed greater flexibility than the latter
in the case of discontinuities, where standard algorithms may oscillate or fail
to converge.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.13373v1,2020-05-27T14:08:46Z,2020-05-27T14:08:46Z,Deep Learning Assisted Data Inspection for Radio Astronomy,"Modern radio telescopes combine thousands of receivers, long-distance
networks, large-scale compute hardware, and intricate software. Due to this
complexity, failures occur relatively frequently. In this work we propose novel
use of unsupervised deep learning to diagnose system health for modern radio
telescopes. The model is a convolutional Variational Autoencoder (VAE) that
enables the projection of the high dimensional time-frequency data to a
low-dimensional prescriptive space. Using this projection, telescope operators
are able to visually inspect failures thereby maintaining system health. We
have trained and evaluated the performance of the VAE quantitatively in
controlled experiments on simulated data from HERA. Moreover, we present a
qualitative assessment of the the model trained and tested on real LOFAR data.
Through the use of a naive SVM classifier on the projected synthesised data, we
show that there is a trade-off between the dimensionality of the projection and
the number of compounded features in a given spectrogram. The VAE and SVM
combination scores between 65% and 90% accuracy depending on the number of
features in a given input. Finally, we show the prototype
system-health-diagnostic web framework that integrates the evaluated model. The
system is currently undergoing testing at the ASTRON observatory.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.12392v2,2020-09-11T04:22:34Z,2020-05-25T20:39:59Z,MTFuzz: Fuzzing with a Multi-Task Neural Network,"Fuzzing is a widely used technique for detecting software bugs and
vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary
search to maximize code coverage. Essentially, these fuzzers start with a set
of seed inputs, mutate them to generate new inputs, and identify the promising
inputs using an evolutionary fitness function for further mutation. Despite
their success, evolutionary fuzzers tend to get stuck in long sequences of
unproductive mutations. In recent years, machine learning (ML) based mutation
strategies have reported promising results. However, the existing ML-based
fuzzers are limited by the lack of quality and diversity of the training data.
As the input space of the target programs is high dimensional and sparse, it is
prohibitively expensive to collect many diverse samples demonstrating
successful and unsuccessful mutations to train the model. In this paper, we
address these issues by using a Multi-Task Neural Network that can learn a
compact embedding of the input space based on diverse training samples for
multiple related tasks (i.e., predicting for different types of coverage). The
compact embedding can guide the mutation process by focusing most of the
mutations on the parts of the embedding where the gradient is high. \tool
uncovers $11$ previously unseen bugs and achieves an average of $2\times$ more
edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world
programs.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.10224v2,2021-06-05T22:47:16Z,2020-05-20T17:41:40Z,The Random Feature Model for Input-Output Maps between Banach Spaces,"Well known to the machine learning community, the random feature model is a
parametric approximation to kernel interpolation or regression methods. It is
typically used to approximate functions mapping a finite-dimensional input
space to the real line. In this paper, we instead propose a methodology for use
of the random feature model as a data-driven surrogate for operators that map
an input Banach space to an output Banach space. Although the methodology is
quite general, we consider operators defined by partial differential equations
(PDEs); here, the inputs and outputs are themselves functions, with the input
parameters being functions required to specify the problem, such as initial
data or coefficients, and the outputs being solutions of the problem. Upon
discretization, the model inherits several desirable attributes from this
infinite-dimensional viewpoint, including mesh-invariant approximation error
with respect to the true PDE solution map and the capability to be trained at
one mesh resolution and then deployed at different mesh resolutions. We view
the random feature model as a non-intrusive data-driven emulator, provide a
mathematical framework for its interpretation, and demonstrate its ability to
efficiently and accurately approximate the nonlinear parameter-to-solution maps
of two prototypical PDEs arising in physical science and engineering
applications: viscous Burgers' equation and a variable coefficient elliptic
equation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.08859v2,2021-08-13T17:58:10Z,2020-05-18T16:34:11Z,"PDE constraints on smooth hierarchical functions computed by neural
  networks","Neural networks are versatile tools for computation, having the ability to
approximate a broad range of functions. An important problem in the theory of
deep neural networks is expressivity; that is, we want to understand the
functions that are computable by a given network. We study real infinitely
differentiable (smooth) hierarchical functions implemented by feedforward
neural networks via composing simpler functions in two cases:
  1) each constituent function of the composition has fewer inputs than the
resulting function;
  2) constituent functions are in the more specific yet prevalent form of a
non-linear univariate function (e.g. tanh) applied to a linear multivariate
function.
  We establish that in each of these regimes there exist non-trivial algebraic
partial differential equations (PDEs), which are satisfied by the computed
functions. These PDEs are purely in terms of the partial derivatives and are
dependent only on the topology of the network. For compositions of polynomial
functions, the algebraic PDEs yield non-trivial equations (of degrees dependent
only on the architecture) in the ambient polynomial space that are satisfied on
the associated functional varieties. Conversely, we conjecture that such PDE
constraints, once accompanied by appropriate non-singularity conditions and
perhaps certain inequalities involving partial derivatives, guarantee that the
smooth function under consideration can be represented by the network. The
conjecture is verified in numerous examples including the case of tree
architectures which are of neuroscientific interest. Our approach is a step
toward formulating an algebraic description of functional spaces associated
with specific neural networks, and may provide new, useful tools for
constructing neural networks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.08337v1,2020-05-17T18:46:23Z,2020-05-17T18:46:23Z,A Survey on Unknown Presentation Attack Detection for Fingerprint,"Fingerprint recognition systems are widely deployed in various real-life
applications as they have achieved high accuracy. The widely used applications
include border control, automated teller machine (ATM), and attendance
monitoring systems. However, these critical systems are prone to spoofing
attacks (a.k.a presentation attacks (PA)). PA for fingerprint can be performed
by presenting gummy fingers made from different materials such as silicone,
gelatine, play-doh, ecoflex, 2D printed paper, 3D printed material, or latex.
Biometrics Researchers have developed Presentation Attack Detection (PAD)
methods as a countermeasure to PA. PAD is usually done by training a machine
learning classifier for known attacks for a given dataset, and they achieve
high accuracy in this task. However, generalizing to unknown attacks is an
essential problem from applicability to real-world systems, mainly because
attacks cannot be exhaustively listed in advance. In this survey paper, we
present a comprehensive survey on existing PAD algorithms for fingerprint
recognition systems, specifically from the standpoint of detecting unknown PAD.
We categorize PAD algorithms, point out their advantages/disadvantages, and
future directions for this area.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.07460v1,2020-05-15T10:34:51Z,2020-05-15T10:34:51Z,"Collective Risk Minimization via a Bayesian Model for Statistical
  Software Testing","In the last four years, the number of distinct autonomous vehicles platforms
deployed in the streets of California increased 6-fold, while the reported
accidents increased 12-fold. This can become a trend with no signs of subsiding
as it is fueled by a constant stream of innovations in hardware sensors and
machine learning software. Meanwhile, if we expect the public and regulators to
trust the autonomous vehicle platforms, we need to find better ways to solve
the problem of adding technological complexity without increasing the risk of
accidents. We studied this problem from the perspective of reliability
engineering in which a given risk of an accident has severity and probability
of occurring. Timely information on accidents is important for engineers to
anticipate and reuse previous failures to approximate the risk of accidents in
a new city. However, this is challenging in the context of autonomous vehicles
because of the sparse nature of data on the operational scenarios (driving
trajectories in a new city). Our approach was to mitigate data sparsity by
reducing the state space through monitoring of multiple-vehicles operations. We
then minimized the risk of accidents by determining proper allocation of tests
for each equivalence class. Our contributions comprise (1) a set of strategies
to monitor the operational data of multiple autonomous vehicles, (2) a Bayesian
model that estimates changes in the risk of accidents, and (3) a feedback
control-loop that minimizes these risks by reallocating test effort. Our
results are promising in the sense that we were able to measure and control
risk for a diversity of changes in the operational scenarios. We evaluated our
models with data from two real cities with distinct traffic patterns and made
the data available for the community.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.05287v2,2020-05-25T12:16:12Z,2020-05-11T17:40:58Z,"Using Computer Vision to enhance Safety of Workforce in Manufacturing in
  a Post COVID World","The COVID-19 pandemic forced governments across the world to impose lockdowns
to prevent virus transmissions. This resulted in the shutdown of all economic
activity and accordingly the production at manufacturing plants across most
sectors was halted. While there is an urgency to resume production, there is an
even greater need to ensure the safety of the workforce at the plant site.
Reports indicate that maintaining social distancing and wearing face masks
while at work clearly reduces the risk of transmission. We decided to use
computer vision on CCTV feeds to monitor worker activity and detect violations
which trigger real time voice alerts on the shop floor. This paper describes an
efficient and economic approach of using AI to create a safe environment in a
manufacturing setup. We demonstrate our approach to build a robust social
distancing measurement algorithm using a mix of modern-day deep learning and
classic projective geometry techniques. We have deployed our solution at
manufacturing plants across the Aditya Birla Group (ABG). We have also
described our face mask detection approach which provides a high accuracy
across a range of customized masks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.04849v2,2020-10-16T03:59:24Z,2020-05-11T03:45:22Z,Revealing hidden dynamics from time-series data by ODENet,"To derive the hidden dynamics from observed data is one of the fundamental
but also challenging problems in many different fields. In this study, we
propose a new type of interpretable network called the ordinary differential
equation network (ODENet), in which the numerical integration of explicit
ordinary differential equations (ODEs) are embedded into the machine learning
scheme to build a general framework for revealing the hidden dynamics buried in
massive time-series data efficiently and reliably. ODENet takes full advantage
of both machine learning algorithms and ODE modeling. On one hand, the
embedding of ODEs makes the framework more interpretable benefiting from the
mature theories of ODEs. On the other hand, the schemes of machine learning
enable data handling, paralleling, and optimization to be easily and
efficiently implemented. From classical Lotka-Volterra equations to chaotic
Lorenz equations, the ODENet exhibits its remarkable capability in handling
time-series data even in the presence of large noise. We further apply the
ODENet to real actin aggregation data, which shows an impressive performance as
well. These results demonstrate the superiority of ODENet in dealing with noisy
data, data with either non-equal spacing or large sampling time steps over
other traditional machine learning algorithms.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.02979v3,2021-10-14T16:40:00Z,2020-05-06T17:31:51Z,"A Survey of Algorithms for Black-Box Safety Validation of Cyber-Physical
  Systems","Autonomous cyber-physical systems (CPS) can improve safety and efficiency for
safety-critical applications, but require rigorous testing before deployment.
The complexity of these systems often precludes the use of formal verification
and real-world testing can be too dangerous during development. Therefore,
simulation-based techniques have been developed that treat the system under
test as a black box operating in a simulated environment. Safety validation
tasks include finding disturbances in the environment that cause the system to
fail (falsification), finding the most-likely failure, and estimating the
probability that the system fails. Motivated by the prevalence of
safety-critical artificial intelligence, this work provides a survey of
state-of-the-art safety validation techniques for CPS with a focus on applied
algorithms and their modifications for the safety validation problem. We
present and discuss algorithms in the domains of optimization, path planning,
reinforcement learning, and importance sampling. Problem decomposition
techniques are presented to help scale algorithms to large state spaces, which
are common for CPS. A brief overview of safety-critical applications is given,
including autonomous vehicles and aircraft collision avoidance systems.
Finally, we present a survey of existing academic and commercially available
safety validation tools.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.01557v1,2020-05-04T15:16:30Z,2020-05-04T15:16:30Z,"Off-the-shelf deep learning is not enough: parsimony, Bayes and
  causality","Deep neural networks (""deep learning"") have emerged as a technology of choice
to tackle problems in natural language processing, computer vision, speech
recognition and gameplay, and in just a few years has led to superhuman level
performance and ushered in a new wave of ""AI."" Buoyed by these successes,
researchers in the physical sciences have made steady progress in incorporating
deep learning into their respective domains. However, such adoption brings
substantial challenges that need to be recognized and confronted. Here, we
discuss both opportunities and roadblocks to implementation of deep learning
within materials science, focusing on the relationship between correlative
nature of machine learning and causal hypothesis driven nature of physical
sciences. We argue that deep learning and AI are now well positioned to
revolutionize fields where causal links are known, as is the case for
applications in theory. When confounding factors are frozen or change only
weakly, this leaves open the pathway for effective deep learning solutions in
experimental domains. Similarly, these methods offer a pathway towards
understanding the physics of real-world systems, either via deriving reduced
representations, deducing algorithmic complexity, or recovering generative
physical models. However, extending deep learning and ""AI"" for models with
unclear causal relationship can produce misleading and potentially incorrect
results. Here, we argue the broad adoption of Bayesian methods incorporating
prior knowledge, development of DL solutions with incorporated physical
constraints, and ultimately adoption of causal models, offers a path forward
for fundamental and applied research. Most notably, while these advances can
change the way science is carried out in ways we cannot imagine, machine
learning is not going to substitute science any time soon.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.00828v1,2020-05-02T13:16:16Z,2020-05-02T13:16:16Z,DroTrack: High-speed Drone-based Object Tracking Under Uncertainty,"We present DroTrack, a high-speed visual single-object tracking framework for
drone-captured video sequences. Most of the existing object tracking methods
are designed to tackle well-known challenges, such as occlusion and cluttered
backgrounds. The complex motion of drones, i.e., multiple degrees of freedom in
three-dimensional space, causes high uncertainty. The uncertainty problem leads
to inaccurate location predictions and fuzziness in scale estimations. DroTrack
solves such issues by discovering the dependency between object representation
and motion geometry. We implement an effective object segmentation based on
Fuzzy C Means (FCM). We incorporate the spatial information into the membership
function to cluster the most discriminative segments. We then enhance the
object segmentation by using a pre-trained Convolution Neural Network (CNN)
model. DroTrack also leverages the geometrical angular motion to estimate a
reliable object scale. We discuss the experimental results and performance
evaluation using two datasets of 51,462 drone-captured frames. The combination
of the FCM segmentation and the angular scaling increased DroTrack precision by
up to $9\%$ and decreased the centre location error by $162$ pixels on average.
DroTrack outperforms all the high-speed trackers and achieves comparable
results in comparison to deep learning trackers. DroTrack offers high frame
rates up to 1000 frame per second (fps) with the best location precision, more
than a set of state-of-the-art real-time trackers.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.14690v4,2021-03-10T06:24:57Z,2020-04-30T11:08:49Z,AIBench Training: Balanced Industry-Standard AI Training Benchmarking,"Earlier-stage evaluations of a new AI architecture/system need affordable
benchmarks. Only using a few AI component benchmarks like MLPerfalone in the
other stages may lead to misleading conclusions. Moreover, the learning
dynamics are not well understood, and the benchmarks' shelf-life is short. This
paper proposes a balanced benchmarking methodology. We use real-world
benchmarks to cover the factors space that impacts the learning dynamics to the
most considerable extent. After performing an exhaustive survey on Internet
service AI domains, we identify and implement nineteen representative AI tasks
with state-of-the-art models. For repeatable performance ranking (RPR subset)
and workload characterization (WC subset), we keep two subsets to a minimum for
affordability. We contribute by far the most comprehensive AI training
benchmark suite. The evaluations show: (1) AIBench Training (v1.1) outperforms
MLPerfTraining (v0.7) in terms of diversity and representativeness of model
complexity, computational cost, convergent rate, computation, and memory access
patterns, and hotspot functions; (2) Against the AIBench full benchmarks, its
RPR subset shortens the benchmarking cost by 64%, while maintaining the primary
workload characteristics; (3) The performance ranking shows the single-purpose
AI accelerator like TPU with the optimized TensorFlowframework performs better
than that of GPUs while losing the latter's general support for various AI
models. The specification, source code, and performance numbers are available
from the AIBench homepage
https://www.benchcouncil.org/aibench-training/index.html.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.12161v1,2020-04-25T14:52:11Z,2020-04-25T14:52:11Z,"DAN-SNR: A Deep Attentive Network for Social-Aware Next
  Point-of-Interest Recommendation","Next (or successive) point-of-interest (POI) recommendation has attracted
increasing attention in recent years. Most of the previous studies attempted to
incorporate the spatiotemporal information and sequential patterns of user
check-ins into recommendation models to predict the target user's next move.
However, none of these approaches utilized the social influence of each user's
friends. In this study, we discuss a new topic of next POI recommendation and
present a deep attentive network for social-aware next POI recommendation
called DAN-SNR. In particular, the DAN-SNR makes use of the self-attention
mechanism instead of the architecture of recurrent neural networks to model
sequential influence and social influence in a unified manner. Moreover, we
design and implement two parallel channels to capture short-term user
preference and long-term user preference as well as social influence,
respectively. By leveraging multi-head self-attention, the DAN-SNR can model
long-range dependencies between any two historical check-ins efficiently and
weigh their contributions to the next destination adaptively. Also, we carried
out a comprehensive evaluation using large-scale real-world datasets collected
from two popular location-based social networks, namely Gowalla and Brightkite.
Experimental results indicate that the DAN-SNR outperforms seven competitive
baseline approaches regarding recommendation performance and is of high
efficiency among six neural-network- and attention-based methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.11003v1,2020-04-23T07:03:20Z,2020-04-23T07:03:20Z,Adaptive Techniques in Practical Quantum Key Distribution,"Quantum Key Distribution (QKD) can provide information-theoretically secure
communications and is a strong candidate for the next generation of
cryptography. However, in practice, the performance of QKD is limited by
""practical imperfections"" in realistic sources, channels, and detectors (such
as multi-photon components or imperfect encoding from the sources, losses and
misalignment in the channels, or dark counts in detectors). Addressing such
practical imperfections is a crucial part of implementing QKD protocols with
good performance in reality. There are two highly important future directions
for QKD: (1) QKD over free space, which can allow secure communications between
mobile platforms such as handheld systems, drones, planes, and even satellites,
and (2) fibre-based QKD networks, which can simultaneously provide QKD service
to numerous users at arbitrary locations. These directions are both highly
promising, but so far they are limited by practical imperfections in the
channels and devices, which pose huge challenges and limit their performance.
In this thesis, we develop adaptive techniques with innovative protocol and
algorithm design, as well as novel techniques such as machine learning, to
address some of these key challenges, including (a) atmospheric turbulence in
channels for free-space QKD, (b) asymmetric losses in channels for QKD network,
and (c) efficient parameter optimization in real time, which is important for
both free-space QKD and QKD networks. We believe that this work will pave the
way to important implementations of free-space QKD and fibre-based QKD networks
in the future.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.09608v3,2022-02-02T14:15:46Z,2020-04-20T20:14:00Z,"Flow-based Algorithms for Improving Clusters: A Unifying Framework,
  Software, and Performance","Clustering points in a vector space or nodes in a graph is a ubiquitous
primitive in statistical data analysis, and it is commonly used for exploratory
data analysis. In practice, it is often of interest to ""refine"" or ""improve"" a
given cluster that has been obtained by some other method. In this survey, we
focus on principled algorithms for this cluster improvement problem. Many such
cluster improvement algorithms are flow-based methods, by which we mean that
operationally they require the solution of a sequence of maximum flow problems
on a (typically implicitly) modified data graph. These cluster improvement
algorithms are powerful, both in theory and in practice, but they have not been
widely adopted for problems such as community detection, local graph
clustering, semi-supervised learning, etc. Possible reasons for this are: the
steep learning curve for these algorithms; the lack of efficient and easy to
use software; and the lack of detailed numerical experiments on real-world data
that demonstrate their usefulness. Our objective here is to address these
issues. To do so, we guide the reader through the whole process of
understanding how to implement and apply these powerful algorithms. We present
a unifying fractional programming optimization framework that permits us to
distill, in a simple way, the crucial components of all these algorithms. It
also makes apparent similarities and differences between related methods.
Viewing these cluster improvement algorithms via a fractional programming
framework suggests directions for future algorithm development. Finally, we
develop efficient implementations of these algorithms in our
LocalGraphClustering Python package, and we perform extensive numerical
experiments to demonstrate the performance of these methods on social networks
and image-based data graphs.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.08771v1,2020-04-19T05:21:20Z,2020-04-19T05:21:20Z,Heterogeneous CPU+GPU Stochastic Gradient Descent Algorithms,"The widely-adopted practice is to train deep learning models with specialized
hardware accelerators, e.g., GPUs or TPUs, due to their superior performance on
linear algebra operations. However, this strategy does not employ effectively
the extensive CPU and memory resources -- which are used only for
preprocessing, data transfer, and scheduling -- available by default on the
accelerated servers. In this paper, we study training algorithms for deep
learning on heterogeneous CPU+GPU architectures. Our two-fold objective --
maximize convergence rate and resource utilization simultaneously -- makes the
problem challenging. In order to allow for a principled exploration of the
design space, we first introduce a generic deep learning framework that
exploits the difference in computational power and memory hierarchy between CPU
and GPU through asynchronous message passing. Based on insights gained through
experimentation with the framework, we design two heterogeneous asynchronous
stochastic gradient descent (SGD) algorithms. The first algorithm -- CPU+GPU
Hogbatch -- combines small batches on CPU with large batches on GPU in order to
maximize the utilization of both resources. However, this generates an
unbalanced model update distribution which hinders the statistical convergence.
The second algorithm -- Adaptive Hogbatch -- assigns batches with continuously
evolving size based on the relative speed of CPU and GPU. This balances the
model updates ratio at the expense of a customizable decrease in utilization.
We show that the implementation of these algorithms in the proposed CPU+GPU
framework achieves both faster convergence and higher resource utilization than
TensorFlow on several real datasets and on two computing architectures -- an
on-premises server and a cloud instance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2005.05024v1,2020-04-15T19:10:12Z,2020-04-15T19:10:12Z,Intelligent Tutoring Systems for Generation Z's Addiction,"As generation Z's big data is flooding the Internet through social nets,
neural network based data processing is turning an important cornerstone,
showing significant potential for fast extraction of data patterns. Online
course delivery and associated tutoring are transforming into customizable,
on-demand services driven by the learner. Besides automated grading, strong
potential exists for the development and deployment of next generation
intelligent tutoring software agents. Self-adaptive, online tutoring agents
exhibiting ""intelligent-like"" behavior, being capable ""to learn"" from the
learner, will become the next educational superstars. Over the past decade,
computer-based tutoring agents were deployed in a variety of extended reality
environments, from patient rehabilitation to psychological trauma healing. Most
of these agents are driven by a set of conditional control statements and a
large answers/questions pairs dataset. This article provides a brief
introduction on Generation Z's addiction to digital information, highlights
important efforts for the development of intelligent dialogue systems, and
explains the main components and important design decisions for Intelligent
Tutoring System.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.05953v1,2020-04-13T14:09:21Z,2020-04-13T14:09:21Z,"Software-Defined Network for End-to-end Networked Science at the
  Exascale","Domain science applications and workflow processes are currently forced to
view the network as an opaque infrastructure into which they inject data and
hope that it emerges at the destination with an acceptable Quality of
Experience. There is little ability for applications to interact with the
network to exchange information, negotiate performance parameters, discover
expected performance metrics, or receive status/troubleshooting information in
real time. The work presented here is motivated by a vision for a new smart
network and smart application ecosystem that will provide a more deterministic
and interactive environment for domain science workflows. The Software-Defined
Network for End-to-end Networked Science at Exascale (SENSE) system includes a
model-based architecture, implementation, and deployment which enables
automated end-to-end network service instantiation across administrative
domains. An intent based interface allows applications to express their
high-level service requirements, an intelligent orchestrator and resource
control systems allow for custom tailoring of scalability and real-time
responsiveness based on individual application and infrastructure operator
requirements. This allows the science applications to manage the network as a
first-class schedulable resource as is the current practice for instruments,
compute, and storage systems. Deployment and experiments on production networks
and testbeds have validated SENSE functions and performance. Emulation based
testing verified the scalability needed to support research and education
infrastructures. Key contributions of this work include an architecture
definition, reference implementation, and deployment. This provides the basis
for further innovation of smart network services to accelerate scientific
discovery in the era of big data, cloud computing, machine learning and
artificial intelligence.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.02569v1,2020-04-06T11:32:37Z,2020-04-06T11:32:37Z,"Gradient-Based Training and Pruning of Radial Basis Function Networks
  with an Application in Materials Physics","Many applications, especially in physics and other sciences, call for easily
interpretable and robust machine learning techniques. We propose a fully
gradient-based technique for training radial basis function networks with an
efficient and scalable open-source implementation. We derive novel closed-form
optimization criteria for pruning the models for continuous as well as binary
data which arise in a challenging real-world material physics problem. The
pruned models are optimized to provide compact and interpretable versions of
larger models based on informed assumptions about the data distribution.
Visualizations of the pruned models provide insight into the atomic
configurations that determine atom-level migration processes in solid matter;
these results may inform future research on designing more suitable descriptors
for use with machine learning algorithms.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.00622v1,2020-04-01T17:59:59Z,2020-04-01T17:59:59Z,Evading Deepfake-Image Detectors with White- and Black-Box Attacks,"It is now possible to synthesize highly realistic images of people who don't
exist. Such content has, for example, been implicated in the creation of
fraudulent social-media profiles responsible for dis-information campaigns.
Significant efforts are, therefore, being deployed to detect
synthetically-generated content. One popular forensic approach trains a neural
network to distinguish real from synthetic content.
  We show that such forensic classifiers are vulnerable to a range of attacks
that reduce the classifier to near-0% accuracy. We develop five attack case
studies on a state-of-the-art classifier that achieves an area under the ROC
curve (AUC) of 0.95 on almost all existing image generators, when only trained
on one generator. With full access to the classifier, we can flip the lowest
bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb
1% of the image area to reduce the classifier's AUC to 0.08; or add a single
noise pattern in the synthesizer's latent space to reduce the classifier's AUC
to 0.17. We also develop a black-box attack that, with no access to the target
classifier, reduces the AUC to 0.22. These attacks reveal significant
vulnerabilities of certain image-forensic classifiers.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2004.00104v1,2020-03-31T20:58:14Z,2020-03-31T20:58:14Z,"Improvement of electronic Governance and mobile Governance in
  Multilingual Countries with Digital Etymology using Sanskrit Grammar","With huge improvement of digital connectivity (Wifi,3G,4G) and digital
devices access to internet has reached in the remotest corners now a days.
Rural people can easily access web or apps from PDAs, laptops, smartphones etc.
This is an opportunity of the Government to reach to the citizen in large
number, get their feedback, associate them in policy decision with e governance
without deploying huge man, material or resourses. But the Government of
multilingual countries face a lot of problem in successful implementation of
Government to Citizen (G2C) and Citizen to Government (C2G) governance as the
rural people tend and prefer to interact in their native languages. Presenting
equal experience over web or app to different language group of speakers is a
real challenge. In this research we have sorted out the problems faced by Indo
Aryan speaking netizens which is in general also applicable to any language
family groups or subgroups. Then we have tried to give probable solutions using
Etymology. Etymology is used to correlate the words using their ROOT forms. In
5th century BC Panini wrote Astadhyayi where he depicted sutras or rules -- how
a word is changed according to person,tense,gender,number etc. Later this book
was followed in Western countries also to derive their grammar of comparatively
new languages. We have trained our system for automatic root extraction from
the surface level or morphed form of words using Panian Gramatical rules. We
have tested our system over 10000 bengali Verbs and extracted the root form
with 98% accuracy. We are now working to extend the program to successfully
lemmatize any words of any language and correlate them by applying those rule
sets in Artificial Neural Network.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.13361v1,2020-03-30T11:43:49Z,2020-03-30T11:43:49Z,Efficient attention guided 5G power amplifier digital predistortion,"We investigate neural network (NN) assisted techniques for compensating the
non-linear behaviour and the memory effect of a 5G PA through digital
predistortion (DPD). Traditionally, the most prevalent compensation technique
computes the compensation element using a Memory Polynomial Model (MPM).
Various neural network proposals have been shown to improve on this
performance. However, thus far they mostly come with prohibitive training or
inference costs for real world implementations. In this paper, we propose a DPD
architecture that builds upon the practical MPM formulation governed by neural
attention. Our approach enables a set of MPM DPD components to individually
learn to target different regions of the data space, combining their outputs
for a superior overall compensation. Our method produces similar performance to
that of higher capacity NN models with minimal complexity. Finally, we view our
approach as a framework that can be extended to a wide variety of local
compensator types.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.11117v1,2020-03-24T21:17:44Z,2020-03-24T21:17:44Z,"COVID-19 and Computer Audition: An Overview on What Speech & Sound
  Analysis Could Contribute in the SARS-CoV-2 Corona Crisis","At the time of writing, the world population is suffering from more than
10,000 registered COVID-19 disease epidemic induced deaths since the outbreak
of the Corona virus more than three months ago now officially known as
SARS-CoV-2. Since, tremendous efforts have been made worldwide to counter-steer
and control the epidemic by now labelled as pandemic. In this contribution, we
provide an overview on the potential for computer audition (CA), i.e., the
usage of speech and sound analysis by artificial intelligence to help in this
scenario. We first survey which types of related or contextually significant
phenomena can be automatically assessed from speech or sound. These include the
automatic recognition and monitoring of breathing, dry and wet coughing or
sneezing sounds, speech under cold, eating behaviour, sleepiness, or pain to
name but a few. Then, we consider potential use-cases for exploitation. These
include risk assessment and diagnosis based on symptom histograms and their
development over time, as well as monitoring of spread, social distancing and
its effects, treatment and recovery, and patient wellbeing. We quickly guide
further through challenges that need to be faced for real-life usage. We come
to the conclusion that CA appears ready for implementation of (pre-)diagnosis
and monitoring tools, and more generally provides rich and significant, yet so
far untapped potential in the fight against COVID-19 spread.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.10548v1,2020-03-23T21:04:43Z,2020-03-23T21:04:43Z,spsurv: An R package for semi-parametric survival analysis,"Software development innovations and advances in computing have enabled more
complex and less costly computations in medical research (survival analysis),
engineering studies (reliability analysis), and social sciences event analysis
(historical analysis). As a result, many semi-parametric modeling efforts
emerged when it comes to time-to-event data analysis. In this context, this
work presents a flexible Bernstein polynomial (BP) based framework for survival
data modeling. This innovative approach is applied to existing families of
models such as proportional hazards (PH), proportional odds (PO), and
accelerated failure time (AFT) models to estimate unknown baseline functions.
Along with this contribution, this work also presents new automated routines in
R, taking advantage of algorithms available in Stan. The proposed computation
routines are tested and explored through simulation studies based on artificial
datasets. The tools implemented to fit the proposed statistical models are
combined and organized in an R package. Also, the BP based proportional hazards
(BPPH), proportional odds (BPPO), and accelerated failure time (BPAFT) models
are illustrated in real applications related to cancer trial data using maximum
likelihood (ML) estimation and Markov chain Monte Carlo (MCMC) methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.09052v2,2020-06-02T15:31:08Z,2020-03-20T00:20:36Z,Design and operation of the ATLAS Transient Science Server,"The Asteroid Terrestrial impact Last Alert System (ATLAS) system consists of
two 0.5m Schmidt telescopes with cameras covering 29 square degrees at plate
scale of 1.86 arcsec per pixel. Working in tandem, the telescopes routinely
survey the whole sky visible from Hawaii (above $\delta > -50^{\circ}$) every
two nights, exposing four times per night, typically reaching $o < 19$
magnitude per exposure when the moon is illuminated and $c < 19.5$ per exposure
in dark skies. Construction is underway of two further units to be sited in
Chile and South Africa which will result in an all-sky daily cadence from 2021.
Initially designed for detecting potentially hazardous near earth objects, the
ATLAS data enable a range of astrophysical time domain science. To extract
transients from the data stream requires a computing system to process the
data, assimilate detections in time and space and associate them with known
astrophysical sources. Here we describe the hardware and software
infrastructure to produce a stream of clean, real, astrophysical transients in
real time. This involves machine learning and boosted decision tree algorithms
to identify extragalactic and Galactic transients. Typically we detect 10-15
supernova candidates per night which we immediately announce publicly. The
ATLAS discoveries not only enable rapid follow-up of interesting sources but
will provide complete statistical samples within the local volume of 100 Mpc. A
simple comparison of the detected supernova rate within 100 Mpc, with no
corrections for completeness, is already significantly higher (factor 1.5 to 2)
than the current accepted rates.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.06705v1,2020-03-14T21:11:02Z,2020-03-14T21:11:02Z,Identifying Individual Dogs in Social Media Images,"We present the results of an initial study focused on developing a visual AI
solution able to recognize individual dogs in unconstrained (wild) images
occurring on social media.
  The work described here is part of joint project done with Pet2Net, a social
network focused on pets and their owners. In order to detect and recognize
individual dogs we combine transfer learning and object detection approaches on
Inception v3 and SSD Inception v2 architectures respectively and evaluate the
proposed pipeline using a new data set containing real data that the users
uploaded to Pet2Net platform. We show that it can achieve 94.59% accuracy in
identifying individual dogs. Our approach has been designed with simplicity in
mind and the goal of easy deployment on all the images uploaded to Pet2Net
platform.
  A purely visual approach to identifying dogs in images, will enhance Pet2Net
features aimed at finding lost dogs, as well as form the basis of future work
focused on identifying social relationships between dogs, which cannot be
inferred from other data collected by the platform.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.08360v1,2020-03-14T13:21:07Z,2020-03-14T13:21:07Z,Compatible Learning for Deep Photonic Neural Network,"Realization of deep learning with coherent optical field has attracted
remarkably attentions presently, which benefits on the fact that optical matrix
manipulation can be executed at speed of light with inherent parallel
computation as well as low latency. Photonic neural network has a significant
potential for prediction-oriented tasks. Yet, real-value Backpropagation
behaves somewhat intractably for coherent photonic intelligent training. We
develop a compatible learning protocol in complex space, of which nonlinear
activation could be selected efficiently depending on the unveiled compatible
condition. Compatibility indicates that matrix representation in complex space
covers its real counterpart, which could enable a single channel mingled
training in real and complex space as a unified model. The phase logical XOR
gate with Mach-Zehnder interferometers and diffractive neural network with
optical modulation mechanism, implementing intelligent weight learned from
compatible learning, are presented to prove the availability. Compatible
learning opens an envisaged window for deep photonic neural network.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.05861v1,2020-03-12T15:52:49Z,2020-03-12T15:52:49Z,"The Chef's Hat Simulation Environment for Reinforcement-Learning-Based
  Agents","To achieve social interactions within Human-Robot Interaction (HRI)
environments is a very challenging task. Most of the current research focuses
on Wizard-of-Oz approaches, which neglect the recent development of intelligent
robots. On the other hand, real-world scenarios usually do not provide the
necessary control and reproducibility which are needed for learning algorithms.
In this paper, we propose a virtual simulation environment that implements the
Chef's Hat card game, designed to be used in HRI scenarios, to provide a
controllable and reproducible scenario for reinforcement-learning algorithms.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.04956v1,2020-03-10T20:26:26Z,2020-03-10T20:26:26Z,"SQUIRL: Robust and Efficient Learning from Video Demonstration of
  Long-Horizon Robotic Manipulation Tasks","Recent advances in deep reinforcement learning (RL) have demonstrated its
potential to learn complex robotic manipulation tasks. However, RL still
requires the robot to collect a large amount of real-world experience. To
address this problem, recent works have proposed learning from expert
demonstrations (LfD), particularly via inverse reinforcement learning (IRL),
given its ability to achieve robust performance with only a small number of
expert demonstrations. Nevertheless, deploying IRL on real robots is still
challenging due to the large number of robot experiences it requires. This
paper aims to address this scalability challenge with a robust,
sample-efficient, and general meta-IRL algorithm, SQUIRL, that performs a new
but related long-horizon task robustly given only a single video demonstration.
First, this algorithm bootstraps the learning of a task encoder and a
task-conditioned policy using behavioral cloning (BC). It then collects
real-robot experiences and bypasses reward learning by directly recovering a
Q-function from the combined robot and expert trajectories. Next, this
algorithm uses the Q-function to re-evaluate all cumulative experiences
collected by the robot to improve the policy quickly. In the end, the policy
performs more robustly (90%+ success) than BC on new tasks while requiring no
trial-and-errors at test time. Finally, our real-robot and simulated
experiments demonstrate our algorithm's generality across different state
spaces, action spaces, and vision-based manipulation tasks, e.g.,
pick-pour-place and pick-carry-drop.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.04273v1,2020-03-09T17:29:39Z,2020-03-09T17:29:39Z,"Finding Input Characterizations for Output Properties in ReLU Neural
  Networks","Deep Neural Networks (DNNs) have emerged as a powerful mechanism and are
being increasingly deployed in real-world safety-critical domains. Despite the
widespread success, their complex architecture makes proving any formal
guarantees about them difficult. Identifying how logical notions of high-level
correctness relate to the complex low-level network architecture is a
significant challenge. In this project, we extend the ideas presented in and
introduce a way to bridge the gap between the architecture and the high-level
specifications. Our key insight is that instead of directly proving the safety
properties that are required, we first prove properties that relate closely to
the structure of the neural net and use them to reason about the safety
properties. We build theoretical foundations for our approach, and empirically
evaluate the performance through various experiments, achieving promising
results than the existing approach by identifying a larger region of input
space that guarantees a certain property on the output.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.03396v1,2020-03-06T19:09:42Z,2020-03-06T19:09:42Z,"Scalable Uncertainty for Computer Vision with Functional Variational
  Inference","As Deep Learning continues to yield successful applications in Computer
Vision, the ability to quantify all forms of uncertainty is a paramount
requirement for its safe and reliable deployment in the real-world. In this
work, we leverage the formulation of variational inference in function space,
where we associate Gaussian Processes (GPs) to both Bayesian CNN priors and
variational family. Since GPs are fully determined by their mean and covariance
functions, we are able to obtain predictive uncertainty estimates at the cost
of a single forward pass through any chosen CNN architecture and for any
supervised learning task. By leveraging the structure of the induced covariance
matrices, we propose numerically efficient algorithms which enable fast
training in the context of high-dimensional tasks such as depth estimation and
semantic segmentation. Additionally, we provide sufficient conditions for
constructing regression loss functions whose probabilistic counterparts are
compatible with aleatoric uncertainty quantification.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.01207v1,2020-03-02T21:55:35Z,2020-03-02T21:55:35Z,"BARD: A structured technique for group elicitation of Bayesian networks
  to support analytic reasoning","In many complex, real-world situations, problem solving and decision making
require effective reasoning about causation and uncertainty. However, human
reasoning in these cases is prone to confusion and error. Bayesian networks
(BNs) are an artificial intelligence technology that models uncertain
situations, supporting probabilistic and causal reasoning and decision making.
However, to date, BN methodologies and software require significant upfront
training, do not provide much guidance on the model building process, and do
not support collaboratively building BNs. BARD (Bayesian ARgumentation via
Delphi) is both a methodology and an expert system that utilises (1) BNs as the
underlying structured representations for better argument analysis, (2) a
multi-user web-based software platform and Delphi-style social processes to
assist with collaboration, and (3) short, high-quality e-courses on demand, a
highly structured process to guide BN construction, and a variety of helpful
tools to assist in building and reasoning with BNs, including an automated
explanation tool to assist effective report writing. The result is an
end-to-end online platform, with associated online training, for groups without
prior BN expertise to understand and analyse a problem, build a model of its
underlying probabilistic causal structure, validate and reason with the causal
model, and use it to produce a written analytic report. Initial experimental
results demonstrate that BARD aids in problem solving, reasoning and
collaboration.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.00671v2,2020-03-04T19:48:50Z,2020-03-02T05:35:32Z,"AutoPhase: Juggling HLS Phase Orderings in Random Forests with Deep
  Reinforcement Learning","The performance of the code a compiler generates depends on the order in
which it applies the optimization passes. Choosing a good order--often referred
to as the phase-ordering problem, is an NP-hard problem. As a result, existing
solutions rely on a variety of heuristics. In this paper, we evaluate a new
technique to address the phase-ordering problem: deep reinforcement learning.
To this end, we implement AutoPhase: a framework that takes a program and uses
deep reinforcement learning to find a sequence of compilation passes that
minimizes its execution time. Without loss of generality, we construct this
framework in the context of the LLVM compiler toolchain and target high-level
synthesis programs. We use random forests to quantify the correlation between
the effectiveness of a given pass and the program's features. This helps us
reduce the search space by avoiding phase orderings that are unlikely to
improve the performance of a given program. We compare the performance of
AutoPhase to state-of-the-art algorithms that address the phase-ordering
problem. In our evaluation, we show that AutoPhase improves circuit performance
by 28% when compared to using the -O3 compiler flag, and achieves competitive
results compared to the state-of-the-art solutions, while requiring fewer
samples. Furthermore, unlike existing state-of-the-art solutions, our deep
reinforcement learning solution shows promising result in generalizing to real
benchmarks and 12,874 different randomly generated programs, after training on
a hundred randomly generated programs.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.00628v3,2020-07-20T02:39:28Z,2020-03-02T01:58:03Z,"Learning Force Control for Contact-rich Manipulation Tasks with Rigid
  Position-controlled Robots","Reinforcement Learning (RL) methods have been proven successful in solving
manipulation tasks autonomously. However, RL is still not widely adopted on
real robotic systems because working with real hardware entails additional
challenges, especially when using rigid position-controlled manipulators. These
challenges include the need for a robust controller to avoid undesired
behavior, that risk damaging the robot and its environment, and constant
supervision from a human operator. The main contributions of this work are,
first, we proposed a learning-based force control framework combining RL
techniques with traditional force control. Within said control scheme, we
implemented two different conventional approaches to achieve force control with
position-controlled robots; one is a modified parallel position/force control,
and the other is an admittance control. Secondly, we empirically study both
control schemes when used as the action space of the RL agent. Thirdly, we
developed a fail-safe mechanism for safely training an RL agent on manipulation
tasks using a real rigid robot manipulator. The proposed methods are validated
on simulation and a real robot, an UR3 e-series robotic arm.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.00205v2,2020-06-03T08:57:51Z,2020-02-29T08:06:19Z,The GWAC Data Processing and Management System,"GWAC will have been built an integrated FOV of 5,000 $degree^2$ and have
already built 1,800 square $degree^2$. The limit magnitude of a 10-second
exposure image in the moonless night is 16R. In each observation night, GWAC
produces about 0.7TB of raw data, and the data processing pipeline generates
millions of single frame alerts. We describe the GWAC Data Processing and
Management System (GPMS), including hardware architecture, database,
detection-filtering-validation of transient candidates, data archiving, and
user interfaces for the check of transient and the monitor of the system. GPMS
combines general technology and software in astronomy and computer field, and
use some advanced technologies such as deep learning. Practical results show
that GPMS can fully meet the scientific data processing requirement of GWAC. It
can online accomplish the detection, filtering and validation of millions of
transient candidates, and feedback the final results to the astronomer in
real-time. During the observation from October of 2018 to December of 2019, we
have already found 102 transients.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.00097v2,2020-06-19T00:22:54Z,2020-02-28T22:32:05Z,Jointly Learning to Recommend and Advertise,"Online recommendation and advertising are two major income channels for
online recommendation platforms (e.g. e-commerce and news feed site). However,
most platforms optimize recommending and advertising strategies by different
teams separately via different techniques, which may lead to suboptimal overall
performances. To this end, in this paper, we propose a novel two-level
reinforcement learning framework to jointly optimize the recommending and
advertising strategies, where the first level generates a list of
recommendations to optimize user experience in the long run; then the second
level inserts ads into the recommendation list that can balance the immediate
advertising revenue from advertisers and the negative influence of ads on
long-term user experience. To be specific, the first level tackles high
combinatorial action space problem that selects a subset items from the large
item space; while the second level determines three internally related tasks,
i.e., (i) whether to insert an ad, and if yes, (ii) the optimal ad and (iii)
the optimal location to insert. The experimental results based on real-world
data demonstrate the effectiveness of the proposed framework. We have released
the implementation code to ease reproductivity.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.07689v3,2022-05-11T06:54:07Z,2020-02-28T05:00:44Z,Braitenberg Vehicles as Developmental Neurosimulation,"Connecting brain and behavior is a longstanding issue in the areas of
behavioral science, artificial intelligence, and neurobiology. As is standard
among models of artificial and biological neural networks, an analogue of the
fully mature brain is presented as a blank slate. However, this does not
consider the realities of biological development and developmental learning.
Our purpose is to model the development of an artificial organism that exhibits
complex behaviors. We introduce three alternate approaches to demonstrate how
developmental embodied agents can be implemented. The resulting developmental
BVs (dBVs) will generate behaviors ranging from stimulus responses to group
behavior that resembles collective motion. We will situate this work in the
domain of artificial brain networks along with broader themes such as embodied
cognition, feedback, and emergence. Our perspective is exemplified by three
software instantiations that demonstrate how a BV-genetic algorithm hybrid
model, multisensory Hebbian learning model, and multi-agent approaches can be
used to approach BV development. We introduce use cases such as optimized
spatial cognition (vehicle-genetic algorithm hybrid model), hinges connecting
behavioral and neural models (multisensory Hebbian learning model), and
cumulative classification (multi-agent approaches). In conclusion, we consider
future applications of the developmental neurosimulation approach.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2003.04816v1,2020-02-21T07:29:15Z,2020-02-21T07:29:15Z,"Data Freshness and Energy-Efficient UAV Navigation Optimization: A Deep
  Reinforcement Learning Approach","In this paper, we design a navigation policy for multiple unmanned aerial
vehicles (UAVs) where mobile base stations (BSs) are deployed to improve the
data freshness and connectivity to the Internet of Things (IoT) devices. First,
we formulate an energy-efficient trajectory optimization problem in which the
objective is to maximize the energy efficiency by optimizing the UAV-BS
trajectory policy. We also incorporate different contextual information such as
energy and age of information (AoI) constraints to ensure the data freshness at
the ground BS. Second, we propose an agile deep reinforcement learning with
experience replay model to solve the formulated problem concerning the
contextual constraints for the UAV-BS navigation. Moreover, the proposed
approach is well-suited for solving the problem, since the state space of the
problem is extremely large and finding the best trajectory policy with useful
contextual features is too complex for the UAV-BSs. By applying the proposed
trained model, an effective real-time trajectory policy for the UAV-BSs
captures the observable network states over time. Finally, the simulation
results illustrate the proposed approach is 3.6% and 3.13% more energy
efficient than those of the greedy and baseline deep Q Network (DQN)
approaches.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.09063v1,2020-02-20T23:37:43Z,2020-02-20T23:37:43Z,"Real-Time Optimal Guidance and Control for Interplanetary Transfers
  Using Deep Networks","We consider the Earth-Venus mass-optimal interplanetary transfer of a
low-thrust spacecraft and show how the optimal guidance can be represented by
deep networks in a large portion of the state space and to a high degree of
accuracy. Imitation (supervised) learning of optimal examples is used as a
network training paradigm. The resulting models are suitable for an on-board,
real-time, implementation of the optimal guidance and control system of the
spacecraft and are called G&CNETs. A new general methodology called Backward
Generation of Optimal Examples is introduced and shown to be able to
efficiently create all the optimal state action pairs necessary to train
G&CNETs without solving optimal control problems. With respect to previous
works, we are able to produce datasets containing a few orders of magnitude
more optimal trajectories and obtain network performances compatible with real
missions requirements. Several schemes able to train representations of either
the optimal policy (thrust profile) or the value function (optimal mass) are
proposed and tested. We find that both policy learning and value function
learning successfully and accurately learn the optimal thrust and that a
spacecraft employing the learned thrust is able to reach the target conditions
orbit spending only 2 permil more propellant than in the corresponding
mathematically optimal transfer. Moreover, the optimal propellant mass can be
predicted (in case of value function learning) within an error well within 1%.
All G&CNETs produced are tested during simulations of interplanetary transfers
with respect to their ability to reach the target conditions optimally starting
from nominal and off-nominal conditions.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.08541v2,2020-10-22T11:28:41Z,2020-02-20T02:41:02Z,Simple and Scalable Sparse k-means Clustering via Feature Ranking,"Clustering, a fundamental activity in unsupervised learning, is notoriously
difficult when the feature space is high-dimensional. Fortunately, in many
realistic scenarios, only a handful of features are relevant in distinguishing
clusters. This has motivated the development of sparse clustering techniques
that typically rely on k-means within outer algorithms of high computational
complexity. Current techniques also require careful tuning of shrinkage
parameters, further limiting their scalability. In this paper, we propose a
novel framework for sparse k-means clustering that is intuitive, simple to
implement, and competitive with state-of-the-art algorithms. We show that our
algorithm enjoys consistency and convergence guarantees. Our core method
readily generalizes to several task-specific algorithms such as clustering on
subsets of attributes and in partially observed data settings. We showcase
these contributions thoroughly via simulated experiments and real data
benchmarks, including a case study on protein expression in trisomic mice.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.08361v2,2020-03-13T20:47:01Z,2020-02-19T17:05:50Z,"Phase Imaging with Computational Specificity (PICS) for measuring dry
  mass changes in sub-cellular compartments","Due to its specificity, fluorescence microscopy (FM) has become a
quintessential imaging tool in cell biology. However, photobleaching,
phototoxicity, and related artifacts continue to limit FM's utility. Recently,
it has been shown that artificial intelligence (AI) can transform one form of
contrast into another. We present PICS, a combination of quantitative phase
imaging and AI, which provides information about unlabeled live cells with high
specificity. Our imaging system allows for automatic training, while inference
is built into the acquisition software and runs in real-time. Applying the
computed fluorescence maps back to the QPI data, we measured the growth of both
nuclei and cytoplasm independently, over many days, without loss of viability.
Using a QPI method that suppresses multiple scattering, we measured the dry
mass content of individual cell nuclei within spheroids. In its current
implementation, PICS offers a versatile quantitative technique for continuous
simultaneous monitoring of individual cellular components in biological
applications where long-term label-free imaging is desirable.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.07684v3,2020-04-17T11:37:42Z,2020-02-18T16:13:24Z,"A Lagrangian Approach to Information Propagation in Graph Neural
  Networks","In many real world applications, data are characterized by a complex
structure, that can be naturally encoded as a graph. In the last years, the
popularity of deep learning techniques has renewed the interest in neural
models able to process complex patterns. In particular, inspired by the Graph
Neural Network (GNN) model, different architectures have been proposed to
extend the original GNN scheme. GNNs exploit a set of state variables, each
assigned to a graph node, and a diffusion mechanism of the states among
neighbor nodes, to implement an iterative procedure to compute the fixed point
of the (learnable) state transition function. In this paper, we propose a novel
approach to the state computation and the learning algorithm for GNNs, based on
a constraint optimisation task solved in the Lagrangian framework. The state
convergence procedure is implicitly expressed by the constraint satisfaction
mechanism and does not require a separate iterative phase for each epoch of the
learning procedure. In fact, the computational structure is based on the search
for saddle points of the Lagrangian in the adjoint space composed of weights,
neural outputs (node states), and Lagrange multipliers. The proposed approach
is compared experimentally with other popular models for processing graphs.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.06637v1,2020-02-16T18:18:19Z,2020-02-16T18:18:19Z,Real-time binaural speech separation with preserved spatial cues,"Deep learning speech separation algorithms have achieved great success in
improving the quality and intelligibility of separated speech from mixed audio.
Most previous methods focused on generating a single-channel output for each of
the target speakers, hence discarding the spatial cues needed for the
localization of sound sources in space. However, preserving the spatial
information is important in many applications that aim to accurately render the
acoustic scene such as in hearing aids and augmented reality (AR). Here, we
propose a speech separation algorithm that preserves the interaural cues of
separated sound sources and can be implemented with low latency and high
fidelity, therefore enabling a real-time modification of the acoustic scene.
Based on the time-domain audio separation network (TasNet), a single-channel
time-domain speech separation system that can be implemented in real-time, we
propose a multi-input-multi-output (MIMO) end-to-end extension of TasNet that
takes binaural mixed audio as input and simultaneously separates target
speakers in both channels. Experimental results show that the proposed
end-to-end MIMO system is able to significantly improve the separation
performance and keep the perceived location of the modified sources intact in
various acoustic scenes.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.05287v2,2020-02-14T01:47:35Z,2020-02-13T00:03:09Z,Geom-GCN: Geometric Graph Convolutional Networks,"Message-passing neural networks (MPNNs) have been successfully applied to
representation learning on graphs in a variety of real-world applications.
However, two fundamental weaknesses of MPNNs' aggregators limit their ability
to represent graph-structured data: losing the structural information of nodes
in neighborhoods and lacking the ability to capture long-range dependencies in
disassortative graphs. Few studies have noticed the weaknesses from different
perspectives. From the observations on classical neural network and network
geometry, we propose a novel geometric aggregation scheme for graph neural
networks to overcome the two weaknesses. The behind basic idea is the
aggregation on a graph can benefit from a continuous space underlying the
graph. The proposed aggregation scheme is permutation-invariant and consists of
three modules, node embedding, structural neighborhood, and bi-level
aggregation. We also present an implementation of the scheme in graph
convolutional networks, termed Geom-GCN (Geometric Graph Convolutional
Networks), to perform transductive learning on graphs. Experimental results
show the proposed Geom-GCN achieved state-of-the-art performance on a wide
range of open datasets of graphs. Code is available at
https://github.com/graphdml-uiuc-jlu/geom-gcn.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.05147v1,2020-02-12T18:46:48Z,2020-02-12T18:46:48Z,"Multi-Agent Reinforcement Learning and Human Social Factors in Climate
  Change Mitigation","Many complex real-world problems, such as climate change mitigation, are
intertwined with human social factors. Climate change mitigation, a social
dilemma made difficult by the inherent complexities of human behavior, has an
impact at a global scale. We propose applying multi-agent reinforcement
learning (MARL) in this setting to develop intelligent agents that can
influence the social factors at play in climate change mitigation. There are
ethical, practical, and technical challenges that must be addressed when
deploying MARL in this way. In this paper, we present these challenges and
outline an approach to address them. Understanding how intelligent agents can
be used to impact human social factors is important to prevent their abuse and
can be beneficial in furthering our knowledge of these complex problems as a
whole. The challenges we present are not limited to our specific application
but are applicable to broader MARL. Thus, developing MARL for social factors in
climate change mitigation helps address general problems hindering MARL's
applicability to other real-world problems while also motivating discussion on
the social implications of MARL deployment.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.04788v4,2022-04-14T01:20:49Z,2020-02-12T04:05:31Z,"To Split or Not to Split: The Impact of Disparate Treatment in
  Classification","Disparate treatment occurs when a machine learning model yields different
decisions for individuals based on a sensitive attribute (e.g., age, sex). In
domains where prediction accuracy is paramount, it could potentially be
acceptable to fit a model which exhibits disparate treatment. To evaluate the
effect of disparate treatment, we compare the performance of split classifiers
(i.e., classifiers trained and deployed separately on each group) with
group-blind classifiers (i.e., classifiers which do not use a sensitive
attribute). We introduce the benefit-of-splitting for quantifying the
performance improvement by splitting classifiers. Computing the
benefit-of-splitting directly from its definition could be intractable since it
involves solving optimization problems over an infinite-dimensional functional
space. Under different performance measures, we (i) prove an equivalent
expression for the benefit-of-splitting which can be efficiently computed by
solving small-scale convex programs; (ii) provide sharp upper and lower bounds
for the benefit-of-splitting which reveal precise conditions where a
group-blind classifier will always suffer from a non-trivial performance gap
from the split classifiers. In the finite sample regime, splitting is not
necessarily beneficial and we provide data-dependent bounds to understand this
effect. Finally, we validate our theoretical results through numerical
experiments on both synthetic and real-world datasets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.04716v1,2020-02-11T22:18:28Z,2020-02-11T22:18:28Z,"Robust multi-scale multi-feature deep learning for atomic and defect
  identification in Scanning Tunneling Microscopy on H-Si(100) 2x1 surface","The nature of the atomic defects on the hydrogen passivated Si (100) surface
is analyzed using deep learning and scanning tunneling microscopy (STM). A
robust deep learning framework capable of identifying atomic species, defects,
in the presence of non-resolved contaminates, step edges, and noise is
developed. The automated workflow, based on the combination of several networks
for image assessment, atom-finding and defect finding, is developed to perform
the analysis at different levels of description and is deployed on an
operational STM platform. This is further extended to unsupervised
classification of the extracted defects using the mean-shift clustering
algorithm, which utilizes features automatically engineered from the combined
output of neural networks. This combined approach allows the identification of
localized and extended defects on the topographically non-uniform surfaces or
real materials. Our approach is universal in nature and can be applied to other
surfaces for building comprehensive libraries of atomic defects in quantum
materials.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.02363v3,2020-10-14T14:05:51Z,2020-02-06T17:11:11Z,"Topological quantum phase transitions retrieved through unsupervised
  machine learning","The discovery of topological features of quantum states plays an important
role in modern condensed matter physics and various artificial systems. Due to
the absence of local order parameters, the detection of topological quantum
phase transitions remains a challenge. Machine learning may provide effective
methods for identifying topological features. In this work, we show that the
unsupervised manifold learning can successfully retrieve topological quantum
phase transitions in momentum and real space. Our results show that the
Chebyshev distance between two data points sharpens the characteristic features
of topological quantum phase transitions in momentum space, while the widely
used Euclidean distance is in general suboptimal. Then a diffusion map or
isometric map can be applied to implement the dimensionality reduction, and to
learn about topological quantum phase transitions in an unsupervised manner. We
demonstrate this method on the prototypical Su-Schrieffer-Heeger (SSH) model,
the Qi-Wu-Zhang (QWZ) model, and the quenched SSH model in momentum space, and
further provide implications and demonstrations for learning in real space,
where the topological invariants could be unknown or hard to compute. The
interpretable good performance of our approach shows the capability of manifold
learning, when equipped with a suitable distance metric, in exploring
topological quantum phase transitions.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.01129v3,2021-07-12T21:18:32Z,2020-02-04T05:08:17Z,Bayesian Meta-Prior Learning Using Empirical Bayes,"Adding domain knowledge to a learning system is known to improve results. In
multi-parameter Bayesian frameworks, such knowledge is incorporated as a prior.
On the other hand, various model parameters can have different learning rates
in real-world problems, especially with skewed data. Two often-faced challenges
in Operation Management and Management Science applications are the absence of
informative priors, and the inability to control parameter learning rates. In
this study, we propose a hierarchical Empirical Bayes approach that addresses
both challenges, and that can generalize to any Bayesian framework. Our method
learns empirical meta-priors from the data itself and uses them to decouple the
learning rates of first-order and second-order features (or any other given
feature grouping) in a Generalized Linear Model. As the first-order features
are likely to have a more pronounced effect on the outcome, focusing on
learning first-order weights first is likely to improve performance and
convergence time. Our Empirical Bayes method clamps features in each group
together and uses the deployed model's observed data to empirically compute a
hierarchical prior in hindsight. We report theoretical results for the
unbiasedness, strong consistency, and optimal frequentist cumulative regret
properties of our meta-prior variance estimator. We apply our method to a
standard supervised learning optimization problem, as well as an online
combinatorial optimization problem in a contextual bandit setting implemented
in an Amazon production system. Both during simulations and live experiments,
our method shows marked improvements, especially in cases of small traffic. Our
findings are promising, as optimizing over sparse data is often a challenge.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.00831v1,2020-02-03T15:39:56Z,2020-02-03T15:39:56Z,An Actor-Critic-Based UAV-BSs Deployment Method for Dynamic Environments,"In this paper, the real-time deployment of unmanned aerial vehicles (UAVs) as
flying base stations (BSs) for optimizing the throughput of mobile users is
investigated for UAV networks. This problem is formulated as a time-varying
mixed-integer non-convex programming (MINP) problem, which is challenging to
find an optimal solution in a short time with conventional optimization
techniques. Hence, we propose an actor-critic-based (AC-based) deep
reinforcement learning (DRL) method to find near-optimal UAV positions at every
moment. In the proposed method, the process searching for the solution
iteratively at a particular moment is modeled as a Markov decision process
(MDP). To handle infinite state and action spaces and improve the robustness of
the decision process, two powerful neural networks (NNs) are configured to
evaluate the UAV position adjustments and make decisions, respectively.
Compared with the heuristic algorithm, sequential least-squares programming and
fixed UAVs methods, simulation results have shown that the proposed method
outperforms these three benchmarks in terms of the throughput at every moment
in UAV networks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.05648v3,2020-04-26T04:59:52Z,2020-02-01T01:15:39Z,Politics of Adversarial Machine Learning,"In addition to their security properties, adversarial machine-learning
attacks and defenses have political dimensions. They enable or foreclose
certain options for both the subjects of the machine learning systems and for
those who deploy them, creating risks for civil liberties and human rights. In
this paper, we draw on insights from science and technology studies,
anthropology, and human rights literature, to inform how defenses against
adversarial attacks can be used to suppress dissent and limit attempts to
investigate machine learning systems. To make this concrete, we use real-world
examples of how attacks such as perturbation, model inversion, or membership
inference can be used for socially desirable ends. Although the predictions of
this analysis may seem dire, there is hope. Efforts to address human rights
concerns in the commercial spyware industry provide guidance for similar
measures to ensure ML systems serve democratic, not authoritarian ends",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2002.00763v1,2020-01-31T02:28:35Z,2020-01-31T02:28:35Z,Two-path Deep Semi-supervised Learning for Timely Fake News Detection,"News in social media such as Twitter has been generated in high volume and
speed. However, very few of them are labeled (as fake or true news) by
professionals in near real time. In order to achieve timely detection of fake
news in social media, a novel framework of two-path deep semi-supervised
learning is proposed where one path is for supervised learning and the other is
for unsupervised learning. The supervised learning path learns on the limited
amount of labeled data while the unsupervised learning path is able to learn on
a huge amount of unlabeled data. Furthermore, these two paths implemented with
convolutional neural networks (CNN) are jointly optimized to complete
semi-supervised learning. In addition, we build a shared CNN to extract the low
level features on both labeled data and unlabeled data to feed them into these
two paths. To verify this framework, we implement a Word CNN based
semi-supervised learning model and test it on two datasets, namely, LIAR and
PHEME. Experimental results demonstrate that the model built on the proposed
framework can recognize fake news effectively with very few labeled data.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.09346v2,2020-03-04T19:22:37Z,2020-01-25T18:43:47Z,"CorGAN: Correlation-Capturing Convolutional Generative Adversarial
  Networks for Generating Synthetic Healthcare Records","Deep learning models have demonstrated high-quality performance in areas such
as image classification and speech processing. However, creating a deep
learning model using electronic health record (EHR) data, requires addressing
particular privacy challenges that are unique to researchers in this domain.
This matter focuses attention on generating realistic synthetic data while
ensuring privacy. In this paper, we propose a novel framework called
correlation-capturing Generative Adversarial Network (CorGAN), to generate
synthetic healthcare records. In CorGAN we utilize Convolutional Neural
Networks to capture the correlations between adjacent medical features in the
data representation space by combining Convolutional Generative Adversarial
Networks and Convolutional Autoencoders. To demonstrate the model fidelity, we
show that CorGAN generates synthetic data with performance similar to that of
real data in various Machine Learning settings such as classification and
prediction. We also give a privacy assessment and report on statistical
analysis regarding realistic characteristics of the synthetic data. The
software of this work is open-source and is available at:
https://github.com/astorfi/cor-gan.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.07427v1,2020-01-21T10:17:39Z,2020-01-21T10:17:39Z,Circuit Implementation of a Four-Dimensional Topological Insulator,"The classification of topological insulators predicts the existence of
high-dimensional topological phases that cannot occur in real materials, as
these are limited to three or fewer spatial dimensions. We use electric
circuits to experimentally implement a four-dimensional (4D) topological
lattice. The lattice dimensionality is established by circuit connections, and
not by mapping to a lower-dimensional system. On the lattice's
three-dimensional surface, we observe topological surface states that are
associated with a nonzero second Chern number but vanishing first Chern
numbers. The 4D lattice belongs to symmetry class AI, which refers to
time-reversal-invariant and spinless systems with no special spatial symmetry.
Class AI is topologically trivial in one to three spatial dimensions, so 4D is
the lowest possible dimension for achieving a topological insulator in this
class. This work paves the way to the use of electric circuits for exploring
high-dimensional topological models.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.05982v2,2020-06-04T15:13:47Z,2020-01-16T18:32:19Z,"A Common Operating Picture Framework Leveraging Data Fusion and Deep
  Learning","Organizations are starting to realize of the combined power of data and
data-driven algorithmic models to gain insights, situational awareness, and
advance their mission. A common challenge to gaining insights is connecting
inherently different datasets. These datasets (e.g. geocoded features, video
streams, raw text, social network data, etc.) per separate they provide very
narrow answers; however collectively they can provide new capabilities. In this
work, we present a data fusion framework for accelerating solutions for
Processing, Exploitation, and Dissemination (PED). Our platform is a collection
of services that extract information from several data sources (per separate)
by leveraging deep learning and other means of processing. This information is
fused by a set of analytical engines that perform data correlations, searches,
and other modeling operations to combine information from the disparate data
sources. As a result, events of interest are detected, geolocated, logged, and
presented into a common operating picture. This common operating picture allows
the user to visualize in real time all the data sources, per separate and their
collective cooperation. In addition, forensic activities have been implemented
and made available through the framework. Users can review archived results and
compare them to the most recent snapshot of the operational environment. In our
first iteration we have focused on visual data (FMV, WAMI, CCTV/PTZ-Cameras,
open source video, etc.) and AIS data streams (satellite and terrestrial
sources). As a proof-of-concept, in our experiments we show how FMV detections
can be combined with vessel tracking signals from AIS sources to confirm
identity, tip-and-cue aerial reconnaissance, and monitor vessel activity in an
area.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.03864v1,2020-01-12T06:06:03Z,2020-01-12T06:06:03Z,"Learning to drive via Apprenticeship Learning and Deep Reinforcement
  Learning","With the implementation of reinforcement learning (RL) algorithms, current
state-of-art autonomous vehicle technology have the potential to get closer to
full automation. However, most of the applications have been limited to game
domains or discrete action space which are far from the real world driving.
Moreover, it is very tough to tune the parameters of reward mechanism since the
driving styles vary a lot among the different users. For instance, an
aggressive driver may prefer driving with high acceleration whereas some
conservative drivers prefer a safer driving style. Therefore, we propose an
apprenticeship learning in combination with deep reinforcement learning
approach that allows the agent to learn the driving and stopping behaviors with
continuous actions. We use gradient inverse reinforcement learning (GIRL)
algorithm to recover the unknown reward function and employ REINFORCE as well
as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal
policy. The performance of our method is evaluated in simulation-based scenario
and the results demonstrate that the agent performs human like driving and even
better in some aspects after training.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.03855v1,2020-01-12T05:25:02Z,2020-01-12T05:25:02Z,"Hyperparameters optimization for Deep Learning based emotion prediction
  for Human Robot Interaction","To enable humanoid robots to share our social space we need to develop
technology for easy interaction with the robots using multiple modes such as
speech, gestures and share our emotions with them. We have targeted this
research towards addressing the core issue of emotion recognition problem which
would require less computation resources and much lesser number of network
hyperparameters which will be more adaptive to be computed on low resourced
social robots for real time communication. More specifically, here we have
proposed an Inception module based Convolutional Neural Network Architecture
which has achieved improved accuracy of upto 6% improvement over the existing
network architecture for emotion classification when combinedly tested over
multiple datasets when tried over humanoid robots in real - time. Our proposed
model is reducing the trainable Hyperparameters to an extent of 94% as compared
to vanilla CNN model which clearly indicates that it can be used in real time
based application such as human robot interaction. Rigorous experiments have
been performed to validate our methodology which is sufficiently robust and
could achieve high level of accuracy. Finally, the model is implemented in a
humanoid robot, NAO in real time and robustness of the model is evaluated.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.03360v4,2020-08-04T00:56:49Z,2020-01-10T09:26:04Z,NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization,"In the last decade, crowd counting and localization attract much attention of
researchers due to its wide-spread applications, including crowd monitoring,
public safety, space design, etc. Many Convolutional Neural Networks (CNN) are
designed for tackling this task. However, currently released datasets are so
small-scale that they can not meet the needs of the supervised CNN-based
algorithms. To remedy this problem, we construct a large-scale congested crowd
counting and localization dataset, NWPU-Crowd, consisting of 5,109 images, in a
total of 2,133,375 annotated heads with points and boxes. Compared with other
real-world datasets, it contains various illumination scenes and has the
largest density range (0~20,033). Besides, a benchmark website is developed for
impartially evaluating the different methods, which allows researchers to
submit the results of the test set. Based on the proposed dataset, we further
describe the data characteristics, evaluate the performance of some mainstream
state-of-the-art (SOTA) methods, and analyze the new problems that arise on the
new data. What's more, the benchmark is deployed at
\url{https://www.crowdbenchmark.com/}, and the dataset/code/models/results are
available at \url{https://gjy3035.github.io/NWPU-Crowd-Sample-Code/}.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.01861v2,2020-07-30T16:58:22Z,2020-01-07T02:39:02Z,Vamsa: Automated Provenance Tracking in Data Science Scripts,"There has recently been a lot of ongoing research in the areas of fairness,
bias and explainability of machine learning (ML) models due to the self-evident
or regulatory requirements of various ML applications. We make the following
observation: All of these approaches require a robust understanding of the
relationship between ML models and the data used to train them. In this work,
we introduce the ML provenance tracking problem: the fundamental idea is to
automatically track which columns in a dataset have been used to derive the
features/labels of an ML model. We discuss the challenges in capturing such
information in the context of Python, the most common language used by data
scientists. We then present Vamsa, a modular system that extracts provenance
from Python scripts without requiring any changes to the users' code. Using 26K
real data science scripts, we verify the effectiveness of Vamsa in terms of
coverage, and performance. We also evaluate Vamsa's accuracy on a smaller
subset of manually labeled data. Our analysis shows that Vamsa's precision and
recall range from 90.4% to 99.1% and its latency is in the order of
milliseconds for average size scripts. Drawing from our experience in deploying
ML models in production, we also present an example in which Vamsa helps
automatically identify models that are affected by data corruption issues.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.03535v4,2020-06-10T23:50:57Z,2020-01-06T05:32:15Z,"AutoDNNchip: An Automated DNN Chip Predictor and Builder for Both FPGAs
  and ASICs","Recent breakthroughs in Deep Neural Networks (DNNs) have fueled a growing
demand for DNN chips. However, designing DNN chips is non-trivial because: (1)
mainstream DNNs have millions of parameters and operations; (2) the large
design space due to the numerous design choices of dataflows, processing
elements, memory hierarchy, etc.; and (3) an algorithm/hardware co-design is
needed to allow the same DNN functionality to have a different decomposition,
which would require different hardware IPs to meet the application
specifications. Therefore, DNN chips take a long time to design and require
cross-disciplinary experts. To enable fast and effective DNN chip design, we
propose AutoDNNchip - a DNN chip generator that can automatically generate both
FPGA- and ASIC-based DNN chip implementation given DNNs from machine learning
frameworks (e.g., PyTorch) for a designated application and dataset.
Specifically, AutoDNNchip consists of two integrated enablers: (1) a Chip
Predictor, built on top of a graph-based accelerator representation, which can
accurately and efficiently predict a DNN accelerator's energy, throughput, and
area based on the DNN model parameters, hardware configuration,
technology-based IPs, and platform constraints; and (2) a Chip Builder, which
can automatically explore the design space of DNN chips (including IP
selection, block configuration, resource balancing, etc.), optimize chip design
via the Chip Predictor, and then generate optimized synthesizable RTL to
achieve the target design metrics. Experimental results show that our Chip
Predictor's predicted performance differs from real-measured ones by < 10% when
validated using 15 DNN models and 4 platforms (edge-FPGA/TPU/GPU and ASIC).
Furthermore, accelerators generated by our AutoDNNchip can achieve better (up
to 3.86X improvement) performance than that of expert-crafted state-of-the-art
accelerators.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.00660v1,2020-01-02T22:56:15Z,2020-01-02T22:56:15Z,A Parallel Sparse Tensor Benchmark Suite on CPUs and GPUs,"Tensor computations present significant performance challenges that impact a
wide spectrum of applications ranging from machine learning, healthcare
analytics, social network analysis, data mining to quantum chemistry and signal
processing. Efforts to improve the performance of tensor computations include
exploring data layout, execution scheduling, and parallelism in common tensor
kernels. This work presents a benchmark suite for arbitrary-order sparse tensor
kernels using state-of-the-art tensor formats: coordinate (COO) and
hierarchical coordinate (HiCOO) on CPUs and GPUs. It presents a set of
reference tensor kernel implementations that are compatible with real-world
tensors and power law tensors extended from synthetic graph generation
techniques. We also propose Roofline performance models for these kernels to
provide insights of computer platforms from sparse tensor view.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.00487v1,2020-01-02T15:22:36Z,2020-01-02T15:22:36Z,"Using CNNs For Users Segmentation In Video See-Through Augmented
  Virtuality","In this paper, we present preliminary results on the use of deep learning
techniques to integrate the users self-body and other participants into a
head-mounted video see-through augmented virtuality scenario. It has been
previously shown that seeing users bodies in such simulations may improve the
feeling of both self and social presence in the virtual environment, as well as
user performance. We propose to use a convolutional neural network for real
time semantic segmentation of users bodies in the stereoscopic RGB video
streams acquired from the perspective of the user. We describe design issues as
well as implementation details of the system and demonstrate the feasibility of
using such neural networks for merging users bodies in an augmented virtuality
simulation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1912.09621v1,2019-12-20T02:57:05Z,2019-12-20T02:57:05Z,"Understanding Deep Neural Network Predictions for Medical Imaging
  Applications","Computer-aided detection has been a research area attracting great interest
in the past decade. Machine learning algorithms have been utilized extensively
for this application as they provide a valuable second opinion to the doctors.
Despite several machine learning models being available for medical imaging
applications, not many have been implemented in the real-world due to the
uninterpretable nature of the decisions made by the network. In this paper, we
investigate the results provided by deep neural networks for the detection of
malaria, diabetic retinopathy, brain tumor, and tuberculosis in different
imaging modalities. We visualize the class activation mappings for all the
applications in order to enhance the understanding of these networks. This type
of visualization, along with the corresponding network performance metrics,
would aid the data science experts in better understanding of their models as
well as assisting doctors in their decision-making process.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1912.08202v2,2020-10-01T06:12:27Z,2019-12-17T23:33:58Z,"Extrinsic Kernel Ridge Regression Classifier for Planar Kendall Shape
  Space","Kernel methods have had great success in Statistics and Machine Learning.
Despite their growing popularity, however, less effort has been drawn towards
developing kernel based classification methods on Riemannian manifolds due to
difficulty in dealing with non-Euclidean geometry. In this paper, motivated by
the extrinsic framework of manifold-valued data analysis, we propose a new
positive definite kernel on planar Kendall shape space $\Sigma_2^k$, called
extrinsic Veronese Whitney Gaussian kernel. We show that our approach can be
extended to develop Gaussian kernels on any embedded manifold. Furthermore,
kernel ridge regression classifier (KRRC) is implemented to address the shape
classification problem on $\Sigma_2^k$, and their promising performances are
illustrated through the real data analysis.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1912.06321v2,2020-08-17T03:26:55Z,2019-12-13T04:29:38Z,"Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World
  Performance?","Does progress in simulation translate to progress on robots? If one method
outperforms another in simulation, how likely is that trend to hold in reality
on a robot? We examine this question for embodied PointGoal navigation,
developing engineering tools and a research paradigm for evaluating a simulator
by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy),
a library for seamless execution of identical code on simulated agents and
robots, transferring simulation-trained agents to a LoCoBot platform with a
one-line code change. Second, we investigate the sim2real predictivity of
Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create
a virtualized replica, and run parallel tests of 9 different models in reality
and simulation. We present a new metric called Sim-vs-Real Correlation
Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as
used for the CVPR19 challenge is low (0.18 for the success metric), suggesting
that performance differences in this simulator-based challenge do not persist
after physical deployment. This gap is largely due to AI agents learning to
exploit simulator imperfections, abusing collision dynamics to 'slide' along
walls, leading to shortcuts through otherwise non-navigable space. Naturally,
such exploits do not work in the real world. Our experiments show that it is
possible to tune simulation parameters to improve sim2real predictivity (e.g.
improving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that
in-simulation comparisons will translate to deployed systems in reality.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1912.03647v2,2020-08-11T03:42:21Z,2019-12-08T09:51:08Z,Compressing 3DCNNs Based on Tensor Train Decomposition,"Three dimensional convolutional neural networks (3DCNNs) have been applied in
many tasks, e.g., video and 3D point cloud recognition. However, due to the
higher dimension of convolutional kernels, the space complexity of 3DCNNs is
generally larger than that of traditional two dimensional convolutional neural
networks (2DCNNs). To miniaturize 3DCNNs for the deployment in confining
environments such as embedded devices, neural network compression is a
promising approach. In this work, we adopt the tensor train (TT) decomposition,
a straightforward and simple in situ training compression method, to shrink the
3DCNN models. Through proposing tensorizing 3D convolutional kernels in TT
format, we investigate how to select appropriate TT ranks for achieving higher
compression ratio. We have also discussed the redundancy of 3D convolutional
kernels for compression, core significance and future directions of this work,
as well as the theoretical computation complexity versus practical executing
time of convolution in TT. In the light of multiple contrast experiments based
on VIVA challenge, UCF11, and UCF101 datasets, we conclude that TT
decomposition can compress 3DCNNs by around one hundred times without
significant accuracy loss, which will enable its applications in extensive real
world scenarios.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1912.02102v1,2019-12-03T02:11:50Z,2019-12-03T02:11:50Z,"Artificial Intelligence for Low-Resource Communities: Influence
  Maximization in an Uncertain World","The potential of Artificial Intelligence (AI) to tackle challenging problems
that afflict society is enormous, particularly in the areas of healthcare,
conservation and public safety and security. Many problems in these domains
involve harnessing social networks of under-served communities to enable
positive change, e.g., using social networks of homeless youth to raise
awareness about Human Immunodeficiency Virus (HIV) and other STDs.
Unfortunately, most of these real-world problems are characterized by
uncertainties about social network structure and influence models, and previous
research in AI fails to sufficiently address these uncertainties. This thesis
addresses these shortcomings by advancing the state-of-the-art to a new
generation of algorithms for interventions in social networks. In particular,
this thesis describes the design and development of new influence maximization
algorithms which can handle various uncertainties that commonly exist in
real-world social networks. These algorithms utilize techniques from sequential
planning problems and social network theory to develop new kinds of AI
algorithms. Further, this thesis also demonstrates the real-world impact of
these algorithms by describing their deployment in three pilot studies to
spread awareness about HIV among actual homeless youth in Los Angeles. This
represents one of the first-ever deployments of computer science based
influence maximization algorithms in this domain. Our results show that our AI
algorithms improved upon the state-of-the-art by 160% in the real-world. We
discuss research and implementation challenges faced in deploying these
algorithms, and lessons that can be gleaned for future deployment of such
algorithms. The positive results from these deployments illustrate the enormous
potential of AI in addressing societally relevant problems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1912.01100v2,2020-03-04T09:50:32Z,2019-12-02T22:16:32Z,Latent Replay for Real-Time Continual Learning,"Training deep neural networks at the edge on light computational devices,
embedded systems and robotic platforms is nowadays very challenging. Continual
learning techniques, where complex models are incrementally trained on small
batches of new data, can make the learning problem tractable even for CPU-only
embedded devices enabling remarkable levels of adaptiveness and autonomy.
However, a number of practical problems need to be solved: catastrophic
forgetting before anything else. In this paper we introduce an original
technique named ""Latent Replay"" where, instead of storing a portion of past
data in the input space, we store activations volumes at some intermediate
layer. This can significantly reduce the computation and storage required by
native rehearsal. To keep the representation stable and the stored activations
valid we propose to slow-down learning at all the layers below the latent
replay one, leaving the layers above free to learn at full pace. In our
experiments we show that Latent Replay, combined with existing continual
learning techniques, achieves state-of-the-art performance on complex video
benchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d.
batches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly
real-time continual learning on the edge through the deployment of the proposed
technique on a smartphone device.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.12672v1,2019-11-28T12:35:55Z,2019-11-28T12:35:55Z,"Improved cross-validation for classifiers that make algorithmic choices
  to minimise runtime without compromising output correctness","Our topic is the use of machine learning to improve software by making
choices which do not compromise the correctness of the output, but do affect
the time taken to produce such output. We are particularly concerned with
computer algebra systems (CASs), and in particular, our experiments are for
selecting the variable ordering to use when performing a cylindrical algebraic
decomposition of $n$-dimensional real space with respect to the signs of a set
of polynomials.
  In our prior work we explored the different ML models that could be used, and
how to identify suitable features of the input polynomials. In the present
paper we both repeat our prior experiments on problems which have more
variables (and thus exponentially more possible orderings), and examine the
metric which our ML classifiers targets. The natural metric is computational
runtime, with classifiers trained to pick the ordering which minimises this.
However, this leads to the situation were models do not distinguish between any
of the non-optimal orderings, whose runtimes may still vary dramatically. In
this paper we investigate a modification to the cross-validation algorithms of
the classifiers so that they do distinguish these cases, leading to improved
results.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.10751v1,2019-11-25T08:02:17Z,2019-11-25T08:02:17Z,"Deep Image-to-Video Adaptation and Fusion Networks for Action
  Recognition","Existing deep learning methods for action recognition in videos require a
large number of labeled videos for training, which is labor-intensive and
time-consuming. For the same action, the knowledge learned from different media
types, e.g., videos and images, may be related and complementary. However, due
to the domain shifts and heterogeneous feature representations between videos
and images, the performance of classifiers trained on images may be
dramatically degraded when directly deployed to videos. In this paper, we
propose a novel method, named Deep Image-to-Video Adaptation and Fusion
Networks (DIVAFN), to enhance action recognition in videos by transferring
knowledge from images using video keyframes as a bridge. The DIVAFN is a
unified deep learning model, which integrates domain-invariant representations
learning and cross-modal feature fusion into a unified optimization framework.
Specifically, we design an efficient cross-modal similarities metric to reduce
the modality shift among images, keyframes and videos. Then, we adopt an
autoencoder architecture, whose hidden layer is constrained to be the semantic
representations of the action class names. In this way, when the autoencoder is
adopted to project the learned features from different domains to the same
space, more compact, informative and discriminative representations can be
obtained. Finally, the concatenation of the learned semantic feature
representations from these three autoencoders are used to train the classifier
for action recognition in videos. Comprehensive experiments on four real-world
datasets show that our method outperforms some state-of-the-art domain
adaptation and action recognition methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.09592v1,2019-11-21T16:37:18Z,2019-11-21T16:37:18Z,"mm-Pose: Real-Time Human Skeletal Posture Estimation using mmWave Radars
  and CNNs","In this paper, mm-Pose, a novel approach to detect and track human skeletons
in real-time using an mmWave radar, is proposed. To the best of the authors'
knowledge, this is the first method to detect >15 distinct skeletal joints
using mmWave radar reflection signals. The proposed method would find several
applications in traffic monitoring systems, autonomous vehicles, patient
monitoring systems and defense forces to detect and track human skeleton for
effective and preventive decision making in real-time. The use of radar makes
the system operationally robust to scene lighting and adverse weather
conditions. The reflected radar point cloud in range, azimuth and elevation are
first resolved and projected in Range-Azimuth and Range-Elevation planes. A
novel low-size high-resolution radar-to-image representation is also presented,
that overcomes the sparsity in traditional point cloud data and offers
significant reduction in the subsequent machine learning architecture. The RGB
channels were assigned with the normalized values of range, elevation/azimuth
and the power level of the reflection signals for each of the points. A forked
CNN architecture was used to predict the real-world position of the skeletal
joints in 3-D space, using the radar-to-image representation. The proposed
method was tested for a single human scenario for four primary motions, (i)
Walking, (ii) Swinging left arm, (iii) Swinging right arm, and (iv) Swinging
both arms to validate accurate predictions for motion in range, azimuth and
elevation. The detailed methodology, implementation, challenges, and validation
results are presented.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.09281v1,2019-11-21T04:19:16Z,2019-11-21T04:19:16Z,"Event Detection in Noisy Streaming Data with Combination of
  Corroborative and Probabilistic Sources","Global physical event detection has traditionally relied on dense coverage of
physical sensors around the world; while this is an expensive undertaking,
there have not been alternatives until recently. The ubiquity of social
networks and human sensors in the field provides a tremendous amount of
real-time, live data about true physical events from around the world. However,
while such human sensor data have been exploited for retrospective large-scale
event detection, such as hurricanes or earthquakes, they has been limited to no
success in exploiting this rich resource for general physical event detection.
  Prior implementation approaches have suffered from the concept drift
phenomenon, where real-world data exhibits constant, unknown, unbounded changes
in its data distribution, making static machine learning models ineffective in
the long term. We propose and implement an end-to-end collaborative drift
adaptive system that integrates corroborative and probabilistic sources to
deliver real-time predictions. Furthermore, out system is adaptive to concept
drift and performs automated continuous learning to maintain high performance.
We demonstrate our approach in a real-time demo available online for landslide
disaster detection, with extensibility to other real-world physical events such
as flooding, wildfires, hurricanes, and earthquakes.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.08448v4,2020-03-17T16:06:58Z,2019-11-19T18:14:58Z,Artificial intelligence approach to momentum risk-taking,"We propose a mathematical model of momentum risk-taking, which is essentially
real-time risk management focused on short-term volatility of stock markets.
Its implementation, our fully automated momentum equity trading system
presented systematically, proved to be successful in extensive historical and
real-time experiments. Momentum risk-taking is one of the key components of
general decision-making, a challenge for artificial intelligence and machine
learning with deep roots in cognitive science; its variants beyond stock
markets are discussed. We begin with a new algebraic-type theory of news impact
on share-prices, which describes well their power growth, periodicity, and the
market phenomena like price targets and profit-taking. This theory generally
requires Bessel and hypergeometric functions. Its discretization results in
some tables of bids, which are basically expected returns for main investment
horizons, the key in our trading system. The ML procedures we use are similar
to those in neural networking. A preimage of our approach is the new contract
card game provided at the end, a combination of bridge and poker. Relations to
random processes and the fractional Brownian motion are outlined.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.08089v2,2019-12-07T03:42:06Z,2019-11-19T04:28:47Z,"""The Human Body is a Black Box"": Supporting Clinical Decision-Making
  with Deep Learning","Machine learning technologies are increasingly developed for use in
healthcare. While research communities have focused on creating
state-of-the-art models, there has been less focus on real world implementation
and the associated challenges to accuracy, fairness, accountability, and
transparency that come from actual, situated use. Serious questions remain
under examined regarding how to ethically build models, interpret and explain
model output, recognize and account for biases, and minimize disruptions to
professional expertise and work cultures. We address this gap in the literature
and provide a detailed case study covering the development, implementation, and
evaluation of Sepsis Watch, a machine learning-driven tool that assists
hospital clinicians in the early diagnosis and treatment of sepsis. We, the
team that developed and evaluated the tool, discuss our conceptualization of
the tool not as a model deployed in the world but instead as a socio-technical
system requiring integration into existing social and professional contexts.
Rather than focusing on model interpretability to ensure a fair and accountable
machine learning, we point toward four key values and practices that should be
considered when developing machine learning to support clinical
decision-making: rigorously define the problem in context, build relationships
with stakeholders, respect professional discretion, and create ongoing feedback
loops with stakeholders. Our work has significant implications for future
research regarding mechanisms of institutional accountability and
considerations for designing machine learning systems. Our work underscores the
limits of model interpretability as a solution to ensure transparency,
accuracy, and accountability in practice. Instead, our work demonstrates other
means and goals to achieve FATML values in design and in practice.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.06636v2,2020-06-16T09:13:58Z,2019-11-15T13:57:35Z,"Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body
  Tasks","We address the longstanding challenge of producing flexible, realistic
humanoid character controllers that can perform diverse whole-body tasks
involving object interactions. This challenge is central to a variety of
fields, from graphics and animation to robotics and motor neuroscience. Our
physics-based environment uses realistic actuation and first-person perception
-- including touch sensors and egocentric vision -- with a view to producing
active-sensing behaviors (e.g. gaze direction), transferability to real robots,
and comparisons to the biology. We develop an integrated neural-network based
approach consisting of a motor primitive module, human demonstrations, and an
instructed reinforcement learning regime with curricula and task variations. We
demonstrate the utility of our approach for several tasks, including
goal-conditioned box carrying and ball catching, and we characterize its
behavioral robustness. The resulting controllers can be deployed in real-time
on a standard PC. See overview video, https://youtu.be/2rQAW-8gQQk .",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.03848v1,2019-11-10T04:36:59Z,2019-11-10T04:36:59Z,Embedded Neural Networks for Robot Autonomy,"We present a library to automatically embed signal processing and neural
network predictions into the material robots are made of. Deep and shallow
neural network models are first trained offline using state-of-the-art machine
learning tools and then transferred onto general purpose microcontrollers that
are co-located with a robot's sensors and actuators. We validate this approach
using multiple examples: a smart robotic tire for terrain classification, a
robotic finger sensor for load classification and a smart composite capable of
regressing impact source localization. In each example, sensing and computation
are embedded inside the material, creating artifacts that serve as stand-in
replacement for otherwise inert conventional parts. The open source software
library takes as inputs trained model files from higher level learning
software, such as Tensorflow/Keras, and outputs code that is readable in a
microcontroller that supports C. We compare the performance of this approach
for various embedded platforms. In particular, we show that low-cost
off-the-shelf microcontrollers can match the accuracy of a desktop computer,
while being fast enough for real-time applications at different neural network
configurations. We provide means to estimate the maximum number of parameters
that the hardware will support based on the microcontroller's specifications.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.04469v1,2019-11-09T19:59:17Z,2019-11-09T19:59:17Z,"A Proposed Artificial intelligence Model for Real-Time Human Action
  Localization and Tracking","In recent years, artificial intelligence (AI) based on deep learning (DL) has
sparked tremendous global interest. DL is widely used today and has expanded
into various interesting areas. It is becoming more popular in cross-subject
research, such as studies of smart city systems, which combine computer science
with engineering applications. Human action detection is one of these areas.
Human action detection is an interesting challenge due to its stringent
requirements in terms of computing speed and accuracy. High-accuracy real-time
object tracking is also considered a significant challenge. This paper
integrates the YOLO detection network, which is considered a state-of-the-art
tool for real-time object detection, with motion vectors and the Coyote
Optimization Algorithm (COA) to construct a real-time human action localization
and tracking system. The proposed system starts with the extraction of motion
information from a compressed video stream and the extraction of appearance
information from RGB frames using an object detector. Then, a fusion step
between the two streams is performed, and the results are fed into the proposed
action tracking model. The COA is used in object tracking due to its accuracy
and fast convergence. The basic foundation of the proposed model is the
utilization of motion vectors, which already exist in a compressed video bit
stream and provide sufficient information to improve the localization of the
target action without requiring high consumption of computational resources
compared with other popular methods of extracting motion information, such as
optical flows. This advantage allows the proposed approach to be implemented in
challenging environments where the computational resources are limited, such as
Internet of Things (IoT) systems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1911.02142v2,2020-03-16T20:05:29Z,2019-11-05T23:39:55Z,Intriguing Properties of Adversarial ML Attacks in the Problem Space,"Recent research efforts on adversarial ML have investigated problem-space
attacks, focusing on the generation of real evasive objects in domains where,
unlike images, there is no clear inverse mapping to the feature space (e.g.,
software). However, the design, comparison, and real-world implications of
problem-space attacks remain underexplored. This paper makes two major
contributions. First, we propose a novel formalization for adversarial ML
evasion attacks in the problem-space, which includes the definition of a
comprehensive set of constraints on available transformations, preserved
semantics, robustness to preprocessing, and plausibility. We shed light on the
relationship between feature space and problem space, and we introduce the
concept of side-effect features as the byproduct of the inverse feature-mapping
problem. This enables us to define and prove necessary and sufficient
conditions for the existence of problem-space attacks. We further demonstrate
the expressive power of our formalization by using it to describe several
attacks from related literature across different domains. Second, building on
our formalization, we propose a novel problem-space attack on Android malware
that overcomes past limitations. Experiments on a dataset with 170K Android
apps from 2017 and 2018 show the practical feasibility of evading a
state-of-the-art malware classifier along with its hardened version. Our
results demonstrate that ""adversarial-malware as a service"" is a realistic
threat, as we automatically generate thousands of realistic and inconspicuous
adversarial applications at scale, where on average it takes only a few minutes
to generate an adversarial app. Our formalization of problem-space attacks
paves the way to more principled research in this domain.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.13676v1,2019-10-30T05:13:33Z,2019-10-30T05:13:33Z,Multi Modal Semantic Segmentation using Synthetic Data,"Semantic understanding of scenes in three-dimensional space (3D) is a
quintessential part of robotics oriented applications such as autonomous
driving as it provides geometric cues such as size, orientation and true
distance of separation to objects which are crucial for taking mission critical
decisions. As a first step, in this work we investigate the possibility of
semantically classifying different parts of a given scene in 3D by learning the
underlying geometric context in addition to the texture cues BUT in the absence
of labelled real-world datasets. To this end we generate a large number of
synthetic scenes, their pixel-wise labels and corresponding 3D representations
using CARLA software framework. We then build a deep neural network that learns
underlying category specific 3D representation and texture cues from color
information of the rendered synthetic scenes. Further on we apply the learned
model on different real world datasets to evaluate its performance. Our
preliminary investigation of results show that the neural network is able to
learn the geometric context from synthetic scenes and effectively apply this
knowledge to classify each point of a 3D representation of a scene in
real-world.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.13580v1,2019-10-29T23:43:01Z,2019-10-29T23:43:01Z,Domain Generalization via Model-Agnostic Learning of Semantic Features,"Generalization capability to unseen domains is crucial for machine learning
models when deploying to real-world conditions. We investigate the challenging
problem of domain generalization, i.e., training a model on multi-domain source
data such that it can directly generalize to target domains with unknown
statistics. We adopt a model-agnostic learning paradigm with gradient-based
meta-train and meta-test procedures to expose the optimization to domain shift.
Further, we introduce two complementary losses which explicitly regularize the
semantic structure of the feature space. Globally, we align a derived soft
confusion matrix to preserve general knowledge about inter-class relationships.
Locally, we promote domain-independent class-specific cohesion and separation
of sample features with a metric-learning component. The effectiveness of our
method is demonstrated with new state-of-the-art results on two common object
recognition benchmarks. Our method also shows consistent improvement on a
medical image segmentation task.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.12980v1,2019-10-28T21:43:22Z,2019-10-28T21:43:22Z,Learning Transferable Graph Exploration,"This paper considers the problem of efficient exploration of unseen
environments, a key challenge in AI. We propose a `learning to explore'
framework where we learn a policy from a distribution of environments. At test
time, presented with an unseen environment from the same distribution, the
policy aims to generalize the exploration strategy to visit the maximum number
of unique states in a limited number of steps. We particularly focus on
environments with graph-structured state-spaces that are encountered in many
important real-world applications like software testing and map building. We
formulate this task as a reinforcement learning problem where the `exploration'
agent is rewarded for transitioning to previously unseen environment states and
employ a graph-structured memory to encode the agent's past trajectory.
Experimental results demonstrate that our approach is extremely effective for
exploration of spatial maps; and when applied on the challenging problems of
coverage-guided software-testing of domain-specific programs and real-world
mobile applications, it outperforms methods that have been hand-engineered by
human experts.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.12750v1,2019-10-28T15:21:48Z,2019-10-28T15:21:48Z,"Deep-Learning-Based Image Segmentation Integrated with Optical
  Microscopy for Automatically Searching for Two-Dimensional Materials","Deep-learning algorithms enable precise image recognition based on
high-dimensional hierarchical image features. Here, we report the development
and implementation of a deep-learning-based image segmentation algorithm in an
autonomous robotic system to search for two-dimensional (2D) materials. We
trained the neural network based on Mask-RCNN on annotated optical microscope
images of 2D materials (graphene, hBN, MoS2, and WTe2). The inference algorithm
is run on a 1024 x 1024 px2 optical microscope images for 200 ms, enabling the
real-time detection of 2D materials. The detection process is robust against
changes in the microscopy conditions, such as illumination and color balance,
which obviates the parameter-tuning process required for conventional
rule-based detection algorithms. Integrating the algorithm with a motorized
optical microscope enables the automated searching and cataloging of 2D
materials. This development will allow researchers to utilize unlimited amounts
of 2D materials simply by exfoliating and running the automated searching
process.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.12202v4,2020-09-14T02:38:16Z,2019-10-27T07:42:15Z,CONNA: Addressing Name Disambiguation on The Fly,"Name disambiguation is a key and also a very tough problem in many online
systems such as social search and academic search. Despite considerable
research, a critical issue that has not been systematically studied is
disambiguation on the fly -- to complete the disambiguation in the real-time.
This is very challenging, as the disambiguation algorithm must be accurate,
efficient, and error tolerance. In this paper, we propose a novel framework --
CONNA -- to train a matching component and a decision component jointly via
reinforcement learning. The matching component is responsible for finding the
top matched candidate for the given paper, and the decision component is
responsible for deciding on assigning the top matched person or creating a new
person. The two components are intertwined and can be bootstrapped via jointly
training. Empirically, we evaluate CONNA on two name disambiguation datasets.
Experimental results show that the proposed framework can achieve a
1.21%-19.84% improvement on F1-score using joint training of the matching and
the decision components. The proposed CONNA has been successfully deployed on
AMiner -- a large online academic search system.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.11779v1,2019-10-25T15:03:11Z,2019-10-25T15:03:11Z,"Toward a better trade-off between performance and fairness with
  kernel-based distribution matching","As recent literature has demonstrated how classifiers often carry unintended
biases toward some subgroups, deploying machine learned models to users demands
careful consideration of the social consequences. How should we address this
problem in a real-world system? How should we balance core performance and
fairness metrics? In this paper, we introduce a MinDiff framework for
regularizing classifiers toward different fairness metrics and analyze a
technique with kernel-based statistical dependency tests. We run a thorough
study on an academic dataset to compare the Pareto frontier achieved by
different regularization approaches, and apply our kernel-based method to two
large-scale industrial systems demonstrating real-world improvements.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/2001.09938v1,2019-10-22T15:57:20Z,2019-10-22T15:57:20Z,"Autonomous discovery of battery electrolytes with robotic
  experimentation and machine-learning","Innovations in batteries take years to formulate and commercialize, requiring
extensive experimentation during the design and optimization phases. We
approached the design and selection of a battery electrolyte through a
black-box optimization algorithm directly integrated into a robotic test-stand.
We report here the discovery of a novel battery electrolyte by this experiment
completely guided by the machine-learning software without human intervention.
Motivated by the recent trend toward super-concentrated aqueous electrolytes
for high-performance batteries, we utilize Dragonfly - a Bayesian
machine-learning software package - to search mixtures of commonly used lithium
and sodium salts for super-concentrated aqueous electrolytes with wide
electrochemical stability windows. Dragonfly autonomously managed the robotic
test-stand, recommending electrolyte designs to test and receiving experimental
feedback in real time. In 40 hours of continuous experimentation over a
four-dimensional design space with millions of potential candidates, Dragonfly
discovered a novel, mixed-anion aqueous sodium electrolyte with a wider
electrochemical stability window than state-of-the-art sodium electrolyte. A
human-guided design process may have missed this optimal electrolyte. This
result demonstrates the possibility of integrating robotics with
machine-learning to rapidly and autonomously discover novel battery materials.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.10045v2,2019-12-26T08:09:25Z,2019-10-22T15:27:30Z,"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,
  Opportunities and Challenges toward Responsible AI","In the last years, Artificial Intelligence (AI) has achieved a notable
momentum that may deliver the best of expectations over many application
sectors across the field. For this to occur, the entire community stands in
front of the barrier of explainability, an inherent problem of AI techniques
brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not
present in the last hype of AI. Paradigms underlying this problem fall within
the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial
feature for the practical deployment of AI models. This overview examines the
existing literature in the field of XAI, including a prospect toward what is
yet to be reached. We summarize previous efforts to define explainability in
Machine Learning, establishing a novel definition that covers prior conceptual
propositions with a major focus on the audience for which explainability is
sought. We then propose and discuss about a taxonomy of recent contributions
related to the explainability of different Machine Learning models, including
those aimed at Deep Learning methods for which a second taxonomy is built. This
literature analysis serves as the background for a series of challenges faced
by XAI, such as the crossroads between data fusion and explainability. Our
prospects lead toward the concept of Responsible Artificial Intelligence,
namely, a methodology for the large-scale implementation of AI methods in real
organizations with fairness, model explainability and accountability at its
core. Our ultimate goal is to provide newcomers to XAI with a reference
material in order to stimulate future research advances, but also to encourage
experts and professionals from other disciplines to embrace the benefits of AI
in their activity sectors, without any prior bias for its lack of
interpretability.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.09435v1,2019-10-21T15:12:32Z,2019-10-21T15:12:32Z,"Background Rejection in Atmospheric Cherenkov Telescopes using Recurrent
  Convolutional Neural Networks","In this work, we present a new, high performance algorithm for background
rejection in imaging atmospheric Cherenkov telescopes. We build on the already
popular machine-learning techniques used in gamma-ray astronomy by the
application of the latest techniques in machine learning, namely recurrent and
convolutional neural networks, to the background rejection problem. Use of
these machine-learning techniques addresses some of the key challenges
encountered in the currently implemented algorithms and helps to significantly
increase the background rejection performance at all energies.
  We apply these machine learning techniques to the H.E.S.S. telescope array,
first testing their performance on simulated data and then applying the
analysis to two well known gamma-ray sources. With real observational data we
find significantly improved performance over the current standard methods, with
a 20-25\% reduction in the background rate when applying the recurrent neural
network analysis. Importantly, we also find that the convolutional neural
network results are strongly dependent on the sky brightness in the source
region which has important implications for the future implementation of this
method in Cherenkov telescope analysis.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.09430v2,2020-02-06T16:28:34Z,2019-10-21T15:06:03Z,Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video,"Key challenges for the deployment of reinforcement learning (RL) agents in
the real world are the discovery, representation and reuse of skills in the
absence of a reward function. To this end, we propose a novel approach to learn
a task-agnostic skill embedding space from unlabeled multi-view videos. Our
method learns a general skill embedding independently from the task context by
using an adversarial loss. We combine a metric learning loss, which utilizes
temporal video coherence to learn a state representation, with an entropy
regularized adversarial skill-transfer loss. The metric learning loss learns a
disentangled representation by attracting simultaneous viewpoints of the same
observations and repelling visually similar frames from temporal neighbors. The
adversarial skill-transfer loss enhances re-usability of learned skill
embeddings over multiple task domains. We show that the learned embedding
enables training of continuous control policies to solve novel tasks that
require the interpolation of previously seen skills. Our extensive evaluation
with both simulation and real world data demonstrates the effectiveness of our
method in learning transferable skills from unlabeled interaction videos and
composing them for new tasks. Code, pretrained models and dataset are available
at http://robotskills.cs.uni-freiburg.de",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.06893v1,2019-10-15T16:07:34Z,2019-10-15T16:07:34Z,"Extracting robust and accurate features via a robust information
  bottleneck","We propose a novel strategy for extracting features in supervised learning
that can be used to construct a classifier which is more robust to small
perturbations in the input space. Our method builds upon the idea of the
information bottleneck by introducing an additional penalty term that
encourages the Fisher information of the extracted features to be small, when
parametrized by the inputs. By tuning the regularization parameter, we can
explicitly trade off the opposing desiderata of robustness and accuracy when
constructing a classifier. We derive the optimal solution to the robust
information bottleneck when the inputs and outputs are jointly Gaussian,
proving that the optimally robust features are also jointly Gaussian in that
setting. Furthermore, we propose a method for optimizing a variational bound on
the robust information bottleneck objective in general settings using
stochastic gradient descent, which may be implemented efficiently in neural
networks. Our experimental results for synthetic and real data sets show that
the proposed feature extraction method indeed produces classifiers with
increased robustness to perturbations.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.06840v3,2020-01-19T09:18:47Z,2019-10-15T14:58:54Z,A Hybrid Compact Neural Architecture for Visual Place Recognition,"State-of-the-art algorithms for visual place recognition, and related visual
navigation systems, can be broadly split into two categories:
computer-science-oriented models including deep learning or image
retrieval-based techniques with minimal biological plausibility, and
neuroscience-oriented dynamical networks that model temporal properties
underlying spatial navigation in the brain. In this letter, we propose a new
compact and high-performing place recognition model that bridges this divide
for the first time. Our approach comprises two key neural models of these
categories: (1) FlyNet, a compact, sparse two-layer neural network inspired by
brain architectures of fruit flies, Drosophila melanogaster, and (2) a
one-dimensional continuous attractor neural network (CANN). The resulting
FlyNet+CANN network incorporates the compact pattern recognition capabilities
of our FlyNet model with the powerful temporal filtering capabilities of an
equally compact CANN, replicating entirely in a hybrid neural implementation
the functionality that yields high performance in algorithmic localization
approaches like SeqSLAM. We evaluate our model, and compare it to three
state-of-the-art methods, on two benchmark real-world datasets with small
viewpoint variations and extreme environmental changes - achieving 87% AUC
results under day to night transitions compared to 60% for Multi-Process
Fusion, 46% for LoST-X and 1% for SeqSLAM, while being 6.5, 310, and 1.5 times
faster, respectively.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.06316v3,2021-03-25T03:03:02Z,2019-10-14T17:58:23Z,NeurVPS: Neural Vanishing Point Scanning via Conic Convolution,"We present a simple yet effective end-to-end trainable deep network with
geometry-inspired convolutional operators for detecting vanishing points in
images. Traditional convolutional neural networks rely on aggregating edge
features and do not have mechanisms to directly exploit the geometric
properties of vanishing points as the intersections of parallel lines. In this
work, we identify a canonical conic space in which the neural network can
effectively compute the global geometric information of vanishing points
locally, and we propose a novel operator named conic convolution that can be
implemented as regular convolutions in this space. This new operator explicitly
enforces feature extractions and aggregations along the structural lines and
yet has the same number of parameters as the regular 2D convolution. Our
extensive experiments on both synthetic and real-world datasets show that the
proposed operator significantly improves the performance of vanishing point
detection over traditional methods. The code and dataset have been made
publicly available at https://github.com/zhou13/neurvps.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.05639v2,2019-11-06T19:40:41Z,2019-10-12T19:57:55Z,"Disentangling Interpretable Generative Parameters of Random and
  Real-World Graphs","While a wide range of interpretable generative procedures for graphs exist,
matching observed graph topologies with such procedures and choices for its
parameters remains an open problem. Devising generative models that closely
reproduce real-world graphs requires domain knowledge and time-consuming
simulation. While existing deep learning approaches rely on less manual
modelling, they offer little interpretability. This work approaches graph
generation (decoding) as the inverse of graph compression (encoding). We show
that in a disentanglement-focused deep autoencoding framework, specifically
Beta-Variational Autoencoders (Beta-VAE), choices of generative procedures and
their parameters arise naturally in the latent space. Our model is capable of
learning disentangled, interpretable latent variables that represent the
generative parameters of procedurally generated random graphs and real-world
graphs. The degree of disentanglement is quantitatively measured using the
Mutual Information Gap (MIG). When training our Beta-VAE model on ER random
graphs, its latent variables have a near one-to-one mapping to the ER random
graph parameters n and p. We deploy the model to analyse the correlation
between graph topology and node attributes measuring their mutual dependence
without handpicking topological properties.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.00024v3,2020-04-28T15:16:17Z,2019-09-30T18:00:05Z,Neural Canonical Transformation with Symplectic Flows,"Canonical transformation plays a fundamental role in simplifying and solving
classical Hamiltonian systems. We construct flexible and powerful canonical
transformations as generative models using symplectic neural networks. The
model transforms physical variables towards a latent representation with an
independent harmonic oscillator Hamiltonian. Correspondingly, the phase space
density of the physical system flows towards a factorized Gaussian distribution
in the latent space. Since the canonical transformation preserves the
Hamiltonian evolution, the model captures nonlinear collective modes in the
learned latent representation. We present an efficient implementation of
symplectic neural coordinate transformations and two ways to train the model.
The variational free energy calculation is based on the analytical form of
physical Hamiltonian. While the phase space density estimation only requires
samples in the coordinate space for separable Hamiltonians. We demonstrate
appealing features of neural canonical transformation using toy problems
including two-dimensional ring potential and harmonic chain. Finally, we apply
the approach to real-world problems such as identifying slow collective modes
in alanine dipeptide and conceptual compression of the MNIST dataset.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1910.07083v1,2019-09-29T22:04:19Z,2019-09-29T22:04:19Z,"Occurence of A Cyber Security Eco-System: A Nature Oriented Project and
  Evaluation of An Indirect Social Experiment","Because of todays technological developments and the influence of digital
systems into every aspect of our lives, importance of cyber security improves
more and more day-by-day. Projects, educational processes and seminars realized
for this aim create and improve awareness among individuals and provide useful
tools for growing equipped generations. The aim of this study is to focus on a
cyber security eco-system, which was self-occurred within the interactive
educational environment designed under the scope of TUBITAK 4004 Nature
Education and Science Schools Projects (with the name of A Cyber Security
Adventure) with the use of important technologies such as virtual reality,
augmented reality, and artificial intelligence. The eco-system occurred within
the interactive educational process where high school students took place
caused both students and the project team to experience an indirect social
experiment environment. In this sense, it is thought that the findings and
comments presented in the study will give important ideas to everyone involved
in cyber security education, life-long learning processes, and the technology
use in software oriented educational tools.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.13343v2,2019-10-01T16:06:39Z,2019-09-29T19:15:08Z,"ISTHMUS: Secure, Scalable, Real-time and Robust Machine Learning
  Platform for Healthcare","In recent times, machine learning (ML) and artificial intelligence (AI) based
systems have evolved and scaled across different industries such as finance,
retail, insurance, energy utilities, etc. Among other things, they have been
used to predict patterns of customer behavior, to generate pricing models, and
to predict the return on investments. But the successes in deploying machine
learning models at scale in those industries have not translated into the
healthcare setting. There are multiple reasons why integrating ML models into
healthcare has not been widely successful, but from a technical perspective,
general-purpose commercial machine learning platforms are not a good fit for
healthcare due to complexities in handling data quality issues, mandates to
demonstrate clinical relevance, and a lack of ability to monitor performance in
a highly regulated environment with stringent security and privacy needs. In
this paper, we describe Isthmus, a turnkey, cloud-based platform which
addresses the challenges above and reduces time to market for operationalizing
ML/AI in healthcare. Towards the end, we describe three case studies which shed
light on Isthmus capabilities. These include (1) supporting an end-to-end
lifecycle of a model which predicts trauma survivability at hospital trauma
centers, (2) bringing in and harmonizing data from disparate sources to create
a community data platform for inferring population as well as patient level
insights for Social Determinants of Health (SDoH), and (3) ingesting
live-streaming data from various IoT sensors to build models, which can
leverage real-time and longitudinal information to make advanced time-sensitive
predictions.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.11822v1,2019-09-25T23:52:57Z,2019-09-25T23:52:57Z,"DisCo: Physics-Based Unsupervised Discovery of Coherent Structures in
  Spatiotemporal Systems","Extracting actionable insight from complex unlabeled scientific data is an
open challenge and key to unlocking data-driven discovery in science.
Complementary and alternative to supervised machine learning approaches,
unsupervised physics-based methods based on behavior-driven theories hold great
promise. Due to computational limitations, practical application on real-world
domain science problems has lagged far behind theoretical development. We
present our first step towards bridging this divide - DisCo - a
high-performance distributed workflow for the behavior-driven local causal
state theory. DisCo provides a scalable unsupervised physics-based
representation learning method that decomposes spatiotemporal systems into
their structurally relevant components, which are captured by the latent local
causal state variables. Complex spatiotemporal systems are generally highly
structured and organize around a lower-dimensional skeleton of coherent
structures, and in several firsts we demonstrate the efficacy of DisCo in
capturing such structures from observational and simulated scientific data. To
the best of our knowledge, DisCo is also the first application software
developed entirely in Python to scale to over 1000 machine nodes, providing
good performance along with ensuring domain scientists' productivity. We
developed scalable, performant methods optimized for Intel many-core processors
that will be upstreamed to open-source Python library packages. Our capstone
experiment, using newly developed DisCo workflow and libraries, performs
unsupervised spacetime segmentation analysis of CAM5.1 climate simulation data,
processing an unprecedented 89.5 TB in 6.6 minutes end-to-end using 1024 Intel
Haswell nodes on the Cori supercomputer obtaining 91% weak-scaling and 64%
strong-scaling efficiency.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.11799v4,2021-08-07T16:30:21Z,2019-09-25T22:28:47Z,"Manifold Oblique Random Forests: Towards Closing the Gap on
  Convolutional Deep Networks","Decision forests (Forests), in particular random forests and gradient
boosting trees, have demonstrated state-of-the-art accuracy compared to other
methods in many supervised learning scenarios. In particular, Forests dominate
other methods in tabular data, that is, when the feature space is unstructured,
so that the signal is invariant to a permutation of the feature indices.
However, in structured data lying on a manifold (such as images, text, and
speech) deep networks (Networks), specifically convolutional deep networks
(ConvNets), tend to outperform Forests. We conjecture that at least part of the
reason for this is that the input to Networks is not simply the feature
magnitudes, but also their indices. In contrast, naive Forest implementations
fail to explicitly consider feature indices. A recently proposed Forest
approach demonstrates that Forests, for each node, implicitly sample a random
matrix from some specific distribution. These Forests, like some classes of
Networks, learn by partitioning the feature space into convex polytopes
corresponding to linear functions. We build on that approach and show that one
can choose distributions in a manifold-aware fashion to incorporate feature
locality. We demonstrate the empirical performance on data whose features live
on three different manifolds: a torus, images, and time-series. Moreover, we
demonstrate its strength in multivariate simulated settings and also show
superiority in predicting surgical outcome in epilepsy patients and predicting
movement direction from raw stereotactic EEG data from non-motor brain regions.
In all simulations and real data, Manifold Oblique Random Forest (MORF)
algorithm outperforms approaches that ignore feature space structure and
challenges the performance of ConvNets. Moreover, MORF runs fast and maintains
interpretability and theoretical justification.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.10707v6,2020-07-05T03:19:06Z,2019-09-24T04:34:58Z,"Invariant Transform Experience Replay: Data Augmentation for Deep
  Reinforcement Learning","Deep Reinforcement Learning (RL) is a promising approach for adaptive robot
control, but its current application to robotics is currently hindered by high
sample requirements. To alleviate this issue, we propose to exploit the
symmetries present in robotic tasks. Intuitively, symmetries from observed
trajectories define transformations that leave the space of feasible RL
trajectories invariant and can be used to generate new feasible trajectories,
which could be used for training. Based on this data augmentation idea, we
formulate a general framework, called Invariant Transform Experience Replay
that we present with two techniques: (i) Kaleidoscope Experience Replay
exploits reflectional symmetries and (ii) Goal-augmented Experience Replay
which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI
Gym, our experimental results show significant increases in learning rates and
success rates. Particularly, we attain a 13, 3, and 5 times speedup in the
pushing, sliding, and pick-and-place tasks respectively in the multi-goal
setting. Performance gains are also observed in similar tasks with obstacles
and we successfully deployed a trained policy on a real Baxter robot. Our work
demonstrates that invariant transformations on RL trajectories are a promising
methodology to speed up learning in deep RL.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.08703v1,2019-09-18T20:57:35Z,2019-09-18T20:57:35Z,"Deep Complex Networks for Protocol-Agnostic Radio Frequency Device
  Fingerprinting in the Wild","Researchers have demonstrated various techniques for fingerprinting and
identifying devices. Previous approaches have identified devices from their
network traffic or transmitted signals while relying on software or operating
system specific artifacts (e.g., predictability of protocol header fields) or
characteristics of the underlying protocol (e.g.,frequency offset). As these
constraints can be a hindrance in real-world settings, we introduce a
practical, generalizable approach that offers significant operational value for
a variety of scenarios, including as an additional factor of authentication for
preventing impersonation attacks. Our goal is to identify artifacts in
transmitted signals that are caused by a device's unique hardware
""imperfections"" without any knowledge about the nature of the signal. We
develop RF-DCN, a novel Deep Complex-valued Neural Network (DCN) that operates
on raw RF signals and is completely agnostic of the underlying applications and
protocols. We present two DCN variations: (i) Convolutional DCN (CDCN) for
modeling full signals, and (ii) Recurrent DCN (RDCN) for modeling time series.
Our system handles raw I/Q data from open air captures within a given spectrum
window, without knowledge of the modulation scheme or even the carrier
frequencies. While our experiments demonstrate the effectiveness of our system,
especially under challenging conditions where other neural network
architectures break down, we identify additional challenges in signal-based
fingerprinting and provide guidelines for future explorations. Our work lays
the foundation for more research within this vast and challenging space by
establishing fundamental directions for using raw RF I/Q data in novel
complex-valued networks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.08030v2,2020-04-01T14:57:13Z,2019-09-17T18:59:15Z,Auto-tuning of double dot devices in situ with machine learning,"The current practice of manually tuning quantum dots (QDs) for qubit
operation is a relatively time-consuming procedure that is inherently
impractical for scaling up and applications. In this work, we report on the
{\it in situ} implementation of a recently proposed autotuning protocol that
combines machine learning (ML) with an optimization routine to navigate the
parameter space. In particular, we show that a ML algorithm trained using
exclusively simulated data to quantitatively classify the state of a double-QD
device can be used to replace human heuristics in the tuning of gate voltages
in real devices. We demonstrate active feedback of a functional double-dot
device operated at millikelvin temperatures and discuss success rates as a
function of the initial conditions and the device performance. Modifications to
the training network, fitness function, and optimizer are discussed as a path
toward further improvement in the success rate when starting both near and far
detuned from the target double-dot range.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.12906v1,2019-09-16T11:59:40Z,2019-09-16T11:59:40Z,Meta Reinforcement Learning for Sim-to-real Domain Adaptation,"Modern reinforcement learning methods suffer from low sample efficiency and
unsafe exploration, making it infeasible to train robotic policies entirely on
real hardware. In this work, we propose to address the problem of sim-to-real
domain transfer by using meta learning to train a policy that can adapt to a
variety of dynamic conditions, and using a task-specific trajectory generation
model to provide an action space that facilitates quick exploration. We
evaluate the method by performing domain adaptation in simulation and analyzing
the structure of the latent space during adaptation. We then deploy this policy
on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a
hockey puck to a target. Our method shows more consistent and stable domain
adaptation than the baseline, resulting in better overall performance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.06993v2,2020-03-08T13:22:41Z,2019-09-16T05:23:14Z,"Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal
  Representations","Machines are a long way from robustly solving open-world perception-control
tasks, such as first-person view (FPV) aerial navigation. While recent advances
in end-to-end Machine Learning, especially Imitation and Reinforcement Learning
appear promising, they are constrained by the need of large amounts of
difficult-to-collect labeled real-world data. Simulated data, on the other
hand, is easy to generate, but generally does not render safe behaviors in
diverse real-life scenarios. In this work we propose a novel method for
learning robust visuomotor policies for real-world deployment which can be
trained purely with simulated data. We develop rich state representations that
combine supervised and unsupervised environment data. Our approach takes a
cross-modal perspective, where separate modalities correspond to the raw camera
data and the system states relevant to the task, such as the relative pose of
gates to the drone in the case of drone racing. We feed both data modalities
into a novel factored architecture, which learns a joint low-dimensional
embedding via Variational Auto Encoders. This compact representation is then
fed into a control policy, which we trained using imitation learning with
expert trajectories in a simulator. We analyze the rich latent spaces learned
with our proposed representations, and show that the use of our cross-modal
architecture significantly improves control policy performance as compared to
end-to-end learning or purely unsupervised feature extractors. We also present
real-world results for drone navigation through gates in different track
configurations and environmental conditions. Our proposed method, which runs
fully onboard, can successfully generalize the learned representations and
policies across simulation and reality, significantly outperforming baseline
approaches.
  Supplementary video: https://youtu.be/VKc3A5HlUU8",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.06769v1,2019-09-15T09:42:33Z,2019-09-15T09:42:33Z,VILD: Variational Imitation Learning with Diverse-quality Demonstrations,"The goal of imitation learning (IL) is to learn a good policy from
high-quality demonstrations. However, the quality of demonstrations in reality
can be diverse, since it is easier and cheaper to collect demonstrations from a
mix of experts and amateurs. IL in such situations can be challenging,
especially when the level of demonstrators' expertise is unknown. We propose a
new IL method called \underline{v}ariational \underline{i}mitation
\underline{l}earning with \underline{d}iverse-quality demonstrations (VILD),
where we explicitly model the level of demonstrators' expertise with a
probabilistic graphical model and estimate it along with a reward function. We
show that a naive approach to estimation is not suitable to large state and
action spaces, and fix its issues by using a variational approach which can be
easily implemented using existing reinforcement learning methods. Experiments
on continuous-control benchmarks demonstrate that VILD outperforms
state-of-the-art methods. Our work enables scalable and data-efficient IL under
more realistic settings than before.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.05189v3,2020-08-20T14:35:49Z,2019-09-11T16:40:03Z,ORES: Lowering Barriers with Participatory Machine Learning in Wikipedia,"Algorithmic systems---from rule-based bots to machine learning
classifiers---have a long history of supporting the essential work of content
moderation and other curation work in peer production projects. From
counter-vandalism to task routing, basic machine prediction has allowed open
knowledge projects like Wikipedia to scale to the largest encyclopedia in the
world, while maintaining quality and consistency. However, conversations about
how quality control should work and what role algorithms should play have
generally been led by the expert engineers who have the skills and resources to
develop and modify these complex algorithmic systems. In this paper, we
describe ORES: an algorithmic scoring service that supports real-time scoring
of wiki edits using multiple independent classifiers trained on different
datasets. ORES decouples several activities that have typically all been
performed by engineers: choosing or curating training data, building models to
serve predictions, auditing predictions, and developing interfaces or automated
agents that act on those predictions. This meta-algorithmic system was designed
to open up socio-technical conversations about algorithms in Wikipedia to a
broader set of participants. In this paper, we discuss the theoretical
mechanisms of social change ORES enables and detail case studies in
participatory machine learning around ORES from the 5 years since its
deployment.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1909.02583v2,2019-11-19T03:36:57Z,2019-09-05T18:04:04Z,"Spatiotemporally Constrained Action Space Attacks on Deep Reinforcement
  Learning Agents","Robustness of Deep Reinforcement Learning (DRL) algorithms towards
adversarial attacks in real world applications such as those deployed in
cyber-physical systems (CPS) are of increasing concern. Numerous studies have
investigated the mechanisms of attacks on the RL agent's state space.
Nonetheless, attacks on the RL agent's action space (AS) (corresponding to
actuators in engineering systems) are equally perverse; such attacks are
relatively less studied in the ML literature. In this work, we first frame the
problem as an optimization problem of minimizing the cumulative reward of an RL
agent with decoupled constraints as the budget of attack. We propose a
white-box Myopic Action Space (MAS) attack algorithm that distributes the
attacks across the action space dimensions. Next, we reformulate the
optimization problem above with the same objective function, but with a
temporally coupled constraint on the attack budget to take into account the
approximated dynamics of the agent. This leads to the white-box Look-ahead
Action Space (LAS) attack algorithm that distributes the attacks across the
action and temporal dimensions. Our results shows that using the same amount of
resources, the LAS attack deteriorates the agent's performance significantly
more than the MAS attack. This reveals the possibility that with limited
resource, an adversary can utilize the agent's dynamics to malevolently craft
attacks that causes the agent to fail. Additionally, we leverage these attack
strategies as a possible tool to gain insights on the potential vulnerabilities
of DRL agents.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.10407v2,2019-09-02T10:43:50Z,2019-08-27T18:50:50Z,"An Energy Approach to the Solution of Partial Differential Equations in
  Computational Mechanics via Machine Learning: Concepts, Implementation and
  Applications","Partial Differential Equations (PDE) are fundamental to model different
phenomena in science and engineering mathematically. Solving them is a crucial
step towards a precise knowledge of the behaviour of natural and engineered
systems. In general, in order to solve PDEs that represent real systems to an
acceptable degree, analytical methods are usually not enough. One has to resort
to discretization methods. For engineering problems, probably the best known
option is the finite element method (FEM). However, powerful alternatives such
as mesh-free methods and Isogeometric Analysis (IGA) are also available. The
fundamental idea is to approximate the solution of the PDE by means of
functions specifically built to have some desirable properties. In this
contribution, we explore Deep Neural Networks (DNNs) as an option for
approximation. They have shown impressive results in areas such as visual
recognition. DNNs are regarded here as function approximation machines. There
is great flexibility to define their structure and important advances in the
architecture and the efficiency of the algorithms to implement them make DNNs a
very interesting alternative to approximate the solution of a PDE. We
concentrate in applications that have an interest for Computational Mechanics.
Most contributions that have decided to explore this possibility have adopted a
collocation strategy. In this contribution, we concentrate in mechanical
problems and analyze the energetic format of the PDE. The energy of a
mechanical system seems to be the natural loss function for a machine learning
method to approach a mechanical problem. As proofs of concept, we deal with
several problems and explore the capabilities of the method for applications in
engineering.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.10398v1,2019-08-27T18:30:49Z,2019-08-27T18:30:49Z,"A Data-Efficient Deep Learning Approach for Deployable Multimodal Social
  Robots","The deep supervised and reinforcement learning paradigms (among others) have
the potential to endow interactive multimodal social robots with the ability of
acquiring skills autonomously. But it is still not very clear yet how they can
be best deployed in real world applications. As a step in this direction, we
propose a deep learning-based approach for efficiently training a humanoid
robot to play multimodal games---and use the game of `Noughts & Crosses' with
two variants as a case study. Its minimum requirements for learning to perceive
and interact are based on a few hundred example images, a few example
multimodal dialogues and physical demonstrations of robot manipulation, and
automatic simulations. In addition, we propose novel algorithms for robust
visual game tracking and for competitive policy learning with high winning
rates, which substantially outperform DQN-based baselines. While an automatic
evaluation shows evidence that the proposed approach can be easily extended to
new games with competitive robot behaviours, a human evaluation with 130 humans
playing with the Pepper robot confirms that highly accurate visual perception
is required for successful game play.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.05376v1,2019-08-15T00:06:23Z,2019-08-15T00:06:23Z,"Maximum Relevance and Minimum Redundancy Feature Selection Methods for a
  Marketing Machine Learning Platform","In machine learning applications for online product offerings and marketing
strategies, there are often hundreds or thousands of features available to
build such models. Feature selection is one essential method in such
applications for multiple objectives: improving the prediction accuracy by
eliminating irrelevant features, accelerating the model training and prediction
speed, reducing the monitoring and maintenance workload for feature data
pipeline, and providing better model interpretation and diagnosis capability.
However, selecting an optimal feature subset from a large feature space is
considered as an NP-complete problem. The mRMR (Minimum Redundancy and Maximum
Relevance) feature selection framework solves this problem by selecting the
relevant features while controlling for the redundancy within the selected
features. This paper describes the approach to extend, evaluate, and implement
the mRMR feature selection methods for classification problem in a marketing
machine learning platform at Uber that automates creation and deployment of
targeting and personalization models at scale. This study first extends the
existing mRMR methods by introducing a non-linear feature redundancy measure
and a model-based feature relevance measure. Then an extensive empirical
evaluation is performed for eight different feature selection methods, using
one synthetic dataset and three real-world marketing datasets at Uber to cover
different use cases. Based on the empirical results, the selected mRMR method
is implemented in production for the marketing machine learning platform. A
description of the production implementation is provided and an online
experiment deployed through the platform is discussed.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.04909v1,2019-08-14T01:31:45Z,2019-08-14T01:31:45Z,Constrained Multi-Objective Optimization for Automated Machine Learning,"Automated machine learning has gained a lot of attention recently. Building
and selecting the right machine learning models is often a multi-objective
optimization problem. General purpose machine learning software that
simultaneously supports multiple objectives and constraints is scant, though
the potential benefits are great. In this work, we present a framework called
Autotune that effectively handles multiple objectives and constraints that
arise in machine learning problems. Autotune is built on a suite of
derivative-free optimization methods, and utilizes multi-level parallelism in a
distributed computing environment for automatically training, scoring, and
selecting good models. Incorporation of multiple objectives and constraints in
the model exploration and selection process provides the flexibility needed to
satisfy trade-offs necessary in practical machine learning applications.
Experimental results from standard multi-objective optimization benchmark
problems show that Autotune is very efficient in capturing Pareto fronts. These
benchmark results also show how adding constraints can guide the search to more
promising regions of the solution space, ultimately producing more desirable
Pareto fronts. Results from two real-world case studies demonstrate the
effectiveness of the constrained multi-objective optimization capability
offered by Autotune.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.08998v2,2019-10-23T14:39:47Z,2019-08-13T10:15:39Z,AIBench: An Industry Standard Internet Service AI Benchmark Suite,"Today's Internet Services are undergoing fundamental changes and shifting to
an intelligent computing era where AI is widely employed to augment services.
In this context, many innovative AI algorithms, systems, and architectures are
proposed, and thus the importance of benchmarking and evaluating them rises.
However, modern Internet services adopt a microservice-based architecture and
consist of various modules. The diversity of these modules and complexity of
execution paths, the massive scale and complex hierarchy of datacenter
infrastructure, the confidential issues of data sets and workloads pose great
challenges to benchmarking. In this paper, we present the first
industry-standard Internet service AI benchmark suite---AIBench with seventeen
industry partners, including several top Internet service providers. AIBench
provides a highly extensible, configurable, and flexible benchmark framework
that contains loosely coupled modules. We identify sixteen prominent AI problem
domains like learning to rank, each of which forms an AI component benchmark,
from three most important Internet service domains: search engine, social
network, and e-commerce, which is by far the most comprehensive AI benchmarking
effort. On the basis of the AIBench framework, abstracting the real-world data
sets and workloads from one of the top e-commerce providers, we design and
implement the first end-to-end Internet service AI benchmark, which contains
the primary modules in the critical paths of an industry scale application and
is scalable to deploy on different cluster scales. The specifications, source
code, and performance numbers are publicly available from the benchmark council
web site http://www.benchcouncil.org/AIBench/index.html.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.04355v4,2020-07-02T13:47:36Z,2019-08-12T19:33:58Z,Adversarial Neural Pruning with Latent Vulnerability Suppression,"Despite the remarkable performance of deep neural networks on various
computer vision tasks, they are known to be susceptible to adversarial
perturbations, which makes it challenging to deploy them in real-world
safety-critical applications. In this paper, we conjecture that the leading
cause of adversarial vulnerability is the distortion in the latent feature
space, and provide methods to suppress them effectively. Explicitly, we define
\emph{vulnerability} for each latent feature and then propose a new loss for
adversarial learning, \emph{Vulnerability Suppression (VS)} loss, that aims to
minimize the feature-level vulnerability during training. We further propose a
Bayesian framework to prune features with high vulnerability to reduce both
vulnerability and loss on adversarial samples. We validate our
\emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)}
method on multiple benchmark datasets, on which it not only obtains
state-of-the-art adversarial robustness but also improves the performance on
clean examples, using only a fraction of the parameters used by the full
network. Further qualitative analysis suggests that the improvements come from
the suppression of feature-level vulnerability.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.04172v2,2019-08-29T19:16:10Z,2019-08-12T14:30:13Z,"nGraph-HE2: A High-Throughput Framework for Neural Network Inference on
  Encrypted Data","In previous work, Boemer et al. introduced nGraph-HE, an extension to the
Intel nGraph deep learning (DL) compiler, that enables data scientists to
deploy models with popular frameworks such as TensorFlow and PyTorch with
minimal code changes. However, the class of supported models was limited to
relatively shallow networks with polynomial activations. Here, we introduce
nGraph-HE2, which extends nGraph-HE to enable privacy-preserving inference on
standard, pre-trained models using their native activation functions and number
fields (typically real numbers). The proposed framework leverages the CKKS
scheme, whose support for real numbers is friendly to data science, and a
client-aided model using a two-party approach to compute activation functions.
  We first present CKKS-specific optimizations, enabling a 3x-88x runtime
speedup for scalar encoding, and doubling the throughput through a novel use of
CKKS plaintext packing into complex numbers. Second, we optimize
ciphertext-plaintext addition and multiplication, yielding 2.6x-4.2x runtime
speedup. Third, we exploit two graph-level optimizations: lazy rescaling and
depth-aware encoding, which allow us to significantly improve performance.
  Together, these optimizations enable state-of-the-art throughput of 1,998
images/s on the CryptoNets network. Using the client-aided model, we also
present homomorphic evaluation of (to our knowledge) the largest network to
date, namely, pre-trained MobileNetV2 models on the ImageNet dataset, with
60.4\percent/82.7\percent\ top-1/top-5 accuracy and an amortized runtime of 381
ms/image.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.02947v1,2019-08-08T06:45:05Z,2019-08-08T06:45:05Z,Graph Node Embeddings using Domain-Aware Biased Random Walks,"The recent proliferation of publicly available graph-structured data has
sparked an interest in machine learning algorithms for graph data. Since most
traditional machine learning algorithms assume data to be tabular, embedding
algorithms for mapping graph data to real-valued vector spaces has become an
active area of research. Existing graph embedding approaches are based purely
on structural information and ignore any semantic information from the
underlying domain. In this paper, we demonstrate that semantic information can
play a useful role in computing graph embeddings. Specifically, we present a
framework for devising embedding strategies aware of domain-specific
interpretations of graph nodes and edges, and use knowledge of downstream
machine learning tasks to identify relevant graph substructures. Using two
real-life domains, we show that our framework yields embeddings that are simple
to implement and yet achieve equal or greater accuracy in machine learning
tasks compared to domain independent approaches.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.04387v3,2019-09-10T17:46:09Z,2019-08-05T02:59:18Z,"Mass Estimation from Images using Deep Neural Network and Sparse Ground
  Truth","Supervised learning is the workhorse for regression and classification tasks,
but the standard approach presumes ground truth for every measurement. In real
world applications, limitations due to expense or general in-feasibility due to
the specific application are common. In the context of agriculture
applications, yield monitoring is one such example where simple-physics based
measurements such as volume or force-impact have been used to quantify mass
flow, which incur error due to sensor calibration. By utilizing semi-supervised
deep learning with gradient aggregation and a sequence of images, in this work
we can accurately estimate a physical quantity (mass) with complex data
structures and sparse ground truth. Using a vision system capturing images of a
sugarcane elevator and running bamboo under controlled testing as a surrogate
material to harvesting sugarcane, mass is accurately predicted from images by
training a DNN using only final load weights. The DNN succeeds in capturing the
complex density physics of random stacking of slender rods internally as part
of the mass prediction model, and surpasses older volumetric-based methods for
mass prediction. Furthermore, by incorporating knowledge about the system
physics through the DNN architecture and penalty terms, improvements in
prediction accuracy and stability, as well as faster learning are obtained. It
is shown that the classic nonlinear regression optimization can be reformulated
with an aggregation term with some independence assumptions to achieve this
feat. Since the number of images for any given run are too large to fit on
typical GPU vRAM, an implementation is shown that compensates for the limited
memory but still achieve fast training times. The same approach presented
herein could be applied to other applications like yield monitoring on grain
combines or other harvesters using vision or other instrumentation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.02150v3,2019-10-22T02:23:42Z,2019-08-04T05:19:43Z,Industrial Artificial Intelligence,"Artificial Intelligence (AI) is a cognitive science to enables human to
explore many intelligent ways to model our sensing and reasoning processes.
Industrial AI is a systematic discipline to enable engineers to systematically
develop and deploy AI algorithms with repeating and consistent successes. In
this paper, the key enablers for this transformative technology along with
their significant advantages are discussed. In addition, this research explains
Lighthouse Factories as an emerging status applying to the top manufacturers
that have implemented Industrial AI in their manufacturing ecosystem and gained
significant financial benefits. It is believed that this research will work as
a guideline and roadmap for researchers and industries towards the real-world
implementation of Industrial AI.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1908.00754v1,2019-08-02T08:31:36Z,2019-08-02T08:31:36Z,"A Visual Technique to Analyze Flow of Information in a Machine Learning
  System","Machine learning (ML) algorithms and machine learning based software systems
implicitly or explicitly involve complex flow of information between various
entities such as training data, feature space, validation set and results.
Understanding the statistical distribution of such information and how they
flow from one entity to another influence the operation and correctness of such
systems, especially in large-scale applications that perform classification or
prediction in real time. In this paper, we propose a visual approach to
understand and analyze flow of information during model training and serving
phases. We build the visualizations using a technique called Sankey Diagram -
conventionally used to understand data flow among sets - to address various use
cases of in a machine learning system. We demonstrate how the proposed
technique, tweaked and twisted to suit a classification problem, can play a
critical role in better understanding of the training data, the features, and
the classifier performance. We also discuss how this technique enables
diagnostic analysis of model predictions and comparative analysis of
predictions from multiple classifiers. The proposed concept is illustrated with
the example of categorization of millions of products in the e-commerce domain
- a multi-class hierarchical classification problem.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.13525v1,2019-07-31T14:28:55Z,2019-07-31T14:28:55Z,"Local Interpretation Methods to Machine Learning Using the Domain of the
  Feature Space","As machine learning becomes an important part of many real world applications
affecting human lives, new requirements, besides high predictive accuracy,
become important. One important requirement is transparency, which has been
associated with model interpretability. Many machine learning algorithms induce
models difficult to interpret, named black box. Moreover, people have
difficulty to trust models that cannot be explained. In particular for machine
learning, many groups are investigating new methods able to explain black box
models. These methods usually look inside the black models to explain their
inner work. By doing so, they allow the interpretation of the decision making
process used by black box models. Among the recently proposed model
interpretation methods, there is a group, named local estimators, which are
designed to explain how the label of particular instance is predicted. For
such, they induce interpretable models on the neighborhood of the instance to
be explained. Local estimators have been successfully used to explain specific
predictions. Although they provide some degree of model interpretability, it is
still not clear what is the best way to implement and apply them. Open
questions include: how to best define the neighborhood of an instance? How to
control the trade-off between the accuracy of the interpretation method and its
interpretability? How to make the obtained solution robust to small variations
on the instance to be explained? To answer to these questions, we propose and
investigate two strategies: (i) using data instance properties to provide
improved explanations, and (ii) making sure that the neighborhood of an
instance is properly defined by taking the geometry of the domain of the
feature space into account. We evaluate these strategies in a regression task
and present experimental results that show that they can improve local
explanations.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.12817v2,2019-07-31T07:14:32Z,2019-07-30T10:00:52Z,"Increasing Scalability of Process Mining using Event Dataframes: How
  Data Structure Matters","Process Mining is a branch of Data Science that aims to extract
process-related information from event data contained in information systems,
that is steadily increasing in amount. Many algorithms, and a general-purpose
open source framework (ProM 6), have been developed in the last years for
process discovery, conformance checking, machine learning on event data.
However, in very few cases scalability has been a target, prioritizing the
quality of the output over the execution speed and the optimization of
resources. This is making progressively more difficult to apply process mining
with mainstream workstations on real-life event data with any open source
process mining framework. Hence, exploring more scalable storage techniques,
in-memory data structures, more performant algorithms is a strictly incumbent
need. In this paper, we propose the usage of mainstream columnar storages and
dataframes to increase the scalability of process mining. These can replace the
classic event log structures in most tasks, but require completely different
implementations with regards to mainstream process mining algorithms.
Dataframes will be defined, some algorithms on such structures will be
presented and their complexity will be calculated.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.10323v1,2019-07-24T09:27:11Z,2019-07-24T09:27:11Z,Fairness in Reinforcement Learning,"Decision support systems (e.g., for ecological conservation) and autonomous
systems (e.g., adaptive controllers in smart cities) start to be deployed in
real applications. Although their operations often impact many users or
stakeholders, no fairness consideration is generally taken into account in
their design, which could lead to completely unfair outcomes for some users or
stakeholders. To tackle this issue, we advocate for the use of social welfare
functions that encode fairness and present this general novel problem in the
context of (deep) reinforcement learning, although it could possibly be
extended to other machine learning tasks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.09209v1,2019-07-22T10:04:22Z,2019-07-22T10:04:22Z,"Automatic Calibration of Artificial Neural Networks for Zebrafish
  Collective Behaviours using a Quality Diversity Algorithm","During the last two decades, various models have been proposed for fish
collective motion. These models are mainly developed to decipher the biological
mechanisms of social interaction between animals. They consider very simple
homogeneous unbounded environments and it is not clear that they can simulate
accurately the collective trajectories. Moreover when the models are more
accurate, the question of their scalability to either larger groups or more
elaborate environments remains open. This study deals with learning how to
simulate realistic collective motion of collective of zebrafish, using
real-world tracking data. The objective is to devise an agent-based model that
can be implemented on an artificial robotic fish that can blend into a
collective of real fish. We present a novel approach that uses Quality
Diversity algorithms, a class of algorithms that emphasise exploration over
pure optimisation. In particular, we use CVT-MAP-Elites, a variant of the
state-of-the-art MAP-Elites algorithm for high dimensional search space.
Results show that Quality Diversity algorithms not only outperform classic
evolutionary reinforcement learning methods at the macroscopic level (i.e.
group behaviour), but are also able to generate more realistic biomimetic
behaviours at the microscopic level (i.e. individual behaviour).",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.07958v1,2019-07-18T09:58:27Z,2019-07-18T09:58:27Z,Transfer Learning Across Simulated Robots With Different Sensors,"For a robot to learn a good policy, it often requires expensive equipment
(such as sophisticated sensors) and a prepared training environment conducive
to learning. However, it is seldom possible to perfectly equip robots for
economic reasons, nor to guarantee ideal learning conditions, when deployed in
real-life environments. A solution would be to prepare the robot in the lab
environment, when all necessary material is available to learn a good policy.
After training in the lab, the robot should be able to get by without the
expensive equipment that used to be available to it, and yet still be
guaranteed to perform well on the field. The transition between the lab
(source) and the real-world environment (target) is related to transfer
learning, where the state-space between the source and target tasks differ. We
tackle a simulated task with continuous states and discrete actions presenting
this challenge, using Bootstrapped Dual Policy Iteration, a model-free
actor-critic reinforcement learning algorithm, and Policy Shaping.
Specifically, we train a BDPI agent, embodied by a virtual robot performing a
task in the V-Rep simulator, sensing its environment through several proximity
sensors. The resulting policy is then used by a second agent learning the same
task in the same environment, but with camera images as input. The goal is to
obtain a policy able to perform the task relying on merely camera images.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.06011v3,2020-09-14T19:04:01Z,2019-07-13T05:21:05Z,"Extracting Interpretable Physical Parameters from Spatiotemporal Systems
  using Unsupervised Learning","Experimental data is often affected by uncontrolled variables that make
analysis and interpretation difficult. For spatiotemporal systems, this problem
is further exacerbated by their intricate dynamics. Modern machine learning
methods are particularly well-suited for analyzing and modeling complex
datasets, but to be effective in science, the result needs to be interpretable.
We demonstrate an unsupervised learning technique for extracting interpretable
physical parameters from noisy spatiotemporal data and for building a
transferable model of the system. In particular, we implement a
physics-informed architecture based on variational autoencoders that is
designed for analyzing systems governed by partial differential equations
(PDEs). The architecture is trained end-to-end and extracts latent parameters
that parameterize the dynamics of a learned predictive model for the system. To
test our method, we train our model on simulated data from a variety of PDEs
with varying dynamical parameters that act as uncontrolled variables. Numerical
experiments show that our method can accurately identify relevant parameters
and extract them from raw and even noisy spatiotemporal data (tested with
roughly 10% added noise). These extracted parameters correlate well (linearly
with $R^2 > 0.95$) with the ground truth physical parameters used to generate
the datasets. We then apply this method to nonlinear fiber propagation data,
generated by an ab-initio simulation, to demonstrate its capabilities on a more
realistic dataset. Our method for discovering interpretable latent parameters
in spatiotemporal systems will allow us to better analyze and understand
real-world phenomena and datasets, which often have unknown and uncontrolled
variables that alter the system dynamics and cause varying behaviors that are
difficult to disentangle.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.02526v1,2019-07-03T21:25:21Z,2019-07-03T21:25:21Z,"Convolutional Neural Network-based Speech Enhancement for Cochlear
  Implant Recipients","Attempts to develop speech enhancement algorithms with improved speech
intelligibility for cochlear implant (CI) users have met with limited success.
To improve speech enhancement methods for CI users, we propose to perform
speech enhancement in a cochlear filter-bank feature space, a feature-set
specifically designed for CI users based on CI auditory stimuli. We leverage a
convolutional neural network (CNN) to extract both stationary and
non-stationary components of environmental acoustics and speech. We propose
three CNN architectures: (1) vanilla CNN that directly generates the enhanced
signal; (2) spectral-subtraction-style CNN (SS-CNN) that first predicts noise
and then generates the enhanced signal by subtracting noise from the noisy
signal; (3) Wiener-style CNN (Wiener-CNN) that generates an optimal mask for
suppressing noise. An important problem of the proposed networks is that they
introduce considerable delays, which limits their real-time application for CI
users. To address this, this study also considers causal variations of these
networks. Our experiments show that the proposed networks (both causal and
non-causal forms) achieve significant improvement over existing baseline
systems. We also found that causal Wiener-CNN outperforms other networks, and
leads to the best overall envelope coefficient measure (ECM). The proposed
algorithms represent a viable option for implementation on the CCi-MOBILE
research platform as a pre-processor for CI users in naturalistic environments.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.00498v4,2020-07-08T22:48:11Z,2019-06-30T23:46:30Z,"Proof of Witness Presence: Blockchain Consensus for Augmented Democracy
  in Smart Cities","Smart Cities evolve into complex and pervasive urban environments with a
citizens' mandate to meet sustainable development goals. Repositioning
democratic values of citizens' choices in these complex ecosystems has turned
out to be imperative in an era of social media filter bubbles, fake news and
opportunities for manipulating electoral results with such means. This paper
introduces a new paradigm of augmented democracy that promises actively
engaging citizens in a more informed decision-making augmented into public
urban space. The proposed concept is inspired by a digital revive of the
Ancient Agora of Athens, an arena of public discourse, a Polis where citizens
assemble to actively deliberate and collectively decide about public matters.
The core contribution of the proposed paradigm is the concept of proving
witness presence: making decision-making subject of providing secure evidence
and testifying for choices made in the physical space. This paper shows how the
challenge of proving witness presence can be tackled with blockchain consensus
to empower citizens' trust and overcome security vulnerabilities of GPS
localization. Moreover, a novel platform for collective decision-making and
crowd-sensing in urban space is introduced: Smart Agora. It is shown how
real-time collective measurements over citizens' choices can be made in a fully
decentralized and privacy-preserving way. Witness presence is tested by
deploying a decentralized system for crowd-sensing the sustainable use of
transport means. Furthermore, witness presence of cycling risk is validated
using official accident data from public authorities, which are compared
against wisdom of the crowd. The paramount role of dynamic consensus,
self-governance and ethically aligned artificial intelligence in the augmented
democracy paradigm is outlined.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.00456v2,2019-07-08T17:21:46Z,2019-06-30T20:53:19Z,"Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human
  Preferences in Dialog","Most deep reinforcement learning (RL) systems are not able to learn
effectively from off-policy data, especially if they cannot explore online in
the environment. These are critical shortcomings for applying RL to real-world
problems where collecting data is expensive, and models must be tested offline
before being deployed to interact with the environment -- e.g. systems that
learn from human interaction. Thus, we develop a novel class of off-policy
batch RL algorithms, which are able to effectively learn offline, without
exploring, from a fixed batch of human interaction data. We leverage models
pre-trained on data as a strong prior, and use KL-control to penalize
divergence from this prior during RL training. We also use dropout-based
uncertainty estimates to lower bound the target Q-values as a more efficient
alternative to Double Q-Learning. The algorithms are tested on the problem of
open-domain dialog generation -- a challenging reinforcement learning problem
with a 20,000-dimensional action space. Using our Way Off-Policy algorithm, we
can extract multiple different reward functions post-hoc from collected human
interaction data, and learn effectively from all of these. We test the
real-world generalization of these systems by deploying them live to converse
with humans in an open-domain setting, and demonstrate that our algorithm
achieves significant improvements over prior methods in off-policy batch RL.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.01522v1,2019-06-28T22:22:41Z,2019-06-28T22:22:41Z,Tucker Tensor Decomposition on FPGA,"Tensor computation has emerged as a powerful mathematical tool for solving
high-dimensional and/or extreme-scale problems in science and engineering. The
last decade has witnessed tremendous advancement of tensor computation and its
applications in machine learning and big data. However, its hardware
optimization on resource-constrained devices remains an (almost) unexplored
field. This paper presents an hardware accelerator for a classical tensor
computation framework, Tucker decomposition. We study three modules of this
architecture: tensor-times-matrix (TTM), matrix singular value decomposition
(SVD), and tensor permutation, and implemented them on Xilinx FPGA for
prototyping. In order to further reduce the computing time, a warm-start
algorithm for the Jacobi iterations in SVD is proposed. A fixed-point simulator
is used to evaluate the performance of our design. Some synthetic data sets and
a real MRI data set are used to validate the design and evaluate its
performance. We compare our work with state-of-the-art software toolboxes
running on both CPU and GPU, and our work shows 2.16 - 30.2x speedup on the
cardiac MRI data set.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1906.10958v3,2019-09-05T02:16:56Z,2019-06-26T10:33:24Z,Signed Graph Attention Networks,"Graph or network data is ubiquitous in the real world, including social
networks, information networks, traffic networks, biological networks and
various technical networks. The non-Euclidean nature of graph data poses the
challenge for modeling and analyzing graph data. Recently, Graph Neural Network
(GNNs) are proposed as a general and powerful framework to handle tasks on
graph data, e.g., node embedding, link prediction and node classification. As a
representative implementation of GNNs, Graph Attention Networks (GATs) are
successfully applied in a variety of tasks on real datasets. However, GAT is
designed to networks with only positive links and fails to handle signed
networks which contain both positive and negative links. In this paper, we
propose Signed Graph Attention Networks (SiGATs), generalizing GAT to signed
networks. SiGAT incorporates graph motifs into GAT to capture two well-known
theories in signed network research, i.e., balance theory and status theory. In
SiGAT, motifs offer us the flexible structural pattern to aggregate and
propagate messages on the signed network to generate node embeddings. We
evaluate the proposed SiGAT method by applying it to the signed link prediction
task. Experimental results on three real datasets demonstrate that SiGAT
outperforms feature-based method, network embedding method and state-of-the-art
GNN-based methods like signed graph convolutional network (SGCN).",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1906.10910v2,2019-07-01T09:03:34Z,2019-06-26T08:37:44Z,"Creating A Neural Pedagogical Agent by Jointly Learning to Review and
  Assess","Machine learning plays an increasing role in intelligent tutoring systems as
both the amount of data available and specialization among students grow.
Nowadays, these systems are frequently deployed on mobile applications. Users
on such mobile education platforms are dynamic, frequently being added,
accessing the application with varying levels of focus, and changing while
using the service. The education material itself, on the other hand, is often
static and is an exhaustible resource whose use in tasks such as problem
recommendation must be optimized. The ability to update user models with
respect to educational material in real-time is thus essential; however,
existing approaches require time-consuming re-training of user features
whenever new data is added. In this paper, we introduce a neural pedagogical
agent for real-time user modeling in the task of predicting user response
correctness, a central task for mobile education applications. Our model,
inspired by work in natural language processing on sequence modeling and
machine translation, updates user features in real-time via bidirectional
recurrent neural networks with an attention mechanism over embedded
question-response pairs. We experiment on the mobile education application
SantaTOEIC, which has 559k users, 66M response data points as well as a set of
10k study problems each expert-annotated with topic tags and gathered since
2016. Our model outperforms existing approaches over several metrics in
predicting user response correctness, notably out-performing other methods on
new users without large question-response histories. Additionally, our
attention mechanism and annotated tag set allow us to create an interpretable
education platform, with a smart review system that addresses the
aforementioned issue of varied user attention and problem exhaustion.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1907.00036v1,2019-06-26T01:16:22Z,2019-06-26T01:16:22Z,"Novel Suboptimal approaches for Hyperparameter Tuning of Deep Neural
  Network [under the shelf of Optical Communication]","Hyperparameter tuning is the main challenge of machine learning (ML)
algorithms. Grid search is a popular method in hyperparameter tuning of simple
ML algorithms; however, high computational complexity in complex ML algorithms
such as Deep Neural Networks (DNN) is the main barrier towards its practical
implementation. In this paper, two novel suboptimal grid search methods are
presented, which search the grid marginally and alternating. In order to
examine these methods, hyperparameter tuning is applied on two different DNN
based Optical Communication (OC) systems (Fiber OC, and Free Space Optical
(FSO) communication). The hyperparameter tuning of ML algorithms, despite its
importance is ignored in ML for OC investigations. In addition, this is the
first consideration of both FSO and Fiber OC systems in an ML for OC
investigation. Results indicate that despite greatly reducing computation load,
favorable performance could be achieved by the proposed methods. In addition,
it is shown that the alternating search method has better performance than
marginal grid search method. In sum, the proposed structures are
cost-effective, and appropriate for real-time applications.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1906.07391v3,2020-12-29T21:20:09Z,2019-06-18T05:51:57Z,"The Breakthrough Listen Search for Intelligent Life: Public Data,
  Formats, Reduction and Archiving","Breakthrough Listen is the most comprehensive and sensitive search for
extraterrestrial intelligence (SETI) to date, employing a collection of
international observational facilities including both radio and optical
telescopes. During the first three years of the Listen program, thousands of
targets have been observed with the Green Bank Telescope (GBT), Parkes
Telescope and Automated Planet Finder. At GBT and Parkes, observations have
been performed ranging from 700 MHz to 26 GHz, with raw data volumes averaging
over 1PB / day. A pseudo-real time software spectroscopy suite is used to
produce multi-resolution spectrograms amounting to approximately 400 GB hr^-1
GHz^-1 beam^-1. For certain targets, raw baseband voltage data is also
preserved. Observations with the Automated Planet Finder produce both
2-dimensional and 1-dimensional high resolution (R~10^5) echelle spectral data.
  Although the primary purpose of Listen data acquisition is for SETI, a range
of secondary science has also been performed with these data, including studies
of fast radio bursts. Other current and potential research topics include
spectral line studies, searches for certain kinds of dark matter, probes of
interstellar scattering, pulsar searches, radio transient searches and
investigations of stellar activity. Listen data are also being used in the
development of algorithms, including machine learning approaches to modulation
scheme classification and outlier detection, that have wide applicability not
just for astronomical research but for a broad range of science and
engineering.
  In this paper, we describe the hardware and software pipeline used for
collection, reduction, archival, and public dissemination of Listen data. We
describe the data formats and tools, and present Breakthrough Listen Data
Release 1.0 (BLDR 1.0), a defined set of publicly-available raw and reduced
data totalling 1 PB.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1906.04450v2,2019-08-14T17:52:37Z,2019-06-11T09:02:35Z,"Quantifying Intrinsic Uncertainty in Classification via Deep Dirichlet
  Mixture Networks","With the widespread success of deep neural networks in science and
technology, it is becoming increasingly important to quantify the uncertainty
of the predictions produced by deep learning. In this paper, we introduce a new
method that attaches an explicit uncertainty statement to the probabilities of
classification using deep neural networks. Precisely, we view that the
classification probabilities are sampled from an unknown distribution, and we
propose to learn this distribution through the Dirichlet mixture that is
flexible enough for approximating any continuous distribution on the simplex.
We then construct credible intervals from the learned distribution to assess
the uncertainty of the classification probabilities. Our approach is easy to
implement, computationally efficient, and can be coupled with any deep neural
network architecture. Our method leverages the crucial observation that, in
many classification applications such as medical diagnosis, more than one class
labels are available for each observational unit. We demonstrate the usefulness
of our approach through simulations and a real data example.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1906.04023v1,2019-06-10T14:35:07Z,2019-06-10T14:35:07Z,Project Thyia: A Forever Gameplayer,"The space of Artificial Intelligence entities is dominated by conversational
bots. Some of them fit in our pockets and we take them everywhere we go, or
allow them to be a part of human homes. Siri, Alexa, they are recognised as
present in our world. But a lot of games research is restricted to existing in
the separate realm of software. We enter different worlds when playing games,
but those worlds cease to exist once we quit. Similarly, AI game-players are
run once on a game (or maybe for longer periods of time, in the case of
learning algorithms which need some, still limited, period for training), and
they cease to exist once the game ends. But what if they didn't? What if there
existed artificial game-players that continuously played games, learned from
their experiences and kept getting better? What if they interacted with the
real world and us, humans: live-streaming games, chatting with viewers,
accepting suggestions for strategies or games to play, forming opinions on
popular game titles? In this paper, we introduce the vision behind a new
project called Thyia, which focuses around creating a present, continuous,
`always-on', interactive game-player.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1906.05658v1,2019-06-07T08:10:16Z,2019-06-07T08:10:16Z,EKT: Exercise-aware Knowledge Tracing for Student Performance Prediction,"For offering proactive services to students in intelligent education, one of
the fundamental tasks is predicting their performance (e.g., scores) on future
exercises, where it is necessary to track each student's knowledge acquisition
during her exercising activities. However, existing approaches can only exploit
the exercising records of students, and the problem of extracting rich
information existed in the exercise's materials (e.g., knowledge concepts,
exercise content) to achieve both precise predictions of student performance
and interpretable analysis of knowledge acquisition remains underexplored. In
this paper, we present a holistic study of student performance prediction. To
directly achieve the primary goal of prediction, we first propose a general
Exercise-Enhanced Recurrent Neural Network (EERNN) framework by exploring both
student's records and the exercise contents. In EERNN, we simply summarize each
student's state into an integrated vector and trace it with a recurrent neural
network, where we design a bidirectional LSTM to learn the encoding of each
exercise's content. For making predictions, we propose two implementations
under EERNN with different strategies, i.e., EERNNM with Markov property and
EERNNA with Attention mechanism. Then, to explicitly track student's knowledge
acquisition on multiple knowledge concepts, we extend EERNN to an explainable
Exercise-aware Knowledge Tracing (EKT) by incorporating the knowledge concept
effects, where the student's integrated state vector is extended to a knowledge
state matrix. In EKT, we further develop a memory network for quantifying how
much each exercise can affect the mastery of students on concepts during the
exercising process. Finally, we conduct extensive experiments on large-scale
real-world data. The results demonstrate the prediction effectiveness of two
frameworks as well as the superior interpretability of EKT.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1906.01974v3,2020-03-05T16:58:35Z,2019-06-03T22:43:00Z,"Willump: A Statistically-Aware End-to-end Optimizer for Machine Learning
  Inference","Systems for ML inference are widely deployed today, but they typically
optimize ML inference workloads using techniques designed for conventional data
serving workloads and miss critical opportunities to leverage the statistical
nature of ML. In this paper, we present Willump, an optimizer for ML inference
that introduces two statistically-motivated optimizations targeting ML
applications whose performance bottleneck is feature computation. First,
Willump automatically cascades feature computation for classification queries:
Willump classifies most data inputs using only high-value, low-cost features
selected through empirical observations of ML model performance, improving
query performance by up to 5x without statistically significant accuracy loss.
Second, Willump accurately approximates ML top-K queries, discarding
low-scoring inputs with an automatically constructed approximate model and then
ranking the remainder with a more powerful model, improving query performance
by up to 10x with minimal accuracy loss. Willump automatically tunes these
optimizations' parameters to maximize query performance while meeting an
accuracy target. Moreover, Willump complements these statistical optimizations
with compiler optimizations to automatically generate fast inference code for
ML applications. We show that Willump improves the end-to-end performance of
real-world ML inference pipelines curated from major data science competitions
by up to 16x without statistically significant loss of accuracy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.13118v1,2019-05-30T15:50:14Z,2019-05-30T15:50:14Z,"Standing on the Shoulders of Giants: AI-driven Calibration of
  Localisation Technologies","High accuracy localisation technologies exist but are prohibitively expensive
to deploy for large indoor spaces such as warehouses, factories, and
supermarkets to track assets and people. However, these technologies can be
used to lend their highly accurate localisation capabilities to low-cost,
commodity, and less-accurate technologies. In this paper, we bridge this link
by proposing a technology-agnostic calibration framework based on artificial
intelligence to assist such low-cost technologies through highly accurate
localisation systems. A single-layer neural network is used to calibrate less
accurate technology using more accurate one such as BLE using UWB and UWB using
a professional motion tracking system. On a real indoor testbed, we demonstrate
an increase in accuracy of approximately 70% for BLE and 50% for UWB. Not only
the proposed approach requires a very short measurement campaign, the low
complexity of the single-layer neural network also makes it ideal for
deployment on constrained devices typically for localisation purposes.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.10364v1,2019-05-24T09:19:42Z,2019-05-24T09:19:42Z,"Deep learning based high-resolution incoherent x-ray imaging with a
  single-pixel detector","X-ray ""ghost"" imaging has drawn great attention for its potential to lower
radiation dose in medical diagnosis. For practical implementation, however, the
efficiency and image quality have to be greatly improved. Here we demonstrate a
computational ghost imaging scheme where a bucket detector and specially
designed modulation masks are used, together with a new robust deep learning
algorithm in which a compressed set of Hadamard matrices is incorporated into a
multi-level wavelet convolutional neural network. By this means we have
obtained an image of a real object from only 18.75% of the Nyquist sampling
rate, using a portable tabletop incoherent x-ray source of ~37 {\mu}m diameter.
A high imaging resolution of ~10 {\mu}m is achieved, which represents a
concrete step towards the realization of a practical low cost x-ray ghost
imaging camera for applications in biomedicine, archeology, material science,
and so forth.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.09673v2,2019-05-29T16:35:23Z,2019-05-23T14:15:51Z,"Deep Q-Learning with Q-Matrix Transfer Learning for Novel Fire
  Evacuation Environment","We focus on the important problem of emergency evacuation, which clearly
could benefit from reinforcement learning that has been largely unaddressed.
Emergency evacuation is a complex task which is difficult to solve with
reinforcement learning, since an emergency situation is highly dynamic, with a
lot of changing variables and complex constraints that makes it difficult to
train on. In this paper, we propose the first fire evacuation environment to
train reinforcement learning agents for evacuation planning. The environment is
modelled as a graph capturing the building structure. It consists of realistic
features like fire spread, uncertainty and bottlenecks. We have implemented the
environment in the OpenAI gym format, to facilitate future research. We also
propose a new reinforcement learning approach that entails pretraining the
network weights of a DQN based agents to incorporate information on the
shortest path to the exit. We achieved this by using tabular Q-learning to
learn the shortest path on the building model's graph. This information is
transferred to the network by deliberately overfitting it on the Q-matrix.
Then, the pretrained DQN model is trained on the fire evacuation environment to
generate the optimal evacuation path under time varying conditions. We perform
comparisons of the proposed approach with state-of-the-art reinforcement
learning algorithms like PPO, VPG, SARSA, A2C and ACKTR. The results show that
our method is able to outperform state-of-the-art models by a huge margin
including the original DQN based models. Finally, we test our model on a large
and complex real building consisting of 91 rooms, with the possibility to move
to any other room, hence giving 8281 actions. We use an attention based
mechanism to deal with large action spaces. Our model achieves near optimal
performance on the real world emergency environment.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.07082v6,2021-06-26T12:14:13Z,2019-05-17T01:35:26Z,"The Audio Auditor: User-Level Membership Inference in Internet of Things
  Voice Services","With the rapid development of deep learning techniques, the popularity of
voice services implemented on various Internet of Things (IoT) devices is ever
increasing. In this paper, we examine user-level membership inference in the
problem space of voice services, by designing an audio auditor to verify
whether a specific user had unwillingly contributed audio used to train an
automatic speech recognition (ASR) model under strict black-box access. With
user representation of the input audio data and their corresponding translated
text, our trained auditor is effective in user-level audit. We also observe
that the auditor trained on specific data can be generalized well regardless of
the ASR model architecture. We validate the auditor on ASR models trained with
LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid
ASR system and the end-to-end ASR system. Finally, we conduct a real-world
trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding
80\%. We hope the methodology developed in this paper and findings can inform
privacy advocates to overhaul IoT privacy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.06809v1,2019-05-16T14:50:28Z,2019-05-16T14:50:28Z,Occupancy Estimation Using Low-Cost Wi-Fi Sniffers,"Real-time measurements on the occupancy status of indoor and outdoor spaces
can be exploited in many scenarios (HVAC and lighting system control, building
energy optimization, allocation and reservation of spaces, etc.). Traditional
systems for occupancy estimation rely on environmental sensors (CO2,
temperature, humidity) or video cameras. In this paper, we depart from such
traditional approaches and propose a novel occupancy estimation system which is
based on the capture of Wi-Fi management packets from users' devices. The
system, implemented on a low-cost ESP8266 microcontroller, leverages a
supervised learning model to adapt to different spaces and transmits occupancy
information through the MQTT protocol to a web-based dashboard. Experimental
results demonstrate the validity of the proposed solution in four different
indoor university spaces.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.04815v1,2019-05-13T00:20:31Z,2019-05-13T00:20:31Z,"Programmable Spectrometry -- Per-pixel Classification of Materials using
  Learned Spectral Filters","Many materials have distinct spectral profiles. This facilitates estimation
of the material composition of a scene at each pixel by first acquiring its
hyperspectral image, and subsequently filtering it using a bank of spectral
profiles. This process is inherently wasteful since only a set of linear
projections of the acquired measurements contribute to the classification task.
We propose a novel programmable camera that is capable of producing images of a
scene with an arbitrary spectral filter. We use this camera to optically
implement the spectral filtering of the scene's hyperspectral image with the
bank of spectral profiles needed to perform per-pixel material classification.
This provides gains both in terms of acquisition speed --- since only the
relevant measurements are acquired --- and in signal-to-noise ratio --- since
we invariably avoid narrowband filters that are light inefficient. Given
training data, we use a range of classical and modern techniques including SVMs
and neural networks to identify the bank of spectral profiles that facilitate
material classification. We verify the method in simulations on standard
datasets as well as real data using a lab prototype of the camera.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1905.03813v4,2019-10-15T06:11:03Z,2019-05-09T18:47:38Z,When Deep Learning Met Code Search,"There have been multiple recent proposals on using deep neural networks for
code search using natural language. Common across these proposals is the idea
of $\mathit{embedding}$ code and natural language queries, into real vectors
and then using vector distance to approximate semantic correlation between code
and the query. Multiple approaches exist for learning these embeddings,
including $\mathit{unsupervised}$ techniques, which rely only on a corpus of
code examples, and $\mathit{supervised}$ techniques, which use an
$\mathit{aligned}$ corpus of paired code and natural language descriptions. The
goal of this supervision is to produce embeddings that are more similar for a
query and the corresponding desired code snippet. Clearly, there are choices in
whether to use supervised techniques at all, and if one does, what sort of
network and training to use for supervision. This paper is the first to
evaluate these choices systematically. To this end, we assembled
implementations of state-of-the-art techniques to run on a common platform,
training and evaluation corpora. To explore the design space in network
complexity, we also introduced a new design point that is a $\mathit{minimal}$
supervision extension to an existing unsupervised technique. Our evaluation
shows that: 1. adding supervision to an existing unsupervised technique can
improve performance, though not necessarily by much; 2. simple networks for
supervision can be more effective that more sophisticated sequence-based
networks for code search; 3. while it is common to use docstrings to carry out
supervision, there is a sizeable gap between the effectiveness of docstrings
and a more query-appropriate supervision corpus.
  The evaluation dataset is now available at arXiv:1908.09804",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.13001v1,2019-04-30T00:24:06Z,2019-04-30T00:24:06Z,"Encoding Categorical Variables with Conjugate Bayesian Models for WeWork
  Lead Scoring Engine","Applied Data Scientists throughout various industries are commonly faced with
the challenging task of encoding high-cardinality categorical features into
digestible inputs for machine learning algorithms. This paper describes a
Bayesian encoding technique developed for WeWork's lead scoring engine which
outputs the probability of a person touring one of our office spaces based on
interaction, enrichment, and geospatial data. We present a paradigm for
ensemble modeling which mitigates the need to build complicated preprocessing
and encoding schemes for categorical variables. In particular, domain-specific
conjugate Bayesian models are employed as base learners for features in a
stacked ensemble model. For each column of a categorical feature matrix we fit
a problem-specific prior distribution, for example, the Beta distribution for a
binary classification problem. In order to analytically derive the moments of
the posterior distribution, we update the prior with the conjugate likelihood
of the corresponding target variable for each unique value of the given
categorical feature. This function of column and value encodes the categorical
feature matrix so that the final learner in the ensemble model ingests
low-dimensional numerical input. Experimental results on both curated and real
world datasets demonstrate impressive accuracy and computational efficiency on
a variety of problem archetypes. Particularly, for the lead scoring engine at
WeWork -- where some categorical features have as many as 300,000 levels -- we
have seen an AUC improvement from 0.87 to 0.97 through implementing conjugate
Bayesian model encoding.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.10698v1,2019-04-24T08:43:18Z,2019-04-24T08:43:18Z,Multi-scale deep neural networks for real image super-resolution,"Single image super-resolution (SR) is extremely difficult if the upscaling
factors of image pairs are unknown and different from each other, which is
common in real image SR. To tackle the difficulty, we develop two multi-scale
deep neural networks (MsDNN) in this work. Firstly, due to the high computation
complexity in high-resolution spaces, we process an input image mainly in two
different downscaling spaces, which could greatly lower the usage of GPU
memory. Then, to reconstruct the details of an image, we design a multi-scale
residual network (MsRN) in the downscaling spaces based on the residual blocks.
Besides, we propose a multi-scale dense network based on the dense blocks to
compare with MsRN. Finally, our empirical experiments show the robustness of
MsDNN for image SR when the upscaling factor is unknown. According to the
preliminary results of NTIRE 2019 image SR challenge, our team
(ZXHresearch@fudan) ranks 21-st among all participants. The implementation of
MsDNN is released https://github.com/shangqigao/gsq-image-SR",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.08489v2,2019-08-15T19:56:17Z,2019-04-17T20:39:17Z,"Semantic Adversarial Attacks: Parametric Transformations That Fool Deep
  Classifiers","Deep neural networks have been shown to exhibit an intriguing vulnerability
to adversarial input images corrupted with imperceptible perturbations.
However, the majority of adversarial attacks assume global, fine-grained
control over the image pixel space. In this paper, we consider a different
setting: what happens if the adversary could only alter specific attributes of
the input image? These would generate inputs that might be perceptibly
different, but still natural-looking and enough to fool a classifier. We
propose a novel approach to generate such `semantic' adversarial examples by
optimizing a particular adversarial loss over the range-space of a parametric
conditional generative model. We demonstrate implementations of our attacks on
binary classifiers trained on face images, and show that such natural-looking
semantic adversarial examples exist. We evaluate the effectiveness of our
attack on synthetic and real data, and present detailed comparisons with
existing attack methods. We supplement our empirical results with theoretical
bounds that demonstrate the existence of such parametric adversarial examples.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.07998v2,2019-11-11T01:48:59Z,2019-04-16T22:10:19Z,"SynC: A Unified Framework for Generating Synthetic Population with
  Gaussian Copula","Synthetic population generation is the process of combining multiple
socioeconomic and demographic datasets from different sources and/or
granularity levels, and downscaling them to an individual level. Although it is
a fundamental step for many data science tasks, an efficient and standard
framework is absent. In this study, we propose a multi-stage framework called
SynC (Synthetic Population via Gaussian Copula) to fill the gap. SynC first
removes potential outliers in the data and then fits the filtered data with a
Gaussian copula model to correctly capture dependencies and marginal
distributions of sampled survey data. Finally, SynC leverages predictive models
to merge datasets into one and then scales them accordingly to match the
marginal constraints. We make three key contributions in this work: 1) propose
a novel framework for generating individual level data from aggregated data
sources by combining state-of-the-art machine learning and statistical
techniques, 2) demonstrate its value as a feature engineering tool, as well as
an alternative to data collection in situations where gathering is difficult
through two real-world datasets, 3) release an easy-to-use framework
implementation for reproducibility, and 4) ensure the methodology is scalable
at the production level and can easily incorporate new data.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.07633v1,2019-04-16T13:02:01Z,2019-04-16T13:02:01Z,"HARK Side of Deep Learning -- From Grad Student Descent to Automated
  Machine Learning","Recent advancements in machine learning research, i.e., deep learning,
introduced methods that excel conventional algorithms as well as humans in
several complex tasks, ranging from detection of objects in images and speech
recognition to playing difficult strategic games. However, the current
methodology of machine learning research and consequently, implementations of
the real-world applications of such algorithms, seems to have a recurring
HARKing (Hypothesizing After the Results are Known) issue. In this work, we
elaborate on the algorithmic, economic and social reasons and consequences of
this phenomenon. We present examples from current common practices of
conducting machine learning research (e.g. avoidance of reporting negative
results) and failure of generalization ability of the proposed algorithms and
datasets in actual real-life usage. Furthermore, a potential future trajectory
of machine learning research and development from the perspective of
accountable, unbiased, ethical and privacy-aware algorithmic decision making is
discussed. We would like to emphasize that with this discussion we neither
claim to provide an exhaustive argumentation nor blame any specific institution
or individual on the raised issues. This is simply a discussion put forth by
us, insiders of the machine learning field, reflecting on us.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.07623v1,2019-04-16T12:33:56Z,2019-04-16T12:33:56Z,"DeepRadioID: Real-Time Channel-Resilient Optimization of Deep
  Learning-based Radio Fingerprinting Algorithms","Radio fingerprinting provides a reliable and energy-efficient IoT
authentication strategy. By mapping inputs onto a very large feature space,
deep learning algorithms can be trained to fingerprint large populations of
devices operating under any wireless standard. One of the most crucial
challenges in radio fingerprinting is to counteract the action of the wireless
channel, which decreases fingerprinting accuracy significantly by disrupting
hardware impairments. On the other hand, due to their sheer size, deep learning
algorithms are hardly re-trainable in real-time. Another aspect that is yet to
be investigated is whether an adversary can successfully impersonate another
device fingerprint. To address these key issues, this paper proposes
DeepRadioID, a system to optimize the accuracy of deep-learning-based radio
fingerprinting algorithms without retraining the underlying deep learning
model. We extensively evaluate DeepRadioID on a experimental testbed of 20
nominally-identical software-defined radios, as well as on two datasets made up
by 500 ADS-B devices and by 500 WiFi devices provided by the DARPA RFMLS
program. Experimental results show that DeepRadioID (i) increases
fingerprinting accuracy by about 35%, 50% and 58% on the three scenarios
considered; (ii) decreases an adversary's accuracy by about 54% when trying to
imitate other device fingerprints by using their filters; (iii) achieves 27%
improvement over the state of the art on a 100-device dataset.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.05343v2,2020-03-26T02:51:10Z,2019-04-10T17:53:38Z,StegaStamp: Invisible Hyperlinks in Physical Photographs,"Printed and digitally displayed photos have the ability to hide imperceptible
digital data that can be accessed through internet-connected imaging systems.
Another way to think about this is physical photographs that have unique QR
codes invisibly embedded within them. This paper presents an architecture,
algorithms, and a prototype implementation addressing this vision. Our key
technical contribution is StegaStamp, a learned steganographic algorithm to
enable robust encoding and decoding of arbitrary hyperlink bitstrings into
photos in a manner that approaches perceptual invisibility. StegaStamp
comprises a deep neural network that learns an encoding/decoding algorithm
robust to image perturbations approximating the space of distortions resulting
from real printing and photography. We demonstrates real-time decoding of
hyperlinks in photos from in-the-wild videos that contain variation in
lighting, shadows, perspective, occlusion and viewing distance. Our prototype
system robustly retrieves 56 bit hyperlinks after error correction - sufficient
to embed a unique code within every photo on the internet.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.04433v1,2019-04-09T02:45:35Z,2019-04-09T02:45:35Z,"Efficient Decision-based Black-box Adversarial Attacks on Face
  Recognition","Face recognition has obtained remarkable progress in recent years due to the
great improvement of deep convolutional neural networks (CNNs). However, deep
CNNs are vulnerable to adversarial examples, which can cause fateful
consequences in real-world face recognition applications with
security-sensitive purposes. Adversarial attacks are widely studied as they can
identify the vulnerability of the models before they are deployed. In this
paper, we evaluate the robustness of state-of-the-art face recognition models
in the decision-based black-box attack setting, where the attackers have no
access to the model parameters and gradients, but can only acquire hard-label
predictions by sending queries to the target model. This attack setting is more
practical in real-world face recognition systems. To improve the efficiency of
previous methods, we propose an evolutionary attack algorithm, which can model
the local geometries of the search directions and reduce the dimension of the
search space. Extensive experiments demonstrate the effectiveness of the
proposed method that induces a minimum perturbation to an input face image with
fewer queries. We also apply the proposed method to attack a real-world face
recognition system successfully.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.00923v1,2019-04-01T15:51:12Z,2019-04-01T15:51:12Z,Robustness of 3D Deep Learning in an Adversarial Setting,"Understanding the spatial arrangement and nature of real-world objects is of
paramount importance to many complex engineering tasks, including autonomous
navigation. Deep learning has revolutionized state-of-the-art performance for
tasks in 3D environments; however, relatively little is known about the
robustness of these approaches in an adversarial setting. The lack of
comprehensive analysis makes it difficult to justify deployment of 3D deep
learning models in real-world, safety-critical applications. In this work, we
develop an algorithm for analysis of pointwise robustness of neural networks
that operate on 3D data. We show that current approaches presented for
understanding the resilience of state-of-the-art models vastly overestimate
their robustness. We then use our algorithm to evaluate an array of
state-of-the-art models in order to demonstrate their vulnerability to
occlusion attacks. We show that, in the worst case, these networks can be
reduced to 0% classification accuracy after the occlusion of at most 6.5% of
the occupied input space.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1904.00035v1,2019-03-29T18:15:24Z,2019-03-29T18:15:24Z,Autonomous Highway Driving using Deep Reinforcement Learning,"The operational space of an autonomous vehicle (AV) can be diverse and vary
significantly. This may lead to a scenario that was not postulated in the
design phase. Due to this, formulating a rule based decision maker for
selecting maneuvers may not be ideal. Similarly, it may not be effective to
design an a-priori cost function and then solve the optimal control problem in
real-time. In order to address these issues and to avoid peculiar behaviors
when encountering unforeseen scenario, we propose a reinforcement learning (RL)
based method, where the ego car, i.e., an autonomous vehicle, learns to make
decisions by directly interacting with simulated traffic. The decision maker
for AV is implemented as a deep neural network providing an action choice for a
given system state. In a critical application such as driving, an RL agent
without explicit notion of safety may not converge or it may need extremely
large number of samples before finding a reliable policy. To best address the
issue, this paper incorporates reinforcement learning with an additional short
horizon safety check (SC). In a critical scenario, the safety check will also
provide an alternate safe action to the agent provided if it exists. This leads
to two novel contributions. First, it generalizes the states that could lead to
undesirable ""near-misses"" or ""collisions "". Second, inclusion of safety check
can provide a safe and stable training environment. This significantly enhances
learning efficiency without inhibiting meaningful exploration to ensure safe
and optimal learned behavior. We demonstrate the performance of the developed
algorithm in highway driving scenario where the trained AV encounters varying
traffic density in a highway setting.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.12493v2,2019-06-01T02:05:24Z,2019-03-29T13:00:03Z,Asymmetric Deep Semantic Quantization for Image Retrieval,"Due to its fast retrieval and storage efficiency capabilities, hashing has
been widely used in nearest neighbor retrieval tasks. By using deep learning
based techniques, hashing can outperform non-learning based hashing technique
in many applications. However, we argue that the current deep learning based
hashing methods ignore some critical problems (e.g., the learned hash codes are
not discriminative due to the hashing methods being unable to discover rich
semantic information and the training strategy having difficulty optimizing the
discrete binary codes). In this paper, we propose a novel image hashing method,
termed as \textbf{\underline{A}}symmetric \textbf{\underline{D}}eep
\textbf{\underline{S}}emantic \textbf{\underline{Q}}uantization
(\textbf{ADSQ}). \textbf{ADSQ} is implemented using three stream frameworks,
which consist of one \emph{LabelNet} and two \emph{ImgNets}. The
\emph{LabelNet} leverages the power of three fully-connected layers, which are
used to capture rich semantic information between image pairs. For the two
\emph{ImgNets}, they each adopt the same convolutional neural network
structure, but with different weights (i.e., asymmetric convolutional neural
networks). The two \emph{ImgNets} are used to generate discriminative compact
hash codes. Specifically, the function of the \emph{LabelNet} is to capture
rich semantic information that is used to guide the two \emph{ImgNets} in
minimizing the gap between the real-continuous features and the discrete binary
codes. Furthermore, \textbf{ADSQ} can utilize the most critical semantic
information to guide the feature learning process and consider the consistency
of the common semantic space and Hamming space. Experimental results on three
benchmarks (i.e., CIFAR-10, NUS-WIDE, and ImageNet) demonstrate that the
proposed \textbf{ADSQ} can outperforms current state-of-the-art methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.05575v1,2019-03-13T16:19:27Z,2019-03-13T16:19:27Z,"On the Efficacy and High-Performance Implementation of Quaternion Matrix
  Multiplication","Quaternion symmetry is ubiquitous in the physical sciences. As such, much
work has been afforded over the years to the development of efficient schemes
to exploit this symmetry using real and complex linear algebra. Recent years
have also seen many advances in the formal theoretical development of
explicitly quaternion linear algebra with promising applications in image
processing and machine learning. Despite these advances, there do not currently
exist optimized software implementations of quaternion linear algebra. The
leverage of optimized linear algebra software is crucial in the achievement of
high levels of performance on modern computing architectures, and thus provides
a central tool in the development of high-performance scientific software. In
this work, a case will be made for the efficacy of high-performance quaternion
linear algebra software for appropriate problems. In this pursuit, an optimized
software implementation of quaternion matrix multiplication will be presented
and will be shown to outperform a vendor tuned implementation for the analogous
complex matrix operation. The results of this work pave the path for further
development of high-performance quaternion linear algebra software which will
improve the performance of the next generation of applicable scientific
applications.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.05431v1,2019-03-13T11:54:04Z,2019-03-13T11:54:04Z,"Resource Abstraction for Reinforcement Learning in Multiagent Congestion
  Problems","Real-world congestion problems (e.g. traffic congestion) are typically very
complex and large-scale. Multiagent reinforcement learning (MARL) is a
promising candidate for dealing with this emerging complexity by providing an
autonomous and distributed solution to these problems. However, there are three
limiting factors that affect the deployability of MARL approaches to congestion
problems. These are learning time, scalability and decentralised coordination
i.e. no communication between the learning agents. In this paper we introduce
Resource Abstraction, an approach that addresses these challenges by allocating
the available resources into abstract groups. This abstraction creates new
reward functions that provide a more informative signal to the learning agents
and aid the coordination amongst them. Experimental work is conducted on two
benchmark domains from the literature, an abstract congestion problem and a
realistic traffic congestion problem. The current state-of-the-art for solving
multiagent congestion problems is a form of reward shaping called difference
rewards. We show that the system using Resource Abstraction significantly
improves the learning speed and scalability, and achieves the highest possible
or near-highest joint performance/social welfare for both congestion problems
in large-scale scenarios involving up to 1000 reinforcement learning agents.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.04507v1,2019-03-11T18:00:18Z,2019-03-11T18:00:18Z,"Realistic On-The-Fly Outcomes of Planetary Collisions: Machine Learning
  Applied to Simulations of Giant Impacts","Planet formation simulations are capable of directly integrating the
evolution of hundreds to thousands of planetary embryos and planetesimals, as
they accrete pairwise to become planets. In principle such investigations allow
us to better understand the final configuration and geochemistry of the
terrestrial planets, as well as to place our solar system in the context of
other exosolar systems. These simulations, however, classically prescribe
collisions to result in perfect mergers, but computational advances have begun
to allow for more complex outcomes to be implemented. Here we apply machine
learning to a large but sparse database of giant impact studies, streamlining
simulations into a classifier of collision outcomes and a regressor of
accretion efficiency. The classifier maps a 4-Dimensional parameter space
(target mass, projectile-to-target mass ratio, impact velocity, impact angle)
into the four major collision types: merger, ""graze-and-merge"", ""hit-and-run"",
and disruption. The definition of the four regimes and their boundary is fully
data-driven; the results do not suffer from any model assumption in the
fitting. The classifier maps the structure of the parameter space and provides
insights about the outcome regimes. The regressor is a neural network which is
trained to closely mimic the functional relationship between the 4-D space of
collision parameters, and a real-variable outcome, the mass of the largest
remnant. This work is a prototype of a more complete surrogate model, based on
extended sets of simulations (""big data""), that will quickly and reliably
predict specific collision outcomes for use in realistic N-body dynamical
studies of planetary formation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.03777v2,2019-04-05T06:54:27Z,2019-03-09T10:41:41Z,"Partial Order Pruning: for Best Speed/Accuracy Trade-off in Neural
  Architecture Search","Achieving good speed and accuracy trade-off on a target platform is very
important in deploying deep neural networks in real world scenarios. However,
most existing automatic architecture search approaches only concentrate on high
performance. In this work, we propose an algorithm that can offer better
speed/accuracy trade-off of searched networks, which is termed ""Partial Order
Pruning"". It prunes the architecture search space with a partial order
assumption to automatically search for the architectures with the best speed
and accuracy trade-off. Our algorithm explicitly takes profile information
about the inference speed on the target platform into consideration. With the
proposed algorithm, we present several Dongfeng (DF) networks that provide high
accuracy and fast inference speed on various application GPU platforms. By
further searching decoder architectures, our DF-Seg real-time segmentation
networks yield state-of-the-art speed/accuracy trade-off on both the target
embedded device and the high-end GPU.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.03404v2,2019-03-28T08:25:41Z,2019-03-08T13:03:11Z,"Accelerating Generalized Linear Models with MLWeaving: A
  One-Size-Fits-All System for Any-precision Learning (Technical Report)","Learning from the data stored in a database is an important function
increasingly available in relational engines. Methods using lower precision
input data are of special interest given their overall higher efficiency but,
in databases, these methods have a hidden cost: the quantization of the real
value into a smaller number is an expensive step. To address the issue, in this
paper we present MLWeaving, a data structure and hardware acceleration
technique intended to speed up learning of generalized linear models in
databases. ML-Weaving provides a compact, in-memory representation enabling the
retrieval of data at any level of precision. MLWeaving also takes advantage of
the increasing availability of FPGA-based accelerators to provide a highly
efficient implementation of stochastic gradient descent. The solution adopted
in MLWeaving is more efficient than existing designs in terms of space (since
it can process any resolution on the same design) and resources (via the use of
bit-serial multipliers). MLWeaving also enables the runtime tuning of
precision, instead of a fixed precision level during the training. We
illustrate this using a simple, dynamic precision schedule. Experimental
results show MLWeaving achieves up to16 performance improvement over
low-precision CPU implementations of first-order methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.04548v1,2019-03-08T08:04:52Z,2019-03-08T08:04:52Z,"A Novel Approach for Protection of Accounts' Names against Hackers
  Combining Cluster Analysis and Chaotic Theory","The last years of the 20 th century and the beginning of the 21 th mark the
facilitation trend of our real life due to the big development and progress of
the computers and other intelligent devices. Algorithms based on artificial
intelligence are basically a part of the software. The transmitted information
by Internet or LAN arises continuously and it is expected that the protection
of the data has been ensured. The aim of the present paper is to reveal false
names of users' accounts as a result of hackers' attacks. The probability a
given account to be either false or actual is calculated using a novel approach
combining machine learning analysis (especially clusters' analysis) with chaos
theory. The suspected account will be used as a pattern and by classification
techniques clusters will be formed with a respective probability this name to
be false. This investigation puts two main purposes: First, to determine if
there exists a trend of appearance of the similar usernames, which arises
during the creation of new accounts. Second, to detect the false usernames and
to discriminate those from the real ones, independently of that if two types of
accounts are generated with the same speed. These security systems are applied
in different areas, where the security of the data in users' accounts is
strictly required. For example, they can be used in on-line voting for
balloting, in studying the social opinion by inquiries, in protection of the
information in different user accounts of given system etc.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.02219v1,2019-03-06T07:39:11Z,2019-03-06T07:39:11Z,Training in Task Space to Speed Up and Guide Reinforcement Learning,"Recent breakthroughs in the reinforcement learning (RL) community have made
significant advances towards learning and deploying policies on real world
robotic systems. However, even with the current state-of-the-art algorithms and
computational resources, these algorithms are still plagued with high sample
complexity, and thus long training times, especially for high degree of freedom
(DOF) systems. There are also concerns arising from lack of perceived stability
or robustness guarantees from emerging policies. This paper aims at mitigating
these drawbacks by: (1) modeling a complex, high DOF system with a
representative simple one, (2) making explicit use of forward and inverse
kinematics without forcing the RL algorithm to ""learn"" them on its own, and (3)
learning locomotion policies in Cartesian space instead of joint space. In this
paper these methods are applied to JPL's Robosimian, but can be readily used on
any system with a base and end effector(s). These locomotion policies can be
produced in just a few minutes, trained on a single laptop. We compare the
robustness of the resulting learned policies to those of other control methods.
An accompanying video for this paper can be found at
https://youtu.be/xDxxSw5ahnc .",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1903.06802v1,2019-02-26T02:09:31Z,2019-02-26T02:09:31Z,"Workflow-Driven Distributed Machine Learning in CHASE-CI: A Cognitive
  Hardware and Software Ecosystem Community Infrastructure","The advances in data, computing and networking over the last two decades led
to a shift in many application domains that includes machine learning on big
data as a part of the scientific process, requiring new capabilities for
integrated and distributed hardware and software infrastructure. This paper
contributes a workflow-driven approach for dynamic data-driven application
development on top of a new kind of networked Cyberinfrastructure called
CHASE-CI. In particular, we present: 1) The architecture for CHASE-CI, a
network of distributed fast GPU appliances for machine learning and storage
managed through Kubernetes on the high-speed (10-100Gbps) Pacific Research
Platform (PRP); 2) A machine learning software containerization approach and
libraries required for turning such a network into a distributed computer for
big data analysis; 3) An atmospheric science case study that can only be made
scalable with an infrastructure like CHASE-CI; 4) Capabilities for virtual
cluster management for data communication and analysis in a dynamically
scalable fashion, and visualization across the network in specialized
visualization facilities in near real-time; and, 5) A step-by-step workflow and
performance measurement approach that enables taking advantage of the dynamic
architecture of the CHASE-CI network and container management infrastructure.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1902.08730v1,2019-02-23T03:45:31Z,2019-02-23T03:45:31Z,AliGraph: A Comprehensive Graph Neural Network Platform,"An increasing number of machine learning tasks require dealing with large
graph datasets, which capture rich and complex relationship among potentially
billions of elements. Graph Neural Network (GNN) becomes an effective way to
address the graph learning problem by converting the graph data into a low
dimensional space while keeping both the structural and property information to
the maximum extent and constructing a neural network for training and
referencing. However, it is challenging to provide an efficient graph storage
and computation capabilities to facilitate GNN training and enable development
of new GNN algorithms. In this paper, we present a comprehensive graph neural
network system, namely AliGraph, which consists of distributed graph storage,
optimized sampling operators and runtime to efficiently support not only
existing popular GNNs but also a series of in-house developed ones for
different scenarios. The system is currently deployed at Alibaba to support a
variety of business scenarios, including product recommendation and
personalized search at Alibaba's E-Commerce platform. By conducting extensive
experiments on a real-world dataset with 492.90 million vertices, 6.82 billion
edges and rich attributes, AliGraph performs an order of magnitude faster in
terms of graph building (5 minutes vs hours reported from the state-of-the-art
PowerGraph platform). At training, AliGraph runs 40%-50% faster with the novel
caching strategy and demonstrates around 12 times speed up with the improved
runtime. In addition, our in-house developed GNN models all showcase their
statistically significant superiorities in terms of both effectiveness and
efficiency (e.g., 4.12%-17.19% lift by F1 scores).",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1902.08638v1,2019-02-22T19:16:32Z,2019-02-22T19:16:32Z,MPP: Model Performance Predictor,"Operations is a key challenge in the domain of machine learning pipeline
deployments involving monitoring and management of real-time prediction
quality. Typically, metrics like accuracy, RMSE etc., are used to track the
performance of models in deployment. However, these metrics cannot be
calculated in production due to the absence of labels. We propose using an ML
algorithm, Model Performance Predictor (MPP), to track the performance of the
models in deployment. We argue that an ensemble of such metrics can be used to
create a score representing the prediction quality in production. This in turn
facilitates formulation and customization of ML alerts, that can be escalated
by an operations team to the data science team. Such a score automates
monitoring and enables ML deployments at scale.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1902.06824v2,2019-06-13T19:14:27Z,2019-02-18T22:31:09Z,"Autonomous Airline Revenue Management: A Deep Reinforcement Learning
  Approach to Seat Inventory Control and Overbooking","Revenue management can enable airline corporations to maximize the revenue
generated from each scheduled flight departing in their transportation network
by means of finding the optimal policies for differential pricing, seat
inventory control and overbooking. As different demand segments in the market
have different Willingness-To-Pay (WTP), airlines use differential pricing,
booking restrictions, and service amenities to determine different fare classes
or products targeted at each of these demand segments. Because seats are
limited for each flight, airlines also need to allocate seats for each of these
fare classes to prevent lower fare class passengers from displacing higher fare
class ones and set overbooking limits in anticipation of cancellations and
no-shows such that revenue is maximized. Previous work addresses these problems
using optimization techniques or classical Reinforcement Learning methods. This
paper focuses on the latter problem - the seat inventory control problem -
casting it as a Markov Decision Process to be able to find the optimal policy.
Multiple fare classes, concurrent continuous arrival of passengers of different
fare classes, overbooking and random cancellations that are independent of
class have been considered in the model. We have addressed this problem using
Deep Q-Learning with the goal of maximizing the reward for each flight
departure. The implementation of this technique allows us to employ large
continuous state space but also presents the potential opportunity to test on
real time airline data. To generate data and train the agent, a basic
air-travel market simulator was developed. The performance of the agent in
different simulated market scenarios was compared against theoretically optimal
solutions and was found to be nearly close to the expected optimal revenue.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1902.05577v2,2020-03-15T13:25:50Z,2019-02-14T19:43:10Z,"A Scalable Platform for Distributed Object Tracking across a Many-camera
  Network","Advances in deep neural networks (DNN) and computer vision (CV) algorithms
have made it feasible to extract meaningful insights from large-scale
deployments of urban cameras. Tracking an object of interest across the camera
network in near real-time is a canonical problem. However, current tracking
platforms have two key limitations: 1) They are monolithic, proprietary and
lack the ability to rapidly incorporate sophisticated tracking models; and 2)
They are less responsive to dynamism across wide-area computing resources that
include edge, fog and cloud abstractions. We address these gaps using Anveshak,
a runtime platform for composing and coordinating distributed tracking
applications. It provides a domain-specific dataflow programming model to
intuitively compose a tracking application, supporting contemporary CV advances
like query fusion and re-identification, and enabling dynamic scoping of the
camera network's search space to avoid wasted computation. We also offer
tunable batching and data-dropping strategies for dataflow blocks deployed on
distributed resources to respond to network and compute variability. These
balance the tracking accuracy, its real-time performance and the active
camera-set size. We illustrate the concise expressiveness of the programming
model for $4$ tracking applications. Our detailed experiments for a network of
1000 camera-feeds on modest resources exhibit the tunable scalability,
performance and quality trade-offs enabled by our dynamic tracking, batching
and dropping strategies.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1902.05009v1,2019-02-13T17:03:33Z,2019-02-13T17:03:33Z,"ATMSeer: Increasing Transparency and Controllability in Automated
  Machine Learning","To relieve the pain of manually selecting machine learning algorithms and
tuning hyperparameters, automated machine learning (AutoML) methods have been
developed to automatically search for good models. Due to the huge model search
space, it is impossible to try all models. Users tend to distrust automatic
results and increase the search budget as much as they can, thereby undermining
the efficiency of AutoML. To address these issues, we design and implement
ATMSeer, an interactive visualization tool that supports users in refining the
search space of AutoML and analyzing the results. To guide the design of
ATMSeer, we derive a workflow of using AutoML based on interviews with machine
learning experts. A multi-granularity visualization is proposed to enable users
to monitor the AutoML process, analyze the searched models, and refine the
search space in real time. We demonstrate the utility and usability of ATMSeer
through two case studies, expert interviews, and a user study with 13 end
users.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1902.02236v1,2019-02-06T15:38:06Z,2019-02-06T15:38:06Z,Dynamic Pricing for Airline Ancillaries with Customer Context,"Ancillaries have become a major source of revenue and profitability in the
travel industry. Yet, conventional pricing strategies are based on business
rules that are poorly optimized and do not respond to changing market
conditions. This paper describes the dynamic pricing model developed by Deepair
solutions, an AI technology provider for travel suppliers. We present a pricing
model that provides dynamic pricing recommendations specific to each customer
interaction and optimizes expected revenue per customer. The unique nature of
personalized pricing provides the opportunity to search over the market space
to find the optimal price-point of each ancillary for each customer, without
violating customer privacy. In this paper, we present and compare three
approaches for dynamic pricing of ancillaries, with increasing levels of
sophistication: (1) a two-stage forecasting and optimization model using a
logistic mapping function; (2) a two-stage model that uses a deep neural
network for forecasting, coupled with a revenue maximization technique using
discrete exhaustive search; (3) a single-stage end-to-end deep neural network
that recommends the optimal price. We describe the performance of these models
based on both offline and online evaluations. We also measure the real-world
business impact of these approaches by deploying them in an A/B test on an
airline's internet booking website. We show that traditional machine learning
techniques outperform human rule-based approaches in an online setting by
improving conversion by 36% and revenue per offer by 10%. We also provide
results for our offline experiments which show that deep learning algorithms
outperform traditional machine learning techniques for this problem. Our
end-to-end deep learning model is currently being deployed by the airline in
their booking system.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1902.01580v1,2019-02-05T08:09:33Z,2019-02-05T08:09:33Z,PUTWorkbench: Analysing Privacy in AI-intensive Systems,"AI intensive systems that operate upon user data face the challenge of
balancing data utility with privacy concerns. We propose the idea and present
the prototype of an open-source tool called Privacy Utility Trade-off (PUT)
Workbench which seeks to aid software practitioners to take such crucial
decisions. We pick a simple privacy model that doesn't require any background
knowledge in Data Science and show how even that can achieve significant
results over standard and real-life datasets. The tool and the source code is
made freely available for extensions and usage.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1902.01084v2,2021-01-15T20:47:41Z,2019-02-04T08:51:50Z,Paracosm: A Language and Tool for Testing Autonomous Driving Systems,"Systematic testing of autonomous vehicles operating in complex real-world
scenarios is a difficult and expensive problem. We present Paracosm, a reactive
language for writing test scenarios for autonomous driving systems. Paracosm
allows users to programmatically describe complex driving situations with
specific visual features, e.g., road layout in an urban environment, as well as
reactive temporal behaviors of cars and pedestrians. Paracosm programs are
executed on top of a game engine that provides realistic physics simulation and
visual rendering. The infrastructure allows systematic exploration of the state
space, both for visual features (lighting, shadows, fog) and for reactive
interactions with the environment (pedestrians, other traffic). We define a
notion of test coverage for Paracosm configurations based on combinatorial
testing and low dispersion sequences. Paracosm comes with an automatic test
case generator that uses random sampling for discrete parameters and
deterministic quasi-Monte Carlo generation for continuous parameters. Through
an empirical evaluation, we demonstrate the modeling and testing capabilities
of Paracosm on a suite of autonomous driving systems implemented using deep
neural networks developed in research and education. We show how Paracosm can
expose incorrect behaviors or degraded performance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1902.00522v1,2019-02-01T19:02:18Z,2019-02-01T19:02:18Z,"Deep Learning for Multi-Messenger Astrophysics: A Gateway for Discovery
  in the Big Data Era","This report provides an overview of recent work that harnesses the Big Data
Revolution and Large Scale Computing to address grand computational challenges
in Multi-Messenger Astrophysics, with a particular emphasis on real-time
discovery campaigns. Acknowledging the transdisciplinary nature of
Multi-Messenger Astrophysics, this document has been prepared by members of the
physics, astronomy, computer science, data science, software and
cyberinfrastructure communities who attended the NSF-, DOE- and NVIDIA-funded
""Deep Learning for Multi-Messenger Astrophysics: Real-time Discovery at Scale""
workshop, hosted at the National Center for Supercomputing Applications,
October 17-19, 2018. Highlights of this report include unanimous agreement that
it is critical to accelerate the development and deployment of novel,
signal-processing algorithms that use the synergy between artificial
intelligence (AI) and high performance computing to maximize the potential for
scientific discovery with Multi-Messenger Astrophysics. We discuss key aspects
to realize this endeavor, namely (i) the design and exploitation of scalable
and computationally efficient AI algorithms for Multi-Messenger Astrophysics;
(ii) cyberinfrastructure requirements to numerically simulate astrophysical
sources, and to process and interpret Multi-Messenger Astrophysics data; (iii)
management of gravitational wave detections and triggers to enable
electromagnetic and astro-particle follow-ups; (iv) a vision to harness future
developments of machine and deep learning and cyberinfrastructure resources to
cope with the scale of discovery in the Big Data Era; (v) and the need to build
a community that brings domain experts together with data scientists on equal
footing to maximize and accelerate discovery in the nascent field of
Multi-Messenger Astrophysics.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1901.11422v1,2019-01-31T15:20:43Z,2019-01-31T15:20:43Z,"High-dimensional Metric Combining for Non-coherent Molecular Signal
  Detection","In emerging Internet-of-Nano-Thing (IoNT), information will be embedded and
conveyed in the form of molecules through complex and diffusive medias. One
main challenge lies in the long-tail nature of the channel response causing
inter-symbol-interference (ISI), which deteriorates the detection performance.
If the channel is unknown, we cannot easily achieve traditional coherent
channel estimation and cancellation, and the impact of ISI will be more severe.
In this paper, we develop a novel high-dimensional non-coherent scheme for
blind detection of molecular signals. We achieve this in a higher-dimensional
metric space by combining different non-coherent metrics that exploit the
transient features of the signals. By deducing the theoretical bit error rate
(BER) for any constructed high-dimensional non-coherent metric, we prove that,
higher dimensionality always achieves a lower BER in the same sample space.
Then, we design a generalised blind detection algorithm that utilizes the
Parzen approximation and its probabilistic neural network (Parzen-PNN) to
detect information bits. Taking advantages of its fast convergence and parallel
implementation, our proposed scheme can meet the needs of detection accuracy
and real-time computing. Numerical simulations demonstrate that our proposed
scheme can gain 10dB BER compared with other state of the art methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1901.10281v1,2019-01-29T13:43:57Z,2019-01-29T13:43:57Z,Structural Material Property Tailoring Using Deep Neural Networks,"Advances in robotics, artificial intelligence, and machine learning are
ushering in a new age of automation, as machines match or outperform human
performance. Machine intelligence can enable businesses to improve performance
by reducing errors, improving sensitivity, quality and speed, and in some cases
achieving outcomes that go beyond current resource capabilities. Relevant
applications include new product architecture design, rapid material
characterization, and life-cycle management tied with a digital strategy that
will enable efficient development of products from cradle to grave. In
addition, there are also challenges to overcome that must be addressed through
a major, sustained research effort that is based solidly on both inferential
and computational principles applied to design tailoring of functionally
optimized structures. Current applications of structural materials in the
aerospace industry demand the highest quality control of material
microstructure, especially for advanced rotational turbomachinery in aircraft
engines in order to have the best tailored material property. In this paper,
deep convolutional neural networks were developed to accurately predict
processing-structure-property relations from materials microstructures images,
surpassing current best practices and modeling efforts. The models
automatically learn critical features, without the need for manual
specification and/or subjective and expensive image analysis. Further, in
combination with generative deep learning models, a framework is proposed to
enable rapid material design space exploration and property identification and
optimization. The implementation must take account of real-time decision cycles
and the trade-offs between speed and accuracy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1901.09049v2,2019-02-21T17:15:18Z,2019-01-25T19:07:36Z,"Biologically inspired alternatives to backpropagation through time for
  learning in recurrent neural nets","The way how recurrently connected networks of spiking neurons in the brain
acquire powerful information processing capabilities through learning has
remained a mystery. This lack of understanding is linked to a lack of learning
algorithms for recurrent networks of spiking neurons (RSNNs) that are both
functionally powerful and can be implemented by known biological mechanisms.
Since RSNNs are simultaneously a primary target for implementations of
brain-inspired circuits in neuromorphic hardware, this lack of algorithmic
insight also hinders technological progress in that area. The gold standard for
learning in recurrent neural networks in machine learning is back-propagation
through time (BPTT), which implements stochastic gradient descent with regard
to a given loss function. But BPTT is unrealistic from a biological
perspective, since it requires a transmission of error signals backwards in
time and in space, i.e., from post- to presynaptic neurons. We show that an
online merging of locally available information during a computation with
suitable top-down learning signals in real-time provides highly capable
approximations to BPTT. For tasks where information on errors arises only late
during a network computation, we enrich locally available information through
feedforward eligibility traces of synapses that can easily be computed in an
online manner. The resulting new generation of learning algorithms for
recurrent neural networks provides a new understanding of network learning in
the brain that can be tested experimentally. In addition, these algorithms
provide efficient methods for on-chip training of RSNNs in neuromorphic
hardware.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1901.08954v1,2019-01-25T16:18:22Z,2019-01-25T16:18:22Z,"Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder
  Anomaly Detection","Despite inherent ill-definition, anomaly detection is a research endeavor of
great interest within machine learning and visual scene understanding alike.
Most commonly, anomaly detection is considered as the detection of outliers
within a given data distribution based on some measure of normality. The most
significant challenge in real-world anomaly detection problems is that
available data is highly imbalanced towards normality (i.e. non-anomalous) and
contains a most a subset of all possible anomalous samples - hence limiting the
use of well-established supervised learning methods. By contrast, we introduce
an unsupervised anomaly detection model, trained only on the normal
(non-anomalous, plentiful) samples in order to learn the normality distribution
of the domain and hence detect abnormality based on deviation from this model.
Our proposed approach employs an encoder-decoder convolutional neural network
with skip connections to thoroughly capture the multi-scale distribution of the
normal data distribution in high-dimensional image space. Furthermore,
utilizing an adversarial training scheme for this chosen architecture provides
superior reconstruction both within high-dimensional image space and a
lower-dimensional latent vector space encoding. Minimizing the reconstruction
error metric within both the image and hidden vector spaces during training
aids the model to learn the distribution of normality as required. Higher
reconstruction metrics during subsequent test and deployment are thus
indicative of a deviation from this normal distribution, hence indicative of an
anomaly. Experimentation over established anomaly detection benchmarks and
challenging real-world datasets, within the context of X-ray security
screening, shows the unique promise of such a proposed approach.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1901.08548v1,2019-01-20T18:51:30Z,2019-01-20T18:51:30Z,A tensorized logic programming language for large-scale data,"We introduce a new logic programming language T-PRISM based on tensor
embeddings. Our embedding scheme is a modification of the distribution
semantics in PRISM, one of the state-of-the-art probabilistic logic programming
languages, by replacing distribution functions with multidimensional arrays,
i.e., tensors. T-PRISM consists of two parts: logic programming part and
numerical computation part. The former provides flexible and interpretable
modeling at the level of first order logic, and the latter part provides
scalable computation utilizing parallelization and hardware acceleration with
GPUs. Combing these two parts provides a remarkably wide range of high-level
declarative modeling from symbolic reasoning to deep learning. To embody this
programming language, we also introduce a new semantics, termed tensorized
semantics, which combines the traditional least model semantics in logic
programming with the embeddings of tensors. In T-PRISM, we first derive a set
of equations related to tensors from a given program using logical inference,
i.e., Prolog execution in a symbolic space and then solve the derived equations
in a continuous space by TensorFlow. Using our preliminary implementation of
T-PRISM, we have successfully dealt with a wide range of modeling. We have
succeeded in dealing with real large-scale data in the declarative modeling.
This paper presents a DistMult model for knowledge graphs using the FB15k and
WN18 datasets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1901.05356v1,2019-01-16T15:56:19Z,2019-01-16T15:56:19Z,"How to Host a Data Competition: Statistical Advice for Design and
  Analysis of a Data Competition","Data competitions rely on real-time leaderboards to rank competitor entries
and stimulate algorithm improvement. While such competitions have become quite
popular and prevalent, particularly in supervised learning formats, their
implementations by the host are highly variable. Without careful planning, a
supervised learning competition is vulnerable to overfitting, where the winning
solutions are so closely tuned to the particular set of provided data that they
cannot generalize to the underlying problem of interest to the host. This paper
outlines some important considerations for strategically designing relevant and
informative data sets to maximize the learning outcome from hosting a
competition based on our experience. It also describes a post-competition
analysis that enables robust and efficient assessment of the strengths and
weaknesses of solutions from different competitors, as well as greater
understanding of the regions of the input space that are well-solved. The
post-competition analysis, which complements the leaderboard, uses exploratory
data analysis and generalized linear models (GLMs). The GLMs not only expand
the range of results we can explore, they also provide more detailed analysis
of individual sub-questions including similarities and differences between
algorithms across different types of scenarios, universally easy or hard
regions of the input space, and different learning objectives. When coupled
with a strategically planned data generation approach, the methods provide
richer and more informative summaries to enhance the interpretation of results
beyond just the rankings on the leaderboard. The methods are illustrated with a
recently completed competition to evaluate algorithms capable of detecting,
identifying, and locating radioactive materials in an urban environment.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1901.05147v1,2019-01-16T06:10:45Z,2019-01-16T06:10:45Z,The Winning Solution to the IEEE CIG 2017 Game Data Mining Competition,"Machine learning competitions such as those organized by Kaggle or KDD
represent a useful benchmark for data science research. In this work, we
present our winning solution to the Game Data Mining competition hosted at the
2017 IEEE Conference on Computational Intelligence and Games (CIG 2017). The
contest consisted of two tracks, and participants (more than 250, belonging to
both industry and academia) were to predict which players would stop playing
the game, as well as their remaining lifetime. The data were provided by a
major worldwide video game company, NCSoft, and came from their successful
massively multiplayer online game Blade and Soul. Here, we describe the long
short-term memory approach and conditional inference survival ensemble model
that made us win both tracks of the contest, as well as the validation
procedure that we followed in order to prevent overfitting. In particular,
choosing a survival method able to deal with censored data was crucial to
accurately predict the moment in which each player would leave the game, as
censoring is inherent in churn. The selected models proved to be robust against
evolving conditions---since there was a change in the business model of the
game (from subscription-based to free-to-play) between the two sample datasets
provided---and efficient in terms of time cost. Thanks to these features and
also to their a ability to scale to large datasets, our models could be readily
implemented in real business settings.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1812.07221v1,2018-12-18T08:11:00Z,2018-12-18T08:11:00Z,"Continuous Trajectory Planning Based on Learning Optimization in High
  Dimensional Input Space for Serial Manipulators","To continuously generate trajectories for serial manipulators with high
dimensional degrees of freedom (DOF) in the dynamic environment, a real-time
optimal trajectory generation method based on machine learning aiming at high
dimensional inputs is presented in this paper. First, a learning optimization
(LO) framework is established, and implementations with different sub-methods
are discussed. Additionally, multiple criteria are defined to evaluate the
performance of LO models. Furthermore, aiming at high dimensional inputs, a
database generation method based on input space dimension-reducing mapping is
proposed. At last, this method is validated on motion planning for haptic
feedback manipulators (HFM) in virtual reality systems. Results show that the
input space dimension-reducing method can significantly elevate the efficiency
and quality of database generation and consequently improve the performance of
the LO. Moreover, using this LO method, real-time trajectory generation with
high dimensional inputs can be achieved, which lays a foundation for continuous
trajectory planning for high-DOF-robots in complex environments.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1812.03078v1,2018-12-07T15:57:54Z,2018-12-07T15:57:54Z,"Evolutionary Games, Complex Networks and Nonlinear Analysis for
  Epileptic Seizures Forecasting","Epileptic seizures detection and forecasting is nowadays widely recognized as
a problem of great significance and social resonance, and still remains an
open, grand challenge. Furthermore, the development of mobile warning systems
and wearable, non invasive, advisory devices are increasingly and strongly
requested, from the patient community and their families and also from
institutional stakeholders. According to the many recent studies, exploiting
machine learning capabilities upon intracranial EEG (iEEG), in this work we
investigate a combination of novel game theory dynamical model on networks for
brain electrical activity and nonlinear time series analysis based on
recurrences quantification. These two methods are then melted together within a
supervised learning scheme and finally, prediction performances are assessed
using EEG scalp datasets, specifically recorded for this study. Our study
achieved mean sensitivity of 70.9% and a mean time in warning of 20.3%, thus
showing an increase of the improvement over chance metric from 42%, reported in
the most recent study, to 50.5%. Moreover, the real time implementation of the
proposed approach is currently under development on a prototype of a wearable
device.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1812.01885v1,2018-12-05T10:02:59Z,2018-12-05T10:02:59Z,"Improving Medical Short Text Classification with Semantic Expansion
  Using Word-Cluster Embedding","Automatic text classification (TC) research can be used for real-world
problems such as the classification of in-patient discharge summaries and
medical text reports, which is beneficial to make medical documents more
understandable to doctors. However, in electronic medical records (EMR), the
texts containing sentences are shorter than that in general domain, which leads
to the lack of semantic features and the ambiguity of semantic. To tackle this
challenge, we propose to add word-cluster embedding to deep neural network for
improving short text classification. Concretely, we first use hierarchical
agglomerative clustering to cluster the word vectors in the semantic space.
Then we calculate the cluster center vector which represents the implicit topic
information of words in the cluster. Finally, we expand word vector with
cluster center vector, and implement classifiers using CNN and LSTM
respectively. To evaluate the performance of our proposed method, we conduct
experiments on public data sets TREC and the medical short sentences data sets
which is constructed and released by us. The experimental results demonstrate
that our proposed method outperforms state-of-the-art baselines in short
sentence classification on both medical domain and general domain.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1901.06242v1,2018-12-01T13:40:03Z,2018-12-01T13:40:03Z,"Data-driven Air Quality Characterisation for Urban Environments: a Case
  Study","The economic and social impact of poor air quality in towns and cities is
increasingly being recognised, together with the need for effective ways of
creating awareness of real-time air quality levels and their impact on human
health. With local authority maintained monitoring stations being
geographically sparse and the resultant datasets also featuring missing labels,
computational data-driven mechanisms are needed to address the data sparsity
challenge. In this paper, we propose a machine learning-based method to
accurately predict the Air Quality Index (AQI), using environmental monitoring
data together with meteorological measurements. To do so, we develop an air
quality estimation framework that implements a neural network that is enhanced
with a novel Non-linear Autoregressive neural network with exogenous input
(NARX), especially designed for time series prediction. The framework is
applied to a case study featuring different monitoring sites in London, with
comparisons against other standard machine-learning based predictive algorithms
showing the feasibility and robust performance of the proposed method for
different kinds of areas within an urban region.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.12692v1,2018-11-30T09:59:20Z,2018-11-30T09:59:20Z,On the generation of probabilistic forecasts from deterministic models,"Most of the methods that produce space weather forecasts are based on
deterministic models. In order to generate a probabilistic forecast, a model
needs to be run several times sampling the input parameter space, in order to
generate an ensemble from which the distribution of outputs can be inferred.
However, ensemble simulations are costly and often preclude the possibility of
real-time forecasting. We introduce a simple and robust method to generate
uncertainties from deterministic models, that does not require ensemble
simulations. The method is based on the simple consideration that a
probabilistic forecast needs to be both accurate and well-calibrated
(reliable). We argue that these two requirements are equally important, and we
introduce the Accuracy-Reliability cost function that quantitatively measures
the trade-off between accuracy and reliability. We then define the optimal
uncertainties as the standard deviation of the Gaussian distribution that
minimizes the cost function. We demonstrate that this simple strategy,
implemented here by means of a regularized deep neural network, produces
accurate and well-calibrated forecasts, showing examples both on synthetic and
real-world space weather data.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.12425v1,2018-11-29T19:00:08Z,2018-11-29T19:00:08Z,Classifying Snapshots of the Doped Hubbard Model with Machine Learning,"Quantum gas microscopes for ultracold atoms can provide high-resolution
real-space snapshots of complex many-body systems. We implement machine
learning to analyze and classify such snapshots of ultracold atoms.
Specifically, we compare the data from an experimental realization of the
two-dimensional Fermi-Hubbard model to two theoretical approaches: a doped
quantum spin liquid state of resonating valence bond type, and the geometric
string theory, describing a state with hidden spin order. This approach
considers all available information without a potential bias towards one
particular theory by the choice of an observable and can therefore select the
theory which is more predictive in general. Up to intermediate doping values,
our algorithm tends to classify experimental snapshots as
geometric-string-like, as compared to the doped spin liquid. Our results
demonstrate the potential for machine learning in processing the wealth of data
obtained through quantum gas microscopy for new physical insights.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.11233v1,2018-11-27T20:08:28Z,2018-11-27T20:08:28Z,"Distributed traffic light control at uncoupled intersections with
  real-world topology by deep reinforcement learning","This work examines the implications of uncoupled intersections with local
real-world topology and sensor setup on traffic light control approaches.
Control approaches are evaluated with respect to: Traffic flow, fuel
consumption and noise emission at intersections.
  The real-world road network of Friedrichshafen is depicted, preprocessed and
the present traffic light controlled intersections are modeled with respect to
state space and action space.
  Different strategies, containing fixed-time, gap-based and time-based control
approaches as well as our deep reinforcement learning based control approach,
are implemented and assessed. Our novel DRL approach allows for modeling the
TLC action space, with respect to phase selection as well as selection of
transition timings. It was found that real-world topologies, and thus
irregularly arranged intersections have an influence on the performance of
traffic light control approaches. This is even to be observed within the same
intersection types (n-arm, m-phases). Moreover we could show, that these
influences can be efficiently dealt with by our deep reinforcement learning
based control approach.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.08955v1,2018-11-21T21:20:24Z,2018-11-21T21:20:24Z,"Integrating Task-Motion Planning with Reinforcement Learning for Robust
  Decision Making in Mobile Robots","Task-motion planning (TMP) addresses the problem of efficiently generating
executable and low-cost task plans in a discrete space such that the (initially
unknown) action costs are determined by motion plans in a corresponding
continuous space. However, a task-motion plan can be sensitive to unexpected
domain uncertainty and changes, leading to suboptimal behaviors or execution
failures. In this paper, we propose a novel framework, TMP-RL, which is an
integration of TMP and reinforcement learning (RL) from the execution
experience, to solve the problem of robust task-motion planning in dynamic and
uncertain domains. TMP-RL features two nested planning-learning loops. In the
inner TMP loop, the robot generates a low-cost, feasible task-motion plan by
iteratively planning in the discrete space and updating relevant action costs
evaluated by the motion planner in continuous space. In the outer loop, the
plan is executed, and the robot learns from the execution experience via
model-free RL, to further improve its task-motion plans. RL in the outer loop
is more accurate to the current domain but also more expensive, and using less
costly task and motion planning leads to a jump-start for learning in the real
world. Our approach is evaluated on a mobile service robot conducting
navigation tasks in an office area. Results show that TMP-RL approach
significantly improves adaptability and robustness (in comparison to TMP
methods) and leads to rapid convergence (in comparison to task planning (TP)-RL
methods). We also show that TMP-RL can reuse learned values to smoothly adapt
to new scenarios during long-term deployments.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1812.00825v2,2018-12-04T05:36:36Z,2018-11-21T21:02:50Z,"Microscope 2.0: An Augmented Reality Microscope with Real-time
  Artificial Intelligence Integration","The brightfield microscope is instrumental in the visual examination of both
biological and physical samples at sub-millimeter scales. One key clinical
application has been in cancer histopathology, where the microscopic assessment
of the tissue samples is used for the diagnosis and staging of cancer and thus
guides clinical therapy. However, the interpretation of these samples is
inherently subjective, resulting in significant diagnostic variability.
Moreover, in many regions of the world, access to pathologists is severely
limited due to lack of trained personnel. In this regard, Artificial
Intelligence (AI) based tools promise to improve the access and quality of
healthcare. However, despite significant advances in AI research, integration
of these tools into real-world cancer diagnosis workflows remains challenging
because of the costs of image digitization and difficulties in deploying AI
solutions. Here we propose a cost-effective solution to the integration of AI:
the Augmented Reality Microscope (ARM). The ARM overlays AI-based information
onto the current view of the sample through the optical pathway in real-time,
enabling seamless integration of AI into the regular microscopy workflow. We
demonstrate the utility of ARM in the detection of lymph node metastases in
breast cancer and the identification of prostate cancer with a latency that
supports real-time workflows. We anticipate that ARM will remove barriers
towards the use of AI in microscopic analysis and thus improve the accuracy and
efficiency of cancer diagnosis. This approach is applicable to other microscopy
tasks and AI algorithms in the life sciences and beyond.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.08716v1,2018-11-21T13:19:28Z,2018-11-21T13:19:28Z,Autonomous Dual-Arm Manipulation of Familiar Objects,"Autonomous dual-arm manipulation is an essential skill to deploy robots in
unstructured scenarios. However, this is a challenging undertaking,
particularly in terms of perception and planning. Unstructured scenarios are
full of objects with different shapes and appearances that have to be grasped
in a very specific manner so they can be functionally used. In this paper we
present an integrated approach to perform dual-arm pick tasks autonomously. Our
method consists of semantic segmentation, object pose estimation, deformable
model registration, grasp planning and arm trajectory optimization. The entire
pipeline can be executed on-board and is suitable for on-line grasping
scenarios. For this, our approach makes use of accumulated knowledge expressed
as convolutional neural network models and low-dimensional latent shape spaces.
For manipulating objects, we propose a stochastic trajectory optimization that
includes a kinematic chain closure constraint. Evaluation in simulation and on
the real robot corroborates the feasibility and applicability of the proposed
methods on a task of picking up unknown watering cans and drills using both
arms.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.07315v1,2018-11-18T11:28:24Z,2018-11-18T11:28:24Z,"Learning to infer: RL-based search for DNN primitive selection on
  Heterogeneous Embedded Systems","Deep Learning is increasingly being adopted by industry for computer vision
applications running on embedded devices. While Convolutional Neural Networks'
accuracy has achieved a mature and remarkable state, inference latency and
throughput are a major concern especially when targeting low-cost and low-power
embedded platforms. CNNs' inference latency may become a bottleneck for Deep
Learning adoption by industry, as it is a crucial specification for many
real-time processes. Furthermore, deployment of CNNs across heterogeneous
platforms presents major compatibility issues due to vendor-specific technology
and acceleration libraries. In this work, we present QS-DNN, a fully automatic
search based on Reinforcement Learning which, combined with an inference engine
optimizer, efficiently explores through the design space and empirically finds
the optimal combinations of libraries and primitives to speed up the inference
of CNNs on heterogeneous embedded devices. We show that, an optimized
combination can achieve 45x speedup in inference latency on CPU compared to a
dependency-free baseline and 2x on average on GPGPU compared to the best vendor
library. Further, we demonstrate that, the quality of results and time
""to-solution"" is much better than with Random Search and achieves up to 15x
better results for a short-time search.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.07234v1,2018-11-17T22:21:12Z,2018-11-17T22:21:12Z,"Improving Automatic Source Code Summarization via Deep Reinforcement
  Learning","Code summarization provides a high level natural language description of the
function performed by code, as it can benefit the software maintenance, code
categorization and retrieval. To the best of our knowledge, most
state-of-the-art approaches follow an encoder-decoder framework which encodes
the code into a hidden space and then decode it into natural language space,
suffering from two major drawbacks: a) Their encoders only consider the
sequential content of code, ignoring the tree structure which is also critical
for the task of code summarization, b) Their decoders are typically trained to
predict the next word by maximizing the likelihood of next ground-truth word
with previous ground-truth word given. However, it is expected to generate the
entire sequence from scratch at test time. This discrepancy can cause an
\textit{exposure bias} issue, making the learnt decoder suboptimal. In this
paper, we incorporate an abstract syntax tree structure as well as sequential
content of code snippets into a deep reinforcement learning framework (i.e.,
actor-critic network). The actor network provides the confidence of predicting
the next word according to current state. On the other hand, the critic network
evaluates the reward value of all possible extensions of the current state and
can provide global guidance for explorations. We employ an advantage reward
composed of BLEU metric to train both networks. Comprehensive experiments on a
real-world dataset show the effectiveness of our proposed model when compared
with some state-of-the-art methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.06885v2,2019-03-25T07:05:04Z,2018-11-16T16:07:23Z,"A Generalized Meta-loss function for regression and classification using
  privileged information","Learning using privileged information (LUPI) is a powerful heterogenous
feature space machine learning framework that allows a machine learning model
to learn from highly informative or privileged features which are available
during training only to generate test predictions using input space features
which are available both during training and testing. LUPI can significantly
improve prediction performance in a variety of machine learning problems.
However, existing large margin and neural network implementations of learning
using privileged information are mostly designed for classification tasks. In
this work, we have proposed a simple yet effective formulation that allows us
to perform regression using privileged information through a custom loss
function. Apart from regression, our formulation allows general application of
LUPI to classification and other related problems as well. We have verified the
correctness, applicability and effectiveness of our method on regression and
classification problems over different synthetic and real-world problems. To
test the usefulness of the proposed model in real-world problems, we have
evaluated our method on the problem of protein binding affinity prediction. The
proposed LUPI regression-based model has shown to outperform the current
state-of-the-art predictor.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.08270v2,2019-02-25T06:50:27Z,2018-11-11T14:15:23Z,Graph Convolutional Neural Networks via Motif-based Attention,"Many real-world problems can be represented as graph-based learning problems.
In this paper, we propose a novel framework for learning spatial and
attentional convolution neural networks on arbitrary graphs. Different from
previous convolutional neural networks on graphs, we first design a
motif-matching guided subgraph normalization method to capture neighborhood
information. Then we implement subgraph-level self-attentional layers to learn
different importances from different subgraphs to solve graph classification
problems. Analogous to image-based attentional convolution networks that
operate on locally connected and weighted regions of the input, we also extend
graph normalization from one-dimensional node sequence to two-dimensional node
grid by leveraging motif-matching, and design self-attentional layers without
requiring any kinds of cost depending on prior knowledge of the graph
structure. Our results on both bioinformatics and social network datasets show
that we can significantly improve graph classification benchmarks over
traditional graph kernel and existing deep models.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.03934v1,2018-11-09T14:46:40Z,2018-11-09T14:46:40Z,"RadIoT: Radio Communications Intrusion Detection for IoT - A Protocol
  Independent Approach","Internet-of-Things (IoT) devices are nowadays massively integrated in daily
life: homes, factories, or public places. This technology offers attractive
services to improve the quality of life as well as new economic markets through
the exploitation of the collected data. However, these connected objects have
also become attractive targets for attackers because their current security
design is often weak or flawed, as illustrated by several vulnerabilities such
as Mirai, Blueborne, etc. This paper presents a novel approach for detecting
intrusions in smart spaces such as smarthomes, or smartfactories, that is based
on the monitoring and profiling of radio communications at the physical layer
using machine learning techniques. The approach is designed to be independent
of the large and heterogeneous set of wireless communication protocols
typically implemented by connected objects such as WiFi, Bluetooth, Zigbee,
Bluetooth-Low-Energy (BLE) or proprietary communication protocols. The main
concepts of the proposed approach are presented together with an experimental
case study illustrating its feasibility based on data collected during the
deployment of the intrusion detection approach in a smart home under real-life
conditions.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.03909v2,2018-12-15T19:39:04Z,2018-11-09T14:10:18Z,"Evidence Transfer for Improving Clustering Tasks Using External
  Categorical Evidence","In this paper we introduce evidence transfer for clustering, a deep learning
method that can incrementally manipulate the latent representations of an
autoencoder, according to external categorical evidence, in order to improve a
clustering outcome. By evidence transfer we define the process by which the
categorical outcome of an external, auxiliary task is exploited to improve a
primary task, in this case representation learning for clustering. Our proposed
method makes no assumptions regarding the categorical evidence presented, nor
the structure of the latent space. We compare our method, against the baseline
solution by performing k-means clustering before and after its deployment.
Experiments with three different kinds of evidence show that our method
effectively manipulates the latent representations when introduced with real
corresponding evidence, while remaining robust when presented with low quality
evidence.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.02872v2,2018-11-12T19:23:15Z,2018-11-07T13:30:40Z,Baselines for Reinforcement Learning in Text Games,"The ability to learn optimal control policies in systems where action space
is defined by sentences in natural language would allow many interesting
real-world applications such as automatic optimisation of dialogue systems.
Text-based games with multiple endings and rewards are a promising platform for
this task, since their feedback allows us to employ reinforcement learning
techniques to jointly learn text representations and control policies. We argue
that the key property of AI agents, especially in the text-games context, is
their ability to generalise to previously unseen games. We present a
minimalistic text-game playing agent, testing its generalisation and transfer
learning performance and showing its ability to play multiple games at once. We
also present pyfiction, an open-source library for universal access to
different text games that could, together with our agent that implements its
interface, serve as a baseline for future research.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1811.02213v1,2018-11-06T08:05:24Z,2018-11-06T08:05:24Z,"Hybrid Approach to Automation, RPA and Machine Learning: a Method for
  the Human-centered Design of Software Robots","One of the more prominent trends within Industry 4.0 is the drive to employ
Robotic Process Automation (RPA), especially as one of the elements of the Lean
approach. The full implementation of RPA is riddled with challenges relating
both to the reality of everyday business operations, from SMEs to SSCs and
beyond, and the social effects of the changing job market. To successfully
address these points there is a need to develop a solution that would adjust to
the existing business operations and at the same time lower the negative social
impact of the automation process.
  To achieve these goals we propose a hybrid, human-centered approach to the
development of software robots. This design and implementation method combines
the Living Lab approach with empowerment through participatory design to
kick-start the co-development and co-maintenance of hybrid software robots
which, supported by variety of AI methods and tools, including interactive and
collaborative ML in the cloud, transform menial job posts into higher-skilled
positions, allowing former employees to stay on as robot co-designers and
maintainers, i.e. as co-programmers who supervise the machine learning
processes with the use of tailored high-level RPA Domain Specific Languages
(DSLs) to adjust the functioning of the robots and maintain operational
flexibility.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.11063v1,2018-10-25T18:42:01Z,2018-10-25T18:42:01Z,Sorry: Ambient Tactical Deception Via Malware-Based Social Engineering,"In this paper we argue, drawing from the perspectives of cybersecurity and
social psychology, that Internet-based manipulation of an individual or group
reality using ambient tactical deception is possible using only software and
changing words in a web browser. We call this attack Ambient Tactical Deception
(ATD). Ambient, in artificial intelligence, describes software that is
""unobtrusive,"" and completely integrated into a user's life. Tactical deception
is an information warfare term for the use of deception on an opposing force.
We suggest that an ATD attack could change the sentiment of text in a web
browser. This could alter the victim's perception of reality by providing
disinformation. Within the limit of online communication, even a pause in
replying to a text can affect how people perceive each other. The outcomes of
an ATD attack could include alienation, upsetting a victim, and influencing
their feelings about an election, a spouse, or a corporation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.10335v1,2018-10-24T12:27:33Z,2018-10-24T12:27:33Z,Emulating quantum computation with artificial neural networks,"We demonstrate, that artificial neural networks (ANN) can be trained to
emulate single or multiple basic quantum operations. In order to realize a
quantum state, we implement a novel ""quantumness gate"" that maps an arbitrary
matrix to the real representation of a positive hermitean normalized density
matrix. We train the CNOT gate, the Hadamard gate and a rotation in Hilbert
space as basic building blocks for processing the quantum density matrices of
two entangled qubits. During the training process the neural networks learn to
represent the complex structure, the hermiticity, the normalization and the
positivity of the output matrix. The requirement of successful training allows
us to find a critical bottleneck dimension which reflects the relevant quantum
information. Chains of individually trained neural quantum gates can be
constructed to realize any unitary transformation. For scaling to larger
quantum systems, we propose to use correlations of stochastic macroscopic
two-level observables or classical bits. This novel concept provides a path for
a classical implementation of computationally relevant quantum information
processing on classical neural networks, in particular on neuromorphic
computing machines featuring stochastic operations.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.08517v1,2018-10-19T14:21:14Z,2018-10-19T14:21:14Z,"Developing a seismic pattern interpretation network (SpiNet) for
  automated seismic interpretation","Seismic interpretation is now serving as a fundamental tool for depicting
subsurface geology and assisting activities in various domains, such as
environmental engineering and petroleum exploration. However, most of the
existing interpretation techniques are designed for interpreting a certain
seismic pattern (e.g., faults and salt domes) in a given seismic dataset at one
time; correspondingly, the rest patterns would be ignored. Interpreting all the
important seismic patterns becomes feasible with the aid of multiple
classification techniques. When implementing them into the seismic domain,
however, the major drawback is the low efficiency particularly for a large
dataset, since the classification need to be repeated at every seismic sample.
To resolve such limitation, this study first present a seismic pattern
interpretation dataset (SpiDat), which tentatively categorizes 12
commonly-observed seismic patterns based on their signal intensity and lateral
geometry, including these of important geologic implications such as faults,
salt domes, gas chimneys, and depositional sequences. Then we propose a seismic
pattern interpretation network (SpiNet) based on the state-of-the-art
deconvolutional neural network, which is capable of automatically recognizing
and annotating the 12 defined seismic patterns in real time. The impacts of the
proposed SpiNet come in two folds. First, applying the SpiNet to a seismic cube
allows interpreters to quickly identify the important seismic patterns as input
to advanced interpretation and modeling. Second, the SpiNet paves the
foundation for deriving more task-oriented seismic interpretation networks,
such as fault detection. It is concluded that the proposed SpiNet holds great
potentials for assisting the major seismic interpretation challenges and
advancing it further towards cognitive seismic data analysis.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.06637v2,2019-05-02T15:03:20Z,2018-10-15T19:46:00Z,"Nonlinear System Identification of Soft Robot Dynamics Using Koopman
  Operator Theory","Soft robots are challenging to model due in large part to the nonlinear
properties of soft materials. Fortunately, this softness makes it possible to
safely observe their behavior under random control inputs, making them amenable
to large-scale data collection and system identification. This paper implements
and evaluates a system identification method based on Koopman operator theory
in which models of nonlinear dynamical systems are constructed via linear
regression of observed data by exploiting the fact that every nonlinear system
has a linear representation in the infinite-dimensional space of real-valued
functions called observables. The approach does not suffer from some of the
shortcomings of other nonlinear system identification methods, which typically
require the manual tuning of training parameters and have limited convergence
guarantees. A dynamic model of a pneumatic soft robot arm is constructed via
this method, and used to predict the behavior of the real system. The total
normalized-root-mean-square error (NRMSE) of its predictions is lower than that
of several other identified models including a neural network, NLARX, nonlinear
Hammerstein-Wiener, and linear state space model.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.05236v3,2019-07-24T22:33:56Z,2018-10-11T20:23:57Z,Practical Design Space Exploration,"Multi-objective optimization is a crucial matter in computer systems design
space exploration because real-world applications often rely on a trade-off
between several objectives. Derivatives are usually not available or
impractical to compute and the feasibility of an experiment can not always be
determined in advance. These problems are particularly difficult when the
feasible region is relatively small, and it may be prohibitive to even find a
feasible experiment, let alone an optimal one.
  We introduce a new methodology and corresponding software framework,
HyperMapper 2.0, which handles multi-objective optimization, unknown
feasibility constraints, and categorical/ordinal variables. This new
methodology also supports injection of the user prior knowledge in the search
when available. All of these features are common requirements in computer
systems but rarely exposed in existing design space exploration systems. The
proposed methodology follows a white-box model which is simple to understand
and interpret (unlike, for example, neural networks) and can be used by the
user to better understand the results of the automatic search.
  We apply and evaluate the new methodology to the automatic static tuning of
hardware accelerators within the recently introduced Spatial programming
language, with minimization of design run-time and compute logic under the
constraint of the design fitting in a target field-programmable gate array
chip. Our results show that HyperMapper 2.0 provides better Pareto fronts
compared to state-of-the-art baselines, with better or competitive hypervolume
indicator and with 8x improvement in sampling budget for most of the benchmarks
explored.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.04793v3,2018-10-25T13:38:34Z,2018-10-10T16:41:05Z,"Patient2Vec: A Personalized Interpretable Deep Representation of the
  Longitudinal Electronic Health Record","The wide implementation of electronic health record (EHR) systems facilitates
the collection of large-scale health data from real clinical settings. Despite
the significant increase in adoption of EHR systems, this data remains largely
unexplored, but presents a rich data source for knowledge discovery from
patient health histories in tasks such as understanding disease correlations
and predicting health outcomes. However, the heterogeneity, sparsity, noise,
and bias in this data present many complex challenges. This complexity makes it
difficult to translate potentially relevant information into machine learning
algorithms. In this paper, we propose a computational framework, Patient2Vec,
to learn an interpretable deep representation of longitudinal EHR data which is
personalized for each patient. To evaluate this approach, we apply it to the
prediction of future hospitalizations using real EHR data and compare its
predictive performance with baseline methods. Patient2Vec produces a vector
space with meaningful structure and it achieves an AUC around 0.799
outperforming baseline methods. In the end, the learned feature importance can
be visualized and interpreted at both the individual and population levels to
bring clinical insights.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.03190v1,2018-10-07T17:59:49Z,2018-10-07T17:59:49Z,"Scalable Solutions for Automated Single Pulse Identification and
  Classification in Radio Astronomy","Data collection for scientific applications is increasing exponentially and
is forecasted to soon reach peta- and exabyte scales. Applications which
process and analyze scientific data must be scalable and focus on execution
performance to keep pace. In the field of radio astronomy, in addition to
increasingly large datasets, tasks such as the identification of transient
radio signals from extrasolar sources are computationally expensive. We present
a scalable approach to radio pulsar detection written in Scala that
parallelizes candidate identification to take advantage of in-memory task
processing using Apache Spark on a YARN distributed system. Furthermore, we
introduce a novel automated multiclass supervised machine learning technique
that we combine with feature selection to reduce the time required for
candidate classification. Experimental testing on a Beowulf cluster with 15
data nodes shows that the parallel implementation of the identification
algorithm offers a speedup of up to 5X that of a similar multithreaded
implementation. Further, we show that the combination of automated multiclass
classification and feature selection speeds up the execution performance of the
RandomForest machine learning algorithm by an average of 54% with less than a
2% average reduction in the algorithm's ability to correctly classify pulsars.
The generalizability of these results is demonstrated by using two real-world
radio astronomy data sets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.03032v1,2018-10-06T17:55:26Z,2018-10-06T17:55:26Z,"Constructing Graph Node Embeddings via Discrimination of Similarity
  Distributions","The problem of unsupervised learning node embeddings in graphs is one of the
important directions in modern network science. In this work we propose a novel
framework, which is aimed to find embeddings by \textit{discriminating
distributions of similarities (DDoS)} between nodes in the graph. The general
idea is implemented by maximizing the \textit{earth mover distance} between
distributions of decoded similarities of similar and dissimilar nodes. The
resulting algorithm generates embeddings which give a state-of-the-art
performance in the problem of link prediction in real-world graphs.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.02648v3,2019-01-25T15:37:49Z,2018-10-05T12:39:37Z,LiveCap: Real-time Human Performance Capture from Monocular Video,"We present the first real-time human performance capture approach that
reconstructs dense, space-time coherent deforming geometry of entire humans in
general everyday clothing from just a single RGB video. We propose a novel
two-stage analysis-by-synthesis optimization whose formulation and
implementation are designed for high performance. In the first stage, a skinned
template model is jointly fitted to background subtracted input video, 2D and
3D skeleton joint positions found using a deep neural network, and a set of
sparse facial landmark detections. In the second stage, dense non-rigid 3D
deformations of skin and even loose apparel are captured based on a novel
real-time capable algorithm for non-rigid tracking using dense photometric and
silhouette constraints. Our novel energy formulation leverages automatically
identified material regions on the template to model the differing non-rigid
deformation behavior of skin and apparel. The two resulting non-linear
optimization problems per-frame are solved with specially-tailored
data-parallel Gauss-Newton solvers. In order to achieve real-time performance
of over 25Hz, we design a pipelined parallel architecture using the CPU and two
commodity GPUs. Our method is the first real-time monocular approach for
full-body performance capture. Our method yields comparable accuracy with
off-line performance capture techniques, while being orders of magnitude
faster.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1810.02688v2,2018-10-19T13:07:57Z,2018-09-28T08:27:59Z,Wikistat 2.0: Educational Resources for Artificial Intelligence,"Big data, data science, deep learning, artificial intelligence are the key
words of intense hype related with a job market in full evolution, that impose
to adapt the contents of our university professional trainings. Which
artificial intelligence is mostly concerned by the job offers? Which
methodologies and technologies should be favored in the training programs?
Which objectives, tools and educational resources do we needed to put in place
to meet these pressing needs? We answer these questions in describing the
contents and operational resources in the Data Science orientation of the
specialty Applied Mathematics at INSA Toulouse. We focus on basic mathematics
training (Optimization, Probability, Statistics), associated with the practical
implementation of the most performing statistical learning algorithms, with the
most appropriate technologies and on real examples. Considering the huge
volatility of the technologies, it is imperative to train students in
seft-training, this will be their technological watch tool when they will be in
professional activity. This explains the structuring of the educational site
github.com/wikistat into a set of tutorials. Finally, to motivate the thorough
practice of these tutorials, a serious game is organized each year in the form
of a prediction contest between students of Master degrees in Applied
Mathematics for IA.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1809.07763v4,2020-05-26T15:15:19Z,2018-09-19T19:14:46Z,"auditor: an R Package for Model-Agnostic Visual Validation and
  Diagnostics","Machine learning models have spread to almost every area of life. They are
successfully applied in biology, medicine, finance, physics, and other fields.
With modern software it is easy to train even a~complex model that fits the
training data and results in high accuracy on the test set. The problem arises
when models fail confronted with real-world data.
  This paper describes methodology and tools for model-agnostic audit.
Introduced techniques facilitate assessing and comparing the goodness of fit
and performance of models. In~addition, they may be used for the analysis of
the similarity of residuals and for identification of~outliers and influential
observations. The examination is carried out by diagnostic scores and visual
verification.
  Presented methods were implemented in the auditor package for R. Due to
flexible and~consistent grammar, it is simple to validate models of any
classes.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1809.03548v2,2018-09-14T11:39:07Z,2018-09-10T18:55:19Z,VPE: Variational Policy Embedding for Transfer Reinforcement Learning,"Reinforcement Learning methods are capable of solving complex problems, but
resulting policies might perform poorly in environments that are even slightly
different. In robotics especially, training and deployment conditions often
vary and data collection is expensive, making retraining undesirable.
Simulation training allows for feasible training times, but on the other hand
suffers from a reality-gap when applied in real-world settings. This raises the
need of efficient adaptation of policies acting in new environments. We
consider this as a problem of transferring knowledge within a family of similar
Markov decision processes.
  For this purpose we assume that Q-functions are generated by some
low-dimensional latent variable. Given such a Q-function, we can find a master
policy that can adapt given different values of this latent variable. Our
method learns both the generative mapping and an approximate posterior of the
latent variables, enabling identification of policies for new tasks by
searching only in the latent space, rather than the space of all policies. The
low-dimensional space, and master policy found by our method enables policies
to quickly adapt to new environments. We demonstrate the method on both a
pendulum swing-up task in simulation, and for simulation-to-real transfer on a
pushing task.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1809.02797v2,2018-09-15T08:31:50Z,2018-09-08T13:08:26Z,Fast Gradient Attack on Network Embedding,"Network embedding maps a network into a low-dimensional Euclidean space, and
thus facilitate many network analysis tasks, such as node classification, link
prediction and community detection etc, by utilizing machine learning methods.
In social networks, we may pay special attention to user privacy, and would
like to prevent some target nodes from being identified by such network
analysis methods in certain cases. Inspired by successful adversarial attack on
deep learning models, we propose a framework to generate adversarial networks
based on the gradient information in Graph Convolutional Network (GCN). In
particular, we extract the gradient of pairwise nodes based on the adversarial
network, and select the pair of nodes with maximum absolute gradient to realize
the Fast Gradient Attack (FGA) and update the adversarial network. This process
is implemented iteratively and terminated until certain condition is satisfied,
i.e., the number of modified links reaches certain predefined value.
Comprehensive attacks, including unlimited attack, direct attack and indirect
attack, are performed on six well-known network embedding methods. The
experiments on real-world networks suggest that our proposed FGA behaves better
than some baseline methods, i.e., the network embedding can be easily disturbed
using FGA by only rewiring few links, achieving state-of-the-art attack
performance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1809.02270v5,2020-11-26T09:40:25Z,2018-09-07T01:33:13Z,"Learning Embeddings of Directed Networks with Text-Associated
  Nodes---with Applications in Software Package Dependency Networks","A network embedding consists of a vector representation for each node in the
network. Its usefulness has been shown in many real-world application domains,
such as social networks and web networks. Directed networks with text
associated with each node, such as software package dependency networks, are
commonplace. However, to the best of our knowledge, their embeddings have
hitherto not been specifically studied. In this paper, we propose PCTADW-1 and
PCTADW-2, two algorithms based on neural networks that learn embeddings of
directed networks with text associated with each node. We create two new
node-labeled such networks: The package dependency networks in two popular
GNU/Linux distributions, Debian and Fedora. We experimentally demonstrate that
the embeddings produced by our algorithms resulted in node classification with
better quality than those of various baselines on these two networks. We
observe that there exist systematic presence of analogies (similar to those in
word embeddings) in the network embeddings of software package dependency
networks. To the best of our knowledge, this is the first time that such
systematic presence of analogies is observed in network and document
embeddings. We further demonstrate that these network embeddings can be novelly
used for better understanding software attributes, such as the development
process and user interface of software, etc.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1809.05193v1,2018-08-31T20:52:10Z,2018-08-31T20:52:10Z,"Context2Name: A Deep Learning-Based Approach to Infer Natural Variable
  Names from Usage Contexts","Most of the JavaScript code deployed in the wild has been minified, a process
in which identifier names are replaced with short, arbitrary and meaningless
names. Minified code occupies less space, but also makes the code extremely
difficult to manually inspect and understand. This paper presents Context2Name,
a deep learningbased technique that partially reverses the effect of
minification by predicting natural identifier names for minified names. The
core idea is to predict from the usage context of a variable a name that
captures the meaning of the variable. The approach combines a lightweight,
token-based static analysis with an auto-encoder neural network that summarizes
usage contexts and a recurrent neural network that predict natural names for a
given usage context. We evaluate Context2Name with a large corpus of real-world
JavaScript code and show that it successfully predicts 47.5% of all minified
identifiers while taking only 2.9 milliseconds on average to predict a name. A
comparison with the state-of-the-art tools JSNice and JSNaughty shows that our
approach performs comparably in terms of accuracy while improving in terms of
efficiency. Moreover, Context2Name complements the state-of-the-art by
predicting 5.3% additional identifiers that are missed by both existing tools.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1808.07840v1,2018-08-23T16:55:53Z,2018-08-23T16:55:53Z,Learning to Importance Sample in Primary Sample Space,"Importance sampling is one of the most widely used variance reduction
strategies in Monte Carlo rendering. In this paper, we propose a novel
importance sampling technique that uses a neural network to learn how to sample
from a desired density represented by a set of samples. Our approach considers
an existing Monte Carlo rendering algorithm as a black box. During a
scene-dependent training phase, we learn to generate samples with a desired
density in the primary sample space of the rendering algorithm using maximum
likelihood estimation. We leverage a recent neural network architecture that
was designed to represent real-valued non-volume preserving ('Real NVP')
transformations in high dimensional spaces. We use Real NVP to non-linearly
warp primary sample space and obtain desired densities. In addition, Real NVP
efficiently computes the determinant of the Jacobian of the warp, which is
required to implement the change of integration variables implied by the warp.
A main advantage of our approach is that it is agnostic of underlying light
transport effects, and can be combined with many existing rendering techniques
by treating them as a black box. We show that our approach leads to effective
variance reduction in several practical scenarios.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1808.06352v1,2018-08-20T09:06:21Z,2018-08-20T09:06:21Z,"Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality","Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1808.06277v1,2018-08-20T00:54:29Z,2018-08-20T00:54:29Z,An Efficient Approach for Geo-Multimedia Cross-Modal Retrieval,"Due to the rapid development of mobile Internet techniques, cloud computation
and popularity of online social networking and location-based services, massive
amount of multimedia data with geographical information is generated and
uploaded to the Internet. In this paper, we propose a novel type of cross-modal
multimedia retrieval called geo-multimedia cross-modal retrieval which aims to
search out a set of geo-multimedia objects based on geographical distance
proximity and semantic similarity between different modalities. Previous
studies for cross-modal retrieval and spatial keyword search cannot address
this problem effectively because they do not consider multimedia data with
geo-tags and do not focus on this type of query. In order to address this
problem efficiently, we present the definition of $k$NN geo-multimedia
cross-modal query at the first time and introduce relevant conceptions such as
cross-modal semantic representation space. To bridge the semantic gap between
different modalities, we propose a method named cross-modal semantic matching
which contains two important component, i.e., CorrProj and LogsTran, which aims
to construct a common semantic representation space for cross-modal semantic
similarity measurement. Besides, we designed a framework based on deep learning
techniques to implement common semantic representation space construction. In
addition, a novel hybrid indexing structure named GMR-Tree combining
geo-multimedia data and R-Tree is presented and a efficient $k$NN search
algorithm called $k$GMCMS is designed. Comprehensive experimental evaluation on
real and synthetic dataset clearly demonstrates that our solution outperforms
the-state-of-the-art methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1808.04511v1,2018-08-14T03:00:29Z,2018-08-14T03:00:29Z,A Record Linkage Model Incorporating Relational Data,"In this paper we introduce a novel Bayesian approach for linking multiple
social networks in order to discover the same real world person having
different accounts across networks. In particular, we develop a latent model
that allow us to jointly characterize the network and linkage structures
relying in both relational and profile data. In contrast to other existing
approaches in the machine learning literature, our Bayesian implementation
naturally provides uncertainty quantification via posterior probabilities for
the linkage structure itself or any function of it. Our findings clearly
suggest that our methodology can produce accurate point estimates of the
linkage structure even in the absence of profile information, and also, in an
identity resolution setting, our results confirm that including relational data
into the matching process improves the linkage accuracy. We illustrate our
methodology using real data from popular social networks such as Twitter,
Facebook, and YouTube.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1808.03941v2,2019-05-05T03:42:19Z,2018-08-12T13:30:27Z,"Denoising of 3-D Magnetic Resonance Images Using a Residual
  Encoder-Decoder Wasserstein Generative Adversarial Network","Structure-preserved denoising of 3D magnetic resonance imaging (MRI) images
is a critical step in medical image analysis. Over the past few years, many
algorithms with impressive performances have been proposed. In this paper,
inspired by the idea of deep learning, we introduce an MRI denoising method
based on the residual encoder-decoder Wasserstein generative adversarial
network (RED-WGAN). Specifically, to explore the structure similarity between
neighboring slices, a 3D configuration is utilized as the basic processing
unit. Residual autoencoders combined with deconvolution operations are
introduced into the generator network. Furthermore, to alleviate the
oversmoothing shortcoming of the traditional mean squared error (MSE) loss
function, the perceptual similarity, which is implemented by calculating the
distances in the feature space extracted by a pretrained VGG-19 network, is
incorporated with the MSE and adversarial losses to form the new loss function.
Extensive experiments are implemented to assess the performance of the proposed
method. The experimental results show that the proposed RED-WGAN achieves
performance superior to several state-of-the-art methods in both simulated and
real clinical data. In particular, our method demonstrates powerful abilities
in both noise suppression and structure preservation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1807.05211v1,2018-07-11T11:05:12Z,2018-07-11T11:05:12Z,"Learning Deployable Navigation Policies at Kilometer Scale from a Single
  Traversal","Model-free reinforcement learning has recently been shown to be effective at
learning navigation policies from complex image input. However, these
algorithms tend to require large amounts of interaction with the environment,
which can be prohibitively costly to obtain on robots in the real world. We
present an approach for efficiently learning goal-directed navigation policies
on a mobile robot, from only a single coverage traversal of recorded data. The
navigation agent learns an effective policy over a diverse action space in a
large heterogeneous environment consisting of more than 2km of travel, through
buildings and outdoor regions that collectively exhibit large variations in
visual appearance, self-similarity, and connectivity. We compare pretrained
visual encoders that enable precomputation of visual embeddings to achieve a
throughput of tens of thousands of transitions per second at training time on a
commodity desktop computer, allowing agents to learn from millions of
trajectories of experience in a matter of hours. We propose multiple forms of
computationally efficient stochastic augmentation to enable the learned policy
to generalise beyond these precomputed embeddings, and demonstrate successful
deployment of the learned policy on the real robot without fine tuning, despite
environmental appearance differences at test time. The dataset and code
required to reproduce these results and apply the technique to other datasets
and robots is made publicly available at rl-navigation.github.io/deployable.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1807.03873v2,2018-07-13T00:19:43Z,2018-07-10T21:36:23Z,Automatic Gradient Boosting,"Automatic machine learning performs predictive modeling with high performing
machine learning tools without human interference. This is achieved by making
machine learning applications parameter-free, i.e. only a dataset is provided
while the complete model selection and model building process is handled
internally through (often meta) optimization. Projects like Auto-WEKA and
auto-sklearn aim to solve the Combined Algorithm Selection and Hyperparameter
optimization (CASH) problem resulting in huge configuration spaces. However,
for most real-world applications, the optimization over only a few different
key learning algorithms can not only be sufficient, but also potentially
beneficial. The latter becomes apparent when one considers that models have to
be validated, explained, deployed and maintained. Here, less complex model are
often preferred, for validation or efficiency reasons, or even a strict
requirement. Automatic gradient boosting simplifies this idea one step further,
using only gradient boosting as a single learning algorithm in combination with
model-based hyperparameter tuning, threshold optimization and encoding of
categorical features. We introduce this general framework as well as a concrete
implementation called autoxgboost. It is compared to current AutoML projects on
16 datasets and despite its simplicity is able to achieve comparable results on
about half of the datasets as well as performing best on two.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1806.10409v1,2018-06-27T10:56:55Z,2018-06-27T10:56:55Z,"A neuro-inspired system for online learning and recognition of parallel
  spike trains, based on spike latency and heterosynaptic STDP","Humans perform remarkably well in many cognitive tasks including pattern
recognition. However, the neuronal mechanisms underlying this process are not
well understood. Nevertheless, artificial neural networks, inspired in brain
circuits, have been designed and used to tackle spatio-temporal pattern
recognition tasks. In this paper we present a multineuronal spike pattern
detection structure able to autonomously implement online learning and
recognition of parallel spike sequences (i.e., sequences of pulses belonging to
different neurons/neural ensembles). The operating principle of this structure
is based on two spiking/synaptic neurocomputational characteristics: spike
latency, that enables neurons to fire spikes with a certain delay and
heterosynaptic plasticity, that allows the own regulation of synaptic weights.
From the perspective of the information representation, the structure allows
mapping a spatio-temporal stimulus into a multidimensional, temporal, feature
space. In this space, the parameter coordinate and the time at which a neuron
fires represent one specific feature. In this sense, each feature can be
considered to span a single temporal axis. We applied our proposed scheme to
experimental data obtained from a motor inhibitory cognitive task. The test
exhibits good classification performance, indicating the adequateness of our
approach. In addition to its effectiveness, its simplicity and low
computational cost suggest a large scale implementation for real time
recognition applications in several areas, such as brain computer interface,
personal biometrics authentication or early detection of diseases.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1806.07851v2,2018-10-08T01:32:02Z,2018-06-20T17:22:12Z,Sim-to-Real Reinforcement Learning for Deformable Object Manipulation,"We have seen much recent progress in rigid object manipulation, but
interaction with deformable objects has notably lagged behind. Due to the large
configuration space of deformable objects, solutions using traditional
modelling approaches require significant engineering work. Perhaps then,
bypassing the need for explicit modelling and instead learning the control in
an end-to-end manner serves as a better approach? Despite the growing interest
in the use of end-to-end robot learning approaches, only a small amount of work
has focused on their applicability to deformable object manipulation. Moreover,
due to the large amount of data needed to learn these end-to-end solutions, an
emerging trend is to learn control policies in simulation and then transfer
them over to the real world. To-date, no work has explored whether it is
possible to learn and transfer deformable object policies. We believe that if
sim-to-real methods are to be employed further, then it should be possible to
learn to interact with a wide variety of objects, and not only rigid objects.
In this work, we use a combination of state-of-the-art deep reinforcement
learning algorithms to solve the problem of manipulating deformable objects
(specifically cloth). We evaluate our approach on three tasks --- folding a
towel up to a mark, folding a face towel diagonally, and draping a piece of
cloth over a hanger. Our agents are fully trained in simulation with domain
randomisation, and then successfully deployed in the real world without having
seen any real deformable objects.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1806.06876v3,2019-03-24T05:20:22Z,2018-06-18T18:24:16Z,"Diving Deep onto Discriminative Ensemble of Histological Hashing &
  Class-Specific Manifold Learning for Multi-class Breast Carcinoma Taxonomy","Histopathological images (HI) encrypt resolution dependent heterogeneous
textures & diverse color distribution variability, manifesting in
micro-structural surface tissue convolutions. Also, inherently high coherency
of cancerous cells poses significant challenges to breast cancer (BC)
multi-classification. As such, multi-class stratification is sparsely explored
& prior work mainly focus on benign & malignant tissue characterization only,
which forestalls further quantitative analysis of subordinate classes like
adenosis, mucinous carcinoma & fibroadenoma etc, for diagnostic competence. In
this work, a fully-automated, near-real-time & computationally inexpensive
robust multi-classification deep framework from HI is presented.
  The proposed scheme employs deep neural network (DNN) aided discriminative
ensemble of holistic class-specific manifold learning (CSML) for underlying HI
sub-space embedding & HI hashing based local shallow signatures. The model
achieves 95.8% accuracy pertinent to multi-classification & 2.8% overall
performance improvement & 38.2% enhancement for Lobular carcinoma (LC)
sub-class recognition rate as compared to the existing state-of-the-art on well
known BreakHis dataset is achieved. Also, 99.3% recognition rate at 200X & a
sensitivity of 100% for binary grading at all magnification validates its
suitability for clinical deployment in hand-held smart devices.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1806.06671v1,2018-06-18T13:43:51Z,2018-06-18T13:43:51Z,"Where to Go Next: A Spatio-temporal LSTM model for Next POI
  Recommendation","Next Point-of-Interest (POI) recommendation is of great value for both
location-based service providers and users. Recently Recurrent Neural Networks
(RNNs) have been proved to be effective on sequential recommendation tasks.
However, existing RNN solutions rarely consider the spatio-temporal intervals
between neighbor check-ins, which are essential for modeling user check-in
behaviors in next POI recommendation. In this paper, we propose a new variant
of LSTM, named STLSTM, which implements time gates and distance gates into LSTM
to capture the spatio-temporal relation between successive check-ins.
Specifically, one-time gate and one distance gate are designed to control
short-term interest update, and another time gate and distance gate are
designed to control long-term interest update. Furthermore, to reduce the
number of parameters and improve efficiency, we further integrate coupled input
and forget gates with our proposed model. Finally, we evaluate the proposed
model using four real-world datasets from various location-based social
networks. Our experimental results show that our model significantly
outperforms the state-of-the-art approaches for next POI recommendation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1806.03600v2,2018-06-12T11:16:34Z,2018-06-10T07:29:41Z,"ML + FV = $\heartsuit$? A Survey on the Application of Machine Learning
  to Formal Verification","Formal Verification (FV) and Machine Learning (ML) can seem incompatible due
to their opposite mathematical foundations and their use in real-life problems:
FV mostly relies on discrete mathematics and aims at ensuring correctness; ML
often relies on probabilistic models and consists of learning patterns from
training data. In this paper, we postulate that they are complementary in
practice, and explore how ML helps FV in its classical approaches: static
analysis, model-checking, theorem-proving, and SAT solving. We draw a landscape
of the current practice and catalog some of the most prominent uses of ML
inside FV tools, thus offering a new perspective on FV techniques that can help
researchers and practitioners to better locate the possible synergies. We
discuss lessons learned from our work, point to possible improvements and offer
visions for the future of the domain in the light of the science of software
and systems modeling.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1806.03342v1,2018-06-08T20:11:05Z,2018-06-08T20:11:05Z,Discovering Signals from Web Sources to Predict Cyber Attacks,"Cyber attacks are growing in frequency and severity. Over the past year alone
we have witnessed massive data breaches that stole personal information of
millions of people and wide-scale ransomware attacks that paralyzed critical
infrastructure of several countries. Combating the rising cyber threat calls
for a multi-pronged strategy, which includes predicting when these attacks will
occur. The intuition driving our approach is this: during the planning and
preparation stages, hackers leave digital traces of their activities on both
the surface web and dark web in the form of discussions on platforms like
hacker forums, social media, blogs and the like. These data provide predictive
signals that allow anticipating cyber attacks. In this paper, we describe
machine learning techniques based on deep neural networks and autoregressive
time series models that leverage external signals from publicly available Web
sources to forecast cyber attacks. Performance of our framework across ground
truth data over real-world forecasting tasks shows that our methods yield a
significant lift or increase of F1 for the top signals on predicted cyber
attacks. Our results suggest that, when deployed, our system will be able to
provide an effective line of defense against various types of targeted cyber
attacks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1806.02350v1,2018-06-06T18:00:02Z,2018-06-06T18:00:02Z,Learning New Physics from a Machine,"We propose using neural networks to detect data departures from a given
reference model, with no prior bias on the nature of the new physics
responsible for the discrepancy. The virtues of neural networks as unbiased
function approximants make them particularly suited for this task. An algorithm
that implements this idea is constructed, as a straightforward application of
the likelihood-ratio hypothesis test. The algorithm compares observations with
an auxiliary set of reference-distributed events, possibly obtained with a
Monte Carlo event generator. It returns a p-value, which measures the
compatibility of the reference model with the data. It also identifies the most
discrepant phase-space region of the data set, to be selected for further
investigation. The most interesting potential applications are
model-independent new physics searches, although our approach could also be
used to compare the theoretical predictions of different Monte Carlo event
generators, or for data validation algorithms. In this work we study the
performance of our algorithm on a few simple examples. The results confirm the
model-independence of the approach, namely that it displays good sensitivity to
a variety of putative signals. Furthermore, we show that the reach does not
depend much on whether a favorable signal region is selected based on prior
expectations. We identify directions for improvement towards applications to
real experimental data sets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1806.00260v1,2018-06-01T09:59:09Z,2018-06-01T09:59:09Z,"The Proximal Alternating Minimization Algorithm for two-block separable
  convex optimization problems with linear constraints","The Alternating Minimization Algorithm (AMA) has been proposed by Tseng to
solve convex programming problems with two-block separable linear constraints
and objectives, whereby (at least) one of the components of the latter is
assumed to be strongly convex. The fact that one of the subproblems to be
solved within the iteration process of AMA does not usually correspond to the
calculation of a proximal operator through a closed formula, affects the
implementability of the algorithm. In this paper we allow in each block of the
objective a further smooth convex function and propose a proximal version of
AMA, called Proximal AMA, which is achieved by equipping the algorithm with
proximal terms induced by variable metrics. For suitable choices of the latter,
the solving of the two subproblems in the iterative scheme can be reduced to
the computation of proximal operators. We investigate the convergence of the
proposed algorithm in a real Hilbert space setting and illustrate its numerical
performances on two applications in image processing and machine learning.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1805.05491v1,2018-05-14T22:59:56Z,2018-05-14T22:59:56Z,"Crowdbreaks: Tracking Health Trends using Public Social Media Data and
  Crowdsourcing","In the past decade, tracking health trends using social media data has shown
great promise, due to a powerful combination of massive adoption of social
media around the world, and increasingly potent hardware and software that
enables us to work with these new big data streams. At the same time, many
challenging problems have been identified. First, there is often a mismatch
between how rapidly online data can change, and how rapidly algorithms are
updated, which means that there is limited reusability for algorithms trained
on past data as their performance decreases over time. Second, much of the work
is focusing on specific issues during a specific past period in time, even
though public health institutions would need flexible tools to assess multiple
evolving situations in real time. Third, most tools providing such capabilities
are proprietary systems with little algorithmic or data transparency, and thus
little buy-in from the global public health and research community. Here, we
introduce Crowdbreaks, an open platform which allows tracking of health trends
by making use of continuous crowdsourced labelling of public social media
content. The system is built in a way which automatizes the typical workflow
from data collection, filtering, labelling and training of machine learning
classifiers and therefore can greatly accelerate the research process in the
public health domain. This work introduces the technical aspects of the
platform and explores its future use cases.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1805.03045v2,2018-06-12T08:10:11Z,2018-05-08T14:15:46Z,"A new method for unveiling Open Clusters in Gaia: new nearby Open
  Clusters confirmed by DR2","The publication of the Gaia Data Release 2 (Gaia DR2) opens a new era in
Astronomy. It includes precise astrometric data (positions, proper motions and
parallaxes) for more than $1.3$ billion sources, mostly stars. To analyse such
a vast amount of new data, the use of data mining techniques and machine
learning algorithms are mandatory. The search for Open Clusters, groups of
stars that were born and move together, located in the disk, is a great example
for the application of these techniques. Our aim is to develop a method to
automatically explore the data space, requiring minimal manual intervention. We
explore the performance of a density based clustering algorithm, DBSCAN, to
find clusters in the data together with a supervised learning method such as an
Artificial Neural Network (ANN) to automatically distinguish between real Open
Clusters and statistical clusters. The development and implementation of this
method to a $5$-Dimensional space ($l$, $b$, $\varpi$, $\mu_{\alpha^*}$,
$\mu_\delta$) to the Tycho-Gaia Astrometric Solution (TGAS) data, and a
posterior validation using Gaia DR2 data, lead to the proposal of a set of new
nearby Open Clusters. We have developed a method to find OCs in astrometric
data, designed to be applied to the full Gaia DR2 archive.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1804.10332v2,2018-05-16T20:35:34Z,2018-04-27T03:42:55Z,Sim-to-Real: Learning Agile Locomotion For Quadruped Robots,"Designing agile locomotion for quadruped robots often requires extensive
expertise and tedious manual tuning. In this paper, we present a system to
automate this process by leveraging deep reinforcement learning techniques. Our
system can learn quadruped locomotion from scratch using simple reward signals.
In addition, users can provide an open loop reference to guide the learning
process when more control over the learned gait is needed. The control policies
are learned in a physics simulator and then deployed on real robots. In
robotics, policies trained in simulation often do not transfer to the real
world. We narrow this reality gap by improving the physics simulator and
learning robust policies. We improve the simulation using system
identification, developing an accurate actuator model and simulating latency.
We learn robust controllers by randomizing the physical environments, adding
perturbations and designing a compact observation space. We evaluate our system
on two agile locomotion gaits: trotting and galloping. After learning in
simulation, a quadruped robot can successfully perform both gaits in the real
world.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1804.10134v2,2018-07-28T10:33:47Z,2018-04-26T16:04:30Z,Detection-Tracking for Efficient Person Analysis: The DetTA Pipeline,"In the past decade many robots were deployed in the wild, and people
detection and tracking is an important component of such deployments. On top of
that, one often needs to run modules which analyze persons and extract higher
level attributes such as age and gender, or dynamic information like gaze and
pose. The latter ones are especially necessary for building a reactive, social
robot-person interaction.
  In this paper, we combine those components in a fully modular
detection-tracking-analysis pipeline, called DetTA. We investigate the benefits
of such an integration on the example of head and skeleton pose, by using the
consistent track ID for a temporal filtering of the analysis modules'
observations, showing a slight improvement in a challenging real-world
scenario. We also study the potential of a so-called ""free-flight"" mode, where
the analysis of a person attribute only relies on the filter's predictions for
certain frames. Here, our study shows that this boosts the runtime
dramatically, while the prediction quality remains stable. This insight is
especially important for reducing power consumption and sharing precious
(GPU-)memory when running many analysis components on a mobile platform,
especially so in the era of expensive deep learning methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1805.00330v1,2018-04-24T22:02:10Z,2018-04-24T22:02:10Z,"Real-Time Human Detection as an Edge Service Enabled by a Lightweight
  CNN","Edge computing allows more computing tasks to take place on the decentralized
nodes at the edge of networks. Today many delay sensitive, mission-critical
applications can leverage these edge devices to reduce the time delay or even
to enable real time, online decision making thanks to their onsite presence.
Human objects detection, behavior recognition and prediction in smart
surveillance fall into that category, where a transition of a huge volume of
video streaming data can take valuable time and place heavy pressure on
communication networks. It is widely recognized that video processing and
object detection are computing intensive and too expensive to be handled by
resource limited edge devices. Inspired by the depthwise separable convolution
and Single Shot Multi-Box Detector (SSD), a lightweight Convolutional Neural
Network (LCNN) is introduced in this paper. By narrowing down the classifier's
searching space to focus on human objects in surveillance video frames, the
proposed LCNN algorithm is able to detect pedestrians with an affordable
computation workload to an edge device. A prototype has been implemented on an
edge node (Raspberry PI 3) using openCV libraries, and satisfactory performance
is achieved using real world surveillance video streams. The experimental study
has validated the design of LCNN and shown it is a promising approach to
computing intensive applications at the edge.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1804.07886v1,2018-04-21T04:16:46Z,2018-04-21T04:16:46Z,Social Bots for Online Public Health Interventions,"According to the Center for Disease Control and Prevention, in the United
States hundreds of thousands initiate smoking each year, and millions live with
smoking-related dis- eases. Many tobacco users discuss their habits and
preferences on social media. This work conceptualizes a framework for targeted
health interventions to inform tobacco users about the consequences of tobacco
use. We designed a Twitter bot named Notobot (short for No-Tobacco Bot) that
leverages machine learning to identify users posting pro-tobacco tweets and
select individualized interventions to address their interest in tobacco use.
We searched the Twitter feed for tobacco-related keywords and phrases, and
trained a convolutional neural network using over 4,000 tweets dichotomously
manually labeled as either pro- tobacco or not pro-tobacco. This model achieves
a 90% recall rate on the training set and 74% on test data. Users posting pro-
tobacco tweets are matched with former smokers with similar interests who
posted anti-tobacco tweets. Algorithmic matching, based on the power of peer
influence, allows for the systematic delivery of personalized interventions
based on real anti-tobacco tweets from former smokers. Experimental evaluation
suggests that our system would perform well if deployed. This research offers
opportunities for public health researchers to increase health awareness at
scale. Future work entails deploying the fully operational Notobot system in a
controlled experiment within a public health campaign.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1804.06207v1,2018-04-17T12:51:52Z,2018-04-17T12:51:52Z,MetaBags: Bagged Meta-Decision Trees for Regression,"Ensembles are popular methods for solving practical supervised learning
problems. They reduce the risk of having underperforming models in
production-grade software. Although critical, methods for learning
heterogeneous regression ensembles have not been proposed at large scale,
whereas in classical ML literature, stacking, cascading and voting are mostly
restricted to classification problems. Regression poses distinct learning
challenges that may result in poor performance, even when using well
established homogeneous ensemble schemas such as bagging or boosting.
  In this paper, we introduce MetaBags, a novel, practically useful stacking
framework for regression. MetaBags is a meta-learning algorithm that learns a
set of meta-decision trees designed to select one base model (i.e. expert) for
each query, and focuses on inductive bias reduction. A set of meta-decision
trees are learned using different types of meta-features, specially created for
this purpose - to then be bagged at meta-level. This procedure is designed to
learn a model with a fair bias-variance trade-off, and its improvement over
base model performance is correlated with the prediction diversity of different
experts on specific input space subregions. The proposed method and
meta-features are designed in such a way that they enable good predictive
performance even in subregions of space which are not adequately represented in
the available training data.
  An exhaustive empirical testing of the method was performed, evaluating both
generalization error and scalability of the approach on synthetic, open and
real-world application datasets. The obtained results show that our method
significantly outperforms existing state-of-the-art approaches.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1804.04756v1,2018-04-13T00:39:45Z,2018-04-13T00:39:45Z,Machine Learning Peeling and Loss Modelling of Time-Domain Reflectometry,"A fundamental pursuit of microwave metrology is the determination of the
characteristic impedance profile of microwave systems. Among other methods,
this can be practically achieved by means of time-domain reflectometry (TDR)
that measures the reflections from a device due to an applied stimulus.
Conventional TDR allows for the measurement of systems comprising a single
impedance. However, real systems typically feature impedance variations that
obscure the determination of all impedances subsequent to the first one. This
problem has been studied previously and is generally known as scattering
inversion or, in the context of microwave metrology, time-domain ""peeling"". In
this article, we demonstrate the implementation of a space-time efficient
peeling algorithm that corrects for the effect of prior impedance mismatch in a
nonuniform lossless transmission line, regardless of the nature of the
stimulus. We generalize TDR measurement analysis by introducing two tools: A
stochastic machine learning clustering tool and an arbitrary lossy transmission
line modeling tool. The former mitigates many of the imperfections typically
plaguing TDR measurements (except for dispersion) and allows for an efficient
processing of large datasets; the latter allows for a complete transmission
line characterization including both conductor and dielectric loss.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1804.03547v3,2022-03-24T11:04:37Z,2018-04-10T14:07:45Z,"A real-time and unsupervised face Re-Identification system for
  Human-Robot Interaction","In the context of Human-Robot Interaction (HRI), face Re-Identification (face
Re-ID) aims to verify if certain detected faces have already been observed by
robots. The ability of distinguishing between different users is crucial in
social robots as it will enable the robot to tailor the interaction strategy
toward the users' individual preferences. So far face recognition research has
achieved great success, however little attention has been paid to the realistic
applications of Face Re-ID in social robots. In this paper, we present an
effective and unsupervised face Re-ID system which simultaneously re-identifies
multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural
Networks to extract features, and an online clustering algorithm to determine
the face's ID. Its performance is evaluated on two datasets: the TERESA video
dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF
Dataset). We demonstrate that the optimised combination of techniques achieves
an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on
YTF dataset. We have implemented the proposed method into a software module in
the HCI^2 Framework for it to be further integrated into the TERESA robot, and
has achieved real-time performance at 10~26 Frames per second.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1804.03289v1,2018-04-10T00:44:29Z,2018-04-10T00:44:29Z,"Planning Multi-Fingered Grasps as Probabilistic Inference in a Learned
  Deep Network","We propose a novel approach to multi-fingered grasp planning leveraging
learned deep neural network models. We train a convolutional neural network to
predict grasp success as a function of both visual information of an object and
grasp configuration. We can then formulate grasp planning as inferring the
grasp configuration which maximizes the probability of grasp success. We
efficiently perform this inference using a gradient-ascent optimization inside
the neural network using the backpropagation algorithm. Our work is the first
to directly plan high quality multifingered grasps in configuration space using
a deep neural network without the need of an external planner. We validate our
inference method performing both multifinger and two-finger grasps on real
robots. Our experimental results show that our planning method outperforms
existing planning methods for neural networks; while offering several other
benefits including being data-efficient in learning and fast enough to be
deployed in real robotic applications.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1804.02395v2,2018-06-12T14:52:29Z,2018-04-06T15:25:14Z,"Structured Evolution with Compact Architectures for Scalable Policy
  Optimization","We present a new method of blackbox optimization via gradient approximation
with the use of structured random orthogonal matrices, providing more accurate
estimators than baselines and with provable theoretical guarantees. We show
that this algorithm can be successfully applied to learn better quality compact
policies than those using standard gradient estimation techniques. The compact
policies we learn have several advantages over unstructured ones, including
faster training algorithms and faster inference. These benefits are important
when the policy is deployed on real hardware with limited resources. Further,
compact policies provide more scalable architectures for derivative-free
optimization (DFO) in high-dimensional spaces. We show that most robotics tasks
from the OpenAI Gym can be solved using neural networks with less than 300
parameters, with almost linear time complexity of the inference phase, with up
to 13x fewer parameters relative to the Evolution Strategies (ES) algorithm
introduced by Salimans et al. (2017). We do not need heuristics such as fitness
shaping to learn good quality policies, resulting in a simple and theoretically
motivated training mechanism.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1804.00064v1,2018-03-30T21:56:38Z,2018-03-30T21:56:38Z,"Learning Beyond Human Expertise with Generative Models for Dental
  Restorations","Computer vision has advanced significantly that many discriminative
approaches such as object recognition are now widely used in real applications.
We present another exciting development that utilizes generative models for the
mass customization of medical products such as dental crowns. In the dental
industry, it takes a technician years of training to design synthetic crowns
that restore the function and integrity of missing teeth. Each crown must be
customized to individual patients, and it requires human expertise in a
time-consuming and labor-intensive process, even with computer-assisted design
software. We develop a fully automatic approach that learns not only from human
designs of dental crowns, but also from natural spatial profiles between
opposing teeth. The latter is hard to account for by technicians but important
for proper biting and chewing functions. Built upon a Generative Adversar-ial
Network architecture (GAN), our deep learning model predicts the customized
crown-filled depth scan from the crown-missing depth scan and opposing depth
scan. We propose to incorporate additional space constraints and statistical
compatibility into learning. Our automatic designs exceed human technicians'
standards for good morphology and functionality, and our algorithm is being
tested for production use.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1803.09786v1,2018-03-26T18:47:55Z,2018-03-26T18:47:55Z,"Transferable Joint Attribute-Identity Deep Learning for Unsupervised
  Person Re-Identification","Most existing person re-identification (re-id) methods require supervised
model learning from a separate large set of pairwise labelled training data for
every single camera pair. This significantly limits their scalability and
usability in real-world large scale deployments with the need for performing
re-id across many camera views. To address this scalability problem, we develop
a novel deep learning method for transferring the labelled information of an
existing dataset to a new unseen (unlabelled) target domain for person re-id
without any supervised learning in the target domain. Specifically, we
introduce an Transferable Joint Attribute-Identity Deep Learning (TJ-AIDL) for
simultaneously learning an attribute-semantic and identitydiscriminative
feature representation space transferrable to any new (unseen) target domain
for re-id tasks without the need for collecting new labelled training data from
the target domain (i.e. unsupervised learning in the target domain). Extensive
comparative evaluations validate the superiority of this new TJ-AIDL model for
unsupervised person re-id over a wide range of state-of-the-art methods on four
challenging benchmarks including VIPeR, PRID, Market-1501, and DukeMTMC-ReID.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1803.08501v1,2018-03-22T14:59:16Z,2018-03-22T14:59:16Z,DOP: Deep Optimistic Planning with Approximate Value Function Evaluation,"Research on reinforcement learning has demonstrated promising results in
manifold applications and domains. Still, efficiently learning effective robot
behaviors is very difficult, due to unstructured scenarios, high uncertainties,
and large state dimensionality (e.g. multi-agent systems or hyper-redundant
robots). To alleviate this problem, we present DOP, a deep model-based
reinforcement learning algorithm, which exploits action values to both (1)
guide the exploration of the state space and (2) plan effective policies.
Specifically, we exploit deep neural networks to learn Q-functions that are
used to attack the curse of dimensionality during a Monte-Carlo tree search.
Our algorithm, in fact, constructs upper confidence bounds on the learned value
function to select actions optimistically. We implement and evaluate DOP on
different scenarios: (1) a cooperative navigation problem, (2) a fetching task
for a 7-DOF KUKA robot, and (3) a human-robot handover with a humanoid robot
(both in simulation and real). The obtained results show the effectiveness of
DOP in the chosen applications, where action values drive the exploration and
reduce the computational demand of the planning process while achieving good
performance.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1803.07870v3,2020-06-07T20:02:21Z,2018-03-21T11:54:57Z,"Reservoir computing approaches for representation and classification of
  multivariate time series","Classification of multivariate time series (MTS) has been tackled with a
large variety of methodologies and applied to a wide range of scenarios.
Reservoir Computing (RC) provides efficient tools to generate a vectorial,
fixed-size representation of the MTS that can be further processed by standard
classifiers. Despite their unrivaled training speed, MTS classifiers based on a
standard RC architecture fail to achieve the same accuracy of fully trainable
neural networks. In this paper we introduce the reservoir model space, an
unsupervised approach based on RC to learn vectorial representations of MTS.
Each MTS is encoded within the parameters of a linear model trained to predict
a low-dimensional embedding of the reservoir dynamics. Compared to other RC
methods, our model space yields better representations and attains comparable
computational performance, thanks to an intermediate dimensionality reduction
procedure. As a second contribution we propose a modular RC framework for MTS
classification, with an associated open-source Python library. The framework
provides different modules to seamlessly implement advanced RC architectures.
The architectures are compared to other MTS classifiers, including deep
learning models and time series kernels. Results obtained on benchmark and
real-world MTS datasets show that RC classifiers are dramatically faster and,
when implemented using our proposed representation, also achieve superior
classification accuracy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1803.07634v1,2018-03-20T20:13:09Z,2018-03-20T20:13:09Z,Domain Adaptation with Randomized Expectation Maximization,"Domain adaptation (DA) is the task of classifying an unlabeled dataset
(target) using a labeled dataset (source) from a related domain. The majority
of successful DA methods try to directly match the distributions of the source
and target data by transforming the feature space. Despite their success, state
of the art methods based on this approach are either involved or unable to
directly scale to data with many features. This article shows that domain
adaptation can be successfully performed by using a very simple randomized
expectation maximization (EM) method. We consider two instances of the method,
which involve logistic regression and support vector machine, respectively. The
underlying assumption of the proposed method is the existence of a good single
linear classifier for both source and target domain. The potential limitations
of this assumption are alleviated by the flexibility of the method, which can
directly incorporate deep features extracted from a pre-trained deep neural
network. The resulting algorithm is strikingly easy to implement and apply. We
test its performance on 36 real-life adaptation tasks over text and image data
with diverse characteristics. The method achieves state-of-the-art results,
competitive with those of involved end-to-end deep transfer-learning methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1803.04873v2,2018-03-14T15:30:00Z,2018-03-13T15:17:30Z,"Using Convolutional Neural Networks for Determining Reticulocyte
  Percentage in Cats","Recent advances in artificial intelligence (AI), specifically in computer
vision (CV) and deep learning (DL), have created opportunities for novel
systems in many fields. In the last few years, deep learning applications have
demonstrated impressive results not only in fields such as autonomous driving
and robotics, but also in the field of medicine, where they have, in some
cases, even exceeded human-level performance. However, despite the huge
potential, adoption of deep learning-based methods is still slow in many areas,
especially in veterinary medicine, where we haven't been able to find any
research papers using modern convolutional neural networks (CNNs) in medical
image processing. We believe that using deep learning-based medical imaging can
enable more accurate, faster and less expensive diagnoses in veterinary
medicine. In order to do so, however, these methods have to be accessible to
everyone in this field, not just to computer scientists. To show the potential
of this technology, we present results on a real-world task in veterinary
medicine that is usually done manually: feline reticulocyte percentage. Using
an open source Keras implementation of the Single-Shot MultiBox Detector (SSD)
model architecture and training it on only 800 labeled images, we achieve an
accuracy of 98.7% at predicting the correct number of aggregate reticulocytes
in microscope images of cat blood smears. The main motivation behind this paper
is to show not only that deep learning can approach or even exceed human-level
performance on a task like this, but also that anyone in the field can
implement it, even without a background in computer science.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1803.04311v3,2019-01-30T12:54:32Z,2018-03-12T15:30:04Z,Deep Learning in Mobile and Wireless Networking: A Survey,"The rapid uptake of mobile devices and the rising popularity of mobile
applications and services pose unprecedented demands on mobile and wireless
networking infrastructure. Upcoming 5G systems are evolving to support
exploding mobile traffic volumes, agile management of network resource to
maximize user experience, and extraction of fine-grained real-time analytics.
Fulfilling these tasks is challenging, as mobile environments are increasingly
complex, heterogeneous, and evolving. One potential solution is to resort to
advanced machine learning techniques to help managing the rise in data volumes
and algorithm-driven applications. The recent success of deep learning
underpins new and powerful tools that tackle problems in this space.
  In this paper we bridge the gap between deep learning and mobile and wireless
networking research, by presenting a comprehensive survey of the crossovers
between the two areas. We first briefly introduce essential background and
state-of-the-art in deep learning techniques with potential applications to
networking. We then discuss several techniques and platforms that facilitate
the efficient deployment of deep learning onto mobile systems. Subsequently, we
provide an encyclopedic review of mobile and wireless networking research based
on deep learning, which we categorize by different domains. Drawing from our
experience, we discuss how to tailor deep learning to mobile environments. We
complete this survey by pinpointing current challenges and open future
directions for research.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1803.03191v1,2018-03-08T16:33:09Z,2018-03-08T16:33:09Z,"A Bayesian and Machine Learning approach to estimating Influence Model
  parameters for IM-RO","The rise of Online Social Networks (OSNs) has caused an insurmountable amount
of interest from advertisers and researchers seeking to monopolize on its
features. Researchers aim to develop strategies for determining how information
is propagated among users within an OSN that is captured by diffusion or
influence models. We consider the influence models for the IM-RO problem, a
novel formulation to the Influence Maximization (IM) problem based on
implementing Stochastic Dynamic Programming (SDP). In contrast to existing
approaches involving influence spread and the theory of submodular functions,
the SDP method focuses on optimizing clicks and ultimately revenue to
advertisers in OSNs. Existing approaches to influence maximization have been
actively researched over the past decade, with applications to multiple fields,
however, our approach is a more practical variant to the original IM problem.
In this paper, we provide an analysis on the influence models of the IM-RO
problem by conducting experiments on synthetic and real-world datasets. We
propose a Bayesian and Machine Learning approach for estimating the parameters
of the influence models for the (Influence Maximization- Revenue Optimization)
IM-RO problem. We present a Bayesian hierarchical model and implement the
well-known Naive Bayes classifier (NBC), Decision Trees classifier (DTC) and
Random Forest classifier (RFC) on three real-world datasets. Compared to
previous approaches to estimating influence model parameters, our strategy has
the great advantage of being directly implementable in standard software
packages such as WinBUGS/OpenBUGS/JAGS and Apache Spark. We demonstrate the
efficiency and usability of our methods in terms of spreading information and
generating revenue for advertisers in the context of OSNs.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1802.08678v2,2018-02-26T11:03:21Z,2018-02-23T18:53:44Z,"Verifying Controllers Against Adversarial Examples with Bayesian
  Optimization","Recent successes in reinforcement learning have lead to the development of
complex controllers for real-world robots. As these robots are deployed in
safety-critical applications and interact with humans, it becomes critical to
ensure safety in order to avoid causing harm. A first step in this direction is
to test the controllers in simulation. To be able to do this, we need to
capture what we mean by safety and then efficiently search the space of all
behaviors to see if they are safe. In this paper, we present an active-testing
framework based on Bayesian Optimization. We specify safety constraints using
logic and exploit structure in the problem in order to test the system for
adversarial counter examples that violate the safety specifications. These
specifications are defined as complex boolean combinations of smooth functions
on the trajectories and, unlike reward functions in reinforcement learning, are
expressive and impose hard constraints on the system. In our framework, we
exploit regularity assumptions on individual functions in form of a Gaussian
Process (GP) prior. We combine these into a coherent optimization framework
using problem structure. The resulting algorithm is able to provably verify
complex safety specifications or alternatively find counter examples.
Experimental results show that the proposed method is able to find adversarial
examples quickly.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1802.06958v1,2018-02-20T04:06:08Z,2018-02-20T04:06:08Z,"Deep Reinforcement Learning for Dynamic Multichannel Access in Wireless
  Networks","We consider a dynamic multichannel access problem, where multiple correlated
channels follow an unknown joint Markov model. A user at each time slot selects
a channel to transmit data and receives a reward based on the success or
failure of the transmission. The objective is to find a policy that maximizes
the expected long-term reward. The problem is formulated as a partially
observable Markov decision process (POMDP) with unknown system dynamics. To
overcome the challenges of unknown system dynamics as well as prohibitive
computation, we apply the concept of reinforcement learning and implement a
Deep Q-Network (DQN) that can deal with large state space without any prior
knowledge of the system dynamics. We provide an analytical study on the optimal
policy for fixed-pattern channel switching with known system dynamics and show
through simulations that DQN can achieve the same optimal performance without
knowing the system statistics. We compare the performance of DQN with a Myopic
policy and a Whittle Index-based heuristic through both simulations as well as
real-data trace and show that DQN achieves near-optimal performance in more
complex situations. Finally, we propose an adaptive DQN approach with the
capability to adapt its learning in time-varying, dynamic scenarios.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1802.06108v3,2019-12-31T20:08:44Z,2018-02-16T20:22:41Z,"Modeling the Formation of Social Conventions from Embodied Real-Time
  Interactions","What is the role of real-time control and learning in the formation of social
conventions? To answer this question, we propose a computational model that
matches human behavioral data in a social decision-making game that was
analyzed both in discrete-time and continuous-time setups. Furthermore, unlike
previous approaches, our model takes into account the role of sensorimotor
control loops in embodied decision-making scenarios. For this purpose, we
introduce the Control-based Reinforcement Learning (CRL) model. CRL is grounded
in the Distributed Adaptive Control (DAC) theory of mind and brain, where
low-level sensorimotor control is modulated through perceptual and behavioral
learning in a layered structure. CRL follows these principles by implementing a
feedback control loop handling the agent's reactive behaviors (pre-wired
reflexes), along with an adaptive layer that uses reinforcement learning to
maximize long-term reward. We test our model in a multi-agent game-theoretic
task in which coordination must be achieved to find an optimal solution. We
show that CRL is able to reach human-level performance on standard
game-theoretic metrics such as efficiency in acquiring rewards and fairness in
reward distribution.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1802.03938v1,2018-02-12T08:51:57Z,2018-02-12T08:51:57Z,"Revisiting the Vector Space Model: Sparse Weighted Nearest-Neighbor
  Method for Extreme Multi-Label Classification","Machine learning has played an important role in information retrieval (IR)
in recent times. In search engines, for example, query keywords are accepted
and documents are returned in order of relevance to the given query; this can
be cast as a multi-label ranking problem in machine learning. Generally, the
number of candidate documents is extremely large (from several thousand to
several million); thus, the classifier must handle many labels. This problem is
referred to as extreme multi-label classification (XMLC). In this paper, we
propose a novel approach to XMLC termed the Sparse Weighted Nearest-Neighbor
Method. This technique can be derived as a fast implementation of
state-of-the-art (SOTA) one-versus-rest linear classifiers for very sparse
datasets. In addition, we show that the classifier can be written as a sparse
generalization of a representer theorem with a linear kernel. Furthermore, our
method can be viewed as the vector space model used in IR. Finally, we show
that the Sparse Weighted Nearest-Neighbor Method can process data points in
real time on XMLC datasets with equivalent performance to SOTA models, with a
single thread and smaller storage footprint. In particular, our method exhibits
superior performance to the SOTA models on a dataset with 3 million labels.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1802.02395v1,2018-02-07T12:01:13Z,2018-02-07T12:01:13Z,Evaluation of Deep Reinforcement Learning Methods for Modular Robots,"We propose a novel framework for Deep Reinforcement Learning (DRL) in modular
robotics using traditional robotic tools that extend state-of-the-art DRL
implementations and provide an end-to-end approach which trains a robot
directly from joint states. Moreover, we present a novel technique to transfer
these DLR methods into the real robot, aiming to close the simulation-reality
gap. We demonstrate the robustness of the performance of state-of-the-art DRL
methods for continuous action spaces in modular robots, with an empirical study
both in simulation and in the real robot where we also evaluate how
accelerating the simulation time affects the robot's performance. Our results
show that extending the modular robot from 3 degrees-of-freedom (DoF), to 4
DoF, does not affect the robot's learning. This paves the way towards training
modular robots using DRL techniques.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1802.00264v1,2018-02-01T12:41:25Z,2018-02-01T12:41:25Z,Automatic Safety Helmet Wearing Detection,"Surveillance is essential for the safety of power substation. The detection
of whether wearing safety helmets or not for perambulatory workers is the key
component of overall intelligent surveillance system in power substation. In
this paper, a novel and practical safety helmet detection framework based on
computer vision, machine learning and image processing is proposed. In order to
ascertain motion objects in power substation, the ViBe background modelling
algorithm is employed. Moreover, based on the result of motion objects
segmentation, real-time human classification framework C4 is applied to locate
pedestrian in power substation accurately and quickly. Finally, according to
the result of pedestrian detection, the safety helmet wearing detection is
implemented using the head location, the color space transformation and the
color feature discrimination. Extensive compelling experimental results in
power substation illustrate the efficiency and effectiveness of the proposed
framework.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1801.09684v2,2018-06-16T16:00:22Z,2018-01-29T19:00:01Z,Latent Space Purification via Neural Density Operators,"Machine learning is actively being explored for its potential to design,
validate, and even hybridize with near-term quantum devices. A central question
is whether neural networks can provide a tractable representation of a given
quantum state of interest. When true, stochastic neural networks can be
employed for many unsupervised tasks, including generative modeling and state
tomography. However, to be applicable for real experiments such methods must be
able to encode quantum mixed states. Here, we parametrize a density matrix
based on a restricted Boltzmann machine that is capable of purifying a mixed
state through auxiliary degrees of freedom embedded in the latent space of its
hidden units. We implement the algorithm numerically and use it to perform
tomography on some typical states of entangled photons, achieving fidelities
competitive with standard techniques.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1801.09271v1,2018-01-28T19:29:50Z,2018-01-28T19:29:50Z,"Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical
  Registry Data","This paper presents the first deep reinforcement learning (DRL) framework to
estimate the optimal Dynamic Treatment Regimes from observational medical data.
This framework is more flexible and adaptive for high dimensional action and
state spaces than existing reinforcement learning methods to model real-life
complexity in heterogeneous disease progression and treatment choices, with the
goal of providing doctor and patients the data-driven personalized decision
recommendations. The proposed DRL framework comprises (i) a supervised learning
step to predict the most possible expert actions, and (ii) a deep reinforcement
learning step to estimate the long-term value function of Dynamic Treatment
Regimes. Both steps depend on deep neural networks.
  As a key motivational example, we have implemented the proposed framework on
a data set from the Center for International Bone Marrow Transplant Research
(CIBMTR) registry database, focusing on the sequence of prevention and
treatments for acute and chronic graft versus host disease after
transplantation. In the experimental results, we have demonstrated promising
accuracy in predicting human experts' decisions, as well as the high expected
reward function in the DRL-based dynamic treatment regimes.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1801.05643v1,2018-01-17T12:51:01Z,2018-01-17T12:51:01Z,"The Case for Automatic Database Administration using Deep Reinforcement
  Learning","Like any large software system, a full-fledged DBMS offers an overwhelming
amount of configuration knobs. These range from static initialisation
parameters like buffer sizes, degree of concurrency, or level of replication to
complex runtime decisions like creating a secondary index on a particular
column or reorganising the physical layout of the store. To simplify the
configuration, industry grade DBMSs are usually shipped with various advisory
tools, that provide recommendations for given workloads and machines. However,
reality shows that the actual configuration, tuning, and maintenance is usually
still done by a human administrator, relying on intuition and experience.
Recent work on deep reinforcement learning has shown very promising results in
solving problems, that require such a sense of intuition. For instance, it has
been applied very successfully in learning how to play complicated games with
enormous search spaces. Motivated by these achievements, in this work we
explore how deep reinforcement learning can be used to administer a DBMS.
First, we will describe how deep reinforcement learning can be used to
automatically tune an arbitrary software system like a DBMS by defining a
problem environment. Second, we showcase our concept of NoDBA at the concrete
example of index selection and evaluate how well it recommends indexes for
given workloads.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1801.02854v3,2018-07-25T07:54:52Z,2018-01-09T09:44:21Z,Riemannian Motion Policies,"We introduce the Riemannian Motion Policy (RMP), a new mathematical object
for modular motion generation. An RMP is a second-order dynamical system
(acceleration field or motion policy) coupled with a corresponding Riemannian
metric. The motion policy maps positions and velocities to accelerations, while
the metric captures the directions in the space important to the policy. We
show that RMPs provide a straightforward and convenient method for combining
multiple motion policies and transforming such policies from one space (such as
the task space) to another (such as the configuration space) in geometrically
consistent ways. The operators we derive for these combinations and
transformations are provably optimal, have linearity properties making them
agnostic to the order of application, and are strongly analogous to the
covariant transformations of natural gradients popular in the machine learning
literature. The RMP framework enables the fusion of motion policies from
different motion generation paradigms, such as dynamical systems, dynamic
movement primitives (DMPs), optimal control, operational space control,
nonlinear reactive controllers, motion optimization, and model predictive
control (MPC), thus unifying these disparate techniques from the literature.
RMPs are easy to implement and manipulate, facilitate controller design,
simplify handling of joint limits, and clarify a number of open questions
regarding the proper fusion of motion generation methods (such as incorporating
local reactive policies into long-horizon optimizers). We demonstrate the
effectiveness of RMPs on both simulation and real robots, including their
ability to naturally and efficiently solve complicated collision avoidance
problems previously handled by more complex planners.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1801.02108v2,2018-06-07T14:44:00Z,2018-01-07T01:03:25Z,SBNet: Sparse Blocks Network for Fast Inference,"Conventional deep convolutional neural networks (CNNs) apply convolution
operators uniformly in space across all feature maps for hundreds of layers -
this incurs a high computational cost for real-time applications. For many
problems such as object detection and semantic segmentation, we are able to
obtain a low-cost computation mask, either from a priori problem knowledge, or
from a low-resolution segmentation network. We show that such computation masks
can be used to reduce computation in the high-resolution main network. Variants
of sparse activation CNNs have previously been explored on small-scale tasks
and showed no degradation in terms of object classification accuracy, but often
measured gains in terms of theoretical FLOPs without realizing a practical
speed-up when compared to highly optimized dense convolution implementations.
In this work, we leverage the sparsity structure of computation masks and
propose a novel tiling-based sparse convolution algorithm. We verified the
effectiveness of our sparse CNN on LiDAR-based 3D object detection, and we
report significant wall-clock speed-ups compared to dense convolution without
noticeable loss of accuracy.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1801.01999v1,2018-01-06T10:38:51Z,2018-01-06T10:38:51Z,Using reinforcement learning to learn how to play text-based games,"The ability to learn optimal control policies in systems where action space
is defined by sentences in natural language would allow many interesting
real-world applications such as automatic optimisation of dialogue systems.
Text-based games with multiple endings and rewards are a promising platform for
this task, since their feedback allows us to employ reinforcement learning
techniques to jointly learn text representations and control policies. We
present a general text game playing agent, testing its generalisation and
transfer learning performance and showing its ability to play multiple games at
once. We also present pyfiction, an open-source library for universal access to
different text games that could, together with our agent that implements its
interface, serve as a baseline for future research.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1801.00864v3,2020-07-16T17:07:07Z,2018-01-02T23:49:13Z,"FNS: an event-driven spiking neural network simulator based on the LIFL
  neuron model","Limitations in processing capabilities and memory of today's computers make
spiking neuron-based (human) whole-brain simulations inevitably characterized
by a compromise between bio-plausibility and computational cost. It translates
into brain models composed of a reduced number of neurons and a simplified
neuron's mathematical model, leading to the search for new simulation
strategies. Taking advantage of the sparse character of brain-like computation,
the event-driven technique could represent a way to carry out efficient
simulation of large-scale Spiking Neural Networks (SNN). The recent Leaky
Integrate-and-Fire with Latency (LIFL) spiking neuron model is event-driven
compatible and exhibits some realistic neuronal features, opening new avenues
for brain modelling. In this paper we introduce FNS, the first LIFL-based
spiking neural network framework, which combines spiking/synaptic neural
modelling with the event-driven approach, allowing us to define heterogeneous
neuron modules and multi-scale connectivity with delayed connections and
plastic synapses. In order to allow multi-thread implementations a novel
parallelization strategy is also introduced. This paper presents mathematical
models, software implementation and simulation routines on which FNS is based.
Finally, a brain subnetwork is modeled on the basis of real brain structural
data, and the resulting simulated activity is compared with associated brain
functional (source-space MEG) data, demonstrating a good matching between the
activity of the model and that of the experimetal data. This work aims to lay
the groundwork for future event-driven based personalised brain models.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1801.00091v2,2018-02-21T22:25:38Z,2017-12-30T06:56:37Z,"PrivySense: $\underline{Pri}$ce $\underline{V}$olatilit$\underline{y}$
  based $\underline{Sen}$timent$\underline{s}$ $\underline{E}$stimation from
  Financial News using Machine Learning","As machine learning ascends the peak of computer science zeitgeist, the usage
and experimentation with sentiment analysis using various forms of textual data
seems pervasive. The effect is especially pronounced in formulating securities
trading strategies, due to a plethora of reasons including the relative ease of
implementation and the abundance of academic research suggesting automated
sentiment analysis can be productively used in trading strategies. The source
data for such analyzers ranges a broad spectrum like social media feeds,
micro-blogs, real-time news feeds, ex-post financial data etc. The abstract
technique underlying these analyzers involve supervised learning of sentiment
classification where the classifier is trained on annotated source corpus, and
accuracy is measured by testing how well the classifiers generalizes on unseen
test data from the corpus. Post training, and validation of fitted models, the
classifiers are used to execute trading strategies, and the corresponding
returns are compared with appropriate benchmark returns (for e.g., the S&P500
returns).
  In this paper, we introduce $\underline{a\ novel\ technique\ of\ using\
price\ volatilities\ to\ empirically\ determine\ the\ sentiment\ in\ news\
data}$, instead of the traditional reverse approach. We also perform meta
sentiment analysis by evaluating the efficacy of existing sentiment classifiers
and the precise definition of sentiment from securities trading context. We
scrutinize the efficacy of using human-annotated sentiment classification and
the tacit assumptions that introduces subjective bias in existing financial
news sentiment classifiers.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1712.06107v1,2017-12-17T13:00:25Z,2017-12-17T13:00:25Z,Railway Track Specific Traffic Signal Selection Using Deep Learning,"With the railway transportation Industry moving actively towards automation,
accurate location and inventory of wayside track assets like traffic signals,
crossings, switches, mileposts, etc. is of extreme importance. With the new
Positive Train Control (PTC) regulation coming into effect, many railway safety
rules will be tied directly to location of assets like mileposts and signals.
Newer speed regulations will be enforced based on location of the Train with
respect to a wayside asset. Hence it is essential for the railroads to have an
accurate database of the types and locations of these assets. This paper talks
about a real-world use-case of detecting railway signals from a camera mounted
on a moving locomotive and tracking their locations. The camera is engineered
to withstand the environment factors on a moving train and provide a consistent
steady image at around 30 frames per second. Using advanced image analysis and
deep learning techniques, signals are detected in these camera images and a
database of their locations is created. Railway signals differ a lot from road
signals in terms of shapes and rules for placement with respect to track. Due
to space constraint and traffic densities in urban areas signals are not placed
on the same side of the track and multiple lines can run in parallel. Hence
there is need to associate signal detected with the track on which the train
runs. We present a method to associate the signals to the specific track they
belong to using a video feed from the front facing camera mounted on the lead
locomotive. A pipeline of track detection, region of interest selection, signal
detection has been implemented which gives an overall accuracy of 94.7% on a
route covering 150km with 247 signals.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1712.02294v4,2018-07-12T14:11:40Z,2017-12-06T17:20:21Z,Joint 3D Proposal Generation and Object Detection from View Aggregation,"We present AVOD, an Aggregate View Object Detection network for autonomous
driving scenarios. The proposed neural network architecture uses LIDAR point
clouds and RGB images to generate features that are shared by two subnetworks:
a region proposal network (RPN) and a second stage detector network. The
proposed RPN uses a novel architecture capable of performing multimodal feature
fusion on high resolution feature maps to generate reliable 3D object proposals
for multiple object classes in road scenes. Using these proposals, the second
stage detection network performs accurate oriented 3D bounding box regression
and category classification to predict the extents, orientation, and
classification of objects in 3D space. Our proposed architecture is shown to
produce state of the art results on the KITTI 3D object detection benchmark
while running in real time with a low memory footprint, making it a suitable
candidate for deployment on autonomous vehicles. Code is at:
https://github.com/kujason/avod",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1712.01785v3,2017-12-16T00:30:53Z,2017-12-05T17:49:18Z,"Towards Practical Verification of Machine Learning: The Case of Computer
  Vision Systems","Due to the increasing usage of machine learning (ML) techniques in security-
and safety-critical domains, such as autonomous systems and medical diagnosis,
ensuring correct behavior of ML systems, especially for different corner cases,
is of growing importance. In this paper, we propose a generic framework for
evaluating security and robustness of ML systems using different real-world
safety properties. We further design, implement and evaluate VeriVis, a
scalable methodology that can verify a diverse set of safety properties for
state-of-the-art computer vision systems with only blackbox access. VeriVis
leverage different input space reduction techniques for efficient verification
of different safety properties. VeriVis is able to find thousands of safety
violations in fifteen state-of-the-art computer vision systems including ten
Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving
system with thousands of neurons as well as five commercial third-party vision
APIs including Google vision and Clarifai for twelve different safety
properties. Furthermore, VeriVis can successfully verify local safety
properties, on average, for around 31.7% of the test images. VeriVis finds up
to 64.8x more violations than existing gradient-based methods that, unlike
VeriVis, cannot ensure non-existence of any violations. Finally, we show that
retraining using the safety violations detected by VeriVis can reduce the
average number of violations up to 60.2%.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1712.03073v3,2021-01-13T00:55:32Z,2017-12-01T16:58:32Z,DeepWear: Adaptive Local Offloading for On-Wearable Deep Learning,"Due to their on-body and ubiquitous nature, wearables can generate a wide
range of unique sensor data creating countless opportunities for deep learning
tasks. We propose DeepWear, a deep learning (DL) framework for wearable devices
to improve the performance and reduce the energy footprint. DeepWear
strategically offloads DL tasks from a wearable device to its paired handheld
device through local network. Compared to the remote-cloud-based offloading,
DeepWear requires no Internet connectivity, consumes less energy, and is robust
to privacy breach. DeepWear provides various novel techniques such as
context-aware offloading, strategic model partition, and pipelining support to
efficiently utilize the processing capacity from nearby paired handhelds.
Deployed as a user-space library, DeepWear offers developer-friendly APIs that
are as simple as those in traditional DL libraries such as TensorFlow. We have
implemented DeepWear on the Android OS and evaluated it on COTS smartphones and
smartwatches with real DL models. DeepWear brings up to 5.08X and 23.0X
execution speedup, as well as 53.5% and 85.5% energy saving compared to
wearable-only and handheld-only strategies, respectively.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1711.10941v1,2017-11-29T16:23:38Z,2017-11-29T16:23:38Z,"Intelligent Traffic Light Control Using Distributed Multi-agent Q
  Learning","The combination of Artificial Intelligence (AI) and Internet-of-Things (IoT),
which is denoted as AI-powered Internet-of-Things (AIoT), is capable of
processing huge amount of data generated from a large number of devices and
handling complex problems in social infrastructures. As AI and IoT technologies
are becoming mature, in this paper, we propose to apply AIoT technologies for
traffic light control, which is an essential component for intelligent
transportation system, to improve the efficiency of smart city's road system.
Specifically, various sensors such as surveillance cameras provide real-time
information for intelligent traffic light control system to observe the states
of both motorized traffic and non-motorized traffic. In this paper, we propose
an intelligent traffic light control solution by using distributed multi-agent
Q learning, considering the traffic information at the neighboring
intersections as well as local motorized and non-motorized traffic, to improve
the overall performance of the entire control system. By using the proposed
multi-agent Q learning algorithm, our solution is targeting to optimize both
the motorized and non-motorized traffic. In addition, we considered many
constraints/rules for traffic light control in the real world, and integrate
these constraints in the learning algorithm, which can facilitate the proposed
solution to be deployed in real operational scenarios. We conducted numerical
simulations for a real-world map with real-world traffic data. The simulation
results show that our proposed solution outperforms existing solutions in terms
of vehicle and pedestrian queue lengths, waiting time at intersections, and
many other key performance metrics.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1711.07910v3,2021-01-06T23:07:06Z,2017-11-21T16:59:43Z,Domain Generalization by Marginal Transfer Learning,"In the problem of domain generalization (DG), there are labeled training data
sets from several related prediction problems, and the goal is to make accurate
predictions on future unlabeled data sets that are not known to the learner.
This problem arises in several applications where data distributions fluctuate
because of environmental, technical, or other sources of variation. We
introduce a formal framework for DG, and argue that it can be viewed as a kind
of supervised learning problem by augmenting the original feature space with
the marginal distribution of feature vectors. While our framework has several
connections to conventional analysis of supervised learning algorithms, several
unique aspects of DG require new methods of analysis.
  This work lays the learning theoretic foundations of domain generalization,
building on our earlier conference paper where the problem of DG was introduced
(Blanchard et al., 2011). We present two formal models of data generation,
corresponding notions of risk, and distribution-free generalization error
analysis. By focusing our attention on kernel methods, we also provide more
quantitative results and a universally consistent algorithm. An efficient
implementation is provided for this algorithm, which is experimentally compared
to a pooling strategy on one synthetic and three real-world data sets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1711.08037v2,2017-11-24T07:02:24Z,2017-11-20T04:19:49Z,The Doctor Just Won't Accept That!,"Calls to arms to build interpretable models express a well-founded discomfort
with machine learning. Should a software agent that does not even know what a
loan is decide who qualifies for one? Indeed, we ought to be cautious about
injecting machine learning (or anything else, for that matter) into
applications where there may be a significant risk of causing social harm.
However, claims that stakeholders ""just won't accept that!"" do not provide a
sufficient foundation for a proposed field of study. For the field of
interpretable machine learning to advance, we must ask the following questions:
What precisely won't various stakeholders accept? What do they want? Are these
desiderata reasonable? Are they feasible? In order to answer these questions,
we'll have to give real-world problems and their respective stakeholders
greater consideration.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1711.06517v2,2018-05-22T05:54:17Z,2017-11-17T12:59:22Z,Wikipedia for Smart Machines and Double Deep Machine Learning,"Very important breakthroughs in data centric deep learning algorithms led to
impressive performance in transactional point applications of Artificial
Intelligence (AI) such as Face Recognition, or EKG classification. With all due
appreciation, however, knowledge blind data only machine learning algorithms
have severe limitations for non-transactional AI applications, such as medical
diagnosis beyond the EKG results. Such applications require deeper and broader
knowledge in their problem solving capabilities, e.g. integrating anatomy and
physiology knowledge with EKG results and other patient findings. Following a
review and illustrations of such limitations for several real life AI
applications, we point at ways to overcome them. The proposed Wikipedia for
Smart Machines initiative aims at building repositories of software structures
that represent humanity science & technology knowledge in various parts of
life; knowledge that we all learn in schools, universities and during our
professional life. Target readers for these repositories are smart machines;
not human. AI software developers will have these Reusable Knowledge structures
readily available, hence, the proposed name ReKopedia. Big Data is by now a
mature technology, it is time to focus on Big Knowledge. Some will be derived
from data, some will be obtained from mankind gigantic repository of knowledge.
Wikipedia for smart machines along with the new Double Deep Learning approach
offer a paradigm for integrating datacentric deep learning algorithms with
algorithms that leverage deep knowledge, e.g. evidential reasoning and
causality reasoning. For illustration, a project is described to produce
ReKopedia knowledge modules for medical diagnosis of about 1,000 disorders.
Data is important, but knowledge deep, basic, and commonsense is equally
important.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1710.02595v2,2017-10-10T05:05:58Z,2017-10-06T21:42:15Z,Intelligent Pothole Detection and Road Condition Assessment,"Poor road conditions are a public nuisance, causing passenger discomfort,
damage to vehicles, and accidents. In the U.S., road-related conditions are a
factor in 22,000 of the 42,000 traffic fatalities each year. Although we often
complain about bad roads, we have no way to detect or report them at scale. To
address this issue, we developed a system to detect potholes and assess road
conditions in real-time. Our solution is a mobile application that captures
data on a car's movement from gyroscope and accelerometer sensors in the phone.
To assess roads using this sensor data, we trained SVM models to classify road
conditions with 93% accuracy and potholes with 92% accuracy, beating the base
rate for both problems. As the user drives, the models use the sensor data to
classify whether the road is good or bad, and whether it contains potholes.
Then, the classification results are used to create data-rich maps that
illustrate road conditions across the city. Our system will empower civic
officials to identify and repair damaged roads which inconvenience passengers
and cause accidents. This paper details our data science process for collecting
training data on real roads, transforming noisy sensor data into useful
signals, training and evaluating machine learning models, and deploying those
models to production through a real-time classification app. It also highlights
how cities can use our system to crowdsource data and deliver road repair
resources to areas in need.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1708.05208v1,2017-08-17T11:31:22Z,2017-08-17T11:31:22Z,"Automatic HVAC Control with Real-time Occupancy Recognition and
  Simulation-guided Model Predictive Control in Low-cost Embedded System","Intelligent building automation systems can reduce the energy consumption of
heating, ventilation and air-conditioning (HVAC) units by sensing the comfort
requirements automatically and scheduling the HVAC operations dynamically.
Traditional building automation systems rely on fairly inaccurate occupancy
sensors and basic predictive control using oversimplified building thermal
response models, all of which prevent such systems from reaching their full
potential. Such limitations can now be avoided due to the recent developments
in embedded system technologies, which provide viable low-cost computing
platforms with powerful processors and sizeable memory storage in a small
footprint. As a result, building automation systems can now efficiently execute
highly-sophisticated computational tasks, such as real-time video processing
and accurate thermal-response simulations. With this in mind, we designed and
implemented an occupancy-predictive HVAC control system in a low-cost yet
powerful embedded system (using Raspberry Pi 3) to demonstrate the following
key features for building automation: (1) real-time occupancy recognition using
video-processing and machine-learning techniques, (2) dynamic analysis and
prediction of occupancy patterns, and (3) model predictive control for HVAC
operations guided by real-time building thermal response simulations (using an
on-board EnergyPlus simulator). We deployed and evaluated our system for
providing automatic HVAC control in the large public indoor space of a mosque,
thereby achieving significant energy savings.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1708.04664v1,2017-08-05T13:15:36Z,2017-08-05T13:15:36Z,"A Novel data Pre-processing method for multi-dimensional and non-uniform
  data","We are in the era of data analytics and data science which is on full bloom.
There is abundance of all kinds of data for example biometrics based data,
satellite images data, chip-seq data, social network data, sensor based data
etc. from a variety of sources. This data abundance is the result of the fact
that storage cost is getting cheaper day by day, so people as well as almost
all business or scientific organizations are storing more and more data. Most
of the real data is multi-dimensional, non-uniform, and big in size, such that
it requires a unique pre-processing before analyzing it. In order to make data
useful for any kind of analysis, pre-processing is a very important step. This
paper presents a unique and novel pre-processing method for multi-dimensional
and non-uniform data with the aim of making it uniform and reduced in size
without losing much of its value. We have chosen biometric signature data to
demonstrate the proposed method as it qualifies for the attributes of being
multi-dimensional, non-uniform and big in size. Biometric signature data does
not only captures the structural characteristics of a signature but also its
behavioral characteristics that are captured using a dynamic signature capture
device. These features like pen pressure, pen tilt angle, time taken to sign a
document when collected in real-time turn out to be of varying dimensions. This
feature data set along with the structural data needs to be pre-processed in
order to use it to train a machine learning based model for signature
verification purposes. We demonstrate the success of the proposed method over
other methods using experimental results for biometric signature data but the
same can be implemented for any other data with similar properties from a
different domain.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1707.08015v1,2017-07-25T14:40:18Z,2017-07-25T14:40:18Z,"Predicting Exploitation of Disclosed Software Vulnerabilities Using
  Open-source Data","Each year, thousands of software vulnerabilities are discovered and reported
to the public. Unpatched known vulnerabilities are a significant security risk.
It is imperative that software vendors quickly provide patches once
vulnerabilities are known and users quickly install those patches as soon as
they are available. However, most vulnerabilities are never actually exploited.
Since writing, testing, and installing software patches can involve
considerable resources, it would be desirable to prioritize the remediation of
vulnerabilities that are likely to be exploited. Several published research
studies have reported moderate success in applying machine learning techniques
to the task of predicting whether a vulnerability will be exploited. These
approaches typically use features derived from vulnerability databases (such as
the summary text describing the vulnerability) or social media posts that
mention the vulnerability by name. However, these prior studies share multiple
methodological shortcomings that inflate predictive power of these approaches.
We replicate key portions of the prior work, compare their approaches, and show
how selection of training and test data critically affect the estimated
performance of predictive models. The results of this study point to important
methodological considerations that should be taken into account so that results
reflect real-world utility.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1707.06600v2,2017-09-06T17:32:44Z,2017-07-20T16:35:02Z,"A multi-agent reinforcement learning model of common-pool resource
  appropriation","Humanity faces numerous problems of common-pool resource appropriation. This
class of multi-agent social dilemma includes the problems of ensuring
sustainable use of fresh water, common fisheries, grazing pastures, and
irrigation systems. Abstract models of common-pool resource appropriation based
on non-cooperative game theory predict that self-interested agents will
generally fail to find socially positive equilibria---a phenomenon called the
tragedy of the commons. However, in reality, human societies are sometimes able
to discover and implement stable cooperative solutions. Decades of behavioral
game theory research have sought to uncover aspects of human behavior that make
this possible. Most of that work was based on laboratory experiments where
participants only make a single choice: how much to appropriate. Recognizing
the importance of spatial and temporal resource dynamics, a recent trend has
been toward experiments in more complex real-time video game-like environments.
However, standard methods of non-cooperative game theory can no longer be used
to generate predictions for this case. Here we show that deep reinforcement
learning can be used instead. To that end, we study the emergent behavior of
groups of independently learning agents in a partially observed Markov game
modeling common-pool resource appropriation. Our experiments highlight the
importance of trial-and-error learning in common-pool resource appropriation
and shed light on the relationship between exclusion, sustainability, and
inequality.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1707.05223v1,2017-07-17T15:18:36Z,2017-07-17T15:18:36Z,A transient search using combined human and machine classifications,"Large modern surveys require efficient review of data in order to find
transient sources such as supernovae, and to distinguish such sources from
artefacts and noise. Much effort has been put into the development of automatic
algorithms, but surveys still rely on human review of targets. This paper
presents an integrated system for the identification of supernovae in data from
Pan-STARRS1, combining classifications from volunteers participating in a
citizen science project with those from a convolutional neural network. The
unique aspect of this work is the deployment, in combination, of both human and
machine classifications for near real-time discovery in an astronomical
project. We show that the combination of the two methods outperforms either one
used individually. This result has important implications for the future
development of transient searches, especially in the era of LSST and other
large-throughput surveys.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1707.04826v1,2017-07-16T05:58:40Z,2017-07-16T05:58:40Z,Machine learning application in the life time of materials,"Materials design and development typically takes several decades from the
initial discovery to commercialization with the traditional trial and error
development approach. With the accumulation of data from both experimental and
computational results, data based machine learning becomes an emerging field in
materials discovery, design and property prediction. This manuscript reviews
the history of materials science as a disciplinary the most common machine
learning method used in materials science, and specifically how they are used
in materials discovery, design, synthesis and even failure detection and
analysis after materials are deployed in real application. Finally, the
limitations of machine learning for application in materials science and
challenges in this emerging field is discussed.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1707.02880v2,2017-08-22T19:26:08Z,2017-07-10T14:34:06Z,Deep Bilateral Learning for Real-Time Image Enhancement,"Performance is a critical challenge in mobile image processing. Given a
reference imaging pipeline, or even human-adjusted pairs of images, we seek to
reproduce the enhancements and enable real-time evaluation. For this, we
introduce a new neural network architecture inspired by bilateral grid
processing and local affine color transforms. Using pairs of input/output
images, we train a convolutional neural network to predict the coefficients of
a locally-affine model in bilateral space. Our architecture learns to make
local, global, and content-dependent decisions to approximate the desired image
transformation. At runtime, the neural network consumes a low-resolution
version of the input image, produces a set of affine transformations in
bilateral space, upsamples those transformations in an edge-preserving fashion
using a new slicing node, and then applies those upsampled transformations to
the full-resolution image. Our algorithm processes high-resolution images on a
smartphone in milliseconds, provides a real-time viewfinder at 1080p
resolution, and matches the quality of state-of-the-art approximation
techniques on a large class of image operators. Unlike previous work, our model
is trained off-line from data and therefore does not require access to the
original operator at runtime. This allows our model to learn complex,
scene-dependent transformations for which no reference implementation is
available, such as the photographic edits of a human retoucher.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1706.06695v1,2017-06-20T22:52:10Z,2017-06-20T22:52:10Z,"Toward Real-Time Decentralized Reinforcement Learning using Finite
  Support Basis Functions","This paper addresses the design and implementation of complex Reinforcement
Learning (RL) behaviors where multi-dimensional action spaces are involved, as
well as the need to execute the behaviors in real-time using robotic platforms
with limited computational resources and training times. For this purpose, we
propose the use of decentralized RL, in combination with finite support basis
functions as alternatives to Gaussian RBF, in order to alleviate the effects of
the curse of dimensionality on the action and state spaces respectively, and to
reduce the computation time. As testbed, a RL based controller for the in-walk
kick in NAO robots, a challenging and critical problem for soccer robotics, is
used. The reported experiments show empirically that our solution saves up to
99.94% of execution time and 98.82% of memory consumption during execution,
without diminishing performance compared to classical approaches.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1706.00163v3,2017-06-05T14:54:55Z,2017-06-01T04:35:33Z,Coding Method for Parallel Iterative Linear Solver,"Computationally intensive distributed and parallel computing is often
bottlenecked by a small set of slow workers known as stragglers. In this paper,
we utilize the emerging idea of ""coded computation"" to design a novel
error-correcting-code inspired technique for solving linear inverse problems
under specific iterative methods in a parallelized implementation affected by
stragglers. Example applications include inverse problems in machine learning
on graphs, such as personalized PageRank and sampling on graphs. We provably
show that our coded-computation technique can reduce the mean-squared error
under a computational deadline constraint. In fact, the ratio of mean-squared
error of replication-based and coded techniques diverges to infinity as the
deadline increases. Our experiments for personalized PageRank performed on real
systems and real social networks show that this ratio can be as large as
$10^4$. Further, unlike coded-computation techniques proposed thus far, our
strategy combines outputs of all workers, including the stragglers, to produce
more accurate estimates at the computational deadline. This also ensures that
the accuracy degrades ""gracefully"" in the event that the number of stragglers
is large.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1705.06599v1,2017-05-17T03:04:43Z,2017-05-17T03:04:43Z,Localized LRR on Grassmann Manifolds: An Extrinsic View,"Subspace data representation has recently become a common practice in many
computer vision tasks. It demands generalizing classical machine learning
algorithms for subspace data. Low-Rank Representation (LRR) is one of the most
successful models for clustering vectorial data according to their subspace
structures. This paper explores the possibility of extending LRR for subspace
data on Grassmann manifolds. Rather than directly embedding the Grassmann
manifolds into the symmetric matrix space, an extrinsic view is taken to build
the LRR self-representation in the local area of the tangent space at each
Grassmannian point, resulting in a localized LRR method on Grassmann manifolds.
A novel algorithm for solving the proposed model is investigated and
implemented. The performance of the new clustering algorithm is assessed
through experiments on several real-world datasets including MNIST handwritten
digits, ballet video clips, SKIG action clips, DynTex++ dataset and highway
traffic video clips. The experimental results show the new method outperforms a
number of state-of-the-art clustering methods",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1705.02436v9,2019-11-30T19:03:11Z,2017-05-06T03:13:21Z,Nonlinear Information Bottleneck,"Information bottleneck (IB) is a technique for extracting information in one
random variable $X$ that is relevant for predicting another random variable
$Y$. IB works by encoding $X$ in a compressed ""bottleneck"" random variable $M$
from which $Y$ can be accurately decoded. However, finding the optimal
bottleneck variable involves a difficult optimization problem, which until
recently has been considered for only two limited cases: discrete $X$ and $Y$
with small state spaces, and continuous $X$ and $Y$ with a Gaussian joint
distribution (in which case optimal encoding and decoding maps are linear). We
propose a method for performing IB on arbitrarily-distributed discrete and/or
continuous $X$ and $Y$, while allowing for nonlinear encoding and decoding
maps. Our approach relies on a novel non-parametric upper bound for mutual
information. We describe how to implement our method using neural networks. We
then show that it achieves better performance than the recently-proposed
""variational IB"" method on several real-world datasets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1705.00346v1,2017-04-30T17:17:44Z,2017-04-30T17:17:44Z,Deep Learning in the Automotive Industry: Applications and Tools,"Deep Learning refers to a set of machine learning techniques that utilize
neural networks with many hidden layers for tasks, such as image
classification, speech recognition, language understanding. Deep learning has
been proven to be very effective in these domains and is pervasively used by
many Internet services. In this paper, we describe different automotive uses
cases for deep learning in particular in the domain of computer vision. We
surveys the current state-of-the-art in libraries, tools and infrastructures
(e.\,g.\ GPUs and clouds) for implementing, training and deploying deep neural
networks. We particularly focus on convolutional neural networks and computer
vision use cases, such as the visual inspection process in manufacturing plants
and the analysis of social media data. To train neural networks, curated and
labeled datasets are essential. In particular, both the availability and scope
of such datasets is typically very limited. A main contribution of this paper
is the creation of an automotive dataset, that allows us to learn and
automatically recognize different vehicle properties. We describe an end-to-end
deep learning application utilizing a mobile app for data collection and
process support, and an Amazon-based cloud backend for storage and training.
For training we evaluate the use of cloud and on-premises infrastructures
(including multiple GPUs) in conjunction with different neural network
architectures and frameworks. We assess both the training times as well as the
accuracy of the classifier. Finally, we demonstrate the effectiveness of the
trained classifier in a real world setting during manufacturing process.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1704.07854v4,2019-02-20T13:27:28Z,2017-04-25T18:21:42Z,Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1704.06735v3,2017-06-12T19:47:02Z,2017-04-22T02:22:19Z,Asynchronous Distributed Variational Gaussian Processes for Regression,"Gaussian processes (GPs) are powerful non-parametric function estimators.
However, their applications are largely limited by the expensive computational
cost of the inference procedures. Existing stochastic or distributed
synchronous variational inferences, although have alleviated this issue by
scaling up GPs to millions of samples, are still far from satisfactory for
real-world large applications, where the data sizes are often orders of
magnitudes larger, say, billions. To solve this problem, we propose ADVGP, the
first Asynchronous Distributed Variational Gaussian Process inference for
regression, on the recent large-scale machine learning platform,
PARAMETERSERVER. ADVGP uses a novel, flexible variational framework based on a
weight space augmentation, and implements the highly efficient, asynchronous
proximal gradient optimization. While maintaining comparable or better
predictive performance, ADVGP greatly improves upon the efficiency of the
existing variational methods. With ADVGP, we effortlessly scale up GP
regression to a real-world application with billions of samples and demonstrate
an excellent, superior prediction accuracy to the popular linear models.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1704.02012v1,2017-04-06T20:25:36Z,2017-04-06T20:25:36Z,"A Software-equivalent SNN Hardware using RRAM-array for Asynchronous
  Real-time Learning","Spiking Neural Network (SNN) naturally inspires hardware implementation as it
is based on biology. For learning, spike time dependent plasticity (STDP) may
be implemented using an energy efficient waveform superposition on memristor
based synapse. However, system level implementation has three challenges.
First, a classic dilemma is that recognition requires current reading for short
voltage$-$spikes which is disturbed by large voltage$-$waveforms that are
simultaneously applied on the same memristor for real$-$time learning i.e. the
simultaneous read$-$write dilemma. Second, the hardware needs to exactly
replicate software implementation for easy adaptation of algorithm to hardware.
Third, the devices used in hardware simulations must be realistic. In this
paper, we present an approach to address the above concerns. First, the
learning and recognition occurs in separate arrays simultaneously in
real$-$time, asynchronously $-$ avoiding non$-$biomimetic clocking based
complex signal management. Second, we show that the hardware emulates software
at every stage by comparison of SPICE (circuit$-$simulator) with MATLAB
(mathematical SNN algorithm implementation in software) implementations. As an
example, the hardware shows 97.5 per cent accuracy in classification which is
equivalent to software for a Fisher$-$Iris dataset. Third, the STDP is
implemented using a model of synaptic device implemented using HfO2 memristor.
We show that an increasingly realistic memristor model slightly reduces the
hardware performance (85 per cent), which highlights the need to engineer RRAM
characteristics specifically for SNN.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1702.06329v1,2017-02-21T11:07:27Z,2017-02-21T11:07:27Z,"Towards a Common Implementation of Reinforcement Learning for Multiple
  Robotic Tasks","Mobile robots are increasingly being employed for performing complex tasks in
dynamic environments. Reinforcement learning (RL) methods are recognized to be
promising for specifying such tasks in a relatively simple manner. However, the
strong dependency between the learning method and the task to learn is a
well-known problem that restricts practical implementations of RL in robotics,
often requiring major modifications of parameters and adding other techniques
for each particular task. In this paper we present a practical core
implementation of RL which enables the learning process for multiple robotic
tasks with minimal per-task tuning or none. Based on value iteration methods,
this implementation includes a novel approach for action selection, called
Q-biased softmax regression (QBIASSR), which avoids poor performance of the
learning process when the robot reaches new unexplored states. Our approach
takes advantage of the structure of the state space by attending the physical
variables involved (e.g., distances to obstacles, X,Y,{\theta} pose, etc.),
thus experienced sets of states may favor the decision-making process of
unexplored or rarely-explored states. This improvement has a relevant role in
reducing the tuning of the algorithm for particular tasks. Experiments with
real and simulated robots, performed with the software framework also
introduced here, show that our implementation is effectively able to learn
different robotic tasks without tuning the learning method. Results also
suggest that the combination of true online SARSA({\lambda}) with QBIASSR can
outperform the existing RL core algorithms in low-dimensional robotic tasks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1702.05596v1,2017-02-18T10:47:16Z,2017-02-18T10:47:16Z,Brain Inspired Cognitive Model with Attention for Self-Driving Cars,"Perception-driven approach and end-to-end system are two major vision-based
frameworks for self-driving cars. However, it is difficult to introduce
attention and historical information of autonomous driving process, which are
the essential factors for achieving human-like driving into these two methods.
In this paper, we propose a novel model for self-driving cars named
brain-inspired cognitive model with attention (CMA). This model consists of
three parts: a convolutional neural network for simulating human visual cortex,
a cognitive map built to describe relationships between objects in complex
traffic scene and a recurrent neural network that combines with the real-time
updated cognitive map to implement attention mechanism and long-short term
memory. The benefit of our model is that can accurately solve three tasks
simultaneously:1) detection of the free space and boundaries of the current and
adjacent lanes. 2)estimation of obstacle distance and vehicle attitude, and 3)
learning of driving behavior and decision making from human driver. More
significantly, the proposed model could accept external navigating instructions
during an end-to-end driving process. For evaluation, we build a large-scale
road-vehicle dataset which contains more than forty thousand labeled road
images captured by three cameras on our self-driving car. Moreover, human
driving activities and vehicle states are recorded in the meanwhile.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1702.02978v1,2017-02-09T20:53:57Z,2017-02-09T20:53:57Z,"Elastic Resource Management with Adaptive State Space Partitioning of
  Markov Decision Processes","Modern large-scale computing deployments consist of complex applications
running over machine clusters. An important issue in these is the offering of
elasticity, i.e., the dynamic allocation of resources to applications to meet
fluctuating workload demands. Threshold based approaches are typically
employed, yet they are difficult to configure and optimize. Approaches based on
reinforcement learning have been proposed, but they require a large number of
states in order to model complex application behavior. Methods that adaptively
partition the state space have been proposed, but their partitioning criteria
and strategies are sub-optimal. In this work we present MDP_DT, a novel
full-model based reinforcement learning algorithm for elastic resource
management that employs adaptive state space partitioning. We propose two novel
statistical criteria and three strategies and we experimentally prove that they
correctly decide both where and when to partition, outperforming existing
approaches. We experimentally evaluate MDP_DT in a real large scale cluster
over variable not-encountered workloads and we show that it takes more informed
decisions compared to static and model-free approaches, while requiring a
minimal amount of training data.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1702.02676v1,2017-02-09T02:02:27Z,2017-02-09T02:02:27Z,Energy Saving Additive Neural Network,"In recent years, machine learning techniques based on neural networks for
mobile computing become increasingly popular. Classical multi-layer neural
networks require matrix multiplications at each stage. Multiplication operation
is not an energy efficient operation and consequently it drains the battery of
the mobile device. In this paper, we propose a new energy efficient neural
network with the universal approximation property over space of Lebesgue
integrable functions. This network, called, additive neural network, is very
suitable for mobile computing. The neural structure is based on a novel vector
product definition, called ef-operator, that permits a multiplier-free
implementation. In ef-operation, the ""product"" of two real numbers is defined
as the sum of their absolute values, with the sign determined by the sign of
the product of the numbers. This ""product"" is used to construct a vector
product in $R^N$. The vector product induces the $l_1$ norm. The proposed
additive neural network successfully solves the XOR problem. The experiments on
MNIST dataset show that the classification performances of the proposed
additive neural networks are very similar to the corresponding multi-layer
perceptron and convolutional neural networks (LeNet).",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1702.01780v1,2017-02-06T20:10:10Z,2017-02-06T20:10:10Z,"Toward the automated analysis of complex diseases in genome-wide
  association studies using genetic programming","Machine learning has been gaining traction in recent years to meet the demand
for tools that can efficiently analyze and make sense of the ever-growing
databases of biomedical data in health care systems around the world. However,
effectively using machine learning methods requires considerable domain
expertise, which can be a barrier of entry for bioinformaticians new to
computational data science methods. Therefore, off-the-shelf tools that make
machine learning more accessible can prove invaluable for bioinformaticians. To
this end, we have developed an open source pipeline optimization tool
(TPOT-MDR) that uses genetic programming to automatically design machine
learning pipelines for bioinformatics studies. In TPOT-MDR, we implement
Multifactor Dimensionality Reduction (MDR) as a feature construction method for
modeling higher-order feature interactions, and combine it with a new expert
knowledge-guided feature selector for large biomedical data sets. We
demonstrate TPOT-MDR's capabilities using a combination of simulated and real
world data sets from human genetics and find that TPOT-MDR significantly
outperforms modern machine learning methods such as logistic regression and
eXtreme Gradient Boosting (XGBoost). We further analyze the best pipeline
discovered by TPOT-MDR for a real world problem and highlight TPOT-MDR's
ability to produce a high-accuracy solution that is also easily interpretable.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1701.08878v1,2017-01-31T00:16:15Z,2017-01-31T00:16:15Z,"Deep Reinforcement Learning for Robotic Manipulation-The state of the
  art","The focus of this work is to enumerate the various approaches and algorithms
that center around application of reinforcement learning in robotic ma-
]]nipulation tasks. Earlier methods utilized specialized policy representations
and human demonstrations to constrict the policy. Such methods worked well with
continuous state and policy space of robots but failed to come up with
generalized policies. Subsequently, high dimensional non-linear function
approximators like neural networks have been used to learn policies from
scratch. Several novel and recent approaches have also embedded control policy
with efficient perceptual representation using deep learning. This has led to
the emergence of a new branch of dynamic robot control system called deep r
inforcement learning(DRL). This work embodies a survey of the most recent
algorithms, architectures and their implementations in simulations and real
world robotic platforms. The gamut of DRL architectures are partitioned into
two different branches namely, discrete action space algorithms(DAS) and
continuous action space algorithms(CAS). Further, the CAS algorithms are
divided into stochastic continuous action space(SCAS) and deterministic
continuous action space(DCAS) algorithms. Along with elucidating an organ-
isation of the DRL algorithms this work also manifests some of the state of the
art applications of these approaches in robotic manipulation tasks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1701.08748v3,2017-03-20T11:03:51Z,2017-01-30T18:39:38Z,"On the realistic validation of photometric redshifts, or why Teddy will
  never be Happy","Two of the main problems encountered in the development and accurate
validation of photometric redshift (photo-z) techniques are the lack of
spectroscopic coverage in feature space (e.g. colours and magnitudes) and the
mismatch between photometric error distributions associated with the
spectroscopic and photometric samples. Although these issues are well known,
there is currently no standard benchmark allowing a quantitative analysis of
their impact on the final photo-z estimation. In this work, we present two
galaxy catalogues, Teddy and Happy, built to enable a more demanding and
realistic test of photo-z methods. Using photometry from the Sloan Digital Sky
Survey and spectroscopy from a collection of sources, we constructed datasets
which mimic the biases between the underlying probability distribution of the
real spectroscopic and photometric sample. We demonstrate the potential of
these catalogues by submitting them to the scrutiny of different photo-z
methods, including machine learning (ML) and template fitting approaches.
Beyond the expected bad results from most ML algorithms for cases with missing
coverage in feature space, we were able to recognize the superiority of global
models in the same situation and the general failure across all types of
methods when incomplete coverage is convoluted with the presence of photometric
errors - a data situation which photo-z methods were not trained to deal with
up to now and which must be addressed by future large scale surveys. Our
catalogues represent the first controlled environment allowing a
straightforward implementation of such tests. The data are publicly available
within the COINtoolbox (https://github.com/COINtoolbox/photoz_catalogues).",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1701.00038v1,2016-12-31T00:27:42Z,2016-12-31T00:27:42Z,Sparsity enabled cluster reduced-order models for control,"Characterizing and controlling nonlinear, multi-scale phenomena play
important roles in science and engineering. Cluster-based reduced-order
modeling (CROM) was introduced to exploit the underlying low-dimensional
dynamics of complex systems. CROM builds a data-driven discretization of the
Perron-Frobenius operator, resulting in a probabilistic model for ensembles of
trajectories. A key advantage of CROM is that it embeds nonlinear dynamics in a
linear framework, and uncertainty can be managed with data assimilation. CROM
is typically computed on high-dimensional data, however, access to and
computations on this full-state data limit the online implementation of CROM
for prediction and control. Here, we address this key challenge by identifying
a small subset of critical measurements to learn an efficient CROM, referred to
as sparsity-enabled CROM. In particular, we leverage compressive measurements
to faithfully embed the cluster geometry and preserve the probabilistic
dynamics. Further, we show how to identify fewer optimized sensor locations
tailored to a specific problem that outperform random measurements. Both of
these sparsity-enabled sensing strategies significantly reduce the burden of
data acquisition and processing for low-latency in-time estimation and control.
We illustrate this unsupervised learning approach on three different
high-dimensional nonlinear dynamical systems from fluids with increasing
complexity, with one application in flow control. Sparsity-enabled CROM is a
critical facilitator for real-time implementation on high-dimensional systems
where full-state information may be inaccessible.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1612.06699v3,2017-06-12T21:38:17Z,2016-12-20T15:04:38Z,Unsupervised Perceptual Rewards for Imitation Learning,"Reward function design and exploration time are arguably the biggest
obstacles to the deployment of reinforcement learning (RL) agents in the real
world. In many real-world tasks, designing a reward function takes considerable
hand engineering and often requires additional sensors to be installed just to
measure whether the task has been executed successfully. Furthermore, many
interesting tasks consist of multiple implicit intermediate steps that must be
executed in sequence. Even when the final outcome can be measured, it does not
necessarily provide feedback on these intermediate steps. To address these
issues, we propose leveraging the abstraction power of intermediate visual
representations learned by deep models to quickly infer perceptual reward
functions from small numbers of demonstrations. We present a method that is
able to identify key intermediate steps of a task from only a handful of
demonstration sequences, and automatically identify the most discriminative
features for identifying these steps. This method makes use of the features in
a pre-trained deep model, but does not require any explicit specification of
sub-goals. The resulting reward functions can then be used by an RL agent to
learn to perform the task in real-world settings. To evaluate the learned
reward, we present qualitative results on two real-world tasks and a
quantitative evaluation against a human-designed reward function. We also show
that our method can be used to learn a real-world door opening skill using a
real robot, even when the demonstration used for reward learning is provided by
a human using their own hand. To our knowledge, these are the first results
showing that complex robotic manipulation skills can be learned directly and
without supervised labels from a video of a human performing the task.
Supplementary material and data are available at
https://sermanet.github.io/rewards",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1611.08930v2,2017-03-28T03:15:07Z,2016-11-27T22:47:23Z,Deep attractor network for single-microphone speaker separation,"Despite the overwhelming success of deep learning in various speech
processing tasks, the problem of separating simultaneous speakers in a mixture
remains challenging. Two major difficulties in such systems are the arbitrary
source permutation and unknown number of sources in the mixture. We propose a
novel deep learning framework for single channel speech separation by creating
attractor points in high dimensional embedding space of the acoustic signals
which pull together the time-frequency bins corresponding to each source.
Attractor points in this study are created by finding the centroids of the
sources in the embedding space, which are subsequently used to determine the
similarity of each bin in the mixture to each source. The network is then
trained to minimize the reconstruction error of each source by optimizing the
embeddings. The proposed model is different from prior works in that it
implements an end-to-end training, and it does not depend on the number of
sources in the mixture. Two strategies are explored in the test time, K-means
and fixed attractor points, where the latter requires no post-processing and
can be implemented in real-time. We evaluated our system on Wall Street Journal
dataset and show 5.49\% improvement over the previous state-of-the-art methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1611.03313v1,2016-11-10T14:32:24Z,2016-11-10T14:32:24Z,X-ray Scattering Image Classification Using Deep Learning,"Visual inspection of x-ray scattering images is a powerful technique for
probing the physical structure of materials at the molecular scale. In this
paper, we explore the use of deep learning to develop methods for automatically
analyzing x-ray scattering images. In particular, we apply Convolutional Neural
Networks and Convolutional Autoencoders for x-ray scattering image
classification. To acquire enough training data for deep learning, we use
simulation software to generate synthetic x-ray scattering images. Experiments
show that deep learning methods outperform previously published methods by 10\%
on synthetic and real datasets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1610.09027v1,2016-10-27T22:38:05Z,2016-10-27T22:38:05Z,Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes,"Neural networks augmented with external memory have the ability to learn
algorithmic solutions to complex tasks. These models appear promising for
applications such as language modeling and machine translation. However, they
scale poorly in both space and time as the amount of memory grows --- limiting
their applicability to real-world domains. Here, we present an end-to-end
differentiable memory access scheme, which we call Sparse Access Memory (SAM),
that retains the representational power of the original approaches whilst
training efficiently with very large memories. We show that SAM achieves
asymptotic lower bounds in space and time complexity, and find that an
implementation runs $1,\!000\times$ faster and with $3,\!000\times$ less
physical memory than non-sparse models. SAM learns with comparable data
efficiency to existing models on a range of synthetic tasks and one-shot
Omniglot character recognition, and can scale to tasks requiring $100,\!000$s
of time steps and memories. As well, we show how our approach can be adapted
for models that maintain temporal associations between memories, as with the
recently introduced Differentiable Neural Computer.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1610.07862v2,2016-10-26T02:32:30Z,2016-10-24T02:15:46Z,Intelligence in Artificial Intelligence,"The elusive quest for intelligence in artificial intelligence prompts us to
consider that instituting human-level intelligence in systems may be (still) in
the realm of utopia. In about a quarter century, we have witnessed the winter
of AI (1990) being transformed and transported to the zenith of tabloid fodder
about AI (2015). The discussion at hand is about the elements that constitute
the canonical idea of intelligence. The delivery of intelligence as a
pay-per-use-service, popping out of an app or from a shrink-wrapped software
defined point solution, is in contrast to the bio-inspired view of intelligence
as an outcome, perhaps formed from a tapestry of events, cross-pollinated by
instances, each with its own microcosm of experiences and learning, which may
not be discrete all-or-none functions but continuous, over space and time. The
enterprise world may not require, aspire or desire such an engaged solution to
improve its services for enabling digital transformation through the deployment
of digital twins, for example. One might ask whether the ""work-flow on
steroids"" version of decision support may suffice for intelligence? Are we
harking back to the era of rule based expert systems? The image conjured by the
publicity machines offers deep solutions with human-level AI and preposterous
claims about capturing the ""brain in a box"" by 2020. Even emulating insects may
be difficult in terms of real progress. Perhaps we can try to focus on worms
(Caenorhabditis elegans) which may be better suited for what business needs to
quench its thirst for so-called intelligence in AI.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1610.04872v1,2016-10-16T15:14:36Z,2016-10-16T15:14:36Z,"Fault Detection Engine in Intelligent Predictive Analytics Platform for
  DCIM","With the advancement of huge data generation and data handling capability,
Machine Learning and Probabilistic modelling enables an immense opportunity to
employ predictive analytics platform in high security critical industries
namely data centers, electricity grids, utilities, airport etc. where downtime
minimization is one of the primary objectives. This paper proposes a novel,
complete architecture of an intelligent predictive analytics platform, Fault
Engine, for huge device network connected with electrical/information flow.
Three unique modules, here proposed, seamlessly integrate with available
technology stack of data handling and connect with middleware to produce online
intelligent prediction in critical failure scenarios. The Markov Failure module
predicts the severity of a failure along with survival probability of a device
at any given instances. The Root Cause Analysis model indicates probable
devices as potential root cause employing Bayesian probability assignment and
topological sort. Finally, a community detection algorithm produces correlated
clusters of device in terms of failure probability which will further narrow
down the search space of finding route cause. The whole Engine has been tested
with different size of network with simulated failure environments and shows
its potential to be scalable in real-time implementation.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1609.08018v1,2016-09-26T15:15:09Z,2016-09-26T15:15:09Z,"Small near-Earth asteroids in the Palomar Transient Factory survey: A
  real-time streak-detection system","Near-Earth asteroids (NEAs) in the 1-100 meter size range are estimated to be
$\sim$1,000 times more numerous than the $\sim$15,000 currently-catalogued
NEAs, most of which are in the 0.5-10 kilometer size range. Impacts from 10-100
meter size NEAs are not statistically life-threatening but may cause
significant regional damage, while 1-10 meter size NEAs with low velocities
relative to Earth are compelling targets for space missions. We describe the
implementation and initial results of a real-time NEA-discovery system
specialized for the detection of small, high angular rate (visually-streaked)
NEAs in Palomar Transient Factory (PTF) images. PTF is a 1.2-m aperture,
7.3-deg$^2$ field-of-view optical survey designed primarily for the discovery
of extragalactic transients (e.g., supernovae) in 60-second exposures reaching
$\sim$20.5 visual magnitude. Our real-time NEA discovery pipeline uses a
machine-learned classifier to filter a large number of false-positive streak
detections, permitting a human scanner to efficiently and remotely identify
real asteroid streaks during the night. Upon recognition of a streaked NEA
detection (typically within an hour of the discovery exposure), the scanner
triggers follow-up with the same telescope and posts the observations to the
Minor Planet Center for worldwide confirmation. We describe our ten initial
confirmed discoveries, all small NEAs that passed 0.3-15 lunar distances from
Earth. Lastly, we derive useful scaling laws for comparing
streaked-NEA-detection capabilities of different surveys as a function of their
hardware and survey-pattern characteristics. This work most directly informs
estimates of the streak-detection capabilities of the Zwicky Transient Facility
(ZTF, planned to succeed PTF in 2017), which will apply PTF's current
resolution and sensitivity over a 47-deg$^2$ field-of-view.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1609.02664v1,2016-09-09T06:04:17Z,2016-09-09T06:04:17Z,"Machine Learning with Guarantees using Descriptive Complexity and SMT
  Solvers","Machine learning is a thriving part of computer science. There are many
efficient approaches to machine learning that do not provide strong theoretical
guarantees, and a beautiful general learning theory. Unfortunately, machine
learning approaches that give strong theoretical guarantees have not been
efficient enough to be applicable. In this paper we introduce a logical
approach to machine learning. Models are represented by tuples of logical
formulas and inputs and outputs are logical structures. We present our
framework together with several applications where we evaluate it using SAT and
SMT solvers. We argue that this approach to machine learning is particularly
suited to bridge the gap between efficiency and theoretical soundness. We
exploit results from descriptive complexity theory to prove strong theoretical
guarantees for our approach. To show its applicability, we present experimental
results including learning complexity-theoretic reductions rules for board
games. We also explain how neural networks fit into our framework, although the
current implementation does not scale to provide guarantees for real-world
neural networks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1609.01926v1,2016-09-07T10:44:28Z,2016-09-07T10:44:28Z,"A modular architecture for transparent computation in Recurrent Neural
  Networks","Computation is classically studied in terms of automata, formal languages and
algorithms; yet, the relation between neural dynamics and symbolic
representations and operations is still unclear in traditional eliminative
connectionism. Therefore, we suggest a unique perspective on this central
issue, to which we would like to refer as to transparent connectionism, by
proposing accounts of how symbolic computation can be implemented in neural
substrates. In this study we first introduce a new model of dynamics on a
symbolic space, the versatile shift, showing that it supports the real-time
simulation of a range of automata. We then show that the Goedelization of
versatile shifts defines nonlinear dynamical automata, dynamical systems
evolving on a vectorial space. Finally, we present a mapping between nonlinear
dynamical automata and recurrent artificial neural networks. The mapping
defines an architecture characterized by its granular modularity, where data,
symbolic operations and their control are not only distinguishable in
activation space, but also spatially localizable in the network itself, while
maintaining a distributed encoding of symbolic representations. The resulting
networks simulate automata in real-time and are programmed directly, in absence
of network training. To discuss the unique characteristics of the architecture
and their consequences, we present two examples: i) the design of a Central
Pattern Generator from a finite-state locomotive controller, and ii) the
creation of a network simulating a system of interactive automata that supports
the parsing of garden-path sentences as investigated in psycholinguistics
experiments.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1611.00315v1,2016-08-28T20:04:25Z,2016-08-28T20:04:25Z,"Rapid Prototyping of a Text Mining Application for Cryptocurrency Market
  Intelligence","Blockchain represents a technology for establishing a shared, immutable
version of the truth between a network of participants that do not trust one
another, and therefore has the potential to disrupt any financial or other
industries that rely on third-parties to establish trust. Recent trends in
computing including: prevalence of Free and Open Source Software (FOSS); easy
access to High Performance Computing (HPC i.e. 'The Cloud'); and increasingly
advanced analytics capabilities such as Natural Language Processing (NLP) and
Machine Learning (ML) allow for rapidly prototyping applications for analysis
of trends in the emergence of Blockchain technology. A scaleable
proof-of-concept pipeline that lays the groundwork for analysis of multiple
streams of semi-structured data posted on social media is demonstrated.
Preliminary analysis and performance metrics are presented and discussed.
Future work is described that will scale the system to cloud-based, real-time,
analysis of multiple data streams, with Information Extraction (IE) (ex.
sentiment analysis) and Machine Learning capability.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1608.01987v1,2016-08-05T19:55:57Z,2016-08-05T19:55:57Z,Human collective intelligence as distributed Bayesian inference,"Collective intelligence is believed to underly the remarkable success of
human society. The formation of accurate shared beliefs is one of the key
components of human collective intelligence. How are accurate shared beliefs
formed in groups of fallible individuals? Answering this question requires a
multiscale analysis. We must understand both the individual decision mechanisms
people use, and the properties and dynamics of those mechanisms in the
aggregate. As of yet, mathematical tools for such an approach have been
lacking. To address this gap, we introduce a new analytical framework: We
propose that groups arrive at accurate shared beliefs via distributed Bayesian
inference. Distributed inference occurs through information processing at the
individual level, and yields rational belief formation at the group level. We
instantiate this framework in a new model of human social decision-making,
which we validate using a dataset we collected of over 50,000 users of an
online social trading platform where investors mimic each others' trades using
real money in foreign exchange and other asset markets. We find that in this
setting people use a decision mechanism in which popularity is treated as a
prior distribution for which decisions are best to make. This mechanism is
boundedly rational at the individual level, but we prove that in the aggregate
implements a type of approximate ""Thompson sampling""---a well-known and highly
effective single-agent Bayesian machine learning algorithm for sequential
decision-making. The perspective of distributed Bayesian inference therefore
reveals how collective rationality emerges from the boundedly rational decision
mechanisms people use.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1607.02168v1,2016-07-07T20:47:05Z,2016-07-07T20:47:05Z,Discovering Boolean Gates in Slime Mould,"Slime mould of Physarum polycephalum is a large cell exhibiting rich spatial
non-linear electrical characteristics. We exploit the electrical properties of
the slime mould to implement logic gates using a flexible hardware platform
designed for investigating the electrical properties of a substrate (MECOBO).
We apply arbitrary electrical signals to `configure' the slime mould, i.e.
change shape of its body and, measure the slime mould's electrical response. We
show that it is possible to find configurations that allow the Physarum to act
as any 2-input Boolean gate. The occurrence frequency of the gates discovered
in the slime was analysed and compared to complexity hierarchies of logical
gates obtained in other unconventional materials. The search for gates was
performed by both sweeping across configurations in the real material as well
as training a neural network-based model and searching the gates therein using
gradient descent.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1606.03966v2,2017-05-09T14:41:15Z,2016-06-13T14:17:00Z,Making Contextual Decisions with Low Technical Debt,"Applications and systems are constantly faced with decisions that require
picking from a set of actions based on contextual information.
Reinforcement-based learning algorithms such as contextual bandits can be very
effective in these settings, but applying them in practice is fraught with
technical debt, and no general system exists that supports them completely. We
address this and create the first general system for contextual learning,
called the Decision Service.
  Existing systems often suffer from technical debt that arises from issues
like incorrect data collection and weak debuggability, issues we systematically
address through our ML methodology and system abstractions. The Decision
Service enables all aspects of contextual bandit learning using four system
abstractions which connect together in a loop: explore (the decision space),
log, learn, and deploy. Notably, our new explore and log abstractions ensure
the system produces correct, unbiased data, which our learner uses for online
learning and to enable real-time safeguards, all in a fully reproducible
manner.
  The Decision Service has a simple user interface and works with a variety of
applications: we present two live production deployments for content
recommendation that achieved click-through improvements of 25-30%, another with
18% revenue lift in the landing page, and ongoing applications in tech support
and machine failure handling. The service makes real-time decisions and learns
continuously and scalably, while significantly lowering technical debt.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1606.03212v1,2016-06-10T07:17:00Z,2016-06-10T07:17:00Z,"Discovery of Latent Factors in High-dimensional Data Using Tensor
  Methods","Unsupervised learning aims at the discovery of hidden structure that drives
the observations in the real world. It is essential for success in modern
machine learning. Latent variable models are versatile in unsupervised learning
and have applications in almost every domain. Training latent variable models
is challenging due to the non-convexity of the likelihood objective. An
alternative method is based on the spectral decomposition of low order moment
tensors. This versatile framework is guaranteed to estimate the correct model
consistently. My thesis spans both theoretical analysis of tensor decomposition
framework and practical implementation of various applications. This thesis
presents theoretical results on convergence to globally optimal solution of
tensor decomposition using the stochastic gradient descent, despite
non-convexity of the objective. This is the first work that gives global
convergence guarantees for the stochastic gradient descent on non-convex
functions with exponentially many local minima and saddle points. This thesis
also presents large-scale deployment of spectral methods carried out on various
platforms. Dimensionality reduction techniques such as random projection are
incorporated for a highly parallel and scalable tensor decomposition algorithm.
We obtain a gain in both accuracies and in running times by several orders of
magnitude compared to the state-of-art variational methods. To solve real world
problems, more advanced models and learning algorithms are proposed. This
thesis discusses generalization of LDA model to mixed membership stochastic
block model for learning user communities in social network, convolutional
dictionary model for learning word-sequence embeddings, hierarchical tensor
decomposition and latent tree structure model for learning disease hierarchy,
and spatial point process mixture model for detecting cell types in
neuroscience.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1604.05091v2,2016-04-19T14:09:26Z,2016-04-18T11:15:56Z,"End-to-End Tracking and Semantic Segmentation Using Recurrent Neural
  Networks","In this work we present a novel end-to-end framework for tracking and
classifying a robot's surroundings in complex, dynamic and only partially
observable real-world environments. The approach deploys a recurrent neural
network to filter an input stream of raw laser measurements in order to
directly infer object locations, along with their identity in both visible and
occluded areas. To achieve this we first train the network using unsupervised
Deep Tracking, a recently proposed theoretical framework for end-to-end space
occupancy prediction. We show that by learning to track on a large amount of
unsupervised data, the network creates a rich internal representation of its
environment which we in turn exploit through the principle of inductive
transfer of knowledge to perform the task of it's semantic classification. As a
result, we show that only a small amount of labelled data suffices to steer the
network towards mastering this additional task. Furthermore we propose a novel
recurrent neural network architecture specifically tailored to tracking and
semantic classification in real-world robotics applications. We demonstrate the
tracking and classification performance of the method on real-world data
collected at a busy road junction. Our evaluation shows that the proposed
end-to-end framework compares favourably to a state-of-the-art, model-free
tracking solution and that it outperforms a conventional one-shot training
scheme for semantic classification.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1603.06212v1,2016-03-20T13:32:27Z,2016-03-20T13:32:27Z,"Evaluation of a Tree-based Pipeline Optimization Tool for Automating
  Data Science","As the field of data science continues to grow, there will be an
ever-increasing demand for tools that make machine learning accessible to
non-experts. In this paper, we introduce the concept of tree-based pipeline
optimization for automating one of the most tedious parts of machine
learning---pipeline design. We implement an open source Tree-based Pipeline
Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a
series of simulated and real-world benchmark data sets. In particular, we show
that TPOT can design machine learning pipelines that provide a significant
improvement over a basic machine learning analysis while requiring little to no
input nor prior knowledge from the user. We also address the tendency for TPOT
to design overly complex pipelines by integrating Pareto optimization, which
produces compact pipelines without sacrificing classification accuracy. As
such, this work represents an important step toward fully automating machine
learning pipeline design.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1602.07188v2,2016-03-13T21:13:57Z,2016-02-23T15:17:55Z,Exploring the Neural Algorithm of Artistic Style,"We explore the method of style transfer presented in the article ""A Neural
Algorithm of Artistic Style"" by Leon A. Gatys, Alexander S. Ecker and Matthias
Bethge (arXiv:1508.06576).
  We first demonstrate the power of the suggested style space on a few
examples. We then vary different hyper-parameters and program properties that
were not discussed in the original paper, among which are the recognition
network used, starting point of the gradient descent and different ways to
partition style and content layers. We also give a brief comparison of some of
the existing algorithm implementations and deep learning frameworks used.
  To study the style space further we attempt to generate synthetic images by
maximizing a single entry in one of the Gram matrices $\mathcal{G}_l$ and some
interesting results are observed. Next, we try to mimic the sparsity and
intensity distribution of Gram matrices obtained from a real painting and
generate more complex textures.
  Finally, we propose two new style representations built on top of network's
features and discuss how one could be used to achieve local and potentially
content-aware style transfer.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1601.07925v1,2016-01-28T21:45:55Z,2016-01-28T21:45:55Z,"Automating biomedical data science through tree-based pipeline
  optimization","Over the past decade, data science and machine learning has grown from a
mysterious art form to a staple tool across a variety of fields in academia,
business, and government. In this paper, we introduce the concept of tree-based
pipeline optimization for automating one of the most tedious parts of machine
learning---pipeline design. We implement a Tree-based Pipeline Optimization
Tool (TPOT) and demonstrate its effectiveness on a series of simulated and
real-world genetic data sets. In particular, we show that TPOT can build
machine learning pipelines that achieve competitive classification accuracy and
discover novel pipeline operators---such as synthetic feature
constructors---that significantly improve classification accuracy on these data
sets. We also highlight the current challenges to pipeline optimization, such
as the tendency to produce pipelines that overfit the data, and suggest future
research paths to overcome these challenges. As such, this work represents an
early step toward fully automating machine learning pipeline design.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1601.04385v1,2016-01-18T02:06:45Z,2016-01-18T02:06:45Z,Real-Time Data Mining of Massive Data Streams from Synoptic Sky Surveys,"The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1512.01818v5,2016-07-14T22:51:39Z,2015-12-06T18:52:51Z,"SentiBench - a benchmark comparison of state-of-the-practice sentiment
  analysis methods","In the last few years thousands of scientific papers have investigated
sentiment analysis, several startups that measure opinions on real data have
emerged and a number of innovative products related to this theme have been
developed. There are multiple methods for measuring sentiments, including
lexical-based and supervised machine learning methods. Despite the vast
interest on the theme and wide popularity of some methods, it is unclear which
one is better for identifying the polarity (i.e., positive or negative) of a
message. Accordingly, there is a strong need to conduct a thorough
apple-to-apple comparison of sentiment analysis methods, \textit{as they are
used in practice}, across multiple datasets originated from different data
sources. Such a comparison is key for understanding the potential limitations,
advantages, and disadvantages of popular methods. This article aims at filling
this gap by presenting a benchmark comparison of twenty-four popular sentiment
analysis methods (which we call the state-of-the-practice methods). Our
evaluation is based on a benchmark of eighteen labeled datasets, covering
messages posted on social networks, movie and product reviews, as well as
opinions and comments in news articles. Our results highlight the extent to
which the prediction performance of these methods varies considerably across
datasets. Aiming at boosting the development of this research area, we open the
methods' codes and datasets used in this article, deploying them in a benchmark
system, which provides an open API for accessing and comparing sentence-level
sentiment analysis methods.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1512.01192v2,2018-04-25T13:40:35Z,2015-12-03T19:06:16Z,Prototypical Priors: From Improving Classification to Zero-Shot Learning,"Recent works on zero-shot learning make use of side information such as
visual attributes or natural language semantics to define the relations between
output visual classes and then use these relationships to draw inference on new
unseen classes at test time. In a novel extension to this idea, we propose the
use of visual prototypical concepts as side information. For most real-world
visual object categories, it may be difficult to establish a unique prototype.
However, in cases such as traffic signs, brand logos, flags, and even natural
language characters, these prototypical templates are available and can be
leveraged for an improved recognition performance. The present work proposes a
way to incorporate this prototypical information in a deep learning framework.
Using prototypes as prior information, the deepnet pipeline learns the input
image projections into the prototypical embedding space subject to minimization
of the final classification loss. Based on our experiments with two different
datasets of traffic signs and brand logos, prototypical embeddings incorporated
in a conventional convolutional neural network improve the recognition
performance. Recognition accuracy on the Belga logo dataset is especially
noteworthy and establishes a new state-of-the-art. In zero-shot learning
scenarios, the same system can be directly deployed to draw inference on unseen
classes by simply adding the prototypical information for these new classes at
test time. Thus, unlike earlier approaches, testing on seen and unseen classes
is handled using the same pipeline, and the system can be tuned for a trade-off
of seen and unseen class performance as per task requirement. Comparison with
one of the latest works in the zero-shot learning domain yields top results on
the two datasets mentioned above.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1511.09120v4,2017-12-18T14:04:48Z,2015-11-30T00:44:41Z,Coresets for Kinematic Data: From Theorems to Real-Time Systems,"A coreset (or core-set) of a dataset is its semantic compression with respect
to a set of queries, such that querying the (small) coreset provably yields an
approximate answer to querying the original (full) dataset. In the last decade,
coresets provided breakthroughs in theoretical computer science for
approximation algorithms, and more recently, in the machine learning community
for learning ""Big data"". However, we are not aware of real-time systems that
compute coresets in a rate of dozens of frames per second. In this paper we
suggest a framework to turn theorems to such systems using coresets. We begin
with a proof of independent interest, that any set of $n$ matrices in
$\mathbb{R}^{d\times d}$ whose sum is $S$, has a positively weighted subset
whose sum has the same center of mass (mean) and orientation (left+right
singular vectors) as $S$, and consists of $O(dr)$ matrices (independent of
$n$), where $r\leq d$ is the rank of $S$. We provide an algorithm that computes
this (core) set in one pass over possibly infinite stream of matrices in
$d^{O(1)}$ time per matrix insertion. By maintaining such a coreset for
kinematic (moving) set of $n$ points, we can run pose-estimation algorithms,
such as Kabsch or PnP, on the small coresets, instead of the $n$ points, in
real-time using weak devices, while obtaining the same results. This enabled us
to implement a low-cost ($<\$100$) IoT wireless system that tracks a toy (and
harmless) quadcopter which guides guests to a desired room (in a hospital,
mall, hotel, museum, etc.) with no help of additional human or remote
controller. We hope that our framework will encourage researchers outside the
theoretical community to design and use coresets in future systems and papers.
To this end, we provide extensive experimental results on both synthetic and
real data, as well as a link to the open code of our system and algorithms.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1511.06201v1,2015-11-19T15:14:02Z,2015-11-19T15:14:02Z,Adjustable Bounded Rectifiers: Towards Deep Binary Representations,"Binary representation is desirable for its memory efficiency, computation
speed and robustness. In this paper, we propose adjustable bounded rectifiers
to learn binary representations for deep neural networks. While hard
constraining representations across layers to be binary makes training
unreasonably difficult, we softly encourage activations to diverge from real
values to binary by approximating step functions. Our final representation is
completely binary. We test our approach on MNIST, CIFAR10, and ILSVRC2012
dataset, and systematically study the training dynamics of the binarization
process. Our approach can binarize the last layer representation without loss
of performance and binarize all the layers with reasonably small degradations.
The memory space that it saves may allow more sophisticated models to be
deployed, thus compensating the loss. To the best of our knowledge, this is the
first work to report results on current deep network architectures using
complete binary middle representations. Given the learned representations, we
find that the firing or inhibition of a binary neuron is usually associated
with a meaningful interpretation across different classes. This suggests that
the semantic structure of a neural network may be manifested through a guided
binarization process.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1510.02055v1,2015-10-07T18:34:36Z,2015-10-07T18:34:36Z,"Diverse Large-Scale ITS Dataset Created from Continuous Learning for
  Real-Time Vehicle Detection","In traffic engineering, vehicle detectors are trained on limited datasets
resulting in poor accuracy when deployed in real world applications. Annotating
large-scale high quality datasets is challenging. Typically, these datasets
have limited diversity; they do not reflect the real-world operating
environment. There is a need for a large-scale, cloud based positive and
negative mining (PNM) process and a large-scale learning and evaluation system
for the application of traffic event detection. The proposed positive and
negative mining process addresses the quality of crowd sourced ground truth
data through machine learning review and human feedback mechanisms. The
proposed learning and evaluation system uses a distributed cloud computing
framework to handle data-scaling issues associated with large numbers of
samples and a high-dimensional feature space. The system is trained using
AdaBoost on $1,000,000$ Haar-like features extracted from $70,000$ annotated
video frames. The trained real-time vehicle detector achieves an accuracy of at
least $95\%$ for $1/2$ and about $78\%$ for $19/20$ of the time when tested on
approximately $7,500,000$ video frames. At the end of 2015, the dataset is
expect to have over one billion annotated video frames.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1508.00317v1,2015-08-03T05:58:52Z,2015-08-03T05:58:52Z,"Time-series modeling with undecimated fully convolutional neural
  networks","We present a new convolutional neural network-based time-series model.
Typical convolutional neural network (CNN) architectures rely on the use of
max-pooling operators in between layers, which leads to reduced resolution at
the top layers. Instead, in this work we consider a fully convolutional network
(FCN) architecture that uses causal filtering operations, and allows for the
rate of the output signal to be the same as that of the input signal. We
furthermore propose an undecimated version of the FCN, which we refer to as the
undecimated fully convolutional neural network (UFCNN), and is motivated by the
undecimated wavelet transform. Our experimental results verify that using the
undecimated version of the FCN is necessary in order to allow for effective
time-series modeling. The UFCNN has several advantages compared to other
time-series models such as the recurrent neural network (RNN) and long
short-term memory (LSTM), since it does not suffer from either the vanishing or
exploding gradients problems, and is therefore easier to train. Convolution
operations can also be implemented more efficiently compared to the recursion
that is involved in RNN-based models. We evaluate the performance of our model
in a synthetic target tracking task using bearing only measurements generated
from a state-space model, a probabilistic modeling of polyphonic music
sequences problem, and a high frequency trading task using a time-series of
ask/bid quotes and their corresponding volumes. Our experimental results using
synthetic and real datasets verify the significant advantages of the UFCNN
compared to the RNN and LSTM baselines.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1502.07402v2,2015-06-15T13:55:15Z,2015-02-25T23:40:19Z,"Real-time capable first principle based modelling of tokamak turbulent
  transport","A real-time capable core turbulence tokamak transport model is developed.
This model is constructed from the regularized nonlinear regression of
quasilinear gyrokinetic transport code output. The regression is performed with
a multilayer perceptron neural network. The transport code input for the neural
network training set consists of five dimensions, and is limited to adiabatic
electrons. The neural network model successfully reproduces transport fluxes
predicted by the original quasilinear model, while gaining five orders of
magnitude in computation time. The model is implemented in a real-time capable
tokamak simulator, and simulates a 300s ITER discharge in 10s. This
proof-of-principle for regression based transport models anticipates a
significant widening of input space dimensionality and physics realism for
future training sets. This aims to provide unprecedented computational speed
coupled with first-principle based physics for real-time control and integrated
modelling applications.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1411.0778v1,2014-11-04T03:48:20Z,2014-11-04T03:48:20Z,"Detecting Suicidal Ideation in Chinese Microblogs with Psychological
  Lexicons","Suicide is among the leading causes of death in China. However, technical
approaches toward preventing suicide are challenging and remaining under
development. Recently, several actual suicidal cases were preceded by users who
posted microblogs with suicidal ideation to Sina Weibo, a Chinese social media
network akin to Twitter. It would therefore be desirable to detect suicidal
ideations from microblogs in real-time, and immediately alert appropriate
support groups, which may lead to successful prevention. In this paper, we
propose a real-time suicidal ideation detection system deployed over Weibo,
using machine learning and known psychological techniques. Currently, we have
identified 53 known suicidal cases who posted suicide notes on Weibo prior to
their deaths.We explore linguistic features of these known cases using a
psychological lexicon dictionary, and train an effective suicidal Weibo post
detection model. 6714 tagged posts and several classifiers are used to verify
the model. By combining both machine learning and psychological knowledge, SVM
classifier has the best performance of different classifiers, yielding an
F-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1411.0440v8,2020-04-19T19:58:37Z,2014-11-03T11:50:19Z,Modelling serendipity in a computational context,"The term serendipity describes a creative process that develops, in context,
with the active participation of a creative agent, but not entirely within that
agent's control. While a system cannot be made to perform serendipitously on
demand, we argue that its $\mathit{serendipity\ potential}$ can be increased by
means of a suitable system architecture and other design choices. We distil a
unified description of serendipitous occurrences from historical theorisations
of serendipity and creativity. This takes the form of a framework with six
phases: $\mathit{perception}$, $\mathit{attention}$, $\mathit{interest}$,
$\mathit{explanation}$, $\mathit{bridge}$, and $\mathit{valuation}$. We then
use this framework to organise a survey of literature in cognitive science,
philosophy, and computing, which yields practical definitions of the six
phases, along with heuristics for implementation. We use the resulting model to
evaluate the serendipity potential of four existing systems developed by
others, and two systems previously developed by two of the authors. Most
existing research that considers serendipity in a computing context deals with
serendipity as a service; here we relate theories of serendipity to the
development of autonomous systems and computational creativity practice. We
argue that serendipity is not teleologically blind, and outline representative
directions for future applications of our model. We conclude that it is
feasible to equip computational systems with the potential for serendipity, and
that this could be beneficial in varied computational creativity/AI
applications, particularly those designed to operate responsively in real-world
contexts.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1410.5792v1,2014-10-21T19:17:19Z,2014-10-21T19:17:19Z,"Generalized Compression Dictionary Distance as Universal Similarity
  Measure","We present a new similarity measure based on information theoretic measures
which is superior than Normalized Compression Distance for clustering problems
and inherits the useful properties of conditional Kolmogorov complexity. We
show that Normalized Compression Dictionary Size and Normalized Compression
Dictionary Entropy are computationally more efficient, as the need to perform
the compression itself is eliminated. Also they scale linearly with exponential
vector size growth and are content independent. We show that normalized
compression dictionary distance is compressor independent, if limited to
lossless compressors, which gives space for optimizations and implementation
speed improvement for real-time and big data applications. The introduced
measure is applicable for machine learning tasks of parameter-free unsupervised
clustering, supervised learning such as classification and regression, feature
selection, and is applicable for big data problems with order of magnitude
speed increase.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1409.7699v3,2018-12-30T17:43:46Z,2014-09-26T20:00:14Z,"The Overlooked Potential of Generalized Linear Models in Astronomy-II:
  Gamma regression and photometric redshifts","Machine learning techniques offer a precious tool box for use within
astronomy to solve problems involving so-called big data. They provide a means
to make accurate predictions about a particular system without prior knowledge
of the underlying physical processes of the data. In this article, and the
companion papers of this series, we present the set of Generalized Linear
Models (GLMs) as a fast alternative method for tackling general astronomical
problems, including the ones related to the machine learning paradigm. To
demonstrate the applicability of GLMs to inherently positive and continuous
physical observables, we explore their use in estimating the photometric
redshifts of galaxies from their multi-wavelength photometry. Using the gamma
family with a log link function we predict redshifts from the PHoto-z Accuracy
Testing simulated catalogue and a subset of the Sloan Digital Sky Survey from
Data Release 10. We obtain fits that result in catastrophic outlier rates as
low as ~1% for simulated and ~2% for real data. Moreover, we can easily obtain
such levels of precision within a matter of seconds on a normal desktop
computer and with training sets that contain merely thousands of galaxies. Our
software is made publicly available as an user-friendly package developed in
Python, R and via an interactive web application
(https://cosmostatisticsinitiative.shinyapps.io/CosmoPhotoz). This software
allows users to apply a set of GLMs to their own photometric catalogues and
generates publication quality plots with minimum effort from the user. By
facilitating their ease of use to the astronomical community, this paper series
aims to make GLMs widely known and to encourage their implementation in future
large-scale projects, such as the Large Synoptic Survey Telescope.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1409.0470v1,2014-09-01T16:20:41Z,2014-09-01T16:20:41Z,"Neural coordination can be enhanced by occasional interruption of normal
  firing patterns: A self-optimizing spiking neural network model","The state space of a conventional Hopfield network typically exhibits many
different attractors of which only a small subset satisfy constraints between
neurons in a globally optimal fashion. It has recently been demonstrated that
combining Hebbian learning with occasional alterations of normal neural states
avoids this problem by means of self-organized enlargement of the best basins
of attraction. However, so far it is not clear to what extent this process of
self-optimization is also operative in real brains. Here we demonstrate that it
can be transferred to more biologically plausible neural networks by
implementing a self-optimizing spiking neural network model. In addition, by
using this spiking neural network to emulate a Hopfield network with Hebbian
learning, we attempt to make a connection between rate-based and temporal
coding based neural systems. Although further work is required to make this
model more realistic, it already suggests that the efficacy of the
self-optimizing process is independent from the simplifying assumptions of a
conventional Hopfield network. We also discuss natural and cultural processes
that could be responsible for occasional alteration of neural firing patterns
in actual brains",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1407.5949v2,2014-12-18T17:04:23Z,2014-07-22T17:25:50Z,Deep Recurrent Neural Networks for Time Series Prediction,"Ability of deep networks to extract high level features and of recurrent
networks to perform time-series inference have been studied. In view of
universality of one hidden layer network at approximating functions under weak
constraints, the benefit of multiple layers is to enlarge the space of
dynamical systems approximated or, given the space, reduce the number of units
required for a certain error. Traditionally shallow networks with manually
engineered features are used, back-propagation extent is limited to one and
attempt to choose a large number of hidden units to satisfy the Markov
condition is made. In case of Markov models, it has been shown that many
systems need to be modeled as higher order. In the present work, we present
deep recurrent networks with longer backpropagation through time extent as a
solution to modeling systems that are high order and to predicting ahead. We
study epileptic seizure suppression electro-stimulator. Extraction of manually
engineered complex features and prediction employing them has not allowed small
low-power implementations as, to avoid possibility of surgery, extraction of
any features that may be required has to be included. In this solution, a
recurrent neural network performs both feature extraction and prediction. We
prove analytically that adding hidden layers or increasing backpropagation
extent increases the rate of decrease of approximation error. A Dynamic
Programming (DP) training procedure employing matrix operations is derived. DP
and use of matrix operations makes the procedure efficient particularly when
using data-parallel computing. The simulation studies show the geometry of the
parameter space, that the network learns the temporal structure, that
parameters converge while model output displays same dynamic behavior as the
system and greater than .99 Average Detection Rate on all real seizure data
tried.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1407.3502v1,2014-07-13T19:09:30Z,2014-07-13T19:09:30Z,"Automated Real-Time Classification and Decision Making in Massive Data
  Streams from Synoptic Sky Surveys","The nature of scientific and technological data collection is evolving
rapidly: data volumes and rates grow exponentially, with increasing complexity
and information content, and there has been a transition from static data sets
to data streams that must be analyzed in real time. Interesting or anomalous
phenomena must be quickly characterized and followed up with additional
measurements via optimal deployment of limited assets. Modern astronomy
presents a variety of such phenomena in the form of transient events in digital
synoptic sky surveys, including cosmic explosions (supernovae, gamma ray
bursts), relativistic phenomena (black hole formation, jets), potentially
hazardous asteroids, etc. We have been developing a set of machine learning
tools to detect, classify and plan a response to transient events for astronomy
applications, using the Catalina Real-time Transient Survey (CRTS) as a
scientific and methodological testbed. The ability to respond rapidly to the
potentially most interesting events is a key bottleneck that limits the
scientific returns from the current and anticipated synoptic sky surveys.
Similar challenge arise in other contexts, from environmental monitoring using
sensor networks to autonomous spacecraft systems. Given the exponential growth
of data rates, and the time-critical response, we need a fully automated and
robust approach. We describe the results obtained to date, and the possible
future developments.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1308.3015v1,2013-08-14T02:30:40Z,2013-08-14T02:30:40Z,"On Generalized Bayesian Data Fusion with Complex Models in Large Scale
  Networks","Recent advances in communications, mobile computing, and artificial
intelligence have greatly expanded the application space of intelligent
distributed sensor networks. This in turn motivates the development of
generalized Bayesian decentralized data fusion (DDF) algorithms for robust and
efficient information sharing among autonomous agents using probabilistic
belief models. However, DDF is significantly challenging to implement for
general real-world applications requiring the use of dynamic/ad hoc network
topologies and complex belief models, such as Gaussian mixtures or hybrid
Bayesian networks. To tackle these issues, we first discuss some new key
mathematical insights about exact DDF and conservative approximations to DDF.
These insights are then used to develop novel generalized DDF algorithms for
complex beliefs based on mixture pdfs and conditional factors. Numerical
examples motivated by multi-robot target search demonstrate that our methods
lead to significantly better fusion results, and thus have great potential to
enhance distributed intelligent reasoning in sensor networks.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1212.4174v1,2012-12-17T21:43:31Z,2012-12-17T21:43:31Z,Feature Clustering for Accelerating Parallel Coordinate Descent,"Large-scale L1-regularized loss minimization problems arise in
high-dimensional applications such as compressed sensing and high-dimensional
supervised learning, including classification and regression problems.
High-performance algorithms and implementations are critical to efficiently
solving these problems. Building upon previous work on coordinate descent
algorithms for L1-regularized problems, we introduce a novel family of
algorithms called block-greedy coordinate descent that includes, as special
cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and
Thread-Greedy. We give a unified convergence analysis for the family of
block-greedy algorithms. The analysis suggests that block-greedy coordinate
descent can better exploit parallelism if features are clustered so that the
maximum inner product between features in different blocks is small. Our
theoretical convergence analysis is supported with experimental re- sults using
data from diverse real-world applications. We hope that algorithmic approaches
and convergence analysis we provide will not only advance the field, but will
also encourage researchers to systematically explore the design space of
algorithms for solving large-scale L1-regularization problems.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1208.2112v2,2013-01-21T08:12:56Z,2012-08-10T08:36:49Z,Inverse Reinforcement Learning with Gaussian Process,"We present new algorithms for inverse reinforcement learning (IRL, or inverse
optimal control) in convex optimization settings. We argue that finite-space
IRL can be posed as a convex quadratic program under a Bayesian inference
framework with the objective of maximum a posterior estimation. To deal with
problems in large or even infinite state space, we propose a Gaussian process
model and use preference graphs to represent observations of decision
trajectories. Our method is distinguished from other approaches to IRL in that
it makes no assumptions about the form of the reward function and yet it
retains the promise of computationally manageable implementations for potential
real-world applications. In comparison with an establish algorithm on
small-scale numerical problems, our method demonstrated better accuracy in
apprenticeship learning and a more robust dependence on the number of
observations.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1204.1653v1,2012-04-07T16:34:20Z,2012-04-07T16:34:20Z,Machine Cognition Models: EPAM and GPS,"Through history, the human being tried to relay its daily tasks to other
creatures, which was the main reason behind the rise of civilizations. It
started with deploying animals to automate tasks in the field of
agriculture(bulls), transportation (e.g. horses and donkeys), and even
communication (pigeons). Millenniums after, come the Golden age with
""Al-jazari"" and other Muslim inventors, which were the pioneers of automation,
this has given birth to industrial revolution in Europe, centuries after. At
the end of the nineteenth century, a new era was to begin, the computational
era, the most advanced technological and scientific development that is driving
the mankind and the reason behind all the evolutions of science; such as
medicine, communication, education, and physics. At this edge of technology
engineers and scientists are trying to model a machine that behaves the same as
they do, which pushed us to think about designing and implementing ""Things
that-Thinks"", then artificial intelligence was. In this work we will cover each
of the major discoveries and studies in the field of machine cognition, which
are the ""Elementary Perceiver and Memorizer""(EPAM) and ""The General Problem
Solver""(GPS). The First one focus mainly on implementing the human-verbal
learning behavior, while the second one tries to model an architecture that is
able to solve problems generally (e.g. theorem proving, chess playing, and
arithmetic). We will cover the major goals and the main ideas of each model, as
well as comparing their strengths and weaknesses, and finally giving their
fields of applications. And Finally, we will suggest a real life implementation
of a cognitive machine.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1201.6626v1,2012-01-31T17:26:17Z,2012-01-31T17:26:17Z,Learning RoboCup-Keepaway with Kernels,"We apply kernel-based methods to solve the difficult reinforcement learning
problem of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in
keepaway are the high-dimensionality of the state space (rendering conventional
discretization-based function approximation like tilecoding infeasible), the
stochasticity due to noise and multiple learning agents needing to cooperate
(meaning that the exact dynamics of the environment are unknown) and real-time
learning (meaning that an efficient online implementation is required). We
employ the general framework of approximate policy iteration with
least-squares-based policy evaluation. As underlying function approximator we
consider the family of regularization networks with subset of regressors
approximation. The core of our proposed solution is an efficient recursive
implementation with automatic supervised selection of relevant basis functions.
Simulation results indicate that the behavior learned through our approach
clearly outperforms the best results obtained earlier with tilecoding by Stone
et al. (2005).",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1107.5462v1,2011-07-27T13:07:39Z,2011-07-27T13:07:39Z,HyFlex: A Benchmark Framework for Cross-domain Heuristic Search,"Automating the design of heuristic search methods is an active research field
within computer science, artificial intelligence and operational research. In
order to make these methods more generally applicable, it is important to
eliminate or reduce the role of the human expert in the process of designing an
effective methodology to solve a given computational search problem.
Researchers developing such methodologies are often constrained on the number
of problem domains on which to test their adaptive, self-configuring
algorithms; which can be explained by the inherent difficulty of implementing
their corresponding domain specific software components.
  This paper presents HyFlex, a software framework for the development of
cross-domain search methodologies. The framework features a common software
interface for dealing with different combinatorial optimisation problems, and
provides the algorithm components that are problem specific. In this way, the
algorithm designer does not require a detailed knowledge the problem domains,
and thus can concentrate his/her efforts in designing adaptive general-purpose
heuristic search algorithms. Four hard combinatorial problems are fully
implemented (maximum satisfiability, one dimensional bin packing, permutation
flow shop and personnel scheduling), each containing a varied set of instance
data (including real-world industrial applications) and an extensive set of
problem specific heuristics and search operators. The framework forms the basis
for the first International Cross-domain Heuristic Search Challenge (CHeSC),
and it is currently in use by the international research community. In summary,
HyFlex represents a valuable new benchmark of heuristic search generality, with
which adaptive cross-domain algorithms are being easily developed, and reliably
compared.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/1106.0134v1,2011-06-01T09:56:49Z,2011-06-01T09:56:49Z,"ProDiGe: PRioritization Of Disease Genes with multitask machine learning
  from positive and unlabeled examples","Elucidating the genetic basis of human diseases is a central goal of genetics
and molecular biology. While traditional linkage analysis and modern
high-throughput techniques often provide long lists of tens or hundreds of
disease gene candidates, the identification of disease genes among the
candidates remains time-consuming and expensive. Efficient computational
methods are therefore needed to prioritize genes within the list of candidates,
by exploiting the wealth of information available about the genes in various
databases. Here we propose ProDiGe, a novel algorithm for Prioritization of
Disease Genes. ProDiGe implements a novel machine learning strategy based on
learning from positive and unlabeled examples, which allows to integrate
various sources of information about the genes, to share information about
known disease genes across diseases, and to perform genome-wide searches for
new disease genes. Experiments on real data show that ProDiGe outperforms
state-of-the-art methods for the prioritization of genes in human diseases.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/0908.3934v3,2010-06-22T04:05:08Z,2009-08-27T16:33:52Z,"A framework for simulating and estimating the state and functional
  topology of complex dynamic geometric networks","We present a framework for simulating signal propagation in geometric
networks (i.e. networks that can be mapped to geometric graphs in some space)
and for developing algorithms that estimate (i.e. map) the state and functional
topology of complex dynamic geometric net- works. Within the framework we
define the key features typically present in such networks and of particular
relevance to biological cellular neural networks: Dynamics, signaling,
observation, and control. The framework is particularly well-suited for
estimating functional connectivity in cellular neural networks from
experimentally observable data, and has been implemented using graphics
processing unit (GPU) high performance computing. Computationally, the
framework can simulate cellular network signaling close to or faster than real
time. We further propose a standard test set of networks to measure performance
and compare different mapping algorithms.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/0810.4945v1,2008-10-27T21:39:04Z,2008-10-27T21:39:04Z,New Approaches to Object Classification in Synoptic Sky Surveys,"Digital synoptic sky surveys pose several new object classification
challenges. In surveys where real-time detection and classification of
transient events is a science driver, there is a need for an effective
elimination of instrument-related artifacts which can masquerade as transient
sources in the detection pipeline, e.g., unremoved large cosmic rays,
saturation trails, reflections, crosstalk artifacts, etc. We have implemented
such an Artifact Filter, using a supervised neural network, for the real-time
processing pipeline in the Palomar-Quest (PQ) survey. After the training phase,
for each object it takes as input a set of measured morphological parameters
and returns the probability of it being a real object. Despite the relatively
low number of training cases for many kinds of artifacts, the overall artifact
classification rate is around 90%, with no genuine transients misclassified
during our real-time scans. Another question is how to assign an optimal
star-galaxy classification in a multi-pass survey, where seeing and other
conditions change between different epochs, potentially producing inconsistent
classifications for the same object. We have implemented a star/galaxy
multipass classifier that makes use of external and a priori knowledge to find
the optimal classification from the individually derived ones. Both these
techniques can be applied to other, similar surveys and data sets.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/math/0401157v1,2004-01-14T16:56:55Z,2004-01-14T16:56:55Z,Generalized PSK in Space Time Coding,"A wireless communication system using multiple antennas promises reliable
transmission under Rayleigh flat fading assumptions. Design criteria and
practical schemes have been presented for both coherent and non-coherent
communication channels. In this paper we generalize one dimensional phase shift
keying (PSK) signals and introduce space time constellations from generalized
phase shift keying (GPSK) signals based on the complex and real orthogonal
designs. The resulting space time constellations reallocate the energy for each
transmitting antenna and feature good diversity products, consequently their
performances are better than some of the existing comparable codes. Moreover
since the maximum likelihood (ML) decoding of our proposed codes can be
decomposed to one dimensional PSK signal demodulation, the ML decoding of our
codes can be implemented in a very efficient way.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
http://arxiv.org/abs/adap-org/9807003v1,1998-07-17T01:07:50Z,1998-07-17T01:07:50Z,Development and Evolution of Neural Networks in an Artificial Chemistry,"We present a model of decentralized growth for Artificial Neural Networks
(ANNs) inspired by the development and the physiology of real nervous systems.
In this model, each individual artificial neuron is an autonomous unit whose
behavior is determined only by the genetic information it harbors and local
concentrations of substrates modeled by a simple artificial chemistry. Gene
expression is manifested as axon and dendrite growth, cell division and
differentiation, substrate production and cell stimulation. We demonstrate the
model's power with a hand-written genome that leads to the growth of a simple
network which performs classical conditioning. To evolve more complex
structures, we implemented a platform-independent, asynchronous, distributed
Genetic Algorithm (GA) that allows users to participate in evolutionary
experiments via the World Wide Web.",arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
